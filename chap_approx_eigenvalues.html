<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US">
<head xmlns:og="http://ogp.me/ns#" xmlns:book="https://ogp.me/ns/book#">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Approximating Eigenvalues and Eigenvectors</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:type" content="book">
<meta property="book:title" content="An Inquiry-Based Introduction to Linear Algebra and Applications">
<meta property="book:author" content="Feryal Alayont">
<meta property="book:author" content="Steven Schlicker">
<script>var runestoneMathReady = new Promise((resolve) => window.rsMathReady = resolve);
window.MathJax = {
  tex: {
    inlineMath: [['\\(','\\)']],
    tags: "none",
    tagSide: "right",
    tagIndent: ".8em",
    packages: {'[+]': ['base', 'extpfeil', 'ams', 'amscd', 'newcommand', 'knowl']}
  },
  options: {
    ignoreHtmlClass: "tex2jax_ignore|ignore-math",
    processHtmlClass: "process-math",
  },
  chtml: {
    scale: 0.88,
    mtextInheritFont: true
  },
  loader: {
    load: ['input/asciimath', '[tex]/extpfeil', '[tex]/amscd', '[tex]/newcommand', '[pretext]/mathjaxknowl3.js'],
    paths: {pretext: "https://pretextbook.org/js/lib"},
  },
  startup: {
    pageReady() {
      return MathJax.startup.defaultPageReady().then(function () {
      console.log("in ready function");
      rsMathReady();
      }
    )}
},
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/themes/prism.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/components/prism-core.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/autoloader/prism-autoloader.min.js"></script><script src="https://unpkg.com/lunr/lunr.js"></script><script src="lunr-pretext-search-index.js"></script><script src="https://pretextbook.org/js/0.13/pretext_search.js"></script><link href="https://pretextbook.org/css/0.4/pretext_search.css" rel="stylesheet" type="text/css">
<script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.13/pretext.js"></script><script>miniversion=0.674</script><script src="https://pretextbook.org/js/0.13/pretext_add_on.js?x=1"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script>sagecellEvalName='Evaluate (Sage)';
</script><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/banner_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/toc_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/knowls_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/style_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/colors_blue_grey.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/setcolors.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="external/custom_style.css">
</head>
<body class="pretext-book ignore-math has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div id="latex-macros" class="hidden-content process-math" style="display:none"><span class="process-math">\(\require{colortbl}\usepackage{amsmath}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\E}{\mathbb{E}}
\DeclareMathOperator{\ch}{char}
\newcommand{\N}{\mathbb{N}}
\newcommand{\W}{\mathbb{W}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\J}{\mathbb{J}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathbb{D}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\NE}{\mathcal{E}}
\newcommand{\Mn}[1]{\mathcal{M}_{#1 \times #1}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\Rn}{\R^n}
\newcommand{\Mat}{\mathbf}
\newcommand{\Seq}{\boldsymbol}
\newcommand{\seq}[1]{\boldsymbol{#1}}
\newcommand{\set}[1]{\mathcal{#1}}
\newcommand{\abs}[1]{\left\lvert{}#1\right\rvert}
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\cq}{\scalebox{.34}{\pscirclebox{ \textbf{?}}}}
\newcommand{\cqup}{\,$^{\text{\scalebox{.2}{\pscirclebox{\textbf{?}}}}}$}
\newcommand{\cqupmath}{\,^{\text{\scalebox{.2}{\pscirclebox{\textbf{?}}}}}}
\newcommand{\cqupmathnospace}{^{\text{\scalebox{.2}{\pscirclebox{\textbf{?}}}}}}
\newcommand{\uspace}[1]{\underline{}}
\newcommand{\muspace}[1]{\underline{\mspace{#1 mu}}}
\newcommand{\bspace}[1]{}
\newcommand{\ie}{\emph{i}.\emph{e}.}
\newcommand{\nq}[1]{\scalebox{.34}{\pscirclebox{\textbf{#1}}}}
\newcommand{\equalwhy}{\stackrel{\cqupmath}{=}}
\newcommand{\notequalwhy}{\stackrel{\cqupmath}{\neq}}
\newcommand{\Ker}{\text{Ker}}
\newcommand{\Image}{\text{Im}}
\newcommand{\polyp}{p(x) = a_nx^n + a_{n-1}x^{n-1} + \cdots a_2x^2 + a_1x + a_0}
\newcommand{\GL}{\text{GL}}
\newcommand{\SL}{\text{SL}}
\newcommand{\lcm}{\text{lcm}}
\newcommand{\Aut}{\text{Aut}}
\newcommand{\Inn}{\text{Inn}}
\newcommand{\Hol}{\text{Hol}}
\newcommand{\cl}{\text{cl}}
\newcommand{\bsq}{\hfill $\blacksquare$}
\newcommand{\NIMdot}{{ $\cdot$ }}
\newcommand{\eG}{e_{\scriptscriptstyle{G}}}
\newcommand{\eGroup}[1]{e_{\scriptscriptstyle{#1}}}
\newcommand{\Gdot}[1]{\cdot_{\scriptscriptstyle{#1}}}
\newcommand{\rbar}{\overline{r}}
\newcommand{\nuG}[1]{\nu_{\scriptscriptstyle{#1}}}
\newcommand{\rightarray}[2]{\left[ \begin{array}{#1} #2 \end{array} \right]}
\newcommand{\polymod}[1]{\mspace{5 mu}(\text{mod} #1)}
\newcommand{\ts}{\mspace{2 mu}}
\newcommand{\ds}{\displaystyle}
\newcommand{\adj}{\text{adj}}
\newcommand{\pol}{\mathbb{P}}
\newcommand{\CS}{\mathcal{S}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\CB}{\mathcal{B}}
\renewcommand{\CD}{\mathcal{D}}
\newcommand{\CP}{\mathcal{P}}
\newcommand{\CQ}{\mathcal{Q}}
\newcommand{\CW}{\mathcal{W}}
\newcommand{\rank}{\text{rank}}
\newcommand{\nullity}{\text{nullity}}
\newcommand{\trace}{\text{trace}}
\newcommand{\Area}{\text{Area}}
\newcommand{\Vol}{\text{Vol}}
\newcommand{\cov}{\text{cov}}
\newcommand{\var}{\text{var}}
\newcommand{\va}{\mathbf{a}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\vc}{\mathbf{c}}
\newcommand{\vd}{\mathbf{d}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\vf}{\mathbf{f}}
\newcommand{\vg}{\mathbf{g}}
\newcommand{\vh}{\mathbf{h}}
\newcommand{\vm}{\mathbf{m}}
\newcommand{\vn}{\mathbf{n}}
\newcommand{\vp}{\mathbf{p}}
\newcommand{\vq}{\mathbf{q}}
\newcommand{\vr}{\mathbf{r}}
\newcommand{\vs}{\mathbf{s}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\vzero}{\mathbf{0}}
\newcommand{\vi}{\mathbf{i}}
\newcommand{\vj}{\mathbf{j}}
\newcommand{\vk}{\mathbf{k}}
\newcommand{\vF}{\mathbf{F}}
\newcommand{\vP}{\mathbf{P}}
\newcommand{\nin}{}
\newcommand{\Dom}{\text{Dom}}
\newcommand{\tr}{\mathsf{T}}
\newcommand{\proj}{\text{proj}}
\newcommand{\comp}{\text{comp}}
\newcommand{\Row}{\text{Row }}
\newcommand{\Col}{\text{Col }}
\newcommand{\Nul}{\text{Nul }}
\newcommand{\Span}{\text{Span}}
\newcommand{\Range}{\text{Range}}
\newcommand{\Domain}{\text{Domain}}
\newcommand{\hthin}{\hlinewd{.1pt}}
\newcommand{\hthick}{\hlinewd{.7pt}}
\newcommand{\pbreaks}{1}
\newcommand{\pbreak}{
}
\newcommand{\lint}{\underline{}
\int}
\newcommand{\uint}{ \underline{}
\int}


\newcommand{\Si}{\text{Si}}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\definecolor{fillinmathshade}{gray}{0.9}
\newcommand{\fillinmath}[1]{\mathchoice{\colorbox{fillinmathshade}{$\displaystyle     \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\textstyle        \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptstyle      \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptscriptstyle\phantom{\,#1\,}$}}}
\)</span></div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="book-1.html"><span class="title">An Inquiry-Based Introduction to Linear Algebra and Applications</span></a></h1>
<p class="byline">Feryal Alayont, Steven Schlicker</p>
</div>
<div class="searchbox"><div class="searchwidget">
<input id="ptxsearch" type="text" name="terms" placeholder="Search" onchange="doSearch()"><button id="searchbutton" type="button" onclick="doSearch()">üîç</button>
</div></div>
<div id="searchresultsplaceholder" style="display: none">
<button id="closesearchresults" onclick="document.getElementById('searchresultsplaceholder').style.display = 'none'; return false;">x</button><h2>Search Results: <span id="searchterms"></span>
</h2>
<ol id="searchresults"></ol>
</div>
</div></div>
<nav id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3">
<a class="index-button toolbar-item button" href="index-1.html" title="Index">Index</a><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="chap_diagonalization.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="part-eigen.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="chap_complex_eigenvalues.html" title="Next">Next</a></span>
</div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="chap_diagonalization.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="part-eigen.html" title="Up">Up</a><a class="next-button button toolbar-item" href="chap_complex_eigenvalues.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link frontmatter">
<a href="frontmatter.html" data-scroll="frontmatter" class="internal"><span class="title">Front Matter</span></a><ul><li><a href="fm_preface.html" data-scroll="fm_preface" class="internal">Preface</a></li></ul>
</li>
<li class="link part"><a href="part-systems.html" data-scroll="part-systems" class="internal"><span class="codenumber">I</span> <span class="title">Systems of Linear Equations</span></a></li>
<li class="link">
<a href="chap_intro_linear_systems.html" data-scroll="chap_intro_linear_systems" class="internal"><span class="codenumber">1</span> <span class="title">Introduction to Systems of Linear Equations</span></a><ul>
<li><a href="chap_intro_linear_systems.html#sec_appl_elec_circuits" data-scroll="sec_appl_elec_circuits" class="internal">Application: Electrical Circuits</a></li>
<li><a href="chap_intro_linear_systems.html#sec_intro_le_intro" data-scroll="sec_intro_le_intro" class="internal">Introduction</a></li>
<li><a href="chap_intro_linear_systems.html#sec_notation" data-scroll="sec_notation" class="internal">Notation and Terminology</a></li>
<li><a href="chap_intro_linear_systems.html#sec_solve_systems" data-scroll="sec_solve_systems" class="internal">Solving Systems of Linear Equations</a></li>
<li><a href="chap_intro_linear_systems.html#sec_geom_solu_sets" data-scroll="sec_geom_solu_sets" class="internal">The Geometry of Solution Sets of Linear Systems</a></li>
<li><a href="chap_intro_linear_systems.html#sec_intro_le_exam" data-scroll="sec_intro_le_exam" class="internal">Examples</a></li>
<li><a href="chap_intro_linear_systems.html#sec_intro_le_summ" data-scroll="sec_intro_le_summ" class="internal">Summary</a></li>
<li><a href="chap_intro_linear_systems.html#sec_intro_le_exer" data-scroll="sec_intro_le_exer" class="internal">Exercises</a></li>
<li><a href="chap_intro_linear_systems.html#sec_1_a_circuits" data-scroll="sec_1_a_circuits" class="internal">Project: Modeling an Electrical Circuit and the Wheatstone Bridge Circuit</a></li>
</ul>
</li>
<li class="link">
<a href="chap_matrix_representation.html" data-scroll="chap_matrix_representation" class="internal"><span class="codenumber">2</span> <span class="title">The Matrix Representation of a Linear System</span></a><ul>
<li><a href="chap_matrix_representation.html#sec_appl_area_curve" data-scroll="sec_appl_area_curve" class="internal">Application: Approximating Area Under a Curve</a></li>
<li><a href="chap_matrix_representation.html#sec_mtx_lin_intro" data-scroll="sec_mtx_lin_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_representation.html#sec_simp_mtx_sys" data-scroll="sec_simp_mtx_sys" class="internal">Simplifying Linear Systems Represented in Matrix Form</a></li>
<li><a href="chap_matrix_representation.html#sec_sys_inf_sols" data-scroll="sec_sys_inf_sols" class="internal">Linear Systems with Infinitely Many Solutions</a></li>
<li><a href="chap_matrix_representation.html#sec_sys_no_sols" data-scroll="sec_sys_no_sols" class="internal">Linear Systems with No Solutions</a></li>
<li><a href="chap_matrix_representation.html#sec_mtx_sys_exam" data-scroll="sec_mtx_sys_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_representation.html#sec_mtx_sys_summ" data-scroll="sec_mtx_sys_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_representation.html#sec_mtx_sys_exer" data-scroll="sec_mtx_sys_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_representation.html#sec_1_b_polynomial" data-scroll="sec_1_b_polynomial" class="internal">Project: Polynomial Interpolation to Approximate the Area Under a Curve</a></li>
</ul>
</li>
<li class="link">
<a href="chap_row_echelon_forms.html" data-scroll="chap_row_echelon_forms" class="internal"><span class="codenumber">3</span> <span class="title">Row Echelon Forms</span></a><ul>
<li><a href="chap_row_echelon_forms.html#sec_appl_chem_react" data-scroll="sec_appl_chem_react" class="internal">Application: Balancing Chemical Reactions</a></li>
<li><a href="chap_row_echelon_forms.html#sec_row_ech_intro" data-scroll="sec_row_ech_intro" class="internal">Introduction</a></li>
<li><a href="chap_row_echelon_forms.html#sec_mtx_ech_forms" data-scroll="sec_mtx_ech_forms" class="internal">The Echelon Forms of a Matrix</a></li>
<li><a href="chap_row_echelon_forms.html#sec_num_sols_ls" data-scroll="sec_num_sols_ls" class="internal">Determining the Number of Solutions of a Linear System</a></li>
<li><a href="chap_row_echelon_forms.html#sec_prod_ech_forms" data-scroll="sec_prod_ech_forms" class="internal">Producing the Echelon Forms</a></li>
<li><a href="chap_row_echelon_forms.html#sec_row_ech_exam" data-scroll="sec_row_ech_exam" class="internal">Examples</a></li>
<li><a href="chap_row_echelon_forms.html#sec_row_ech_summ" data-scroll="sec_row_ech_summ" class="internal">Summary</a></li>
<li><a href="chap_row_echelon_forms.html#sec_row_ech_exer" data-scroll="sec_row_ech_exer" class="internal">Exercises</a></li>
<li><a href="chap_row_echelon_forms.html#sec_prof_chem_react" data-scroll="sec_prof_chem_react" class="internal">Project: Modeling a Chemical Reaction</a></li>
</ul>
</li>
<li class="link">
<a href="chap_vector_representation.html" data-scroll="chap_vector_representation" class="internal"><span class="codenumber">4</span> <span class="title">Vector Representation</span></a><ul>
<li><a href="chap_vector_representation.html#sec_appl_knight" data-scroll="sec_appl_knight" class="internal">Application: The Knight's Tour</a></li>
<li><a href="chap_vector_representation.html#sec_vec_rep_intro" data-scroll="sec_vec_rep_intro" class="internal">Introduction</a></li>
<li><a href="chap_vector_representation.html#sec_vec_ops" data-scroll="sec_vec_ops" class="internal">Vectors and Vector Operations</a></li>
<li><a href="chap_vector_representation.html#sec_geom_vec_ops" data-scroll="sec_geom_vec_ops" class="internal">Geometric Representation of Vectors and Vector Operations</a></li>
<li><a href="chap_vector_representation.html#sec_lin_comb_vec" data-scroll="sec_lin_comb_vec" class="internal">Linear Combinations of Vectors</a></li>
<li><a href="chap_vector_representation.html#sec_vec_span" data-scroll="sec_vec_span" class="internal">The Span of a Set of Vectors</a></li>
<li><a href="chap_vector_representation.html#sec_vec_rep_exam" data-scroll="sec_vec_rep_exam" class="internal">Examples</a></li>
<li><a href="chap_vector_representation.html#sec_vec_rep_summ" data-scroll="sec_vec_rep_summ" class="internal">Summary</a></li>
<li><a href="chap_vector_representation.html#exercises-4" data-scroll="exercises-4" class="internal">Exercises</a></li>
<li><a href="chap_vector_representation.html#sec_proj_knight" data-scroll="sec_proj_knight" class="internal">Project: Analyzing Knight Moves</a></li>
</ul>
</li>
<li class="link">
<a href="chap_matrix_vector.html" data-scroll="chap_matrix_vector" class="internal"><span class="codenumber">5</span> <span class="title">The Matrix-Vector Form of a Linear System</span></a><ul>
<li><a href="chap_matrix_vector.html#sec_appl_model_econ" data-scroll="sec_appl_model_econ" class="internal">Application: Modeling an Economy</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form_intro" data-scroll="sec_mv_form_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_prod" data-scroll="sec_mv_prod" class="internal">The Matrix-Vector Product</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form" data-scroll="sec_mv_form" class="internal">The Matrix-Vector Form of a Linear System</a></li>
<li><a href="chap_matrix_vector.html#sec_homog_sys" data-scroll="sec_homog_sys" class="internal">Homogeneous and Nonhomogeneous Systems</a></li>
<li><a href="chap_matrix_vector.html#sec_geom_homog_sys" data-scroll="sec_geom_homog_sys" class="internal">The Geometry of Solutions to the Homogeneous System</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form_exam" data-scroll="sec_mv_form_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form_summ" data-scroll="sec_mv_form_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form_exer" data-scroll="sec_mv_form_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_vector.html#sec_proj_io_models" data-scroll="sec_proj_io_models" class="internal">Project: Input-Output Models</a></li>
</ul>
</li>
<li class="link">
<a href="chap_independence.html" data-scroll="chap_independence" class="internal"><span class="codenumber">6</span> <span class="title">Linear Dependence and Independence</span></a><ul>
<li><a href="chap_independence.html#sec_appl_bezier" data-scroll="sec_appl_bezier" class="internal">Application: B√©zier Curves</a></li>
<li><a href="chap_independence.html#sec_indep_intro" data-scroll="sec_indep_intro" class="internal">Introduction</a></li>
<li><a href="chap_independence.html#lin_indep_intro_new" data-scroll="lin_indep_intro_new" class="internal">Linear Independence</a></li>
<li><a href="chap_independence.html#sec_determ_lin_ind" data-scroll="sec_determ_lin_ind" class="internal">Determining Linear Independence</a></li>
<li><a href="chap_independence.html#sec_min_span_set" data-scroll="sec_min_span_set" class="internal">Minimal Spanning Sets</a></li>
<li><a href="chap_independence.html#sec_indep_exam" data-scroll="sec_indep_exam" class="internal">Examples</a></li>
<li><a href="chap_independence.html#sec_indep_summ" data-scroll="sec_indep_summ" class="internal">Summary</a></li>
<li><a href="chap_independence.html#sec_indep_exer" data-scroll="sec_indep_exer" class="internal">Exercises</a></li>
<li><a href="chap_independence.html#sec_proj_bezier" data-scroll="sec_proj_bezier" class="internal">Project: Generating B√©zier Curves</a></li>
</ul>
</li>
<li class="link">
<a href="chap_matrix_transformations.html" data-scroll="chap_matrix_transformations" class="internal"><span class="codenumber">7</span> <span class="title">Matrix Transformations</span></a><ul>
<li><a href="chap_matrix_transformations.html#sec_appl_graphics" data-scroll="sec_appl_graphics" class="internal">Application: Computer Graphics</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_intro" data-scroll="sec_mtx_trans_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_prop" data-scroll="sec_mtx_trans_prop" class="internal">Properties of Matrix Transformations</a></li>
<li><a href="chap_matrix_transformations.html#sec_trans_onto_oto" data-scroll="sec_trans_onto_oto" class="internal">Onto and One-to-One Transformations</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_exam" data-scroll="sec_mtx_trans_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_summ" data-scroll="sec_mtx_trans_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_exer" data-scroll="sec_mtx_trans_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_transformations.html#sec_proj_geom_mtx" data-scroll="sec_proj_geom_mtx" class="internal">Project: The Geometry of Matrix Transformations</a></li>
</ul>
</li>
<li class="link part"><a href="part-matrices.html" data-scroll="part-matrices" class="internal"><span class="codenumber">II</span> <span class="title">Matrices</span></a></li>
<li class="link">
<a href="chap_matrix_operations.html" data-scroll="chap_matrix_operations" class="internal"><span class="codenumber">8</span> <span class="title">Matrix Operations</span></a><ul>
<li><a href="chap_matrix_operations.html#sec_appl_mtx_mult" data-scroll="sec_appl_mtx_mult" class="internal">Application: Algorithms for Matrix Multiplication</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_ops_intro" data-scroll="sec_mtx_ops_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_add_smult" data-scroll="sec_mtx_add_smult" class="internal">Properties of Matrix Addition and Multiplication by Scalars</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_prod" data-scroll="sec_mtx_prod" class="internal">A Matrix Product</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_transpose" data-scroll="sec_mtx_transpose" class="internal">The Transpose of a Matrix</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_transpose_prop" data-scroll="sec_mtx_transpose_prop" class="internal">Properties of the Matrix Transpose</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_ops_exam" data-scroll="sec_mtx_ops_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_ops_summ" data-scroll="sec_mtx_ops_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_ops_exer" data-scroll="sec_mtx_ops_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_operations.html#sec_proj_starassen" data-scroll="sec_proj_starassen" class="internal">Project: Strassen's Algorithm and Partitioned Matrices</a></li>
</ul>
</li>
<li class="link">
<a href="chap_intro_eigenvals_eigenvects.html" data-scroll="chap_intro_eigenvals_eigenvects" class="internal"><span class="codenumber">9</span> <span class="title">Introduction to Eigenvalues and Eigenvectors</span></a><ul>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_appl_pagerank" data-scroll="sec_appl_pagerank" class="internal">Application: The Google PageRank Algorithm</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigen_intro" data-scroll="sec_eigen_intro" class="internal">Introduction</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigval_eigvec" data-scroll="sec_eigval_eigvec" class="internal">Eigenvalues and Eigenvectors</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_dynam_sys" data-scroll="sec_dynam_sys" class="internal">Dynamical Systems</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigen_exam" data-scroll="sec_eigen_exam" class="internal">Examples</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigen_summ" data-scroll="sec_eigen_summ" class="internal">Summary</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigen_exer" data-scroll="sec_eigen_exer" class="internal">Exercises</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_proj_pagerank" data-scroll="sec_proj_pagerank" class="internal">Project: Understanding the PageRank Algorithm</a></li>
</ul>
</li>
<li class="link">
<a href="chap_matrix_inverse.html" data-scroll="chap_matrix_inverse" class="internal"><span class="codenumber">10</span> <span class="title">The Inverse of a Matrix</span></a><ul>
<li><a href="chap_matrix_inverse.html#sec_appl_arms_race" data-scroll="sec_appl_arms_race" class="internal">Application: Modeling an Arms Race</a></li>
<li><a href="chap_matrix_inverse.html#sec_inverse_intro" data-scroll="sec_inverse_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_inverse.html#sec_mtx_invertible" data-scroll="sec_mtx_invertible" class="internal">Invertible Matrices</a></li>
<li><a href="chap_matrix_inverse.html#sec_mtx_inverse" data-scroll="sec_mtx_inverse" class="internal">Finding the Inverse of a Matrix</a></li>
<li><a href="chap_matrix_inverse.html#sec_mtx_inverse_props" data-scroll="sec_mtx_inverse_props" class="internal">Properties of the Matrix Inverse</a></li>
<li><a href="chap_matrix_inverse.html#sec_inverse_exam" data-scroll="sec_inverse_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_inverse.html#sec_inverse_summ" data-scroll="sec_inverse_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_inverse.html#sec_inverse_exer" data-scroll="sec_inverse_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_inverse.html#sec_proj_arms_race" data-scroll="sec_proj_arms_race" class="internal">Project: The Richardson Arms Race Model</a></li>
</ul>
</li>
<li class="link">
<a href="chap_IMT.html" data-scroll="chap_IMT" class="internal"><span class="codenumber">11</span> <span class="title">The Invertible Matrix Theorem</span></a><ul>
<li><a href="chap_IMT.html#sec_imt_intro" data-scroll="sec_imt_intro" class="internal">Introduction</a></li>
<li><a href="chap_IMT.html#sec_imt" data-scroll="sec_imt" class="internal">The Invertible Matrix Theorem</a></li>
<li><a href="chap_IMT.html#sec_imt_exam" data-scroll="sec_imt_exam" class="internal">Examples</a></li>
<li><a href="chap_IMT.html#sec_imt_summ" data-scroll="sec_imt_summ" class="internal">Summary</a></li>
<li><a href="chap_IMT.html#sec_imt_exer" data-scroll="sec_imt_exer" class="internal">Exercises</a></li>
</ul>
</li>
<li class="link part"><a href="part-vector-rn.html" data-scroll="part-vector-rn" class="internal"><span class="codenumber">III</span> <span class="title">The Vector Space <span class="process-math">\(\R^n\)</span></span></a></li>
<li class="link">
<a href="chap_R_n.html" data-scroll="chap_R_n" class="internal"><span class="codenumber">12</span> <span class="title">The Structure of <span class="process-math">\(\R^n\)</span></span></a><ul>
<li><a href="chap_R_n.html#sec_appl_romania" data-scroll="sec_appl_romania" class="internal">Application: Connecting GDP and Consumption in Romania</a></li>
<li><a href="chap_R_n.html#sec_rn_intro" data-scroll="sec_rn_intro" class="internal">Introduction</a></li>
<li><a href="chap_R_n.html#sec_vec_spaces" data-scroll="sec_vec_spaces" class="internal">Vector Spaces</a></li>
<li><a href="chap_R_n.html#sec_sub_space_span" data-scroll="sec_sub_space_span" class="internal">The Subspace Spanned by a Set of Vectors</a></li>
<li><a href="chap_R_n.html#sec_rn_exam" data-scroll="sec_rn_exam" class="internal">Examples</a></li>
<li><a href="chap_R_n.html#sec_rn_summ" data-scroll="sec_rn_summ" class="internal">Summary</a></li>
<li><a href="chap_R_n.html#sec_rn_exer" data-scroll="sec_rn_exer" class="internal">Exercises</a></li>
<li><a href="chap_R_n.html#sec_proj_ls_approx" data-scroll="sec_proj_ls_approx" class="internal">Project: Least Squares Linear Approximation</a></li>
</ul>
</li>
<li class="link">
<a href="chap_null_space.html" data-scroll="chap_null_space" class="internal"><span class="codenumber">13</span> <span class="title">The Null Space and Column Space of a Matrix</span></a><ul>
<li><a href="chap_null_space.html#sec_appl_lights_out" data-scroll="sec_appl_lights_out" class="internal">Application: The Lights Out Game</a></li>
<li><a href="chap_null_space.html#sec_null_intro" data-scroll="sec_null_intro" class="internal">Introduction</a></li>
<li><a href="chap_null_space.html#sec_null_kernel" data-scroll="sec_null_kernel" class="internal">The Null Space of a Matrix and the Kernel of a Matrix Transformation</a></li>
<li><a href="chap_null_space.html#sec_column_range" data-scroll="sec_column_range" class="internal">The Column Space of a Matrix and the Range of a Matrix Transformation</a></li>
<li><a href="chap_null_space.html#sec_row_space" data-scroll="sec_row_space" class="internal">The Row Space of a Matrix</a></li>
<li><a href="chap_null_space.html#sec_null_col_base" data-scroll="sec_null_col_base" class="internal">Bases for <span class="process-math">\(\Nul A\)</span> and <span class="process-math">\(\Col A\)</span></a></li>
<li><a href="chap_null_space.html#sec_null_exam" data-scroll="sec_null_exam" class="internal">Examples</a></li>
<li><a href="chap_null_space.html#sec_null_summ" data-scroll="sec_null_summ" class="internal">Summary</a></li>
<li><a href="chap_null_space.html#sec_null_exer" data-scroll="sec_null_exer" class="internal">Exercises</a></li>
<li><a href="chap_null_space.html#sec_proj_lights_out" data-scroll="sec_proj_lights_out" class="internal">Project: Solving the Lights Out Game</a></li>
</ul>
</li>
<li class="link">
<a href="chap_eigenspaces.html" data-scroll="chap_eigenspaces" class="internal"><span class="codenumber">14</span> <span class="title">Eigenspaces of a Matrix</span></a><ul>
<li><a href="chap_eigenspaces.html#sec_appl_pop_dynam" data-scroll="sec_appl_pop_dynam" class="internal">Application: Population Dynamics</a></li>
<li><a href="chap_eigenspaces.html#sec_egspace_intro" data-scroll="sec_egspace_intro" class="internal">Introduction</a></li>
<li><a href="chap_eigenspaces.html#sec_mtx_egspace" data-scroll="sec_mtx_egspace" class="internal">Eigenspaces of Matrix</a></li>
<li><a href="chap_eigenspaces.html#sec_lin_ind_egvec" data-scroll="sec_lin_ind_egvec" class="internal">Linearly Independent Eigenvectors</a></li>
<li><a href="chap_eigenspaces.html#sec_egspace_exam" data-scroll="sec_egspace_exam" class="internal">Examples</a></li>
<li><a href="chap_eigenspaces.html#sec_egspace_summ" data-scroll="sec_egspace_summ" class="internal">Summary</a></li>
<li><a href="chap_eigenspaces.html#sec_egspace_exer" data-scroll="sec_egspace_exer" class="internal">Exercises</a></li>
<li><a href="chap_eigenspaces.html#sec_proj_migration" data-scroll="sec_proj_migration" class="internal">Project: Modeling Population Migration</a></li>
</ul>
</li>
<li class="link">
<a href="chap_bases_dimension.html" data-scroll="chap_bases_dimension" class="internal"><span class="codenumber">15</span> <span class="title">Bases and Dimension</span></a><ul>
<li><a href="chap_bases_dimension.html#sec_appl_latt_crypt" data-scroll="sec_appl_latt_crypt" class="internal">Application: Lattice Based Cryptography</a></li>
<li><a href="chap_bases_dimension.html#sec_base_dim_intro" data-scroll="sec_base_dim_intro" class="internal">Introduction</a></li>
<li><a href="chap_bases_dimension.html#sec_dim_sub_rn" data-scroll="sec_dim_sub_rn" class="internal">The Dimension of a Subspace of <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_bases_dimension.html#sec_cond_basis_subspace" data-scroll="sec_cond_basis_subspace" class="internal">Conditions for a Basis of a Subspace of <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_bases_dimension.html#sec_find_basis_subspace" data-scroll="sec_find_basis_subspace" class="internal">Finding a Basis for a Subspace</a></li>
<li><a href="chap_bases_dimension.html#sec_mtx_rank" data-scroll="sec_mtx_rank" class="internal">Rank of a Matrix</a></li>
<li><a href="chap_bases_dimension.html#sec_base_dim_exam" data-scroll="sec_base_dim_exam" class="internal">Examples</a></li>
<li><a href="chap_bases_dimension.html#sec_base_dim_summ" data-scroll="sec_base_dim_summ" class="internal">Summary</a></li>
<li><a href="chap_bases_dimension.html#sec_base_dim_exer" data-scroll="sec_base_dim_exer" class="internal">Exercises</a></li>
<li><a href="chap_bases_dimension.html#sec_proj_ggh_crypto" data-scroll="sec_proj_ggh_crypto" class="internal">Project: The GGH Cryptosystem</a></li>
</ul>
</li>
<li class="link">
<a href="chap_coordinate_vectors.html" data-scroll="chap_coordinate_vectors" class="internal"><span class="codenumber">16</span> <span class="title">Coordinate Vectors and Change of Basis</span></a><ul>
<li><a href="chap_coordinate_vectors.html#sec_appl_orbits" data-scroll="sec_appl_orbits" class="internal">Application: Describing Orbits of Planets</a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_intro" data-scroll="sec_cob_intro" class="internal">Introduction</a></li>
<li><a href="chap_coordinate_vectors.html#sec_coor_base" data-scroll="sec_coor_base" class="internal">Bases as Coordinate Systems in <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_rn" data-scroll="sec_cob_rn" class="internal">Change of Basis in <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_coordinate_vectors.html#sec_mtx_cob" data-scroll="sec_mtx_cob" class="internal">The Change of Basis Matrix in <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_coordinate_vectors.html#sec_prop_mtx_cob" data-scroll="sec_prop_mtx_cob" class="internal">Properties of the Change of Basis Matrix</a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_exam" data-scroll="sec_cob_exam" class="internal">Examples</a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_summ" data-scroll="sec_cob_summ" class="internal">Summary</a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_exer" data-scroll="sec_cob_exer" class="internal">Exercises</a></li>
<li><a href="chap_coordinate_vectors.html#sec_proj_orbits_cob" data-scroll="sec_proj_orbits_cob" class="internal">Project: Planetary Orbits and Change of Basis</a></li>
</ul>
</li>
<li class="link part"><a href="part-eigen.html" data-scroll="part-eigen" class="internal"><span class="codenumber">IV</span> <span class="title">Eigenvalues and Eigenvectors</span></a></li>
<li class="link">
<a href="chap_determinants.html" data-scroll="chap_determinants" class="internal"><span class="codenumber">17</span> <span class="title">The Determinant</span></a><ul>
<li><a href="chap_determinants.html#sec_appl_area_vol" data-scroll="sec_appl_area_vol" class="internal">Application: Area and Volume</a></li>
<li><a href="chap_determinants.html#sec_det_intro" data-scroll="sec_det_intro" class="internal">Introduction</a></li>
<li><a href="chap_determinants.html#sec_det_square" data-scroll="sec_det_square" class="internal">The Determinant of a Square Matrix</a></li>
<li><a href="chap_determinants.html#sec_cofactors" data-scroll="sec_cofactors" class="internal">Cofactors</a></li>
<li><a href="chap_determinants.html#sec_det_3by3" data-scroll="sec_det_3by3" class="internal">The Determinant of a <span class="process-math">\(3 \times 3\)</span> Matrix</a></li>
<li><a href="chap_determinants.html#sec_det_remember" data-scroll="sec_det_remember" class="internal">Two Devices for Remembering Determinants</a></li>
<li><a href="chap_determinants.html#sec_det_exam" data-scroll="sec_det_exam" class="internal">Examples</a></li>
<li><a href="chap_determinants.html#sec_det_summ" data-scroll="sec_det_summ" class="internal">Summary</a></li>
<li><a href="chap_determinants.html#sec_det_exer" data-scroll="sec_det_exer" class="internal">Exercises</a></li>
<li><a href="chap_determinants.html#sec_proj_det_area_vol" data-scroll="sec_proj_det_area_vol" class="internal">Project: Area and Volume Using Determinants</a></li>
</ul>
</li>
<li class="link">
<a href="chap_characteristic_equation.html" data-scroll="chap_characteristic_equation" class="internal"><span class="codenumber">18</span> <span class="title">The Characteristic Equation</span></a><ul>
<li><a href="chap_characteristic_equation.html#sec_appl_thermo" data-scroll="sec_appl_thermo" class="internal">Application: Modeling the Second Law of Thermodynamics</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq_intro" data-scroll="sec_chareq_intro" class="internal">Introduction</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq" data-scroll="sec_chareq" class="internal">The Characteristic Equation</a></li>
<li><a href="chap_characteristic_equation.html#sec_egspace_geom" data-scroll="sec_egspace_geom" class="internal">Eigenspaces, A Geometric Example</a></li>
<li><a href="chap_characteristic_equation.html#sec_egspace_dims" data-scroll="sec_egspace_dims" class="internal">Dimensions of Eigenspaces</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq_exam" data-scroll="sec_chareq_exam" class="internal">Examples</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq_summ" data-scroll="sec_chareq_summ" class="internal">Summary</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq_exer" data-scroll="sec_chareq_exer" class="internal">Exercises</a></li>
<li><a href="chap_characteristic_equation.html#sec_proj_ehrenfest" data-scroll="sec_proj_ehrenfest" class="internal">Project: The Ehrenfest Model</a></li>
</ul>
</li>
<li class="link">
<a href="chap_diagonalization.html" data-scroll="chap_diagonalization" class="internal"><span class="codenumber">19</span> <span class="title">Diagonalization</span></a><ul>
<li><a href="chap_diagonalization.html#sec_appl_fib_num" data-scroll="sec_appl_fib_num" class="internal">Application: The Fibonacci Numbers</a></li>
<li><a href="chap_diagonalization.html#sec_diag_intro" data-scroll="sec_diag_intro" class="internal">Introduction</a></li>
<li><a href="chap_diagonalization.html#sec_diag" data-scroll="sec_diag" class="internal">Diagonalization</a></li>
<li><a href="chap_diagonalization.html#sec_mtx_similar" data-scroll="sec_mtx_similar" class="internal">Similar Matrices</a></li>
<li><a href="chap_diagonalization.html#sec_sim_mtx_trans" data-scroll="sec_sim_mtx_trans" class="internal">Similarity and Matrix Transformations</a></li>
<li><a href="chap_diagonalization.html#sec_diag_general" data-scroll="sec_diag_general" class="internal">Diagonalization in General</a></li>
<li><a href="chap_diagonalization.html#sec_diag_exam" data-scroll="sec_diag_exam" class="internal">Examples</a></li>
<li><a href="chap_diagonalization.html#sec_diag_summ" data-scroll="sec_diag_summ" class="internal">Summary</a></li>
<li><a href="chap_diagonalization.html#sec_diag_exer" data-scroll="sec_diag_exer" class="internal">Exercises</a></li>
<li><a href="chap_diagonalization.html#sec_proj_binet_fibo" data-scroll="sec_proj_binet_fibo" class="internal">Project: Binet's Formula for the Fibonacci Numbers</a></li>
</ul>
</li>
<li class="link active">
<a href="chap_approx_eigenvalues.html" data-scroll="chap_approx_eigenvalues" class="internal"><span class="codenumber">20</span> <span class="title">Approximating Eigenvalues and Eigenvectors</span></a><ul>
<li><a href="chap_approx_eigenvalues.html#sec_appl_leslie_mtx" data-scroll="sec_appl_leslie_mtx" class="internal">Application: Leslie Matrices and Population Modeling</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_app_eigen_intro" data-scroll="sec_app_eigen_intro" class="internal">Introduction</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_power_method" data-scroll="sec_power_method" class="internal">The Power Method</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_power_method_inv" data-scroll="sec_power_method_inv" class="internal">The Inverse Power Method</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_app_eigen_exam" data-scroll="sec_app_eigen_exam" class="internal">Examples</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_app_eigen_summ" data-scroll="sec_app_eigen_summ" class="internal">Summary</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_app_eigen_exer" data-scroll="sec_app_eigen_exer" class="internal">Exercises</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_proj_sheep_herd" data-scroll="sec_proj_sheep_herd" class="internal">Project: Managing a Sheep Herd</a></li>
</ul>
</li>
<li class="link">
<a href="chap_complex_eigenvalues.html" data-scroll="chap_complex_eigenvalues" class="internal"><span class="codenumber">21</span> <span class="title">Complex Eigenvalues</span></a><ul>
<li><a href="chap_complex_eigenvalues.html#sec_appl_gershgorin" data-scroll="sec_appl_gershgorin" class="internal">Application: The Gershgorin Disk Theorem</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen_intro" data-scroll="sec_comp_eigen_intro" class="internal">Introduction</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen" data-scroll="sec_comp_eigen" class="internal">Complex Eigenvalues</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_mtx_rotate_scale" data-scroll="sec_mtx_rotate_scale" class="internal">Rotation and Scaling Matrices</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_mtx_comp_eigen" data-scroll="sec_mtx_comp_eigen" class="internal">Matrices with Complex Eigenvalues</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen_exam" data-scroll="sec_comp_eigen_exam" class="internal">Examples</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen_summ" data-scroll="sec_comp_eigen_summ" class="internal">Summary</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen_exer" data-scroll="sec_comp_eigen_exer" class="internal">Exercises</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_proj_gershgorin" data-scroll="sec_proj_gershgorin" class="internal">Project: Understanding the Gershgorin Disk Theorem</a></li>
</ul>
</li>
<li class="link">
<a href="chap_det_properties.html" data-scroll="chap_det_properties" class="internal"><span class="codenumber">22</span> <span class="title">Properties of Determinants</span></a><ul>
<li><a href="chap_det_properties.html#sec_det_prop_intro" data-scroll="sec_det_prop_intro" class="internal">Introduction</a></li>
<li><a href="chap_det_properties.html#sec_det_row_ops" data-scroll="sec_det_row_ops" class="internal">Elementary Row Operations and Their Effects on the Determinant</a></li>
<li><a href="chap_det_properties.html#sec_mtx_elem" data-scroll="sec_mtx_elem" class="internal">Elementary Matrices</a></li>
<li><a href="chap_det_properties.html#sec_det_geom" data-scroll="sec_det_geom" class="internal">Geometric Interpretation of the Determinant</a></li>
<li><a href="chap_det_properties.html#sec_inv_cramers" data-scroll="sec_inv_cramers" class="internal">An Explicit Formula for the Inverse and Cramer's Rule</a></li>
<li><a href="chap_det_properties.html#sec_det_transpose" data-scroll="sec_det_transpose" class="internal">The Determinant of the Transpose</a></li>
<li><a href="chap_det_properties.html#sec_det_row_swap" data-scroll="sec_det_row_swap" class="internal">Row Swaps and Determinants</a></li>
<li><a href="chap_det_properties.html#sec_cofactor_expand" data-scroll="sec_cofactor_expand" class="internal">Cofactor Expansions</a></li>
<li><a href="chap_det_properties.html#sec_mtx_lu_factor" data-scroll="sec_mtx_lu_factor" class="internal">The LU Factorization of a Matrix</a></li>
<li><a href="chap_det_properties.html#sec_det_prop_exam" data-scroll="sec_det_prop_exam" class="internal">Examples</a></li>
<li><a href="chap_det_properties.html#sec_det_prop_summ" data-scroll="sec_det_prop_summ" class="internal">Summary</a></li>
<li><a href="chap_det_properties.html#sec_det_prop_exer" data-scroll="sec_det_prop_exer" class="internal">Exercises</a></li>
</ul>
</li>
<li class="link part"><a href="part-orthog.html" data-scroll="part-orthog" class="internal"><span class="codenumber">V</span> <span class="title">Orthogonality</span></a></li>
<li class="link">
<a href="chap_dot_product.html" data-scroll="chap_dot_product" class="internal"><span class="codenumber">23</span> <span class="title">The Dot Product in <span class="process-math">\(\R^n\)</span></span></a><ul>
<li><a href="chap_dot_product.html#sec_appl_figs_computer" data-scroll="sec_appl_figs_computer" class="internal">Application: Hidden Figures in Computer Graphics</a></li>
<li><a href="chap_dot_product.html#sec_dot_prod_intro" data-scroll="sec_dot_prod_intro" class="internal">Introduction</a></li>
<li><a href="chap_dot_product.html#sec_dist_vec" data-scroll="sec_dist_vec" class="internal">The Distance Between Vectors</a></li>
<li><a href="chap_dot_product.html#sec_angle_vec" data-scroll="sec_angle_vec" class="internal">The Angle Between Two Vectors</a></li>
<li><a href="chap_dot_product.html#sec_orthog_proj" data-scroll="sec_orthog_proj" class="internal">Orthogonal Projections</a></li>
<li><a href="chap_dot_product.html#sec_orthog_comp" data-scroll="sec_orthog_comp" class="internal">Orthogonal Complements</a></li>
<li><a href="chap_dot_product.html#sec_dot_prod_exam" data-scroll="sec_dot_prod_exam" class="internal">Examples</a></li>
<li><a href="chap_dot_product.html#sec_dot_prod_summ" data-scroll="sec_dot_prod_summ" class="internal">Summary</a></li>
<li><a href="chap_dot_product.html#sec_dot_prod_exer" data-scroll="sec_dot_prod_exer" class="internal">Exercises</a></li>
<li><a href="chap_dot_product.html#sec_proj_back_face" data-scroll="sec_proj_back_face" class="internal">Project: Back-Face Culling</a></li>
</ul>
</li>
<li class="link">
<a href="chap_orthogonal_basis.html" data-scroll="chap_orthogonal_basis" class="internal"><span class="codenumber">24</span> <span class="title">Orthogonal and Orthonormal Bases in <span class="process-math">\(\R^n\)</span></span></a><ul>
<li><a href="chap_orthogonal_basis.html#sec_appl_3d_rotate" data-scroll="sec_appl_3d_rotate" class="internal">Application: Rotations in 3D</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_set_intro" data-scroll="sec_orthog_set_intro" class="internal">Introduction</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_sets" data-scroll="sec_orthog_sets" class="internal">Orthogonal Sets</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_bases_prop" data-scroll="sec_orthog_bases_prop" class="internal">Properties of Orthogonal Bases</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthon_bases" data-scroll="sec_orthon_bases" class="internal">Orthonormal Bases</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_mtx" data-scroll="sec_orthog_mtx" class="internal">Orthogonal Matrices</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_set_exam" data-scroll="sec_orthog_set_exam" class="internal">Examples</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_set_summ" data-scroll="sec_orthog_set_summ" class="internal">Summary</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_set_exer" data-scroll="sec_orthog_set_exer" class="internal">Exercises</a></li>
<li><a href="chap_orthogonal_basis.html#sec_proj_3d_rotate" data-scroll="sec_proj_3d_rotate" class="internal">Project: Understanding Rotations in 3-Space</a></li>
</ul>
</li>
<li class="link">
<a href="chap_gram_schmidt.html" data-scroll="chap_gram_schmidt" class="internal"><span class="codenumber">25</span> <span class="title">Projections onto Subspaces and the Gram-Schmidt Process in <span class="process-math">\(\R^n\)</span></span></a><ul>
<li><a href="chap_gram_schmidt.html#sec_mimo" data-scroll="sec_mimo" class="internal">Application: MIMO Systems</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_intro_noip" data-scroll="sec_gram_schmidt_intro_noip" class="internal">Introduction</a></li>
<li><a href="chap_gram_schmidt.html#sec_proj_subsp_orth" data-scroll="sec_proj_subsp_orth" class="internal">Projections onto Subspaces and Orthogonal Projections</a></li>
<li><a href="chap_gram_schmidt.html#sec_best_approx" data-scroll="sec_best_approx" class="internal">Best Approximations</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_process" data-scroll="sec_gram_schmidt_process" class="internal">The Gram-Schmidt Process</a></li>
<li><a href="chap_gram_schmidt.html#sec_qr_fact" data-scroll="sec_qr_fact" class="internal">The QR Factorization of a Matrix</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_examples" data-scroll="sec_gram_schmidt_examples" class="internal">Examples</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_summ_noips" data-scroll="sec_gram_schmidt_summ_noips" class="internal">Summary</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_exercises" data-scroll="sec_gram_schmidt_exercises" class="internal">Exercises</a></li>
<li><a href="chap_gram_schmidt.html#sec_project_mimo" data-scroll="sec_project_mimo" class="internal">Project: MIMO Systems and Householder Transformations</a></li>
</ul>
</li>
<li class="link">
<a href="chap_least_squares.html" data-scroll="chap_least_squares" class="internal"><span class="codenumber">26</span> <span class="title">Least Squares Approximations</span></a><ul>
<li><a href="chap_least_squares.html#sec_appl_fit_func" data-scroll="sec_appl_fit_func" class="internal">Application: Fitting Functions to Data</a></li>
<li><a href="chap_least_squares.html#sec_ls_intro" data-scroll="sec_ls_intro" class="internal">Introduction</a></li>
<li><a href="chap_least_squares.html#sec_ls_approx" data-scroll="sec_ls_approx" class="internal">Least Squares Approximations</a></li>
<li><a href="chap_least_squares.html#sec_ls_exam" data-scroll="sec_ls_exam" class="internal">Examples</a></li>
<li><a href="chap_least_squares.html#sec_ls_summ" data-scroll="sec_ls_summ" class="internal">Summary</a></li>
<li><a href="chap_least_squares.html#sec_ls_exer" data-scroll="sec_ls_exer" class="internal">Exercises</a></li>
<li><a href="chap_least_squares.html#sec_proj_ls_approx_other" data-scroll="sec_proj_ls_approx_other" class="internal">Project: Other Least Squares Approximations</a></li>
</ul>
</li>
<li class="link part"><a href="part-app-orthog.html" data-scroll="part-app-orthog" class="internal"><span class="codenumber">VI</span> <span class="title">Applications of Orthogonality</span></a></li>
<li class="link">
<a href="chap_orthogonal_diagonalization.html" data-scroll="chap_orthogonal_diagonalization" class="internal"><span class="codenumber">27</span> <span class="title">Orthogonal Diagonalization</span></a><ul>
<li><a href="chap_orthogonal_diagonalization.html#sec_appl_mulit_2nd_deriv" data-scroll="sec_appl_mulit_2nd_deriv" class="internal">Application: The Multivariable Second Derivative Test</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_orthog_diag_intro" data-scroll="sec_orthog_diag_intro" class="internal">Introduction</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_mtx_symm" data-scroll="sec_mtx_symm" class="internal">Symmetric Matrices</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_spec_decomp_symm_mtx" data-scroll="sec_spec_decomp_symm_mtx" class="internal">The Spectral Decomposition of a Symmetric Matrix <span class="process-math">\(A\)</span></a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_orthog_diag_exam" data-scroll="sec_orthog_diag_exam" class="internal">Examples</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_orthog_diag_summ" data-scroll="sec_orthog_diag_summ" class="internal">Summary</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_orthog_diag_exer" data-scroll="sec_orthog_diag_exer" class="internal">Exercises</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_proj_two_var_deriv" data-scroll="sec_proj_two_var_deriv" class="internal">Project: The Second Derivative Test for Functions of Two Variables</a></li>
</ul>
</li>
<li class="link">
<a href="chap_principal_axis_theorem.html" data-scroll="chap_principal_axis_theorem" class="internal"><span class="codenumber">28</span> <span class="title">Quadratic Forms and the Principal Axis Theorem</span></a><ul>
<li><a href="chap_principal_axis_theorem.html#sec_appl_tennis" data-scroll="sec_appl_tennis" class="internal">Application: The Tennis Racket Effect</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_intro" data-scroll="sec_pat_intro" class="internal">Introduction</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_eqs_quad_r2" data-scroll="sec_eqs_quad_r2" class="internal">Equations Involving Quadratic Forms in <span class="process-math">\(\R^2\)</span></a></li>
<li><a href="chap_principal_axis_theorem.html#sec_class_quad_forms" data-scroll="sec_class_quad_forms" class="internal">Classifying Quadratic Forms</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_inner_prod" data-scroll="sec_pat_inner_prod" class="internal">Inner Products</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_exam" data-scroll="sec_pat_exam" class="internal">Examples</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_summ" data-scroll="sec_pat_summ" class="internal">Summary</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_exer" data-scroll="sec_pat_exer" class="internal">Exercises</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_proj_tennis" data-scroll="sec_proj_tennis" class="internal">Project: The Tennis Racket Theorem</a></li>
</ul>
</li>
<li class="link">
<a href="chap_SVD.html" data-scroll="chap_SVD" class="internal"><span class="codenumber">29</span> <span class="title">The Singular Value Decomposition</span></a><ul>
<li><a href="chap_SVD.html#sec_appl_search_engn" data-scroll="sec_appl_search_engn" class="internal">Application: Search Engines and Semantics</a></li>
<li><a href="chap_SVD.html#sec_svd_intro" data-scroll="sec_svd_intro" class="internal">Introduction</a></li>
<li><a href="chap_SVD.html#sec_mtx_op_norm" data-scroll="sec_mtx_op_norm" class="internal">The Operator Norm of a Matrix</a></li>
<li><a href="chap_SVD.html#sec_svd" data-scroll="sec_svd" class="internal">The SVD</a></li>
<li><a href="chap_SVD.html#sec_svd_mtx_spaces" data-scroll="sec_svd_mtx_spaces" class="internal">SVD and the Null, Column, and Row Spaces of a Matrix</a></li>
<li><a href="chap_SVD.html#sec_svd_exam" data-scroll="sec_svd_exam" class="internal">Examples</a></li>
<li><a href="chap_SVD.html#sec_svd_summ" data-scroll="sec_svd_summ" class="internal">Summary</a></li>
<li><a href="chap_SVD.html#sec_svd_exer" data-scroll="sec_svd_exer" class="internal">Exercises</a></li>
<li><a href="chap_SVD.html#sec_proj_indexing" data-scroll="sec_proj_indexing" class="internal">Project: Latent Semantic Indexing</a></li>
</ul>
</li>
<li class="link">
<a href="chap_pseudoinverses.html" data-scroll="chap_pseudoinverses" class="internal"><span class="codenumber">30</span> <span class="title">Using the Singular Value Decomposition</span></a><ul>
<li><a href="chap_pseudoinverses.html#sec_appl_gps" data-scroll="sec_appl_gps" class="internal">Application: Global Positioning System</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudo_intro" data-scroll="sec_pseudo_intro" class="internal">Introduction</a></li>
<li><a href="chap_pseudoinverses.html#sec_img_conpress" data-scroll="sec_img_conpress" class="internal">Image Compression</a></li>
<li><a href="chap_pseudoinverses.html#sec_err_approx_img" data-scroll="sec_err_approx_img" class="internal">Calculating the Error in Approximating an Image</a></li>
<li><a href="chap_pseudoinverses.html#sec_mtx_cond_num" data-scroll="sec_mtx_cond_num" class="internal">The Condition Number of a Matrix</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudoinverses" data-scroll="sec_pseudoinverses" class="internal">Pseudoinverses</a></li>
<li><a href="chap_pseudoinverses.html#sec_ls_approx_SVD" data-scroll="sec_ls_approx_SVD" class="internal">Least Squares Approximations</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudo_exam" data-scroll="sec_pseudo_exam" class="internal">Examples</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudo_summ" data-scroll="sec_pseudo_summ" class="internal">Summary</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudo_exer" data-scroll="sec_pseudo_exer" class="internal">Exercises</a></li>
<li><a href="chap_pseudoinverses.html#sec_proj_gps" data-scroll="sec_proj_gps" class="internal">Project: GPS and Least Squares</a></li>
</ul>
</li>
<li class="link part"><a href="part-vec-spaces.html" data-scroll="part-vec-spaces" class="internal"><span class="codenumber">VII</span> <span class="title">Vector Spaces</span></a></li>
<li class="link">
<a href="chap_vector_spaces.html" data-scroll="chap_vector_spaces" class="internal"><span class="codenumber">31</span> <span class="title">Vector Spaces</span></a><ul>
<li><a href="chap_vector_spaces.html#sec_appl_hat_puzzle" data-scroll="sec_appl_hat_puzzle" class="internal">Application: The Hat Puzzle</a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space_intro" data-scroll="sec_vec_space_intro" class="internal">Introduction</a></li>
<li><a href="chap_vector_spaces.html#sec_space_like_rn" data-scroll="sec_space_like_rn" class="internal">Spaces with Similar Structure to <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space" data-scroll="sec_vec_space" class="internal">Vector Spaces</a></li>
<li><a href="chap_vector_spaces.html#sec_subspaces" data-scroll="sec_subspaces" class="internal">Subspaces</a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space_exam" data-scroll="sec_vec_space_exam" class="internal">Examples</a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space_summ" data-scroll="sec_vec_space_summ" class="internal">Summary</a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space_exer" data-scroll="sec_vec_space_exer" class="internal">Exercises</a></li>
<li><a href="chap_vector_spaces.html#sec_proj_hamming_hat_puzzle" data-scroll="sec_proj_hamming_hat_puzzle" class="internal">Project: Hamming Codes and the Hat Puzzle</a></li>
</ul>
</li>
<li class="link">
<a href="chap_bases.html" data-scroll="chap_bases" class="internal"><span class="codenumber">32</span> <span class="title">Bases for Vector Spaces</span></a><ul>
<li><a href="chap_bases.html#sec_img_compress" data-scroll="sec_img_compress" class="internal">Application: Image Compression</a></li>
<li><a href="chap_bases.html#sec_bases_intro" data-scroll="sec_bases_intro" class="internal">Introduction</a></li>
<li><a href="chap_bases.html#sec_lin_indep" data-scroll="sec_lin_indep" class="internal">Linear Independence</a></li>
<li><a href="chap_bases.html#sec_bases" data-scroll="sec_bases" class="internal">Bases</a></li>
<li><a href="chap_bases.html#sec_basis_vec_space" data-scroll="sec_basis_vec_space" class="internal">Finding a Basis for a Vector Space</a></li>
<li><a href="chap_bases.html#sec_bases_exam" data-scroll="sec_bases_exam" class="internal">Examples</a></li>
<li><a href="chap_bases.html#sec_bases_summ" data-scroll="sec_bases_summ" class="internal">Summary</a></li>
<li><a href="chap_bases.html#sec_bases_exer" data-scroll="sec_bases_exer" class="internal">Exercises</a></li>
<li><a href="chap_bases.html#sec_proj_img_compress" data-scroll="sec_proj_img_compress" class="internal">Project: Image Compression with Wavelets</a></li>
</ul>
</li>
<li class="link">
<a href="chap_dimension.html" data-scroll="chap_dimension" class="internal"><span class="codenumber">33</span> <span class="title">The Dimension of a Vector Space</span></a><ul>
<li><a href="chap_dimension.html#sec_appl_pca" data-scroll="sec_appl_pca" class="internal">Application: Principal Component Analysis</a></li>
<li><a href="chap_dimension.html#sec_dims_intro" data-scroll="sec_dims_intro" class="internal">Introduction</a></li>
<li><a href="chap_dimension.html#sec_finite_dim_space" data-scroll="sec_finite_dim_space" class="internal">Finite Dimensional Vector Spaces</a></li>
<li><a href="chap_dimension.html#sec_dim_subspace" data-scroll="sec_dim_subspace" class="internal">The Dimension of a Subspace</a></li>
<li><a href="chap_dimension.html#sec_cond_basis_vec_space" data-scroll="sec_cond_basis_vec_space" class="internal">Conditions for a Basis of a Vector Space</a></li>
<li><a href="chap_dimension.html#sec_dims_exam" data-scroll="sec_dims_exam" class="internal">Examples</a></li>
<li><a href="chap_dimension.html#sec_dims_summ" data-scroll="sec_dims_summ" class="internal">Summary</a></li>
<li><a href="chap_dimension.html#sec_dims_exer" data-scroll="sec_dims_exer" class="internal">Exercises</a></li>
<li><a href="chap_dimension.html#sec_proj_pca" data-scroll="sec_proj_pca" class="internal">Project: Understanding Principal Component Analysis</a></li>
</ul>
</li>
<li class="link">
<a href="chap_coordinate_vectors_vector_spaces.html" data-scroll="chap_coordinate_vectors_vector_spaces" class="internal"><span class="codenumber">34</span> <span class="title">Coordinate Vectors and Coordinate Transformations</span></a><ul>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_appl_sums" data-scroll="sec_appl_sums" class="internal">Application: Calculating Sums</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coor_vec_intro" data-scroll="sec_coor_vec_intro" class="internal">Introduction</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coord_trans" data-scroll="sec_coord_trans" class="internal">The Coordinate Transformation</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coord_vec_exam" data-scroll="sec_coord_vec_exam" class="internal">Examples</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coord_vec_summ" data-scroll="sec_coord_vec_summ" class="internal">Summary</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coord_vec_exer" data-scroll="sec_coord_vec_exer" class="internal">Exercises</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_proj_sum_powers" data-scroll="sec_proj_sum_powers" class="internal">Project: Finding Formulas for Sums of Powers</a></li>
</ul>
</li>
<li class="link">
<a href="chap_inner_products.html" data-scroll="chap_inner_products" class="internal"><span class="codenumber">35</span> <span class="title">Inner Product Spaces</span></a><ul>
<li><a href="chap_inner_products.html#sec_appl_fourier" data-scroll="sec_appl_fourier" class="internal">Application: Fourier Series</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_intro" data-scroll="sec_inner_prod_intro" class="internal">Introduction</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_spaces" data-scroll="sec_inner_prod_spaces" class="internal">Inner Product Spaces</a></li>
<li><a href="chap_inner_products.html#sec_vec_length" data-scroll="sec_vec_length" class="internal">The Length of a Vector</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_orthog" data-scroll="sec_inner_prod_orthog" class="internal">Orthogonality in Inner Product Spaces</a></li>
<li><a href="chap_inner_products.html#sec_inner_prog_orthog_bases" data-scroll="sec_inner_prog_orthog_bases" class="internal">Orthogonal and Orthonormal Bases in Inner Product Spaces</a></li>
<li><a href="chap_inner_products.html#sec_orthog_proj_subspace" data-scroll="sec_orthog_proj_subspace" class="internal">Orthogonal Projections onto Subspaces</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_approx" data-scroll="sec_inner_prod_approx" class="internal">Best Approximations in Inner Product Spaces</a></li>
<li><a href="chap_inner_products.html#sec_orthog_comp_ip" data-scroll="sec_orthog_comp_ip" class="internal">Orthogonal Complements</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_exam" data-scroll="sec_inner_prod_exam" class="internal">Examples</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_summ" data-scroll="sec_inner_prod_summ" class="internal">Summary</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_exer" data-scroll="sec_inner_prod_exer" class="internal">Exercises</a></li>
<li><a href="chap_inner_products.html#sec_proj_fourier" data-scroll="sec_proj_fourier" class="internal">Project: Fourier Series and Musical Tones</a></li>
</ul>
</li>
<li class="link">
<a href="chap_gram_schmidt_ips.html" data-scroll="chap_gram_schmidt_ips" class="internal"><span class="codenumber">36</span> <span class="title">The Gram-Schmidt Process in Inner Product Spaces</span></a><ul>
<li><a href="chap_gram_schmidt_ips.html#sec_appl_gaussian_quad" data-scroll="sec_appl_gaussian_quad" class="internal">Application: Gaussian Quadrature</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_intro" data-scroll="sec_gram_schmidt_intro" class="internal">Introduction</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_inner_prod" data-scroll="sec_gram_schmidt_inner_prod" class="internal">The Gram-Schmidt Process using Inner Products</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_exam" data-scroll="sec_gram_schmidt_exam" class="internal">Examples</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_summ_ips" data-scroll="sec_gram_schmidt_summ_ips" class="internal">Summary</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_exer" data-scroll="sec_gram_schmidt_exer" class="internal">Exercises</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_proj_gaussian_quad" data-scroll="sec_proj_gaussian_quad" class="internal">Project: Gaussian Quadrature and Legendre Polynomials</a></li>
</ul>
</li>
<li class="link part"><a href="part-lin-trans.html" data-scroll="part-lin-trans" class="internal"><span class="codenumber">VIII</span> <span class="title">Linear Transformations</span></a></li>
<li class="link">
<a href="chap_linear_transformation.html" data-scroll="chap_linear_transformation" class="internal"><span class="codenumber">37</span> <span class="title">Linear Transformations</span></a><ul>
<li><a href="chap_linear_transformation.html#sec_appl_fractals" data-scroll="sec_appl_fractals" class="internal">Application: Fractals</a></li>
<li><a href="chap_linear_transformation.html#sec_lin_trans_intro" data-scroll="sec_lin_trans_intro" class="internal">Introduction</a></li>
<li><a href="chap_linear_transformation.html#sec_onto_oneone" data-scroll="sec_onto_oneone" class="internal">Onto and One-to-One Transformations</a></li>
<li><a href="chap_linear_transformation.html#sec_kernel_range" data-scroll="sec_kernel_range" class="internal">The Kernel and Range of Linear Transformation</a></li>
<li><a href="chap_linear_transformation.html#sec_isomorph" data-scroll="sec_isomorph" class="internal">Isomorphisms</a></li>
<li><a href="chap_linear_transformation.html#sec_lin_trans_exam" data-scroll="sec_lin_trans_exam" class="internal">Examples</a></li>
<li><a href="chap_linear_transformation.html#sec_lin_trans_summ" data-scroll="sec_lin_trans_summ" class="internal">Summary</a></li>
<li><a href="chap_linear_transformation.html#sec_lin_trans_exer" data-scroll="sec_lin_trans_exer" class="internal">Exercises</a></li>
<li><a href="chap_linear_transformation.html#sec_proj_fractals" data-scroll="sec_proj_fractals" class="internal">Project: Fractals via Iterated Function Systems</a></li>
</ul>
</li>
<li class="link">
<a href="chap_transformation_matrix.html" data-scroll="chap_transformation_matrix" class="internal"><span class="codenumber">38</span> <span class="title">The Matrix of a Linear Transformation</span></a><ul>
<li><a href="chap_transformation_matrix.html#sec_appl_secret" data-scroll="sec_appl_secret" class="internal">Application: Secret Sharing Algorithms</a></li>
<li><a href="chap_transformation_matrix.html#sec_mtxof_trans_intro" data-scroll="sec_mtxof_trans_intro" class="internal">Introduction</a></li>
<li><a href="chap_transformation_matrix.html#sec_trans_rn_rm" data-scroll="sec_trans_rn_rm" class="internal">Linear Transformations from <span class="process-math">\(\R^n\)</span> to <span class="process-math">\(\R^m\)</span></a></li>
<li><a href="chap_transformation_matrix.html#sec_mtx_lin_trans" data-scroll="sec_mtx_lin_trans" class="internal">The Matrix of a Linear Transformation</a></li>
<li><a href="chap_transformation_matrix.html#sec_ker_mtx" data-scroll="sec_ker_mtx" class="internal">A Connection between <span class="process-math">\(\Ker(T)\)</span> and a Matrix Representation of <span class="process-math">\(T\)</span></a></li>
<li><a href="chap_transformation_matrix.html#sec_mtxof_trans_exam" data-scroll="sec_mtxof_trans_exam" class="internal">Examples</a></li>
<li><a href="chap_transformation_matrix.html#sec_mtxof_trans_summ" data-scroll="sec_mtxof_trans_summ" class="internal">Summary</a></li>
<li><a href="chap_transformation_matrix.html#sec_mtxof_trans_exer" data-scroll="sec_mtxof_trans_exer" class="internal">Exercises</a></li>
<li><a href="chap_transformation_matrix.html#sec_proj_secret" data-scroll="sec_proj_secret" class="internal">Project: Shamir's Secret Sharing and Lagrange Polynomials</a></li>
</ul>
</li>
<li class="link">
<a href="chap_transformations_eigenvalues.html" data-scroll="chap_transformations_eigenvalues" class="internal"><span class="codenumber">39</span> <span class="title">Eigenvalues of Linear Transformations</span></a><ul>
<li><a href="chap_transformations_eigenvalues.html#sec_appl_diff_eq" data-scroll="sec_appl_diff_eq" class="internal">Application: Linear Differential Equations</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_eigen_trans_intro" data-scroll="sec_eigen_trans_intro" class="internal">Introduction</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_find_eigen_trans" data-scroll="sec_find_eigen_trans" class="internal">Finding Eigenvalues and Eigenvectors of Linear Transformations</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_diagonal" data-scroll="sec_diagonal" class="internal">Diagonalization</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_eigen_trans_exam" data-scroll="sec_eigen_trans_exam" class="internal">Examples</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_eigen_trans_summ" data-scroll="sec_eigen_trans_summ" class="internal">Summary</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_eigen_trans_exer" data-scroll="sec_eigen_trans_exer" class="internal">Exercises</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_proj_diff_eq" data-scroll="sec_proj_diff_eq" class="internal">Project: Linear Transformations and Differential Equations</a></li>
</ul>
</li>
<li class="link">
<a href="chap_JCF.html" data-scroll="chap_JCF" class="internal"><span class="codenumber">40</span> <span class="title">The Jordan Canonical Form</span></a><ul>
<li><a href="chap_JCF.html#sec_appl_epidemic" data-scroll="sec_appl_epidemic" class="internal">Application: The Bailey Model of an Epidemic</a></li>
<li><a href="chap_JCF.html#sec_jordan_intro" data-scroll="sec_jordan_intro" class="internal">Introduction</a></li>
<li><a href="chap_JCF.html#sec_eigen_dne" data-scroll="sec_eigen_dne" class="internal">When an Eigenvalue Decomposition Does Not Exist</a></li>
<li><a href="chap_JCF.html#sec_gen_eigen_jordan" data-scroll="sec_gen_eigen_jordan" class="internal">Generalized Eigenvectors and the Jordan Canonical Form</a></li>
<li><a href="chap_JCF.html#sec_mtx_jordan_geom" data-scroll="sec_mtx_jordan_geom" class="internal">Geometry of Matrix Transformations using the Jordan Canonical Form</a></li>
<li><a href="chap_JCF.html#sec_jordan_proof" data-scroll="sec_jordan_proof" class="internal">Proof of the Existence of the Jordan Canonical Form</a></li>
<li><a href="chap_JCF.html#sec_mtx_nilpotent" data-scroll="sec_mtx_nilpotent" class="internal">Nilpotent Matrices and Invariant Subspaces</a></li>
<li><a href="chap_JCF.html#sec_jordan" data-scroll="sec_jordan" class="internal">The Jordan Canonical Form</a></li>
<li><a href="chap_JCF.html#sec_jordan_exam" data-scroll="sec_jordan_exam" class="internal">Examples</a></li>
<li><a href="chap_JCF.html#sec_jordan_summ" data-scroll="sec_jordan_summ" class="internal">Summary</a></li>
<li><a href="chap_JCF.html#sec_jordan_exer" data-scroll="sec_jordan_exer" class="internal">Exercises</a></li>
<li><a href="chap_JCF.html#sec_proj_epidemic" data-scroll="sec_proj_epidemic" class="internal">Project: Modeling an Epidemic</a></li>
</ul>
</li>
<li class="link backmatter"><a href="backmatter-1.html" data-scroll="backmatter-1" class="internal"><span class="title">Back Matter</span></a></li>
<li class="link">
<a href="app_complex_numbers.html" data-scroll="app_complex_numbers" class="internal"><span class="codenumber">A</span> <span class="title">Complex Numbers</span></a><ul>
<li><a href="app_complex_numbers.html#sec_complex_numbers" data-scroll="sec_complex_numbers" class="internal">Complex Numbers</a></li>
<li><a href="app_complex_numbers.html#sec_conj_modulus" data-scroll="sec_conj_modulus" class="internal">Conjugates and Modulus</a></li>
<li><a href="app_complex_numbers.html#sec_complex_vect" data-scroll="sec_complex_vect" class="internal">Complex Vectors</a></li>
</ul>
</li>
<li class="link"><a href="app_answers.html" data-scroll="app_answers" class="internal"><span class="codenumber">B</span> <span class="title">Answers and Hints for Selected Exercises</span></a></li>
<li class="link"><a href="index-1.html" data-scroll="index-1" class="internal"><span class="title">Index</span></a></li>
</ul></nav><div class="extras"><nav><a class="pretext-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content">
<section class="chapter" id="chap_approx_eigenvalues"><h2 class="heading">
<span class="type">Section</span> <span class="codenumber">20</span> <span class="title">Approximating Eigenvalues and Eigenvectors</span>
</h2>
<section class="introduction" id="introduction-316"><article class="objectives goal-like" id="objectives-20"><h3 class="heading"><span class="type">Focus Questions</span></h3>
<div class="introduction" id="introduction-317"><p id="p-3360">By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.</p></div>
<ul class="disc">
<li id="li-587"><p id="p-3361">What is the power method for?</p></li>
<li id="li-588"><p id="p-3362">How does the power method work?</p></li>
<li id="li-589"><p id="p-3363">How can we use the inverse power method to approximate any eigenvalue/eigenvector pair?</p></li>
</ul></article></section><section class="section" id="sec_appl_leslie_mtx"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber"></span> <span class="title">Application: Leslie Matrices and Population Modeling</span>
</h3>
<p id="p-3364">The Leslie Matrix (also called the Leslie Model) is a powerful model for describing an age distributed growth of a population that is closed to migration. In a Leslie model, it is usually the case that only one gender (most often female) is considered. As an example, we will later consider a population of sheep that is being grown commercially. A natural question that we will address is how we can harvest the population to build a sustainable environment.</p>
<p id="p-3365">When working with populations, the matrices we use are often large. For large matrices, using the characteristic polynomial to calculate eigenvalues is too time and resource consuming to be practical, and we generally cannot find the exact values of the eigenvalues. As a result, approximation techniques are very important. In this section we will explore a method for approximating eigenvalues. The eigenvalues of a Leslie matrix are important because they describe the limiting or steady-state behavior of a population. The matrix and model were introduced by Patrick H. Leslie in <span class="articletitle">‚ÄúOn the Use of Matrices in Certain Population Mathematics‚Äù</span>, Leslie, P.H., <span class="booktitle">Biometrika</span>, Volume XXXIII, November 1945, pp. 183-212.</p></section><section class="section" id="sec_app_eigen_intro"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber"></span> <span class="title">Introduction</span>
</h3>
<p id="p-3366">We have used the characteristic polynomial to find the eigenvalues of a matrix, and for each eigenvalue row reduced a corresponding matrix to find the eigenvectors This method is only practical for small matrices ‚Äî for more realistic applications approximation techniques are used. We investigate one such technique in this section, the <dfn class="terminology">power method</dfn>.</p>
<article class="exploration project-like" id="pa_4_d"><h4 class="heading">
<span class="type">Preview Activity</span><span class="space"> </span><span class="codenumber">20.1</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-318"><p id="p-3367">Let <span class="process-math">\(A = \left[ \begin{array}{cc} 2\amp 6 \\ 5\amp 3 \end{array} \right]\text{.}\)</span> Our goal is to find a scalar <span class="process-math">\(\lambda\)</span> and a nonzero vector <span class="process-math">\(\vv\)</span> so that <span class="process-math">\(A \vv = \lambda \vv\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1112"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-3368">If we have no prior knowledge of the eigenvalues and eigenvectors of this matrix, we might just begin with a guess. Let <span class="process-math">\(\vx_0 = [1 \ 0]^{\tr}\)</span> be such a guess for an eigenvector. Calculate <span class="process-math">\(A \vx_0\text{.}\)</span> Is <span class="process-math">\(\vx_0\)</span> an eigenvector of <span class="process-math">\(A\text{?}\)</span> Explain.</p></article><article class="task exercise-like" id="task-1113"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-3369">If <span class="process-math">\(\vx_0\)</span> is not a good approximation to an eigenvector of <span class="process-math">\(A\text{,}\)</span> then we need to make a better guess. We have little to work with other than just random guessing, but we can use <span class="process-math">\(\vx_1 = A\vx_0\)</span> as another guess. We calculated <span class="process-math">\(\vx_1\)</span> in part 1. Is <span class="process-math">\(\vx_1\)</span> an eigenvector for <span class="process-math">\(A\text{?}\)</span> Explain.</p></article><article class="task exercise-like" id="task-1114"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-3370">In parts (a) and (b) you might have noticed that in some sense <span class="process-math">\(\vx_1\)</span> is closer to being an eigenvector of <span class="process-math">\(A\)</span> than <span class="process-math">\(\vx_0\)</span> was. So maybe continuing this process will get us closer to an eigenvector of <span class="process-math">\(A\text{.}\)</span> In other words, for each positive integer <span class="process-math">\(k\)</span> we define <span class="process-math">\(\vx_k\)</span> as <span class="process-math">\(A \vx_{k-1}\text{.}\)</span> Before we proceed, however, we should note that as we calculate the vectors <span class="process-math">\(\vx_1\text{,}\)</span> <span class="process-math">\(\vx_2\text{,}\)</span> <span class="process-math">\(\vx_3\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> the entries in the vectors get large very quickly. So it will be useful to scale the entries so that they stay at a reasonable size, which makes it easier to interpret the output. One way to do this is to divide each vector <span class="process-math">\(\vx_i\)</span> by its largest component in absolute value so that all of the entries stay between <span class="process-math">\(-1\)</span> and <span class="process-math">\(1\text{.}\)</span><a href="" data-knowl="" class="id-ref fn-knowl original" data-refid="hk-fn-37" id="fn-37"><sup>‚Äâ37‚Äâ</sup></a> So in our example we have <span class="process-math">\(\vx_0 = [1 \ 0]^{\tr}\text{,}\)</span> <span class="process-math">\(\vx_1 = [2/5 \ 1]^{\tr}\text{,}\)</span> and <span class="process-math">\(\vx_2 = [1 \ 25/34]^{\tr}\text{.}\)</span> Explain why scaling our vectors will not affect our search for an eigenvector.</p></article><article class="task exercise-like" id="task-1115"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<p id="p-3371">Use an appropriate technological tool to find the vectors <span class="process-math">\(\vx_k\)</span> up to <span class="process-math">\(k=10\text{.}\)</span> What do you think the limiting vector <span class="process-math">\(\lim_{k \to \infty} \vx_k\)</span> is? Is this limiting vector an eigenvector of <span class="process-math">\(A\text{?}\)</span> If so, what is the corresponding eigenvalue?</p></article></article></section><section class="section" id="sec_power_method"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber"></span> <span class="title">The Power Method</span>
</h3>
<p id="p-3372">While the examples we present in this text are small in order to highlight the concepts, matrices that appear in real life applications are often enormous. For example, in Google's PageRank algorithm that is used to determine relative rankings of the importance of web pages, matrices of staggering size are used (most entries in the matrices are zero, but the size of the matrices is still huge). Finding eigenvalues of such large matrices through the characteristic polynomial is impractical. In fact, finding the roots of all but the smallest degree characteristic polynomials is a very difficult problem. As a result, using the characteristic polynomial to find eigenvalues and then finding eigenvectors is not very practical in general, and it is often a better option to use a numeric approximation method. We will consider one such method in this section, the <dfn class="terminology">power method</dfn>.</p>
<p id="p-3373">In <a href="" class="xref" data-knowl="./knowl/pa_4_d.html" title="Preview Activity 20.1">Preview Activity¬†20.1</a>, we saw an example of a matrix <span class="process-math">\(A = \left[ \begin{array}{cc} 2\amp 6 \\ 5\amp 3 \end{array} \right]\)</span> so that the sequence <span class="process-math">\(\{\vx_k\}\text{,}\)</span> where <span class="process-math">\(\vx_k = A \vx_{k-1}\text{,}\)</span> converged to a dominant eigenvector of <span class="process-math">\(A\)</span> for an initial guess vector <span class="process-math">\(\vx_0 = [1 \ 0]^{\tr}\text{.}\)</span> The vectors <span class="process-math">\(\vx_i\)</span> for <span class="process-math">\(i\)</span> from <span class="process-math">\(1\)</span> to <span class="process-math">\(6\)</span> (with scaling) are approximately</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/pa_4_d.html" id="md-140">
\begin{align*}
\vx_1 \amp = \left[ \begin{array}{c} 0.4 \\ 1
\end{array}  \right] \amp \vx_2 \amp = \left[ \begin{array}{c} 1 \\ 0.7353
\end{array}  \right] \amp \vx_3 \amp = \left[ \begin{array}{c} 0.8898 \\ 1
\end{array}  \right]\\
\vx_4 \amp = \left[ \begin{array}{c} 1 \\ 0.9575
\end{array}  \right] \amp \vx_5 \amp = \left[ \begin{array}{c} 0.9838 \\ 1
\end{array}  \right] \amp \vx_6 \amp = \left[ \begin{array}{c} 1 \\ 0.9939
\end{array}  \right]\text{.}
\end{align*}
</div>
<p id="p-3374">Numerically we can see that the sequence <span class="process-math">\(\{\vx_k\}\)</span> approaches the vector <span class="process-math">\([1 \ 1]^{\tr}\text{,}\)</span> and <a href="" class="xref" data-knowl="./knowl/F_4_e_1.html" title="Figure 20.1">Figure¬†20.1</a> illustrates this geometrically as well.</p>
<figure class="figure figure-like" id="F_4_e_1"><div class="image-box" style="width: 30%; margin-left: 35%; margin-right: 35%;"><img src="external/4_e_Power_Method_1.svg" role="img" class="contained"></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">20.1<span class="period">.</span></span><span class="space"> </span>The power method.</figcaption></figure><p id="p-3375"> This method of successive approximations <span class="process-math">\(\vx_k = A \vx_{k-1}\)</span> is called the <dfn class="terminology">power method</dfn> (since we could write <span class="process-math">\(\vx_k\)</span> as <span class="process-math">\(A^k \vx_0\)</span>). Our task now is to show that this method works in general. In the next activity we restrict our argument to the <span class="process-math">\(2 \times 2\)</span> case, and then discuss the general case afterwards.</p>
<p id="p-3376"> Let <span class="process-math">\(A\)</span> be an arbitrary <span class="process-math">\(2 \times 2\)</span> matrix with two linearly independent eigenvectors <span class="process-math">\(\vv_1\)</span> and <span class="process-math">\(\vv_2\)</span> and corresponding eigenvalues <span class="process-math">\(\lambda_1\)</span> and <span class="process-math">\(\lambda_2\text{,}\)</span> respectively. We will also assume <span class="process-math">\(|\lambda_1| &gt; |\lambda_2|\text{.}\)</span> An eigenvalue whose absolute value is larger than that of any other eigenvalue is called a <dfn class="terminology">dominant eigenvalue</dfn>. Any eigenvector for a dominant eigenvalue is called a <dfn class="terminology">dominant eigenvector</dfn>. Before we show that our method can be used to approximate a dominant eigenvector, we recall that since <span class="process-math">\(\vv_1\)</span> and <span class="process-math">\(\vv_2\)</span> are eigenvectors corresponding to distinct eigenvalues, then <span class="process-math">\(\vv_1\)</span> and <span class="process-math">\(\vv_2\)</span> are linearly independent. So there exist scalars <span class="process-math">\(a_1\)</span> and <span class="process-math">\(a_2\)</span> such that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\vx_0 = a_1 \vv_1 + a_2 \vv_2\text{.}
\end{equation*}
</div>
<p id="p-3377">We have seen that for each positive integer <span class="process-math">\(k\)</span> we can write <span class="process-math">\(\vx_n\)</span> as</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="eq_4_e_1">
\begin{equation}
\vx_k = a_1 \lambda_1^k \vv_1 + a_2 \lambda_2^k \vv_2\text{.}\tag{20.1}
\end{equation}
</div>
<p id="p-3378">With this representation of <span class="process-math">\(\vx_0\)</span> we can now see why the power method approximates a dominant eigenvector of <span class="process-math">\(A\text{.}\)</span></p>
<article class="activity project-like" id="act_4_e_1"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">20.2</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-319"><p id="p-3379">Assume as above that <span class="process-math">\(A\)</span> is an arbitrary <span class="process-math">\(2 \times 2\)</span> matrix with two linearly independent eigenvectors <span class="process-math">\(\vv_1\)</span> and <span class="process-math">\(\vv_2\)</span> and corresponding eigenvalues <span class="process-math">\(\lambda_1\)</span> and <span class="process-math">\(\lambda_2\text{,}\)</span> respectively. (We are assuming that we don't know these eigenvectors, but we can assume that they exist.) Assume that <span class="process-math">\(\lambda_1\)</span> is the dominant eigenvalue for <span class="process-math">\(A\text{,}\)</span> <span class="process-math">\(\vx_0\)</span> is some initial guess to an eigenvector for <span class="process-math">\(A\text{,}\)</span> that <span class="process-math">\(\vx_0 = a_1 \vv_1 + a_2 \vv_2\text{,}\)</span> and that <span class="process-math">\(\vx_k = A\vx_{k-1}\)</span> for <span class="process-math">\(k \geq 1\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1116"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-3380">We divide both sides of equation <a href="" class="xref" data-knowl="./knowl/eq_4_e_1.html" title="Equation 20.1">(20.1)</a> by <span class="process-math">\(\lambda_1^k\)</span> (since <span class="process-math">\(\lambda_1\)</span> is the dominant eigenvalue, we know that <span class="process-math">\(\lambda_1\)</span> is not <span class="process-math">\(0\)</span>) to obtain</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_4_e_1.html ./knowl/eq_4_e_2.html" id="eq_4_e_2">
\begin{equation}
\frac{1}{\lambda_1^k}\vx_k = a_1 \vv_1 + a_2 \left(\frac{\lambda_2}{\lambda_1}\right)^k \vv_2\text{.}\tag{20.2}
\end{equation}
</div>
<p class="continuation">Recall that <span class="process-math">\(\lambda_1\)</span> is the dominant eigenvalue for <span class="process-math">\(A\text{.}\)</span> What happens to <span class="process-math">\(\left( \frac{\lambda_2}{\lambda_1} \right)^k\)</span> as <span class="process-math">\(k \to \infty\text{?}\)</span> Explain what happens to the right hand side of equation <a href="" class="xref" data-knowl="./knowl/eq_4_e_2.html" title="Equation 20.2">(20.2)</a> as <span class="process-math">\(k \to \infty\text{.}\)</span></p></article><article class="task exercise-like" id="task-1117"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-3381">Explain why the previous result tells us that the vectors <span class="process-math">\(\vx_k\)</span> are approaching a vector in the <em class="emphasis">direction</em> of <span class="process-math">\(\vv_1\)</span> or <span class="process-math">\(-\vv_1\)</span> as <span class="process-math">\(k \to \infty\text{,}\)</span> assuming <span class="process-math">\(a_1 \neq 0\text{.}\)</span> (Why do we need <span class="process-math">\(a_1 \neq 0\text{?}\)</span> What happens if <span class="process-math">\(a_1=0\text{?}\)</span>)</p></article><article class="task exercise-like" id="task-1118"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-3382">What does all of this tell us about the sequence <span class="process-math">\(\{\vx_k\}\)</span> as <span class="process-math">\(k \to \infty\text{?}\)</span></p></article></article><p id="p-3383">The power method is straightforward to implement, but it is not without its drawbacks. We began by assuming that we had a basis of eigenvectors of a matrix <span class="process-math">\(A\text{.}\)</span> So we are also assuming that <span class="process-math">\(A\)</span> is diagonalizable. We also assumed that <span class="process-math">\(A\)</span> had a dominant eigenvalue <span class="process-math">\(\lambda_1\text{.}\)</span> That is, if <span class="process-math">\(A\)</span> is <span class="process-math">\(n \times n\)</span> we assume that <span class="process-math">\(A\)</span> has eigenvalues <span class="process-math">\(\lambda_1\text{,}\)</span> <span class="process-math">\(\lambda_2\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\(\lambda_n\text{,}\)</span> not necessarily distinct, with</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
|\lambda_1| &gt; |\lambda_2| \geq |\lambda_3| \geq \cdots \geq |\lambda_n|
\end{equation*}
</div>
<p class="continuation">and with <span class="process-math">\(\vv_i\)</span> an eigenvector of <span class="process-math">\(A\)</span> with eigenvalue <span class="process-math">\(\lambda_i\text{.}\)</span> We could then write any initial guess <span class="process-math">\(\vx_0\)</span> in the form</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\vx_0 = a_1 \vv_1 +  a_2\vv_2 + \cdots + a_n \vv_n\text{.}
\end{equation*}
</div>
<p class="continuation">The initial guess is also called a <dfn class="terminology">seed</dfn>.</p>
<p id="p-3384">Then</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\vx_k = a_1 \lambda_1^k \vv_1 + a_2 \lambda_2^k \vv_2 + \cdots + a_n \lambda_n^k \vv_n
\end{equation*}
</div>
<p class="continuation">and</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="eq_4_e_3">
\begin{equation}
\frac{1}{\lambda_1^k} \vx_k = a_1 \vv_1 + a_2 \left(\frac{\lambda_2}{\lambda_1}\right)^k \vv_2 + \cdots + a_n \left(\frac{\lambda_n}{\lambda_1}\right)^k \vv_n\text{.}\tag{20.3}
\end{equation}
</div>
<p id="p-3385">Notice that we are not actually calculating the vectors <span class="process-math">\(\vx_k\)</span> here ‚Äî this is a theoretical argument and we don't know <span class="process-math">\(\lambda_1\)</span> and are not performing any scaling like we did in <a href="" class="xref" data-knowl="./knowl/pa_4_d.html" title="Preview Activity 20.1">Preview Activity¬†20.1</a>. We are assuming that <span class="process-math">\(\lambda_1\)</span> is the dominant eigenvalue of <span class="process-math">\(A\text{,}\)</span> though, so for each <span class="process-math">\(i\)</span> the terms <span class="process-math">\(\left(\frac{\lambda_i}{\lambda_1}\right)^k\)</span> converge to <span class="process-math">\(0\)</span> as <span class="process-math">\(k\)</span> goes to infinity. Thus,</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/pa_4_d.html">
\begin{equation*}
\vx_k \approx \lambda_1^k a_1 \vv_1
\end{equation*}
</div>
<p class="continuation">for large values of <span class="process-math">\(k\text{,}\)</span> which makes the sequence <span class="process-math">\(\{\vx_k\}\)</span> converge to a vector in the direction of a dominant eigenvector <span class="process-math">\(\vv_1\)</span> provided <span class="process-math">\(a_1 \neq 0\text{.}\)</span> So we need to be careful enough to choose a seed that has a nonzero component in the direction of <span class="process-math">\(\vv_1\text{.}\)</span> Of course, we generally don't know that our matrix is diagonalizable before we make these calculations, but for many matrices the sequence <span class="process-math">\(\{\vx_k\}\)</span> will approach a dominant eigenvector.</p>
<p id="p-3386">The power method approximates a dominant eigenvector, and there are ways that we can approximate the dominant eigenvalue. <a href="" class="xref" data-knowl="./knowl/ex_4_d_dominant_eigenvalue.html" title="Exercise 8">Exercise¬†8</a> presents one way ‚Äî by keeping track of the components of the <span class="process-math">\(\vx_k\)</span> that have the largest absolute values, and the next activity shows another.</p>
<article class="activity project-like" id="act_4_d_Rayleigh"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">20.3</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-320"><p id="p-3387">Let <span class="process-math">\(A\)</span> be an <span class="process-math">\(n \times n\)</span> matrix with eigenvalue <span class="process-math">\(\lambda\)</span> and corresponding eigenvector <span class="process-math">\(\vv\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1119"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-3388">Explain why <span class="process-math">\(\lambda = \frac{\lambda (\vv \cdot \vv)}{\vv \cdot \vv}\text{.}\)</span></p></article><article class="task exercise-like" id="task-1120"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-3389">Use the result of part (a) to explain why <span class="process-math">\(\lambda = \frac{(A\vv) \cdot \vv}{\vv \cdot \vv}\text{.}\)</span></p></article></article><p id="p-3390"> The result of <a href="" class="xref" data-knowl="./knowl/act_4_d_Rayleigh.html" title="Activity 20.3">Activity¬†20.3</a> is that, when the vectors in the sequence <span class="process-math">\(\{\vx_k\}\)</span> approximate a dominant eigenvector of a matrix <span class="process-math">\(A\text{,}\)</span> the quotients</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/act_4_d_Rayleigh.html ./knowl/eq_Rayleigh.html" id="eq_Rayleigh">
\begin{equation}
\frac{(A\vx_k) \cdot \vx_k}{\vx_k \cdot \vx_k} = \frac{\vx_k^{\tr}A\vx_k}{\vx_k^{\tr}\vx_k}\tag{20.4}
\end{equation}
</div>
<p class="continuation">approximate the dominant eigenvalue of <span class="process-math">\(A\text{.}\)</span> The quotients in <a href="" class="xref" data-knowl="./knowl/eq_Rayleigh.html" title="Equation 20.4">(20.4)</a> are called <dfn class="terminology">Rayleigh quotients</dfn>.</p>
<p id="p-3391">To summarize, the procedure for applying the power method for approximating a dominant eigenvector and dominant eigenvalue of a matrix <span class="process-math">\(A\)</span> is as follows.</p>
<dl class="description-list">
<dt id="li-590">Step 1</dt>
<dd><p id="p-3392">Select an arbitrary nonzero vector <span class="process-math">\(\vx_0\)</span> as an initial guess to a dominant eigenvector.</p></dd>
<dt id="li-591">Step 2</dt>
<dd><p id="p-3393">Let <span class="process-math">\(\vx_1 = A\vx_0\text{.}\)</span> Let <span class="process-math">\(k = 1\text{.}\)</span></p></dd>
<dt id="li-592">Step 3</dt>
<dd><p id="p-3394">To avoid having the magnitudes of successive approximations become excessively large, scale this approximation <span class="process-math">\(\vx_k\text{.}\)</span> That is, find the entry <span class="process-math">\(\alpha_k\)</span> of <span class="process-math">\(\vx_k\)</span> that is largest in absolute value. Then replace <span class="process-math">\(\vx_k\)</span> by <span class="process-math">\(\frac{1}{\alpha_k} \vx_k\text{.}\)</span></p></dd>
<dt id="li-593">Step 4</dt>
<dd><p id="p-3395">Calculate the Rayleigh quotient <span class="process-math">\(r_k = \frac{(A\vx_{k}) \cdot \vx_k}{\vx_k \cdot \vx_k}\text{.}\)</span></p></dd>
<dt id="li-594">Step 5</dt>
<dd><p id="p-3396">Let let <span class="process-math">\(\vx_{k+1} = A \vx_k\text{.}\)</span> Increase <span class="process-math">\(k\)</span> by <span class="process-math">\(1\)</span> and repeat Steps 3 through 5.</p></dd>
</dl>
<p id="p-3397">If the sequence <span class="process-math">\(\{\vx_k\}\)</span> converges to a dominant eigenvector of <span class="process-math">\(A\text{,}\)</span> then the sequence <span class="process-math">\(\{r_k\}\)</span> converges to the dominant eigenvalue of <span class="process-math">\(A\text{.}\)</span></p>
<p id="p-3398"> The power method can be useful for approximating a dominant eigenvector as long as the successive multiplications by <span class="process-math">\(A\)</span> are fairly simple ‚Äî for example, if many entries of <span class="process-math">\(A\)</span> are zero.<a href="" data-knowl="" class="id-ref fn-knowl original" data-refid="hk-fn-38" id="fn-38"><sup>‚Äâ38‚Äâ</sup></a> The rate of convergence of the sequence <span class="process-math">\(\{\vx_k\}\)</span> depends on the ratio <span class="process-math">\(\frac{\lambda_2}{\lambda_1}\text{.}\)</span> If this ratio is close to <span class="process-math">\(1\text{,}\)</span> then it can take many iterations before the power <span class="process-math">\(\left(\frac{\lambda_2}{\lambda_1}\right)^k\)</span> makes the <span class="process-math">\(\vv_2\)</span> term negligible. There are other methods for approximating eigenvalues and eigenvectors, e.g., the QR factorization, that we will not discuss at this point.</p></section><section class="section" id="sec_power_method_inv"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber"></span> <span class="title">The Inverse Power Method</span>
</h3>
<p id="p-3399">The power method only allows us to approximate the dominant eigenvalue and a dominant eigenvector for a matrix <span class="process-math">\(A\text{.}\)</span> It is possible to modify this method to approximate other eigenvectors and eigenvalues under certain conditions. We consider an example in the next activity to motivate the general situation.</p>
<article class="activity project-like" id="act_4_e_2"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">20.4</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-321"><p id="p-3400">Let <span class="process-math">\(A = \left[ \begin{array}{cc} 2\amp 6 \\ 5\amp 3 \end{array} \right]\)</span> be the matrix from <a href="" class="xref" data-knowl="./knowl/pa_4_d.html" title="Preview Activity 20.1">Preview Activity¬†20.1</a>. Recall that <span class="process-math">\(8\)</span> is an eigenvalue for <span class="process-math">\(A\text{,}\)</span> and a quick calculation can show that <span class="process-math">\(-3\)</span> is the other eigenvalue of <span class="process-math">\(A\text{.}\)</span> Consider the matrix <span class="process-math">\(B = (A - (-2)I_2)^{-1} = \frac{1}{10}\left[ \begin{array}{rr} -5\amp 6 \\ 5\amp -4 \end{array} \right]\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1121"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-3401">Show that <span class="process-math">\(\frac{1}{8-(-2)}\)</span> and <span class="process-math">\(\frac{1}{-3-(-2)}\)</span> are the eigenvalues of <span class="process-math">\(B\text{.}\)</span></p></article><article class="task exercise-like" id="task-1122"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-3402">Recall that <span class="process-math">\(\vv_1 = [1 \ 1]^{\tr}\)</span> is an eigenvector of <span class="process-math">\(A\)</span> corresponding to the eigenvalue <span class="process-math">\(8\)</span> and assume that <span class="process-math">\(\vv_2 = [-6 \ 5]^{\tr}\)</span> is an eigenvector for <span class="process-math">\(A\)</span> corresponding to the eigenvalue <span class="process-math">\(-3\text{.}\)</span> Calculate the products <span class="process-math">\(B \vv_1\)</span> and <span class="process-math">\(B \vv_2\text{.}\)</span> How do the products relate to the results of part (a)?</p></article></article><p id="p-3403"><a href="" class="xref" data-knowl="./knowl/act_4_e_2.html" title="Activity 20.4">Activity¬†20.4</a> provides evidence that we can translate the matrix <span class="process-math">\(A\)</span> having a dominant eigenvalue to a different matrix <span class="process-math">\(B\)</span> with the same eigenvectors as <span class="process-math">\(A\)</span> and with a dominant eigenvalue of our choosing. To see why, let <span class="process-math">\(A\)</span> be an <span class="process-math">\(n \times n\)</span> matrix with eigenvalues <span class="process-math">\(\lambda_1\text{,}\)</span> <span class="process-math">\(\lambda_2\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\(\lambda_n\text{,}\)</span> and let <span class="process-math">\(\alpha\)</span> be any real number distinct from the eigenvalues. Let <span class="process-math">\(B = (A - \alpha I_n)^{-1}\text{.}\)</span> In our example in <a href="" class="xref" data-knowl="./knowl/act_4_e_2.html" title="Activity 20.4">Activity¬†20.4</a> the numbers</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/act_4_e_2.html ./knowl/act_4_e_2.html">
\begin{equation*}
\frac{1}{\lambda_1 - \alpha}, \  \frac{1}{\lambda_2 - \alpha}, \ \frac{1}{\lambda_3 - \alpha}, \ \ldots, \frac{1}{\lambda_n - \alpha}
\end{equation*}
</div>
<p class="continuation">were the eigenvalues of <span class="process-math">\(B\text{,}\)</span> and that if <span class="process-math">\(\vv_i\)</span> is an eigenvector for <span class="process-math">\(A\)</span> corresponding to the eigenvalue <span class="process-math">\(\lambda_i\text{,}\)</span> then <span class="process-math">\(\vv_i\)</span> is an eigenvector of <span class="process-math">\(B\)</span> corresponding to the eigenvalue <span class="process-math">\(\frac{1}{\lambda_i - \alpha}\text{.}\)</span> To see why, let <span class="process-math">\(\lambda\)</span> be an eigenvalue of an <span class="process-math">\(n \times n\)</span> matrix <span class="process-math">\(A\)</span> with corresponding eigenvector <span class="process-math">\(\vv\text{.}\)</span> Let <span class="process-math">\(\alpha\)</span> be a scalar that is not an eigenvalue of <span class="process-math">\(A\text{,}\)</span> and let <span class="process-math">\(B = (A - \alpha I_n)^{-1}\text{.}\)</span> Now</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/act_4_e_2.html ./knowl/act_4_e_2.html" id="md-141">
\begin{align*}
A \vv \amp = \lambda \vv\\
A \vv - \alpha \vv \amp = \lambda \vv - \alpha \vv\\
(A-\alpha I_n) \vv \amp = (\lambda - \alpha) \vv\\
\frac{1}{\lambda - \alpha} \vv \amp = (A-\alpha I_n)^{-1} \vv\text{.}
\end{align*}
</div>
<p id="p-3404">So <span class="process-math">\(\frac{1}{\lambda - \alpha}\)</span> is an eigenvalue of <span class="process-math">\(B\)</span> with eigenvector <span class="process-math">\(\vv\text{.}\)</span></p>
<p id="p-3405">Now suppose that <span class="process-math">\(A\)</span> is an <span class="process-math">\(n \times n\)</span> matrix with eigenvalues <span class="process-math">\(\lambda_1\text{,}\)</span> <span class="process-math">\(\lambda_2\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\(\lambda_n\text{,}\)</span> and that we want to approximate an eigenvector and corresponding eigenvalue <span class="process-math">\(\lambda_i\)</span> of <span class="process-math">\(A\text{.}\)</span> If we can somehow find a value of <span class="process-math">\(\alpha\)</span> so that <span class="process-math">\(|\lambda_i - \alpha| \lt |\lambda_j - \alpha|\)</span> for all <span class="process-math">\(j \neq i\text{,}\)</span> then <span class="process-math">\(\left| \frac{1}{\lambda_i - \alpha} \right| &gt; \left| \frac{1}{\lambda_j - \alpha} \right|\)</span> for any <span class="process-math">\(j \neq i\text{.}\)</span> Thus, the matrix <span class="process-math">\(B = (A - \alpha I_n)^{-1}\)</span> has <span class="process-math">\(\frac{1}{\lambda_i - \alpha}\)</span> as its dominant eigenvalue and we can use the power method to approximate an eigenvector and the Rayleigh quotient to approximate the eigenvalue <span class="process-math">\(\frac{1}{\lambda_i - \alpha}\text{,}\)</span> and hence approximate <span class="process-math">\(\lambda_i\text{.}\)</span></p>
<article class="activity project-like" id="act_4_e_3"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">20.5</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-322"><p id="p-3406">Let <span class="process-math">\(A = \frac{1}{8}\left[ \begin{array}{crr} 7\amp 3\amp 3 \\ 30\amp 22\amp -10 \\ 15\amp -21\amp 11 \end{array} \right]\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1123"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-3407">Apply the power method to the matrix <span class="process-math">\(B = (A - I_3)^{-1}\)</span> with initial vector <span class="process-math">\(\vx_0 = [1 \ 0 \ 0]^{\tr}\)</span> to fill in <a href="" class="xref" data-knowl="./knowl/T_4_e_2.html" title="Table 20.2: Applying the power method to (A - I_3)^{-1}">Table¬†20.2</a> (to four decimal places). Use this information to estimate an eigenvalue for <span class="process-math">\(A\)</span> and a corresponding eigenvector. <figure class="table table-like" id="T_4_e_2"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">20.2<span class="period">.</span></span><span class="space"> </span>Applying the power method to <span class="process-math">\((A - I_3)^{-1}\)</span></figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(k\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(10\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(15\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(20\)</span></td>
</tr>
<tr>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(\vx_k\)</span></td>
<td class="c m b1 r0 l0 t0 lines"></td>
<td class="c m b1 r0 l0 t0 lines"></td>
<td class="c m b1 r0 l0 t0 lines"></td>
</tr>
<tr>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(\frac{\vx_k^{\tr}A\vx_k}{\vx_k^{\tr}\vx_k}\)</span></td>
<td class="c m b1 r0 l0 t0 lines"></td>
<td class="c m b1 r0 l0 t0 lines"></td>
<td class="c m b1 r0 l0 t0 lines"></td>
</tr>
</table></div></figure></p></article><article class="task exercise-like" id="task-1124"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-3408">Applying the power method to the matrix <span class="process-math">\(B = (A - 0I_3)^{-1}\)</span> with initial vector <span class="process-math">\(\vx_0 = [1 \ 0 \ 0]^{\tr}\)</span> yields the information in <a href="" class="xref" data-knowl="./knowl/T_4_e_3.html" title="Table 20.3: Applying the power method to (A - 0I_3)^{-1}">Table¬†20.3</a> (to four decimal places). Use this information to estimate an eigenvalue for <span class="process-math">\(A\)</span> and a corresponding eigenvector. <figure class="table table-like" id="T_4_e_3"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">20.3<span class="period">.</span></span><span class="space"> </span>Applying the power method to <span class="process-math">\((A - 0I_3)^{-1}\)</span></figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="c m b1 r0 l0 t1 lines"><span class="process-math">\(k\)</span></td>
<td class="c m b1 r0 l0 t1 lines"><span class="process-math">\(10\)</span></td>
<td class="c m b1 r0 l0 t1 lines"><span class="process-math">\(15\)</span></td>
<td class="c m b1 r0 l0 t1 lines"><span class="process-math">\(20\)</span></td>
</tr>
<tr>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(\vx_k\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(\left[ \begin{array}{r} -0.3344 \\ 0.6677 \\ 1.0000 \end{array} \right]\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(\left[\begin{array}{r} -0.3333 \\ 0.6666 \\ 1.0000 \end{array} \right]\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(\left[ \begin{array}{r} -0.3333 \\ 0.6666 \\ 1.0000 \end{array} \right]\)</span></td>
</tr>
<tr>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(\frac{\vx_k^{\tr}A\vx_k}{\vx_k^{\tr}\vx_k}\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(-1.0014\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(-1.0000\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(-1.0000\)</span></td>
</tr>
</table></div></figure></p></article><article class="task exercise-like" id="task-1125"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-3409">Applying the power method to the matrix <span class="process-math">\(B = (A - 5I_3)^{-1}\)</span> with initial vector <span class="process-math">\(\vx_0 = [1 \ 0 \ 0]^{\tr}\)</span> yields the information in <a href="" class="xref" data-knowl="./knowl/T_4_e_4.html" title="Table 20.4: Applying the power method to (A - 5I_3)^{-1}">Table¬†20.4</a> (to four decimal places). Use this information to estimate an eigenvalue for <span class="process-math">\(A\)</span> and a corresponding eigenvector. <figure class="table table-like" id="T_4_e_4"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">20.4<span class="period">.</span></span><span class="space"> </span>Applying the power method to <span class="process-math">\((A - 5I_3)^{-1}\)</span></figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="c m b1 r0 l0 t1 lines"><span class="process-math">\(k\)</span></td>
<td class="c m b1 r0 l0 t1 lines"><span class="process-math">\(10\)</span></td>
<td class="c m b1 r0 l0 t1 lines"><span class="process-math">\(15\)</span></td>
<td class="c m b1 r0 l0 t1 lines"><span class="process-math">\(20\)</span></td>
</tr>
<tr>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(\vx_k\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(\left[ \begin{array}{r} 0.0000 \\ 1.0000 \\ -1.0000 \end{array} \right]\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(\left[ \begin{array}{r} 0.0000 \\ 1.0000 \\ -1.0000 \end{array} \right]\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(\left[ \begin{array}{r} 0.0000 \\ 1.0000 \\ -1.0000 \end{array} \right]\)</span></td>
</tr>
<tr>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(\frac{\vx_k^{\tr}A\vx_k}{\vx_k^{\tr}\vx_k}\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(-1.0000\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(-1.0000\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(-1.0000\)</span></td>
</tr>
</table></div></figure></p></article></article></section><section class="section" id="sec_app_eigen_exam"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber"></span> <span class="title">Examples</span>
</h3>
<p id="p-3410">What follows are worked examples that use the concepts from this section.</p>
<article class="example example-like" id="example-40"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">20.5</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-323"><p id="p-3411">Let <span class="process-math">\(A = \left[ \begin{array}{ccc} 1\amp 2\amp 3\\4\amp 5\amp 6\\7\amp 8\amp 9 \end{array} \right]\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1126"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-3412">Approximate the dominant eigenvalue of <span class="process-math">\(A\)</span> accurate to two decimal places using the power method. Use technology as appropriate.</p>
<div class="solution solution-like" id="solution-122">
<h5 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h5>
<p id="p-3413">We use technology to calculate the scaled vectors <span class="process-math">\(A^k \vx_0\)</span> for values of <span class="process-math">\(k\)</span> until the components don't change in the second decimal place. We start with the seed <span class="process-math">\(\vx_0 = [1 \ 1 \ 1]^{\tr}\text{.}\)</span> For example, to two decimal places we have <span class="process-math">\(\vx_k = [0.28 \ 0.64 \ 1.00]^{\tr}\)</span> for <span class="process-math">\(k  \geq 20\text{.}\)</span> So we suspect that <span class="process-math">\(\left[ 0.28 \ 0.64 \ 1.00 \right]^{\tr}\)</span> is close to a dominant eigenvector for <span class="process-math">\(A\text{.}\)</span> For the dominant eigenvalue, we can calculate the Rayleigh quotients <span class="process-math">\(\frac{(A\vx_k) \cdot \vx_k}{\vx_k \cdot \vx_k}\)</span> until they do not change to two decimal places. For <span class="process-math">\(k \geq 4\text{,}\)</span> our Rayleigh quotients are all (to two decimal places) equal to <span class="process-math">\(16.12\text{.}\)</span> So we expect that the dominant eigenvalue of <span class="process-math">\(A\)</span> is close to <span class="process-math">\(16.12\text{.}\)</span> Notice that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A [0.28 \ 0.64 \ 1.00]^{\tr} =  [4.56 \ 10.32 \ 16.08]^{\tr}\text{,}
\end{equation*}
</div>
<p class="continuation">which is not far off from <span class="process-math">\(16.12 [0.28 \ 0.64 \ 1.00]^{\tr}\text{.}\)</span></p>
</div></article><article class="task exercise-like" id="task-1127"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-3414">Find the characteristic polynomial <span class="process-math">\(p(\lambda)\)</span> of <span class="process-math">\(A\text{.}\)</span> Then find the the root of <span class="process-math">\(p(\lambda)\)</span> farthest from the origin. Compare to the result of part (a). Use technology as appropriate.</p>
<div class="solution solution-like" id="solution-123">
<h5 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h5>
<p id="p-3415">The characteristic polynomial of <span class="process-math">\(A\)</span> is</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
p(\lambda) =-\lambda^3 + 15 \lambda^2 + 18 \lambda = -\lambda(\lambda^2-15\lambda-18)\text{.}
\end{equation*}
</div>
<p class="continuation">The quadratic formula gives the nonzero roots of <span class="process-math">\(p(\lambda)\)</span> as</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\frac{15 \pm \sqrt{15^2 + 4(18)}}{2} = \frac{15 \pm 3\sqrt{33}}{2}\text{.}
\end{equation*}
</div>
<p class="continuation">The roots farthest from the origin is approximately <span class="process-math">\(16.12\text{,}\)</span> as was also calculated in part (a).</p>
</div></article></article><article class="example example-like" id="example-41"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">20.6</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-324"><p id="p-3416">Let <span class="process-math">\(A = \left[ \begin{array}{ccc} 2\amp 1\amp 0 \\ 1\amp 3\amp 1 \\ 0\amp 1\amp 2 \end{array} \right]\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1128"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-3417">Use the power method to approximate the dominant eigenvalue and a corresponding eigenvector (using scaling) accurate to two decimal places. Use <span class="process-math">\(\vx_0 = [1 \ 1 \ 1]^{\tr}\)</span> as the seed.</p>
<div class="solution solution-like" id="solution-124">
<h5 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h5>
<p id="p-3418">We use technology to calculate the scaled vectors <span class="process-math">\(A^k \vx_0\)</span> for values of <span class="process-math">\(k\)</span> until the components don't change in the second decimal place. For example, to two decimal places we have <span class="process-math">\(\vx_k = [0.50 \ 1.00 \ 0.50]^{\tr}\)</span> for <span class="process-math">\(k \geq 4\text{.}\)</span> So we suspect that <span class="process-math">\(\left[ \frac{1}{2} \ 1 \ \frac{1}{2} \right]^{\tr}\)</span> is a dominant eigenvector for <span class="process-math">\(A\text{.}\)</span> For the dominant eigenvalue, we can calculate the Rayleigh quotients <span class="process-math">\(\frac{(A\vx_k) \cdot \vx_k}{\vx_k \cdot \vx_k}\)</span> until they do not change to two decimal places. For <span class="process-math">\(k \geq 2\text{,}\)</span> our Rayleigh quotients are all (to two decimal places) equal to <span class="process-math">\(4\text{.}\)</span> So we expect that the dominant eigenvalue of <span class="process-math">\(A\)</span> is <span class="process-math">\(4\text{.}\)</span> We could also use the fact that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A \left[ \frac{1}{2} \ 1 \ \frac{1}{2} \right]^{\tr} = [2 \ 4 \ 2]^{\tr} = 4\left[ \frac{1}{2} \ 1 \ \frac{1}{2} \right]^{\tr}
\end{equation*}
</div>
<p class="continuation">to see that <span class="process-math">\(\left[ \frac{1}{2} \ 1 \ \frac{1}{2} \right]^{\tr}\)</span> is a dominant eigenvector for <span class="process-math">\(A\)</span> with eigenvalue <span class="process-math">\(4\text{.}\)</span></p>
</div></article><article class="task exercise-like" id="task-1129"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-3419">Determine the exact value of the dominant eigenvalue of <span class="process-math">\(A\)</span> and compare to your result from part (a).</p>
<div class="solution solution-like" id="solution-125">
<h5 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h5>
<p id="p-3420">Technology shows that the characteristic polynomial of <span class="process-math">\(A - \lambda I_3\)</span> is</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
p(\lambda) = -\lambda^3 + 7\lambda^2 - 14 \lambda + 8 = -(\lambda-1)(\lambda-2)(\lambda-4)\text{.}
\end{equation*}
</div>
<p class="continuation">We can see from the characteristic polynomial that <span class="process-math">\(4\)</span> is the dominant eigenvalue of <span class="process-math">\(A\text{.}\)</span></p>
</div></article><article class="task exercise-like" id="task-1130"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-3421">Approximate the remaining eigenvalues of <span class="process-math">\(A\)</span> using the inverse power method.</p>
<div class="hint solution-like" id="hint-26">
<h5 class="heading">
<span class="type">Hint</span><span class="period">.</span>
</h5>
<p id="p-3422">Try <span class="process-math">\(\alpha = 0.5\)</span> and <span class="process-math">\(\alpha = 1.8\text{.}\)</span></p>
</div>
<div class="solution solution-like" id="solution-126">
<h5 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h5>
<p id="p-3423">Applying the power method to <span class="process-math">\(B = (A-0.5I_3)^{-1}\)</span> with seed <span class="process-math">\(\vx_0 = [1 \ 1 \ 1]^{\tr}\)</span> gives <span class="process-math">\(\vx_k \approx [ 0.50 \ 1.00 \ 0.50]^{\tr}\)</span> for <span class="process-math">\(k \geq 5\text{,}\)</span> with Rayleigh quotients of <span class="process-math">\(2\)</span> (to several decimal places). So <span class="process-math">\(2\)</span> is the dominant eigenvalue of <span class="process-math">\(B\text{.}\)</span> But <span class="process-math">\(\frac{1}{\lambda-0.5}\)</span> is also the dominant eigenvalue of <span class="process-math">\(B\text{,}\)</span> where <span class="process-math">\(\lambda\)</span> is the corresponding eigenvalue of <span class="process-math">\(A\text{.}\)</span> . So to find <span class="process-math">\(\lambda\text{,}\)</span> we note that <span class="process-math">\(\frac{1}{\lambda-0.5} = 2\)</span> implies that <span class="process-math">\(\lambda = 1\)</span> is an eigenvalue of <span class="process-math">\(A\text{.}\)</span> Now applying the power method to <span class="process-math">\(B = (A-1.8I_3)^{-1}\)</span> with seed <span class="process-math">\(\vx_0 = [1 \ 1 \ 1]^{\tr}\)</span> gives <span class="process-math">\(\vx_k \approx [ 1.00 \ -1.00 \ 1.00]^{\tr}\)</span> for large enough <span class="process-math">\(k\text{,}\)</span> with Rayleigh quotients of <span class="process-math">\(5\)</span> (to several decimal places). To find the corresponding eigenvalue <span class="process-math">\(\lambda\)</span> for <span class="process-math">\(A\text{,}\)</span> we note that <span class="process-math">\(\frac{1}{\lambda-1.8} = 5\text{,}\)</span> or <span class="process-math">\(\lambda = 2\)</span> is an eigenvalue of <span class="process-math">\(A\text{.}\)</span> Admittedly, this method is very limited. Finding good choices for <span class="process-math">\(\alpha\)</span> often depends on having some information about the eigenvalues of <span class="process-math">\(A\text{.}\)</span> Choosing <span class="process-math">\(\alpha\)</span> close to an eigenvalue provides the best chance of obtaining that eigenvalue.</p>
</div></article></article></section><section class="section" id="sec_app_eigen_summ"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber"></span> <span class="title">Summary</span>
</h3>
<ul class="disc">
<li id="li-595"><p id="p-3424">The power method is an iterative method that can be used to approximate the dominant eigenvalue of an <span class="process-math">\(n \times n\)</span> matrix <span class="process-math">\(A\)</span> that has <span class="process-math">\(n\)</span> linearly independent eigenvectors and a dominant eigenvalue.</p></li>
<li id="li-596"><p id="p-3425">To use the power method we start with a seed <span class="process-math">\(\vx_0\)</span> and then calculate the sequence <span class="process-math">\(\{\vx_k\}\)</span> of vectors, where <span class="process-math">\(\vx_k = A \vx_{k-1}\text{.}\)</span> If <span class="process-math">\(\vx_0\)</span> is chosen well, then the sequence <span class="process-math">\(\{\vx_k\}\)</span> converges to a dominant eigenvector of <span class="process-math">\(A\text{.}\)</span></p></li>
<li id="li-597"><p id="p-3426">If <span class="process-math">\(A\)</span> is an <span class="process-math">\(n \times n\)</span> matrix with eigenvalues <span class="process-math">\(\lambda_1\text{,}\)</span> <span class="process-math">\(\lambda_2\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\(\lambda_n\text{,}\)</span> to approximate an eigenvector of <span class="process-math">\(A\)</span> corresponding to the eigenvalue <span class="process-math">\(\lambda_i\text{,}\)</span> we apply the power method to the matrix <span class="process-math">\(B = (A - \alpha I_n)^{-1}\text{,}\)</span> where <span class="process-math">\(\alpha\)</span> is not a eigenvalue of <span class="process-math">\(A\)</span> and <span class="process-math">\(\left| \frac{1}{\lambda_i - \alpha} \right| &gt; \left| \frac{1}{\lambda_j - \alpha} \right|\)</span> for any <span class="process-math">\(j \neq i\text{.}\)</span></p></li>
</ul></section><section class="exercises" id="sec_app_eigen_exer"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber"></span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-190"><h4 class="heading"><span class="codenumber">1<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-325"><p id="p-3427">Let <span class="process-math">\(A = \left[ \begin{array}{cc} 1\amp 2\\2\amp 1 \end{array} \right]\text{.}\)</span> Let <span class="process-math">\(\vx_0 = [1 \ 0]^{\tr}\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1131"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-3428">Find the eigenvalues and corresponding eigenvectors for <span class="process-math">\(A\text{.}\)</span></p></article><article class="task exercise-like" id="task-1132"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-3430">Use appropriate technology to calculate <span class="process-math">\(\vx_k = A^k \vx_0\)</span> for <span class="process-math">\(k\)</span> up to 10. Compare to a dominant eigenvector for <span class="process-math">\(A\text{.}\)</span></p></article><article class="task exercise-like" id="task-1133"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-3432">Use the eigenvectors from part (b) to approximate the dominant eigenvalue for <span class="process-math">\(A\text{.}\)</span> Compare to the exact value of the dominant eigenvalue of <span class="process-math">\(A\text{.}\)</span></p></article><article class="task exercise-like" id="task-1134"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<p id="p-3434">Assume that the other eigenvalue for <span class="process-math">\(A\)</span> is close to <span class="process-math">\(0\text{.}\)</span> Apply the inverse power method and compare the results to the remaining eigenvalue and eigenvectors for <span class="process-math">\(A\text{.}\)</span></p></article></article><article class="exercise exercise-like" id="exercise-191"><h4 class="heading"><span class="codenumber">2<span class="period">.</span></span></h4>
<p id="p-3436">Let <span class="process-math">\(A = \left[ \begin{array}{rcc} 1\amp 2\amp 0\\-2\amp 1\amp 2 \\ 1\amp 3\amp 1 \end{array} \right]\text{.}\)</span> Use the power method to approximate a dominant eigenvector for <span class="process-math">\(A\text{.}\)</span> Use <span class="process-math">\(\vx_0 = [1 \ 1 \ 1]^{\tr}\)</span> as the seed. Then approximate the dominant eigenvalue of <span class="process-math">\(A\text{.}\)</span></p></article><article class="exercise exercise-like" id="exercise-192"><h4 class="heading"><span class="codenumber">3<span class="period">.</span></span></h4>
<p id="p-3437">Let <span class="process-math">\(A = \left[ \begin{array}{rr} 3\amp -1\\-1\amp 3 \end{array} \right]\text{.}\)</span> Use the power method starting with <span class="process-math">\(\vx_0 = [1 \ 1]^{\tr}\text{.}\)</span> Explain why the method fails in this case to approximate a dominant eigenvector, and how you could adjust the seed to make the process work.</p></article><article class="exercise exercise-like" id="exercise-193"><h4 class="heading"><span class="codenumber">4<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-326"><p id="p-3439">Let <span class="process-math">\(A = \left[ \begin{array}{cc} 0\amp 1\\1\amp 0 \end{array} \right]\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1135"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-3440">Find the eigenvalues and an eigenvector for each eigenvalue.</p></article><article class="task exercise-like" id="task-1136"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-3441">Apply the power method with an initial starting vector <span class="process-math">\(\vx_0 = [0 \ 1]^{\tr}\text{.}\)</span> What is the resulting sequence?</p></article><article class="task exercise-like" id="task-1137"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-3442">Use equation <a href="" class="xref" data-knowl="./knowl/eq_4_e_3.html" title="Equation 20.3">(20.3)</a> to explain the sequence you found in part (b).</p></article></article><article class="exercise exercise-like" id="exercise-194"><h4 class="heading"><span class="codenumber">5<span class="period">.</span></span></h4>
<p id="p-3443">Let <span class="process-math">\(A = \left[ \begin{array}{cc} 2\amp 6 \\ 5\amp 3 \end{array} \right]\text{.}\)</span> Fill in the entries in <a href="" class="xref" data-knowl="./knowl/T_4_e_1.html" title="Table 20.7: Values of the Rayleigh quotient">Table¬†20.7</a>, where <span class="process-math">\(\vx_k\)</span> is the <span class="process-math">\(k\)</span>th approximation to a dominant eigenvector using the power method, starting with the seed <span class="process-math">\(\vx_0 = [1 \ 0]^{\tr}\text{.}\)</span> Compare the results of this table to the eigenvalues of <span class="process-math">\(A\)</span> and <span class="process-math">\(\lim_{k \to \infty} \frac{\vx_{k+1}\cdot \vx_k}{\vx_k \cdot \vx_k}\text{.}\)</span> What do you notice? <figure class="table table-like" id="T_4_e_1"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">20.7<span class="period">.</span></span><span class="space"> </span>Values of the Rayleigh quotient</figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(\vv\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(\vx_0\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(\vx_1\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(\vx_2\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(\vx_3\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(\vx_4\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(\vx_5\)</span></td>
</tr>
<tr>
<td class="l m b3 r0 l0 t0 lines"><span class="process-math">\(\frac{\vv^{\tr}A\vv}{\vv^{\tr}\vv}\)</span></td>
<td class="l m b3 r0 l0 t0 lines">¬†</td>
<td class="l m b3 r0 l0 t0 lines">¬†</td>
<td class="l m b3 r0 l0 t0 lines">¬†</td>
<td class="l m b3 r0 l0 t0 lines">¬†</td>
<td class="l m b3 r0 l0 t0 lines">¬†</td>
<td class="l m b3 r0 l0 t0 lines">¬†</td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(\vv\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(\vx_6\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(\vx_7\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(\vx_8\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(\vx_9\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(\vx_{10}\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(\vx_{11}\)</span></td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(\frac{\vv^{\tr}A\vv}{\vv^{\tr}\vv}\)</span></td>
<td class="l m b0 r0 l0 t0 lines">¬†</td>
<td class="l m b0 r0 l0 t0 lines">¬†</td>
<td class="l m b0 r0 l0 t0 lines">¬†</td>
<td class="l m b0 r0 l0 t0 lines">¬†</td>
<td class="l m b0 r0 l0 t0 lines">¬†</td>
<td class="l m b0 r0 l0 t0 lines">¬†</td>
</tr>
</table></div></figure></p></article><article class="exercise exercise-like" id="exercise-195"><h4 class="heading"><span class="codenumber">6<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-327"><p id="p-3445">Let <span class="process-math">\(A = \left[ \begin{array}{cr} 4\amp -5 \\ 2\amp 15 \end{array} \right]\text{.}\)</span> The power method will approximate the dominant eigenvalue <span class="process-math">\(\lambda = 14\text{.}\)</span> In this exercise we explore what happens if we apply the power method to <span class="process-math">\(A^{-1}\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1138"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-3446">Apply the power method to <span class="process-math">\(A^{-1}\)</span> to approximate the dominant eigenvalue of <span class="process-math">\(A^{-1}\text{.}\)</span> Use <span class="process-math">\([1 \ 1]^{\tr}\)</span> as the seed. How is this eigenvalue related to an eigenvalue of <span class="process-math">\(A\text{?}\)</span></p></article><article class="task exercise-like" id="task-1139"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-3447">Explain in general why applying the power method to the inverse of an invertible matrix <span class="process-math">\(B\)</span> might give an approximation to an eigenvalue of <span class="process-math">\(B\)</span> of smallest magnitude. When might this not work?</p></article></article><article class="exercise exercise-like" id="exercise-196"><h4 class="heading"><span class="codenumber">7<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-328"><p id="p-3448">There are other algebraic methods that do not rely on the determinant of a matrix that can be used to find eigenvalues of a matrix. We examine one such method in this exercise. Let <span class="process-math">\(A\)</span> be any <span class="process-math">\(n \times n\)</span> matrix, and let <span class="process-math">\(\vv\)</span> be any vector in <span class="process-math">\(\R^n\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1140"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-3449">Explain why the vectors</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\vv, \ A\vv, \ A^2\vv, \ \ldots, \ A^n\vv
\end{equation*}
</div>
<p class="continuation">are linearly dependent.</p></article><article class="task exercise-like" id="task-1141"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-3451">Let <span class="process-math">\(c_0\text{,}\)</span> <span class="process-math">\(c_1\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\(c_n\)</span> be scalars, not all 0, so that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
c_0 \vv + c_1 A\vv + c_2 A^2 \vv + \cdots + c_n A^n \vv = \vzero\text{.}
\end{equation*}
</div>
<p class="continuation">Explain why there must be a smallest positive integer <span class="process-math">\(k\)</span> so that there are scalars <span class="process-math">\(a_0\text{,}\)</span> <span class="process-math">\(a_1\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\(a_k\)</span> with <span class="process-math">\(a_k \neq 0\text{.}\)</span> such that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
a_0 \vv + a_1 A\vv + a_2 A^2 \vv + \cdots + a_k A^k \vv = \vzero\text{.}
\end{equation*}
</div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-27" id="hint-27"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-27"><div class="hint solution-like"><p id="p-3452">Proceed down the list <span class="process-math">\(c_{n-1}\text{,}\)</span> <span class="process-math">\(c_{n-2}\text{,}\)</span> etc., until you reach a weight that is non-zero.</p></div></div>
</div></article><article class="task exercise-like" id="task-1142"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-3453">Let</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
q(t) = a_0 + a_1t + a_2t^2 + \cdots + a_kt^k\text{.}
\end{equation*}
</div>
<p class="continuation">Then</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
q(A) = a_0 + a_1A + a_2A^2 + \cdots + a_kA^k
\end{equation*}
</div>
<p class="continuation">and</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-142">
\begin{align*}
q(A)\vv \amp = (a_0 + a_1A + a_2A^2 + \cdots + a_kA^k)\vv\\
\amp = a_0 \vv + a_1 A\vv + a_2 A^2 \vv + \cdots + a_k A^k \vv\\
\amp = \vzero\text{.}
\end{align*}
</div>
<p class="continuation">Suppose the polynomial <span class="process-math">\(q(t)\)</span> has a linear factor, say <span class="process-math">\(q(t) = (t-\lambda)Q(t)\)</span> for some degree <span class="process-math">\(k-1\)</span> polynomial <span class="process-math">\(Q(t)\text{.}\)</span> Explain why, if <span class="process-math">\(Q(A) \vv\)</span> is non-zero, <span class="process-math">\(\lambda\)</span> is an eigenvalue of <span class="process-math">\(A\)</span> with eigenvector <span class="process-math">\(Q(A) \vv\text{.}\)</span></p></article><article class="task exercise-like" id="task-1143"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<div class="introduction" id="introduction-329"><p id="p-3455">This method allows us to find certain eigenvalues and eigenvectors, the roots of the polynomial <span class="process-math">\(q(t)\text{.}\)</span> Any other eigenvector must lie outside the eigenspaces we have already found, so repeating the process with a vector <span class="process-math">\(\vv\)</span> not in any of the known eigenspaces will produce different eigenvalues and eigenvectors. Let <span class="process-math">\(A = \left[ \begin{array}{ccr} 2\amp 2\amp -1 \\ 2\amp 2\amp 2 \\ 0\amp 0\amp 6 \end{array} \right]\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1144"><h6 class="heading"><span class="codenumber">(i)</span></h6>
<p id="p-3456">Find the polynomial <span class="process-math">\(q(t)\text{.}\)</span> Use <span class="process-math">\(\vv = [1 \ 1 \ 1]^{\tr}\text{.}\)</span></p></article><article class="task exercise-like" id="task-1145"><h6 class="heading"><span class="codenumber">(ii)</span></h6>
<p id="p-3458">Find all of the roots of <span class="process-math">\(q(t)\text{.}\)</span></p></article><article class="task exercise-like" id="task-1146"><h6 class="heading"><span class="codenumber">(iii)</span></h6>
<p id="p-3460">For each root <span class="process-math">\(\lambda\)</span> of <span class="process-math">\(q(t)\text{,}\)</span> find the polynomial <span class="process-math">\(Q(t)\)</span> and use this polynomial to determine an eigenvector of <span class="process-math">\(A\text{.}\)</span> Verify your work.</p></article></article></article><article class="exercise exercise-like" id="ex_4_d_dominant_eigenvalue"><h4 class="heading"><span class="codenumber">8<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-330"><p id="p-3462">We have seen that the Rayleigh quotients approximate the dominant eigenvalue of a matrix <span class="process-math">\(A\text{.}\)</span> As an alternative to using Rayleigh quotients, we can keep track of the scaling factors. Recall that the scaling in the power method can be used to make the magnitudes of the successive approximations smaller and easier to work with. Let <span class="process-math">\(A\)</span> be an <span class="process-math">\(n \times n\)</span> matrix and begin with a non-zero seed <span class="process-math">\(\vv_0\text{.}\)</span> We now want to keep track of the scaling factors, so let <span class="process-math">\(\alpha_0\)</span> be the component of <span class="process-math">\(\vv_0\)</span> with largest absolute value and let <span class="process-math">\(\vx_0 = \frac{1}{\alpha_0}\vv_0\text{.}\)</span> For <span class="process-math">\(k \geq 0\text{,}\)</span> let <span class="process-math">\(\vv_k = A\vx_{k-1}\text{,}\)</span> let <span class="process-math">\(\alpha_k\)</span> be the component of <span class="process-math">\(\vv_k\)</span> with largest absolute value and let <span class="process-math">\(\vx_k = \frac{1}{\alpha_k}\vv_k\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1147"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-3463">Let <span class="process-math">\(A = \left[ \begin{array}{rc} 0\amp 1 \\ -8\amp 6 \end{array} \right]\text{.}\)</span> Use <span class="process-math">\(\vx_0 = [1 \ 1]^{\tr}\)</span> as the seed and calculate <span class="process-math">\(\alpha_k\)</span> for <span class="process-math">\(k\)</span> from <span class="process-math">\(1\)</span> to <span class="process-math">\(10\text{.}\)</span> Compare to the dominant eigenvalue of <span class="process-math">\(A\text{.}\)</span></p></article><article class="task exercise-like" id="task-1148"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-3464">Assume that for large <span class="process-math">\(k\)</span> the vectors <span class="process-math">\(\vx_k\)</span> approach a dominant eigenvector with dominant eigenvalue <span class="process-math">\(\lambda\text{.}\)</span> Show now in general that the sequence of scaling factors <span class="process-math">\(\alpha_k\)</span> approaches <span class="process-math">\(\lambda\text{.}\)</span></p></article></article><article class="exercise exercise-like" id="exercise-198"><h4 class="heading"><span class="codenumber">9<span class="period">.</span></span></h4>
<p id="p-3465">Let <span class="process-math">\(A\)</span> be an <span class="process-math">\(n \times n\)</span> matrix and let <span class="process-math">\(\alpha\)</span> be a scalar that is not an eigenvalue of <span class="process-math">\(A\text{.}\)</span> Suppose that <span class="process-math">\(\vx\)</span> is an eigenvector of <span class="process-math">\(B = (A-\alpha I_n)^{-1}\)</span> with eigenvalue <span class="process-math">\(\beta\text{.}\)</span> Find an eigenvalue of <span class="process-math">\(A\)</span> in terms of <span class="process-math">\(\beta\)</span> and <span class="process-math">\(\alpha\)</span> with corresponding eigenvector <span class="process-math">\(\vx\text{.}\)</span></p></article><article class="exercise exercise-like" id="exercise-199"><h4 class="heading"><span class="codenumber">10<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-331"><p id="p-3467">Label each of the following statements as True or False. Provide justification for your response.</p></div>
<article class="task exercise-like" id="task-1149"><h5 class="heading">
<span class="codenumber">(a)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-3468">The largest eigenvalue of a matrix is a dominant eigenvalue.</p></article><article class="task exercise-like" id="task-1150"><h5 class="heading">
<span class="codenumber">(b)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-3470">If an <span class="process-math">\(n \times n\)</span> matrix <span class="process-math">\(A\)</span> has <span class="process-math">\(n\)</span> linearly independent eigenvectors and a dominant eigenvalue, then the sequence <span class="process-math">\(\{A^k \vx_0\}\)</span> converges to a dominant eigenvector of <span class="process-math">\(A\)</span> for any initial vector <span class="process-math">\(\vx_0\text{.}\)</span></p></article><article class="task exercise-like" id="task-1151"><h5 class="heading">
<span class="codenumber">(c)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-3471">If <span class="process-math">\(\lambda\)</span> is an eigenvalue of an <span class="process-math">\(n \times n\)</span> matrix <span class="process-math">\(A\)</span> and <span class="process-math">\(\alpha\)</span> is not an eigenvalue of <span class="process-math">\(A\text{,}\)</span> then <span class="process-math">\(\lambda - \alpha\)</span> is an eigenvalue of <span class="process-math">\(A - \alpha I_n\text{.}\)</span></p></article><article class="task exercise-like" id="task-1152"><h5 class="heading">
<span class="codenumber">(d)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-3473">Every square matrix has a dominant eigenvalue.</p></article></article></section><section class="section" id="sec_proj_sheep_herd"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber"></span> <span class="title">Project: Managing a Sheep Herd</span>
</h3>
<p id="p-3474">Sheep farming is a significant industry in New Zealand. New Zealand is reported to have the highest density of sheep in the world. Sheep can begin to reproduce after one year, and give birth only once per year. <a href="" class="xref" data-knowl="./knowl/T_Sheep.html" title="Table 20.8: New Zealand female sheep data by age group">Table¬†20.8</a> gives Birth and Survival Rates for Female New Zealand Sheep (from G. Caughley, <span class="articletitle">‚ÄúParameters for Seasonally Breeding Populations,‚Äù</span> <span class="booktitle">Ecology</span>, <em class="emphasis">48</em>, (1967), 834-839). Since sheep hardly ever live past 12 years, we will only consider the population through 12 years.</p>
<figure class="table table-like" id="T_Sheep"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">20.8<span class="period">.</span></span><span class="space"> </span>New Zealand female sheep data by age group</figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="c m b1 r1 l1 t1 lines">Age (years)</td>
<td class="c m b1 r1 l0 t1 lines">Birth Rate</td>
<td class="c m b1 r1 l0 t1 lines">Survival Rate</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">0-1</td>
<td class="c m b1 r1 l0 t0 lines">0.000</td>
<td class="c m b1 r1 l0 t0 lines">0.845</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">1-2</td>
<td class="c m b1 r1 l0 t0 lines">0.045</td>
<td class="c m b1 r1 l0 t0 lines">0.975</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">2-3</td>
<td class="c m b1 r1 l0 t0 lines">0.391</td>
<td class="c m b1 r1 l0 t0 lines">0.965</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">3-4</td>
<td class="c m b1 r1 l0 t0 lines">0.472</td>
<td class="c m b1 r1 l0 t0 lines">0.950</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">4-5</td>
<td class="c m b1 r1 l0 t0 lines">0.484</td>
<td class="c m b1 r1 l0 t0 lines">0.926</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">5-6</td>
<td class="c m b1 r1 l0 t0 lines">0.546</td>
<td class="c m b1 r1 l0 t0 lines">0.895</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">6-7</td>
<td class="c m b1 r1 l0 t0 lines">0.543</td>
<td class="c m b1 r1 l0 t0 lines">0.850</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">7-8</td>
<td class="c m b1 r1 l0 t0 lines">0.502</td>
<td class="c m b1 r1 l0 t0 lines">0.786</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">8-9</td>
<td class="c m b1 r1 l0 t0 lines">0.468</td>
<td class="c m b1 r1 l0 t0 lines">0.691</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">9-10</td>
<td class="c m b1 r1 l0 t0 lines">0.459</td>
<td class="c m b1 r1 l0 t0 lines">0.561</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">10-11</td>
<td class="c m b1 r1 l0 t0 lines">0.433</td>
<td class="c m b1 r1 l0 t0 lines">0.370</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">11-12</td>
<td class="c m b1 r1 l0 t0 lines">0.421</td>
<td class="c m b1 r1 l0 t0 lines">0.000</td>
</tr>
</table></div></figure><p id="p-3475">As sheep reproduce, they add to the 0-1 sheep (lamb) population. The potential to produce offspring is called <dfn class="terminology">fecundity</dfn> (derived from the word <dfn class="terminology">fecund</dfn> which generally refers to reproductive ability) and determines how many lamb are added to the population. Let <span class="process-math">\(F_k\)</span> (the fecundity rate) be the rate at which females in age class <span class="process-math">\(k\)</span> give birth to female offspring. Not all members of a given age group survive to the next age groups, so let <span class="process-math">\(s_k\)</span> be the fraction of individuals that survives from age group <span class="process-math">\(k\)</span> to age group <span class="process-math">\(k+1\text{.}\)</span> With these ideas in mind, we can create a life cycle chart as in <a href="" class="xref" data-knowl="./knowl/F_Life_cycle.html" title="Figure 20.9">Figure¬†20.9</a> that illustrates how the population of sheep changes on a farm (for the sake of space, we illustrate with four age classes).</p>
<figure class="figure figure-like" id="F_Life_cycle"><div class="image-box" style="width: 50%; margin-left: 25%; margin-right: 25%;"><img src="external/4_d_Cycles.svg" role="img" class="contained"></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">20.9<span class="period">.</span></span><span class="space"> </span>Life cycle with four age classes.</figcaption></figure><p id="p-3476">To model the sheep population, we need a few variables. Let <span class="process-math">\(n_1^{(0)}\)</span> be the number of sheep in age group 0-1, <span class="process-math">\(n_2^{(0)}\)</span> the number in age group 1-2, <span class="process-math">\(n_3\)</span> the number in age group 2-3 and, in general, <span class="process-math">\(n_k^{(0)}\)</span> the number of sheep in age group <span class="process-math">\((k-1)\)</span>-<span class="process-math">\(k\)</span> at some initial time (time <span class="process-math">\(0\)</span>), and let</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\vx_0 = \left[ n_1^{(0)} \ n_2^{(0)} \ n_3^{(0)} \ \cdots \ n_{12}^{(0)} \right]^{\tr}\text{.}
\end{equation*}
</div>
<p id="p-3477">We wish to determine the populations in the different groups after one year. Let</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\vx_1 = \left[ n_1^{(1)} \ n_2^{(1)} \ n_3^{(1)}  \ \cdots \ n_{12}^{(1)} \right]^{\tr}\text{,}
\end{equation*}
</div>
<p class="continuation">where <span class="process-math">\(n_1^{(1)}\)</span> denotes the number of sheep in age group 0-1, <span class="process-math">\(n_2^{(1)}\)</span> the number of sheep in age group 1-2 and, in general, <span class="process-math">\(n_{k}^{(1)}\)</span> the number of tilapia in age group <span class="process-math">\((k-1)\)</span>-<span class="process-math">\(k\)</span> after one year.</p>
<article class="project project-like" id="Leslie_1"><h4 class="heading">
<span class="type">Project Activity</span><span class="space"> </span><span class="codenumber">20.6</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-332"><p id="p-3478"><a href="" class="xref" data-knowl="./knowl/T_Sheep.html" title="Table 20.8: New Zealand female sheep data by age group">Table¬†20.8</a> shows that, on average, each female in age group 1-2 produces <span class="process-math">\(0.045\)</span> female offspring in a year. Since there are <span class="process-math">\(n_2\)</span> females in age group 1-2, the lamb population increases by <span class="process-math">\(0.045n_2\)</span> in a year.</p></div>
<article class="task exercise-like" id="task-1153"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-3479">Continue this analysis to explain why</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-143">
\begin{align*}
n_1^{(1)} \amp = 0.045n_2+0.391 n_3+0.472n_4+0.484 n_5+0.546n_6+0.543n_7\\
\amp \qquad+0.502 n_8+0.468 n_9+0.459n_{10}+0.433 n_{11}+0.421 n_{12}\text{.}
\end{align*}
</div></article><article class="task exercise-like" id="task-1154"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-3480">Explain why <span class="process-math">\(n_2^{(1)} = 0.845n_1\text{.}\)</span></p></article><article class="task exercise-like" id="task-1155"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-3481">Now explain why</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="eq_Leslie_1">
\begin{equation}
\vx_1 = L\vx_0\text{,}\tag{20.5}
\end{equation}
</div>
<p class="continuation">where <span class="process-math">\(L\)</span> is the matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="eq_Leslie_2">
\begin{equation}
{\scriptsize  \left[  \begin{array}{cccccccccccc} 0       \amp  0.045 \amp  0.391 \amp  0.472 \amp  0.484 \amp  0.546 \amp  0.543 \amp  0.502 \amp  0.468 \amp  0.459 \amp  0.433 \amp  0.421 \\ 0.845   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0.975 \amp  0    \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0.965  \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0.950  \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0.926 \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0.895  \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0    \amp  0.850  \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0    \amp  0    \amp  0.786  \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0    \amp  0    \amp  0    \amp  0.691  \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0    \amp  0    \amp  0    \amp  0    \amp  0.561  \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0    \amp  0    \amp  0    \amp  0    \amp  0    \amp  0.370  \amp  0 \end{array}  \right]}\text{.}\tag{20.6}
\end{equation}
</div></article></article><p id="p-3482"> Notice that our matrix <span class="process-math">\(L\)</span> has the form</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\left[ \begin{array}{cccccc} F_1\amp F_2\amp F_3\amp \cdots\amp F_{n-1}\amp F_n \\ s_1\amp 0\amp 0\amp \cdots\amp 0\amp 0 \\ 0\amp s_2\amp 0\amp \cdots\amp 0\amp 0 \\  0\amp 0\amp s_3\amp \cdots\amp 0\amp 0 \\ \amp \amp \amp \ddots\amp \amp  \\ 0\amp 0\amp 0\amp \cdots\amp s_{n-1}\amp 0 \end{array}  \right]\text{.}
\end{equation*}
</div>
<p class="continuation">Such a matrix is called a <dfn class="terminology">Leslie matrix</dfn>.</p>
<p id="p-3483">Leslie matrices have certain useful properties, and one eigenvalue of a Leslie matrix can tell us a lot about the long-term behavior of the situation being modeled. You can take these properties as fact unless otherwise directed.</p>
<ol class="decimal">
<li id="li-598"><p id="p-3484">A Leslie matrix <span class="process-math">\(L\)</span> has a unique positive eigenvalue <span class="process-math">\(\lambda_1\)</span> with a corresponding eigenvector <span class="process-math">\(\vv_1\)</span> whose entries are all positive.</p></li>
<li id="li-599"><p id="p-3485">If <span class="process-math">\(\lambda_i\)</span> (<span class="process-math">\(i &gt; 1\)</span>) is any other eigenvalue (real or complex) of <span class="process-math">\(L\text{,}\)</span> then <span class="process-math">\(|\lambda_i| \leq \lambda_1\text{.}\)</span> If <span class="process-math">\(\lambda_1\)</span> is the largest magnitude eigenvalue of a matrix <span class="process-math">\(L\text{,}\)</span> we call <span class="process-math">\(\lambda_1\)</span> a <dfn class="terminology">dominant eigenvalue</dfn> of <span class="process-math">\(L\text{.}\)</span></p></li>
<li id="li-600"><p id="p-3486">If any two successive entries in the first row of <span class="process-math">\(L\)</span> are both positive, then <span class="process-math">\(|\lambda_i| \lt \lambda_1\)</span> for every <span class="process-math">\(i &gt; 1\text{.}\)</span> In this case we say that <span class="process-math">\(\lambda_1\)</span> is a <dfn class="terminology">strictly dominant eigenvalue</dfn> of <span class="process-math">\(L\text{.}\)</span> In a Leslie model, this happens when the females in two successive age classes are fertile, which is almost always the case.</p></li>
<li id="li-601"><p id="p-3487">If <span class="process-math">\(\lambda_1\)</span> is a strictly dominant eigenvalue, then <span class="process-math">\(\vx_k\)</span> is approximately a scalar multiple of <span class="process-math">\(\vv_1\)</span> for large values of <span class="process-math">\(k\text{,}\)</span> regardless of the initial state <span class="process-math">\(\vx_0\text{.}\)</span> In other words, large state vectors are close to eigenvectors for <span class="process-math">\(\lambda_1\text{.}\)</span></p></li>
</ol>
<p id="p-3488">We can use these properties to determine the long-term behavior of the sheep herd.</p>
<article class="project project-like" id="act_Leslie_2"><h4 class="heading">
<span class="type">Project Activity</span><span class="space"> </span><span class="codenumber">20.7</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-333">
<p id="p-3489">Assume that <span class="process-math">\(L\)</span> is defined by <a href="" class="xref" data-knowl="./knowl/eq_Leslie_2.html" title="Equation 20.6">(20.6)</a>, and let</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_Leslie_2.html">
\begin{equation*}
\vx_m = \left[ n_1^{(m)} \ n_2^{(m)} \ n_3^{(m)} \ \cdots \ n_{12}^{(m)} \right]^{\tr}\text{,}
\end{equation*}
</div>
<p class="continuation">where <span class="process-math">\(n_1^{(m)}\)</span> denotes the number of sheep in age group 0-1, <span class="process-math">\(n_2^{(m)}\)</span> the number of sheep in age group 1-2 and, in general, <span class="process-math">\(n_k^{(m)}\)</span> the number of sheep in age group <span class="process-math">\((k-1)\)</span>-<span class="process-math">\(k\)</span> after <span class="process-math">\(k\)</span> years.</p>
</div>
<article class="task exercise-like" id="task-1156"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-3490">Assume that <span class="process-math">\(\vx_0 = \left[100 \ 100 \ 100 \ \cdots \ 100 \right]^{\tr}\text{.}\)</span> Use appropriate technology to calculate <span class="process-math">\(\vx_{22}\text{,}\)</span> <span class="process-math">\(\vx_{23}\text{,}\)</span> <span class="process-math">\(\vx_{24}\text{,}\)</span> and <span class="process-math">\(\vx_{25}\text{.}\)</span> Round to the nearest whole number. What do you notice about the sheep population? You may use the GeoGebra applet at <a class="external" href="https://www.geogebra.org/m/yqss88xq" target="_blank"><code class="code-inline tex2jax_ignore">geogebra.org/m/yqss88xq</code></a>.</p>
<p id="p-3491">We can use the third and fourth properties of Leslie matrices to better understand the long-term behavior of the sheep population. Since successive entries in the first row of the Leslie matrix in <a href="" class="xref" data-knowl="./knowl/eq_Leslie_2.html" title="Equation 20.6">(20.6)</a> are positive, our Leslie matrix has a strictly dominant eigenvalue <span class="process-math">\(\lambda_1\text{.}\)</span> Given the dimensions of our Leslie matrix, finding this dominant eigenvalue through algebraic means is not feasible. Use the power method to approximate the dominant eigenvalue <span class="process-math">\(\lambda_1\)</span> of the Leslie matrix in <a href="" class="xref" data-knowl="./knowl/eq_Leslie_2.html" title="Equation 20.6">(20.6)</a> to five decimal places. Explain your process. Then explain how this dominant eigenvalue tells us that, unchecked, the sheep population grows at a rate that is roughly exponential. What is the growth rate of this exponential growth? You may use the GeoGebra applet at <a class="external" href="https://www.geogebra.org/m/yqss88xq" target="_blank"><code class="code-inline tex2jax_ignore">geogebra.org/m/yqss88xq</code></a>.</p></article></article><p id="p-3492"><a href="" class="xref" data-knowl="./knowl/act_Leslie_2.html" title="Project Activity 20.7">Project Activity¬†20.7</a> indicates that, unchecked, the sheep population will grow without bound, roughly exponentially with ratio equal to the dominant eigenvalue of our Leslie matrix <span class="process-math">\(L\text{.}\)</span> Of course, a sheep farmer cannot provide the physical environment or the resources to support an unlimited population of sheep. In addition, most sheep farmers cannot support themselves only by shearing sheep for the wool. Consequently, some harvesting of the sheep population each year for meat and skin is necessary. A sustainable harvesting policy allows for the regular harvesting of some sheep while maintaining the population at a stable level. It is necessary for the farmer to find an optimal harvesting rate to attain this stable population and the following activity leads us through an analysis of how such a harvesting rate can be determined.</p>
<article class="project project-like" id="act_Leslie_harvest"><h4 class="heading">
<span class="type">Project Activity</span><span class="space"> </span><span class="codenumber">20.8</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-334"><p id="p-3493">The Leslie model can be modified to consider harvesting. It is possible to harvest different age groups at different rates, and to harvest only some age groups and not others. In the case of sheep, it might make sense to only harvest from the youngest population since lamb is more desirable than mutton and the lamb population grows the fastest. Assume that this is our harvesting strategy and that we harvest our sheep from only the youngest age group at the start of each year. Let <span class="process-math">\(h\)</span> be the fraction of sheep we harvest from the youngest age group each year after considering growth.</p></div>
<article class="task exercise-like" id="task-1157"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-3494">If we begin with an initial population <span class="process-math">\(\vx_0\text{,}\)</span> then the state vector after births and expected deaths is <span class="process-math">\(L\vx_0\text{.}\)</span> Now we harvest. Explain why if we harvest a fraction <span class="process-math">\(h\)</span> from the youngest age group after considering growth, then the state vector after 1 year will be</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\vx_1 = L\vx_0 - HL\vx_0\text{,}
\end{equation*}
</div>
<p class="continuation">where</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
H =  \left[ \begin{array}{c c c c c c c c c c c c} h      \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0     \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0    \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0    \amp  0    \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0    \amp  0    \amp  0    \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0    \amp  0    \amp  0    \amp  0    \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0    \amp  0    \amp  0    \amp  0    \amp  0    \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0    \amp  0    \amp  0    \amp  0    \amp  0    \amp  0    \amp  0 \end{array}  \right]\text{.}
\end{equation*}
</div></article><article class="task exercise-like" id="task-1158"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-3495">Our goal is to find a harvesting rate that will lead to a steady state in which the sheep population remains the same each year. In other words, we want to find a value of <span class="process-math">\(h\text{,}\)</span> if one exists, that satisfies</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_Harvest.html" id="eq_Harvest">
\begin{equation}
\vx = L\vx - HL\vx\text{.}\tag{20.7}
\end{equation}
</div>
<p class="continuation">Show that <a href="" class="xref" data-knowl="./knowl/eq_Harvest.html" title="Equation 20.7">(20.7)</a> is equivalent to the matrix equation</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_Harvest.html" id="eq_Harvest2">
\begin{equation}
\vx = (I_{12}-H)L\vx\text{.}\tag{20.8}
\end{equation}
</div></article><article class="task exercise-like" id="task-1159"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-3496">Use appropriate technology to experiment numerically with different values of <span class="process-math">\(h\)</span> to find the value you think gives the best uniform harvest rate. Explain your reasoning. You may use the GeoGebra applet at <a class="external" href="https://www.geogebra.org/m/yqss88xq" target="_blank"><code class="code-inline tex2jax_ignore">geogebra.org/m/yqss88xq</code></a>.</p></article><article class="task exercise-like" id="task-1160"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<div class="introduction" id="introduction-335">
<p id="p-3497">Now we will use some algebra to find an equation that explicitly gives us the harvest rate in the general setting. This will take a bit of work, but none of it is too difficult. To simplify our work but yet illustrate the overall idea, let us consider the general <span class="process-math">\(4 \times 4\)</span> case with arbitrary Leslie matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_Harvest2.html">
\begin{equation*}
L = \left[ \begin{array}{cccc} F_1 \amp  F_2   \amp  F_3   \amp  F_4 \\ s_1 \amp  0   \amp  0   \amp  0  \\ 0   \amp  s_2   \amp  0   \amp  0 \\ 0   \amp  0   \amp  s_3  \amp  0 \end{array}  \right]\text{.}
\end{equation*}
</div>
<p class="continuation">Recall that we want to find a value of <span class="process-math">\(h\)</span> that satisfies <a href="" class="xref" data-knowl="./knowl/eq_Harvest2.html" title="Equation 20.8">(20.8)</a> with <span class="process-math">\(H = \left[ \begin{array}{cccc} h   \amp  0   \amp  0   \amp  0 \\ 0   \amp  0   \amp  0   \amp  0  \\ 0   \amp  0   \amp  0   \amp  0 \\ 0   \amp  0   \amp  0      \amp  0 \end{array}  \right]\text{.}\)</span> Let <span class="process-math">\(\vx = [x_1 \  x_2 \ x_3 \ x_4]^{\tr}\text{.}\)</span></p>
</div>
<article class="task exercise-like" id="task-1161"><h6 class="heading"><span class="codenumber">(i)</span></h6>
<p id="p-3498">Calculate the matrix product <span class="process-math">\((I_{4}-H)L\text{.}\)</span> Explain why this product is again a Leslie matrix and why <span class="process-math">\((I_{4}-H)L\)</span> will have a dominant eigenvalue of 1.</p></article><article class="task exercise-like" id="task-1162"><h6 class="heading"><span class="codenumber">(ii)</span></h6>
<p id="p-3499">Now calculate <span class="process-math">\((I_{4}-H)L\vx\)</span> and set it equal to <span class="process-math">\(\vx\text{.}\)</span> Write down the resulting system of 4 equations that must be satisfied. Be sure that your first equation is</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="eq_harvestval1">
\begin{equation}
x_1 = (1-h)F_1x_1 + (1-h)F_2x_2 + (1-h)F_3x_3 + (1-h)F_4x_4\text{.}\tag{20.9}
\end{equation}
</div></article><article class="task exercise-like" id="task-1163"><h6 class="heading"><span class="codenumber">(iii)</span></h6>
<p id="p-3500">Equation <a href="" class="xref" data-knowl="./knowl/eq_harvestval1.html" title="Equation 20.9">(20.9)</a> as written depends on the entries of the vector <span class="process-math">\(\vx\text{,}\)</span> but we should be able to arrive at a result that is independent of <span class="process-math">\(\vx\text{.}\)</span> To see how we do this, we assume the population of the youngest group is never 0, so we can divide both sides of <a href="" class="xref" data-knowl="./knowl/eq_harvestval1.html" title="Equation 20.9">(20.9)</a> by <span class="process-math">\(x_1\)</span> to obtain</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_harvestval1.html ./knowl/eq_harvestval1.html" id="eq_harvestval2">
\begin{equation}
1 = (1-h)F_1 + (1-h)F_2 \frac{x_2}{x_1} + (1-h)F_3 \frac{x_3}{x_1} + (1-h)F_4 \frac{x_4}{x_1}\text{.}\tag{20.10}
\end{equation}
</div>
<p class="continuation">Now we need to write the fractions <span class="process-math">\(\frac{x_2}{x_1}\text{,}\)</span>   <span class="process-math">\(\frac{x_3}{x_1}\text{,}\)</span> and <span class="process-math">\(\frac{x_4}{x_1}\)</span> so that they do not involve the <span class="process-math">\(x_i\text{.}\)</span> Use the remaining equations in your system to show that</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_harvestval1.html ./knowl/eq_harvestval1.html" id="md-144">
\begin{align*}
\frac{x_2}{x_1} \amp = s_1\\
\frac{x_3}{x_1} \amp = s_1s_2\\
\frac{x_4}{x_1} \amp = s_1s_2s_3\text{.}
\end{align*}
</div></article><article class="task exercise-like" id="task-1164"><h6 class="heading"><span class="codenumber">(iv)</span></h6>
<p id="p-3501">Now conclude that the harvesting value <span class="process-math">\(h\)</span> must satisfy the equation</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="eq_harvestval3">
\begin{equation}
1 = (1-h) [F_1 + F_2 s_1 + F_3 s_1s_2 + F_4 s_1s_2s_3]\text{.}\tag{20.11}
\end{equation}
</div>
<p class="continuation">The value <span class="process-math">\(R = F_1 + F_2 s_1 + F_3 s_1s_2 + F_4 s_1s_2s_3\)</span> is called the <dfn class="terminology">net reproduction rate of the population</dfn> and turns out to be the average number of daughters born to a female in her expected lifetime.</p></article></article><article class="task exercise-like" id="task-1165"><h5 class="heading"><span class="codenumber">(e)</span></h5>
<p id="p-3502">Extend <a href="" class="xref" data-knowl="./knowl/eq_harvestval3.html" title="Equation 20.11">(20.11)</a> to the 12 age group case of the sheep herd. Calculate the value of <span class="process-math">\(R\)</span> for this sheep herd and then find the value of <span class="process-math">\(h\text{.}\)</span> Compare this <span class="process-math">\(h\)</span> to the value you obtained through experimentation earlier. Find the fraction of the lambs that should be harvested each year and explain what the stable population state vector <span class="process-math">\(\vx\)</span> tells us about the sheep population for this harvesting policy.</p></article></article></section></section><div class="hidden-content tex2jax_ignore" id="hk-fn-37"><div class="fn">There are several other ways to scale, but we won't consider them here.</div></div>
<div class="hidden-content tex2jax_ignore" id="hk-fn-38"><div class="fn">A matrix in which most entries are zero is called a <dfn class="terminology">sparse</dfn> matrix.</div></div>
</div></main>
</div>
</body>
</html>
