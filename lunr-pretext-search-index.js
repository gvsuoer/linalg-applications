var ptx_lunr_docs = [
{
  "id": "fm_preface",
  "level": "1",
  "url": "fm_preface.html",
  "type": "Preface",
  "number": "",
  "title": "Preface",
  "body": "Preface A Free and Open-Source Linear Algebra Text \n      Mathematics is for everyone   whether as a gateway to other fields or as background for higher level mathematics. With linear algebra gaining importance in many applications, we feel that access to or the cost of a textbook should not stand in the way of a successful experience in learning linear algebra. Therefore, we made our textbook available to everyone for free download for their own non-commercial use. We especially encourage its use in linear algebra classrooms for instructors who are looking for an inquiry-based textbook or a supplemental resource to accompany their course.If an instructor would like to make changes to any of the files to better suit your students' needs, we offer source files for the text by making a request to the authors.\n     \n      This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. The graphic   that appears throughout the text shows that the work is licensed with the Creative Commons, that the work may be used for free by any party so long as attribution is given to the author(s), that the work and its derivatives are used in the spirit of  share and share alike,  and that no party may sell this work or any of its derivatives for profit. Full details may be found by visiting\n       the Creative Commons Website \n      or sending a letter to Creative Commons, 444 Castro Street, Suite 900, Mountain View, California, 94041, USA.\n     Motivation \n      The material in this text was developed with a strong focus on linear algebra in   for the first two-thirds of the course, along with an emphasis on applications. We believe that the main ideas of linear algebra are more accessible to students if they are introduced in the more familiar context of vectors in   rather than in abstract vector spaces. For this reason, vector spaces do no arise until late in the text.\n     Goals \n       An Inquiry-Based Introduction to Linear Algebra and Applications  provides a novel inquiry-based learning approach to linear algebra, as well as incorporating aspects of an inverted classroom. The impetus for this book lies in our approach to teaching linear algebra. We place an emphasis on active learning and on developing students' intuition through their investigation of examples. For us, active learning involves students   they are DOING something instead of being passive learners. What students are doing when they are actively learning might include discovering, processing, discussing, applying information, writing intensive assignments, engaging in common intellectual in-class experiences or collaborative assignments and projects. Although it is difficult to capture the essence of active learning in a textbook, this book is our attempt to do just that.\n     \n      Additionally, linear algebra is one of the most applicable branches of mathematics to other disciplines. In fact, the Linear Algebra Curriculum Study Group ( LACSG ) (formed in 1990 to  initiate substantial and sustained national interest in improving the undergraduate linear  algebra curriculum  and funded by the National Science Foundation) made recommendations for the linear algebra curriculum, one of which was making linear algebra a matrix-oriented course that includes applications to give an indication of the  pervasive use  of linear algebra in other disciplines.\n\n       David Carlson, Charles R. Johnson, David Lay, Duane Porter.\n       The Linear Algebra Curriculum Study Group Recommendations for the First Course in Linear \n      Algebra. \n       The College Mathematics Journal , Vol. 24, No. 1, 1993, 41-46.\n        Also,\n      the most recent Committee on the Undergraduate Program in Mathematics ( CUPM ) report\n      \n       \n       2015 CUPM Curriculum Guide to Majors in the Mathematical Sciences , Carol S. Schumacher \n      and Martha J. Siegel, Co-Chairs, Paul Zorn, Editor.\n        \n      \n      to the Mathematical Association of America recommends  every Linear Algebra course should incorporate interesting applications, both to highlight the broad usefulness of linear algebra and to help students see the role of the theory in the subject as it is applied.  An important component of this text is its inclusion of significant  real-life  applications of linear algebra to help students see that linear algebra is widely applicable in  many disciplines. Our goals for these materials are several.\n\n       \n           \n            To carefully introduce the ideas behind the definitions and theorems to help students develop intuition and understand the logic behind them.\n           \n         \n           \n            To help students understand that mathematics is not done as it is often presented. We expect students to experiment through examples, make conjectures, and then refine their conjectures. We believe it is important for students to learn that definitions and theorems don't pop up completely formed in the minds of most mathematicians, but are the result of much thought and work.\n           \n         \n           \n            To help students develop their communication skills in mathematics. We expect our students to read and complete activities before class and come prepared with questions. While in class, students work to discover many concepts on their own through guided activities. Of course, students also individually write solutions to exercises on which they receive significant feedback. Communication skills are essential in any discipline and we place a heavy focus on their development.\n           \n         \n           \n            To have students actively involved in each of these items through in-class and out-of-class activities, in-class presentations (this is of course up to the instructor), and problem sets.\n           \n         \n           \n            To expose students to significant real-life applications of linear algebra.\n           \n         \n     Layout \n      This text is formatted into sections, each of which contains preview activities, in-class activities, worked examples, and exercises. Most sections conclude with an application project   an application of the material in the section. The various types of activities serve different purposes.\n       \n           \n            Preview activities are designed for students to complete before class to motivate the upcoming topic and prepare them with the background and information they need for the class activities and discussion.\n           \n         \n           \n            We generally use the regular activities to engage students during class in critical thinking experiences. These activities are used to provide motivation for the material,opportunities for students to develop course material on their own, or examples to help reinforce the meanings of definitions or theorems. The ultimate goal is to help students develop their intuition for and understanding of linear algebra concepts.\n           \n         \n           \n            Worked examples are included in each section. Part of the philosophy of this text is that students will develop the tools and understanding they need to work homework assignments through the preview, in-class activities, and class discussions. But some students express a desire for fully-worked examples in the text to reference during homework. In order to preserve the flow of material within a section, we include worked examples at the end of each section.\n           \n         \n           \n            The suggestion by the Linear Algebra Curriculum Study Group ( LACSG ) that,  Mathematics departments should seriously consider making their first course in linear algebra a matrix-oriented course.  is followed in this text. They suggest that this approach  implies less emphasis on abstraction and more emphasis on problem solving and motivating applications.  and that  Some applications of linear algebra should be included to give an indication of the pervasive use of linear algebra in many client disciplines. Such applications necessarily will be limited by the need to minimize technical jargon and information from outside the course. Students should see the course as one of the most potentially useful mathematics courses they will take as an undergraduate.  The focus of this text for the first two semesters of linear algebra ( )  is on  , and applications of linear algebra in real spaces. At our institution, these sections comprise a two-semester entry-level sequence into our major, suitable for freshmen without a calculus prerequisite. Abstract vector spaces do not appear until Section 28, which we consider to be the beginning of a third semester of linear algebra, a junior or senior level mathematics course. Also, the 2015 Committee on the Undergraduate Program in Mathematics ( CUPM ) report to the Mathematical Association of America recommends that  Every Linear Algebra course should incorporate interesting applications,both to highlight the broad usefulness of linear algebra and to help students see the role of the theory in the subject as it is applied. Attractive applications may also entice students majoring in other disciplines to choose a minor or additional major in mathematics.  All but two sections in this text include a substantial application. Each section begins with a short description of an application that uses the material from the section, then concludes with a project that develops the application in more detail. (The two sections that do not include projects are sections that are essentially long proofs   one section that contains formal proofs of the equivalences of the different parts of the Invertible Matrix Theorem, and the other contains algebraic proofs of the properties of the determinant.) The projects are independent of the material in the text   the text can be used without the applications. The applications are written in such a way that they could also be used with other textbooks. The projects are written following an inquiry-based style similar to the text, with important parts of the applications developed through activities. So the projects can be assigned outside of class as independent work for students. Several of the projects are accompanied by GeoGebra applets or Sage worksheets which are designed to help the students better understand the applications.\n           \n         \n           \n            Each investigation contains a collection of exercises. The exercises occur at a variety of levels of difficulty and most force students to extend their knowledge in different ways. While there are some standard, classic problems that are included in the exercises,many problems are open ended and expect a student to develop and then verify conjectures.\n           \n         \n     Acknowledgements \n      The many beautiful images that are in this text were created by our colleague David Austin, to whom we owe a debt of gratitude. The original  latex  version of the book was partially supported by Grand Valley State University through an internal grant and a sabbatical.\n     \n      The  HTML  version of the text is possible because of the work of Rob Beezer and the  PreTeXt  team for the development of the  PreTeXt  platform. David Farmer at the American Institute of Mathematics provided an initial PreTeXt conversion. From this version, Ian Curtis, Editorial Assistant for the GVSU Libraries, as part of the  Accelerating Open Educational Resources Initiative at Grand Valley State University , invested a significant amount of time and effort to create the  PreTeXt  version that you see. This work was conducted with support from the University Libraries and the  GVSU  President's Innovation Fund.\n     \n      The code used to generate this book can be found in the  GVSU   OER  GitHub repository . This includes the PreTeXt  XML  files, figures, customization files ( XSL  and  CSS ), and the output ( HTML  and  PDF ). Contributors are encouraged to submit an issue or a pull request with changes, especially regarding accessibility (as mentioned below in  ).\n     Accessibility \n      In creating this  PreTeXt  version we (the authors and the Grand Valley State University Libraries) strive to ensure our tools, devices, services, and environments are available to and usable by as many people as possible.\n     \n      The web version of  An Inquiry-Based Introduction to Linear Algebra and Applications  incorporates the following features to support accessibility:\n       \n           \n            All content can be navigated by use of a keyboard\n           \n         \n         \n          Links, headings, and tables have been designed to work with screen readers\n         \n       \n         \n          Mathematics in  PreTeXt  are rendered with MathJax so they can be understood by people using screen readers and\/or other assistive devices.\n         \n       \n     \n      We have identified some accessibility issues which were beyond the author's and  GVSU  Libraries' capacity to address in the course of this adaptation. These present an opportunity for future adaptation or revision by educators with expertise in accessibility issues in mathematics documents. Known accessibility issues include: \n       \n           \n            A small number of exercises and activities in this text require students to visually interpret diagrams or figures. These activities cannot be made accessible through alt text alone, and may need to be redesigned or replaced for full accessibility.\n           \n         \n           \n            Some visual representations of concepts (see, for example,  ) are not accessible to screen readers. However, they are fully described in text.\n           \n         \n           \n            The matrix table represented in   is too complex to encode with the current functionality of  PreTeXt  and MathJax. It is presented as an image and is not accessible to screen readers. Full accessibility likely requires redesigning the project or waiting for expanded features in  PreTeXt  and MathJax. \n           \n         \n     \n      We are always looking for how we can make our resources more accessible. If you are having problems accessing this resource, please  contact us by email  to let us know.\n     To the Student \n      The inquiry-based format of this book means that you can be in charge of your own learning. The guidance of your instructor and support of your fellow classmates can help you develop your understanding of the topic. Your learning will benefit best if you engage in the material by completing all preview and in-class activities in order to fully develop your own intuition and understanding of the material, which can be achieved only by reflecting on the ideas and concepts and communicating your reflections with others. Don't be afraid to ask questions or to be wrong at times, for this is how people learn. Good luck! We will be happy to hear your comments about the book.\n     "
},
{
  "id": "chap_intro_linear_systems",
  "level": "1",
  "url": "chap_intro_linear_systems.html",
  "type": "Section",
  "number": "1",
  "title": "Introduction to Systems of Linear Equations",
  "body": "Introduction to Systems of Linear Equations \n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is a linear equation?\n           \n         \n           \n            What is a system of linear equations?\n           \n         \n           \n            What is a solution set of a system of linear equations?\n           \n         \n           \n            What are equivalent systems of linear equations?\n           \n         \n           \n            What operations can we use to solve a system of linear equations?\n           \n         Application: Electrical Circuits modeling junctions nodes A circuit. \n    The source creates a charge that produces potential energy   measured in volts (V).\n    Current flows out of the positive terminal of a source and runs through each branch of the circuit.\n    Let  ,  ,\n    and   be the currents as illustrated in  .\n    The goal is to find the current flowing in each branch of the circuit.\n   \n    Linear algebra comes into play when analyzing a circuit based on the relationship between current  ,\n    resistance  , and voltage  .\n    There are laws governing electrical circuits that state that   across a resistor.\n    Additionally, Kirchoff's Current and Voltage Laws indicate how current behaves within the whole circuit.\n    Using all these laws together, we derive the system\n     ,\n    where  ,  ,\n    and   are the currents at the points indicated in  .\n    To finish analyzing the circuit,\n    we now need to solve this system.\n    In this section we will begin to learn systematic methods for solving systems of linear equations.\n    More details about the derivation of these circuit equations can be found at the end of this section.\n   Introduction \n    Systems of linear equations arise in almost every field of study:\n    mathematics, statistics, physics,\n    chemistry, biology, economics, sociology,\n    computer science, engineering, and many, many others.\n    We will study the theory behind solving systems of linear equations,\n    implications of this theory,\n    and applications of linear algebra as we proceed throughout this text.\n   \n            Consider the following system of two linear equations in two unknowns,\n             :\n             .\n           \n            One way to solve such a system of linear equations is the method of substitution\n            (where one equation is solved for one variable and then the resulting expression is \n            substituted into the remaining equations).\n            This method works well for simple systems of two equations in two unknowns,\n            but becomes complicated if the number or complexity of the equations is increased.\n           \n            Another method is elimination   the method that we will adopt in this book.\n            Recall that the elimination method works by multiplying each equation by a suitable \n            constant so that the coefficients of one of the variables in each equation is the same.\n            Then we subtract corresponding sides of these equations to eliminate that variable.\n           \n            Use the method of elimination to show that this system has the unique solution \n              and  .\n            Explain the specific steps you perform when using elimination.\n           \n            Recall that a linear equation in two variables can be represented as a line in  ,\n            the Cartesian plane,\n            where one variable corresponds to the horizontal axis and the other to the vertical axis.\n            Represent the two equations   and\n              in   and illustrate the solution to the system in your picture.\n           \n            The previous example should be familiar to you as a system of two equations in two unknowns.\n            Now we consider a system of three equations in three unknowns\n             \n            that arises from our electrical circuit in  ,\n            with currents  ,\n             , and   as indicated in the circuit.\n           A circuit. \n            In the remainder of this preview activity we will apply the method of elimination to solve \n            the system of linear equations  ,\n             , and  .\n           \n                  Replace equation   with the new equation obtained by \n                  multiplying both sides of equation   by 5 and then \n                  subtracting corresponding sides of this equation from the appropriate sides of \n                  equation  .\n                  Show that the resulting system is\n                   .\n                 \n                  Now eliminate the variable   from the last two equations in the system in \n                  part (a) by using equations   and \n                    to show that  .\n                  Explain your process.\n                 \n                  Once you know the value for  ,\n                  how can you find  ?\n                  Then how do you find  ?\n                  Use your method to show that the solution to this system is the ordered triple (1,1.5,0.5).\n                  Interpret the result in terms of currents.\n                 Notation and Terminology \n    To study linear algebra,\n    we will need to agree on some general notation and terminology to represent our systems.\n   \n    An equation like   is called a linear equation because the variables \n    (  and   in this case) are raised to the first power,\n    and there are no products of variables.\n    The equation   is a linear equation in two variables,\n    but we can make a linear equation with any number of variables we like.\n   linear equation linear equation coefficients linear equation coefficients \n    We can use any labels for the variables in a linear equation that we like,\n    e.g.,  ,  ,  ,\n    and you should become comfortable working with variables in any form.\n    We will usually use subscripts,\n    as in  ,\n    to represent the variables as this notation allows us to have any number of variables.\n    Other examples of linear equations are\n     .\n   \n    On the other hand, the equations\n     \n    are non-linear equations.\n   system of linear equations system of linear equations \n    For example, the two equations\n     \n\n    form a system of two linear equations in variables  ,\n     .\n   system of linear equations solution solution solution set of the system Solving Systems of Linear Equations system of linear equations equivalent systems elementary operations system of linear equations operations equivalent elementary operations on a system of linear equations elementary operations elementary operations \n    When we apply these elementary operations our ultimate goal is to produce a system of linear equations in a simplified form with the same solution set,\n    where the number of variables eliminated from the equations increase as we move from top to bottom.\n    This method is called the  elimination system of linear equations elimination  method.\n   \n      For systems of linear equations with a small number of variables,\n      many different methods could be used to find a solution.\n      However, when a system gets large,\n      ad-hoc methods become unwieldy.\n      One of our goals is to develop an algorithmic approach to solving systems of linear equations \n      that can be programmed and applied to any linear system,\n      so we want to work in a very prescribed method as indicated in this activity.\n      Ultimately, once we understand how the algorithm works,\n      we will use calculators\/computers to do the work.\n      Apply the elimination method as described to show that the solution set of the following system \n      is  :\n\n       .\n     \n            Use the first equation to eliminate the variable   in the second and third equations.\n           \n            Use the new second equation to eliminate the variable   in the third equation \n            and find the value of  .\n           \n            Find values of   and then  .\n           Important Note \n    Technically,\n    we don't really add two equations or multiply an equation by a scalar.\n    When we refer to a scalar multiple of an equation,\n    we mean the equation obtained by equating the scalar multiple of the expression on the left \n    side of the equation and the same scalar multiple of the expression on the right side of the equation.\n    Similarly, when we refer to a sum of two equations,\n    we don't really add the equations themselves.\n    Instead, we mean the equation obtained by equating the sum of the expressions on the left sides\n     of the equations to the sum of the expressions on the right sides of the equations.\n    We will use the terminology\n     scalar multiple of an equation \n    and\n     sum of two equations \n    as shorthand to mean what is described here.\n   Another Important Note \n    There is an important and subtle point to consider here.\n    When we use these operations to find a solution to a system of equations,\n    we are assuming that the system has a solution.\n    The application of these operations then tells us what a solution must look like.\n    However, there is no guarantee that the outcome is actually a solution   to be safe we \n    should check to make sure that our result is a solution to the system.\n    In the case of linear systems, though,\n    every one of our operations on equations is reversible\n    (if applied correctly),\n    so the result will always be a solution\n    (but this is not true in general for non-linear systems).\n   Terminology system of linear equations consistent system of linear equations inconsistent consistent inconsistent The Geometry of Solution Sets of Linear Systems \n    We are familiar with linear equations in two variables from basic algebra and calculus\n    (through linear approximations).\n    The set of solutions to a system of linear equations in two variables has some geometry connected \n    to it.\n   \n      Recall that we examined the geometry of the system\n       \n\n      in  \n      to show that the resulting solution set consists of a single point in the plane.\n     \n      In this activity we examine the geometry of the system\n       .\n     \n        Consider the linear equation  \n        (or, equivalently  ).\n        What is the graph of the solution set (the set of points\n          satisfying this equation) of this single equation in the plane?\n        Draw the graph to illustrate.\n       \n        How can we represent the solution set of the system   of two equations graphically?\n        How is this solution set related to the solution set of the single equation  ?\n        Why?\n        How many solutions does the system   have?\n       \n        There are exactly three possibilities for the number of solutions to a general system of two \n        linear equations in two unknowns.\n        Describe the geometric representations of solution sets for each of the possibilities.\n        Illustrate each with a specific example\n        (of your own)\n        using a system of equations and sketching its geometric representation.\n       \n     \n    shows that there are three options for the solution set of a system: A system can have no solutions,\n    one solution, or infinitely many solutions.\n   \n    Now we consider systems of three variables.\n    As an example,\n    let us look at the linear equation   in the three variables  ,\n     ,\n    and  .\n    Notice that the points  ,\n     , and   all satisfy this equation.\n    As a linear equation,\n    the graph of   will be a plane in three dimensions that contains these three points,\n    as shown in  .\n    Hence when we consider a linear system in three unknowns,\n    we are looking for a point in the three dimensional space that lies on all the planes described \n    by the equations.\n   The plane  . \n      In this activity we examine the geometry of linear systems of three equations in three unknowns.\n      Recall that each linear equation in three variables has a plane as its solution set.\n      Use a piece of paper to represent each plane.\n     \n           Is it possible for a general system of three linear equations in three unknowns to have no \n           solutions?\n            If so, geometrically describe this situation and then illustrate each with a specific\n            example using a system of equations.\n            If not, explain why not.\n         \n          Is it possible for a general system of three linear equations in three unknowns to have \n          exactly one solution?\n          If so, geometrically describe this situation and then illustrate each with a specific \n          example using a system of equations.\n          If not, explain why not.\n         \n          Is it possible for a general system of three linear equations in three unknowns to have \n          infinitely many solutions?\n          If so, geometrically describe this situation and then illustrate each with a specific \n          example using a system of equations.\n          If not, explain why not.\n         Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Apply the allowable operations on equations to solve the system\n         \n       \n        We begin by eliminating the variable   from all but the first equation.\n        To do so, we replace the third equation with the third equation minus the first equation to \n        obtain the equivalent system\n         \n       \n        Then we replace the fourth equation with the fourth equation minus 2 times the first to obtain \n        the equivalent system\n         \n       \n        To continue the elimination process,\n        we want to eliminate the   variable from our latest third and fourth equations.\n        To do so, we use the second equation so that we do not reinstate an   variable in \n        our new equations.\n        We replace equation three with equation 3 minus 2 times equation 2 to produce the equivalent \n        system\n         \n       \n        Then we replace equation four with equation four minus 7 times equation 2, giving us the \n        equivalent system\n         \n       \n        With one more step we can determine the value of  .\n        We use the last two equations to eliminate   from the fourth equation by replacing \n        equation four with equation four minus 2 times equation 3.\n        This results in the equivalent system\n         \n       \n        The last equation tells us that  , or  .\n        Substituting into the third equation shows that\n         .\n       \n        The second equation shows that\n         .\n       \n        Finally, the first equation tells us that\n         .\n       \n        So the solution to our system is  ,  ,\n         , and  .\n        It is worth substituting back into our original system to check to make sure that we have not \n        made any arithmetic mistakes.\n       \n        A mining company has three mines.\n        One day of operation at the mines produces the following output.\n         \n             \n              Mine 1 produces 25 tons of copper, 600 kilograms of silver and 15 tons of manganese.\n             \n           \n             \n              Mine 2 produces 30 tons of copper, 500 kilograms of silver and 10 tons of manganese.\n             \n           \n             \n              Mine 3 produces 20 tons of copper, 550 kilograms of silver and 12 tons of manganese.\n             \n           \n       \n        Suppose the company has orders for 550 tons of copper, 11350 kilograms of silver and 250 \n        tons of manganese.\n       \n        Write a system of equations to answer the question:\n        how many days should the company operate each mine to exactly fill the orders?\n        State clearly what the variables in your system represent.\n        Then find the general solution of your system.\n       \n        For our system,\n        let   be the number of days mine 1 operates,\n          be the number of days mine 2 operates,\n        and   be the number of days mine 3 operates.\n        Since mine 1 produces 25 tons of copper each day,\n        in   days mine 1 will produce   tons of copper.\n        Mine 2 produces 30 tons of copper each day,\n        so in   days mine 2 will produce   tons of copper.\n        Also, mine 3 produces 20 tons of copper each day,\n        so in   days mine 3 will produce   tons of copper.\n        Since the company needs to supply a total of 550 tons of copper,\n        we need to have  .\n        Similar analyses of silver and manganese give us the system\n         \n       \n        To solve the system,\n        we eliminate the variable   from the second and third equations by replacing equation \n        two with equation two minus 24 times equation one and replacing equation three with equation \n        three minus   times equation one.\n        This produces the equvalent system\n         \n       \n        We are fortunate now that we can determine the value of   from the third equation,\n        which tells us that  .\n        Substituting into the second equation shows that\n         .\n       \n        Substituting into the first equation allows us to determine the value for  :\n         .\n       \n        So the company should run mine 1 for 6 days,\n        mine 2 for 10 days, and mine 3 for 5 days to meet this demand.\n       Summary \n    In this section we introduced linear equations and systems of linear equations.\n     \n         \n          Informally, a linear equation is an equation in which each term is either a constant or a \n          constant times a variable.\n          More formally,\n          a linear equation in the variables  ,  ,\n           ,   is an equation of the form\n           ,\n          where   is a positive integer and  ,  ,\n           ,   and   are constants.\n         \n       \n         \n          A system of linear equations is a collection of one or more linear equations in the same \n          variables.\n         \n       \n         \n          Informally, a solution to a system of linear equations is a point that satisfies all of \n          the equations in the system.\n          More formally,\n          a solution to a system of linear equation in   variables  ,\n           ,  ,\n            is an ordered  -tuple\n            of numbers so that we obtain all true statements in the system when we replace \n            with  ,\n            with  ,  ,\n          and   with  .\n         \n       \n         \n          Two linear systems are equivalent if they have the same solution set.\n         \n       \n         \n          The following operations on a system of equations do not change the solution set:\n           \n               \n                Replace one equation by the sum of that equation and a scalar multiple of another \n                equation.\n               \n             \n               \n                Interchange two equations.\n               \n             \n               \n                Replace an equation by a nonzero scalar multiple of itself.\n               \n             \n         \n       \n   \n        In the method of elimination there are three operations we can apply to solve a system of \n        linear equations.\n        For this exercise we focus on a system of equations in three unknowns  ,\n         , and  ,\n        but the arguments generalize to a system with any number of variables.\n        Consider the general system of three equations in three unknowns\n         .\n        The goal of this exercise is to understand why the three operations on a system do not \n        change the solutions to the system.\n        Recall that a solution to a system with unknowns  ,\n         ,\n        and   is a set of three numbers,\n        one for  , one for  ,\n        and one for   that satisfy all of the equations in the system.\n       \n          Explain why, if we have a solution to this system,\n          then that solution is also a solution to any constant   times the second equation.\n         \n          Consider the equations\n           ,\n          and\n           .\n         \n          Explain why, if we have a solution to this system,\n          then that solution is also a solution to the sum of the first equation and   \n          times the third equation for any constant  .\n         \n          Similar to part (a)\n         \n        Alice stopped by a coffee shop two days in a row at a conference to buy drinks and pastries.\n        On the first day,\n        she bought a cup of coffee and two muffins for which she paid $6.87.\n        The next day she bought two cups of coffee and three muffins\n        (for herself and a friend).\n        Her bill was $11.25.\n        Use the method of elimination to determine the price of a cup of coffee,\n        and the price of a muffin.\n        Clearly explain your set-up for the problem. (Assume you are explaining your solution to\n        someone who has not solved the problem herself\/himself).\n       \n        Alice stopped by a coffee shop three days in a row at a conference to buy drinks and pastries.\n        On the first day, she bought a cup of coffee,\n        a muffin and a scone for which she paid $6.15.\n        The next day she bought two cups of coffee, three muffins and a scone\n        (for herself and friends).\n        Her bill was $12.20.\n        The last day she bought a cup of coffee,\n        two muffins and two scones, and paid $10.35.\n        Determine the price of a cup of coffee,\n        the price of a muffin and the price of a scone.\n        Clearly explain your set-up for the problem. (Assume you are explaining your solution to \n        someone who has not solved the problem herself\/himself).\n       \n        A cup of coffee costs $ ,\n        a muffin costs $ , and a scone costs $ .\n       \n        Find an example of a system of two linear equations in variables  ,\n          for each of the following three cases:\n       \n              where the equations correspond to two non-parallel lines,\n             \n              two parallel distinct lines,\n             \n              two identical lines\n              (represented with different equations).\n             \n          Describe how the relationship between the coefficients of the variables of the two \n          equations in parts (ii) and (iii) are different than the relationship between those \n          coefficients in part (i) (Note: Please make sure your system examples are different than \n          the examples in the activities,\n          and that they are your own examples.)\n         \n        In a grid of wires in thermal equilibrium,\n        the temperature at interior nodes is the average of the temperatures at adjacent nodes.\n        Consider the grid as shown in  ,\n        with  ,  , and   the temperatures\n        (in degrees Centigrade)\n        at the indicated interior nodes,\n        and fixed temperatures at the other nodes as shown.\n        For example,\n        the nodes adjacent to the node with temperature   have temperatures of  ,\n         ,  , and  ,\n        so when the grid is in thermal equilibrium   is the average of these temperatures:\n         .\n       A grid of wires. \n              Determine equations for the temperatures   and   if the grid is in \n              thermal equilibrium to construct a system of three linear equations in  ,\n               ,\n              and   that models node temperatures in the grid in thermal equilibrium.\n             \n               \n             \n              Use the method of elimination to find a specific solution to the system that makes \n              sense in context.\n             \n               ,   and  .\n             \n        We have seen that a linear system of two equations in two unknowns can have no solutions,\n        one solution, or infinitely many solutions.\n        Find, if possible, a specific example of each of the following.\n        If not possible, explain why.\n       \n              A linear system of three equations in two unknowns with no solutions.\n             \n              A linear system of three equations in two unknowns with exactly one solution.\n             \n              A linear system of three equations in two unknowns with exactly two solutions.\n             \n              A linear system of three equations in two unknowns with infinitely many solutions.\n             \n        We have seen that a linear system of three equations in three unknowns can have no solutions,\n        one solution, or infinitely many solutions.\n        Find, if possible, a specific example of each of the following.\n        If not possible, explain why.\n       \n              A linear system of two equations in three unknowns with no solutions.\n             \n               \n             \n              A linear system of two equations in three unknowns with exactly one solution.\n             \n              Impossible.\n             \n              A linear system of two equations in three unknowns with exactly two solutions.\n             \n              Impossible.\n             \n              A linear system of two equations in three unknowns with infinitely many solutions.\n             \n               \n             \n        Find a system of three linear equations in two variables  ,\n          whose solution is  ,  .\n       \n        Consider the system of linear equations\n         \n        where   is an unknown constant.\n       \n              Determine the solution(s) of this system for all possible   values,\n              if a solution exists.\n              (Note: Your answers for the variables will depend on the  .)\n             \n              This system has the solution   and\n                as long as  .\n             \n              How do your answers change if the second equation in the system above is changed to \n               ?\n             \n              When  ,\n                and   is free.\n             \n        Suppose we are given a system of two linear equations\n         .\n        Find another system of two linear equations   and   in the variables  ,\n         ,\n        and   that are not multiples of each other or of equations   \n        or   so that any solution\n          to the system   and   is a \n        solution to the system   and  .\n       True\/False Questions \n    In many sections you will be given True\/False questions.\n    In each of the True\/False questions,\n    you will be given a statement, such as\n     if we add corresponding sides of two linear equations,\n    then the resulting equation is a linear equation \n    and\n     one can find a system of two equations in two unknowns that has infinitely many solutions. \n    Your task will be to determine the truth value of the statement and to give a brief justification \n    for your choice.\n   \n    Note that a  general  statement is considered  true \n    only when it is always true.\n    For example, the first of the above statements  \n     if we add corresponding sides of two linear equations,\n    then the resulting equation is a linear equation \n      is a general statement.\n    For this statement to be true,\n    the equation we obtain by adding corresponding sides of any two linear equations has to be linear.\n    If we can find two equations that do not give a linear equation when combined in this way,\n    then this statement is false.\n   \n    Note that an  existential \n    statement is considered  true \n    if there is at least one example which makes is true.\n    For example, the latter of the above statements  \n     one can find a system of two equations in two unknowns that has infinitely many solutions \n      is an existential statement.\n    For this statement to be true,\n    existence of a system of two equations in two unknowns with infinitely many solutions should suffice.\n    If it is impossible to find two such equations,\n    then this statement is false.\n   \n    To justify that something always happens or never happens,\n    one would need to refer to other statements whose truth is known,\n    such as theorems, definitions.\n    In particular, giving an  example \n    of two linear equations that produce a linear equation when we add corresponding sides\n     does not justify  why the sum of  any \n    two linear equations is also linear.\n    Using the definition of linear equations, however,\n    we can justify why this new equation will always be linear:\n    each side of a linear equation is linear,\n    and adding linear expressions always produces a linear sum.\n   \n    To justify that there are examples of something happening or not happening,\n    one would need to give a specific example.\n    For example,\n    in justifying the claim that there is a system of two equations in two unknowns with infinitely many solutions,\n    it is not enough to say\n     An equation in two unknowns is a line in the  -plane,\n    so there can be two equations with the same line as their solution. \n    In general, you should avoid the words\n     can, \n     possibly, \n     maybe, \n    etc., in your justifications.\n    Instead, giving an example such as\n     The linear system   and   of two equations in two unknowns has \n    infinitely many solutions since the second equation gives the same line as the first in the \n     -plane. \n    provides complete justification beyond a reasonable doubt.\n   \n    Each response to a True\/False statement should be more than just True or False.\n    It is important that you provide\n     justification  for your responses.\n   True\/False \n              The set of all solutions of a linear equation can be represented graphically as a line.\n             \n              F\n             True\/False \n              The set of all solutions of a linear equation in two variables can be represented graphically as a line.\n             True\/False \n              The set of all solutions of an equation in two variables can be represented graphically as a line.\n             \n              F\n             True\/False \n              A system of three linear equations in two unknowns cannot have a unique solution.\n             True\/False \n              A system of three linear equations in three unknowns has a unique solution.\n             \n              F\n             Project: Modeling an Electrical Circuit and the Wheatstone Bridge Circuit Ohm's Law Kirchoff's Current Law Kirchoff's Voltage Law \n    Mathematical modeling,\n    or the act of creating equations to model given information,\n    is an important part of problem solving.\n    In this section we will see how we derived the system of equations\n     \n    to represent the electrical current in the circuit shown in  .\n    Recall that a circuit consists of\n     \n         \n          one or more electrical sources\n          (like a battery),\n          denoted by  \n         \n       \n         \n          one or more resistors\n          (like any appliance that you plug into a wall outlet),\n          denoted by   .\n         \n       \n   \n    The source creates a charge that produces potential energy   measured in volts (V).\n    No substance conducts electricity perfectly,\n    there is always some price to pay\n    (energy loss)\n    to moving electricity.\n    Electrical current   in amperes (A) is the flow of the electric charge in the circuit. (A current of 1 ampere means that\n      electrons pass through the circuit per second.) Current flows out of the positive terminal of a source and runs through each branch of the circuit.\n    Let   be the current flowing through the upper branch,\n      the current through middle branch,\n    and   the current through the lower branch as illustrated in  .\n    The goal is to find the current flowing in each branch of the circuit.\n   \n    Linear algebra comes into play when analyzing a circuit based on the relationship between current,\n    resistance, and potential.\n    Three basic principles govern current low in a circuit.\n     \n         \n          Resistance   in ohms ( ) can be thought of as a measure of how difficult \n          it is to move a charge along a circuit.\n          When a current flows through a resistor,\n          it must expend some energy, called a  voltage drop .\n          Ohm's Law\n          states that the voltage drop   across a resistor is the product of the current \n            passing through the resistor and the resistance  .\n          That is,\n           .\n         \n       \n         \n          Kirchoff's Current Law\n          states that at any point in an electrical circuit,\n          the sum of currents flowing into that point is equal to the sum of currents flowing out \n          of that point.\n         \n       \n         \n          Kirchoff's Voltage Law\n          says that around any closed loop the sum of the voltage drops is equal to the sum of the voltage rises.\n         \n       \n   \n    To see how these laws allow us to model the circuit in  ,\n    we will need three equations in  ,  ,\n    and   to determine the values of these currents.\n    Let us first apply Kirchoff's Current Law to the point P. The currents flowing into point P are   and  ,\n    and the current flowing out is  .\n    This produces the equation  , or\n     .\n   \n      Apply Kirchoff's Current Law to the point   to obtain an equation in  ,\n       , and  .\n      What do you notice?\n     \n    We have three variables to determine,\n    so we still need two more equations in  ,\n     , and  .\n    Next we apply Kirchoff's Voltage Law to the top loop in the circuit in  .\n    We will assume the following sign conventions:\n     \n         \n          A current passing through a resistor produces a voltage drop if it flows in the \n          direction of loop\n          (and a voltage rise if the current passes in the opposite direction of the loop).\n         \n       \n         \n          A current passing through a source in the direction of the loop produces a voltage drop \n          if it flows from   to   and a voltage rise if it flows from   to  ,\n          while a current passing through a source in the opposite direction of the loop produces a \n          voltage rise if it flows from   to   and a voltage drop if it flows from   to  .\n         \n       \n   \n    (The directions chosen in  \n    for the voltage flow are arbitrary   if we reverse the flow then we just replace \n    voltage drops with voltage rises and obtain the same equations.\n    If a solution shows that a current is negative,\n    then that current flows in the direction opposite of what is shown.)\n   \n    If we move in the counterclockwise direction around the top loop in the circuit in  ,\n    there is a voltage rise through the source of 8 volts.\n    This must equal the voltage drop in this loop.\n    The current   passing though the resistor of resistance 2   produces a voltage drop of   volts.\n    Similarly, the current   passing through the resistor of resistance\n      produces a voltage drop of   volts.\n    Finally, the current   passing through the resistor of resistance 2   produces a voltage drop of   volts.\n    So Kirchoff's Voltage Law applied to the top loop in the circuit in  \n    gives us the equation   or\n     .\n   \n      Apply Kirchoff's Voltage Law to the bottom loop in the circuit in  \n      to obtain an equation in  ,\n       , and  .\n      Compare the three equations we have found to those in the introduction.\n     A Wheatstone bridge circuit. \n      Consider the circuit as shown in  ,\n      with a single source and five resistors with resistances  ,\n       ,  ,  ,\n      and   as labeled.\n     \n            Assume the following information.\n            The voltage   is   volts,\n             ,\n            and  .\n            Follow the directions given to find the currents  ,\n             ,  ,  ,\n             , and  .\n           \n                  Use Kirchoff's Current Law to show that  ,\n                   , and  .\n                  Thus, we reduce the problem to three variables.\n                 \n                  Apply Kirchoff's Voltage Law to three loops to show that the currents must satisfy \n                  the linear system\n                   \n                 \n                  Solve the system to find the unknown currents.\n                 \n            The circuit pictured in  \n            is called a  Wheatstone bridge \n            (invented by Samuel Hunter Christie in 1833 and popularized by Sir Charles Wheatstone in 1843).\n            The Wheatstone bridge is a circuit designed to determine an unknown resistance by balancing two paths in a circuit.\n            It is set up so that the resistances of resistors   and   are known,\n              is a variable resistor and we want to find the resistance of  .\n            The resistor   is replaced with a voltmeter,\n            and the resistance of   is varied until the voltmeter reads  .\n            This balances the circuit and tells the resistance of resistor  .\n            Show that if the current   in   is  \n            (so the circuit is balanced),\n            then  \n            (which is how we calculate the unknown resistance  ).\n            Do this in general and do not use any specific values for the resistances or the voltage.\n           "
},
{
  "id": "objectives-1",
  "level": "2",
  "url": "chap_intro_linear_systems.html#objectives-1",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is a linear equation?\n           \n         \n           \n            What is a system of linear equations?\n           \n         \n           \n            What is a solution set of a system of linear equations?\n           \n         \n           \n            What are equivalent systems of linear equations?\n           \n         \n           \n            What operations can we use to solve a system of linear equations?\n           \n         "
},
{
  "id": "p-37",
  "level": "2",
  "url": "chap_intro_linear_systems.html#p-37",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "modeling "
},
{
  "id": "p-40",
  "level": "2",
  "url": "chap_intro_linear_systems.html#p-40",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "junctions nodes "
},
{
  "id": "F_circuit1",
  "level": "2",
  "url": "chap_intro_linear_systems.html#F_circuit1",
  "type": "Figure",
  "number": "1.1",
  "title": "",
  "body": "A circuit. "
},
{
  "id": "pa_1_a",
  "level": "2",
  "url": "chap_intro_linear_systems.html#pa_1_a",
  "type": "Preview Activity",
  "number": "1.1",
  "title": "",
  "body": "\n            Consider the following system of two linear equations in two unknowns,\n             :\n             .\n           \n            One way to solve such a system of linear equations is the method of substitution\n            (where one equation is solved for one variable and then the resulting expression is \n            substituted into the remaining equations).\n            This method works well for simple systems of two equations in two unknowns,\n            but becomes complicated if the number or complexity of the equations is increased.\n           \n            Another method is elimination   the method that we will adopt in this book.\n            Recall that the elimination method works by multiplying each equation by a suitable \n            constant so that the coefficients of one of the variables in each equation is the same.\n            Then we subtract corresponding sides of these equations to eliminate that variable.\n           \n            Use the method of elimination to show that this system has the unique solution \n              and  .\n            Explain the specific steps you perform when using elimination.\n           \n            Recall that a linear equation in two variables can be represented as a line in  ,\n            the Cartesian plane,\n            where one variable corresponds to the horizontal axis and the other to the vertical axis.\n            Represent the two equations   and\n              in   and illustrate the solution to the system in your picture.\n           \n            The previous example should be familiar to you as a system of two equations in two unknowns.\n            Now we consider a system of three equations in three unknowns\n             \n            that arises from our electrical circuit in  ,\n            with currents  ,\n             , and   as indicated in the circuit.\n           A circuit. \n            In the remainder of this preview activity we will apply the method of elimination to solve \n            the system of linear equations  ,\n             , and  .\n           \n                  Replace equation   with the new equation obtained by \n                  multiplying both sides of equation   by 5 and then \n                  subtracting corresponding sides of this equation from the appropriate sides of \n                  equation  .\n                  Show that the resulting system is\n                   .\n                 \n                  Now eliminate the variable   from the last two equations in the system in \n                  part (a) by using equations   and \n                    to show that  .\n                  Explain your process.\n                 \n                  Once you know the value for  ,\n                  how can you find  ?\n                  Then how do you find  ?\n                  Use your method to show that the solution to this system is the ordered triple (1,1.5,0.5).\n                  Interpret the result in terms of currents.\n                 "
},
{
  "id": "definition-1",
  "level": "2",
  "url": "chap_intro_linear_systems.html#definition-1",
  "type": "Definition",
  "number": "1.3",
  "title": "",
  "body": "linear equation linear equation coefficients linear equation coefficients "
},
{
  "id": "definition-2",
  "level": "2",
  "url": "chap_intro_linear_systems.html#definition-2",
  "type": "Definition",
  "number": "1.4",
  "title": "",
  "body": "system of linear equations system of linear equations "
},
{
  "id": "definition-3",
  "level": "2",
  "url": "chap_intro_linear_systems.html#definition-3",
  "type": "Definition",
  "number": "1.5",
  "title": "",
  "body": "system of linear equations solution solution "
},
{
  "id": "p-62",
  "level": "2",
  "url": "chap_intro_linear_systems.html#p-62",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "solution set of the system "
},
{
  "id": "p-63",
  "level": "2",
  "url": "chap_intro_linear_systems.html#p-63",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "equivalent "
},
{
  "id": "p-64",
  "level": "2",
  "url": "chap_intro_linear_systems.html#p-64",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "elementary operations on a system of linear equations elementary operations "
},
{
  "id": "theorem-1",
  "level": "2",
  "url": "chap_intro_linear_systems.html#theorem-1",
  "type": "Theorem",
  "number": "1.6",
  "title": "",
  "body": "elementary operations "
},
{
  "id": "activity-1",
  "level": "2",
  "url": "chap_intro_linear_systems.html#activity-1",
  "type": "Activity",
  "number": "1.2",
  "title": "",
  "body": "\n      For systems of linear equations with a small number of variables,\n      many different methods could be used to find a solution.\n      However, when a system gets large,\n      ad-hoc methods become unwieldy.\n      One of our goals is to develop an algorithmic approach to solving systems of linear equations \n      that can be programmed and applied to any linear system,\n      so we want to work in a very prescribed method as indicated in this activity.\n      Ultimately, once we understand how the algorithm works,\n      we will use calculators\/computers to do the work.\n      Apply the elimination method as described to show that the solution set of the following system \n      is  :\n\n       .\n     \n            Use the first equation to eliminate the variable   in the second and third equations.\n           \n            Use the new second equation to eliminate the variable   in the third equation \n            and find the value of  .\n           \n            Find values of   and then  .\n           "
},
{
  "id": "p-76",
  "level": "2",
  "url": "chap_intro_linear_systems.html#p-76",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "consistent inconsistent "
},
{
  "id": "act_1_a_2",
  "level": "2",
  "url": "chap_intro_linear_systems.html#act_1_a_2",
  "type": "Activity",
  "number": "1.3",
  "title": "",
  "body": "\n      Recall that we examined the geometry of the system\n       \n\n      in  \n      to show that the resulting solution set consists of a single point in the plane.\n     \n      In this activity we examine the geometry of the system\n       .\n     \n        Consider the linear equation  \n        (or, equivalently  ).\n        What is the graph of the solution set (the set of points\n          satisfying this equation) of this single equation in the plane?\n        Draw the graph to illustrate.\n       \n        How can we represent the solution set of the system   of two equations graphically?\n        How is this solution set related to the solution set of the single equation  ?\n        Why?\n        How many solutions does the system   have?\n       \n        There are exactly three possibilities for the number of solutions to a general system of two \n        linear equations in two unknowns.\n        Describe the geometric representations of solution sets for each of the possibilities.\n        Illustrate each with a specific example\n        (of your own)\n        using a system of equations and sketching its geometric representation.\n       "
},
{
  "id": "F_1_a_plane",
  "level": "2",
  "url": "chap_intro_linear_systems.html#F_1_a_plane",
  "type": "Figure",
  "number": "1.7",
  "title": "",
  "body": "The plane  . "
},
{
  "id": "act_1_a_3",
  "level": "2",
  "url": "chap_intro_linear_systems.html#act_1_a_3",
  "type": "Activity",
  "number": "1.4",
  "title": "",
  "body": "\n      In this activity we examine the geometry of linear systems of three equations in three unknowns.\n      Recall that each linear equation in three variables has a plane as its solution set.\n      Use a piece of paper to represent each plane.\n     \n           Is it possible for a general system of three linear equations in three unknowns to have no \n           solutions?\n            If so, geometrically describe this situation and then illustrate each with a specific\n            example using a system of equations.\n            If not, explain why not.\n         \n          Is it possible for a general system of three linear equations in three unknowns to have \n          exactly one solution?\n          If so, geometrically describe this situation and then illustrate each with a specific \n          example using a system of equations.\n          If not, explain why not.\n         \n          Is it possible for a general system of three linear equations in three unknowns to have \n          infinitely many solutions?\n          If so, geometrically describe this situation and then illustrate each with a specific \n          example using a system of equations.\n          If not, explain why not.\n         "
},
{
  "id": "example-1",
  "level": "2",
  "url": "chap_intro_linear_systems.html#example-1",
  "type": "Example",
  "number": "1.8",
  "title": "",
  "body": "\n        Apply the allowable operations on equations to solve the system\n         \n       \n        We begin by eliminating the variable   from all but the first equation.\n        To do so, we replace the third equation with the third equation minus the first equation to \n        obtain the equivalent system\n         \n       \n        Then we replace the fourth equation with the fourth equation minus 2 times the first to obtain \n        the equivalent system\n         \n       \n        To continue the elimination process,\n        we want to eliminate the   variable from our latest third and fourth equations.\n        To do so, we use the second equation so that we do not reinstate an   variable in \n        our new equations.\n        We replace equation three with equation 3 minus 2 times equation 2 to produce the equivalent \n        system\n         \n       \n        Then we replace equation four with equation four minus 7 times equation 2, giving us the \n        equivalent system\n         \n       \n        With one more step we can determine the value of  .\n        We use the last two equations to eliminate   from the fourth equation by replacing \n        equation four with equation four minus 2 times equation 3.\n        This results in the equivalent system\n         \n       \n        The last equation tells us that  , or  .\n        Substituting into the third equation shows that\n         .\n       \n        The second equation shows that\n         .\n       \n        Finally, the first equation tells us that\n         .\n       \n        So the solution to our system is  ,  ,\n         , and  .\n        It is worth substituting back into our original system to check to make sure that we have not \n        made any arithmetic mistakes.\n       "
},
{
  "id": "example-2",
  "level": "2",
  "url": "chap_intro_linear_systems.html#example-2",
  "type": "Example",
  "number": "1.9",
  "title": "",
  "body": "\n        A mining company has three mines.\n        One day of operation at the mines produces the following output.\n         \n             \n              Mine 1 produces 25 tons of copper, 600 kilograms of silver and 15 tons of manganese.\n             \n           \n             \n              Mine 2 produces 30 tons of copper, 500 kilograms of silver and 10 tons of manganese.\n             \n           \n             \n              Mine 3 produces 20 tons of copper, 550 kilograms of silver and 12 tons of manganese.\n             \n           \n       \n        Suppose the company has orders for 550 tons of copper, 11350 kilograms of silver and 250 \n        tons of manganese.\n       \n        Write a system of equations to answer the question:\n        how many days should the company operate each mine to exactly fill the orders?\n        State clearly what the variables in your system represent.\n        Then find the general solution of your system.\n       \n        For our system,\n        let   be the number of days mine 1 operates,\n          be the number of days mine 2 operates,\n        and   be the number of days mine 3 operates.\n        Since mine 1 produces 25 tons of copper each day,\n        in   days mine 1 will produce   tons of copper.\n        Mine 2 produces 30 tons of copper each day,\n        so in   days mine 2 will produce   tons of copper.\n        Also, mine 3 produces 20 tons of copper each day,\n        so in   days mine 3 will produce   tons of copper.\n        Since the company needs to supply a total of 550 tons of copper,\n        we need to have  .\n        Similar analyses of silver and manganese give us the system\n         \n       \n        To solve the system,\n        we eliminate the variable   from the second and third equations by replacing equation \n        two with equation two minus 24 times equation one and replacing equation three with equation \n        three minus   times equation one.\n        This produces the equvalent system\n         \n       \n        We are fortunate now that we can determine the value of   from the third equation,\n        which tells us that  .\n        Substituting into the second equation shows that\n         .\n       \n        Substituting into the first equation allows us to determine the value for  :\n         .\n       \n        So the company should run mine 1 for 6 days,\n        mine 2 for 10 days, and mine 3 for 5 days to meet this demand.\n       "
},
{
  "id": "exercise-1",
  "level": "2",
  "url": "chap_intro_linear_systems.html#exercise-1",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        In the method of elimination there are three operations we can apply to solve a system of \n        linear equations.\n        For this exercise we focus on a system of equations in three unknowns  ,\n         , and  ,\n        but the arguments generalize to a system with any number of variables.\n        Consider the general system of three equations in three unknowns\n         .\n        The goal of this exercise is to understand why the three operations on a system do not \n        change the solutions to the system.\n        Recall that a solution to a system with unknowns  ,\n         ,\n        and   is a set of three numbers,\n        one for  , one for  ,\n        and one for   that satisfy all of the equations in the system.\n       \n          Explain why, if we have a solution to this system,\n          then that solution is also a solution to any constant   times the second equation.\n         \n          Consider the equations\n           ,\n          and\n           .\n         \n          Explain why, if we have a solution to this system,\n          then that solution is also a solution to the sum of the first equation and   \n          times the third equation for any constant  .\n         \n          Similar to part (a)\n         "
},
{
  "id": "exercise-2",
  "level": "2",
  "url": "chap_intro_linear_systems.html#exercise-2",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Alice stopped by a coffee shop two days in a row at a conference to buy drinks and pastries.\n        On the first day,\n        she bought a cup of coffee and two muffins for which she paid $6.87.\n        The next day she bought two cups of coffee and three muffins\n        (for herself and a friend).\n        Her bill was $11.25.\n        Use the method of elimination to determine the price of a cup of coffee,\n        and the price of a muffin.\n        Clearly explain your set-up for the problem. (Assume you are explaining your solution to\n        someone who has not solved the problem herself\/himself).\n       "
},
{
  "id": "exercise-3",
  "level": "2",
  "url": "chap_intro_linear_systems.html#exercise-3",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Alice stopped by a coffee shop three days in a row at a conference to buy drinks and pastries.\n        On the first day, she bought a cup of coffee,\n        a muffin and a scone for which she paid $6.15.\n        The next day she bought two cups of coffee, three muffins and a scone\n        (for herself and friends).\n        Her bill was $12.20.\n        The last day she bought a cup of coffee,\n        two muffins and two scones, and paid $10.35.\n        Determine the price of a cup of coffee,\n        the price of a muffin and the price of a scone.\n        Clearly explain your set-up for the problem. (Assume you are explaining your solution to \n        someone who has not solved the problem herself\/himself).\n       \n        A cup of coffee costs $ ,\n        a muffin costs $ , and a scone costs $ .\n       "
},
{
  "id": "exercise-4",
  "level": "2",
  "url": "chap_intro_linear_systems.html#exercise-4",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Find an example of a system of two linear equations in variables  ,\n          for each of the following three cases:\n       \n              where the equations correspond to two non-parallel lines,\n             \n              two parallel distinct lines,\n             \n              two identical lines\n              (represented with different equations).\n             \n          Describe how the relationship between the coefficients of the variables of the two \n          equations in parts (ii) and (iii) are different than the relationship between those \n          coefficients in part (i) (Note: Please make sure your system examples are different than \n          the examples in the activities,\n          and that they are your own examples.)\n         "
},
{
  "id": "exercise-5",
  "level": "2",
  "url": "chap_intro_linear_systems.html#exercise-5",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        In a grid of wires in thermal equilibrium,\n        the temperature at interior nodes is the average of the temperatures at adjacent nodes.\n        Consider the grid as shown in  ,\n        with  ,  , and   the temperatures\n        (in degrees Centigrade)\n        at the indicated interior nodes,\n        and fixed temperatures at the other nodes as shown.\n        For example,\n        the nodes adjacent to the node with temperature   have temperatures of  ,\n         ,  , and  ,\n        so when the grid is in thermal equilibrium   is the average of these temperatures:\n         .\n       A grid of wires. \n              Determine equations for the temperatures   and   if the grid is in \n              thermal equilibrium to construct a system of three linear equations in  ,\n               ,\n              and   that models node temperatures in the grid in thermal equilibrium.\n             \n               \n             \n              Use the method of elimination to find a specific solution to the system that makes \n              sense in context.\n             \n               ,   and  .\n             "
},
{
  "id": "exercise-6",
  "level": "2",
  "url": "chap_intro_linear_systems.html#exercise-6",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        We have seen that a linear system of two equations in two unknowns can have no solutions,\n        one solution, or infinitely many solutions.\n        Find, if possible, a specific example of each of the following.\n        If not possible, explain why.\n       \n              A linear system of three equations in two unknowns with no solutions.\n             \n              A linear system of three equations in two unknowns with exactly one solution.\n             \n              A linear system of three equations in two unknowns with exactly two solutions.\n             \n              A linear system of three equations in two unknowns with infinitely many solutions.\n             "
},
{
  "id": "exercise-7",
  "level": "2",
  "url": "chap_intro_linear_systems.html#exercise-7",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        We have seen that a linear system of three equations in three unknowns can have no solutions,\n        one solution, or infinitely many solutions.\n        Find, if possible, a specific example of each of the following.\n        If not possible, explain why.\n       \n              A linear system of two equations in three unknowns with no solutions.\n             \n               \n             \n              A linear system of two equations in three unknowns with exactly one solution.\n             \n              Impossible.\n             \n              A linear system of two equations in three unknowns with exactly two solutions.\n             \n              Impossible.\n             \n              A linear system of two equations in three unknowns with infinitely many solutions.\n             \n               \n             "
},
{
  "id": "exercise-8",
  "level": "2",
  "url": "chap_intro_linear_systems.html#exercise-8",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Find a system of three linear equations in two variables  ,\n          whose solution is  ,  .\n       "
},
{
  "id": "exercise-9",
  "level": "2",
  "url": "chap_intro_linear_systems.html#exercise-9",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        Consider the system of linear equations\n         \n        where   is an unknown constant.\n       \n              Determine the solution(s) of this system for all possible   values,\n              if a solution exists.\n              (Note: Your answers for the variables will depend on the  .)\n             \n              This system has the solution   and\n                as long as  .\n             \n              How do your answers change if the second equation in the system above is changed to \n               ?\n             \n              When  ,\n                and   is free.\n             "
},
{
  "id": "exercise-10",
  "level": "2",
  "url": "chap_intro_linear_systems.html#exercise-10",
  "type": "Exercise",
  "number": "10",
  "title": "",
  "body": "\n        Suppose we are given a system of two linear equations\n         .\n        Find another system of two linear equations   and   in the variables  ,\n         ,\n        and   that are not multiples of each other or of equations   \n        or   so that any solution\n          to the system   and   is a \n        solution to the system   and  .\n       "
},
{
  "id": "sec_intro_le_tf",
  "level": "2",
  "url": "chap_intro_linear_systems.html#sec_intro_le_tf",
  "type": "Exercise",
  "number": "11",
  "title": "True\/False Questions.",
  "body": "True\/False Questions \n    In many sections you will be given True\/False questions.\n    In each of the True\/False questions,\n    you will be given a statement, such as\n     if we add corresponding sides of two linear equations,\n    then the resulting equation is a linear equation \n    and\n     one can find a system of two equations in two unknowns that has infinitely many solutions. \n    Your task will be to determine the truth value of the statement and to give a brief justification \n    for your choice.\n   \n    Note that a  general  statement is considered  true \n    only when it is always true.\n    For example, the first of the above statements  \n     if we add corresponding sides of two linear equations,\n    then the resulting equation is a linear equation \n      is a general statement.\n    For this statement to be true,\n    the equation we obtain by adding corresponding sides of any two linear equations has to be linear.\n    If we can find two equations that do not give a linear equation when combined in this way,\n    then this statement is false.\n   \n    Note that an  existential \n    statement is considered  true \n    if there is at least one example which makes is true.\n    For example, the latter of the above statements  \n     one can find a system of two equations in two unknowns that has infinitely many solutions \n      is an existential statement.\n    For this statement to be true,\n    existence of a system of two equations in two unknowns with infinitely many solutions should suffice.\n    If it is impossible to find two such equations,\n    then this statement is false.\n   \n    To justify that something always happens or never happens,\n    one would need to refer to other statements whose truth is known,\n    such as theorems, definitions.\n    In particular, giving an  example \n    of two linear equations that produce a linear equation when we add corresponding sides\n     does not justify  why the sum of  any \n    two linear equations is also linear.\n    Using the definition of linear equations, however,\n    we can justify why this new equation will always be linear:\n    each side of a linear equation is linear,\n    and adding linear expressions always produces a linear sum.\n   \n    To justify that there are examples of something happening or not happening,\n    one would need to give a specific example.\n    For example,\n    in justifying the claim that there is a system of two equations in two unknowns with infinitely many solutions,\n    it is not enough to say\n     An equation in two unknowns is a line in the  -plane,\n    so there can be two equations with the same line as their solution. \n    In general, you should avoid the words\n     can, \n     possibly, \n     maybe, \n    etc., in your justifications.\n    Instead, giving an example such as\n     The linear system   and   of two equations in two unknowns has \n    infinitely many solutions since the second equation gives the same line as the first in the \n     -plane. \n    provides complete justification beyond a reasonable doubt.\n   \n    Each response to a True\/False statement should be more than just True or False.\n    It is important that you provide\n     justification  for your responses.\n   True\/False \n              The set of all solutions of a linear equation can be represented graphically as a line.\n             \n              F\n             True\/False \n              The set of all solutions of a linear equation in two variables can be represented graphically as a line.\n             True\/False \n              The set of all solutions of an equation in two variables can be represented graphically as a line.\n             \n              F\n             True\/False \n              A system of three linear equations in two unknowns cannot have a unique solution.\n             True\/False \n              A system of three linear equations in three unknowns has a unique solution.\n             \n              F\n             "
},
{
  "id": "project-1",
  "level": "2",
  "url": "chap_intro_linear_systems.html#project-1",
  "type": "Project Activity",
  "number": "1.5",
  "title": "",
  "body": "\n      Apply Kirchoff's Current Law to the point   to obtain an equation in  ,\n       , and  .\n      What do you notice?\n     "
},
{
  "id": "project-2",
  "level": "2",
  "url": "chap_intro_linear_systems.html#project-2",
  "type": "Project Activity",
  "number": "1.6",
  "title": "",
  "body": "\n      Apply Kirchoff's Voltage Law to the bottom loop in the circuit in  \n      to obtain an equation in  ,\n       , and  .\n      Compare the three equations we have found to those in the introduction.\n     "
},
{
  "id": "F_Wheatstone",
  "level": "2",
  "url": "chap_intro_linear_systems.html#F_Wheatstone",
  "type": "Figure",
  "number": "1.11",
  "title": "",
  "body": "A Wheatstone bridge circuit. "
},
{
  "id": "project-3",
  "level": "2",
  "url": "chap_intro_linear_systems.html#project-3",
  "type": "Project Activity",
  "number": "1.7",
  "title": "",
  "body": "\n      Consider the circuit as shown in  ,\n      with a single source and five resistors with resistances  ,\n       ,  ,  ,\n      and   as labeled.\n     \n            Assume the following information.\n            The voltage   is   volts,\n             ,\n            and  .\n            Follow the directions given to find the currents  ,\n             ,  ,  ,\n             , and  .\n           \n                  Use Kirchoff's Current Law to show that  ,\n                   , and  .\n                  Thus, we reduce the problem to three variables.\n                 \n                  Apply Kirchoff's Voltage Law to three loops to show that the currents must satisfy \n                  the linear system\n                   \n                 \n                  Solve the system to find the unknown currents.\n                 \n            The circuit pictured in  \n            is called a  Wheatstone bridge \n            (invented by Samuel Hunter Christie in 1833 and popularized by Sir Charles Wheatstone in 1843).\n            The Wheatstone bridge is a circuit designed to determine an unknown resistance by balancing two paths in a circuit.\n            It is set up so that the resistances of resistors   and   are known,\n              is a variable resistor and we want to find the resistance of  .\n            The resistor   is replaced with a voltmeter,\n            and the resistance of   is varied until the voltmeter reads  .\n            This balances the circuit and tells the resistance of resistor  .\n            Show that if the current   in   is  \n            (so the circuit is balanced),\n            then  \n            (which is how we calculate the unknown resistance  ).\n            Do this in general and do not use any specific values for the resistances or the voltage.\n           "
},
{
  "id": "chap_matrix_representation",
  "level": "1",
  "url": "chap_matrix_representation.html",
  "type": "Section",
  "number": "2",
  "title": "The Matrix Representation of a Linear System",
  "body": "The Matrix Representation of a Linear System \n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is a matrix?\n           \n         \n           \n            How do we associate a matrix to a system of linear equations?\n           \n         \n           \n            What row operations can we perform on an augmented matrix of a linear system to solve the system of linear equations?\n           \n         \n           \n            What are pivots, basic variables, and free variables?\n           \n         \n           \n            How many solutions can a system of linear equations have?\n           \n         \n           \n            When is a linear system consistent?\n           \n         \n           \n            When does a linear system have infinitely many solutions?\n            A unique solution?\n           \n         \n           \n            How can we represent the set of solutions to a consistent system if the system has infinitely many solutions?\n           \n         Application: Approximating Area Under a Curve \n    We know from basic geometry how to find areas of circles and triangles.\n    However, it is much more difficult to find areas of other geometric objects.\n    In fact, it is generally an impossible problem to determine the exact area bounded by a \n    complicated curve.\n    For this reason, approximation methods are used.\n    One such method involves approximating curves using quadratic functions.\n   \n    Unless you have learned some calculus,\n    you have probably never calculated the area under a parabola.\n    In the ancient work  Quadrature of the Parabola \n    (3rd century BC), Archimedes determined a method for finding the area of a region bounded by a parabola by using mechanics and then by geometric methods.\n    Once we know how to calculate the area of a region bounded by a parabola, Simpson's Rule uses parabolas to approximate a function,\n    and then approximates the area under the graph of the graph of the function by using the areas under the parabolas.\n    In order to use Simpsons Rule,\n    we need to know how to exactly fit a quadratic function to three points.\n    More details about this process can be found at the end of this section.\n    This idea of fitting a polynomial to a set of data points has uses in other areas as well.\n    For example,\n    two common applications of Bzier curves are font design and drawing tools.\n    When fitting a polynomial to a large set of data points,\n    our systems of equations can become quite large,\n    and can be difficult to solve by hand.\n    In this section we will see how to use matrices to more conveniently represent systems of equations of any size.\n    We also consider how the elimination process works on the matrix representation of a linear system and how we can determine the existence of solutions and the form of solutions of a linear system.\n   Introduction matrix matrix matrix \n    We usually delineate a matrix by enclosing its entries in square brackets  .\n    For the system in  , there are two corresponding matrices:\n   coefficient matrix augmented matrix Terminology matrix entry matrix row matrix column matrix size entry row column size \n          Write the augmented matrix for the following linear system.\n          If needed, rearrange an equation to ensure that the variables appear in the same order on \n          the left side in each equation with the constants being on the right hand side of each equation.\n           \n         \n          Write the linear system in variables   and  ,\n          appearing in the natural order that corresponds to the following augmented matrix.\n          Then solve the linear system using the elimination method.\n           \n         \n          Consider the three types of elementary operations on systems of equations introduced in \n           .\n          Each row of an augmented matrix of a system corresponds to an equation,\n          so each elementary operation on equations corresponds to an operation on rows\n          (called row operations).\n         \n            Describe the row operation that corresponds to interchanging two equations.\n           \n            Describe the row operation that corresponds to multiplying an equation by a nonzero scalar.\n           \n            Describe the row operation that corresponds to replacing one equation by the sum of that \n            equation and a scalar multiple of another equation.\n           Simplifying Linear Systems Represented in Matrix Form \n    Once we have stored the information about a linear system in an augmented matrix,\n    we can perform the elementary operations directly on the augmented matrix.\n   \n    Recall that the allowable operations on a system of equations are the following:\n     \n         \n          Replacing one equation by the sum of that equation and a scalar multiple of another equation.\n         \n       \n         \n          Interchanging the positions of two equations.\n         \n       \n         \n          Replacing an equation by a nonzero scalar multiple of itself.\n         \n       \n   row operations elementary row operations \n      Consider the system\n       \n      with corresponding augmented matrix\n       \n     \n            As a first step in solving our system,\n            we might eliminate   from the second equation.\n            This means that the corresponding entry in the second row and first column of the \n            augmented matrix will become 0.\n            Find a row operation that adds a multiple of the first row to the second row to achieve \n            this goal.\n            Then write the system of equations that corresponds to this new augmented matrix.\n           \n            Now that we have eliminated the   terms from the second equation,\n            we eliminate the   term from the third equation.\n            Find an appropriate row operation that does that,\n            and write the corresponding system of linear equations that corresponds to the new \n            augmented matrix.\n           \n            Now you should have a system in which the last two rows correspond to a system of 2 \n            linear equations in two unknowns.\n            Use a row operation that adds a multiple of the second row to the third row to turn the \n            coefficient of   in the third row to 0.\n            Then write the corresponding system of linear equations.\n           row echelon form back-substitution \n    Do you see how this standard elimination process can be generalized to any linear system with \n    any number of variables to produce a simplified system?\n    Do you see why the process does not change the solutions of the system?\n    If needed, can you modify the standard elimination process to obtain a simplified system in which \n    the last equation contains only the variable  ,\n    the next to last equation contains only the variables  ,\n    etc.? Understanding the standard process will enable you to be able to modify it,\n    if needed, in a problem.\n   \n     \n    illustrates how we can perform all of the operations on equations with operations on the rows of \n    augmented matrices to reduce a system to a solvable form.\n    Each time we perform an operation on the system of equations\n    (or on the rows of an augmented matrix)\n    we obtain an equivalent system\n    (or an augmented matrix corresponding to an equivalent system).\n    For completeness,\n    we list the operations on equations and the corresponding row operations below that can be used \n    to solve our polynomial fitting system.\n    Throughout the process we will let  ,  ,\n    and   be the first, second,\n    and third equations in the system and  ,  ,\n    and   the first, second,\n    and third rows of the augmented matrices.\n    The notation   placed next to equation   means means that we replace the \n    second equation in the system with the sum of the first two equations.\n    We start with the system\n     \n   \n    On the left we demonstrate the operations on equations and on the right the corresponding \n    operations on rows of the augmented matrix.\n   \n     \n         \n       \n       \n       \n         \n       \n\n       \n       \n\n     \n         \n       \n       \n       \n         \n       \n\n       \n       \n\n     \n         \n       \n       \n       \n         \n       \n\n       \n       \n   \n    Now we can solve the last equation for   to find that  .\n    The second equation gives us  . \n    If there had been an   term in the second equation,\n    we could have substituted   and solved for  \n      Finally,\n    using the first equation with the already determined values of   and   gives \n    us  .\n    Thus we have found the solution to the polynomial fitting system to be  ,\n     , and  .\n   \n    We summarize the steps of the (partial) elimination on matrices we used above to solve a general \n    linear system in the variables  ,\n     ,  ,  .\n\n     \n         \n          Interchange equations if needed to ensure that the coefficient of   (or,\n          more generally,\n          the first non-zero variable) in the first equation is non-zero.\n         \n       \n         \n          Use the first equation to eliminate   (or,\n          the first non-zero variable) from other equations by adding a multiple of the first equation to the others.\n         \n       \n         \n          After   is eliminated from all equations but the first equation,\n          focus on the rest of the equations.\n          Repeat the process of elimination on these equations to eliminate   (or,\n          the next non-zero variable) all but the second equation.\n         \n       \n         \n          Once the process of eliminating variables recursively is finished,\n          solve for the variables in a backwards fashion starting with the last equation and substituting known values in the equations above as they become known.\n         \n       \n   forward elimination phase back substitution row echelon form Linear Systems with Infinitely Many Solutions \n    Each of the systems that we solved so far have had a unique\n    (exactly one)\n    solution.\n    The geometric representation of linear systems with two equations in two variables shows that \n    this does not always have to be the case.\n    We also have linear systems with no solution and systems with infinitely many solutions.\n    We now consider the problem of how to represent the set of solutions of a linear system that has \n    infinitely many solutions.\n    (Systems with infinitely many solutions will also be of special interest to us a bit later when \n    we study eigenspaces of a matrix.)\n   \n      Consider the system\n       \n     \n            Without explicitly solving the system,\n            check that   and\n              are solutions to this system.\n           parametric solution parameter  \n            Part (b) shows that our system has infinitely many solutions.\n            We were given solutions in part (b)   but how do we find these solutions and how \n            do we know that these are all of the solutions?\n            We address those questions now.\n            If we apply row operations to the augmented matrix\n             \n            of this system,\n            we can reduce this system to one with augmented matrix\n             .\n           \n                  What is it about this reduced form of the augmented matrix that indicates that the \n                  system has infinitely many solutions?\n                 \n                  Since the system has infinitely many solutions,\n                  we will not be able to explicitly determine values for each of the variables.\n                  Instead, at least one of the variables can be chosen arbitrarily.\n                  What is it about the reduced form of the augmented matrix that indicates that \n                    is convenient to choose as the arbitrary variable?\n                 \n                  Letting   be arbitrary\n                  (we call   a  free  variable),\n                  use the second row to show that  \n                  (so that we can write   in terms of the arbitrary variable  ).\n                 \n                  Use the first row to show that  \n                  (and we can write   in terms of the arbitrary variable  ).\n                  Compare this to the solutions from part (b).\n                 pivot basic variable free pivot basic variable free variable pivot basic variable free variable \n      Each matrix is an augmented matrix for a linear system after elimination.\n      Identify the basic variables\n      (if any)\n      and free variables\n      (if any).\n      Then write the general solution\n      (if there is a solution)\n      expressing all variables in terms of the free variables.\n      Use any symbols you like for the variables.\n       \n             \n           \n             \n           \n             \n           \n    Does the existence of a row of 0's always mean a free variable?\n    Can you think of an example where there is a row of 0's but none of the variables is free?\n    How do the numbers of equations and the variables compare in that case?\n   Linear Systems with No Solutions \n    We saw in the previous section that geometrically two parallel and distinct lines represent a \n    linear system with two equations in two unknowns which has no solution.\n    Similarly, two parallel and distinct planes in three dimensions represent a linear system with \n    two equations in three unknowns which has no solution.\n    We can have at least four different geometric configurations of three planes in three dimensions \n    representing a system with no solution.\n    But how do these geometrical configurations manifest themselves algebraically?\n   \n      Consider the linear system\n       \n     \n            Apply the elimination process to the augmented matrix of this system.\n            Write the system of equations that corresponds to the final reduced matrix.\n           \n            Discuss which feature in the final simplified system makes it easy to determine that the system has no solution.\n            Similarly, what features in the matrix representation makes is easy to see the system has no solution?\n           \n    We summarize our observations about when a system has a solution,\n    and which of those cases has a unique solution.\n   \n        A linear system is consistent if after the elimination process there is no equation of the \n        form   where   is a non-zero number.\n        If a linear system is consistent and has a free variable,\n        then it has infinitely many solutions.\n        If it is consistent and has no free variables,\n        then there is a unique solution.\n       Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Consider the linear system\n         \n       \n              Set up the augmented matrix for this linear system.\n             \n              The augmented matrix for this system is\n               .\n            \n             \n              Find all solutions to the system using forward elimination.\n             \n              We apply forward elimination,\n              first making the entries below the 1 in the upper left all 0.\n              We do this by replacing row two with row two minus 2 times row 1, row three with row \n              three minus row 1, and row four with row four minus 4 row one.\n              This produces the augmented matrix\n               .\n              Now we eliminate the leading 5 in the fourth row by replacing row four with row four minus row two to obtain the augmented matrix\n               .\n              When we replace row four with row four minus row three,\n              we wind up with a row of zeros:\n               .\n              We see that there is no pivot in column four,\n              so   is a free variable.\n              We can solve for the other variables in terms of  .\n              The third row shows us that\n               .\n              The second row tells us that\n               .\n              Finally, the first row gives us\n               .\n              So this system has infinitely many solutions,\n              with  ,  ,\n               , and   is arbitrary.\n              As a check, notice that\n               \n              and so this solution satisfies the first equation in our system.\n              You should check to verify that it also satisfies the other three equations.\n             \n              Suppose, after forward elimination,\n              the augmented matrix of the system\n               \n              has the form\n               .\n              For which values of   does this system have:\n             \n                    No solutions?\n                   \n                    The system has no solutions when there is an equation of the form   for \n                    some nonzero number  .\n                    The last row will correspond to an equation of the form  .\n                    So our system will have no solutions when  .\n                   \n                    A unique solution?\n                    Find the solution.\n                   \n                    When  , the system has no solutions.\n                    When  ,\n                    the variable   is a free variable and the system has infinitely many \n                    solutions.\n                    So there are no values of   for which the system has exactly one solution.\n                   \n                    Infinitely many solution?\n                    Determine all solutions?\n                   \n                    When  ,\n                    the variable   is a free variable and the system has infinitely many \n                    solutions.\n                    The solutions were already found in part (a).\n                   \n        After applying row operations to the augmented matrix of a system of linear equations,\n        each of which describes a plane in 3-space,\n        the following augmented matrix was obtained:\n         .\n       \n              Describe, algebraically and geometrically, all solutions\n              (if any),\n              to this system when   and  .\n             \n              Throughout,\n              we will let the variables  ,\n               , and   correspond to the first, second,\n              and third columns, respectively, of our augmented matrix.\n             \n              When   and   our augmented matrix has the form\n               .\n              This matrix corresponds to the system\n               \n              There are no equations of the form   for a nonzero constant  ,\n              so the system is consistent.\n              There are no free variables, so the system has a unique solution.\n              Algebraically, the solution is  ,\n               , and  .\n              Geometrically,\n              this tells us that the three planes given by the original system intersect in a single point.\n             \n              Describe, algebraically and geometrically, all solutions\n              (if any),\n              to this system when   and  .\n             \n              Throughout,\n              we will let the variables  ,\n               , and   correspond to the first, second,\n              and third columns, respectively, of our augmented matrix.\n             \n              When   and   our augmented matrix has the form\n               .\n              The last row corresponds to the equation  ,\n              so our system is inconsistent and has no solution.\n              Geometrically,\n              this tells us that the three planes given by the original system do not all intersect at any common points.\n             \n              Describe, algebraically and geometrically, all solutions\n              (if any),\n              to this system when   and  .\n             \n              Throughout,\n              we will let the variables  ,\n               , and   correspond to the first, second,\n              and third columns, respectively, of our augmented matrix.\n             \n              When   and   our augmented matrix reduces to\n               .\n              There are no rows that correspond to equations of the form   for a nonzero constant  ,\n              so the system is consistent.\n              The variable   is a free variable,\n              so the system has infinitely many solutions.\n              Algebraically, the solutions are   is free,\n              is  , and  .\n              Geometrically,\n              this tells us that the three planes given by the original system intersect in the line with  ,\n              and  .\n             Summary \n       \n        A matrix is just a rectangular array of numbers or objects.\n       \n     \n       \n        Given a system of linear equations,\n        with the variables listed in the same order in each equation,\n        we represent the system by writing the coefficients of the first equation as the first row \n        of a matrix,\n        the coefficients of the second equation as the second row,\n        and so on.\n        This creates the coefficient matrix of the system.\n        We then augment the coefficient matrix with a column of the constants that appear in the \n        equations.\n        This gives us the augmented matrix of the system.\n       \n     \n       \n        The operations that we can perform on equations translate exactly to row operations that we \n        can perform on an augmented matrix:\n         \n             \n              Replacing one row by the sum of that row and a scalar multiple of another row.\n             \n           \n             \n              Interchanging two rows.\n             \n           \n             \n              Replacing a row by a nonzero scalar multiple of itself.\n             \n           \n       \n     \n       \n        The forward elimination phase of the elimination method recursively eliminates the variables \n        in a linear system to reach an equivalent but simplified system.\n       \n     \n       \n        The first non-zero entry in an equation in a linear system after elimination is called a pivot.\n       \n     \n       \n        A basic variable in a linear system corresponds to a pivot of the system.\n        A free variable is a variable that is not basic.\n       \n     \n       \n        A linear system can be inconsistent\n        (no solutions),\n        have a unique solution\n        (if consistent and every variable is a basic variable),\n        or have infinitely many solutions\n        (if consistent and there is a free variable).\n       \n     \n       \n        A linear system has no solutions if, after elimination,\n        there is an equation of the form   where   is a nonzero number.\n       \n     \n       \n        A linear system after the elimination method can be solved using back-substitution.\n        The free variables can be chosen arbitrarily and the basic variables can be solved in \n        terms of the free variables through the back-substitution process.\n       \n     Exercises \n        Consider the system of linear equations whose augmented matrix is\n         \n        where   and   are unknown constants.\n        For which values of   and   does this system have\n       \n              a unique solution,\n             \n               \n             \n              infinitely many solutions,\n             \n                and  .\n             \n              no solution?\n             \n                and  .\n             \n        Consider the following system:\n         \n        Check that when   the system has infinitely many solutions,\n        while when   the system has a unique solution.\n       \n        If possible, find a system of three equations\n        (not in reduced form)\n        in three variables whose solution set consists only of the point  .\n       \n         \n       \n        What are the possible geometrical descriptions of the solution set of two linear \n        equations in  ? (Recall that   is the three-dimensional \n         -space   that is,\n        the set of all ordered triples of the form  ).\n       \n        Two students are talking about when a linear system has infinitely many solutions.\n         \n          Student 1: So, if we have a linear system whose augmented matrix has a row of zeros, then the system has infinitely many solutions, doesn't it?\n         \n         \n          Student 2: Well, but what if there is a row of the form   with a non-zero   right above the row of 0's?\n         \n         \n          Student 1: OK, maybe I should ask\n           If we have a consistent linear system whose augmented matrix has a row of zeros, then the system has infinitely many solutions, doesn't it? \n         \n          Student 2: I don't know. It still doesn't sound enough to me, but I'm not sure why.\n         \n        Is Student 1 right?\n        Or is Student 2's hunch correct?\n        Justify your answer with a specific example if possible.\n       \n        Student 2's hunch is correct.\n       \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              A system of linear equations in two unknowns can have exactly five solutions.\n             \n              F\n             True\/False \n              A system of equations with all the right hand sides equal to 0 has at least one solution.\n             True\/False \n              A system of equations where there are fewer equations than the number of unknowns\n              (known as an underdetermined system)\n              cannot have a unique solution.\n             \n              T\n             True\/False \n              A system of equations where there are more equations than the number of unknowns\n              (known as an overdetermined system)\n              cannot have a unique solution.\n             True\/False \n              A consistent system of two equations in three unknowns cannot have a unique solution.\n             \n              T\n             True\/False \n              If a system with three equations and three unknowns has a solution,\n              then the solution is unique.\n             True\/False \n              If a system of equations has two different solutions,\n              then it has infinitely many solutions.\n             \n              F\n             True\/False \n              If there is a row of zeros in the row echelon form of the augmented matrix of a system of equations,\n              the system has infinitely many solutions.\n             True\/False \n              If there is a row of zeros in the row echelon form of the augmented matrix of a system of   equations in   variables,\n              the system has infinitely many solutions.\n             \n              F\n             True\/False \n              If a system has no free variables,\n              then the system has a unique solution.\n             True\/False \n              If a system has a free variable,\n              then the system has infinitely many solutions.\n             \n              F\n             Project: Polynomial Interpolation to Approximate the Area Under a Curve \n    Suppose we want to approximate the area of the region shown in  .\n    As discussed in the introduction,\n    we can approximate the area under a curve by approximating the curve by quadratics.\n    First, we will see how Archimedes approached the problem of finding the area of a quadratic region,\n    then we will determine how to determine a quadratic function that passes through three points,\n    then we put it all together to approximate the area under a curve as in  .\n   A region whose area we want to approximate. \n    Archimedes approached the problem of calculating the area of a quadratic region in the following way.\n    Given a quadratic   on an interval  , Archimedes drew in a base given by the \n    secant line connecting the points   and\n      as illustrated at left in  .\n    Then he found the point in the interval   at which the tangent line to the curve is \n    parallel to the secant line.\n    Archimedes gave an argument using mechanics\n    (based on balance points),\n    and then another using geometry\n    (through a method of exhaustion)\n    to show that the area of the parabolic region is\n      times the area of the triangle determined by the endpoints and the point of tangency as shown at right in  .\n   Archimedes method. \n    Although we won't go through the details,\n    a conclusion we can draw from Archimedes argument,\n    using the formula for the area of a rectangle and the area of a triangle,\n    is that the area between the graph of a quadratic with equation\n      and the  -axis on an interval\n      as illustrated in   is\n     .\n   Region between a parabola and the  -axis. \n    To approximate the area under the graph of a function,\n    we will approximate the function itself with a collection of quadratics,\n    and then use equation   repeatedly.\n    To do this, we need to know how to fit a quadratic curve to a three points.\n    We consider that question now.\n   polynomial curve fitting \n    As an example, we use the points  ,\n     ,  .\n    To fit a quadratic to these points,\n    consider a general quadratic of the form  .\n    By substituting the   value of each of the given points and setting that equal to the   value of that point,\n    we find three equations\n     \n    that give us a system of three equations in the three unknowns  ,\n     , and  :\n     .\n   \n    This system is the example we considered in  ,\n    whose solution is  ,\n     , and  .\n    A graph of   along with the three points  ,\n     ,\n      is shown in  .\n   A quadratic fit to the points  ,  ,  . \n    Now that we know how to fit a quadratic to three points,\n    we next approximate a curve with a collection of quadratics.\n    The method we use is to break the interval on which our curve is defined into several subintervals and create quadratics on each subinterval.\n    The basic idea is contained in our first project activity.\n   \n      In this activity we model the function   defined by\n        on the interval  ,\n      where   and   with a collection of quadratics.\n      Let  .\n      We divide the interval   into three subintervals using the six points \n       ,\n       ,\n       ,  ,\n       ,\n       , and  .\n      We need three points to determine a quadratic,\n      so the three subintervals of the interval   will be the intervals  ,\n       , and  .\n      An illustration of the process of dividing our interval   and approximating by \n      quadratics can be found at  . \n      Round all calculations in this activity to the nearest thousandth.\n     \n            Set up a system of linear equations to fit a quadratic\n              to the three points  ,\n             , and  .\n            (The solution to this system to the nearest thousandth is  ,\n             , and  .)\n           \n            Set up a system of linear equations to fit a quadratic\n              to the three points  ,\n             , and  .\n            (The solution to this system to the nearest thousandth is  ,\n             , and  .)\n           \n            Set up a system of linear equations to fit a quadratic\n              to the 3 points  ,\n             , and  .\n            (The solution to this system to the nearest thousandth is  ,\n             , and  .)\n           \n            Use the GeoGebra applet at   to graph the three quadratics on their intervals on the same axes as the graph of  .\n            Explain what you see.\n           \n     \n    illustrates how we can model a function on an interval using a sequence of quadratic functions.\n    Now we apply this polynomial curve fitting technique to derive the general formula for approximating the area between a graph of a function   and the  -axis.\n    We use parabolic arcs to approximate the graph of   on each subinterval.\n   \n    We start by dividing the interval   over which our function is defined into some number of subintervals. We need an even number of subintervals, since we have to use three points to define each parabola. Let   be the number of subintervals we use. In order to make the calculations a bit easier, let the subintervals all have the same length, which we denote by   (the symbol   is often used in mathematics to indicate a change in a quantity). Since we have   subintervals, the length of each subinterval will be  . For each   we let   and  . Note that   and  . This labeling scheme is illustrated in  \n   Subdividing the interval  . \n    We approximate   on each subinterval using a quadratic.\n    So we need to find the quadratic\n      that passes through two consecutive end points as well as the midpoint of a subinterval.\n    That is, we need to find the coefficients of   so that   passes through the points  ,\n     ,\n    and the midpoint   on the interval  \n    (so that we have three points to which to fit a parabola)\n    as shown at left in  .\n    Note that the length of the interval   is  .\n    To make the calculations easier,\n    we will translate our function so that our leftmost point is  .\n    Then the middle point is   and the rightmost point is\n      as illustrated at right in  ,\n    where  .\n   Left: Three points. Right: Translated points. \n            Set up a linear system that will determine the coefficients  ,\n             ,\n            and   so that the polynomial\n              passes through the points  ,\n             , and   with  .\n            Remember that the unknowns in this system are  ,\n             , and  .\n           \n            Explain why the coefficient matrix of the system in   is  .\n            Then explain why row reducing the matrix\n              will find the coefficients we want.\n            Assume that a row echelon form of the matrix   is\n             .\n            Use these matrices to explain why  ,\n             , and  .\n           \n            Our goal is to ultimately approximate the area under the curve on the interval   by approximating   with quadratics on each subinterval.\n            Use the Archimedean formula   to show that the area under the quadratic   from part (a) is\n             .\n           \n            Now add up all of the area approximations on each subinterval to show that the approximate area under the graph is given by the formula\n             .\n           \n    We conclude with an example.\n   \n      Let   on the interval  .\n      A graph of   is shown in  .\n      Use our approximation formula with   to approximate the area of the shaded region in  .\n      Show all of your work and round all calculations to the nearest thousandth.\n     "
},
{
  "id": "objectives-2",
  "level": "2",
  "url": "chap_matrix_representation.html#objectives-2",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is a matrix?\n           \n         \n           \n            How do we associate a matrix to a system of linear equations?\n           \n         \n           \n            What row operations can we perform on an augmented matrix of a linear system to solve the system of linear equations?\n           \n         \n           \n            What are pivots, basic variables, and free variables?\n           \n         \n           \n            How many solutions can a system of linear equations have?\n           \n         \n           \n            When is a linear system consistent?\n           \n         \n           \n            When does a linear system have infinitely many solutions?\n            A unique solution?\n           \n         \n           \n            How can we represent the set of solutions to a consistent system if the system has infinitely many solutions?\n           \n         "
},
{
  "id": "p-206",
  "level": "2",
  "url": "chap_matrix_representation.html#p-206",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "matrix "
},
{
  "id": "definition-4",
  "level": "2",
  "url": "chap_matrix_representation.html#definition-4",
  "type": "Definition",
  "number": "2.1",
  "title": "",
  "body": "matrix matrix "
},
{
  "id": "p-209",
  "level": "2",
  "url": "chap_matrix_representation.html#p-209",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "coefficient matrix augmented matrix "
},
{
  "id": "p-210",
  "level": "2",
  "url": "chap_matrix_representation.html#p-210",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "entry row column size "
},
{
  "id": "pa_1_b",
  "level": "2",
  "url": "chap_matrix_representation.html#pa_1_b",
  "type": "Preview Activity",
  "number": "2.1",
  "title": "",
  "body": "\n          Write the augmented matrix for the following linear system.\n          If needed, rearrange an equation to ensure that the variables appear in the same order on \n          the left side in each equation with the constants being on the right hand side of each equation.\n           \n         \n          Write the linear system in variables   and  ,\n          appearing in the natural order that corresponds to the following augmented matrix.\n          Then solve the linear system using the elimination method.\n           \n         \n          Consider the three types of elementary operations on systems of equations introduced in \n           .\n          Each row of an augmented matrix of a system corresponds to an equation,\n          so each elementary operation on equations corresponds to an operation on rows\n          (called row operations).\n         \n            Describe the row operation that corresponds to interchanging two equations.\n           \n            Describe the row operation that corresponds to multiplying an equation by a nonzero scalar.\n           \n            Describe the row operation that corresponds to replacing one equation by the sum of that \n            equation and a scalar multiple of another equation.\n           "
},
{
  "id": "p-226",
  "level": "2",
  "url": "chap_matrix_representation.html#p-226",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "row operations elementary row operations "
},
{
  "id": "act_A1_2_1",
  "level": "2",
  "url": "chap_matrix_representation.html#act_A1_2_1",
  "type": "Activity",
  "number": "2.2",
  "title": "",
  "body": "\n      Consider the system\n       \n      with corresponding augmented matrix\n       \n     \n            As a first step in solving our system,\n            we might eliminate   from the second equation.\n            This means that the corresponding entry in the second row and first column of the \n            augmented matrix will become 0.\n            Find a row operation that adds a multiple of the first row to the second row to achieve \n            this goal.\n            Then write the system of equations that corresponds to this new augmented matrix.\n           \n            Now that we have eliminated the   terms from the second equation,\n            we eliminate the   term from the third equation.\n            Find an appropriate row operation that does that,\n            and write the corresponding system of linear equations that corresponds to the new \n            augmented matrix.\n           \n            Now you should have a system in which the last two rows correspond to a system of 2 \n            linear equations in two unknowns.\n            Use a row operation that adds a multiple of the second row to the third row to turn the \n            coefficient of   in the third row to 0.\n            Then write the corresponding system of linear equations.\n           row echelon form back-substitution "
},
{
  "id": "remark-1",
  "level": "2",
  "url": "chap_matrix_representation.html#remark-1",
  "type": "Reflection",
  "number": "2.2",
  "title": "",
  "body": "\n    Do you see how this standard elimination process can be generalized to any linear system with \n    any number of variables to produce a simplified system?\n    Do you see why the process does not change the solutions of the system?\n    If needed, can you modify the standard elimination process to obtain a simplified system in which \n    the last equation contains only the variable  ,\n    the next to last equation contains only the variables  ,\n    etc.? Understanding the standard process will enable you to be able to modify it,\n    if needed, in a problem.\n   "
},
{
  "id": "p-256",
  "level": "2",
  "url": "chap_matrix_representation.html#p-256",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "forward elimination phase back substitution row echelon form "
},
{
  "id": "act_A1_2_3",
  "level": "2",
  "url": "chap_matrix_representation.html#act_A1_2_3",
  "type": "Activity",
  "number": "2.3",
  "title": "",
  "body": "\n      Consider the system\n       \n     \n            Without explicitly solving the system,\n            check that   and\n              are solutions to this system.\n           parametric solution parameter  \n            Part (b) shows that our system has infinitely many solutions.\n            We were given solutions in part (b)   but how do we find these solutions and how \n            do we know that these are all of the solutions?\n            We address those questions now.\n            If we apply row operations to the augmented matrix\n             \n            of this system,\n            we can reduce this system to one with augmented matrix\n             .\n           \n                  What is it about this reduced form of the augmented matrix that indicates that the \n                  system has infinitely many solutions?\n                 \n                  Since the system has infinitely many solutions,\n                  we will not be able to explicitly determine values for each of the variables.\n                  Instead, at least one of the variables can be chosen arbitrarily.\n                  What is it about the reduced form of the augmented matrix that indicates that \n                    is convenient to choose as the arbitrary variable?\n                 \n                  Letting   be arbitrary\n                  (we call   a  free  variable),\n                  use the second row to show that  \n                  (so that we can write   in terms of the arbitrary variable  ).\n                 \n                  Use the first row to show that  \n                  (and we can write   in terms of the arbitrary variable  ).\n                  Compare this to the solutions from part (b).\n                 "
},
{
  "id": "p-266",
  "level": "2",
  "url": "chap_matrix_representation.html#p-266",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "pivot basic variable free "
},
{
  "id": "definition-5",
  "level": "2",
  "url": "chap_matrix_representation.html#definition-5",
  "type": "Definition",
  "number": "2.3",
  "title": "",
  "body": "pivot basic variable free variable pivot basic variable free variable "
},
{
  "id": "act_A1_2_4",
  "level": "2",
  "url": "chap_matrix_representation.html#act_A1_2_4",
  "type": "Activity",
  "number": "2.4",
  "title": "",
  "body": "\n      Each matrix is an augmented matrix for a linear system after elimination.\n      Identify the basic variables\n      (if any)\n      and free variables\n      (if any).\n      Then write the general solution\n      (if there is a solution)\n      expressing all variables in terms of the free variables.\n      Use any symbols you like for the variables.\n       \n             \n           \n             \n           \n             \n           "
},
{
  "id": "remark-2",
  "level": "2",
  "url": "chap_matrix_representation.html#remark-2",
  "type": "Reflection",
  "number": "2.4",
  "title": "",
  "body": "\n    Does the existence of a row of 0's always mean a free variable?\n    Can you think of an example where there is a row of 0's but none of the variables is free?\n    How do the numbers of equations and the variables compare in that case?\n   "
},
{
  "id": "activity-7",
  "level": "2",
  "url": "chap_matrix_representation.html#activity-7",
  "type": "Activity",
  "number": "2.5",
  "title": "",
  "body": "\n      Consider the linear system\n       \n     \n            Apply the elimination process to the augmented matrix of this system.\n            Write the system of equations that corresponds to the final reduced matrix.\n           \n            Discuss which feature in the final simplified system makes it easy to determine that the system has no solution.\n            Similarly, what features in the matrix representation makes is easy to see the system has no solution?\n           "
},
{
  "id": "theorem-2",
  "level": "2",
  "url": "chap_matrix_representation.html#theorem-2",
  "type": "Theorem",
  "number": "2.5",
  "title": "",
  "body": "\n        A linear system is consistent if after the elimination process there is no equation of the \n        form   where   is a non-zero number.\n        If a linear system is consistent and has a free variable,\n        then it has infinitely many solutions.\n        If it is consistent and has no free variables,\n        then there is a unique solution.\n       "
},
{
  "id": "example-3",
  "level": "2",
  "url": "chap_matrix_representation.html#example-3",
  "type": "Example",
  "number": "2.6",
  "title": "",
  "body": "\n        Consider the linear system\n         \n       \n              Set up the augmented matrix for this linear system.\n             \n              The augmented matrix for this system is\n               .\n            \n             \n              Find all solutions to the system using forward elimination.\n             \n              We apply forward elimination,\n              first making the entries below the 1 in the upper left all 0.\n              We do this by replacing row two with row two minus 2 times row 1, row three with row \n              three minus row 1, and row four with row four minus 4 row one.\n              This produces the augmented matrix\n               .\n              Now we eliminate the leading 5 in the fourth row by replacing row four with row four minus row two to obtain the augmented matrix\n               .\n              When we replace row four with row four minus row three,\n              we wind up with a row of zeros:\n               .\n              We see that there is no pivot in column four,\n              so   is a free variable.\n              We can solve for the other variables in terms of  .\n              The third row shows us that\n               .\n              The second row tells us that\n               .\n              Finally, the first row gives us\n               .\n              So this system has infinitely many solutions,\n              with  ,  ,\n               , and   is arbitrary.\n              As a check, notice that\n               \n              and so this solution satisfies the first equation in our system.\n              You should check to verify that it also satisfies the other three equations.\n             \n              Suppose, after forward elimination,\n              the augmented matrix of the system\n               \n              has the form\n               .\n              For which values of   does this system have:\n             \n                    No solutions?\n                   \n                    The system has no solutions when there is an equation of the form   for \n                    some nonzero number  .\n                    The last row will correspond to an equation of the form  .\n                    So our system will have no solutions when  .\n                   \n                    A unique solution?\n                    Find the solution.\n                   \n                    When  , the system has no solutions.\n                    When  ,\n                    the variable   is a free variable and the system has infinitely many \n                    solutions.\n                    So there are no values of   for which the system has exactly one solution.\n                   \n                    Infinitely many solution?\n                    Determine all solutions?\n                   \n                    When  ,\n                    the variable   is a free variable and the system has infinitely many \n                    solutions.\n                    The solutions were already found in part (a).\n                   "
},
{
  "id": "example-4",
  "level": "2",
  "url": "chap_matrix_representation.html#example-4",
  "type": "Example",
  "number": "2.7",
  "title": "",
  "body": "\n        After applying row operations to the augmented matrix of a system of linear equations,\n        each of which describes a plane in 3-space,\n        the following augmented matrix was obtained:\n         .\n       \n              Describe, algebraically and geometrically, all solutions\n              (if any),\n              to this system when   and  .\n             \n              Throughout,\n              we will let the variables  ,\n               , and   correspond to the first, second,\n              and third columns, respectively, of our augmented matrix.\n             \n              When   and   our augmented matrix has the form\n               .\n              This matrix corresponds to the system\n               \n              There are no equations of the form   for a nonzero constant  ,\n              so the system is consistent.\n              There are no free variables, so the system has a unique solution.\n              Algebraically, the solution is  ,\n               , and  .\n              Geometrically,\n              this tells us that the three planes given by the original system intersect in a single point.\n             \n              Describe, algebraically and geometrically, all solutions\n              (if any),\n              to this system when   and  .\n             \n              Throughout,\n              we will let the variables  ,\n               , and   correspond to the first, second,\n              and third columns, respectively, of our augmented matrix.\n             \n              When   and   our augmented matrix has the form\n               .\n              The last row corresponds to the equation  ,\n              so our system is inconsistent and has no solution.\n              Geometrically,\n              this tells us that the three planes given by the original system do not all intersect at any common points.\n             \n              Describe, algebraically and geometrically, all solutions\n              (if any),\n              to this system when   and  .\n             \n              Throughout,\n              we will let the variables  ,\n               , and   correspond to the first, second,\n              and third columns, respectively, of our augmented matrix.\n             \n              When   and   our augmented matrix reduces to\n               .\n              There are no rows that correspond to equations of the form   for a nonzero constant  ,\n              so the system is consistent.\n              The variable   is a free variable,\n              so the system has infinitely many solutions.\n              Algebraically, the solutions are   is free,\n              is  , and  .\n              Geometrically,\n              this tells us that the three planes given by the original system intersect in the line with  ,\n              and  .\n             "
},
{
  "id": "exercise-12",
  "level": "2",
  "url": "chap_matrix_representation.html#exercise-12",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Consider the system of linear equations whose augmented matrix is\n         \n        where   and   are unknown constants.\n        For which values of   and   does this system have\n       \n              a unique solution,\n             \n               \n             \n              infinitely many solutions,\n             \n                and  .\n             \n              no solution?\n             \n                and  .\n             "
},
{
  "id": "exercise-13",
  "level": "2",
  "url": "chap_matrix_representation.html#exercise-13",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Consider the following system:\n         \n        Check that when   the system has infinitely many solutions,\n        while when   the system has a unique solution.\n       "
},
{
  "id": "exercise-14",
  "level": "2",
  "url": "chap_matrix_representation.html#exercise-14",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        If possible, find a system of three equations\n        (not in reduced form)\n        in three variables whose solution set consists only of the point  .\n       \n         \n       "
},
{
  "id": "exercise-15",
  "level": "2",
  "url": "chap_matrix_representation.html#exercise-15",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        What are the possible geometrical descriptions of the solution set of two linear \n        equations in  ? (Recall that   is the three-dimensional \n         -space   that is,\n        the set of all ordered triples of the form  ).\n       "
},
{
  "id": "exercise-16",
  "level": "2",
  "url": "chap_matrix_representation.html#exercise-16",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Two students are talking about when a linear system has infinitely many solutions.\n         \n          Student 1: So, if we have a linear system whose augmented matrix has a row of zeros, then the system has infinitely many solutions, doesn't it?\n         \n         \n          Student 2: Well, but what if there is a row of the form   with a non-zero   right above the row of 0's?\n         \n         \n          Student 1: OK, maybe I should ask\n           If we have a consistent linear system whose augmented matrix has a row of zeros, then the system has infinitely many solutions, doesn't it? \n         \n          Student 2: I don't know. It still doesn't sound enough to me, but I'm not sure why.\n         \n        Is Student 1 right?\n        Or is Student 2's hunch correct?\n        Justify your answer with a specific example if possible.\n       \n        Student 2's hunch is correct.\n       "
},
{
  "id": "exercise-17",
  "level": "2",
  "url": "chap_matrix_representation.html#exercise-17",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              A system of linear equations in two unknowns can have exactly five solutions.\n             \n              F\n             True\/False \n              A system of equations with all the right hand sides equal to 0 has at least one solution.\n             True\/False \n              A system of equations where there are fewer equations than the number of unknowns\n              (known as an underdetermined system)\n              cannot have a unique solution.\n             \n              T\n             True\/False \n              A system of equations where there are more equations than the number of unknowns\n              (known as an overdetermined system)\n              cannot have a unique solution.\n             True\/False \n              A consistent system of two equations in three unknowns cannot have a unique solution.\n             \n              T\n             True\/False \n              If a system with three equations and three unknowns has a solution,\n              then the solution is unique.\n             True\/False \n              If a system of equations has two different solutions,\n              then it has infinitely many solutions.\n             \n              F\n             True\/False \n              If there is a row of zeros in the row echelon form of the augmented matrix of a system of equations,\n              the system has infinitely many solutions.\n             True\/False \n              If there is a row of zeros in the row echelon form of the augmented matrix of a system of   equations in   variables,\n              the system has infinitely many solutions.\n             \n              F\n             True\/False \n              If a system has no free variables,\n              then the system has a unique solution.\n             True\/False \n              If a system has a free variable,\n              then the system has infinitely many solutions.\n             \n              F\n             "
},
{
  "id": "F_1_b_area",
  "level": "2",
  "url": "chap_matrix_representation.html#F_1_b_area",
  "type": "Figure",
  "number": "2.8",
  "title": "",
  "body": "A region whose area we want to approximate. "
},
{
  "id": "F_1_b_Archimedes",
  "level": "2",
  "url": "chap_matrix_representation.html#F_1_b_Archimedes",
  "type": "Figure",
  "number": "2.9",
  "title": "",
  "body": "Archimedes method. "
},
{
  "id": "F_1_b_quad_area",
  "level": "2",
  "url": "chap_matrix_representation.html#F_1_b_quad_area",
  "type": "Figure",
  "number": "2.10",
  "title": "",
  "body": "Region between a parabola and the  -axis. "
},
{
  "id": "p-349",
  "level": "2",
  "url": "chap_matrix_representation.html#p-349",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "polynomial curve fitting "
},
{
  "id": "F_1_b_quadratic_fit",
  "level": "2",
  "url": "chap_matrix_representation.html#F_1_b_quadratic_fit",
  "type": "Figure",
  "number": "2.11",
  "title": "",
  "body": "A quadratic fit to the points  ,  ,  . "
},
{
  "id": "act_1_b_Simpson_ex_1",
  "level": "2",
  "url": "chap_matrix_representation.html#act_1_b_Simpson_ex_1",
  "type": "Project Activity",
  "number": "2.6",
  "title": "",
  "body": "\n      In this activity we model the function   defined by\n        on the interval  ,\n      where   and   with a collection of quadratics.\n      Let  .\n      We divide the interval   into three subintervals using the six points \n       ,\n       ,\n       ,  ,\n       ,\n       , and  .\n      We need three points to determine a quadratic,\n      so the three subintervals of the interval   will be the intervals  ,\n       , and  .\n      An illustration of the process of dividing our interval   and approximating by \n      quadratics can be found at  . \n      Round all calculations in this activity to the nearest thousandth.\n     \n            Set up a system of linear equations to fit a quadratic\n              to the three points  ,\n             , and  .\n            (The solution to this system to the nearest thousandth is  ,\n             , and  .)\n           \n            Set up a system of linear equations to fit a quadratic\n              to the three points  ,\n             , and  .\n            (The solution to this system to the nearest thousandth is  ,\n             , and  .)\n           \n            Set up a system of linear equations to fit a quadratic\n              to the 3 points  ,\n             , and  .\n            (The solution to this system to the nearest thousandth is  ,\n             , and  .)\n           \n            Use the GeoGebra applet at   to graph the three quadratics on their intervals on the same axes as the graph of  .\n            Explain what you see.\n           "
},
{
  "id": "F_1_b_partition",
  "level": "2",
  "url": "chap_matrix_representation.html#F_1_b_partition",
  "type": "Figure",
  "number": "2.12",
  "title": "",
  "body": "Subdividing the interval  . "
},
{
  "id": "F_1_b_quad_fit",
  "level": "2",
  "url": "chap_matrix_representation.html#F_1_b_quad_fit",
  "type": "Figure",
  "number": "2.13",
  "title": "",
  "body": "Left: Three points. Right: Translated points. "
},
{
  "id": "project-5",
  "level": "2",
  "url": "chap_matrix_representation.html#project-5",
  "type": "Project Activity",
  "number": "2.7",
  "title": "",
  "body": "\n            Set up a linear system that will determine the coefficients  ,\n             ,\n            and   so that the polynomial\n              passes through the points  ,\n             , and   with  .\n            Remember that the unknowns in this system are  ,\n             , and  .\n           \n            Explain why the coefficient matrix of the system in   is  .\n            Then explain why row reducing the matrix\n              will find the coefficients we want.\n            Assume that a row echelon form of the matrix   is\n             .\n            Use these matrices to explain why  ,\n             , and  .\n           \n            Our goal is to ultimately approximate the area under the curve on the interval   by approximating   with quadratics on each subinterval.\n            Use the Archimedean formula   to show that the area under the quadratic   from part (a) is\n             .\n           \n            Now add up all of the area approximations on each subinterval to show that the approximate area under the graph is given by the formula\n             .\n           "
},
{
  "id": "project-6",
  "level": "2",
  "url": "chap_matrix_representation.html#project-6",
  "type": "Project Activity",
  "number": "2.8",
  "title": "",
  "body": "\n      Let   on the interval  .\n      A graph of   is shown in  .\n      Use our approximation formula with   to approximate the area of the shaded region in  .\n      Show all of your work and round all calculations to the nearest thousandth.\n     "
},
{
  "id": "chap_row_echelon_forms",
  "level": "1",
  "url": "chap_row_echelon_forms.html",
  "type": "Section",
  "number": "3",
  "title": "Row Echelon Forms",
  "body": "Row Echelon Forms \n      By the end of this section, you should be able to give precise and thorough answers to the\n      questions listed below. You may want to keep these questions in mind to focus your thoughts\n      as you complete the section.\n       \n           \n            What is the row echelon form of a matrix?\n           \n         \n           \n            What is the procedure to obtain the row echelon form of any matrix?\n           \n         \n           \n            What is the reduced row echelon form of a matrix?\n           \n         \n           \n            What is the procedure to obtain the reduced row echelon form of any matrix?\n           \n         \n           \n            What do the echelon forms of the augmented matrix for a linear system tell us about the solutions to the system?\n           \n         Application: Balancing Chemical Reactions \n    Linear systems have applications in chemistry when balancing chemical equations.\n    When a chemical reaction occurs,\n    molecules of different substances combine to create molecules of other substances.\n    Chemists represent such reactions with chemical equations.\n    To balance a chemical equation means to find the number of atoms of each element involved that \n    will preserve the number of atoms in the reaction.\n    As an example, consider the chemical equation\n     .\n   reactants products diatomic Introduction \n    In the previous sections,\n    we identified operations on a given linear system with corresponding equivalent operations on \n    the matrix representations which simplify the system and its matrix representation without \n    changing the solutions of the system.\n    Our end goal was to obtain a system which could be solved using back substitution, such as\n     \n   \n    The augmented matrix for this system is\n     .\n   row echelon form echelon form \n      We want to determine a suitable form for an augmented matrix that can be obtained from row \n      operations so that it is straightforward to find the solutions to the system.\n      We begin with some examples.\n     \n          Write the linear system corresponding to each of the following augmented matrices.\n          Use the linear system to determine which systems have their variables eliminated completely \n          in the forward direction,\n          or equivalently determine for which systems the next step in the solution process is back \n          substitution\n          (possibly using free variables).\n          Explain your reasoning.\n          You do not need to solve the systems.\n         \n               \n             \n               \n             \n               \n             \n               \n             \n          Shown below are two row reduced forms of the system\n           \n          Of the systems that correspond to these augmented matrices,\n          which is easier to solve and why?\n           \n         The Echelon Forms of a Matrix \n    In the previous sections we saw how to simplify a linear system and its matrix representation via \n    the elimination method without changing the solution set.\n    This process is more efficient when performed on the matrix representation rather than on the \n    system itself.\n    Furthermore,\n    the process of applying row operations to any augmented matrix is one that can be automated.\n    In order to write an algorithm that can be used with any size augmented matrix to the extent \n    that it can be applied even by a computer program,\n    it is necessary to have a consistent procedure and a stopping point for the simplification process.\n    The two main properties that we want the simplified augmented matrix to satisfy are that it should \n    be easy to see if the system has solutions from the simplified matrix,\n    and in cases when there are solutions,\n    the general form of the solutions can be easily found.\n    Hence the topic of this section is to define the process of elimination completely and generally.\n   row echelon echelon row echelon echelon row echelon form pivot row echelon form echelon form pivot leading entry pivot positions pivot columns \n    Compare the row echelon form of an augmented matrix to the corresponding system.\n    Do you clearly see the correspondence between the requirements of the row echelon form and the\n     properly eliminated variables in the system?\n    Can you quickly come up with a system which will be in row echelon form when represented in \n    augmented matrix form?\n    Can you modify the standard row echelon form definition to cover cases where the elimination \n    process eliminates the variables from last to first?\n    For example,\n    in a system with three equations in three unknowns,\n    the last variable, say  ,\n    can be eliminated from the second equation,\n    and the last two variables,\n    say   can be eliminated from the last equation.\n    How would you define this modified row echelon form for a general system with this modified \n    elimination process?\n   \n    Once an augmented matrix is in row echelon form,\n    we can use back substitution to solve the corresponding system.\n    However, we can make solving much easier with just a little more elimination work.\n   \n    Row operations are easy to apply, so if we are inclined,\n    there is no reason to stop at the row echelon form.\n    For example, starting with the following matrix\n     \n    in row echelon form,\n    we could take the row operations even farther and avoid the process of back substitution altogether.\n    First, we multiply the last row by   to simplify that row:\n     .\n\n    Then we use the third row to eliminate entries above the third pivot:\n     .\n   backward elimination reduced row echelon row echelon form reduced reduced row echelon form reduced echelon form \n    In short, the reduced row echelon form of a matrix is a row echelon form in which all the \n    pivots are 1 and any entries\n     below and above  the pivots are 0.\n   \n    If we use either of these two row echelon forms,\n    solving the original system becomes straightforward and,\n    as a result,\n    these matrix forms are stopping points for the row operation algorithm to solve a system.\n    It is also very easy to write a computer program to perform row operations to obtain and row \n    echelon or reduced row echelon form of the matrix,\n    making hand computations unnecessary.\n    We will discuss this shortly.\n   \n    Compare the reduced row echelon form of an augmented matrix to the corresponding system.\n    Do you clearly see the correspondence between the requirements of the reduced row echelon form \n    and the way the variables appear in the equations in the system?\n    Can you quickly come up with a system which will be in reduced row echelon form when represented \n    in augmented matrix form?\n   Note \n      We have used the elimination method on augmented matrices so far.\n      However, the elimination method can be applied on just the coefficient matrix,\n      or other matrices that will arise in other contexts,\n      and will provide useful information in each of those cases.\n      Therefore, the row echelon form and reduced row echelon form is defined for\n       any matrix , and from now on,\n      a matrix will be a general matrix unless explicitly specified to be an augmented matrix.\n     \n      Identify which of the following matrices is in row echelon form (REF) and\/or reduced row \n      echelon form (RREF).\n      For those in row and\/or reduced row echelon form,\n      identify the pivots clearly by circling them.\n      For those that are not in a given form,\n      state which properties the matrix fails to satisfy.\n     \n             \n           \n             \n           \n             \n           \n             \n           \n             \n           Determining the Number of Solutions of a Linear System \n    Consider the system\n     \n   \n    The augmented matrix for this system is\n     .\n   \n    Note that this matrix is already in row echelon form.\n    The reduced row echelon form of this augmented matrix is\n     .\n   \n    Since there are leading 1s in the first three columns,\n    we can use those entries to write  ,  ,\n    and   in terms of  .\n    We then choose   to be arbitrary and write the remaining variables in terms of  .\n    Let  .\n    Solving the third equation for   gives us  .\n    The second equation shows that  ,\n    and the first that  .\n    Each value of   provides a solution to the system,\n    so our system has infinitely many solutions.\n    These solutions are\n     ,\n    where   can have any value.\n   \n      We have seen examples of systems with no solutions,\n      one solution, and infinitely many solutions.\n      As we will see in this activity,\n      we can recognize the number of solutions to a system by analyzing the pivot positions in the \n      augmented matrix of the system.\n     \n            Write an example of an augmented matrix in row echelon form so that the last column \n            of the (whole) matrix is a pivot column.\n            What is the system of equations corresponding to your augmented matrix?\n            How many solutions does your system have?\n            Why?\n           \n            Consider the reduced row echelon form  .\n            Based on the columns of this matrix,\n            explain how we know that the system it represents is consistent.\n           \n            The system with reduced row echelon form   is consistent.\n            What is it about the columns of the coefficient matrix that tells us that this system has \n            infinitely many solutions?\n           \n            Suppose that a linear system is consistent and that the coefficient matrix has \n              rows and   columns.\n           \n                  If every column of the coefficient matrix is a pivot column,\n                  how many solutions must the system have?\n                  Why?\n                  What relationship must exist between   and  ?\n                  Explain.\n                 \n                  If the coefficient matrix has at least one non-pivot column,\n                  how many solutions must the system have?\n                  Why?\n                 \n    When solving a linear system of equations,\n    the free variables can be chosen arbitrarily and we can write the basic variables in terms of \n    the free variables.\n    Therefore, the existence of a free variable leads to infinitely many solutions for consistent systems.\n    However, it is possible to have a system with free variables which is inconsistent. \n    (Can you think of an example?)\n   Producing the Echelon Forms \n    In this part,\n    we consider the formal process of creating the row and reduced row echelon forms of matrices.\n    The process of creating the row echelon form is the equivalent of the elimination method on \n    systems of linear equations.\n   \n      Each of the following matrices is at most a few steps away from being in the requested echelon form.\n      Determine what row operations need to be completed to turn the matrix into the required form.\n     \n            Turn into REF:  \n           \n            Turn into REF:  \n           \n            Turn into RREF:  \n           \n            Turn into RREF:  \n           \n            Turn into RREF:  \n           \n            Turn into RREF:  \n           \n    The complete process of applying row operations to reduce an augmented matrix to a row or reduced \n    row echelon form can be expressed as a recursive process in an algorithmic fashion,\n    making it possible to program computers to solve linear systems.\n    Here are the steps to do so:\n     \n         Step 1 \n         \n          Begin with the leftmost nonzero column\n          (if there is one).\n          This will be a pivot column.\n         \n       \n         Step 2 \n         \n          Select a nonzero entry in this pivot column as a pivot.\n          If necessary,\n          interchange rows to move this entry to the first row\n          (this entry will be a pivot).\n         \n       \n         Step 3 \n         \n          Use row operations to create zeros in all positions below the pivot.\n         \n       \n         Step 4 \n         \n          Cover (or ignore) the row containing the pivot position and cover all rows, if any,\n          above it.\n          Apply steps 1-3 to the submatrix that remains.\n          Repeat the process until there are no more nonzero rows to modify.\n         \n         \n          To obtain the reduced row echelon form we need one more step.\n         \n       \n         Step 5 \n         \n          Beginning with the rightmost pivot and working upward and to the left,\n          create zeros above each pivot.\n          If a pivot is not 1, make it 1 by an appropriate row multiplication.\n         \n       \n   Gaussian elimination Gauss-Jordan elimination \n      Consider the matrix \n       .\n     \n            Perform Gaussian elimination to reduce the matrix to row echelon form.\n            Clearly identify each step used.\n            Compare your row echelon form to that of another group.\n            Do your results agree?\n            If not, who is right?\n           \n            Now continue applying row operations to obtain the reduced row echelon form of the matrix.\n            Clearly identify each step.\n            Compare your row echelon form to that of another group.\n            Do your results agree?\n            If not, who is right?\n           \n    If we compare row echelon forms from  ,\n    it is likely that different groups or individuals produced different row echelon forms.\n    That is because the row echelon form of a matrix is not unique. (Is the row echelon form ever unique?)\n   \n    However, if row operations are applied correctly,\n    then we will all arrive at the same reduced row echelon form in  :\n     .\n\n    It turns out that the reduced row echelon form of a matrix is unique.\n   row equivalent row equivalent matrices row equivalent \n    Since every elementary row operation is reversible,\n    if   is row equivalent to  ,\n    then   is also row equivalent to  .\n    Thus, we just say that   and   are row equivalent.\n    While the row echelon form of a matrix is not unique,\n    it is the case that the reduced row echelon form of a matrix is unique.\n   \n        Every matrix is row equivalent to a unique matrix in reduced row echelon form.\n       \n    The reduced row echelon form of a matrix that corresponds to a system of linear equations \n    provides us with an equivalent system whose solutions are easy to find.\n    As an example, consider the system\n     \n    with augmented matrix\n     .\n\n    Notice that the coefficient matrix\n    (the left hand side portion of the augmented matrix)\n    of this system is same as the matrix we considered in  .\n    Since we are augmenting with a column of zeros,\n    no row operations will change those zeros in the augmented column.\n    So the row operations applied in  \n    will give us the reduced row echelon form of this augmented matrix as\n     .\n   \n    Note that the third column is not a pivot column.\n    That means that the variable   is a free variable.\n    There are pivots in the other three columns of the coefficient matrix,\n    so we can solve for  ,  ,\n    and   in terms of  .\n    These variables are the basic variables.\n    The third row of the augmented matrix tells us that  .\n    The second row corresponds to the equation  ,\n    and solving for   shows that  .\n    Finally, the first row tells us that\n     , so  .\n    Therefore, the general solution to this system of equations is\n     .\n   \n    The fact that   is free means that we can choose any value for   that we \n    like and obtain a specific solution to the system.\n    For example, if  ,\n    then we have the solution  ,\n     ,  , and  .\n    Check this to be sure.\n   \n      Each matrix below is an augmented matrix for a linear system after elimination with \n      variables   in that order.\n      Identify the basic variables\n      (if any)\n      and free variables\n      (if any).\n      Then find the general solution\n      (if there is a solution)\n      expressing all variables in terms of the free variables.\n     \n             \n           \n             \n           \n             \n           \n             \n           \n             \n           \n    Recall that in the previous section,\n    we determined the criteria for when a system has a unique solution,\n    or infinitely many solutions, or no solution.\n    With the use of the row echelon form of the augmented matrix,\n    we can rewrite these criteria as follows:\n   \n         \n             \n              A linear system is consistent if in the row echelon form of the augmented matrix \n              representing the system no pivot is in the rightmost column.\n             \n           \n             \n              If a linear system is consistent and the row echelon form of the coefficient matrix \n              does not have a pivot in every column,\n              then the system has infinitely many solutions.\n             \n           \n             \n              If a linear system is consistent and there is a pivot in every column of the row \n              echelon form of the coefficient matrix,\n              then the system has a unique solution.\n             \n           \n       Figures for  . \n            For each part,\n            the reduced row echelon form of the augmented matrix of a system of equations in \n            variables  ,\n             ,\n            and  \n            (in that order)\n            is given.\n            Use the reduced row echelon form to find the solution set to the original system of \n            equations.\n           \n                    \n                 \n                   \n                 \n                     \n                 \n                  Each of the three systems above is represented as one of the graphs in  .\n                  Match each figure with a system.\n                 \n            The reduced row echelon form of the augmented matrix of a system of equations in \n            variables  ,\n             ,  , and  \n            (in that order)\n            is given.\n            Use the reduced row echelon form to find the solution set to the original system of equations:\n             .\n           Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Consider the linear system\n         .\n       \n              Find the augmented matrix for this system.\n             \n              Before we can find the augmented matrix of this system,\n              we need to rewrite the system so that the variables are all on one side and the constant \n              terms are on the other side of the equations.\n              Doing so yields the equivalent system\n               \n             \n              Note that this is not the only way to rearrange the system.\n              For example, for the second equation,\n              could be written instead as\n                to minimize the number of negative signs in the equation.\n             \n              The augmented matrix for this system is\n               .\n             \n              Use row operations to find a row echelon form of the augmented matrix of this system.\n             \n              Our first steps to row echelon form are to eliminate the entries below the leading entry in the first row.\n              To do this we replace row two with row two plus 2 times row 1 and we replace row three with row three plus row one.\n              This produces the row equivalent matrix\n               .\n              This matrix is now in row echelon form.\n             \n              Use row operations to find the reduced row echelon form of the augmented matrix of \n              this system.\n             \n              To continue to find the reduced row echelon form,\n              we replace row two with row two times\n                to get a leading 1 in the second row,\n              and we replace row three with row three times\n                to get a leading 1 in the third row and obtain the row equivalent matrix\n               .\n              Now we perform backwards elimination to make the entries above the leading  s \n              equal to  ,\n              starting with the third column and working backwards.\n              Replace row one with row one minus   times row three and replace row two with \n              row two plus\n                row three to obtain the row equivalent matrix\n               .\n              For the second column,\n              we replace row one with row one plus row two to obtain the row equivalent matrix\n               .\n              Since the leading entry in row one is not a one,\n              we have one more step before we have the reduced row echelon form.\n              Finally, we replace row one with row one times  .\n              This gives us the reduced row echelon form\n               .\n             \n              Find the solution(s), if any, to the system.\n             \n              We can read off the solution to the system from the reduced row echelon form:\n               ,  , and  .\n              You should check in the original equations to make sure we have the correct solution.\n             \n        In this example,   and   are unknown scalars.\n        Consider the system with augmented matrix\n         .\n      \n      \n        Find all values of   and   so that the system has:\n       \n              Exactly one solution (and find the solution)\n             \n              Let  ,  ,\n              and   be the variables corresponding to the first, second,\n              and third columns, respectively, of the augmented matrix.\n              To answer these questions, we row reduce the augmented matrix.\n              We interchange rows one and two and then also rows two and three to obtain the matrix\n               .\n             \n              Now we replace row three with row three minus row one to produce the row equivalent matrix\n               .\n             \n              Next, replace row three with row three minus   times row two.\n              This yields the row equivalent matrix\n               .\n             \n              We now have a row echelon form.\n             \n              The system will have exactly one solution when the last row has the form\n                where   is not zero.\n              Thus, the system has exactly one solution when\n               , or when  .\n              In this case, the solution is\n               .\n              You should check to ensure that this solution is correct.\n              The other cases occur when  .\n             \n              No solutions\n             \n              When   and  \n              (or  ),\n              then we have a row of the form  ,\n              where   is not  .\n              In these cases there are no solutions.\n             \n              Infinitely many solutions (and find all solutions)\n             \n              When   and  ,\n              then the last row is a row of all zeros.\n              In this case,\n              the system is consistent and   is a free variable,\n              so the system has infinitely many solutions.\n              The solutions are\n               \n              You should check to ensure that this solution is correct.\n             Summary \n    In this section we learned about the row echelon and reduced row echelon forms of a matrix and \n    some of the things these forms tell us about solutions to systems of linear equations.\n   \n       \n        A matrix is in row echelon form if\n       \n       \n           \n            All nonzero rows are above any rows of all zeros.\n           \n         \n           pivot \n         \n     \n       \n        Once an augmented matrix is in row echelon form,\n        we can use back substitution to solve the corresponding linear system.\n       \n     \n       \n        To reduce a matrix to row echelon form we do the following:\n       \n       \n           \n            Begin with the leftmost nonzero column\n            (if there is one).\n            This will be a pivot column.\n           \n         \n           \n            Select a nonzero entry in this pivot column as a pivot.\n            If necessary,\n            interchange rows to move this entry to the first row\n            (this entry will be a pivot).\n           \n         \n           \n            Use row operations to create zeros in all positions below the pivot.\n           \n         \n           \n            Cover (or ignore) the row containing the pivot position and cover all rows, if any,\n            above it.\n            Apply the preceding steps to the submatrix that remains.\n            Repeat the process until there are no more nonzero rows to modify.\n           \n         \n     \n       \n        A matrix is in reduced row echelon form if it is in row echelon form and\n         \n             \n              The pivot in each nonzero row is 1.\n             \n           \n             \n              Each pivot is the only nonzero entry in its column.\n             \n           \n       \n     \n       \n        To obtain the reduced row echelon form from the row echelon form,\n        beginning with the rightmost pivot and working upward and to the left,\n        create zeros above each pivot.\n        If a pivot is not 1, make it 1 by an appropriate row multiplication.\n       \n     \n       \n        Both row echelon forms of an augmented matrix tell us about the number of solutions to the \n        corresponding linear system.\n         \n             \n              A linear system is inconsistent if and only if a row echelon form of the augmented matrix of the system contains a row of the form\n               ,\n              where   is not zero.\n              Another way to say this is that a linear system is inconsistent if and only if the last \n              column of the augmented matrix of the system is a pivot column.\n             \n           \n             \n              A consistent linear system will have a unique solution if and only if each column but \n              the last in the augmented matrix of the system is a pivot column.\n              This is equivalent to saying that a consistent linear system will have a unique \n              solution if and only if the consistent system has no free variables.\n             \n           \n             \n              A consistent linear system will have infinitely many solutions if and only if the \n              coefficient matrix of the system contains a non-pivot column.\n              In that case,\n              the free variables corresponding to the non-pivot columns can be chosen arbitrarily \n              and the basic variables corresponding to pivot columns can be written in terms of the\n              free variables.\n             \n           \n             \n              A linear system can have no solutions,\n              exactly one solution, or infinitely many solutions.\n             \n           \n       \n     \n        Represent the following linear system in variables\n          in augmented matrix form and use row reduction to find the general solution of the system.\n         \n       \n        Augmented matrix:\n \n         ,\n        solution  ,  , and  \n       \n        Represent the following linear system in variables\n          in augmented matrix form after rearranging the terms and use row reduction to find all solutions to the system.\n         .\n       \n        Check that the reduced row echelon form of the matrix\n         \n        is\n         .\n       \n        Row operations show this.\n       \n        Consider the following system:\n         \n       \n              Find a row echelon form of the augmented matrix for this system.\n             \n              For which values of  , if any,\n              does the system have (i.) no solutions, (ii.) exactly one solution, (iii.) infinitely many solutions?\n              Find the solutions in each case.\n             \n        Find the general solution of the linear system corresponding to the following augmented matrix:\n         .\n       \n         ,\n         ,   is free,  \n       \n        What are the conditions, if any, on the\n          values so that the following augmented matrix corresponds to a consistent linear system?\n        How many solutions will the consistent system have?\n        Explain.\n         .\n       \n        In this exercise the symbol\n          denotes a non-zero number and the symbol * denotes any real number\n        (including  ).\n       \n              Is the augmented matrix\n               \n              in a form to which back substitution will easily give the solutions to the system?\n              Explain your reasoning.\n              \n             \n              In order to help see what happens in the general case,\n              substitute some numbers in place of the\n               's and *'s and answer the question for that specific system first.\n              Then determine if your answer generalizes.\n             \n              Yes\n             \n              The above matrix is a possible form of an augmented matrix with 2 rows and 3 columns corresponding to a linear system after forward elimination,\n              i.e., a linear system for which back substitution will easily give the solutions.\n              Determine the other possible such forms of the nonzero augmented matrices with 2 rows and 3 columns.\n              As in part (a), use the symbol\n                to denote a non-zero number and * to denote any real number.\n             \n               ,\n               \n             \n        Give an example of a linear system with a unique solution for which a row echelon form of the augmented matrix of the system has a row of 0's.\n       \n        Come up with an example of an augmented matrix with 0's in the rightmost column corresponding to an inconsistent system,\n        if possible.\n        If not, explain why not.\n       \n        This is not possible.\n       \n        Find two different row echelon forms which are equivalent to the same matrix not given in row echelon form.\n       \n        Determine all possible row echelon forms of a   matrix.\n        Use the symbol   to denote a non-zero number and * to denote a real number with no condition on being 0 or not to represent entries.\n       \n          and  .\n       \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              The number of pivots of an\n                matrix cannot exceed  .\n              (Note: Here  ,   are some unknown numbers.)\n             \n              T\n             True\/False \n              The row echelon form of a matrix is unique.\n             True\/False \n              The reduced row echelon form of a matrix is unique.\n             \n              T\n             True\/False \n              A system of equations where there are fewer equations than the number of unknowns\n              (known as an underdetermined system)\n              cannot have a unique solution.\n             True\/False \n              A system of equations where there are more equations than the number of unknowns\n              (known as an overdetermined system)\n              cannot have a unique solution.\n             \n              F\n             True\/False augmented matrix True\/False \n              If the coefficient matrix of a system has pivots in every row,\n              then the system is consistent.\n             \n              T\n             True\/False \n              If there is a row of zeros in a row echelon form of the augmented matrix of a system of equations,\n              the system has infinitely many solutions.\n             True\/False \n              If there is a row of zeros in a row echelon form of the augmented matrix of a system of   equations in   variables,\n              the system has infinitely many solutions.\n             \n              F\n             True\/False \n              If a linear system has no free variables,\n              then the system has a unique solution.\n             True\/False \n              If a linear system has a free variable,\n              then the system has infinitely many solutions.\n             \n              F\n             Project: Modeling a Chemical Reaction reactants products \n    Let   be the number of molecules of  ,\n      the number of molecules of  ,\n      the number of molecules of  ,\n    and   the number of molecules of   in the reaction.\n    We can then represent this reaction as\n     .\n   \n    In each molecule (e.g., ethane  ),\n    the subscripts indicate the number of atoms of each element in the molecule.\n    So 1 molecule of ethane contains 2 atoms of carbon and 6 atoms of hydrogen.\n    Thus, there are 2 atoms of carbon in\n      and 0 atoms of carbon in  ,\n    giving us   carbon atoms in   molecules of\n      and 0 carbon atoms in   molecules of  .\n    On the product side of the reaction there is 1 carbon atom in\n      and 0 carbon atoms in  .\n    To balance the reaction,\n    we know that the number of carbon atoms in the products must equal the number of carbon atoms in \n    the reactants.\n   \n            Set up an equation that balances the number of carbon atoms on both sides of the reaction.\n           \n            Balance the numbers of hydrogen and oxygen atoms in the reaction to explain why\n             .\n           \n            So the system of linear equations that models this chemical reaction is\n             \n            Find all solutions to this system and then balance the reaction.\n            Note that we cannot have a fraction of a molecule in our reaction.\n           \n            Some of the work needed is done in  .\n           \n      Chemical reactions can be very interesting.\n     \n            Carbon dioxide,  ,\n            is a familiar product of combustion.\n            For example, when we burn glucose,\n             ,\n            the products of the reaction are carbon dioxide and water:\n             .\n            Use the techniques developed in this project to balance this reaction.\n           \n            To burn glucose,\n            we need to add oxygen to make the combustion happen.\n            Carbon dioxide is different in that it can burn without the presence of oxygen.\n            For example,\n            when we mix magnesium (Mg) with dry ice ( ),\n            the products are magnesium oxide (MgO) and carbon (C).\n            This is an interesting reaction to watch:\n            you can see it at many websites,  ,\n             \n            or  \n            Use the method determined above to balance the chemical reaction\n             .\n           "
},
{
  "id": "objectives-3",
  "level": "2",
  "url": "chap_row_echelon_forms.html#objectives-3",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n      By the end of this section, you should be able to give precise and thorough answers to the\n      questions listed below. You may want to keep these questions in mind to focus your thoughts\n      as you complete the section.\n       \n           \n            What is the row echelon form of a matrix?\n           \n         \n           \n            What is the procedure to obtain the row echelon form of any matrix?\n           \n         \n           \n            What is the reduced row echelon form of a matrix?\n           \n         \n           \n            What is the procedure to obtain the reduced row echelon form of any matrix?\n           \n         \n           \n            What do the echelon forms of the augmented matrix for a linear system tell us about the solutions to the system?\n           \n         "
},
{
  "id": "p-374",
  "level": "2",
  "url": "chap_row_echelon_forms.html#p-374",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "reactants products diatomic "
},
{
  "id": "p-377",
  "level": "2",
  "url": "chap_row_echelon_forms.html#p-377",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "row echelon form echelon form "
},
{
  "id": "pa_1_c",
  "level": "2",
  "url": "chap_row_echelon_forms.html#pa_1_c",
  "type": "Preview Activity",
  "number": "3.1",
  "title": "",
  "body": "\n      We want to determine a suitable form for an augmented matrix that can be obtained from row \n      operations so that it is straightforward to find the solutions to the system.\n      We begin with some examples.\n     \n          Write the linear system corresponding to each of the following augmented matrices.\n          Use the linear system to determine which systems have their variables eliminated completely \n          in the forward direction,\n          or equivalently determine for which systems the next step in the solution process is back \n          substitution\n          (possibly using free variables).\n          Explain your reasoning.\n          You do not need to solve the systems.\n         \n               \n             \n               \n             \n               \n             \n               \n             \n          Shown below are two row reduced forms of the system\n           \n          Of the systems that correspond to these augmented matrices,\n          which is easier to solve and why?\n           \n         "
},
{
  "id": "p-386",
  "level": "2",
  "url": "chap_row_echelon_forms.html#p-386",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "row echelon echelon row echelon echelon "
},
{
  "id": "definition-6",
  "level": "2",
  "url": "chap_row_echelon_forms.html#definition-6",
  "type": "Definition",
  "number": "3.1",
  "title": "",
  "body": "row echelon form pivot row echelon form echelon form pivot "
},
{
  "id": "p-390",
  "level": "2",
  "url": "chap_row_echelon_forms.html#p-390",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "leading entry pivot positions pivot columns "
},
{
  "id": "remark-3",
  "level": "2",
  "url": "chap_row_echelon_forms.html#remark-3",
  "type": "Reflection",
  "number": "3.2",
  "title": "",
  "body": "\n    Compare the row echelon form of an augmented matrix to the corresponding system.\n    Do you clearly see the correspondence between the requirements of the row echelon form and the\n     properly eliminated variables in the system?\n    Can you quickly come up with a system which will be in row echelon form when represented in \n    augmented matrix form?\n    Can you modify the standard row echelon form definition to cover cases where the elimination \n    process eliminates the variables from last to first?\n    For example,\n    in a system with three equations in three unknowns,\n    the last variable, say  ,\n    can be eliminated from the second equation,\n    and the last two variables,\n    say   can be eliminated from the last equation.\n    How would you define this modified row echelon form for a general system with this modified \n    elimination process?\n   "
},
{
  "id": "p-394",
  "level": "2",
  "url": "chap_row_echelon_forms.html#p-394",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "backward elimination "
},
{
  "id": "p-395",
  "level": "2",
  "url": "chap_row_echelon_forms.html#p-395",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "reduced row echelon "
},
{
  "id": "definition-7",
  "level": "2",
  "url": "chap_row_echelon_forms.html#definition-7",
  "type": "Definition",
  "number": "3.3",
  "title": "",
  "body": "row echelon form reduced reduced row echelon form reduced echelon form "
},
{
  "id": "remark-4",
  "level": "2",
  "url": "chap_row_echelon_forms.html#remark-4",
  "type": "Reflection",
  "number": "3.4",
  "title": "",
  "body": "\n    Compare the reduced row echelon form of an augmented matrix to the corresponding system.\n    Do you clearly see the correspondence between the requirements of the reduced row echelon form \n    and the way the variables appear in the equations in the system?\n    Can you quickly come up with a system which will be in reduced row echelon form when represented \n    in augmented matrix form?\n   "
},
{
  "id": "act_1_c_1",
  "level": "2",
  "url": "chap_row_echelon_forms.html#act_1_c_1",
  "type": "Activity",
  "number": "3.2",
  "title": "",
  "body": "\n      Identify which of the following matrices is in row echelon form (REF) and\/or reduced row \n      echelon form (RREF).\n      For those in row and\/or reduced row echelon form,\n      identify the pivots clearly by circling them.\n      For those that are not in a given form,\n      state which properties the matrix fails to satisfy.\n     \n             \n           \n             \n           \n             \n           \n             \n           \n             \n           "
},
{
  "id": "act_1_c_2",
  "level": "2",
  "url": "chap_row_echelon_forms.html#act_1_c_2",
  "type": "Activity",
  "number": "3.3",
  "title": "",
  "body": "\n      We have seen examples of systems with no solutions,\n      one solution, and infinitely many solutions.\n      As we will see in this activity,\n      we can recognize the number of solutions to a system by analyzing the pivot positions in the \n      augmented matrix of the system.\n     \n            Write an example of an augmented matrix in row echelon form so that the last column \n            of the (whole) matrix is a pivot column.\n            What is the system of equations corresponding to your augmented matrix?\n            How many solutions does your system have?\n            Why?\n           \n            Consider the reduced row echelon form  .\n            Based on the columns of this matrix,\n            explain how we know that the system it represents is consistent.\n           \n            The system with reduced row echelon form   is consistent.\n            What is it about the columns of the coefficient matrix that tells us that this system has \n            infinitely many solutions?\n           \n            Suppose that a linear system is consistent and that the coefficient matrix has \n              rows and   columns.\n           \n                  If every column of the coefficient matrix is a pivot column,\n                  how many solutions must the system have?\n                  Why?\n                  What relationship must exist between   and  ?\n                  Explain.\n                 \n                  If the coefficient matrix has at least one non-pivot column,\n                  how many solutions must the system have?\n                  Why?\n                 "
},
{
  "id": "activity-10",
  "level": "2",
  "url": "chap_row_echelon_forms.html#activity-10",
  "type": "Activity",
  "number": "3.4",
  "title": "",
  "body": "\n      Each of the following matrices is at most a few steps away from being in the requested echelon form.\n      Determine what row operations need to be completed to turn the matrix into the required form.\n     \n            Turn into REF:  \n           \n            Turn into REF:  \n           \n            Turn into RREF:  \n           \n            Turn into RREF:  \n           \n            Turn into RREF:  \n           \n            Turn into RREF:  \n           "
},
{
  "id": "p-436",
  "level": "2",
  "url": "chap_row_echelon_forms.html#p-436",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Gaussian elimination Gauss-Jordan elimination "
},
{
  "id": "act_1_c_3",
  "level": "2",
  "url": "chap_row_echelon_forms.html#act_1_c_3",
  "type": "Activity",
  "number": "3.5",
  "title": "",
  "body": "\n      Consider the matrix \n       .\n     \n            Perform Gaussian elimination to reduce the matrix to row echelon form.\n            Clearly identify each step used.\n            Compare your row echelon form to that of another group.\n            Do your results agree?\n            If not, who is right?\n           \n            Now continue applying row operations to obtain the reduced row echelon form of the matrix.\n            Clearly identify each step.\n            Compare your row echelon form to that of another group.\n            Do your results agree?\n            If not, who is right?\n           "
},
{
  "id": "p-442",
  "level": "2",
  "url": "chap_row_echelon_forms.html#p-442",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "row equivalent "
},
{
  "id": "definition-8",
  "level": "2",
  "url": "chap_row_echelon_forms.html#definition-8",
  "type": "Definition",
  "number": "3.5",
  "title": "",
  "body": "row equivalent matrices row equivalent "
},
{
  "id": "theorem-3",
  "level": "2",
  "url": "chap_row_echelon_forms.html#theorem-3",
  "type": "Theorem",
  "number": "3.6",
  "title": "",
  "body": "\n        Every matrix is row equivalent to a unique matrix in reduced row echelon form.\n       "
},
{
  "id": "activity-12",
  "level": "2",
  "url": "chap_row_echelon_forms.html#activity-12",
  "type": "Activity",
  "number": "3.6",
  "title": "",
  "body": "\n      Each matrix below is an augmented matrix for a linear system after elimination with \n      variables   in that order.\n      Identify the basic variables\n      (if any)\n      and free variables\n      (if any).\n      Then find the general solution\n      (if there is a solution)\n      expressing all variables in terms of the free variables.\n     \n             \n           \n             \n           \n             \n           \n             \n           \n             \n           "
},
{
  "id": "theorem-4",
  "level": "2",
  "url": "chap_row_echelon_forms.html#theorem-4",
  "type": "Theorem",
  "number": "3.7",
  "title": "",
  "body": "\n         \n             \n              A linear system is consistent if in the row echelon form of the augmented matrix \n              representing the system no pivot is in the rightmost column.\n             \n           \n             \n              If a linear system is consistent and the row echelon form of the coefficient matrix \n              does not have a pivot in every column,\n              then the system has infinitely many solutions.\n             \n           \n             \n              If a linear system is consistent and there is a pivot in every column of the row \n              echelon form of the coefficient matrix,\n              then the system has a unique solution.\n             \n           \n       "
},
{
  "id": "F_1_c_1",
  "level": "2",
  "url": "chap_row_echelon_forms.html#F_1_c_1",
  "type": "Figure",
  "number": "3.8",
  "title": "",
  "body": "Figures for  . "
},
{
  "id": "act_1_c_4",
  "level": "2",
  "url": "chap_row_echelon_forms.html#act_1_c_4",
  "type": "Activity",
  "number": "3.7",
  "title": "",
  "body": "\n            For each part,\n            the reduced row echelon form of the augmented matrix of a system of equations in \n            variables  ,\n             ,\n            and  \n            (in that order)\n            is given.\n            Use the reduced row echelon form to find the solution set to the original system of \n            equations.\n           \n                    \n                 \n                   \n                 \n                     \n                 \n                  Each of the three systems above is represented as one of the graphs in  .\n                  Match each figure with a system.\n                 \n            The reduced row echelon form of the augmented matrix of a system of equations in \n            variables  ,\n             ,  , and  \n            (in that order)\n            is given.\n            Use the reduced row echelon form to find the solution set to the original system of equations:\n             .\n           "
},
{
  "id": "example-5",
  "level": "2",
  "url": "chap_row_echelon_forms.html#example-5",
  "type": "Example",
  "number": "3.9",
  "title": "",
  "body": "\n        Consider the linear system\n         .\n       \n              Find the augmented matrix for this system.\n             \n              Before we can find the augmented matrix of this system,\n              we need to rewrite the system so that the variables are all on one side and the constant \n              terms are on the other side of the equations.\n              Doing so yields the equivalent system\n               \n             \n              Note that this is not the only way to rearrange the system.\n              For example, for the second equation,\n              could be written instead as\n                to minimize the number of negative signs in the equation.\n             \n              The augmented matrix for this system is\n               .\n             \n              Use row operations to find a row echelon form of the augmented matrix of this system.\n             \n              Our first steps to row echelon form are to eliminate the entries below the leading entry in the first row.\n              To do this we replace row two with row two plus 2 times row 1 and we replace row three with row three plus row one.\n              This produces the row equivalent matrix\n               .\n              This matrix is now in row echelon form.\n             \n              Use row operations to find the reduced row echelon form of the augmented matrix of \n              this system.\n             \n              To continue to find the reduced row echelon form,\n              we replace row two with row two times\n                to get a leading 1 in the second row,\n              and we replace row three with row three times\n                to get a leading 1 in the third row and obtain the row equivalent matrix\n               .\n              Now we perform backwards elimination to make the entries above the leading  s \n              equal to  ,\n              starting with the third column and working backwards.\n              Replace row one with row one minus   times row three and replace row two with \n              row two plus\n                row three to obtain the row equivalent matrix\n               .\n              For the second column,\n              we replace row one with row one plus row two to obtain the row equivalent matrix\n               .\n              Since the leading entry in row one is not a one,\n              we have one more step before we have the reduced row echelon form.\n              Finally, we replace row one with row one times  .\n              This gives us the reduced row echelon form\n               .\n             \n              Find the solution(s), if any, to the system.\n             \n              We can read off the solution to the system from the reduced row echelon form:\n               ,  , and  .\n              You should check in the original equations to make sure we have the correct solution.\n             "
},
{
  "id": "example-6",
  "level": "2",
  "url": "chap_row_echelon_forms.html#example-6",
  "type": "Example",
  "number": "3.10",
  "title": "",
  "body": "\n        In this example,   and   are unknown scalars.\n        Consider the system with augmented matrix\n         .\n      \n      \n        Find all values of   and   so that the system has:\n       \n              Exactly one solution (and find the solution)\n             \n              Let  ,  ,\n              and   be the variables corresponding to the first, second,\n              and third columns, respectively, of the augmented matrix.\n              To answer these questions, we row reduce the augmented matrix.\n              We interchange rows one and two and then also rows two and three to obtain the matrix\n               .\n             \n              Now we replace row three with row three minus row one to produce the row equivalent matrix\n               .\n             \n              Next, replace row three with row three minus   times row two.\n              This yields the row equivalent matrix\n               .\n             \n              We now have a row echelon form.\n             \n              The system will have exactly one solution when the last row has the form\n                where   is not zero.\n              Thus, the system has exactly one solution when\n               , or when  .\n              In this case, the solution is\n               .\n              You should check to ensure that this solution is correct.\n              The other cases occur when  .\n             \n              No solutions\n             \n              When   and  \n              (or  ),\n              then we have a row of the form  ,\n              where   is not  .\n              In these cases there are no solutions.\n             \n              Infinitely many solutions (and find all solutions)\n             \n              When   and  ,\n              then the last row is a row of all zeros.\n              In this case,\n              the system is consistent and   is a free variable,\n              so the system has infinitely many solutions.\n              The solutions are\n               \n              You should check to ensure that this solution is correct.\n             "
},
{
  "id": "p-492",
  "level": "2",
  "url": "chap_row_echelon_forms.html#p-492",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "pivot "
},
{
  "id": "exercise-18",
  "level": "2",
  "url": "chap_row_echelon_forms.html#exercise-18",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Represent the following linear system in variables\n          in augmented matrix form and use row reduction to find the general solution of the system.\n         \n       \n        Augmented matrix:\n \n         ,\n        solution  ,  , and  \n       "
},
{
  "id": "exercise-19",
  "level": "2",
  "url": "chap_row_echelon_forms.html#exercise-19",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Represent the following linear system in variables\n          in augmented matrix form after rearranging the terms and use row reduction to find all solutions to the system.\n         .\n       "
},
{
  "id": "exercise-20",
  "level": "2",
  "url": "chap_row_echelon_forms.html#exercise-20",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Check that the reduced row echelon form of the matrix\n         \n        is\n         .\n       \n        Row operations show this.\n       "
},
{
  "id": "exercise-21",
  "level": "2",
  "url": "chap_row_echelon_forms.html#exercise-21",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Consider the following system:\n         \n       \n              Find a row echelon form of the augmented matrix for this system.\n             \n              For which values of  , if any,\n              does the system have (i.) no solutions, (ii.) exactly one solution, (iii.) infinitely many solutions?\n              Find the solutions in each case.\n             "
},
{
  "id": "exercise-22",
  "level": "2",
  "url": "chap_row_echelon_forms.html#exercise-22",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Find the general solution of the linear system corresponding to the following augmented matrix:\n         .\n       \n         ,\n         ,   is free,  \n       "
},
{
  "id": "exercise-23",
  "level": "2",
  "url": "chap_row_echelon_forms.html#exercise-23",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        What are the conditions, if any, on the\n          values so that the following augmented matrix corresponds to a consistent linear system?\n        How many solutions will the consistent system have?\n        Explain.\n         .\n       "
},
{
  "id": "exercise-24",
  "level": "2",
  "url": "chap_row_echelon_forms.html#exercise-24",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        In this exercise the symbol\n          denotes a non-zero number and the symbol * denotes any real number\n        (including  ).\n       \n              Is the augmented matrix\n               \n              in a form to which back substitution will easily give the solutions to the system?\n              Explain your reasoning.\n              \n             \n              In order to help see what happens in the general case,\n              substitute some numbers in place of the\n               's and *'s and answer the question for that specific system first.\n              Then determine if your answer generalizes.\n             \n              Yes\n             \n              The above matrix is a possible form of an augmented matrix with 2 rows and 3 columns corresponding to a linear system after forward elimination,\n              i.e., a linear system for which back substitution will easily give the solutions.\n              Determine the other possible such forms of the nonzero augmented matrices with 2 rows and 3 columns.\n              As in part (a), use the symbol\n                to denote a non-zero number and * to denote any real number.\n             \n               ,\n               \n             "
},
{
  "id": "exercise-25",
  "level": "2",
  "url": "chap_row_echelon_forms.html#exercise-25",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Give an example of a linear system with a unique solution for which a row echelon form of the augmented matrix of the system has a row of 0's.\n       "
},
{
  "id": "exercise-26",
  "level": "2",
  "url": "chap_row_echelon_forms.html#exercise-26",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        Come up with an example of an augmented matrix with 0's in the rightmost column corresponding to an inconsistent system,\n        if possible.\n        If not, explain why not.\n       \n        This is not possible.\n       "
},
{
  "id": "exercise-27",
  "level": "2",
  "url": "chap_row_echelon_forms.html#exercise-27",
  "type": "Exercise",
  "number": "10",
  "title": "",
  "body": "\n        Find two different row echelon forms which are equivalent to the same matrix not given in row echelon form.\n       "
},
{
  "id": "exercise-28",
  "level": "2",
  "url": "chap_row_echelon_forms.html#exercise-28",
  "type": "Exercise",
  "number": "11",
  "title": "",
  "body": "\n        Determine all possible row echelon forms of a   matrix.\n        Use the symbol   to denote a non-zero number and * to denote a real number with no condition on being 0 or not to represent entries.\n       \n          and  .\n       "
},
{
  "id": "exercise-29",
  "level": "2",
  "url": "chap_row_echelon_forms.html#exercise-29",
  "type": "Exercise",
  "number": "12",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              The number of pivots of an\n                matrix cannot exceed  .\n              (Note: Here  ,   are some unknown numbers.)\n             \n              T\n             True\/False \n              The row echelon form of a matrix is unique.\n             True\/False \n              The reduced row echelon form of a matrix is unique.\n             \n              T\n             True\/False \n              A system of equations where there are fewer equations than the number of unknowns\n              (known as an underdetermined system)\n              cannot have a unique solution.\n             True\/False \n              A system of equations where there are more equations than the number of unknowns\n              (known as an overdetermined system)\n              cannot have a unique solution.\n             \n              F\n             True\/False augmented matrix True\/False \n              If the coefficient matrix of a system has pivots in every row,\n              then the system is consistent.\n             \n              T\n             True\/False \n              If there is a row of zeros in a row echelon form of the augmented matrix of a system of equations,\n              the system has infinitely many solutions.\n             True\/False \n              If there is a row of zeros in a row echelon form of the augmented matrix of a system of   equations in   variables,\n              the system has infinitely many solutions.\n             \n              F\n             True\/False \n              If a linear system has no free variables,\n              then the system has a unique solution.\n             True\/False \n              If a linear system has a free variable,\n              then the system has infinitely many solutions.\n             \n              F\n             "
},
{
  "id": "p-549",
  "level": "2",
  "url": "chap_row_echelon_forms.html#p-549",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "reactants products "
},
{
  "id": "act_1_c_reaction_1",
  "level": "2",
  "url": "chap_row_echelon_forms.html#act_1_c_reaction_1",
  "type": "Project Activity",
  "number": "3.8",
  "title": "",
  "body": "\n            Set up an equation that balances the number of carbon atoms on both sides of the reaction.\n           \n            Balance the numbers of hydrogen and oxygen atoms in the reaction to explain why\n             .\n           \n            So the system of linear equations that models this chemical reaction is\n             \n            Find all solutions to this system and then balance the reaction.\n            Note that we cannot have a fraction of a molecule in our reaction.\n           \n            Some of the work needed is done in  .\n           "
},
{
  "id": "project-8",
  "level": "2",
  "url": "chap_row_echelon_forms.html#project-8",
  "type": "Project Activity",
  "number": "3.9",
  "title": "",
  "body": "\n      Chemical reactions can be very interesting.\n     \n            Carbon dioxide,  ,\n            is a familiar product of combustion.\n            For example, when we burn glucose,\n             ,\n            the products of the reaction are carbon dioxide and water:\n             .\n            Use the techniques developed in this project to balance this reaction.\n           \n            To burn glucose,\n            we need to add oxygen to make the combustion happen.\n            Carbon dioxide is different in that it can burn without the presence of oxygen.\n            For example,\n            when we mix magnesium (Mg) with dry ice ( ),\n            the products are magnesium oxide (MgO) and carbon (C).\n            This is an interesting reaction to watch:\n            you can see it at many websites,  ,\n             \n            or  \n            Use the method determined above to balance the chemical reaction\n             .\n           "
},
{
  "id": "chap_vector_representation",
  "level": "1",
  "url": "chap_vector_representation.html",
  "type": "Section",
  "number": "4",
  "title": "Vector Representation",
  "body": "Vector Representation \n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is a vector?\n           \n         \n           \n            How do we define operations on vectors?\n           \n         \n           \n            What is a linear combination of vectors?\n           \n         \n           \n            How do we determine if one vector is a linear combination of a given set of vectors?\n           \n         \n           \n            How do we represent a linear system as a vector equation?\n           \n         \n           \n            What is the span of a set of vectors?\n           \n         \n           \n            What are possible geometric representations of the span of a vector,\n            or the span of two vectors?\n           \n         Application: The Knight's Tour \n    Chess is a game played on an\n      grid which utilizes a variety of different pieces.\n    One piece, the knight,\n    is different from the other pieces in that it can jump over other pieces.\n    However, the knight is limited in how far it can move in a given turn.\n    For these reasons, the knight is a powerful,\n    but often under-utilized, piece.\n   \n    A knight can move two units either horizontally or vertically,\n    and one unit perpendicular to that.\n    Four knight moves are as illustrated in  ,\n    and the other four moves are the opposites of these.\n   Moves a knight can make. \n    The knight's tour problem is the mathematical problem of finding a knight's tour,\n    that is a sequence of knight moves so the the knight visits each square exactly once.\n    While we won't consider a knight's tour in this text,\n    we will see using linear combinations of vectors that a knight can move from its initial \n    position to any other position on the board,\n    and that it is possible to determine a sequence of moves to make that happen.\n   Introduction vectors vector vector entry vector component vector entry component Note \n    For the majority of this text,\n    we will work with real vectors.\n    However, a vector does not need to be restricted to have real entries.\n    At times we will use complex vectors and even vectors in other types of sets.\n    The types of sets we use will be ones that have structure just like the real numbers.\n    Recall that a real number is a number that has a decimal representation,\n    either finite or repeating\n    (rational numbers)\n    or otherwise\n    (irrational numbers).\n    We can add and multiply real numbers as we have done throughout our mathematical careers,\n    and the real numbers have a certain structure given in the following theorem that we will treat \n    as an axiom   that is,\n    we assume these properties without proof.\n    We denote the set of real numbers with the symbol  .\n   \n        Let  ,  , and   be real numbers.\n        Then\n         \n             \n                and   (The name given to this property is closure.\n              That is, the set   is closed under addition and multiplication.)\n             \n           \n             \n                and   (The name given to this property is commutativity.\n              That is addition and multiplication are commutative operations in  .)\n             \n           \n             \n                and\n                (The name given to this property is associativity.\n              That is, addition and multiplication is associative operations in  .)\n             \n           \n             \n              There is an element   in   such that\n                (The element   is called the additive identity in  .)\n             \n           \n             \n              There is an element   in   such that\n                (The element   is called the multiplicative identity in  .)\n             \n           \n             \n              There is an element   in   such that\n                (The element   is the additive inverse of   in  .)\n             \n           \n             \n              If  , there is an element\n                in   such that   (The element\n                is the multiplicative inverse of the nonzero element   in  .)\n             \n           \n             \n                (The is the distributive property.\n              That is, multiplication distributes over addition in  .)\n             \n           \n       field scalars \n    We will algebraically represent a vector as a matrix with one column.\n    For example,\n      is a vector with 2 entries,\n    and we say that   is a vector in 2-space.\n    By 2-space we mean  ,\n    which can be geometrically modeled as the plane.\n    Here the symbol   indicates that the entries of   are real numbers and the \n    superscript 2 tells us that   has two entries.\n    Similarly, vectors in   have three entries,\n    e.g.,  .\n    The collection of column vectors with three entries can be geometrically modeled as three-dimensional space.\n    If a vector   has   entries we say that   is a vector in  \n    (or  -space).\n    Vectors are also often indicated with arrows,\n    so we might also see a vector   written as  .\n    It is important when writing to differentiate between a vector   and a scalar  .\n    These are quite different objects and it is up to us to make sure we are clear what a symbol represents.\n    We will use boldface letters to represent vectors.\n   column vector \n    Similarly, we can define scalar multiplication of a vector by multiplying each component of the \n    vector by the scalar.\n    For example,\n     .\n   \n    Since we can add vectors and multiply vectors by scalars,\n    we can then add together scalar multiples of vectors.\n    For completeness,\n    we define vector subtraction as adding a scalar multiple:\n     .\n   \n    This definition is equivalent to defining subtraction of   from   by \n    subtracting components of   from the corresponding components of  .\n   \n            Given vectors\n             ,\n            determine the components of the vector\n              using the operations defined above.\n           addition of vectors is a commutative operation \n            One way to geometrically represent vectors with two components uses a point in the \n            plane to correspond to a vector.\n            Specifically,\n            the vector   corresponds \n            to the point   in the plane.\n            As a specific example, the vector\n              corresponds to the point \n              in the plane.\n            This representation will be especially handy when we consider infinite collections \n            of vectors as we will do in this problem.\n           \n                  On the same set of axes,\n                  plot the points that correspond to 5-6 scalar multiples of the vector  .\n                  Make sure to use variety of scalar multiples covering possibilities with  .\n                  If we consider the collection of all possible scalar multiples of this vector,\n                  what do we obtain?\n                 \n                  What would the collection of all scalar multiples of the vector   form in the plane?\n                 \n                  What would the collection of all scalar multiples of the vector\n                    form in the three-dimensional space?\n                 \n            Let   and \n              in  .\n            We are interested in finding all vectors that can be formed as a sum of scalar \n            multiples of   and  .\n           \n                  On the same set of axes,\n                  plot the points that correspond to the vectors  .\n                  Plot other random sums of scalar multiples of   and   using several scalar multiples (including those less than 1 or negative)\n                  (that is, find other vectors of the form\n                    where   and   are any scalars.).\n                 \n                  If we considered sums of all scalar multiples of  ,\n                  which vectors will we obtain?\n                  Can we obtain any vector in   in this form?\n                 Vectors and Vector Operations vector spaces \n    In  \n    we saw how to add vectors and multiply vectors by scalars in  ,\n    and this idea extends to   for any  .\n    Before we do so,\n    one thing we didn't address in  \n    is what it means for two vectors to be equal.\n    It should seem reasonable that two vectors are equal if and only if they have the same \n    corresponding components.\n    More formally, if we let\n     \n    be vectors in  ,\n    then   if   for every   between 1 and  .\n    Note that this statement implies that a vector in   cannot equal a vector in \n      because they don't have the same number of components.\n    With this in mind we can now define the sum\n      of the vectors   and   to be the vector in   \n    defined by\n     .\n   \n    In other words, to add two vectors of the same size,\n    we add corresponding components.\n   \n    Similarly, we can define scalar multiplication of a vector.\n    If   is a scalar,\n    then the scalar multiple   of the vector   is the vector in   \n    defined by\n     .\n   \n    In other words,\n    the scalar multiple   of the vector   is the vector obtained by \n    multiplying each component of the vector   by the scalar  .\n    Since we can add vectors and multiply vectors by scalars,\n    we can then add together scalar multiples of vectors.\n    For completeness,\n    we define vector subtraction as adding a scalar multiple:\n     .\n   \n    This definition is equivalent to defining subtraction of   from   by \n    subtracting components of   from the corresponding components of  .\n   \n    After defining operations on objects,\n    we should wonder what kinds of properties these operations have.\n    For example,\n    with the operation of addition of real numbers we know that   is equal to  .\n    This is called the  commutative \n    property of scalar addition and says that order does not matter when we add real numbers.\n    It is natural for us to ask if similar properties hold for the vector operations,\n    addition and scalar multiplication, we defined.\n    You showed in  \n    that the addition operation is also commutative on vectors in  .\n   \n    In the activity below we consider how the two operations,\n    addition and scalar multiplication, interact with each other.\n    In real numbers,\n    we know that multiplication is distributive over addition.\n    Is that true with vectors as well?\n   \n      We work with vectors in   to make the notation easier.\n     \n      Let   be an arbitrary scalar,\n      and   and   be two\n       arbitrary  vectors in  .\n      Is   equal to  ?\n      What property does this imply about the scalar multiplication and addition operations on vectors?\n     \n    Similar arguments can be used to show the following properties of vector addition and\n    multiplication by scalars.\n   zero vector additive inverse vector space Geometric Representation of Vectors and Vector Operations \n    We can geometrically represent a vector\n      in   as the point\n      in the plane as we did in  .\n    We can similarly represent a vector\n      in   as the point\n      in the three-dimensional space.\n    This geometric representation will be handy when we consider collections of infinitely many vectors,\n    as we will do when we consider the span of a collection of vectors later in this section.\n   \n    We can also represent the vector\n      in   as the \n    directed line segment\n    (or arrow)\n    from the origin to the point\n      as shown in   to aid in the visualization.\n   The vector   in  . \n    The fact that the vector in  \n    is represented by the directed line segment from the origin to the point (4,6) means that \n    this vector is the vector  .\n    If   is the origin and   is the point  ,\n    we will also denote this vector as     so\n     .\n   length \n    Thinking of vectors having direction and length is especially useful in visualizing the \n    addition of vectors.\n    The geometric interpretation of the sum of two vectors can be seen in \n     \n    and  .\n   A vector sum. Geometric interpretation. \n    Let   and \n     .\n    Then   as shown in \n     .\n     \n    provides a context to interpret this vector sum geometrically.\n    Using the parallelogram imposed on the three vectors,\n    we see that if vectors   and   are both placed to start at the origin,\n    then the vector sum   can be visualized geometrically as the directed line \n    segment from the origin to the fourth corner of the parallelogram.\n   \n    In  \n    we considered scalar multiples of a vector in  .\n    The arrow representation helps in visualizing scalar multiples as well.\n    Geometrically,\n    a scalar multiple   of a nonzero vector   is a vector in the same direction \n    as   if   and in the opposite direction as   if  .\n    If  , scalar multiplication stretches the vector,\n    while   shrinks the vector.\n    We also saw that the collection of all scalar multiples of a vector   in   \n    gives us a line through the origin and  ,\n    except when   in which case we only obtain  .\n    In other words, for a nonzero vector  ,\n    the set   is the line through the origin and \n      in  .\n   \n    All of these properties generalize to vectors in  .\n    Specifically,\n    the scalar multiple   is a vector in the same or opposite direction as   \n    based on the sign of  ,\n    and is a stretched or shrunken version of   based on whether   or  .\n    Also, the collection of all multiples of a non-zero vector   in   \n    form a line through the origin.\n   Linear Combinations of Vectors \n    The concept of linear combinations is one of the fundamental ideas in linear algebra.\n    We will use linear combinations to describe almost every important concept in linear \n    algebra   the span of a set of vectors,\n    the range of a linear transformation, bases,\n    the dimension of a vector space   to name just a few.\n   \n    In  ,\n    we considered the sets of all scalar multiples of a single nonzero vector in \n      and in  .\n    We also considered the set of all sums of scalar multiples of two nonzero vectors.\n    These results so far gives us an idea of geometrical descriptions of sets of vectors \n    generated by one or two vectors.\n    Oftentimes we are interested in what vectors can be made from a given collection of vectors.\n    For example,\n    suppose we have two different water-benzene-acetic acid chemical solutions,\n    one with 40% water, 50% benzene and 10% acetic acid,\n    the other with 52% water, 42% benzene and 6% acid.\n    An experiment we want to conduct requires a chemical solution with 43% water, 48% benzene \n    and 9% acid.\n    We would like to know if we make this new chemical solution by mixing the first two \n    chemical solutions,\n    or do we have to run to the chemical solutions market to get the chemical solution we want.\n   \n    We can set up a system of equations for each ingredient and find the answer.\n    But we can also consider each chemical solution as a vector,\n    where the components represent the water,\n    benzene and acid percentages.\n    So the two chemical solutions we have are represented by the vectors\n      and \n     .\n    If we mix the two chemical solutions with varying amounts of each ingredient,\n    then the question of whether we can make the desired chemical solution becomes the \n    question of whether the equation\n     \n    has a solution.\n    (You will determine if this equation has a solution in  .)\n   \n    We might also be interested in what other chemical solutions we can make from the two given solutions.\n    This amounts to determining which vectors can be written in the form\n      \n    for scalars   and  .\n    Vectors that are created from sums of scalar multiples of given vectors are called linear \n    combinations of those vectors.\n    More formally,\n   linear combination linear combination weights linear combination weights \n    In the chemical solutions example,\n    the vector   \n    for scalars   and   is a linear combination of the vectors \n      and\n      with weights   and  ,\n    and the set of linear combinations of the given chemical solution vectors tells us exactly which \n    chemical solutions we can make from the given ones.\n    This is one example of how linear combinations can arise in applications.\n   \n    The set of all linear combinations of a fixed collection of vectors has a very nice algebraic \n    structure and,\n    in small dimensions,\n    allows us to use a geometrical description to aid our understanding.\n    In the above example,\n    this collection gives us the type of chemical solutions we can make by combining the first two \n    solutions in varying amounts.\n   \n      Our chemical solution example illustrates that it can be of interest to determine whether certain vectors can be written as a linear combination of given vectors.\n      We explore that idea in more depth in this activity.\n      Let   and \n       .\n     \n            Calculate the linear combination of   and   with corresponding weights\n            (scalar multiples)\n            1 and 2.\n            The resulting vector is a vector which can be written as a linear combination of   and  .\n           \n            Can   \n            be written as a linear combination of   and  ?\n            If so, which linear combination?\n            If not, explain why not.\n           \n            Can   be written \n            as a linear combination of   and  ?\n            If so, which linear combination?\n            If not, explain why not.\n           \n            Let  .\n            The problem of determining if   is a linear combination of   \n            and   is equivalent to the problem of finding scalars   and \n              so that\n             .\n           \n                  Combine the vectors on the right hand side of equation   \n                  into one vector,\n                  and then set the components of the vectors on both sides equal to each other to \n                  convert the vector equation   to a linear system of \n                  three equations in two variables.\n                 \n                  Use row operations to find a solution, if it exists,\n                  to the system you found in the previous part of this activity.\n                  If you find a solution,\n                  verify in   that you have found appropriate weights to \n                  produce the vector   as a linear combination of   and  .\n                 \n    Note that to find the weights that make   a linear combination of the vectors \n      and  ,\n    we simply solved the linear system corresponding to the augmented matrix\n     ,\n    where the vectors  ,\n     ,\n    and   form the columns of an augmented matrix,\n    and the solution of the system gave us the weights of the linear combination.\n    In general, if we want to find weights  ,\n     ,  ,\n      so that a vector   in   is a linear \n    combination of the vectors  ,\n     ,  ,\n      in  ,\n    we solve the system corresponding to the augmented matrix\n     .\n   \n    Any solution to this system will gives us the weights.\n    If this system has no solutions,\n    then   cannot be written as a linear combination of the vectors  ,\n     ,  ,  .\n    This shows us the equivalence of the linear system and its vector equation representation.\n    Specifically, we have the following result.\n   \n        The vector equation\n         \n        has the same solution set as the linear system represented by the augmented matrix\n         .\n       \n        In particular,\n        the system has a solution if and only if   is a linear combination of the vectors \n         .\n       \n            Represent the following linear system as a vector equation.\n            After finding the vector equation,\n            compare your vector equation to the matrix representation you found in  .\n            (Note that this is the same linear system from  .)\n             \n           \n            Represent the following vector equation as a linear system and solve the linear system.\n             \n           The Span of a Set of Vectors span \n    Our work in  \n    seems to indicate that the span of a set of vectors,\n    i.e., the collection of all linear combinations of this set of vectors,\n    has a nice structure.\n    As we mentioned above,\n    the span of a set of vectors represents the collection of all constant vectors for which a \n    linear system has a solution,\n    but we will also see that other important objects in linear algebra can be represented as \n    the span of a set of vectors.\n   span span Notation \n    We denote the span of a set of vectors  ,\n     ,  ,   as\n     .\n   \n    So\n     .\n   \n    The curly braces,  ,\n    are used in denoting sets.\n    They represent the whole set formed by the objects included between them.\n    So   represents the collection of the vectors formed by\n      for an arbitrary number  .\n    Note that   can be 1, meaning that the collection can contain only one vector  .\n   \n    We now investigate what the span of a set of one or two vectors is,\n    both from an algebraic and geometric perspective,\n    and consider what happens for more general spanning sets.\n   \n            By definition,\n              is the collection of all vectors which are scalar multiples of  .\n            Determine which vectors are in this collection.\n            If we plot all these vectors with each vector being represented as a point in the plane,\n            what do they form?\n           \n            Let   and \n              in  .\n            By definition,\n             \n            is the collection of all linear combinations of the form\n             ,\n            where   and   are any scalars.\n           \n                  Find four different vectors in\n                    and indicate the weights\n                  (the values of   and  )\n                  for each linear combination.\n                  \n                 \n                  It is really easy to find 3 vectors in\n                    for any  .\n                 \n                  Are there any vectors in   that are not in  ?\n                  Explain.\n                  Verify your result.\n                 \n                  Set up a linear system to determine which vectors\n                    are in \n                   .\n                  Specifically,\n                  which   can be expressed as a linear combination of   and \n                   ?\n                 \n                  Geometrically, what shape do the vectors in\n                    form inside  ?\n                 \n            Is it possible for   to be a line for two vectors   in  ?\n           \n            What do you think are the possible geometric descriptions of a span of a set of vectors in  ?\n            Explain.\n           \n            What do you think are the possible spans of a set of vectors in  ?\n            Explain.\n           Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        For each of the following systems,\n         \n             \n              express an arbitrary solution to the system algebraically as a linear combination of vectors,\n             \n           \n             \n              find a set of vectors that spans the solution set,\n             \n           \n             \n              describe the solution set geometrically.\n             \n           \n       \n               \n             \n              In each example,\n              we use technology to find the reduced row echelon form of the augmented matrix.\n             \n              The reduced row echelon form of the augmented matrix\n               \n              is\n               .\n               \n                   \n                    There is no pivot in the   column,\n                    so   is a free variable.\n                    Since the system is consistent,\n                    it has infinitely many solutions.\n                    We can write both   and   in terms of   as\n                      and  .\n                    So the general solution to the system has the algebraic form\n                     .\n                    So every solution to this system is a scalar multiple\n                    (linear combination)\n                    of the vector  .\n                   \n                 \n                   \n                    Since every solution to the system is a scalar multiple of the vector \n                     ,\n                    the solution set to the system is \n                     .\n                   \n                 \n                   \n                    As the set of scalar multiples of a single vector,\n                    the solution set to this system is a line in   through the \n                    origin and the point  .\n                   \n                 \n             \n               \n               \n             \n              The reduced row echelon form of the augmented matrix\n               \n              is\n               .\n               \n                   \n                    There are no pivots in the   and   columns,\n                    so   and   are free variables.\n                    Since the system is consistent,\n                    it has infinitely many solutions.\n                    We can write    in terms of    and   as  .\n                    So the general solution to the system has the algebraic form\n                     .\n                    So every solution to this system is a linear combination of the vectors\n                      and  .\n                   \n                 \n                   \n                    Since every solution to the system is a linear combination of the vectors\n                      and  ,\n                    the solution set to the system is\n                     .\n                   \n                 \n                   \n                    As the set of linear combinations of two vectors,\n                    the solution set to this system is a plane in   through the origin and the points   and  .\n                   \n                 \n             \n        Let  .\n       \n              Find three vectors  ,  ,\n              and   such that  .\n             \n              Every vector in   has the form\n               \n              for some real numbers  ,  , and  .\n              Thus,   where  ,\n               ,\n              and  .\n             \n              Can   be\n              written as a linear combination of the vectors  ,\n               ,  ?\n              If so, find such a linear combination.\n              If not, justify your response.\n              What does your result tell us about the relationship between   and  ?\n              Explain.\n             \n              To determine if   is a linear combination of  ,\n               , and  ,\n              we row reduced the augmented matrix  .\n              The reduced row echelon form of the matrix   is\n               .\n              The system with this as augmented matrix is consistent.\n              If we let  ,  ,\n              and   be the variables corresponding to the first three columns,\n              respectively, of this augmented matrix,\n              then we see that  ,\n               , and  .\n              So   can be written as a linear combination of  ,\n               , and   as\n               .\n              Since  ,\n              it follows that  .\n             \n              Can   be \n              written as a linear combination of the vectors  ,\n               ,  ?\n              If so, find such a linear combination.\n              If not, justify your response.\n              What does your result tell us about the relationship between   and  ?\n              Explain.\n             \n              To determine if   is a linear combination of  ,\n               , and  ,\n              we row reduced the augmented matrix  .\n              The reduced row echelon form of the matrix   is\n               .\n              The last row shows that the system with this as augmented matrix is inconsistent.\n              So   cannot be written as a linear combination of  ,\n               , and  .\n              Since  ,\n              it follows that  .\n             \n              What relationship, if any,\n              exists between   and  ?\n              Explain.\n             \n              We know that  .\n              Now   contains the linear combinations of vectors in  ,\n              which are all linear combinations of the vectors  ,\n               , and  .\n              Thus,   is just the set of linear combinations of  ,\n               , and  .\n              We conclude that  .\n             Summary \n       \n        A vector is a list of numbers in a specified order.\n       \n     \n       \n        We add two vectors of the same size by adding corresponding components.\n        In other words,\n        if   and   are vectors of the same size and   and \n          are the   components of   and  ,\n        respectively,\n        then   is the vector whose  th component is   \n        for each  .\n        Geometrically,\n        we represent the sum of two vectors using the Parallelogram Rule: The vector \n          is the directed line segment from the origin to the 4th point of the parallelogram formed by the origin and the vectors  .\n       \n     \n       \n        A scalar multiple of a vector is found by multiplying each component of the vector \n        by that scalar.\n        In other words,\n        if   is the   component of the vector   and   is any scalar,\n        then   is the vector whose   component is   for each  .\n        Geometrically,\n        a scalar multiple of a nonzero vector   is a vector in the same direction \n        as   if   and in the opposite direction if  .\n        If  , the vector is stretched,\n        and if  , the vector is shrunk.\n       \n     \n       \n        An important concept is that of a linear combination of vectors.\n        In words, a linear combination of a collection of vectors is a sum of scalar multiples \n        of the vectors.\n        More formally,\n        we defined a linear combination of vectors  ,\n         ,  ,\n          in   is any vector of the form \n         ,\n        where  ,  ,\n         ,   are scalars.\n       \n     \n       \n        To find weights  ,  ,  ,\n          so that a vector   in   is a linear combination \n        of the vectors  ,\n         ,  ,   in  ,\n        we simply solve the system corresponding to the augmented matrix\n         .\n       \n     \n       \n        The collection of all linear combinations of a set of vectors is called the span of \n        the set of vectors.\n        More formally,\n        the span of the vectors  ,  ,\n         ,   in   is the set\n         ,\n        which we denote as  .\n        Geometrically,\n        the span of a single nonzero vector   in any dimension is the line through \n        the origin and the vector  .\n        The span of two vectors   in any dimension neither of which is a \n        multiple of the other is a plane through the origin containing both vectors.\n       \n     \n        Given vectors   and \n          in  ,\n        determine if   \n        can be written as a linear combination of   and  .\n        If so, determine the weights of   and   which produce  .\n       \n         \n       \n        Given vectors  ,\n          and   in  ,\n        determine if   can be written as a linear combination of  ,\n          and  .\n        If so, determine the weights of  ,\n          and   which produce  .\n        Reflect on the result.\n        Is there anything special about the given vectors  ,\n          and  ?\n       \n        Let   and   in  .\n        Determine which vectors   in   can be written as a linear combination of   and  .\n        Does the set of  's include the 0 vector?\n        If so, determine which weights in the linear combination produce the 0 vector.\n        If not, explain why not.\n       \n        Only when  .\n         \n       \n        Consider vectors   and \n          in  .\n       \n              Find four specific linear combinations of the vectors   and  .\n             \n              Explain why the zero vector must be a linear combination of   and  .\n             \n              What kind of geometric shape does the set of all linear combinations of \n                and   have in  ?\n             \n              Can we obtain any vector in   as a linear combination of   \n              and  ?\n              Explain.\n             \n        Suppose we have two different water-benzene-acetic acid solutions,\n        one with 40% water, 50% benzene and 10% acetic acid,\n        the other with 52% water, 42% benzene and 6% acid.\n       \n              An experiment we want to conduct requires a solution with 43% water,\n              48% benzene and 9% acid.\n              Representing each acid solution as a vector,\n              determine if we can we make this new acid solution by mixing the first two solutions,\n              or do we have to run to the chemical solutions market to get the solution we want?\n             \n              We cannot make the desired solution.\n             \n              Using the water-benzene-acetic acid solutions in the previous problem,\n              can we obtain an acid solution which contains 50% water, 43% benzene and 7% acid?\n             \n              We can make the desired chemical solution with\n                of solution   for every\n                of solution  .\n             \n              Determine the relationship between the percentages of water, benzene,\n              and acid in solutions which can be obtained by mixing the two given \n              water-benzene-acetic acid solutions above.\n             \n              If   can be made from our original two chemical solutions,\n              then  .\n             \n        Is the vector   in  ?\n        Justify your answer.\n       \n        Describe geometrically each of the following sets.\n       \n                in  \n             \n                is the line in   through the origin and the point  .\n             \n                in  \n             \n                is the plane in   through the origin and the points   and  .\n             \n        Consider the linear system\n         \n       \n              Find the general solution to this system.\n             \n              Find two specific vectors   and   so that the solution set to this system is  .\n             \n        Answer the following question as yes or no.\n        Verify your answer.\n        If   and   are vectors in  ,\n        then   is in  .\n       \n        Yes.\n       \n        Let  ,  ,\n        and   be vectors in   and let   and   be scalars.\n        Verify  .\n        That is, show that\n       \n               \n             \n               \n             \n              The vector   has the property that  .\n             \n               .\n             \n               \n             \n               \n             \n               \n             \n               .\n             \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              A vector in  ,\n              i.e. a two-dimensional vector,\n              is also a vector in  .\n             \n              F\n             True\/False \n              Any vector in   can be visualized as a vector in   by \n              adding a 0 as the last coordinate.\n             True\/False \n              The zero vector is a scalar multiple of any other vector\n              (of the same size).\n             \n              T\n             True\/False \n              The zero vector cannot be a linear combination of two non-zero vectors.\n             True\/False \n              Given two vectors   and  ,\n              the vector   is a linear combination of   and  .\n             \n              T\n             True\/False \n              Given any two non-zero vectors   and   in  ,\n              we can obtain any vector in   as a linear combination of   and  .\n             True\/False \n              Given any two distinct vectors   and   in  ,\n              we can obtain any vector in   as a linear combination of   and  .\n             \n              F\n             True\/False \n              If   can be expressed as a linear combination of   and  ,\n              then   can also be expressed as a linear combination of   and  .\n             True\/False \n              The span of any two vectors neither of which is a multiple of the other can be visualized as a plane through the origin.\n             \n              T\n             True\/False \n              Given any vector,\n              the collection of all linear combinations of this vector can be visualized as a line through the origin.\n             True\/False \n              The span of any collection of vectors includes the   vector.\n             \n              T\n             True\/False \n              If the span of   and   is all of  ,\n              then so is the span of   and  .\n             True\/False \n              If the span of\n                and   is all of  ,\n              then so is the span of   and  .\n             \n              F\n             Project: Analyzing Knight Moves \n    To understand where a knight can move in a chess game,\n    we need to know the initial setup.\n    A chess board is an   grid.\n    To be able to refer to the individual positions on the board,\n    we will place the board so that its lower left corner is at the origin,\n    make each square in the grid have side length  ,\n    and label each square with the point at the lower left corner.\n    This is illustrated at left in  .\n   Initial knight placement and moves. \n    Each player has two knights to start the game,\n    for one player the knights would begin in positions   and  .\n    Because of the symmetry of the knight's moves,\n    we will only analyze the moves of the knight that begins at position  .\n    This knight has only three allowable moves from its starting point\n    (assuming that the board is empty),\n    as shown at right in  .\n    The questions we will ask are:\n    given any position on the board,\n    can the knight move from its start position to that position using only knight moves and,\n    what sequence of moves will make that happen.\n    To answer these questions we will use linear combinations of knight moves described as vectors.\n   \n    Each knight move can be described by a vector.\n    A move one position to the right and two up can be represented as  .\n    Three other moves are  ,\n     ,\n    and  .\n    The other four knight moves are the additive inverses of these four.\n    Any sequence of moves by the knight is given by the linear combination\n     .\n   \n    A word of caution: the knight can only make complete moves,\n    so we are restricted to integer\n    (either positive or negative)\n    values for  ,  ,\n     , and  .\n    You can use the GeoGebra app at  \n    to see the effects the weights have on the knight moves.\n    We should note here that since addition of vectors is commutative,\n    the order in which we apply our moves does not matter.\n    However, we may need to be careful with the order so that our knight does not leave the \n    chess board.\n   \n            Explain why the vector equation\n             \n            will tell us if it is possible for the knight to move from its initial \n            position at   to the position (5,2).\n           \n            Find all solutions, if any, to the system from part (a).\n            If it is possible to find a sequence of moves that take the knight from its \n            initial position to position  ,\n            find weights  ,  ,\n             , and   to accomplish this move.\n            (Be careful   we must have solutions in which  ,\n             ,  , and   are integers.)\n            Is there more than one sequence of possible moves?\n            You can check your solution with the GeoGebra app at \n             .\n           \n     \n    shows that it is possible for our knight to move to position   on the board.\n    We would like to know if it is possible to move to any position on the board.\n    That is, we would like to know if the integer span of the four moves  ,\n     ,  ,\n    and   will allow our knight to cover the entire board.\n    This takes a bit more work.\n   \n      Given any position  ,\n      we want to know if our knight can move from its start position   \n      to position  .\n     \n            Write a vector equation whose solution will tell us if it is possible for our knight to move from its start position   to position  .\n           \n            Show that the solution to part (a) can be written in the form\n             \n           \n    To answer our question if our knight can reach any position,\n    we now need to determine if we can always find integer values of   and \n      to make equations   and \n      have integer solutions.\n    In other words,\n    we need to find values of   and   so that   and\n      are multiples of  .\n    How we do this could depend on the parity\n    (even or odd)\n    of   and  .\n    For example, if   is odd and   is even,\n    say   and   for some integers   and  , then\n     .\n   \n    With a little trial and error we can see that if we let  ,\n    then   and   is a solution with integer weights.\n    For example,\n    when   and   we have   and  .\n    This makes  ,\n     ,  .\n    Compare this to the solution(s) you found in  .\n    This analysis shows us how to move our knight to any position   where \n      is odd and   is even.\n   \n      Complete the analysis as above to determine if there are integer solutions to our knight's \n      move system in the following cases.\n     \n              odd and   odd\n           \n              even and   even\n           \n              even and   odd.\n           \n     \n    shows that for any position on the chess board,\n    using linear combinations of move vectors,\n    we can find a sequence of moves that takes our knight to that position.\n    (We actually haven't shown that these moves can be made so that our knight always stays \n    on the board   we leave that question to you.)\n   "
},
{
  "id": "objectives-4",
  "level": "2",
  "url": "chap_vector_representation.html#objectives-4",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is a vector?\n           \n         \n           \n            How do we define operations on vectors?\n           \n         \n           \n            What is a linear combination of vectors?\n           \n         \n           \n            How do we determine if one vector is a linear combination of a given set of vectors?\n           \n         \n           \n            How do we represent a linear system as a vector equation?\n           \n         \n           \n            What is the span of a set of vectors?\n           \n         \n           \n            What are possible geometric representations of the span of a vector,\n            or the span of two vectors?\n           \n         "
},
{
  "id": "F_knight_1",
  "level": "2",
  "url": "chap_vector_representation.html#F_knight_1",
  "type": "Figure",
  "number": "4.1",
  "title": "",
  "body": "Moves a knight can make. "
},
{
  "id": "p-570",
  "level": "2",
  "url": "chap_vector_representation.html#p-570",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "vectors "
},
{
  "id": "definition-9",
  "level": "2",
  "url": "chap_vector_representation.html#definition-9",
  "type": "Definition",
  "number": "4.2",
  "title": "",
  "body": "vector vector entry vector component vector entry component "
},
{
  "id": "thm_1_d_reals",
  "level": "2",
  "url": "chap_vector_representation.html#thm_1_d_reals",
  "type": "Theorem",
  "number": "4.3",
  "title": "",
  "body": "\n        Let  ,  , and   be real numbers.\n        Then\n         \n             \n                and   (The name given to this property is closure.\n              That is, the set   is closed under addition and multiplication.)\n             \n           \n             \n                and   (The name given to this property is commutativity.\n              That is addition and multiplication are commutative operations in  .)\n             \n           \n             \n                and\n                (The name given to this property is associativity.\n              That is, addition and multiplication is associative operations in  .)\n             \n           \n             \n              There is an element   in   such that\n                (The element   is called the additive identity in  .)\n             \n           \n             \n              There is an element   in   such that\n                (The element   is called the multiplicative identity in  .)\n             \n           \n             \n              There is an element   in   such that\n                (The element   is the additive inverse of   in  .)\n             \n           \n             \n              If  , there is an element\n                in   such that   (The element\n                is the multiplicative inverse of the nonzero element   in  .)\n             \n           \n             \n                (The is the distributive property.\n              That is, multiplication distributes over addition in  .)\n             \n           \n       "
},
{
  "id": "p-582",
  "level": "2",
  "url": "chap_vector_representation.html#p-582",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "field scalars "
},
{
  "id": "p-584",
  "level": "2",
  "url": "chap_vector_representation.html#p-584",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "column vector "
},
{
  "id": "pa_1_d",
  "level": "2",
  "url": "chap_vector_representation.html#pa_1_d",
  "type": "Preview Activity",
  "number": "4.1",
  "title": "",
  "body": "\n            Given vectors\n             ,\n            determine the components of the vector\n              using the operations defined above.\n           addition of vectors is a commutative operation \n            One way to geometrically represent vectors with two components uses a point in the \n            plane to correspond to a vector.\n            Specifically,\n            the vector   corresponds \n            to the point   in the plane.\n            As a specific example, the vector\n              corresponds to the point \n              in the plane.\n            This representation will be especially handy when we consider infinite collections \n            of vectors as we will do in this problem.\n           \n                  On the same set of axes,\n                  plot the points that correspond to 5-6 scalar multiples of the vector  .\n                  Make sure to use variety of scalar multiples covering possibilities with  .\n                  If we consider the collection of all possible scalar multiples of this vector,\n                  what do we obtain?\n                 \n                  What would the collection of all scalar multiples of the vector   form in the plane?\n                 \n                  What would the collection of all scalar multiples of the vector\n                    form in the three-dimensional space?\n                 \n            Let   and \n              in  .\n            We are interested in finding all vectors that can be formed as a sum of scalar \n            multiples of   and  .\n           \n                  On the same set of axes,\n                  plot the points that correspond to the vectors  .\n                  Plot other random sums of scalar multiples of   and   using several scalar multiples (including those less than 1 or negative)\n                  (that is, find other vectors of the form\n                    where   and   are any scalars.).\n                 \n                  If we considered sums of all scalar multiples of  ,\n                  which vectors will we obtain?\n                  Can we obtain any vector in   in this form?\n                 "
},
{
  "id": "p-597",
  "level": "2",
  "url": "chap_vector_representation.html#p-597",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "vector spaces "
},
{
  "id": "act_A1_3_1",
  "level": "2",
  "url": "chap_vector_representation.html#act_A1_3_1",
  "type": "Activity",
  "number": "4.2",
  "title": "",
  "body": "\n      We work with vectors in   to make the notation easier.\n     \n      Let   be an arbitrary scalar,\n      and   and   be two\n       arbitrary  vectors in  .\n      Is   equal to  ?\n      What property does this imply about the scalar multiplication and addition operations on vectors?\n     "
},
{
  "id": "thm_vector_properties",
  "level": "2",
  "url": "chap_vector_representation.html#thm_vector_properties",
  "type": "Theorem",
  "number": "4.4",
  "title": "",
  "body": "zero vector additive inverse "
},
{
  "id": "p-617",
  "level": "2",
  "url": "chap_vector_representation.html#p-617",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "vector space "
},
{
  "id": "F_Vector1",
  "level": "2",
  "url": "chap_vector_representation.html#F_Vector1",
  "type": "Figure",
  "number": "4.5",
  "title": "",
  "body": "The vector   in  . "
},
{
  "id": "p-621",
  "level": "2",
  "url": "chap_vector_representation.html#p-621",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "length "
},
{
  "id": "F_vector_sum_1",
  "level": "2",
  "url": "chap_vector_representation.html#F_vector_sum_1",
  "type": "Figure",
  "number": "4.6",
  "title": "",
  "body": "A vector sum. "
},
{
  "id": "F_vector_sum_2",
  "level": "2",
  "url": "chap_vector_representation.html#F_vector_sum_2",
  "type": "Figure",
  "number": "4.7",
  "title": "",
  "body": "Geometric interpretation. "
},
{
  "id": "x1_d_linear_combination",
  "level": "2",
  "url": "chap_vector_representation.html#x1_d_linear_combination",
  "type": "Definition",
  "number": "4.8",
  "title": "",
  "body": "linear combination linear combination weights linear combination weights "
},
{
  "id": "act_A1_3_7",
  "level": "2",
  "url": "chap_vector_representation.html#act_A1_3_7",
  "type": "Activity",
  "number": "4.3",
  "title": "",
  "body": "\n      Our chemical solution example illustrates that it can be of interest to determine whether certain vectors can be written as a linear combination of given vectors.\n      We explore that idea in more depth in this activity.\n      Let   and \n       .\n     \n            Calculate the linear combination of   and   with corresponding weights\n            (scalar multiples)\n            1 and 2.\n            The resulting vector is a vector which can be written as a linear combination of   and  .\n           \n            Can   \n            be written as a linear combination of   and  ?\n            If so, which linear combination?\n            If not, explain why not.\n           \n            Can   be written \n            as a linear combination of   and  ?\n            If so, which linear combination?\n            If not, explain why not.\n           \n            Let  .\n            The problem of determining if   is a linear combination of   \n            and   is equivalent to the problem of finding scalars   and \n              so that\n             .\n           \n                  Combine the vectors on the right hand side of equation   \n                  into one vector,\n                  and then set the components of the vectors on both sides equal to each other to \n                  convert the vector equation   to a linear system of \n                  three equations in two variables.\n                 \n                  Use row operations to find a solution, if it exists,\n                  to the system you found in the previous part of this activity.\n                  If you find a solution,\n                  verify in   that you have found appropriate weights to \n                  produce the vector   as a linear combination of   and  .\n                 "
},
{
  "id": "theorem-7",
  "level": "2",
  "url": "chap_vector_representation.html#theorem-7",
  "type": "Theorem",
  "number": "4.9",
  "title": "",
  "body": "\n        The vector equation\n         \n        has the same solution set as the linear system represented by the augmented matrix\n         .\n       \n        In particular,\n        the system has a solution if and only if   is a linear combination of the vectors \n         .\n       "
},
{
  "id": "act_A1_3_8",
  "level": "2",
  "url": "chap_vector_representation.html#act_A1_3_8",
  "type": "Activity",
  "number": "4.4",
  "title": "",
  "body": "\n            Represent the following linear system as a vector equation.\n            After finding the vector equation,\n            compare your vector equation to the matrix representation you found in  .\n            (Note that this is the same linear system from  .)\n             \n           \n            Represent the following vector equation as a linear system and solve the linear system.\n             \n           "
},
{
  "id": "p-646",
  "level": "2",
  "url": "chap_vector_representation.html#p-646",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "span "
},
{
  "id": "def_1_d_span",
  "level": "2",
  "url": "chap_vector_representation.html#def_1_d_span",
  "type": "Definition",
  "number": "4.10",
  "title": "",
  "body": "span span "
},
{
  "id": "act_A1_3_9",
  "level": "2",
  "url": "chap_vector_representation.html#act_A1_3_9",
  "type": "Activity",
  "number": "4.5",
  "title": "",
  "body": "\n            By definition,\n              is the collection of all vectors which are scalar multiples of  .\n            Determine which vectors are in this collection.\n            If we plot all these vectors with each vector being represented as a point in the plane,\n            what do they form?\n           \n            Let   and \n              in  .\n            By definition,\n             \n            is the collection of all linear combinations of the form\n             ,\n            where   and   are any scalars.\n           \n                  Find four different vectors in\n                    and indicate the weights\n                  (the values of   and  )\n                  for each linear combination.\n                  \n                 \n                  It is really easy to find 3 vectors in\n                    for any  .\n                 \n                  Are there any vectors in   that are not in  ?\n                  Explain.\n                  Verify your result.\n                 \n                  Set up a linear system to determine which vectors\n                    are in \n                   .\n                  Specifically,\n                  which   can be expressed as a linear combination of   and \n                   ?\n                 \n                  Geometrically, what shape do the vectors in\n                    form inside  ?\n                 \n            Is it possible for   to be a line for two vectors   in  ?\n           \n            What do you think are the possible geometric descriptions of a span of a set of vectors in  ?\n            Explain.\n           \n            What do you think are the possible spans of a set of vectors in  ?\n            Explain.\n           "
},
{
  "id": "example-7",
  "level": "2",
  "url": "chap_vector_representation.html#example-7",
  "type": "Example",
  "number": "4.11",
  "title": "",
  "body": "\n        For each of the following systems,\n         \n             \n              express an arbitrary solution to the system algebraically as a linear combination of vectors,\n             \n           \n             \n              find a set of vectors that spans the solution set,\n             \n           \n             \n              describe the solution set geometrically.\n             \n           \n       \n               \n             \n              In each example,\n              we use technology to find the reduced row echelon form of the augmented matrix.\n             \n              The reduced row echelon form of the augmented matrix\n               \n              is\n               .\n               \n                   \n                    There is no pivot in the   column,\n                    so   is a free variable.\n                    Since the system is consistent,\n                    it has infinitely many solutions.\n                    We can write both   and   in terms of   as\n                      and  .\n                    So the general solution to the system has the algebraic form\n                     .\n                    So every solution to this system is a scalar multiple\n                    (linear combination)\n                    of the vector  .\n                   \n                 \n                   \n                    Since every solution to the system is a scalar multiple of the vector \n                     ,\n                    the solution set to the system is \n                     .\n                   \n                 \n                   \n                    As the set of scalar multiples of a single vector,\n                    the solution set to this system is a line in   through the \n                    origin and the point  .\n                   \n                 \n             \n               \n               \n             \n              The reduced row echelon form of the augmented matrix\n               \n              is\n               .\n               \n                   \n                    There are no pivots in the   and   columns,\n                    so   and   are free variables.\n                    Since the system is consistent,\n                    it has infinitely many solutions.\n                    We can write    in terms of    and   as  .\n                    So the general solution to the system has the algebraic form\n                     .\n                    So every solution to this system is a linear combination of the vectors\n                      and  .\n                   \n                 \n                   \n                    Since every solution to the system is a linear combination of the vectors\n                      and  ,\n                    the solution set to the system is\n                     .\n                   \n                 \n                   \n                    As the set of linear combinations of two vectors,\n                    the solution set to this system is a plane in   through the origin and the points   and  .\n                   \n                 \n             "
},
{
  "id": "example_1_d_span",
  "level": "2",
  "url": "chap_vector_representation.html#example_1_d_span",
  "type": "Example",
  "number": "4.12",
  "title": "",
  "body": "\n        Let  .\n       \n              Find three vectors  ,  ,\n              and   such that  .\n             \n              Every vector in   has the form\n               \n              for some real numbers  ,  , and  .\n              Thus,   where  ,\n               ,\n              and  .\n             \n              Can   be\n              written as a linear combination of the vectors  ,\n               ,  ?\n              If so, find such a linear combination.\n              If not, justify your response.\n              What does your result tell us about the relationship between   and  ?\n              Explain.\n             \n              To determine if   is a linear combination of  ,\n               , and  ,\n              we row reduced the augmented matrix  .\n              The reduced row echelon form of the matrix   is\n               .\n              The system with this as augmented matrix is consistent.\n              If we let  ,  ,\n              and   be the variables corresponding to the first three columns,\n              respectively, of this augmented matrix,\n              then we see that  ,\n               , and  .\n              So   can be written as a linear combination of  ,\n               , and   as\n               .\n              Since  ,\n              it follows that  .\n             \n              Can   be \n              written as a linear combination of the vectors  ,\n               ,  ?\n              If so, find such a linear combination.\n              If not, justify your response.\n              What does your result tell us about the relationship between   and  ?\n              Explain.\n             \n              To determine if   is a linear combination of  ,\n               , and  ,\n              we row reduced the augmented matrix  .\n              The reduced row echelon form of the matrix   is\n               .\n              The last row shows that the system with this as augmented matrix is inconsistent.\n              So   cannot be written as a linear combination of  ,\n               , and  .\n              Since  ,\n              it follows that  .\n             \n              What relationship, if any,\n              exists between   and  ?\n              Explain.\n             \n              We know that  .\n              Now   contains the linear combinations of vectors in  ,\n              which are all linear combinations of the vectors  ,\n               , and  .\n              Thus,   is just the set of linear combinations of  ,\n               , and  .\n              We conclude that  .\n             "
},
{
  "id": "sec_vec_rep_exer",
  "level": "2",
  "url": "chap_vector_representation.html#sec_vec_rep_exer",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Given vectors   and \n          in  ,\n        determine if   \n        can be written as a linear combination of   and  .\n        If so, determine the weights of   and   which produce  .\n       \n         \n       "
},
{
  "id": "exercise-31",
  "level": "2",
  "url": "chap_vector_representation.html#exercise-31",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Given vectors  ,\n          and   in  ,\n        determine if   can be written as a linear combination of  ,\n          and  .\n        If so, determine the weights of  ,\n          and   which produce  .\n        Reflect on the result.\n        Is there anything special about the given vectors  ,\n          and  ?\n       "
},
{
  "id": "exercise-32",
  "level": "2",
  "url": "chap_vector_representation.html#exercise-32",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Let   and   in  .\n        Determine which vectors   in   can be written as a linear combination of   and  .\n        Does the set of  's include the 0 vector?\n        If so, determine which weights in the linear combination produce the 0 vector.\n        If not, explain why not.\n       \n        Only when  .\n         \n       "
},
{
  "id": "exercise-33",
  "level": "2",
  "url": "chap_vector_representation.html#exercise-33",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Consider vectors   and \n          in  .\n       \n              Find four specific linear combinations of the vectors   and  .\n             \n              Explain why the zero vector must be a linear combination of   and  .\n             \n              What kind of geometric shape does the set of all linear combinations of \n                and   have in  ?\n             \n              Can we obtain any vector in   as a linear combination of   \n              and  ?\n              Explain.\n             "
},
{
  "id": "ex_1_d_acid",
  "level": "2",
  "url": "chap_vector_representation.html#ex_1_d_acid",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Suppose we have two different water-benzene-acetic acid solutions,\n        one with 40% water, 50% benzene and 10% acetic acid,\n        the other with 52% water, 42% benzene and 6% acid.\n       \n              An experiment we want to conduct requires a solution with 43% water,\n              48% benzene and 9% acid.\n              Representing each acid solution as a vector,\n              determine if we can we make this new acid solution by mixing the first two solutions,\n              or do we have to run to the chemical solutions market to get the solution we want?\n             \n              We cannot make the desired solution.\n             \n              Using the water-benzene-acetic acid solutions in the previous problem,\n              can we obtain an acid solution which contains 50% water, 43% benzene and 7% acid?\n             \n              We can make the desired chemical solution with\n                of solution   for every\n                of solution  .\n             \n              Determine the relationship between the percentages of water, benzene,\n              and acid in solutions which can be obtained by mixing the two given \n              water-benzene-acetic acid solutions above.\n             \n              If   can be made from our original two chemical solutions,\n              then  .\n             "
},
{
  "id": "exercise-35",
  "level": "2",
  "url": "chap_vector_representation.html#exercise-35",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Is the vector   in  ?\n        Justify your answer.\n       "
},
{
  "id": "exercise-36",
  "level": "2",
  "url": "chap_vector_representation.html#exercise-36",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        Describe geometrically each of the following sets.\n       \n                in  \n             \n                is the line in   through the origin and the point  .\n             \n                in  \n             \n                is the plane in   through the origin and the points   and  .\n             "
},
{
  "id": "exercise-37",
  "level": "2",
  "url": "chap_vector_representation.html#exercise-37",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Consider the linear system\n         \n       \n              Find the general solution to this system.\n             \n              Find two specific vectors   and   so that the solution set to this system is  .\n             "
},
{
  "id": "exercise-38",
  "level": "2",
  "url": "chap_vector_representation.html#exercise-38",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        Answer the following question as yes or no.\n        Verify your answer.\n        If   and   are vectors in  ,\n        then   is in  .\n       \n        Yes.\n       "
},
{
  "id": "exercise-39",
  "level": "2",
  "url": "chap_vector_representation.html#exercise-39",
  "type": "Exercise",
  "number": "10",
  "title": "",
  "body": "\n        Let  ,  ,\n        and   be vectors in   and let   and   be scalars.\n        Verify  .\n        That is, show that\n       \n               \n             \n               \n             \n              The vector   has the property that  .\n             \n               .\n             \n               \n             \n               \n             \n               \n             \n               .\n             "
},
{
  "id": "exercise-40",
  "level": "2",
  "url": "chap_vector_representation.html#exercise-40",
  "type": "Exercise",
  "number": "11",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              A vector in  ,\n              i.e. a two-dimensional vector,\n              is also a vector in  .\n             \n              F\n             True\/False \n              Any vector in   can be visualized as a vector in   by \n              adding a 0 as the last coordinate.\n             True\/False \n              The zero vector is a scalar multiple of any other vector\n              (of the same size).\n             \n              T\n             True\/False \n              The zero vector cannot be a linear combination of two non-zero vectors.\n             True\/False \n              Given two vectors   and  ,\n              the vector   is a linear combination of   and  .\n             \n              T\n             True\/False \n              Given any two non-zero vectors   and   in  ,\n              we can obtain any vector in   as a linear combination of   and  .\n             True\/False \n              Given any two distinct vectors   and   in  ,\n              we can obtain any vector in   as a linear combination of   and  .\n             \n              F\n             True\/False \n              If   can be expressed as a linear combination of   and  ,\n              then   can also be expressed as a linear combination of   and  .\n             True\/False \n              The span of any two vectors neither of which is a multiple of the other can be visualized as a plane through the origin.\n             \n              T\n             True\/False \n              Given any vector,\n              the collection of all linear combinations of this vector can be visualized as a line through the origin.\n             True\/False \n              The span of any collection of vectors includes the   vector.\n             \n              T\n             True\/False \n              If the span of   and   is all of  ,\n              then so is the span of   and  .\n             True\/False \n              If the span of\n                and   is all of  ,\n              then so is the span of   and  .\n             \n              F\n             "
},
{
  "id": "F_knight_2",
  "level": "2",
  "url": "chap_vector_representation.html#F_knight_2",
  "type": "Figure",
  "number": "4.13",
  "title": "",
  "body": "Initial knight placement and moves. "
},
{
  "id": "act_knight_1",
  "level": "2",
  "url": "chap_vector_representation.html#act_knight_1",
  "type": "Project Activity",
  "number": "4.6",
  "title": "",
  "body": "\n            Explain why the vector equation\n             \n            will tell us if it is possible for the knight to move from its initial \n            position at   to the position (5,2).\n           \n            Find all solutions, if any, to the system from part (a).\n            If it is possible to find a sequence of moves that take the knight from its \n            initial position to position  ,\n            find weights  ,  ,\n             , and   to accomplish this move.\n            (Be careful   we must have solutions in which  ,\n             ,  , and   are integers.)\n            Is there more than one sequence of possible moves?\n            You can check your solution with the GeoGebra app at \n             .\n           "
},
{
  "id": "act_knight_2",
  "level": "2",
  "url": "chap_vector_representation.html#act_knight_2",
  "type": "Project Activity",
  "number": "4.7",
  "title": "",
  "body": "\n      Given any position  ,\n      we want to know if our knight can move from its start position   \n      to position  .\n     \n            Write a vector equation whose solution will tell us if it is possible for our knight to move from its start position   to position  .\n           \n            Show that the solution to part (a) can be written in the form\n             \n           "
},
{
  "id": "act_knight_3",
  "level": "2",
  "url": "chap_vector_representation.html#act_knight_3",
  "type": "Project Activity",
  "number": "4.8",
  "title": "",
  "body": "\n      Complete the analysis as above to determine if there are integer solutions to our knight's \n      move system in the following cases.\n     \n              odd and   odd\n           \n              even and   even\n           \n              even and   odd.\n           "
},
{
  "id": "chap_matrix_vector",
  "level": "1",
  "url": "chap_matrix_vector.html",
  "type": "Section",
  "number": "5",
  "title": "The Matrix-Vector Form of a Linear System",
  "body": "The Matrix-Vector Form of a Linear System \n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            How and when is the matrix-vector product   defined?\n           \n         \n           \n            How can a system of linear equations be written in matrix-vector form?\n           \n         \n           \n            How can we tell if the system\n              is consistent for a given vector  ?\n           \n         \n           \n            How can we tell if the system\n              is consistent for every vector  ?\n           \n         \n           \n            What is a homogeneous system?\n            What can we say about the solution set to a homogeneous system?\n           \n         \n           \n            What must be true about pivots in the coefficient matrix   in order for the \n            homogeneous system\n              to have a unique solution?\n           \n         \n           \n            How are the solutions to the nonhomogeneous system\n              related to the solutions of the corresponding homogeneous \n            system  ?\n           \n         Application: Modeling an Economy \n    An economy is a very complex system.\n    An economy is not a well-defined object,\n    there are many factors that influence an economy,\n    and it is often unclear how the factors influence each other.\n    Mathematical modeling plays an important role in attempting to understand an economy.\n   \n    In 1941 Wassily Leontief developed the first empirical model of a national economy.\n    Around 1949 Leontief used data from the U.S. Bureau of Labor Statistics to divide the \n    U.S. economy into 500 sectors.\n    He then set up linear equations for each sector.\n    This system was too large for the computers at the time to solve,\n    so he then aggregated the information into 42 sectors.\n    The Harvard Mark II computer was used to solve this system,\n    one of the first significant uses of computers for mathematical modeling.\n    Leontief won the 1973 Nobel Prize in economics for his work.\n   input-output Introduction \n    There is another useful way to represent a system of linear equations using a matrix-vector \n    product that we investigate in this section.\n    To understand how this product comes about,\n    recall that we can represent the linear system\n     \n    as a vector equation as\n     .\n   matrix-vector product matrix-vector product \n    With this definition,\n    the vector equation in   can be expressed as a matrix-vector equation as\n     .\n   matrix-vector form \n    We can use the above definition of the matrix-vector product as a linear combination with \n    any matrix and any vector,\n    as long as it is meaningful to use the entries in the vector as weights for the columns \n    of the matrix.\n    For example,\n    for   and \n     ,\n    then we can define   to be the linear combination of the columns of   \n    with weights 3 and 4:\n     .\n   \n    However, note that if   had three entries,\n    this definition would not make sense since we do not have three columns in  .\n    In those cases, we say   is not defined.\n    We will later see that this definition can be generalized to matrix-matrix products,\n    by treating the vector as a special case of a matrix with one column.\n   \n            Write the vector equation\n             \n            in matrix-vector form.\n            Note that this is the vector equation whose augmented matrix representation was given in Problem 2 in  .\n            Compare your matrix   and the right hand side vector to the augmented matrix.\n            Do not solve the system.\n           \n            Given the matrix-vector equation\n             \n            represent the system corresponding to this equation.\n            Note that this should correspond to the system (or an equivalent system where an equation might be multiplied by  ) in Problem 1 of  .\n           \n            Find the indicated matrix-vector products, if possible.\n            Express as one vector.\n           \n                   \n                 \n                   \n                 \n                   \n                 nonhomogeneous system homogeneous system parametric vector form nonhomogeneous homogeneous parametric vector form \n                  Find the general solution to the homogeneous system\n                   \n                  with   and   as in   and compare \n                  it to the solution to the nonhomogeneous system in  .\n                  What do you notice?\n                 \n                  Find the general solution to the nonhomogeneous system\n                   \n                  with\n                   .\n                  and express it in parametric vector form.\n                  Then find the general solution to the corresponding homogeneous \n                  system and express it in parametric vector form.\n                  How are the two solution sets related?\n                 \n                  Make a conjecture about the relationship between the solutions to a \n                  consistent nonhomogeneous system\n                    and the corresponding homogeneous system  .\n                  Be as specific as possible.\n                 The Matrix-Vector Product \n    The matrix-vector product we defined in  \n    for a specific example generalizes in a very straightforward manner,\n    and provides a convenient way to represent a system of linear equations of any size \n    using matrices and vectors.\n    In addition to providing us with an algebraic approach to solving systems via matrices \n    and vectors   leading to a powerful geometric relationship between solution sets of \n    homogeneous and non-homogeneous systems   this representation allows us to think \n    of a linear system from a dynamic perspective,\n    as we will see later in the section on matrix transformations.\n   rows columns size \n    We can also write   in terms of its columns,\n     , as\n     .\n   \n    In general, the product of a matrix with a vector is defined as follows.\n   matrix-vector product matrix-vector product Important Note \n    The matrix-vector product   is defined only when the number of entries of \n    the vector   is equal to the number of columns of the matrix  .\n    That is, if   is an   matrix,\n    then   is defined only if   is a column vector with   entries.\n   The Matrix-Vector Form of a Linear System \n      As we saw in  ,\n      the matrix-vector product provides us with a short hand way of representing a \n      system of linear equations.\n      In general, every linear system can be written in matrix-vector form as follows.\n     \n      The linear system\n       \n      of   equations in   unknowns can be written in matrix-vector form as \n       , where\n       .\n     \n      This general system can also be written in the vector form\n       .\n     \n      With this last representation,\n      we now have four different ways to represent a system of linear equations\n      (as a system of linear equations, as an augmented matrix,\n      in vector equation form, and in matrix-vector equation form),\n      and it is important to be able to translate between them.\n      As an example, the system\n\n       \n\n      from the introduction to this section has corresponding augmented matrix\n       ,\n      is expressed in vector form as\n       ,\n      and has matrix-vector form\n       .\n     \n        In this activity,\n        we will use the equivalence of the different representations of a system to \n        make useful observations about when a system represented as   has a solution.\n       \n              Consider the system\n               .\n              Write the matrix-vector product on the left side of this equation as a linear combination of the columns of the coefficient matrix.\n              Find weights that make the vector\n                a linear combination of the columns of the coefficient matrix.\n             \n              From this point on we consider the general case where   is an   matrix.\n              Use the vector equation representation to explain why the system\n                has a solution if and only if   is a linear combination of the columns of  .\n              (Note that  if and only if  is an expression to mean that if one side of the expression is true,\n              then the other side must also be true.)\n              (Hint: Compare to what you did in part (a).)\n             \n              Use part (b) and the definition of span to explain why the system\n                has a solution if and only if the vector   is in the span of the columns of  .\n             \n              Use part (c) to explain why the system\n                always has a solution for any vector   in   if and only if the span of the columns of   is all of  .\n             \n              Use the augmented matrix representation and the criterion for a consistent system to explain why the system\n                is consistent for all vectors   if and only if   has a pivot position in every row.\n             \n      We summarize our observations from the above activity in the following theorem.\n     \n          Let   be an   matrix.\n          The following statements are equivalent:\n           \n               \n                The matrix equation   has a solution for every vector \n                  in  .\n               \n             \n               \n                Every vector   in   can be written as a linear combination \n                of the columns of  .\n               \n             \n               \n                The span of the columns of   is  .\n               \n             \n               \n                The matrix   has a pivot position in each row.\n               \n             \n         \n\n      In the future,\n      if we need to determine whether a system has a solution for every  ,\n      we can refer to this theorem without having to argue our reasoning from scratch.\n     Properties of the Matrix-Vector Product \n      As we have done before, we have a new operation\n      (the matrix-vector product),\n      so we should wonder what properties it has.\n     \n        In this activity,\n        we consider whether the matrix-vector product distributes vector addition.\n        In other words: Is   equal to  ?\n       \n        We work with arbitrary vectors\n          in   and an arbitrary matrix   with 3 columns\n        (so that   and   are defined)\n        to simplify notation.\n        Let  \n        (note that each   represents a column of  ),\n         ,\n        and  .\n        Use the definition of the matrix-vector product along with the properties of vector \n        operations to show that\n         .\n       \n      Similar arguments using the definition of matrix-vector product along with the properties \n      of vector operations can be used to show the following theorem:\n     \n          Let   be an   matrix,\n            and     vectors, and   a scalar.\n          Then\n           \n               \n                 \n               \n             \n               \n                 \n               \n             \n         Homogeneous and Nonhomogeneous Systems nonhomogeneous homogeneous \n      In this activity we will consider the relationship between the solution sets of \n      nonhomogeneous systems and those of the corresponding homogeneous systems.\n     \n            Find the solution sets of the system\n             \n            where\n             \n            and the corresponding homogeneous system (i.e. where we replace   with \n             .)\n           \n            Find the solution sets of the system\n             \n            where\n             \n            and the corresponding homogeneous system.\n           \n            What are the similarities\/differences between solutions of the nonhomogeneous \n            system and its homogeneous counterpart?\n           parametric vector form \n    From this representation,\n    we see that the solution set is a line through the origin\n    (formed by multiples of  )\n    shifted by the added vector  .\n    The solution to the homogeneous system on the other does not have the shift.\n   \n    Algebraically,\n    we see that every solution to the nonhomogeneous system\n      can be written in the form  ,\n    where   is a particular solution to\n      and   is a solution to the corresponding homogeneous \n    system  .\n   \n    To understand why this  always  happens,\n    we will verify the result algebraically for an arbitrary   and  .\n    Assuming that   is a particular solution to the nonhomogeneous system\n     , we need to show that:\n     \n         \n          if   is an arbitrary solution to the nonhomogeneous system,\n          then  ,\n          where   is some solution to the homogeneous system  , and\n         \n       \n         \n          if   is an arbitrary solution to the homogeneous system,\n          then   is a solution to the nonhomogeneous system.\n         \n       \n   \n    To verify the first condition,\n    suppose that   is a solution to the nonhomogeneous system  .\n    Since we want  ,\n    we need to verify that   is a solution for the homogeneous system so that \n    we can assign  .\n    Note that\n     ,\n    using the distributive property of matrix-vector product over vector addition.\n    Hence   is of the form\n      with  .\n   \n    To verify the second condition,\n    consider a vector of the form  ,\n    where   is a homogeneous solution.\n    We have\n     ,\n    and so   is a solution to  .\n   \n    Our work above proves the following theorem.\n   \n        Suppose the equation   is consistent for some   and \n          is a solution.\n        Then the solution set of   consists of all vectors of the form\n          where   is a solution to  .\n       The Geometry of Solutions to the Homogeneous System \n    There is a simple geometric interpretation to the solution set of the homogeneous system\n      based on the number of free variables that imposes a geometry on the \n    solution set of the corresponding nonhomogeneous system  \n    (when consistent)\n    due to  .\n   \n      In this activity we consider geometric interpretations of the solution sets of homogeneous \n      and nonhomogeneous systems.\n     \n            Consider the system   where\n              and  .\n            The general solution to this system has the form  ,\n            where   is any real number.\n           \n                  Let  .\n                  What does the set of all vectors of the form   look like geometrically?\n                  Draw a picture in   to illustrate.\n                  (Recall that we refer to all the vectors of the form   simply as  .)\n                 \n                  Let  .\n                  What effect does adding the vector   to each vector in\n                    have on the geometry of  ?\n                  Finally, what does this mean about the geometry of the solution set to the nonhomogeneous system  ?\n                 \n            Consider the system   where\n              and  .\n            The general solution to this system has the form \n             ,\n            where   are any real numbers.\n           \n                  Let  .\n                  Use our results from Section 4 to determine the geometric shape of \n                   ,\n                  the set of all vectors of the form \n                   ,\n                  where   are any real numbers.\n                 \n                  Let  .\n                  What's the geometric effect of adding the vector   to each vector in \n                   ?\n                  Finally, what does this mean about the geometry of the solution set to the \n                  nonhomogeneous system  ?\n                 \n    Our work in the above activity shows the geometric shape of the solution set of a \n    consistent nonhomogeneous system is the same as the geometric shape of the solution set \n    of the corresponding homogeneous system.\n    The only difference between the two solution sets is that one is a shifted version of the other.\n   Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        We now have several different ways to represent a system of linear equations.\n        Rewrite the system in an equivalent form\n         \n       \n              as an augmented matrix\n             \n              The augmented matrix for this system is\n               .\n             \n              as an equation involving a linear combination of vectors\n             \n              If we make vectors from the columns of the augmented matrix,\n              we can write this system in vector form as\n               .\n             \n              using a matrix-vector product\n             \n              The coefficient matrix for this system is  ,\n              and the matrix-vector form of the system is\n               .\n             \n              Then solve the system.\n             \n              Using technology,\n              we find that the reduced row echelon form of the augmented matrix for this system is\n               .\n             \n              So the solution to this system is  ,\n               ,  , and  .\n             \n        Consider the homogeneous system\n         \n       \n              Find the general solution to this homogeneous system and express the system in \n              parametric vector form.\n             \n              The augmented matrix of the homogeneous system is\n               ,\n              and the reduced row echelon form of this augmented matrix is\n               .\n              Since there is no corresponding equation of the form   for a nonzero \n              constant  ,\n              this system is consistent.\n              The third column contains no pivot,\n              so the variable   is free,\n                and  .\n              In parametric vector form the general solution to the homogeneous system is\n               .\n             \n              Let  ,\n              and let  .\n              Show that   is a \n              solution to the non-homogeneous system  .\n             \n              Since\n               ,\n              we conclude that   \n              is a solution to the non-homogeneous system  .\n             \n              Use the results from part (a) and (b) to write the parametric vector form of the \n              general solution to the non-homogeneous system  .\n              (Do this without directly solving the system  .)\n             \n              We know that every solution to the non-homogeneous system\n                has the form of the general solution to the homogeneous \n              system plus a particular solution to the non-homogeneous system.\n              Combining the results of (a) and (b) we see that the general solution to the \n              non-homogeneous system   is\n               ,\n              where   can be any real number.\n             \n              Describe what the general solution to the homogeneous system\n                and the general solution to the non-homogeneous system\n                look like geometrically.\n             \n              The solution to the homogeneous system\n                is the span of the vector \n               .\n              Geometrically,\n              this set of points is a line through the origin and the point   in \n               .\n              The solution to the non-homogeneous system\n                is the translation of the line through the origin and\n                by the vector  .\n              In other words, the solution to the non-homogeneous system\n                is the line in   through the points\n                and  .\n             Summary \n       \n        If   is an\n          matrix with columns  ,\n         ,  ,  ,\n        and if   \n        is a vector in  ,\n        then the matrix-vector product   is defined to be the linear combination of \n        the columns of   with corresponding weights from     that is\n         .\n       \n     \n       \n        A linear system\n         \n        can be written in matrix form as\n         ,\n        where\n         .\n       \n     \n       \n        The matrix equation   has a solution if and only if   is a \n        linear combination of the columns of  .\n       \n     \n       \n        The system   is consistent for every vector   if every row of \n          contains a pivot.\n       \n     \n       \n        A homogeneous system is a system of the form\n          for some   matrix  .\n        Since the zero vector in   satisfies  ,\n        a homogeneous system is always consistent.\n       \n     \n       \n        A homogeneous system can have one or infinitely many different solutions.\n        The homogeneous system   has exactly one solution if and only if \n        each column of   is a pivot column.\n       \n     \n       \n        The solutions to the consistent nonhomogeneous system\n          have the form  ,\n        where   is a particular solution to the nonhomogeneous system\n          and   is a solution to the homogeneous system \n         .\n        In other words,\n        the solution space to a consistent nonhomogeneous system\n          is a translation of the solution space of the homogeneous system\n          by a particular solution to the nonhomogeneous system.\n       \n     \n    Finally, we argued an important theorem.\n   \n    Let   be an   matrix.\n    The following statements are equivalent.\n     \n         \n          The matrix equation   has a solution for every vector \n            in  .\n         \n       \n         \n          Every vector   in   can be written as a linear combination of \n          the columns of  .\n         \n       \n         \n          The span of the columns of   is  .\n         \n       \n         \n          The matrix   has a pivot position in each row.\n         \n       \n   \n    We will continue to add to this theorem,\n    so it is a good idea for you to begin now to remember the equivalent conditions of this theorem.\n   \n        Write the system\n         \n        in matrix-vector form.\n        Explicitly identify the coefficient matrix and the vector of constants.\n       \n        The matrix-vector form of the system is  ,\n        where  .\n       \n        Write the linear combination\n         \n        as a matrix-vector product.\n       \n        Represent the following matrix-vector equation as a linear system and find its solution.\n         \n       \n        The system corresponding to this matrix-vector equation is\n         \n       \n        The solution to the system is\n         .\n       \n        Represent the following matrix-vector equation as a linear system and find its solution.\n         \n       scalar product dot product \n        Both methods give\n         .\n       \n        Find the value of   such that\n         \n        where  's represent unknown values.\n       \n        Suppose we have\n         \n        where  's represent unknown values.\n       \n              In order to find the value of  ,\n              which of the  's do we need to know?\n              Why?\n             \n               \n             \n              Suppose the  (s) that we need to know is(are) equal to 9.\n              What is the value of  ?\n             \n              If  , then  .\n             \n        Suppose we are given\n         \n        for an unknown   and two unknown vectors   in  .\n        Using matrix-vector product properties,\n        evaluate   where  .\n       \n        Suppose we are given\n         .\n        After expressing   \n        as a linear combination of\n          and \n         ,\n        use the matrix-vector product properties to determine \n         .\n       \n         .\n       \n            The non-homogeneous system (with unknown constants   and  )\n             \n            has a solution which lies on the  -axis (i.e.\n             ).\n            Find this solution.\n           \n            If the corresponding homogeneous system\n             \n            has its general solution expressed in parametric vector form as \n             ,\n            find the general solution for the non-homogeneous system using your answer to part (a).\n           \n            Find the conditions on   and   that make the system from (a) have the \n            general solution you found in (b).\n           \n        Find the general solution to the non-homogeneous system\n         \n        Using the parametric vector form of the solutions,\n        determine what the solution set to this non-homogeneous system looks like geometrically.\n        Be as specific as possible.\n        (Include information such as whether the solution set is a point, a line,\n        or a plane,\n        etc.; whether the solution set passes through the origin or is shifted from the \n        origin in a specific direction by a specific number of units;\n        and how the solution is related to the corresponding homogeneous system.)\n       \n        The general solution to the system is\n         .\n       \n        The solution is the plane in   through the points  ,\n         , and  .\n       \n        Come up with an example of a\n          matrix   for which the solution set of   is a line,\n        and a   matrix   for which the solution set of   \n        is a plane.\n       \n        Suppose we have three vectors\n          and   satisfying  .\n        Let   be the matrix with vectors\n          and   as the columns in that order.\n        Find a non-zero   such that\n          using this information.\n       \n         \n       \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If the system\n                has infinitely many solutions,\n              then so does the system   for  any \n              right-hand-side  .\n             \n              F\n             True\/False \n              If   is a solution for\n                and   is a solution for  ,\n              then   is a solution for  .\n             True\/False \n              If an   matrix   has a pivot in every row,\n              then the equation   has a unique solution for every  .\n             \n              T\n             True\/False \n              If an   matrix   has a pivot in every row,\n              then the equation   has a solution for every  .\n             True\/False \n              If   and   are row equivalent matrices and the columns of   span  ,\n              then so do the columns of  .\n             \n              T\n             True\/False \n              All homogeneous systems have either a unique solution or infinitely many solutions.\n             True\/False \n              If a linear system is not homogeneous,\n              then the solution set does not include the origin.\n             \n              T\n             True\/False \n              If a solution set of a linear system does not include the origin,\n              the system is not homogeneous.\n             True\/False \n              If the system\n                has a unique solution for some  ,\n              then the homogeneous system has only the trivial solution.\n             \n              T\n             True\/False \n              If   is a   matrix,\n              then the homogeneous equation\n                has non-trivial solutions.\n             True\/False \n              If   is a   matrix,\n              then the homogeneous equation\n                has non-trivial solutions.\n             \n              T\n             Project: Input-Output Models input-output models \n    There are two basic types of input-output models:\n    closed and open.\n    The closed model assumes that all goods produced are consumed within the economy   \n    no trading takes place with outside entities.\n    In the open model,\n    goods produced within the economy can be traded outside the economy.\n   \n    To work with a closed model, we use an example\n    (from  Input-Output Economics  by Wassily Leontief).\n    Assume a simple three-sector economy consisting of agriculture\n    (growing wheat),\n    manufacturing\n    (producing cloth),\n    and households\n    (supplying labor).\n    Each sector of the economy relies on goods from the other sectors to operate (e.g., \n    people must eat to work and need to be clothed).\n    To model the interactions between the sectors,\n    we consider how many units of product is needed as input from one sector to another to \n    produce one unit of product in the second sector.\n    For example, assume the following:\n     \n         \n          to produce one unit\n          (say dollars worth)\n          of agricultural goods requires 25% of a unit of agricultural output, \n          28% of a unit of manufacturing output,\n          and 27% of a unit of household output;\n         \n       \n         \n          to produce one unit of manufactured goods requires 20% of a unit of agricultural \n          output, 60% of a unit of manufacturing output,\n          and 60% of a unit of household output;\n         \n       \n         \n          to produce one unit of household goods requires 55% of a unit of agricultural \n          output, 12% of a unit of manufacturing output,\n          and 13% of a unit of household output.\n         \n       \n   \n    These assumptions are summarized in  .\n   Summary of simple three sector economy into\\from Agriculture Manufacture Households Agriculture 0.25 0.28 0.27 Manufacture 0.20 0.60 0.60 Households 0.55 0.12 0.13 closed open \n    The economist's goal is to determine what level of production in each section meets the \n    following requirements:\n     \n         \n          the production from each sector meets the needs of all of the sectors and\n         \n       \n         \n          there is no overproduction.\n         \n       \n   \n      We can use techniques from linear algebra to determine the levels of production that \n      precisely meet the two goals of the economist.\n     production vector \n            Find the augmented matrix for the system of linear equations that represent \n            production of the three sectors from part (a),\n            and then solve the system to find the production levels that meet the economist's \n            two goals.\n           \n            Suppose the production level of the household sector is 200 million units\n            (dollars).\n            Find the production levels of the agricultural and manufacturing sectors that \n            meet the economist's two goals.\n           consumption \n    In our example,\n    if we let  ,\n    then we can write the equations that guarantee that the production levels satisfy the \n    two economists' goal in matrix form as\n     .\n   steady state \n      Is there a steady state solution for the closed system of Agriculture, Manufacturing,\n      and Households?\n      If so, find the general steady state solution.\n      If no, explain why.\n     closed open model \n    A summary of this information is in  .\n    Assume the units are measured in dollars.\n   Summary of four sector economy into\\from Petroleum Textiles Transportation Chemicals Petroleum 0.10 0.00 0.20 0.40 Textiles 0.40 0.10 0.15 0.30 Transportation 0.60 0.00 0.10 0.25 Chemicals 0.20 0.10 0.30 0.20 open sector final demand vector exchange \n            Suppose the final demand vector in our four sector economy is \n             .\n            Find the production levels that satisfy our system.\n           \n            Does this economy defined by the exchange matrix   have production levels \n            that exactly meet internal and external demands regardless of the external demands?\n            That is, does the system of equations\n             \n            have a solution regardless of the values of  ,\n             ,  , and  ?\n            Explain.\n           "
},
{
  "id": "objectives-5",
  "level": "2",
  "url": "chap_matrix_vector.html#objectives-5",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            How and when is the matrix-vector product   defined?\n           \n         \n           \n            How can a system of linear equations be written in matrix-vector form?\n           \n         \n           \n            How can we tell if the system\n              is consistent for a given vector  ?\n           \n         \n           \n            How can we tell if the system\n              is consistent for every vector  ?\n           \n         \n           \n            What is a homogeneous system?\n            What can we say about the solution set to a homogeneous system?\n           \n         \n           \n            What must be true about pivots in the coefficient matrix   in order for the \n            homogeneous system\n              to have a unique solution?\n           \n         \n           \n            How are the solutions to the nonhomogeneous system\n              related to the solutions of the corresponding homogeneous \n            system  ?\n           \n         "
},
{
  "id": "p-779",
  "level": "2",
  "url": "chap_matrix_vector.html#p-779",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "input-output "
},
{
  "id": "p-781",
  "level": "2",
  "url": "chap_matrix_vector.html#p-781",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "matrix-vector product matrix-vector product "
},
{
  "id": "p-783",
  "level": "2",
  "url": "chap_matrix_vector.html#p-783",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "matrix-vector form "
},
{
  "id": "pa_1_e",
  "level": "2",
  "url": "chap_matrix_vector.html#pa_1_e",
  "type": "Preview Activity",
  "number": "5.1",
  "title": "",
  "body": "\n            Write the vector equation\n             \n            in matrix-vector form.\n            Note that this is the vector equation whose augmented matrix representation was given in Problem 2 in  .\n            Compare your matrix   and the right hand side vector to the augmented matrix.\n            Do not solve the system.\n           \n            Given the matrix-vector equation\n             \n            represent the system corresponding to this equation.\n            Note that this should correspond to the system (or an equivalent system where an equation might be multiplied by  ) in Problem 1 of  .\n           \n            Find the indicated matrix-vector products, if possible.\n            Express as one vector.\n           \n                   \n                 \n                   \n                 \n                   \n                 nonhomogeneous system homogeneous system parametric vector form nonhomogeneous homogeneous parametric vector form \n                  Find the general solution to the homogeneous system\n                   \n                  with   and   as in   and compare \n                  it to the solution to the nonhomogeneous system in  .\n                  What do you notice?\n                 \n                  Find the general solution to the nonhomogeneous system\n                   \n                  with\n                   .\n                  and express it in parametric vector form.\n                  Then find the general solution to the corresponding homogeneous \n                  system and express it in parametric vector form.\n                  How are the two solution sets related?\n                 \n                  Make a conjecture about the relationship between the solutions to a \n                  consistent nonhomogeneous system\n                    and the corresponding homogeneous system  .\n                  Be as specific as possible.\n                 "
},
{
  "id": "p-797",
  "level": "2",
  "url": "chap_matrix_vector.html#p-797",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "rows columns size "
},
{
  "id": "definition-12",
  "level": "2",
  "url": "chap_matrix_vector.html#definition-12",
  "type": "Definition",
  "number": "5.1",
  "title": "",
  "body": "matrix-vector product matrix-vector product "
},
{
  "id": "activity-18",
  "level": "2",
  "url": "chap_matrix_vector.html#activity-18",
  "type": "Activity",
  "number": "5.2",
  "title": "",
  "body": "\n        In this activity,\n        we will use the equivalence of the different representations of a system to \n        make useful observations about when a system represented as   has a solution.\n       \n              Consider the system\n               .\n              Write the matrix-vector product on the left side of this equation as a linear combination of the columns of the coefficient matrix.\n              Find weights that make the vector\n                a linear combination of the columns of the coefficient matrix.\n             \n              From this point on we consider the general case where   is an   matrix.\n              Use the vector equation representation to explain why the system\n                has a solution if and only if   is a linear combination of the columns of  .\n              (Note that  if and only if  is an expression to mean that if one side of the expression is true,\n              then the other side must also be true.)\n              (Hint: Compare to what you did in part (a).)\n             \n              Use part (b) and the definition of span to explain why the system\n                has a solution if and only if the vector   is in the span of the columns of  .\n             \n              Use part (c) to explain why the system\n                always has a solution for any vector   in   if and only if the span of the columns of   is all of  .\n             \n              Use the augmented matrix representation and the criterion for a consistent system to explain why the system\n                is consistent for all vectors   if and only if   has a pivot position in every row.\n             "
},
{
  "id": "thm_nm_mtx",
  "level": "2",
  "url": "chap_matrix_vector.html#thm_nm_mtx",
  "type": "Theorem",
  "number": "5.2",
  "title": "",
  "body": "\n          Let   be an   matrix.\n          The following statements are equivalent:\n           \n               \n                The matrix equation   has a solution for every vector \n                  in  .\n               \n             \n               \n                Every vector   in   can be written as a linear combination \n                of the columns of  .\n               \n             \n               \n                The span of the columns of   is  .\n               \n             \n               \n                The matrix   has a pivot position in each row.\n               \n             \n         "
},
{
  "id": "act_A1_4_8",
  "level": "2",
  "url": "chap_matrix_vector.html#act_A1_4_8",
  "type": "Activity",
  "number": "5.3",
  "title": "",
  "body": "\n        In this activity,\n        we consider whether the matrix-vector product distributes vector addition.\n        In other words: Is   equal to  ?\n       \n        We work with arbitrary vectors\n          in   and an arbitrary matrix   with 3 columns\n        (so that   and   are defined)\n        to simplify notation.\n        Let  \n        (note that each   represents a column of  ),\n         ,\n        and  .\n        Use the definition of the matrix-vector product along with the properties of vector \n        operations to show that\n         .\n       "
},
{
  "id": "thm_IMT_1_e",
  "level": "2",
  "url": "chap_matrix_vector.html#thm_IMT_1_e",
  "type": "Theorem",
  "number": "5.3",
  "title": "",
  "body": "\n          Let   be an   matrix,\n            and     vectors, and   a scalar.\n          Then\n           \n               \n                 \n               \n             \n               \n                 \n               \n             \n         "
},
{
  "id": "p-825",
  "level": "2",
  "url": "chap_matrix_vector.html#p-825",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "nonhomogeneous homogeneous "
},
{
  "id": "activity-20",
  "level": "2",
  "url": "chap_matrix_vector.html#activity-20",
  "type": "Activity",
  "number": "5.4",
  "title": "",
  "body": "\n      In this activity we will consider the relationship between the solution sets of \n      nonhomogeneous systems and those of the corresponding homogeneous systems.\n     \n            Find the solution sets of the system\n             \n            where\n             \n            and the corresponding homogeneous system (i.e. where we replace   with \n             .)\n           \n            Find the solution sets of the system\n             \n            where\n             \n            and the corresponding homogeneous system.\n           \n            What are the similarities\/differences between solutions of the nonhomogeneous \n            system and its homogeneous counterpart?\n           "
},
{
  "id": "p-830",
  "level": "2",
  "url": "chap_matrix_vector.html#p-830",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "parametric vector form "
},
{
  "id": "thm_1_e_1",
  "level": "2",
  "url": "chap_matrix_vector.html#thm_1_e_1",
  "type": "Theorem",
  "number": "5.4",
  "title": "",
  "body": "\n        Suppose the equation   is consistent for some   and \n          is a solution.\n        Then the solution set of   consists of all vectors of the form\n          where   is a solution to  .\n       "
},
{
  "id": "ex_Homogeneous2",
  "level": "2",
  "url": "chap_matrix_vector.html#ex_Homogeneous2",
  "type": "Activity",
  "number": "5.5",
  "title": "",
  "body": "\n      In this activity we consider geometric interpretations of the solution sets of homogeneous \n      and nonhomogeneous systems.\n     \n            Consider the system   where\n              and  .\n            The general solution to this system has the form  ,\n            where   is any real number.\n           \n                  Let  .\n                  What does the set of all vectors of the form   look like geometrically?\n                  Draw a picture in   to illustrate.\n                  (Recall that we refer to all the vectors of the form   simply as  .)\n                 \n                  Let  .\n                  What effect does adding the vector   to each vector in\n                    have on the geometry of  ?\n                  Finally, what does this mean about the geometry of the solution set to the nonhomogeneous system  ?\n                 \n            Consider the system   where\n              and  .\n            The general solution to this system has the form \n             ,\n            where   are any real numbers.\n           \n                  Let  .\n                  Use our results from Section 4 to determine the geometric shape of \n                   ,\n                  the set of all vectors of the form \n                   ,\n                  where   are any real numbers.\n                 \n                  Let  .\n                  What's the geometric effect of adding the vector   to each vector in \n                   ?\n                  Finally, what does this mean about the geometry of the solution set to the \n                  nonhomogeneous system  ?\n                 "
},
{
  "id": "example-9",
  "level": "2",
  "url": "chap_matrix_vector.html#example-9",
  "type": "Example",
  "number": "5.5",
  "title": "",
  "body": "\n        We now have several different ways to represent a system of linear equations.\n        Rewrite the system in an equivalent form\n         \n       \n              as an augmented matrix\n             \n              The augmented matrix for this system is\n               .\n             \n              as an equation involving a linear combination of vectors\n             \n              If we make vectors from the columns of the augmented matrix,\n              we can write this system in vector form as\n               .\n             \n              using a matrix-vector product\n             \n              The coefficient matrix for this system is  ,\n              and the matrix-vector form of the system is\n               .\n             \n              Then solve the system.\n             \n              Using technology,\n              we find that the reduced row echelon form of the augmented matrix for this system is\n               .\n             \n              So the solution to this system is  ,\n               ,  , and  .\n             "
},
{
  "id": "example-10",
  "level": "2",
  "url": "chap_matrix_vector.html#example-10",
  "type": "Example",
  "number": "5.6",
  "title": "",
  "body": "\n        Consider the homogeneous system\n         \n       \n              Find the general solution to this homogeneous system and express the system in \n              parametric vector form.\n             \n              The augmented matrix of the homogeneous system is\n               ,\n              and the reduced row echelon form of this augmented matrix is\n               .\n              Since there is no corresponding equation of the form   for a nonzero \n              constant  ,\n              this system is consistent.\n              The third column contains no pivot,\n              so the variable   is free,\n                and  .\n              In parametric vector form the general solution to the homogeneous system is\n               .\n             \n              Let  ,\n              and let  .\n              Show that   is a \n              solution to the non-homogeneous system  .\n             \n              Since\n               ,\n              we conclude that   \n              is a solution to the non-homogeneous system  .\n             \n              Use the results from part (a) and (b) to write the parametric vector form of the \n              general solution to the non-homogeneous system  .\n              (Do this without directly solving the system  .)\n             \n              We know that every solution to the non-homogeneous system\n                has the form of the general solution to the homogeneous \n              system plus a particular solution to the non-homogeneous system.\n              Combining the results of (a) and (b) we see that the general solution to the \n              non-homogeneous system   is\n               ,\n              where   can be any real number.\n             \n              Describe what the general solution to the homogeneous system\n                and the general solution to the non-homogeneous system\n                look like geometrically.\n             \n              The solution to the homogeneous system\n                is the span of the vector \n               .\n              Geometrically,\n              this set of points is a line through the origin and the point   in \n               .\n              The solution to the non-homogeneous system\n                is the translation of the line through the origin and\n                by the vector  .\n              In other words, the solution to the non-homogeneous system\n                is the line in   through the points\n                and  .\n             "
},
{
  "id": "exercise-41",
  "level": "2",
  "url": "chap_matrix_vector.html#exercise-41",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Write the system\n         \n        in matrix-vector form.\n        Explicitly identify the coefficient matrix and the vector of constants.\n       \n        The matrix-vector form of the system is  ,\n        where  .\n       "
},
{
  "id": "exercise-42",
  "level": "2",
  "url": "chap_matrix_vector.html#exercise-42",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Write the linear combination\n         \n        as a matrix-vector product.\n       "
},
{
  "id": "exercise-43",
  "level": "2",
  "url": "chap_matrix_vector.html#exercise-43",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Represent the following matrix-vector equation as a linear system and find its solution.\n         \n       \n        The system corresponding to this matrix-vector equation is\n         \n       \n        The solution to the system is\n         .\n       "
},
{
  "id": "exercise-44",
  "level": "2",
  "url": "chap_matrix_vector.html#exercise-44",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Represent the following matrix-vector equation as a linear system and find its solution.\n         \n       "
},
{
  "id": "ex_1_e_scalar_product",
  "level": "2",
  "url": "chap_matrix_vector.html#ex_1_e_scalar_product",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "scalar product dot product \n        Both methods give\n         .\n       "
},
{
  "id": "exercise-46",
  "level": "2",
  "url": "chap_matrix_vector.html#exercise-46",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Find the value of   such that\n         \n        where  's represent unknown values.\n       "
},
{
  "id": "exercise-47",
  "level": "2",
  "url": "chap_matrix_vector.html#exercise-47",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        Suppose we have\n         \n        where  's represent unknown values.\n       \n              In order to find the value of  ,\n              which of the  's do we need to know?\n              Why?\n             \n               \n             \n              Suppose the  (s) that we need to know is(are) equal to 9.\n              What is the value of  ?\n             \n              If  , then  .\n             "
},
{
  "id": "exercise-48",
  "level": "2",
  "url": "chap_matrix_vector.html#exercise-48",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Suppose we are given\n         \n        for an unknown   and two unknown vectors   in  .\n        Using matrix-vector product properties,\n        evaluate   where  .\n       "
},
{
  "id": "exercise-49",
  "level": "2",
  "url": "chap_matrix_vector.html#exercise-49",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        Suppose we are given\n         .\n        After expressing   \n        as a linear combination of\n          and \n         ,\n        use the matrix-vector product properties to determine \n         .\n       \n         .\n       "
},
{
  "id": "exercise-50",
  "level": "2",
  "url": "chap_matrix_vector.html#exercise-50",
  "type": "Exercise",
  "number": "10",
  "title": "",
  "body": "\n            The non-homogeneous system (with unknown constants   and  )\n             \n            has a solution which lies on the  -axis (i.e.\n             ).\n            Find this solution.\n           \n            If the corresponding homogeneous system\n             \n            has its general solution expressed in parametric vector form as \n             ,\n            find the general solution for the non-homogeneous system using your answer to part (a).\n           \n            Find the conditions on   and   that make the system from (a) have the \n            general solution you found in (b).\n           "
},
{
  "id": "exercise-51",
  "level": "2",
  "url": "chap_matrix_vector.html#exercise-51",
  "type": "Exercise",
  "number": "11",
  "title": "",
  "body": "\n        Find the general solution to the non-homogeneous system\n         \n        Using the parametric vector form of the solutions,\n        determine what the solution set to this non-homogeneous system looks like geometrically.\n        Be as specific as possible.\n        (Include information such as whether the solution set is a point, a line,\n        or a plane,\n        etc.; whether the solution set passes through the origin or is shifted from the \n        origin in a specific direction by a specific number of units;\n        and how the solution is related to the corresponding homogeneous system.)\n       \n        The general solution to the system is\n         .\n       \n        The solution is the plane in   through the points  ,\n         , and  .\n       "
},
{
  "id": "exercise-52",
  "level": "2",
  "url": "chap_matrix_vector.html#exercise-52",
  "type": "Exercise",
  "number": "12",
  "title": "",
  "body": "\n        Come up with an example of a\n          matrix   for which the solution set of   is a line,\n        and a   matrix   for which the solution set of   \n        is a plane.\n       "
},
{
  "id": "exercise-53",
  "level": "2",
  "url": "chap_matrix_vector.html#exercise-53",
  "type": "Exercise",
  "number": "13",
  "title": "",
  "body": "\n        Suppose we have three vectors\n          and   satisfying  .\n        Let   be the matrix with vectors\n          and   as the columns in that order.\n        Find a non-zero   such that\n          using this information.\n       \n         \n       "
},
{
  "id": "exercise-54",
  "level": "2",
  "url": "chap_matrix_vector.html#exercise-54",
  "type": "Exercise",
  "number": "14",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If the system\n                has infinitely many solutions,\n              then so does the system   for  any \n              right-hand-side  .\n             \n              F\n             True\/False \n              If   is a solution for\n                and   is a solution for  ,\n              then   is a solution for  .\n             True\/False \n              If an   matrix   has a pivot in every row,\n              then the equation   has a unique solution for every  .\n             \n              T\n             True\/False \n              If an   matrix   has a pivot in every row,\n              then the equation   has a solution for every  .\n             True\/False \n              If   and   are row equivalent matrices and the columns of   span  ,\n              then so do the columns of  .\n             \n              T\n             True\/False \n              All homogeneous systems have either a unique solution or infinitely many solutions.\n             True\/False \n              If a linear system is not homogeneous,\n              then the solution set does not include the origin.\n             \n              T\n             True\/False \n              If a solution set of a linear system does not include the origin,\n              the system is not homogeneous.\n             True\/False \n              If the system\n                has a unique solution for some  ,\n              then the homogeneous system has only the trivial solution.\n             \n              T\n             True\/False \n              If   is a   matrix,\n              then the homogeneous equation\n                has non-trivial solutions.\n             True\/False \n              If   is a   matrix,\n              then the homogeneous equation\n                has non-trivial solutions.\n             \n              T\n             "
},
{
  "id": "T_ThreeSectorTable",
  "level": "2",
  "url": "chap_matrix_vector.html#T_ThreeSectorTable",
  "type": "Table",
  "number": "5.7",
  "title": "Summary of simple three sector economy",
  "body": "Summary of simple three sector economy into\\from Agriculture Manufacture Households Agriculture 0.25 0.28 0.27 Manufacture 0.20 0.60 0.60 Households 0.55 0.12 0.13 "
},
{
  "id": "p-934",
  "level": "2",
  "url": "chap_matrix_vector.html#p-934",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "closed open "
},
{
  "id": "act_Closed_model",
  "level": "2",
  "url": "chap_matrix_vector.html#act_Closed_model",
  "type": "Project Activity",
  "number": "5.6",
  "title": "",
  "body": "\n      We can use techniques from linear algebra to determine the levels of production that \n      precisely meet the two goals of the economist.\n     production vector \n            Find the augmented matrix for the system of linear equations that represent \n            production of the three sectors from part (a),\n            and then solve the system to find the production levels that meet the economist's \n            two goals.\n           \n            Suppose the production level of the household sector is 200 million units\n            (dollars).\n            Find the production levels of the agricultural and manufacturing sectors that \n            meet the economist's two goals.\n           "
},
{
  "id": "p-942",
  "level": "2",
  "url": "chap_matrix_vector.html#p-942",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "consumption "
},
{
  "id": "p-946",
  "level": "2",
  "url": "chap_matrix_vector.html#p-946",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "steady state "
},
{
  "id": "project-13",
  "level": "2",
  "url": "chap_matrix_vector.html#project-13",
  "type": "Project Activity",
  "number": "5.7",
  "title": "",
  "body": "\n      Is there a steady state solution for the closed system of Agriculture, Manufacturing,\n      and Households?\n      If so, find the general steady state solution.\n      If no, explain why.\n     "
},
{
  "id": "p-948",
  "level": "2",
  "url": "chap_matrix_vector.html#p-948",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "closed open model "
},
{
  "id": "T_FourSectors",
  "level": "2",
  "url": "chap_matrix_vector.html#T_FourSectors",
  "type": "Table",
  "number": "5.8",
  "title": "Summary of four sector economy",
  "body": "Summary of four sector economy into\\from Petroleum Textiles Transportation Chemicals Petroleum 0.10 0.00 0.20 0.40 Textiles 0.40 0.10 0.15 0.30 Transportation 0.60 0.00 0.10 0.25 Chemicals 0.20 0.10 0.30 0.20 "
},
{
  "id": "p-954",
  "level": "2",
  "url": "chap_matrix_vector.html#p-954",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "open sector final demand vector "
},
{
  "id": "p-955",
  "level": "2",
  "url": "chap_matrix_vector.html#p-955",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "exchange "
},
{
  "id": "project-14",
  "level": "2",
  "url": "chap_matrix_vector.html#project-14",
  "type": "Project Activity",
  "number": "5.8",
  "title": "",
  "body": "\n            Suppose the final demand vector in our four sector economy is \n             .\n            Find the production levels that satisfy our system.\n           \n            Does this economy defined by the exchange matrix   have production levels \n            that exactly meet internal and external demands regardless of the external demands?\n            That is, does the system of equations\n             \n            have a solution regardless of the values of  ,\n             ,  , and  ?\n            Explain.\n           "
},
{
  "id": "chap_independence",
  "level": "1",
  "url": "chap_independence.html",
  "type": "Section",
  "number": "6",
  "title": "Linear Dependence and Independence",
  "body": "Linear Dependence and Independence \n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What are two ways to describe what it means for a set of vectors in \n              to be linearly independent?\n           \n         \n           \n            What are two ways to describe what it means for a set of vectors in \n              to be linearly dependent?\n           \n         \n           \n            If   is a set of vectors,\n            what do we mean by a basis for  ?\n           \n         \n           \n            Given a nonzero set   of vectors,\n            how can we find a linearly independent subset of   that has the same \n            span as  ?\n           \n         \n           \n            How do we recognize if the columns of a matrix   are linearly independent?\n           \n         \n           \n            How can we use a matrix to determine if a set\n              of vectors is linearly independent?\n           \n         \n           \n            How can we use a matrix to find a minimal spanning set for a set\n              of vectors in  ?\n           \n         Application: Bzier Curves \n    Bzier curves are simple curves that were first developed in 1959 by French mathematician \n    Paul de Casteljau,\n    who was working at the French automaker Citron.\n    The curves were made public in 1962 by Pierre Bzier who used them in his work designing \n    automobiles at the French car maker Renault.\n    In addition to automobile design, Bzier curves have many other uses.\n    Two of the most common applications of Bzier curves are font design and drawing tools.\n    As an example, the letter\n     S \n    in Palatino font is shown using Bzier curves in  .\n    If you've used Adobe Illustrator, Photoshop, Macromedia Freehand, Fontographer,\n    or any other of a number of drawing programs,\n    then you've used Bzier curves.\n    At the end of this section we will see how Bzier curves can be defined using linearly \n    independent vectors and linear combinations of vectors.\n   A letter  . Introduction \n    In  \n    we saw how to represent water-benzene-acetic acid chemical solutions with vectors,\n    where the components represent the water,\n    benzene and acid percentages.\n    We then considered a problem of determining if a given chemical solution could be made by \n    mixing other chemical solutions.\n    Suppose we now have three different water-benzene-acetic acid chemical solutions,\n    one with 40% water, 50% benzene and 10% acetic acid,\n    the second with 52% water, 42% benzene and 6% acid,\n    and a third with 46% water, 46% benzene and 8% acid.\n    We represent the first chemical solution with the vector \n     ,\n    the second with the vector \n     ,\n    and the third with the vector \n     .\n    By combining these three chemical solutions we can make a chemical solution with 43% water, \n    48% benzene and 9% acid as follows\n     .\n   \n    However, if we had noticed that the third chemical solution can actually be made from the \n    first two, that is,\n     ,\n    we might have realized that we don't need the third chemical solution to make the 43% water, \n    48% benzene and 9% acid chemical solution.\n    In fact,\n     .\n   \n    (See  \n    of  .) Using the third chemical solution\n    (represented by  )\n    uses more information than we actually need to make the desired 43% water, 48% benzene and \n    9% acid chemical solution because the vector   is redundant   all of the \n    material we need to make   is contained in   and  .\n    This is the basic idea behind linear independence   representing information in the \n    most efficient way.\n   minimal spanning set Review of useful information \n    Recall that a linear combination of vectors  ,\n     ,  ,\n      in   is a sum of scalar multiples of  ,\n     ,  ,  .\n    That is, a linear combination of the vectors  ,\n     ,\n     ,   is a vector of the form\n     ,\n    where  ,  ,\n     ,   are scalars.\n   \n    Recall also that the collection of all linear combinations of a set  ,\n     ,  ,\n      of vectors in   is called the span of the set of vectors.\n    That is, the span   of the set  ,\n     ,\n     ,   of vectors in   is the set\n     .\n   \n    For example,\n    a linear combination of vectors \n      and\n      is \n     .\n    All linear combinations of these two vectors can be expressed as the collection of \n    vectors of the form\n      where \n      are scalars.\n    Suppose we want to determine whether   is in the span,\n    in other words if   is a linear combination of  .\n    This means we are looking for   such that\n     .\n    we solve for the system represented with the augmented matrix\n     .\n   \n    By reducing this matrix, we find that there are no solutions of the system,\n    which implies that   is not a linear combination of  .\n    Note that we can use any names we please for the scalars,\n    say  , if we prefer.\n   \n      Let  ,\n       ,\n      and  ,\n      and let  .\n      If   is in  ,\n      we are interested in the most efficient way to represent   as a linear \n      combination of  ,\n       , and  .\n     \n            The vector   is in\n              if there exist  ,\n             , and   so that\n             .\n            (Recall that we can use any letters we want for the scalars.\n            They are simply unknown scalars we want to solve for.)\n           \n                  Explain why   is in  .\n                  \n                 \n                  What is the matrix we need to reduce?\n                 \n                  Write   as a linear combination of  ,\n                   , and  .\n                  In how many ways can   be written as a linear combination of \n                  the vectors  ,\n                   , and  ?\n                  Explain.\n                 \n            In  \n            we saw that the vector   could be written in infinitely many different \n            ways as linear combinations of  ,\n             , and  .\n            We now ask the question if we really need all of the vectors  ,\n             ,\n            and   to make   as a linear combination in a unique way.\n           \n                  Can the vector   be written as a linear combination of the vectors \n                    and  ?\n                  If not, why not?\n                  If so, in how many ways can   be written as a linear combination of \n                    and  ?\n                  Explain.\n                 \n                  If possible,\n                  write   as a linear combination of   and  .\n                 \n            In  \n            we saw that   could be written in infinitely many different ways as a \n            linear combination of the vectors  ,\n             , and  .\n            However, the vector   could only be written in one way as a linear \n            combination of   and  .\n            So   is in   and   is also \n            in  .\n            This raises a question   is  any \n            vector in   also in  .\n            If so, then the vector   is redundant in terms of forming the span of \n             ,\n             , and  .\n            For the sake of efficiency,\n            we want to recognize and eliminate this redundancy.\n           \n                  Can   be written as a linear combination of the vectors   \n                  and  ?\n                  If not, why not?\n                  If so, write   as a linear combination of   and  .\n                 \n                  Use the result of part (a) to decide if  any \n                  vector in   is also in \n                   .\n                 Linear Independence minimal spanning set minimal spanning set basis minimal minimal linearly independent only linear independence independence linear linear dependence dependence linear linearly independent linearly dependent \n    Alternatively, we say that the vectors   are linearly\n    independent (or dependent) if the set   is linearly\n    independent (or dependent).\n   \n    Note that the definition tells us that a set   of vectors\n    in   is linearly dependent if there are scalars  , not all\n    of which are 0 so that\n     .\n    \n   \n        Which of the following sets in   or   is linearly independent and which\n        is linearly dependent? Why? For the linearly dependent sets, write one of the vectors as a\n        linear combination of the others, if possible.\n       \n           \n         \n           \n         \n          What relationship must exist between two vectors if they are linearly dependent?\n         \n          The vectors  ,  , and   as shown in  .\n         Vectors  ,  , and  . \n      and   illustrate how we can write one of the\n    vectors in a linearly dependent set as a linear combination of the others. This would allow\n    us to write at least one of the vectors in the span of the set in more than one way as a\n    linear combination of vectors in this set. We prove this result in general in the following\n    theorem.\n   \n        A set   of vectors in   is linearly\n        dependent if and only if at least one of the vectors in the set can be written as a\n        linear combination of the remaining vectors in the set.\n       \n    The next activity is intended to help set the stage for the proof of  .\n   \n        The statement of   is a bi-conditional statement (an if and\n        only if statement). To prove this statement about the set   we need to show two\n        things about  . One: we must demonstrate that if   is a linearly dependent\n        set, then at least one vector in   is a linear combination of the other vectors\n        (this is the  only if  part ofthe biconditional statement) and Two: if at least one\n        vector in   is a linear combination of the others, then   is linearly\n        dependent (this is the  if  part of the biconditional statement). We illustrate the\n        main idea of the proof using a three vector set  .\n       \n          First let us assume that   is a linearly dependent set and show that at least\n          one vector in   is a linear combination of the other vectors. Since   is\n          linearly dependent we can write the zero vector as a linear combination of\n           ,  , and   with at least one nonzero weight. For\n          example, suppose\n           \n          Solve Equation   for the vector   to show\n          that   can be written as a linear combination of   and  .\n          Conclude that   is a linear combination of the other vectors in the set  .\n         \n          Now we assume that at least one of the vectors in   is a linear combination of\n          the others. For example, suppose that \n           .\n          Use vector algebra to rewrite Equation   so that \n            is expressed as a linear combination of  ,  ,\n          and   such that the weight on   is not zero. Conclude that the\n          set   is linearly dependent.\n         \n    Now we provide a formal prof of  , using the ideas from\n     .\n   Proof of  \n      Let   be a set of vectors in  . We will\n      begin by verifying the first statement.\n     \n      We assume that   is a linearly dependent set and show that at least one vector in\n        is a linear combination of the others. Since   is linearly dependent, there\n      are scalarc  , not all of which are 0, so that \n       .\n      We don't know which scalar(s) are not zero, but there is at least one. So let us assume\n      that   is not zero for some   between 1 and  . we can then subtract\n        from both sides of Equation   and divide\n      by   to obtain\n       .\n      Thus, the vector   is a linear combination of  ,  , \n       ,  ,  ,  ,  , and at\n      least one of the vectors in  is a linear combination of the other vectors in  .\n     \n      To verify the second statement, we assume that at least one of the vectors in   can\n      be written as a linear combination of the others and show that   is then a linearly\n      dependent set. We don't know which vector(s) in   can be written as a linear\n      combination of the others, but there is at least one. Let us suppose that   is a \n      linear combination of the others, but there is at least one. Let us suppose that  \n      is a linear combination of the vectors  ,  , \n       ,  ,  ,  ,  , for some\n        between 1 and  . Then there exist scalars  ,  , \n       ,  ,  ,  ,   so that \n       .\n      It follows that\n       .\n      So there are scalars  ,  ,  ,   (with  ),\n      not all of which are 0, so that\n       .\n      This makes   a linearly dependent set.\n     \n    With a linearly dependent set, at least one of the vectors in the set is a linear combination\n    of the others. With a linearly independent set, this cannot happen   no vector in the\n    set can be written as a linear combination of the others. This result is given in the next\n    theorem. You may be able to see how   and \n      are \n    logically equivalent.\n   \n        A set   of vectors in   is \n        linearly independent if and only if no vector in the set can be written as a linear \n        combination of the remaining vectors in the set.\n       \n        As was hinted at in  , an important consequence of a linearly\n        independent set is that every vector in the span of the set can be written in one\n        and only one way as a linear combination of vectors in the set. It is this uniqueness\n        that makes linearly independent sets so useful. We explore this idea in this activity \n        for a linearly independent set of three vectors. Let  \n        be a linearly independent set of vectors in   for some  , and let \n          be a vector in  . To show that   can be written in \n        exactly one way as a linear combination of vectors in  , we assume that\n         \n        for some scalars  ,  ,  ,  ,  , and  .\n        We need to demonstrate that  ,  , and  .\n       \n          use the two different ways of writing   as a linear combination of  ,\n           , and   to come up with a linear combination expressing  \n          as a linear combination of these vectors.\n         \n          Use the linear independence of the vectors  ,  , and  \n          to explain why  ,  , and  .\n         coordinate system \n    In the next theorem we state and prove the general case of any number of linearly independent\n    vectors productin unique representations as linear combinations.\n   \n        Let   be a linearly independent set of vectors\n        in  . Any vector in   can be written in one and only one way as \n        as linear combination of the vectors  ,  , and  .\n       \n        Let   be a linearly independent set of vectors\n        in   and let   be a vector in  . By definition, it follows\n        that   can be written as a linear combination of the vectors in  .\n        It remains for us to show that this representation is unique. So assume that \n         \n        for some scalars  ,  ,  ,   and  ,\n         ,  ,  . Then\n         .\n        Subtracting all terms from the right side and using a little vector algebra gives us\n         .\n        The fact that   is a linearly independent set implies that\n         ,\n        showing that   for every   between 1 and  . We conclude\n        that the representation of   as a linear combination of the linearly independent\n        vectors in   is unique.\n       Determining Linear Independence \n    The definition and our previous work give us a straightforward method for determining \n    when a set of vectors in   is linearly independent or dependent.\n   \n      In this activity we learn how to use a matrix to determine in general if a set of \n      vectors in   is linearly independent or dependent.\n      Suppose we have   vectors  ,\n       ,  ,   in  .\n      To see if these vectors are linearly independent,\n      we need to find the solutions to the vector equation\n       .\n     \n      If we let   and \n       ,\n      then we can write the vector equation   in \n      matrix form  .\n      Let   be the reduced row echelon form of  .\n     \n            What can we say about the pivots of   in order for\n              to have exactly one solution?\n            Under these conditions, are the vectors  ,  ,\n             ,   linearly independent or dependent?\n           \n            What can we say about the rows or columns of   in order for\n              to have infinitely many solutions?\n            Under these conditions, are the vectors  ,  ,\n             ,   linearly independent or dependent?\n           \n            Use the result of parts (a) and (b) to determine if the vectors \n             ,\n             ,\n            and   in \n              are linearly independent or dependent.\n            If dependent,\n            write one of the vectors as a linear combination of the others.\n            You may use the fact that the matrix\n              is row equivalent to  .\n           Minimal Spanning Sets \n    It is important to note the differences and connections between linear independence,\n    span, and minimal spanning set.\n     \n         \n          The set   is not a minimal spanning set for   even though   is a linearly independent set.\n          Note that   does not span   since the vector\n            is not in  .\n         \n       \n         \n          The set   is not a minimal spanning set for   even though  .\n          Note that\n           ,\n          so   is not a linearly independent set.\n         \n       \n         \n          The set   is a minimal spanning set for   since it satisfies both characteristics of a minimal spanning set:\n            AND   is linearly independent.\n         \n       \n   \n    The three concepts   linear independence,\n    span, and minimal spanning set   are different.\n    The important point to note is that minimal spanning set must be both linearly \n    independent and span the space.\n   \n    To find a minimal spanning set we will often need to find a smallest subset of a \n    given set of vectors that has the same span as the original set of vectors.\n    In this section we determine a method for doing so.\n   \n      Let  ,\n       ,\n       ,\n      and   in  .\n      Assume that the reduced row echelon form of the matrix\n        is  .\n     \n            Write the general solution to the homogeneous system  ,\n            where  .\n            Write all linear combinations of  ,\n             ,  ,\n            and   that are equal to  ,\n            using weights that only involve   and  .\n           \n            Explain how we can conveniently choose the weights in the general solution to\n              to show that the vector   is a linear \n            combination of  ,\n             , and  .\n            What does this tell us about\n              and  ?\n           \n            Explain how we can conveniently choose the weights in the general solution to\n              to show why the vector   is a linear combination \n            of   and  .\n            What does this tell us about\n              and  ?\n           \n            Is   a minimal spanning set for \n             ?\n            Explain your response.\n           \n     \n    illustrates how we can use a matrix to determine a minimal spanning set for a given set of \n    vectors   in  .\n     \n         \n          Form the matrix  .\n         \n       \n         \n          Find the reduced row echelon form\n            of  .\n          If   contains non-pivot columns,\n          say for example that the  th column is a non-pivot column,\n          then we can choose the weight   corresponding to the  th column \n          to be 1 and all weights corresponding to the other non-pivot columns to be 0 to make a \n          linear combination of the columns of   that is equal to  .\n          This allows us to write   as a linear combination of the vectors \n          corresponding to the pivot columns of   as we did in the proof of \n           .\n          So every vector corresponding to a non-pivot column is in the span of the set of \n          vectors corresponding to the pivot columns.\n          The vectors corresponding to the pivot columns are linearly independent,\n          since the matrix with those columns has every column as a pivot column.\n          Thus, the set of vectors corresponding to the pivot columns of   forms a \n          minimal spanning set for  .\n         \n       \n   IMPORTANT NOTE \n    The set of pivot columns of the reduced row echelon form of   will normally not \n    have the same span as the set of columns of  ,\n    so it is critical that we use columns of  ,  not    in our \n    minimal spanning set.\n   \n      Find a minimal spanning set for the span of the set\n       .\n     \n     \n    also illustrates a general process by which we can find a minimal spanning set   \n    that is the smallest subset of vectors that has the same span.\n    This process will be useful later when we consider vectors in arbitrary vector spaces.\n    The idea is that if we can write one of the vectors in a set   as a linear \n    combination of the remaining vectors,\n    then we can remove that vector from the set and maintain the same span.\n    In other words,\n    begin with the span of a set   and follow these steps:\n     \n         Step 1 \n         \n          If   is a linearly independent set,\n          we already have a minimal spanning set.\n         \n       \n         Step 2 \n         \n          If   is not a linearly independent set,\n          then one of the vectors in   is a linear combination of the others.\n          Remove that vector from   to obtain a new set  .\n          It will be the case that  .\n         \n       \n         Step 3 \n         \n          If   is a linearly independent set,\n          then   is a minimal spanning set.\n          If not, repeat steps 2 and 3 for the set   until you arrive at a linearly \n          independent set.\n         \n       \n   \n    This process is guaranteed to stop as long as the set contains at least one nonzero vector.\n    A verification of the statement in Step 2 that\n      is given in the next theorem.\n   \n        Let   be a set of vectors in   so \n        that for some   between 1 and  ,\n          is in  .\n        Then\n         .\n       \n      Let   be a set of vectors in   so that \n        is in the span of  ,\n       ,\n       ,  ,\n       ,  ,\n      and   for some   between 1 and  .\n      To show that\n       ,\n      we need to show that\n       \n           \n            every vector in   is in  , and\n           \n         \n           \n            every vector in   is in  .\n           \n         \n     \n      Let us consider the second containment.\n      Let   be a vector in the span of  ,\n       ,  ,\n       ,  ,\n       , and  .\n      Then\n       \n      for some scalars  ,  ,  ,\n       ,  ,\n       ,  .\n      Note that\n       \n      as well, so   is in  .\n      Thus,\n       .\n     \n      (This same argument shows a more general statement that if   is a subset of  ,\n      then  .)\n     \n      Now we demonstrate the first containment.\n      Here we need the assumption that   is in  ,\n       ,\n       ,  ,\n       ,  ,\n        for some   between 1 and  .\n      That assumption gives us\n       \n      for some scalars  ,  ,  ,\n       ,  ,\n       ,  .\n      Now let   be a vector in the span of  ,\n       ,  ,  .\n      Then\n       \n      for some scalars  ,  ,\n       ,  .\n      Substituting from   shows that\n       .\n     \n      So   is in   and\n       .\n     \n      Since the two sets are subsets of each other, they must be equal sets.\n      We conclude that\n       .\n     \n    The result of  \n    is that if we have a finite set   of vectors in  ,\n    we can eliminate those vectors that are linear combinations of others until we obtain a \n    smallest set of vectors that still has the same span.\n    As mentioned earlier, we call such a minimal spanning set a basis.\n   basis basis IMPORTANT NOTE \n    A basis is defined by two characteristics.\n    A basis must span the space in question and a basis must be a linearly independent set.\n    It is the linear independence that makes a basis a\n     minimal  spanning set.\n   \n    We have worked with a familiar basis in   throughout our mathematical careers.\n    A vector   in   can be \n    written as\n     .\n   standard basis standard basis \n    As we will see later, bases \n    The plural of basis is bases.\n      are of fundamental importance in linear algebra in that bases will allow us to \n    define the dimension of a vector space and will provide us with coordinate systems.\n   \n    We conclude this section with an important theorem that is similar to\n     .\n   \n        Let   be an   matrix.\n        The following statements are equivalent.\n         \n             \n              The matrix equation   has a unique solution for every vector \n                in the span of the columns of  .\n             \n           \n             \n              The matrix equation   has the unique solution  .\n             \n           \n             \n              The columns of   are linearly independent.\n             \n           \n             \n              The matrix   has a pivot position in each column.\n             \n           \n       Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Let  ,\n         ,\n         ,\n        and  .\n       \n              Is the set   linearly independent or \n              dependent.\n              If independent, explain why.\n              If dependent,\n              write one of the vectors in   as a linear combination of the other vectors \n              in  .\n             \n              We need to know the solutions to the vector equation\n               .\n              If the equation has as its only solution  \n              (the trivial solution),\n              then the set   is linearly independent.\n              Otherwise the set   is linearly dependent.\n              To find the solutions to this system,\n              we row reduce the augmented matrix\n               .\n              (Note that we really don't need the augmented column of zeros   row \n              operations won't change that column at all.\n              We just need to know that the column of zeros is there.) Technology shows\n              that the reduced row echelon form of this augmented matrix is\n               .\n              The reduced row echelon form tells us that the vector equation is consistent,\n              and the fact that there is no pivot in the fourth column shows that the system \n              has a free variable and more than just the trivial solution.\n              We conclude that   is linearly dependent.\n              Moreover, the general solution to our vector equation is\n               .\n              Letting   and   shows that one non-trivial solution \n              to our vector equation is\n               .\n              Thus,\n               ,\n              or\n               \n              and we have written one vector in   as a linear combination of the \n              other vectors in  .\n             \n              Find a subset   of   that is a basis for  .\n              Explain how you know you have a basis.\n             \n              We have seen that the pivot columns in a matrix   form a minimal spanning set\n              (or basis)\n              for the span of the columns of  .\n              From part (a) we see that the pivot columns in the reduced row echelon form of\n                are the first and second columns.\n              So a basis for the span of the columns of   is  .\n              Since the elements of   are the columns of  ,\n              we conclude that the set   is a subset of   \n              that is a basis for  .\n             \n        Let  ,\n         ,\n        and  .\n       \n              Is the set   a basis for  ?\n              Explain.\n             \n              We need to know if the vectors in   are linearly independent and span \n               .\n              Technology shows that the reduced row echelon form of\n               \n              is\n               .\n              Since every column of   is a pivot column,\n              the set   is linearly independent.\n              The fact that there is a pivot in every row of the matrix   means that \n              the equation\n                is consistent for every   in  .\n              Since   is a linear combination of the columns of   with \n              weights from  ,\n              tt follows that the columns of   span  .\n              We conclude that the set   is a basis for  .\n             \n              Let  ,\n              where   is a scalar.\n              Are there any values of   for which the set\n                is not a basis for  ?\n              If so, find all such values of   and explain why   is not a \n              basis for   for those values of  .\n             \n              Technology shows that a row echelon form of   is\n               .\n              The columns of   are all pivot columns\n              (hence linearly independent)\n              as long as  ,\n              and are linearly dependent when  .\n              So the only value of   for which   is not a basis for   \n              is  .\n             Summary \n       \n        A set   of vectors in   is linearly \n        independent if the vector equation\n         \n        for scalars   has only the trivial solution\n         .\n        Another way to think about this is that a set of vectors is linearly independent if \n        no vector in the set can be written as a linear combination of the other vectors in the set.\n       \n     \n       \n        A set   of vectors in   is linearly \n        dependent if the vector equation\n         \n        has a nontrivial solution.\n        That is, we can find scalars\n          that are not all 0 so that\n         .\n        Another way to think about this is that a set of vectors is linearly dependent if \n        at least one vector in the set can be written as a linear combination of the other vectors in the set.\n       \n     \n       \n        If   is a set of vectors,\n        a subset   of   is a basis for\n          if   is a linearly independent set and  .\n       \n     \n       \n        Given a nonzero set   of vectors,\n        we can remove vectors from   that are linear combinations of remaining\n        vectors in   to obtain a linearly independent subset of   that has the \n        same span as  .\n       \n     \n       \n        The columns of a matrix   are linearly independent if the equation\n          has only the trivial solution  .\n       \n     \n       \n        The set   is linearly independent if and only \n        if every column of the matrix  ,\n        is a pivot column.\n       \n     \n       \n        If  ,\n        then the vectors in the pivot columns of   form a minimal spanning set for \n         .\n       \n     \n        Consider the following vectors in  :\n         \n        Is the set consisting of these vectors linearly independent?\n        If so, explain why.\n        If not, make a single change in one of the vectors so that the set is linearly independent.\n       \n        Linearly dependent.\n        Change the last entry in   to a  .\n       \n        Consider the following vectors in  :\n         \n        For which values of   is the set consisting of these vectors linearly independent?\n       \n        In a lab, there are three different water-benzene-acetic acid solutions: The first \n        one with 36% water, 50% benzene and 14% acetic acid;\n        the second one with 44% water, 46% benzene and 10% acetic acid;\n        and the last one with 38% water, 49% benzene and 13% acid.\n        Since the lab needs space,\n        the lab coordinator wants to determine whether all solutions are needed,\n        or if it is possible to create one of the solutions using the other two.\n        Can you help the lab coordinator?\n       \n        Only two of the solutions are necessary.\n       \n        Given vectors   and \n         ,\n        find a vector   in   so that the set consisting of\n          and   is linearly independent.\n       \n        Consider the span of   where\n         .\n       \n              Is the set   a minimal spanning set of  ?\n              If not, determine a minimal spanning set,\n              i.e. a basis, of  .\n             \n               \n             \n              Check that the vector   is in  .\n              Find the unique representation of   in terms of the basis vectors.\n             \n               \n             \n        Come up with a   matrix with linearly independent columns,\n        if possible.\n        If not, explain why not.\n       \n        Come up with a   matrix with linearly independent columns,\n        if possible.\n        If not, explain why not.\n       \n        This is not possible.\n       \n        Give an example of vectors\n          such that a minimal spanning set for\n          is equal to that of  ;\n        and an example of three vectors\n          such that a minimal spanning set for\n          is equal to that of  .\n       \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If  ,\n                and   are three vectors none of which is a multiple \n              of another,\n              then these vectors form a linearly independent set.\n             \n              F\n             True\/False \n              If  ,\n                and   in   are linearly independent vectors,\n              then so are  ,  ,\n                and   for any   in  .\n             True\/False \n              If  ,  ,\n                and   in   are linearly independent vectors,\n              then so are  ,   and  .\n             \n              T\n             True\/False \n              A   matrix cannot have linearly independent columns.\n             True\/False \n              If two vectors span  ,\n              then they are linearly independent.\n             \n              T\n             True\/False \n              The space   cannot contain four linearly independent vectors.\n             True\/False \n              If two vectors are linearly dependent,\n              then one is a scalar multiple of the other.\n             \n              T\n             True\/False \n              If a set of vectors in   is linearly dependent,\n              then the set contains more than   vectors.\n             True\/False \n              The columns of a matrix   are linearly independent if the equation\n                has only the trivial solution.\n             \n              T\n             True\/False \n              Let  .\n              If   is a minimal spanning set for  ,\n              then   cannot also be a minimal spanning set for  .\n             True\/False \n              Let  .\n              If   is a minimal spanning set for  ,\n              then   cannot also be a minimal spanning set for  .\n             \n              T\n             True\/False \n              If  ,\n              then   is a minimal spanning set for \n               .\n             Project: Generating Bzier Curves \n    Bzier curves can be created as linear combinations of vectors.\n    In this section we will investigate how cubic Bzier curves\n    (the ones used for fonts)\n    can be realized through linear and quadratic Bzier curves.\n    We begin with linear Bzier curves.\n   \n      Start with two vectors   and  .\n      Linear Bzier curves are linear combinations\n       \n      of the vectors   and   for scalars   between 0 and 1. \n      (You can visualize these linear combinations using the GeoGebra file  Linear Bezier  \n      at  . \n      With this file you can draw the vectors \n        for varying values of  .\n      You can move the points   and   in the GeoGebra file,\n      and the slider controls the values of  .\n      The point identified with   is traced as   is changed.) For this activity,\n      we will see what the curve   corresponds to by evaluating certain points on the curve in a specific example.\n      Let   and \n       .\n     \n            What are the components of the vector\n              if  ?\n            Where is this vector in relation to   and  ?\n            Explain.\n           \n            What are the components of the vector\n              if  ?\n            Where is this vector in relation to   and  ?\n            Explain.\n           \n            What are the components of the vector\n              for an arbitrary  ?\n            Where is this vector in relation to   and  ?\n            Explain.\n           \n    For each value of  ,\n    the vector   is a linear combination of the vectors \n      and  .\n    Note that when  ,\n    we have   and when   we have  ,\n    and for    \n    shows that the vectors   trace out the line segment from   to  .\n    The span   of the vectors   and   for\n      is a linear Bzier curve.\n    Once we have a construction like this,\n    it is natural in mathematics to extend it and see what happens.\n    We do that in the next activity to construct quadratic Bzier curves.\n   \n      Let  ,  ,\n      and   be vectors in the plane.\n      We can then let\n       \n      be the linear Bzier curves as defined in  .\n      Since   and   are vectors,\n      we can define   as\n       .\n     \n      (You can visualize these linear combinations using the GeoGebra file \n       Quadratic Bezier  at  . \n      With this file you can draw the vectors   for varying values of  .\n      You can move the points  ,  ,\n      and   in the GeoGebra file,\n      and the slider controls the values of  .\n      The point identified with   is traced as   is changed.) In this activity \n      we investigate how the vectors   change as   changes.\n      For the remainder of this activity,\n      let  ,\n       ,\n      and  .\n     \n            At what point\n            (in terms of  ,  , and  )\n            is the vector   when  ?\n            Explain using the definition of  .\n           \n            At what point\n            (in terms of  ,  , and  )\n            is the vector   when  ?\n            Explain using the definition of  .\n           \n            Find by hand the components of the vector\n              with  .\n            Compare with the result of the GeoGebra file.\n           \n    The span   of the vectors   and  ,\n    or the set of points traced out by the vectors   for  ,\n    is a quadratic Bzier curve.\n    To understand why this curve is called quadratic,\n    we examine the situation in a general context in the following activity.\n   quadratic \n    Notice that if any one of the   lies on the line determined by the other two vectors,\n    then the quadratic Bzier curve is just a line segment.\n    So to obtain something non-linear we need to choose our vectors so that that doesn't happen.\n   \n    Quadratic Bzier curves are limited,\n    because their graphs are parabolas.\n    For applications we need higher order Bzier curves.\n    In the next activity we consider cubic Bzier curves.\n   \n      Start with four vectors  ,\n       ,  ,\n          the points defined by these vectors are called\n       control points  for the curve.\n      As with the linear and quadratic Bzier curves, we let\n       .\n     \n      Then let\n       .\n     \n      We take this one step further to generate the cubic Bzier curves by letting\n       .\n     \n      (You can visualize these linear combinations using the GeoGebra file  Cubic Bezier  \n      at  . \n      With this file you can draw the vectors   for varying values of  .\n      You can move the points  ,\n       ,  ,\n      and   in the GeoGebra file,\n      and the slider controls the values of  .\n      The point identified with   is traced as   is changed.) In this \n      activity we investigate how the vectors   change as   changes.\n      For the remainder of this activity,\n      let  ,\n       ,\n       ,\n      and  .\n     \n            At what point\n            (in terms of  ,  ,\n             , and  )\n            is the vector   when  ?\n            Explain using the definition of  .\n           \n            At what point\n            (in terms of  ,  ,\n             , and  )\n            is the vector   when  ?\n            Explain using the definition of  .\n           \n            Find by hand the components of the vector\n              with  .\n            Compare with the result of the GeoGebra file.\n           \n    The span   of the vectors   and  ,\n    or the set of points traced out by the vectors   for  ,\n    is a cubic Bzier curve.\n    To understand why this curve is called cubic,\n    we examine the situation in a general context in the following activity.\n   cubic \n    Just as with the quadratic case,\n    we need certain subsets of the set of control vectors to be linearly independent so that \n    the cubic Bzier curve does not degenerate to a quadratic or linear Bzier curve.\n   \n    More complicated and realistic shapes can be represented by piecing together two or \n    more Bzier curves as illustrated with the letter\n     S \n    in  .\n    Suppose we have two cubic Bzier curves,\n    the first with control points  ,\n     ,  ,\n    and   and the second with control points  ,\n     ,\n     , and  .\n    You may have noticed that   lies on the tangent line to the first Bzier \n    curve at   and that   lies on the tangent line to the first \n    Bzier curve at  .\n    (Play around with the program  Cubic Bezier  to convince yourself of these statements.\n    This can be proved in a straightforward manner using vector calculus.)\n    So if we want to make a smooth curve from these two Bzier curves,\n    the curves will need to join together smoothly at   and  .\n    This will force   and the tangents at   will \n    have to match.\n    This implies that  ,  ,\n    and   all have to lie on this common tangent line.\n    Keeping this idea in mind,\n    use the GeoGebra file  Cubic Bezier Pair  at \n      to find \n    control points for the pair of Bzier curves that create your own letter S.\n   "
},
{
  "id": "objectives-6",
  "level": "2",
  "url": "chap_independence.html#objectives-6",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What are two ways to describe what it means for a set of vectors in \n              to be linearly independent?\n           \n         \n           \n            What are two ways to describe what it means for a set of vectors in \n              to be linearly dependent?\n           \n         \n           \n            If   is a set of vectors,\n            what do we mean by a basis for  ?\n           \n         \n           \n            Given a nonzero set   of vectors,\n            how can we find a linearly independent subset of   that has the same \n            span as  ?\n           \n         \n           \n            How do we recognize if the columns of a matrix   are linearly independent?\n           \n         \n           \n            How can we use a matrix to determine if a set\n              of vectors is linearly independent?\n           \n         \n           \n            How can we use a matrix to find a minimal spanning set for a set\n              of vectors in  ?\n           \n         "
},
{
  "id": "F_Letter_S",
  "level": "2",
  "url": "chap_independence.html#F_Letter_S",
  "type": "Figure",
  "number": "6.1",
  "title": "",
  "body": "A letter  . "
},
{
  "id": "p-970",
  "level": "2",
  "url": "chap_independence.html#p-970",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "minimal spanning set "
},
{
  "id": "pa_1_f",
  "level": "2",
  "url": "chap_independence.html#pa_1_f",
  "type": "Preview Activity",
  "number": "6.1",
  "title": "",
  "body": "\n      Let  ,\n       ,\n      and  ,\n      and let  .\n      If   is in  ,\n      we are interested in the most efficient way to represent   as a linear \n      combination of  ,\n       , and  .\n     \n            The vector   is in\n              if there exist  ,\n             , and   so that\n             .\n            (Recall that we can use any letters we want for the scalars.\n            They are simply unknown scalars we want to solve for.)\n           \n                  Explain why   is in  .\n                  \n                 \n                  What is the matrix we need to reduce?\n                 \n                  Write   as a linear combination of  ,\n                   , and  .\n                  In how many ways can   be written as a linear combination of \n                  the vectors  ,\n                   , and  ?\n                  Explain.\n                 \n            In  \n            we saw that the vector   could be written in infinitely many different \n            ways as linear combinations of  ,\n             , and  .\n            We now ask the question if we really need all of the vectors  ,\n             ,\n            and   to make   as a linear combination in a unique way.\n           \n                  Can the vector   be written as a linear combination of the vectors \n                    and  ?\n                  If not, why not?\n                  If so, in how many ways can   be written as a linear combination of \n                    and  ?\n                  Explain.\n                 \n                  If possible,\n                  write   as a linear combination of   and  .\n                 \n            In  \n            we saw that   could be written in infinitely many different ways as a \n            linear combination of the vectors  ,\n             , and  .\n            However, the vector   could only be written in one way as a linear \n            combination of   and  .\n            So   is in   and   is also \n            in  .\n            This raises a question   is  any \n            vector in   also in  .\n            If so, then the vector   is redundant in terms of forming the span of \n             ,\n             , and  .\n            For the sake of efficiency,\n            we want to recognize and eliminate this redundancy.\n           \n                  Can   be written as a linear combination of the vectors   \n                  and  ?\n                  If not, why not?\n                  If so, write   as a linear combination of   and  .\n                 \n                  Use the result of part (a) to decide if  any \n                  vector in   is also in \n                   .\n                 "
},
{
  "id": "p-986",
  "level": "2",
  "url": "chap_independence.html#p-986",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "minimal spanning set "
},
{
  "id": "p-987",
  "level": "2",
  "url": "chap_independence.html#p-987",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "minimal spanning set basis minimal "
},
{
  "id": "p-988",
  "level": "2",
  "url": "chap_independence.html#p-988",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "minimal linearly independent only "
},
{
  "id": "def_linear_independence_Rn",
  "level": "2",
  "url": "chap_independence.html#def_linear_independence_Rn",
  "type": "Definition",
  "number": "6.2",
  "title": "",
  "body": "linear independence independence linear linear dependence dependence linear linearly independent linearly dependent "
},
{
  "id": "act_1_f_1",
  "level": "2",
  "url": "chap_independence.html#act_1_f_1",
  "type": "Activity",
  "number": "6.2",
  "title": "",
  "body": "\n        Which of the following sets in   or   is linearly independent and which\n        is linearly dependent? Why? For the linearly dependent sets, write one of the vectors as a\n        linear combination of the others, if possible.\n       \n           \n         \n           \n         \n          What relationship must exist between two vectors if they are linearly dependent?\n         \n          The vectors  ,  , and   as shown in  .\n         Vectors  ,  , and  . "
},
{
  "id": "thm_dependence",
  "level": "2",
  "url": "chap_independence.html#thm_dependence",
  "type": "Theorem",
  "number": "6.4",
  "title": "",
  "body": "\n        A set   of vectors in   is linearly\n        dependent if and only if at least one of the vectors in the set can be written as a\n        linear combination of the remaining vectors in the set.\n       "
},
{
  "id": "act_1_f_1b",
  "level": "2",
  "url": "chap_independence.html#act_1_f_1b",
  "type": "Activity",
  "number": "6.3",
  "title": "",
  "body": "\n        The statement of   is a bi-conditional statement (an if and\n        only if statement). To prove this statement about the set   we need to show two\n        things about  . One: we must demonstrate that if   is a linearly dependent\n        set, then at least one vector in   is a linear combination of the other vectors\n        (this is the  only if  part ofthe biconditional statement) and Two: if at least one\n        vector in   is a linear combination of the others, then   is linearly\n        dependent (this is the  if  part of the biconditional statement). We illustrate the\n        main idea of the proof using a three vector set  .\n       \n          First let us assume that   is a linearly dependent set and show that at least\n          one vector in   is a linear combination of the other vectors. Since   is\n          linearly dependent we can write the zero vector as a linear combination of\n           ,  , and   with at least one nonzero weight. For\n          example, suppose\n           \n          Solve Equation   for the vector   to show\n          that   can be written as a linear combination of   and  .\n          Conclude that   is a linear combination of the other vectors in the set  .\n         \n          Now we assume that at least one of the vectors in   is a linear combination of\n          the others. For example, suppose that \n           .\n          Use vector algebra to rewrite Equation   so that \n            is expressed as a linear combination of  ,  ,\n          and   such that the weight on   is not zero. Conclude that the\n          set   is linearly dependent.\n         "
},
{
  "id": "proof-1",
  "level": "2",
  "url": "chap_independence.html#proof-1",
  "type": "Proof",
  "number": "1",
  "title": "Proof of Theorem6.4.",
  "body": "Proof of  \n      Let   be a set of vectors in  . We will\n      begin by verifying the first statement.\n     \n      We assume that   is a linearly dependent set and show that at least one vector in\n        is a linear combination of the others. Since   is linearly dependent, there\n      are scalarc  , not all of which are 0, so that \n       .\n      We don't know which scalar(s) are not zero, but there is at least one. So let us assume\n      that   is not zero for some   between 1 and  . we can then subtract\n        from both sides of Equation   and divide\n      by   to obtain\n       .\n      Thus, the vector   is a linear combination of  ,  , \n       ,  ,  ,  ,  , and at\n      least one of the vectors in  is a linear combination of the other vectors in  .\n     \n      To verify the second statement, we assume that at least one of the vectors in   can\n      be written as a linear combination of the others and show that   is then a linearly\n      dependent set. We don't know which vector(s) in   can be written as a linear\n      combination of the others, but there is at least one. Let us suppose that   is a \n      linear combination of the others, but there is at least one. Let us suppose that  \n      is a linear combination of the vectors  ,  , \n       ,  ,  ,  ,  , for some\n        between 1 and  . Then there exist scalars  ,  , \n       ,  ,  ,  ,   so that \n       .\n      It follows that\n       .\n      So there are scalars  ,  ,  ,   (with  ),\n      not all of which are 0, so that\n       .\n      This makes   a linearly dependent set.\n     "
},
{
  "id": "thm_Independence",
  "level": "2",
  "url": "chap_independence.html#thm_Independence",
  "type": "Theorem",
  "number": "6.5",
  "title": "",
  "body": "\n        A set   of vectors in   is \n        linearly independent if and only if no vector in the set can be written as a linear \n        combination of the remaining vectors in the set.\n       "
},
{
  "id": "act_1_f_2",
  "level": "2",
  "url": "chap_independence.html#act_1_f_2",
  "type": "Activity",
  "number": "6.4",
  "title": "",
  "body": "\n        As was hinted at in  , an important consequence of a linearly\n        independent set is that every vector in the span of the set can be written in one\n        and only one way as a linear combination of vectors in the set. It is this uniqueness\n        that makes linearly independent sets so useful. We explore this idea in this activity \n        for a linearly independent set of three vectors. Let  \n        be a linearly independent set of vectors in   for some  , and let \n          be a vector in  . To show that   can be written in \n        exactly one way as a linear combination of vectors in  , we assume that\n         \n        for some scalars  ,  ,  ,  ,  , and  .\n        We need to demonstrate that  ,  , and  .\n       \n          use the two different ways of writing   as a linear combination of  ,\n           , and   to come up with a linear combination expressing  \n          as a linear combination of these vectors.\n         \n          Use the linear independence of the vectors  ,  , and  \n          to explain why  ,  , and  .\n         "
},
{
  "id": "p-1012",
  "level": "2",
  "url": "chap_independence.html#p-1012",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "coordinate system "
},
{
  "id": "thm_1_f_unique_representation",
  "level": "2",
  "url": "chap_independence.html#thm_1_f_unique_representation",
  "type": "Theorem",
  "number": "6.6",
  "title": "",
  "body": "\n        Let   be a linearly independent set of vectors\n        in  . Any vector in   can be written in one and only one way as \n        as linear combination of the vectors  ,  , and  .\n       \n        Let   be a linearly independent set of vectors\n        in   and let   be a vector in  . By definition, it follows\n        that   can be written as a linear combination of the vectors in  .\n        It remains for us to show that this representation is unique. So assume that \n         \n        for some scalars  ,  ,  ,   and  ,\n         ,  ,  . Then\n         .\n        Subtracting all terms from the right side and using a little vector algebra gives us\n         .\n        The fact that   is a linearly independent set implies that\n         ,\n        showing that   for every   between 1 and  . We conclude\n        that the representation of   as a linear combination of the linearly independent\n        vectors in   is unique.\n       "
},
{
  "id": "act_1_f_3",
  "level": "2",
  "url": "chap_independence.html#act_1_f_3",
  "type": "Activity",
  "number": "6.5",
  "title": "",
  "body": "\n      In this activity we learn how to use a matrix to determine in general if a set of \n      vectors in   is linearly independent or dependent.\n      Suppose we have   vectors  ,\n       ,  ,   in  .\n      To see if these vectors are linearly independent,\n      we need to find the solutions to the vector equation\n       .\n     \n      If we let   and \n       ,\n      then we can write the vector equation   in \n      matrix form  .\n      Let   be the reduced row echelon form of  .\n     \n            What can we say about the pivots of   in order for\n              to have exactly one solution?\n            Under these conditions, are the vectors  ,  ,\n             ,   linearly independent or dependent?\n           \n            What can we say about the rows or columns of   in order for\n              to have infinitely many solutions?\n            Under these conditions, are the vectors  ,  ,\n             ,   linearly independent or dependent?\n           \n            Use the result of parts (a) and (b) to determine if the vectors \n             ,\n             ,\n            and   in \n              are linearly independent or dependent.\n            If dependent,\n            write one of the vectors as a linear combination of the others.\n            You may use the fact that the matrix\n              is row equivalent to  .\n           "
},
{
  "id": "act_1_f_4",
  "level": "2",
  "url": "chap_independence.html#act_1_f_4",
  "type": "Activity",
  "number": "6.6",
  "title": "",
  "body": "\n      Let  ,\n       ,\n       ,\n      and   in  .\n      Assume that the reduced row echelon form of the matrix\n        is  .\n     \n            Write the general solution to the homogeneous system  ,\n            where  .\n            Write all linear combinations of  ,\n             ,  ,\n            and   that are equal to  ,\n            using weights that only involve   and  .\n           \n            Explain how we can conveniently choose the weights in the general solution to\n              to show that the vector   is a linear \n            combination of  ,\n             , and  .\n            What does this tell us about\n              and  ?\n           \n            Explain how we can conveniently choose the weights in the general solution to\n              to show why the vector   is a linear combination \n            of   and  .\n            What does this tell us about\n              and  ?\n           \n            Is   a minimal spanning set for \n             ?\n            Explain your response.\n           "
},
{
  "id": "act_1_f_5",
  "level": "2",
  "url": "chap_independence.html#act_1_f_5",
  "type": "Activity",
  "number": "6.7",
  "title": "",
  "body": "\n      Find a minimal spanning set for the span of the set\n       .\n     "
},
{
  "id": "thm_minimal_spanning_set",
  "level": "2",
  "url": "chap_independence.html#thm_minimal_spanning_set",
  "type": "Theorem",
  "number": "6.7",
  "title": "",
  "body": "\n        Let   be a set of vectors in   so \n        that for some   between 1 and  ,\n          is in  .\n        Then\n         .\n       \n      Let   be a set of vectors in   so that \n        is in the span of  ,\n       ,\n       ,  ,\n       ,  ,\n      and   for some   between 1 and  .\n      To show that\n       ,\n      we need to show that\n       \n           \n            every vector in   is in  , and\n           \n         \n           \n            every vector in   is in  .\n           \n         \n     \n      Let us consider the second containment.\n      Let   be a vector in the span of  ,\n       ,  ,\n       ,  ,\n       , and  .\n      Then\n       \n      for some scalars  ,  ,  ,\n       ,  ,\n       ,  .\n      Note that\n       \n      as well, so   is in  .\n      Thus,\n       .\n     \n      (This same argument shows a more general statement that if   is a subset of  ,\n      then  .)\n     \n      Now we demonstrate the first containment.\n      Here we need the assumption that   is in  ,\n       ,\n       ,  ,\n       ,  ,\n        for some   between 1 and  .\n      That assumption gives us\n       \n      for some scalars  ,  ,  ,\n       ,  ,\n       ,  .\n      Now let   be a vector in the span of  ,\n       ,  ,  .\n      Then\n       \n      for some scalars  ,  ,\n       ,  .\n      Substituting from   shows that\n       .\n     \n      So   is in   and\n       .\n     \n      Since the two sets are subsets of each other, they must be equal sets.\n      We conclude that\n       .\n     "
},
{
  "id": "def_1_f_basis",
  "level": "2",
  "url": "chap_independence.html#def_1_f_basis",
  "type": "Definition",
  "number": "6.8",
  "title": "",
  "body": "basis basis "
},
{
  "id": "p-1056",
  "level": "2",
  "url": "chap_independence.html#p-1056",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "standard basis "
},
{
  "id": "p-1057",
  "level": "2",
  "url": "chap_independence.html#p-1057",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "standard basis "
},
{
  "id": "thm_IMT_1_f",
  "level": "2",
  "url": "chap_independence.html#thm_IMT_1_f",
  "type": "Theorem",
  "number": "6.9",
  "title": "",
  "body": "\n        Let   be an   matrix.\n        The following statements are equivalent.\n         \n             \n              The matrix equation   has a unique solution for every vector \n                in the span of the columns of  .\n             \n           \n             \n              The matrix equation   has the unique solution  .\n             \n           \n             \n              The columns of   are linearly independent.\n             \n           \n             \n              The matrix   has a pivot position in each column.\n             \n           \n       "
},
{
  "id": "example-11",
  "level": "2",
  "url": "chap_independence.html#example-11",
  "type": "Example",
  "number": "6.10",
  "title": "",
  "body": "\n        Let  ,\n         ,\n         ,\n        and  .\n       \n              Is the set   linearly independent or \n              dependent.\n              If independent, explain why.\n              If dependent,\n              write one of the vectors in   as a linear combination of the other vectors \n              in  .\n             \n              We need to know the solutions to the vector equation\n               .\n              If the equation has as its only solution  \n              (the trivial solution),\n              then the set   is linearly independent.\n              Otherwise the set   is linearly dependent.\n              To find the solutions to this system,\n              we row reduce the augmented matrix\n               .\n              (Note that we really don't need the augmented column of zeros   row \n              operations won't change that column at all.\n              We just need to know that the column of zeros is there.) Technology shows\n              that the reduced row echelon form of this augmented matrix is\n               .\n              The reduced row echelon form tells us that the vector equation is consistent,\n              and the fact that there is no pivot in the fourth column shows that the system \n              has a free variable and more than just the trivial solution.\n              We conclude that   is linearly dependent.\n              Moreover, the general solution to our vector equation is\n               .\n              Letting   and   shows that one non-trivial solution \n              to our vector equation is\n               .\n              Thus,\n               ,\n              or\n               \n              and we have written one vector in   as a linear combination of the \n              other vectors in  .\n             \n              Find a subset   of   that is a basis for  .\n              Explain how you know you have a basis.\n             \n              We have seen that the pivot columns in a matrix   form a minimal spanning set\n              (or basis)\n              for the span of the columns of  .\n              From part (a) we see that the pivot columns in the reduced row echelon form of\n                are the first and second columns.\n              So a basis for the span of the columns of   is  .\n              Since the elements of   are the columns of  ,\n              we conclude that the set   is a subset of   \n              that is a basis for  .\n             "
},
{
  "id": "example-12",
  "level": "2",
  "url": "chap_independence.html#example-12",
  "type": "Example",
  "number": "6.11",
  "title": "",
  "body": "\n        Let  ,\n         ,\n        and  .\n       \n              Is the set   a basis for  ?\n              Explain.\n             \n              We need to know if the vectors in   are linearly independent and span \n               .\n              Technology shows that the reduced row echelon form of\n               \n              is\n               .\n              Since every column of   is a pivot column,\n              the set   is linearly independent.\n              The fact that there is a pivot in every row of the matrix   means that \n              the equation\n                is consistent for every   in  .\n              Since   is a linear combination of the columns of   with \n              weights from  ,\n              tt follows that the columns of   span  .\n              We conclude that the set   is a basis for  .\n             \n              Let  ,\n              where   is a scalar.\n              Are there any values of   for which the set\n                is not a basis for  ?\n              If so, find all such values of   and explain why   is not a \n              basis for   for those values of  .\n             \n              Technology shows that a row echelon form of   is\n               .\n              The columns of   are all pivot columns\n              (hence linearly independent)\n              as long as  ,\n              and are linearly dependent when  .\n              So the only value of   for which   is not a basis for   \n              is  .\n             "
},
{
  "id": "exercise-55",
  "level": "2",
  "url": "chap_independence.html#exercise-55",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Consider the following vectors in  :\n         \n        Is the set consisting of these vectors linearly independent?\n        If so, explain why.\n        If not, make a single change in one of the vectors so that the set is linearly independent.\n       \n        Linearly dependent.\n        Change the last entry in   to a  .\n       "
},
{
  "id": "exercise-56",
  "level": "2",
  "url": "chap_independence.html#exercise-56",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Consider the following vectors in  :\n         \n        For which values of   is the set consisting of these vectors linearly independent?\n       "
},
{
  "id": "exercise-57",
  "level": "2",
  "url": "chap_independence.html#exercise-57",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        In a lab, there are three different water-benzene-acetic acid solutions: The first \n        one with 36% water, 50% benzene and 14% acetic acid;\n        the second one with 44% water, 46% benzene and 10% acetic acid;\n        and the last one with 38% water, 49% benzene and 13% acid.\n        Since the lab needs space,\n        the lab coordinator wants to determine whether all solutions are needed,\n        or if it is possible to create one of the solutions using the other two.\n        Can you help the lab coordinator?\n       \n        Only two of the solutions are necessary.\n       "
},
{
  "id": "exercise-58",
  "level": "2",
  "url": "chap_independence.html#exercise-58",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Given vectors   and \n         ,\n        find a vector   in   so that the set consisting of\n          and   is linearly independent.\n       "
},
{
  "id": "exercise-59",
  "level": "2",
  "url": "chap_independence.html#exercise-59",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Consider the span of   where\n         .\n       \n              Is the set   a minimal spanning set of  ?\n              If not, determine a minimal spanning set,\n              i.e. a basis, of  .\n             \n               \n             \n              Check that the vector   is in  .\n              Find the unique representation of   in terms of the basis vectors.\n             \n               \n             "
},
{
  "id": "exercise-60",
  "level": "2",
  "url": "chap_independence.html#exercise-60",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Come up with a   matrix with linearly independent columns,\n        if possible.\n        If not, explain why not.\n       "
},
{
  "id": "exercise-61",
  "level": "2",
  "url": "chap_independence.html#exercise-61",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        Come up with a   matrix with linearly independent columns,\n        if possible.\n        If not, explain why not.\n       \n        This is not possible.\n       "
},
{
  "id": "exercise-62",
  "level": "2",
  "url": "chap_independence.html#exercise-62",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Give an example of vectors\n          such that a minimal spanning set for\n          is equal to that of  ;\n        and an example of three vectors\n          such that a minimal spanning set for\n          is equal to that of  .\n       "
},
{
  "id": "exercise-63",
  "level": "2",
  "url": "chap_independence.html#exercise-63",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If  ,\n                and   are three vectors none of which is a multiple \n              of another,\n              then these vectors form a linearly independent set.\n             \n              F\n             True\/False \n              If  ,\n                and   in   are linearly independent vectors,\n              then so are  ,  ,\n                and   for any   in  .\n             True\/False \n              If  ,  ,\n                and   in   are linearly independent vectors,\n              then so are  ,   and  .\n             \n              T\n             True\/False \n              A   matrix cannot have linearly independent columns.\n             True\/False \n              If two vectors span  ,\n              then they are linearly independent.\n             \n              T\n             True\/False \n              The space   cannot contain four linearly independent vectors.\n             True\/False \n              If two vectors are linearly dependent,\n              then one is a scalar multiple of the other.\n             \n              T\n             True\/False \n              If a set of vectors in   is linearly dependent,\n              then the set contains more than   vectors.\n             True\/False \n              The columns of a matrix   are linearly independent if the equation\n                has only the trivial solution.\n             \n              T\n             True\/False \n              Let  .\n              If   is a minimal spanning set for  ,\n              then   cannot also be a minimal spanning set for  .\n             True\/False \n              Let  .\n              If   is a minimal spanning set for  ,\n              then   cannot also be a minimal spanning set for  .\n             \n              T\n             True\/False \n              If  ,\n              then   is a minimal spanning set for \n               .\n             "
},
{
  "id": "act_1_d_linear_Bezier",
  "level": "2",
  "url": "chap_independence.html#act_1_d_linear_Bezier",
  "type": "Project Activity",
  "number": "6.8",
  "title": "",
  "body": "\n      Start with two vectors   and  .\n      Linear Bzier curves are linear combinations\n       \n      of the vectors   and   for scalars   between 0 and 1. \n      (You can visualize these linear combinations using the GeoGebra file  Linear Bezier  \n      at  . \n      With this file you can draw the vectors \n        for varying values of  .\n      You can move the points   and   in the GeoGebra file,\n      and the slider controls the values of  .\n      The point identified with   is traced as   is changed.) For this activity,\n      we will see what the curve   corresponds to by evaluating certain points on the curve in a specific example.\n      Let   and \n       .\n     \n            What are the components of the vector\n              if  ?\n            Where is this vector in relation to   and  ?\n            Explain.\n           \n            What are the components of the vector\n              if  ?\n            Where is this vector in relation to   and  ?\n            Explain.\n           \n            What are the components of the vector\n              for an arbitrary  ?\n            Where is this vector in relation to   and  ?\n            Explain.\n           "
},
{
  "id": "act_1_d_quadratic_Bezier",
  "level": "2",
  "url": "chap_independence.html#act_1_d_quadratic_Bezier",
  "type": "Project Activity",
  "number": "6.9",
  "title": "",
  "body": "\n      Let  ,  ,\n      and   be vectors in the plane.\n      We can then let\n       \n      be the linear Bzier curves as defined in  .\n      Since   and   are vectors,\n      we can define   as\n       .\n     \n      (You can visualize these linear combinations using the GeoGebra file \n       Quadratic Bezier  at  . \n      With this file you can draw the vectors   for varying values of  .\n      You can move the points  ,  ,\n      and   in the GeoGebra file,\n      and the slider controls the values of  .\n      The point identified with   is traced as   is changed.) In this activity \n      we investigate how the vectors   change as   changes.\n      For the remainder of this activity,\n      let  ,\n       ,\n      and  .\n     \n            At what point\n            (in terms of  ,  , and  )\n            is the vector   when  ?\n            Explain using the definition of  .\n           \n            At what point\n            (in terms of  ,  , and  )\n            is the vector   when  ?\n            Explain using the definition of  .\n           \n            Find by hand the components of the vector\n              with  .\n            Compare with the result of the GeoGebra file.\n           "
},
{
  "id": "act_1_d_quadratic_Bezier_general",
  "level": "2",
  "url": "chap_independence.html#act_1_d_quadratic_Bezier_general",
  "type": "Project Activity",
  "number": "6.10",
  "title": "",
  "body": "quadratic "
},
{
  "id": "act_1_d_cubic_Bezier",
  "level": "2",
  "url": "chap_independence.html#act_1_d_cubic_Bezier",
  "type": "Project Activity",
  "number": "6.11",
  "title": "",
  "body": "\n      Start with four vectors  ,\n       ,  ,\n          the points defined by these vectors are called\n       control points  for the curve.\n      As with the linear and quadratic Bzier curves, we let\n       .\n     \n      Then let\n       .\n     \n      We take this one step further to generate the cubic Bzier curves by letting\n       .\n     \n      (You can visualize these linear combinations using the GeoGebra file  Cubic Bezier  \n      at  . \n      With this file you can draw the vectors   for varying values of  .\n      You can move the points  ,\n       ,  ,\n      and   in the GeoGebra file,\n      and the slider controls the values of  .\n      The point identified with   is traced as   is changed.) In this \n      activity we investigate how the vectors   change as   changes.\n      For the remainder of this activity,\n      let  ,\n       ,\n       ,\n      and  .\n     \n            At what point\n            (in terms of  ,  ,\n             , and  )\n            is the vector   when  ?\n            Explain using the definition of  .\n           \n            At what point\n            (in terms of  ,  ,\n             , and  )\n            is the vector   when  ?\n            Explain using the definition of  .\n           \n            Find by hand the components of the vector\n              with  .\n            Compare with the result of the GeoGebra file.\n           "
},
{
  "id": "act_1_d_cubic_Bezier_general",
  "level": "2",
  "url": "chap_independence.html#act_1_d_cubic_Bezier_general",
  "type": "Project Activity",
  "number": "6.12",
  "title": "",
  "body": "cubic "
},
{
  "id": "chap_matrix_transformations",
  "level": "1",
  "url": "chap_matrix_transformations.html",
  "type": "Section",
  "number": "7",
  "title": "Matrix Transformations",
  "body": "Matrix Transformations \n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is a matrix transformation?\n           \n         \n           \n            What properties do matrix transformations have? (In particular,\n            what properties make matrix transformations  linear ?)\n           \n         \n           \n            What is the domain of a matrix transformation defined by an   matrix?\n            Why?\n           \n         \n           \n            What are the range and codomain of a matrix transformation defined by an \n              matrix?\n            Why?\n           \n         \n           \n            What does it mean for a matrix transformation to be one-to-one?\n            If   is a matrix transformation represented as  ,\n            what are the conditions on   that make   a one-to-one transformation?\n           \n         \n           \n            What does it mean for a matrix transformation to be onto?\n            If   is a matrix transformation represented as  ,\n            what are the conditions on   that make   an onto transformation?\n           \n         Application: Computer Graphics \n    As we will discuss, left multiplication by an\n      matrix defines a function from   to  .\n    Such a function defined by matrix multiplication is called a matrix transformation.\n    In this section we study some of the properties of matrix transformations and understand how,\n    using the pivots of the matrix,\n    to determine when the output of a matrix transformation covers the whole space   \n    or when a transformation maps distinct vectors to distinct outputs.\n   \n    Matrix transformations are used extensively in computer graphics to produce animations as \n    seen in video games and movies.\n    For example,\n    consider the dancing figure at left in  .\n    We can identify certain control points (e.g., the point at the neck,\n    where the arms join the torso,\n    etc.) to mark the locations of important points.\n    Using just the control points we can reconstruct the figure.\n    Each control point can be represented as a vector,\n    and so we can manipulate the figure by manipulating the control points with matrix transformations.\n    We will explore this idea in more detail later in this section.\n   A dancing figure and a rotated dancing figure. Introduction \n    In this section we will consider special functions which take vectors as inputs and produce \n    vectors as outputs.\n    We will use matrix multiplication to produce the output vectors.\n   image \n    These functions are the matrix transformations.\n   matrix transformation transformation matrix matrix transformation \n    Many of the transformations we consider in this section are from \n      to   so that we can visualize the transformations.\n    As an example,\n    let us consider the transformation   defined by\n     .\n   \n    If we plot the input vectors  ,\n     ,\n     ,\n    and   \n    (as (blue) circles) and their images \n     ,\n     ,\n     ,\n    and   \n    (as (red)  's) on the same set of axes as shown in  ,\n    we see that this transformation reflects the input vectors across the  -axis.\n    We can also see this algebraically since the reflection of the point\n      around the  -axis is the point  , and\n     .\n   Inputs and outputs of the transformation  . \n      We now consider other transformations from   to  .\n     \n            Suppose a transformation   is defined by\n             .\n           \n                  Find   for each of \n                   ,\n                   ,\n                   ,\n                  and  .\n                  (In other words, substitute\n                    into the formula above to see what \n                  output is obtained.)\n                 \n                  Plot all input vectors and their images on the same axes in  .\n                  Clearly identify which image corresponds to which input vector.\n                  Then give a geometric description of what this transformation does.\n                 \n            The transformation in the introduction performs a reflection across the  -axis.\n            Find a matrix transformation that performs a reflection across the  -axis.\n           \n            Suppose a transformation   is defined by\n             ,\n            where\n             .\n           \n                  Find   for each of \n                   ,\n                   ,\n                   ,\n                  and  .\n                 \n                  Plot all input vectors and their images on the same axes in  .\n                  Give a geometric description of this transformation.\n                 \n                  Is there an input vector which produces \n                    as an output vector?\n                 \n                  Find all input vectors that produce the output vector \n                   .\n                  Is there a unique input vector,\n                  or multiple input vectors?\n                 Properties of Matrix Transformations domain of a matrix transformation codomain of a matrix transformation range of a matrix transformation image of an element under a transformation domain codomain range range of a matrix transformation range image image \n    Because of the properties of the matrix-vector product,\n    if the matrix transformation   is defined by   for some\n      matrix  , then\n     \n    and\n     \n    for any vectors   and   in   and any scalar  .\n    So every matrix transformation   satisfies the following two important properties:\n     \n         \n            and\n         \n       \n         \n           .\n         \n       \n   \n    The first property says that a matrix transformation   preserves sums of \n    vectors and the second that   preserves scalar multiples of vectors.\n   \n      Let   be a matrix transformation,\n      and let   and   be vectors in the domain of   so that\n        and \n       .\n     \n            Exactly which vector is  ?\n            Explain.\n           \n            If   and   are any scalars,\n            what is the vector  ?\n            Why?\n           \n    As we saw in  ,\n    we can combine the two properties of a matrix transformation   into one:\n    for any scalars   and   and any vectors   and   \n    in the domain of   we have\n     .\n   \n    We can then extend equation  \n    (by mathematical induction)\n    to any finite linear combination of vectors.\n    That is, if  ,  ,  ,\n      are any vectors in the domain of a matrix transformation   and if  ,\n     ,  ,   are any scalars, then\n     .\n   linear \n    There is one other important property of a matrix transformation for us to consider.\n    The functions we encountered in earlier mathematics courses,\n    e.g.,  , could send the input 0 to any output.\n    However, as a consequence of the definition,\n    any matrix transformation   maps the zero vector to the zero vector because\n     .\n   \n    Note that the two vectors   in the last equation may not be the \n    same vector   if  ,\n    then the first   is in   and the second in  .\n    It should be clear from the context which vector   is meant.\n   Onto and One-to-One Transformations \n    The problems we have been asking about solutions to systems of linear equations can \n    be rephrased in terms of matrix transformations.\n    The question about whether a system\n      is consistent for any vector   is also a question about the \n    existence of a vector   so that  ,\n    where   is the matrix transformation defined by  .\n   \n      Let   be the matrix transformation defined by   where   is\n       .\n     \n            Find   and  .\n            If it is not possible to find one or both of the output vectors,\n            indicate why.\n           \n            What are the domain and codomain of  ?\n            Why? (Recall that the domain is the space of all input vectors,\n            while the codomain is the space in which the output vectors are contained.)\n           \n            Can you find a vector   for which  ?\n            Can you find a vector   for which  ?\n           \n            Which   are the image vectors for this transformation?\n            Is the range of   equal to the codomain of  ?\n            Explain.\n           \n            The previous question can be rephrased as a matrix equation question.\n            We are asking whether   is consistent for every  .\n            How is the answer to this question related to the pivots of  ?\n           \n    If   is a matrix transformation,\n     \n    illustrates that the range of a matrix transformation   may not equal its codomain.\n    In other words,\n    there may be vectors   in the codomain of   that are not the image of \n    any vector in the domain of  .\n    If it is the case for a matrix transformation   that there is always a vector \n      in the domain of   such that\n      for any vector   in the codomain of  ,\n    then   is given a special name.\n   onto onto at least one \n    So the matrix transformation   from   to   defined by\n      is onto if the equation\n      has a solution for each vector   in  .\n    Since the vectors   are linear combinations of the columns of  ,\n      is onto exactly when the span of the columns of   is all of  .\n     \n    shows us that   is onto if every row of   contains a pivot.\n   \n    Another question to ask about matrix transformations is how many vectors there \n    can be that map onto a given output vector.\n   \n      Let   be the matrix transformation defined by   where   is\n       .\n     \n            Find   and \n             .\n            If it is not possible to find one or both of the output vectors,\n            indicate why.\n           \n            What are the domain and codomain of  ?\n            Why?\n           \n            Find  .\n            Are there any other  's for which   is this same output vector?\n            \n           \n            Set up an equation to solve for such  's.\n           \n            Assume more generally that for some vector  ,\n            there is a vector   so that  .\n            Write this as a matrix equation to determine how many solutions this equation has.\n            Explain.\n            How is the answer to this question related to the pivots of  ?\n           \n    The uniqueness of a solution to\n      is the same as saying that the matrix transformation   defined by\n      maps exactly one vector to  .\n    A matrix transformation   that has the property that every image vector is an \n    image in exactly one way is also a special type of transformation.\n   one-to-one one-to-one at most \n    So the matrix transformation   from   to   defined by\n      is one-to-one if the equation\n      has a unique solution whenever   is consistent.\n    Since the vectors   are linear combinations of the columns of  ,\n    the unique solution requirement indicates that any output vector can be written in \n    exactly one way as a linear combination of the columns of  .\n    This implies that the columns of   are linearly independent.\n     \n    indicates that this happens when every column of   is a pivot column.\n   \n    To summarize,\n    if   is a matrix transformation defined by  ,\n    then   is onto if every row of   contains a pivot,\n    and   is one-to-one if every column of   is a pivot column.\n    It is important to note the difference:\n    being one-to-one depends on the rows of   and being onto depends on the columns \n    of  .\n   \n    Having a matrix transformation from   to   can tell us things \n    about   and  .\n    For example,\n    when a matrix transformation from   to   is one-to-one,\n    it means that there is a unique input vector for every output vector.\n    Since a matrix transformation preserves the algebraic structure of  ,\n    this implies that the collection of the images of the vectors in the domain of \n      form a copy of   inside of  .\n    If we think of   as a one-to-one matrix transformation with\n      for some   matrix,\n    then every column of   will have to be a pivot column.\n    It follows that if there is a one-to-one matrix transformation from   to  ,\n    we must have  .\n    Similarly, if a matrix transformation   from   to   is onto,\n    then for each   in  ,\n    if we select one vector in the domain of   whose image is  ,\n    then the collection of these vectors in the domain of   is a copy of   \n    inside of  .\n    So if there is an onto matrix transformation from   to  ,\n    then  .\n    As a consequence,\n    the only way a matrix transformation from   to   is both one-to-one \n    and onto is if  .\n   \n    We conclude this section by adding new equivalent conditions to\n      \n    and  \n    from  \n    and  .\n   \n        Let   be an   matrix.\n        The following statements are equivalent.\n         \n             \n              The matrix equation   has a solution for every vector \n                in  .\n             \n           \n             \n              Every vector   in   can be written as a linear combination \n              of the columns of  .\n             \n           \n             \n              The span of the columns of   is  .\n             \n           \n             \n              The matrix   has a pivot position in each row.\n             \n           \n             \n              The matrix transformation   from   to   \n              defined by   is onto.\n             \n           \n       \n        Let   be an   matrix.\n        The following statements are equivalent.\n         \n             \n              The matrix equation   has a unique solution for every \n              vector   in the span of the columns of  .\n             \n           \n             \n              The matrix equation   has the unique solution  .\n             \n           \n             \n              The columns of   are linearly independent.\n             \n           \n             \n              The matrix   has a pivot position in each column.\n             \n           \n             \n              The matrix transformation   from   to   \n              defined by   is one-to-one.\n             \n           \n       \n    We will continue to add to these theorems,\n    which will eventually give us many different but equivalent perspectives to look at a \n    linear algebra problem.\n    Please keep these equivalent criteria in mind when considering the best possible approach \n    to a problem.\n   Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Let   \n        and let  .\n       \n              Identify the domain of  .\n              Explain your reasoning.\n             \n              Since   is a   matrix,\n                has four columns.\n              Now   is a linear combination of the columns of   with \n              weights from  ,\n              so   must have four entries to correspond to the columns of  .\n              We conclude that the domain of   is  .\n             \n              Is   one-to-one.\n              Explain.\n             \n              Technology shows that the reduced row echelon form of   is\n               .\n              Since   contains non-pivot columns,\n              the homogeneous system   has infinitely many solutions.\n              So   is not one-to-one.\n              In other words,\n              if there is a column of   that is a non-pivot column,\n              then   is not one-to-one.\n             \n              Is   onto?\n              If yes, explain why.\n              If no, describe the range of   as best you can,\n              both algebraically and graphically.\n             \n              Since the reduced row echelon form of   has rows of zeros,\n              there will be vectors   in   such that the reduced row \n              echelon form of\n                will have a row of the form\n                for some nonzero scalar  .\n              This means that   will have no solution and   \n              is not onto.\n              In other words,\n              if there is a row of   that does not contain a pivot,\n              then   is not onto.\n             \n        A matrix transformation   defined by\n         \n        is a contraction in the   direction if\n          and a dilation in the   direction if  .\n       \n              Find a matrix   such that  .\n             \n              Since\n               ,\n              the matrix   \n              has the property that  .\n             \n              Sketch the square   with vertices \n               ,\n               ,\n               ,\n              and  .\n              Determine and sketch the image of   under   if  .\n             \n              We can determine the image of   under   by calculating what \n                does to the vertices of  .\n              Notice that\n               \n              Since   is a linear map,\n              the image of   under   is the polygon with vertices  ,\n               ,  ,\n              and   as shown in  .\n              From  \n              we can see that   stretches the figure in the   direction \n              only by a factor of 2.\n               The input square   and the output  . \n             Summary one-to-one onto \n        Given matrix  ,\n        write the coordinate form of the transformation   defined by  .\n        (Note: Writing a transformation in coordinate form refers to writing the \n        transformation in terms of the entries of the input and output vectors.)\n       \n        If  ,\n        then  .\n       \n        Suppose the transformation   is defined by   where\n         .\n        Determine if   is \n        in the range of  .\n        If so, find all  's which map to  .\n       \n        Suppose   is a matrix transformation and\n         \n        Find  .\n       \n         .\n       \n        Given a matrix transformation defined as\n         \n        determine the matrix   for which  .\n       \n        Suppose a matrix transformation   defined by\n          for some unknown   matrix satisfies\n         .\n        Use the matrix transformation properties to determine   where \n         .\n        Use the expression for   to determine the matrix  .\n       \n         \n       \n        For each of the following matrices,\n        determine if the transformation   defined by\n          is onto and if   is one-to-one.\n       \n               \n             \n               \n             \n               \n             \n               \n             \n        Come up with an example of a one-to-one transformation from   to  ,\n        if possible.\n        If not, explain why not.\n       \n          defined by  \n       \n        Come up with an example of an onto transformation from   to  ,\n        if possible.\n        If not, explain why not.\n       \n        Come up with an example of a one-to-one but not onto transformation from \n          to  ,\n        if possible.\n        If not, explain why not.\n       \n        Not possible.\n       \n        Two students are talking about when a matrix transformation is one-to-one.\n         \n          Student 1: If we have a matrix transformation, then we need to check that \n            has a unique solution for every   for \n          which   has a solution, right?\n         \n         \n          Student 2: Well, that's the definition. Each   in the codomain has to be the \n          image of at most one   in the domain. So when   is in the range, \n          corresponding to   having a solution, then there is exactly one \n          solution  .\n         \n         \n          Student 1: But wouldn't it be enough to check that   has a \n          unique solution? Doesn't that translate to the other   vectors? If \n          there is a unique solution for one  , then there can't be infinitely \n          many solutions for another  .\n         \n         \n          Student 2: I don't know. It feels to me as if changing the right hand side \n          could change whether there is a unique solution, or infinitely many solutions, \n          or no solution.\n         \n        Which part of the above conversation do you agree with?\n        Which parts need fixing?\n       \n        Show that if   is a matrix transformation from   to   \n        and   is a line in  ,\n        then  ,\n        the image of  , is a line or a single vector.\n        (Note that a line in   is the set of all vectors of the form\n          where   is a scalar,\n        and   are two fixed vectors in  .)\n       \n         ,  \n       \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              The range of a transformation is the same as the codomain of the transformation.\n             \n              F\n             True\/False \n              The codomain of a transformation   defined by\n                is the span of the columns of  .\n             True\/False \n              A one-to-one transformation is a transformation where each input has a unique output.\n             \n              F\n             True\/False \n              A one-to-one transformation is a transformation where each output can only \n              come from a unique input.\n             True\/False \n              If a matrix transformation from   to   is one-to-one,\n              then it is also onto.\n             \n              T\n             True\/False \n              A matrix transformation from   to   cannot be onto.\n             True\/False \n              A matrix transformation from   to   cannot be onto.\n             \n              F\n             True\/False \n              A matrix transformation from   to   cannot be one-to-one.\n             True\/False \n              If the columns of a matrix   are linearly independent,\n              then the transformation   defined by   is onto.\n             \n              F\n             True\/False \n              If the columns of a matrix   are linearly independent,\n              then the transformation   defined by   is one-to-one.\n             True\/False \n              If   is an\n                matrix with   pivots,\n              then the transformation   is onto.\n             \n              F\n             True\/False \n              If   is an\n                matrix with   pivots,\n              then the transformation   is one-to-one.\n             True\/False \n              If   is in the range of a matrix transformation  ,\n              then there is an   in the domain of   such that  .\n             \n              T\n             True\/False \n              If   is a one-to-one matrix transformation,\n              then   has a non-trivial solution.\n             True\/False \n              If the transformations\n                and   are onto,\n              then the transformation   defined by \n                is also onto.\n             \n              T\n             True\/False \n              If the transformations\n                and   are one-to-one,\n              then the transformation   defined by \n                is also one-to-one.\n             Project: The Geometry of Matrix Transformations \n    In this section we will consider certain types of matrix transformations and analyze their \n    geometry.\n    Much more would be needed for real computer graphics,\n    but the essential ideas are contained in our examples.\n    A GeoGebra applet is available at  \n    for you to use to visualize the transformations in this project.\n   rotation matrix \n      We begin with transformations that produce the rotated dancing image in \n       .\n      Let   be the matrix transformation from   to   defined by\n       .\n      These matrices are the rotation matrices.\n     \n            Suppose  .\n            Then\n             .\n           \n                  Find the images of  ,\n                   ,\n                  and   under \n                   .\n                 \n                  Plot the points determined by the vectors from part i.\n                  The matrix transformation   performs a rotation.\n                  Based on this small amount of data,\n                  what would you say the angle of rotation is for this transformation  ?\n                 >\n           \n            Now let   be the general matrix transformation defined by the matrix\n             .\n            Follow the steps indicated to show that   performs a counterclockwise \n            rotation of an angle   around the origin.\n            Let   be the point defined by the vector\n              and   \n            the point defined by the vector\n              \n            as illustrated in  .\n             A rotation in the plane. \n           \n                  Use the angle sum trigonometric identities\n                   \n                  to show that\n                   .\n                 \n                  Now explain why the counterclockwise rotation around the origin by an angle\n                    can be represented by left multiplication by the matrix\n                   .\n                 \n     \n    presented the rotation matrices.\n    Other matrices have different effects.\n   \n      Different matrix transformations\n     shear shear contraction contraction \n    So far we have seen specific matrix transformations perform a rotations, shears,\n    and contractions.\n    We can combine these, and other,\n    matrix transformations by composition to change figures in different ways,\n    and to created animations of geometric figures. (As we will see later,\n    combining transformations needs to be done carefully in order to obtain the result we want.\n    For example, if we want to first rotate then translate,\n    in what order should the matrices be applied?)\n   "
},
{
  "id": "objectives-7",
  "level": "2",
  "url": "chap_matrix_transformations.html#objectives-7",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is a matrix transformation?\n           \n         \n           \n            What properties do matrix transformations have? (In particular,\n            what properties make matrix transformations  linear ?)\n           \n         \n           \n            What is the domain of a matrix transformation defined by an   matrix?\n            Why?\n           \n         \n           \n            What are the range and codomain of a matrix transformation defined by an \n              matrix?\n            Why?\n           \n         \n           \n            What does it mean for a matrix transformation to be one-to-one?\n            If   is a matrix transformation represented as  ,\n            what are the conditions on   that make   a one-to-one transformation?\n           \n         \n           \n            What does it mean for a matrix transformation to be onto?\n            If   is a matrix transformation represented as  ,\n            what are the conditions on   that make   an onto transformation?\n           \n         "
},
{
  "id": "F_Rotate_dance",
  "level": "2",
  "url": "chap_matrix_transformations.html#F_Rotate_dance",
  "type": "Figure",
  "number": "7.1",
  "title": "",
  "body": "A dancing figure and a rotated dancing figure. "
},
{
  "id": "p-1153",
  "level": "2",
  "url": "chap_matrix_transformations.html#p-1153",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "image "
},
{
  "id": "definition-15",
  "level": "2",
  "url": "chap_matrix_transformations.html#definition-15",
  "type": "Definition",
  "number": "7.2",
  "title": "",
  "body": "matrix transformation transformation matrix matrix transformation "
},
{
  "id": "F_PA_1g_1",
  "level": "2",
  "url": "chap_matrix_transformations.html#F_PA_1g_1",
  "type": "Figure",
  "number": "7.3",
  "title": "",
  "body": "Inputs and outputs of the transformation  . "
},
{
  "id": "pa_1_g",
  "level": "2",
  "url": "chap_matrix_transformations.html#pa_1_g",
  "type": "Preview Activity",
  "number": "7.1",
  "title": "",
  "body": "\n      We now consider other transformations from   to  .\n     \n            Suppose a transformation   is defined by\n             .\n           \n                  Find   for each of \n                   ,\n                   ,\n                   ,\n                  and  .\n                  (In other words, substitute\n                    into the formula above to see what \n                  output is obtained.)\n                 \n                  Plot all input vectors and their images on the same axes in  .\n                  Clearly identify which image corresponds to which input vector.\n                  Then give a geometric description of what this transformation does.\n                 \n            The transformation in the introduction performs a reflection across the  -axis.\n            Find a matrix transformation that performs a reflection across the  -axis.\n           \n            Suppose a transformation   is defined by\n             ,\n            where\n             .\n           \n                  Find   for each of \n                   ,\n                   ,\n                   ,\n                  and  .\n                 \n                  Plot all input vectors and their images on the same axes in  .\n                  Give a geometric description of this transformation.\n                 \n                  Is there an input vector which produces \n                    as an output vector?\n                 \n                  Find all input vectors that produce the output vector \n                   .\n                  Is there a unique input vector,\n                  or multiple input vectors?\n                 "
},
{
  "id": "p-1168",
  "level": "2",
  "url": "chap_matrix_transformations.html#p-1168",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "domain codomain range "
},
{
  "id": "definition-16",
  "level": "2",
  "url": "chap_matrix_transformations.html#definition-16",
  "type": "Definition",
  "number": "7.4",
  "title": "",
  "body": "range of a matrix transformation range "
},
{
  "id": "p-1170",
  "level": "2",
  "url": "chap_matrix_transformations.html#p-1170",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "image image "
},
{
  "id": "act_1_g_2",
  "level": "2",
  "url": "chap_matrix_transformations.html#act_1_g_2",
  "type": "Activity",
  "number": "7.2",
  "title": "",
  "body": "\n      Let   be a matrix transformation,\n      and let   and   be vectors in the domain of   so that\n        and \n       .\n     \n            Exactly which vector is  ?\n            Explain.\n           \n            If   and   are any scalars,\n            what is the vector  ?\n            Why?\n           "
},
{
  "id": "p-1180",
  "level": "2",
  "url": "chap_matrix_transformations.html#p-1180",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "linear "
},
{
  "id": "act_1_g_4",
  "level": "2",
  "url": "chap_matrix_transformations.html#act_1_g_4",
  "type": "Activity",
  "number": "7.3",
  "title": "",
  "body": "\n      Let   be the matrix transformation defined by   where   is\n       .\n     \n            Find   and  .\n            If it is not possible to find one or both of the output vectors,\n            indicate why.\n           \n            What are the domain and codomain of  ?\n            Why? (Recall that the domain is the space of all input vectors,\n            while the codomain is the space in which the output vectors are contained.)\n           \n            Can you find a vector   for which  ?\n            Can you find a vector   for which  ?\n           \n            Which   are the image vectors for this transformation?\n            Is the range of   equal to the codomain of  ?\n            Explain.\n           \n            The previous question can be rephrased as a matrix equation question.\n            We are asking whether   is consistent for every  .\n            How is the answer to this question related to the pivots of  ?\n           "
},
{
  "id": "definition-17",
  "level": "2",
  "url": "chap_matrix_transformations.html#definition-17",
  "type": "Definition",
  "number": "7.5",
  "title": "",
  "body": "onto onto at least one "
},
{
  "id": "act_1_g_5",
  "level": "2",
  "url": "chap_matrix_transformations.html#act_1_g_5",
  "type": "Activity",
  "number": "7.4",
  "title": "",
  "body": "\n      Let   be the matrix transformation defined by   where   is\n       .\n     \n            Find   and \n             .\n            If it is not possible to find one or both of the output vectors,\n            indicate why.\n           \n            What are the domain and codomain of  ?\n            Why?\n           \n            Find  .\n            Are there any other  's for which   is this same output vector?\n            \n           \n            Set up an equation to solve for such  's.\n           \n            Assume more generally that for some vector  ,\n            there is a vector   so that  .\n            Write this as a matrix equation to determine how many solutions this equation has.\n            Explain.\n            How is the answer to this question related to the pivots of  ?\n           "
},
{
  "id": "definition-18",
  "level": "2",
  "url": "chap_matrix_transformations.html#definition-18",
  "type": "Definition",
  "number": "7.6",
  "title": "",
  "body": "one-to-one one-to-one at most "
},
{
  "id": "thm_IMT_1_g_a",
  "level": "2",
  "url": "chap_matrix_transformations.html#thm_IMT_1_g_a",
  "type": "Theorem",
  "number": "7.7",
  "title": "",
  "body": "\n        Let   be an   matrix.\n        The following statements are equivalent.\n         \n             \n              The matrix equation   has a solution for every vector \n                in  .\n             \n           \n             \n              Every vector   in   can be written as a linear combination \n              of the columns of  .\n             \n           \n             \n              The span of the columns of   is  .\n             \n           \n             \n              The matrix   has a pivot position in each row.\n             \n           \n             \n              The matrix transformation   from   to   \n              defined by   is onto.\n             \n           \n       "
},
{
  "id": "thm_IMT_1_g_b",
  "level": "2",
  "url": "chap_matrix_transformations.html#thm_IMT_1_g_b",
  "type": "Theorem",
  "number": "7.8",
  "title": "",
  "body": "\n        Let   be an   matrix.\n        The following statements are equivalent.\n         \n             \n              The matrix equation   has a unique solution for every \n              vector   in the span of the columns of  .\n             \n           \n             \n              The matrix equation   has the unique solution  .\n             \n           \n             \n              The columns of   are linearly independent.\n             \n           \n             \n              The matrix   has a pivot position in each column.\n             \n           \n             \n              The matrix transformation   from   to   \n              defined by   is one-to-one.\n             \n           \n       "
},
{
  "id": "example-13",
  "level": "2",
  "url": "chap_matrix_transformations.html#example-13",
  "type": "Example",
  "number": "7.9",
  "title": "",
  "body": "\n        Let   \n        and let  .\n       \n              Identify the domain of  .\n              Explain your reasoning.\n             \n              Since   is a   matrix,\n                has four columns.\n              Now   is a linear combination of the columns of   with \n              weights from  ,\n              so   must have four entries to correspond to the columns of  .\n              We conclude that the domain of   is  .\n             \n              Is   one-to-one.\n              Explain.\n             \n              Technology shows that the reduced row echelon form of   is\n               .\n              Since   contains non-pivot columns,\n              the homogeneous system   has infinitely many solutions.\n              So   is not one-to-one.\n              In other words,\n              if there is a column of   that is a non-pivot column,\n              then   is not one-to-one.\n             \n              Is   onto?\n              If yes, explain why.\n              If no, describe the range of   as best you can,\n              both algebraically and graphically.\n             \n              Since the reduced row echelon form of   has rows of zeros,\n              there will be vectors   in   such that the reduced row \n              echelon form of\n                will have a row of the form\n                for some nonzero scalar  .\n              This means that   will have no solution and   \n              is not onto.\n              In other words,\n              if there is a row of   that does not contain a pivot,\n              then   is not onto.\n             "
},
{
  "id": "example-14",
  "level": "2",
  "url": "chap_matrix_transformations.html#example-14",
  "type": "Example",
  "number": "7.10",
  "title": "",
  "body": "\n        A matrix transformation   defined by\n         \n        is a contraction in the   direction if\n          and a dilation in the   direction if  .\n       \n              Find a matrix   such that  .\n             \n              Since\n               ,\n              the matrix   \n              has the property that  .\n             \n              Sketch the square   with vertices \n               ,\n               ,\n               ,\n              and  .\n              Determine and sketch the image of   under   if  .\n             \n              We can determine the image of   under   by calculating what \n                does to the vertices of  .\n              Notice that\n               \n              Since   is a linear map,\n              the image of   under   is the polygon with vertices  ,\n               ,  ,\n              and   as shown in  .\n              From  \n              we can see that   stretches the figure in the   direction \n              only by a factor of 2.\n               The input square   and the output  . \n             "
},
{
  "id": "p-1232",
  "level": "2",
  "url": "chap_matrix_transformations.html#p-1232",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "one-to-one onto "
},
{
  "id": "exercise-64",
  "level": "2",
  "url": "chap_matrix_transformations.html#exercise-64",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Given matrix  ,\n        write the coordinate form of the transformation   defined by  .\n        (Note: Writing a transformation in coordinate form refers to writing the \n        transformation in terms of the entries of the input and output vectors.)\n       \n        If  ,\n        then  .\n       "
},
{
  "id": "exercise-65",
  "level": "2",
  "url": "chap_matrix_transformations.html#exercise-65",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Suppose the transformation   is defined by   where\n         .\n        Determine if   is \n        in the range of  .\n        If so, find all  's which map to  .\n       "
},
{
  "id": "exercise-66",
  "level": "2",
  "url": "chap_matrix_transformations.html#exercise-66",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Suppose   is a matrix transformation and\n         \n        Find  .\n       \n         .\n       "
},
{
  "id": "exercise-67",
  "level": "2",
  "url": "chap_matrix_transformations.html#exercise-67",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Given a matrix transformation defined as\n         \n        determine the matrix   for which  .\n       "
},
{
  "id": "exercise-68",
  "level": "2",
  "url": "chap_matrix_transformations.html#exercise-68",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Suppose a matrix transformation   defined by\n          for some unknown   matrix satisfies\n         .\n        Use the matrix transformation properties to determine   where \n         .\n        Use the expression for   to determine the matrix  .\n       \n         \n       "
},
{
  "id": "exercise-69",
  "level": "2",
  "url": "chap_matrix_transformations.html#exercise-69",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        For each of the following matrices,\n        determine if the transformation   defined by\n          is onto and if   is one-to-one.\n       \n               \n             \n               \n             \n               \n             \n               \n             "
},
{
  "id": "exercise-70",
  "level": "2",
  "url": "chap_matrix_transformations.html#exercise-70",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        Come up with an example of a one-to-one transformation from   to  ,\n        if possible.\n        If not, explain why not.\n       \n          defined by  \n       "
},
{
  "id": "exercise-71",
  "level": "2",
  "url": "chap_matrix_transformations.html#exercise-71",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Come up with an example of an onto transformation from   to  ,\n        if possible.\n        If not, explain why not.\n       "
},
{
  "id": "exercise-72",
  "level": "2",
  "url": "chap_matrix_transformations.html#exercise-72",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        Come up with an example of a one-to-one but not onto transformation from \n          to  ,\n        if possible.\n        If not, explain why not.\n       \n        Not possible.\n       "
},
{
  "id": "exercise-73",
  "level": "2",
  "url": "chap_matrix_transformations.html#exercise-73",
  "type": "Exercise",
  "number": "10",
  "title": "",
  "body": "\n        Two students are talking about when a matrix transformation is one-to-one.\n         \n          Student 1: If we have a matrix transformation, then we need to check that \n            has a unique solution for every   for \n          which   has a solution, right?\n         \n         \n          Student 2: Well, that's the definition. Each   in the codomain has to be the \n          image of at most one   in the domain. So when   is in the range, \n          corresponding to   having a solution, then there is exactly one \n          solution  .\n         \n         \n          Student 1: But wouldn't it be enough to check that   has a \n          unique solution? Doesn't that translate to the other   vectors? If \n          there is a unique solution for one  , then there can't be infinitely \n          many solutions for another  .\n         \n         \n          Student 2: I don't know. It feels to me as if changing the right hand side \n          could change whether there is a unique solution, or infinitely many solutions, \n          or no solution.\n         \n        Which part of the above conversation do you agree with?\n        Which parts need fixing?\n       "
},
{
  "id": "exercise-74",
  "level": "2",
  "url": "chap_matrix_transformations.html#exercise-74",
  "type": "Exercise",
  "number": "11",
  "title": "",
  "body": "\n        Show that if   is a matrix transformation from   to   \n        and   is a line in  ,\n        then  ,\n        the image of  , is a line or a single vector.\n        (Note that a line in   is the set of all vectors of the form\n          where   is a scalar,\n        and   are two fixed vectors in  .)\n       \n         ,  \n       "
},
{
  "id": "exercise-75",
  "level": "2",
  "url": "chap_matrix_transformations.html#exercise-75",
  "type": "Exercise",
  "number": "12",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              The range of a transformation is the same as the codomain of the transformation.\n             \n              F\n             True\/False \n              The codomain of a transformation   defined by\n                is the span of the columns of  .\n             True\/False \n              A one-to-one transformation is a transformation where each input has a unique output.\n             \n              F\n             True\/False \n              A one-to-one transformation is a transformation where each output can only \n              come from a unique input.\n             True\/False \n              If a matrix transformation from   to   is one-to-one,\n              then it is also onto.\n             \n              T\n             True\/False \n              A matrix transformation from   to   cannot be onto.\n             True\/False \n              A matrix transformation from   to   cannot be onto.\n             \n              F\n             True\/False \n              A matrix transformation from   to   cannot be one-to-one.\n             True\/False \n              If the columns of a matrix   are linearly independent,\n              then the transformation   defined by   is onto.\n             \n              F\n             True\/False \n              If the columns of a matrix   are linearly independent,\n              then the transformation   defined by   is one-to-one.\n             True\/False \n              If   is an\n                matrix with   pivots,\n              then the transformation   is onto.\n             \n              F\n             True\/False \n              If   is an\n                matrix with   pivots,\n              then the transformation   is one-to-one.\n             True\/False \n              If   is in the range of a matrix transformation  ,\n              then there is an   in the domain of   such that  .\n             \n              T\n             True\/False \n              If   is a one-to-one matrix transformation,\n              then   has a non-trivial solution.\n             True\/False \n              If the transformations\n                and   are onto,\n              then the transformation   defined by \n                is also onto.\n             \n              T\n             True\/False \n              If the transformations\n                and   are one-to-one,\n              then the transformation   defined by \n                is also one-to-one.\n             "
},
{
  "id": "act_1_g_rotation",
  "level": "2",
  "url": "chap_matrix_transformations.html#act_1_g_rotation",
  "type": "Project Activity",
  "number": "7.5",
  "title": "",
  "body": "rotation matrix \n      We begin with transformations that produce the rotated dancing image in \n       .\n      Let   be the matrix transformation from   to   defined by\n       .\n      These matrices are the rotation matrices.\n     \n            Suppose  .\n            Then\n             .\n           \n                  Find the images of  ,\n                   ,\n                  and   under \n                   .\n                 \n                  Plot the points determined by the vectors from part i.\n                  The matrix transformation   performs a rotation.\n                  Based on this small amount of data,\n                  what would you say the angle of rotation is for this transformation  ?\n                 >\n           \n            Now let   be the general matrix transformation defined by the matrix\n             .\n            Follow the steps indicated to show that   performs a counterclockwise \n            rotation of an angle   around the origin.\n            Let   be the point defined by the vector\n              and   \n            the point defined by the vector\n              \n            as illustrated in  .\n             A rotation in the plane. \n           \n                  Use the angle sum trigonometric identities\n                   \n                  to show that\n                   .\n                 \n                  Now explain why the counterclockwise rotation around the origin by an angle\n                    can be represented by left multiplication by the matrix\n                   .\n                 "
},
{
  "id": "act_1_g_shear",
  "level": "2",
  "url": "chap_matrix_transformations.html#act_1_g_shear",
  "type": "Project Activity",
  "number": "7.6",
  "title": "Different matrix transformations.",
  "body": "\n      Different matrix transformations\n     shear shear contraction contraction "
},
{
  "id": "chap_matrix_operations",
  "level": "1",
  "url": "chap_matrix_operations.html",
  "type": "Section",
  "number": "8",
  "title": "Matrix Operations",
  "body": "Matrix Operations \n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            Under what conditions can we add two matrices and how is the matrix sum defined?\n           \n         \n           \n            Under what conditions can we multiply a matrix by a scalar and how is a scalar \n            multiple of a matrix defined?\n           \n         \n           \n            Under what conditions can we multiply two matrices and how is the matrix product defined?\n           \n         \n           \n            What properties do matrix addition,\n            scalar multiplication of matrices and matrix multiplication satisfy?\n            Are these properties similar to properties that are satisfied by vector operations?\n           \n         \n           \n            What are two properties that make matrix multiplication fundamentally \n            different than our standard product of real numbers?\n           \n         \n           \n            What is the interpretation of matrix multiplication from the perspective \n            of linear transformations?\n           \n         \n           \n            How is the transpose of a matrix defined?\n           \n         Application: Algorithms for Matrix Multiplication \n    Matrix multiplication is widely used in applications ranging from scientific computing \n    and pattern recognition to counting paths in graphs.\n    As a consequence,\n    much work is being done in developing efficient algorithms for matrix multiplication.\n   \n    We will see that a matrix product can be calculated through the row-column method.\n    Recall that the product of two   matrices\n      \n    and   is given by\n     ,\n   \n    This product involves eight scalar multiplications and some scalar additions.\n    As we will see,\n    multiplication is more computationally expensive than addition,\n    so we will focus on multiplication.\n    In 1969, a German mathematician named Volker Strassen showed \n    Strassen, Volker, Gaussian Elimination is not Optimal, Number.\n    Math. 13, p. 354-356, 1969\n      that the product of two\n      matrices can be calculated using only seven multiplications.\n    While this is not much of an improvement,\n    the Strassen algorithm can be applied to larger matrices,\n    using matrix partitions\n    (which allow for parallel computation),\n    and its publication led to additional research on faster algorithms for matrix multiplication.\n    More details are provided later in this section.\n   Introduction \n    A vector is a list of numbers in a specified order and a matrix is an ordered array of objects.\n    In fact, a vector can be thought of as a matrix of size  .\n    Vectors and matrices are so alike in this way that it would seem natural that we can \n    define operations on matrices just as we did with vectors.\n   columns size \n    We can generalize the operations of addition and scalar multiplication on vectors to \n    matrices similarly.\n    Given two matrices   and   of the same size,\n    we define the sum   by\n     \n    when the sizes of the matrices   and   match.\n    In other words,\n    for matrices of the same size the matrix addition is defined by adding \n    corresponding entries in the matrices.\n    For example,\n     .\n   \n   matrix scalar multiple \n    We define the scalar multiple of a matrix\n      by scalar   to be the matrix   defined by\n     ,\n   \n    This means that we multiply each entry of the matrix   by the scalar  .\n    As an example,\n     .\n   \n    Even though we did not have a multiplication operation on vectors,\n    we had a matrix-vector product,\n    which is a special case of a matrix-matrix product since a vector is a matrix with one column.\n    However, generalizing the matrix-vector product to a matrix-matrix product is not \n    immediate as it is not immediately clear what we can do with the other columns.\n    We will consider this question in this section.\n   \n    Note that all of the matrix operations can be performed on a calculator.\n    After entering each matrix in the calculator, just use  ,\n      and   operations to find the result of the matrix operation.\n    (Just for fun,\n    try using   with matrices to see if it will work.)\n   \n            Pick three different varying sizes of pairs of   matrices which can be added.\n            For each pair:\n           \n                  Find the matrices   and  .\n                 \n                  How are the two matrices   and   related?\n                  What does this tell us about matrix addition?\n                 \n            Let  ,\n             ,\n            and  .\n            Determine the entries of the matrix  .\n           \n            Now we turn to multiplication of matrices.\n            Our first goal is to find out what conditions we need on the sizes of \n            matrices   and   if the matrix-matrix product   is \n            defined and what the size of the resulting product matrix is.\n            We know the condition and the size of the result in the special case of   \n            being a vector,\n            i.e., a matrix with one column.\n            So our conjectures for the general case should match what we know in the special case.\n            In each part of this problem,\n            use any appropriate tool (e.g., your calculator, Maple, Mathematica, \n            Wolfram Alpha) to determine the matrix product  ,\n            if it exists.\n            If you obtain a product,\n            write it down and explain how its size is related to the sizes of   and  .\n            If you receive an error,\n            write down the error and guess why the error occurred and\/or what it means.\n           \n                   \n                 \n                   \n                 \n                   \n                 \n                   \n                 \n                  Make a guess for the condition on the sizes of two matrices   for which the product   is defined.\n                  How is the size of the product matrix related to the sizes of   and  ?\n                 \n            The final matrix products, when defined,\n            in  problem \n            might seem unrelated to the individual matrices at first.\n            In this problem,\n            we will uncover this relationship using our knowledge of the matrix-vector product.\n            Let   \n            and  .\n           \n                  Calculate   using any tool.\n                 \n                  Using the matrix-vector product,\n                  calculate   where   is the first column \n                  (i.e., calculate  ),\n                  and then the second column of   \n                  (i.e., calculate  ),\n                  and then the third column of   \n                  (i.e., calculate  ).\n                  Do you notice these output vectors within  ?\n                 \n                  Describe as best you can a definition of   using the matrix-vector \n                  product.\n                 Properties of Matrix Addition and Multiplication by Scalars \n    Just as we were able to define an algebra of vectors with addition and multiplication by scalars,\n    we can define an algebra of matrices.\n    We will see that the properties of these operations on matrices are immediate generalizations \n    of the properties of the operations on vectors.\n    We will then see how the matrix product arises through the connection of matrices to linear \n    transformations.\n    Finally, we define the transpose of a matrix.\n    The transpose of a matrix will be useful in applications such as graph theory and \n    least-squares fitting of curves,\n    as well as in advanced topics such as inner product spaces and the dual space of a \n    vector space.\n   \n   matrix sum \n    We learned in  \n    that we can add two matrices of the same size together by adding corresponding entries \n    and we can multiply any matrix by a scalar by multiplying each entry of the matrix by \n    that scalar.\n    More generally, if   and   are\n      matrices and   is any scalar, then\n     .\n   \n    As we have done each time we have introduced a new operation,\n    we ask what properties the operation has.\n    For example,\n    you determined in  \n    that addition of matrices is a commutative operation.\n    More specifically, for every two\n      matrices   and  ,  .\n    We can use similar arguments to verify the following properties of matrix addition \n    and multiplication by scalars.\n    Notice that these properties are very similar to the properties of addition and scalar \n    multiplication of vectors we discussed earlier.\n    This should come as no surprise since the  -dimensional vectors are   \n    matrices.\n    In a strange twist,\n    we will see that matrices themselves can be considered as vectors when we discuss vector \n    spaces in a later section.\n   commutative associative zero matrix additive inverse vector space A Matrix Product \n      As we saw in  ,\n      a matrix-matrix product can be found in a way which makes use of and also generalizes\n       the matrix-vector product.\n     matrix product matrix product \n      We now consider the motivation behind this definition by thinking about the matrix \n      transformations corresponding to each of the matrices   and  .\n      Recall that left multiplication by an\n        matrix   defines a transformation   from  \n       to   by  .\n      The domain of   is   because the number of components of   \n      have to match the number of entries in each of row of   in order for the \n      matrix-vector product   to be defined.\n      Similarly, a   matrix   defines a transformation   from \n        to  .\n      Since transformations are functions,\n      we can compose them as long as the output vectors of the inside transformation \n      lie in the domain of the outside transformation.\n      Therefore if   is the inside transformation and   is the outside transformation,\n      the composition   is defined.\n      So a natural question to ask is if we are given\n\n       \n           \n            a transformation   from   to   where\n              for an   matrix   and\n           \n         \n           \n            a transformation   from   to   with\n              for some   matrix  ,\n           \n         \n\n      is there a matrix that represents the transformation\n        defined by  ?\n      We investigate this question in the next activity in the special case of a\n        matrix   and a   matrix  .\n     \n        In this activity,\n        we look for the meaning of the matrix product from a transformation perspective.\n        Let   and   be matrix transformations defined by\n         ,\n        where\n         .\n       \n              What are the domains and codomains of   and  ?\n              Why is the composite transformation   defined?\n              What is the domain of  ?\n              What is the codomain of  ? (Recall that\n                is defined by  ,\n              i.e., we substitute the output   as the input into the \n              transformation  .)\n             \n              Let  .\n              Determine the components of  .\n             \n              Find the components of  .\n             \n              Find a matrix   so that  .\n             \n              Use the definition of composition of transformations and the definitions \n              of the   and   transformations to explain why it is reasonable \n              to define   to be the matrix  .\n              Does the matrix   agree with the\n               \n              you found in   using technology?\n             \n      We now consider this result in the general case of a\n        matrix   and an   matrix  ,\n      where   and   define matrix transformations   and  ,\n      respectively.\n      In other words,\n        and   are matrix transformations defined by\n        and  .\n      The domain of   is   and the codomain is  .\n      The domain of   is   and the codomain is  .\n      The composition   is defined because the output vectors of   \n      are in   and they lie in the domain of  .\n      The domain of   is the same as the domain of   since the input \n      vectors first go through the   transformation.\n      The codomain of   is the same as the codomain of   since the \n      final output vectors are produced by applying the   transformation.\n     \n      Let us see how we can obtain the matrix corresponding to the transformation  .\n      Let  ,\n      where   is the  th column of  ,\n      and let  .\n      Recall that the matrix vector product   is the linear combination of the \n      columns of   with the corresponding weights from  .\n      So\n       .\n     \n      Note that each of the   vectors are in   since   is an \n        matrix.\n      Therefore, each of these vectors can be multiplied by matrix   and we can \n      evaluate  .\n      Therefore,   is defined and\n       .\n     \n      The properties of matrix-vector products show that\n       .\n     \n      This expression is a linear combination of  's with  's \n      being the weights.\n      Therefore, if we let   be the matrix with columns  ,\n       ,  ,  , that is\n       ,\n      then\n       \n      by definition of the matrix-vector product.\n      Combining equations  ,\n       , and   shows that\n       \n      where  .\n     \n      Also note that since   and  , we find\n       .\n     \n      Since the matrix representing the transformation   is the matrix\n       \n      where  ,  ,\n       ,\n        are the columns of the matrix  ,\n      it is natural to define   to be this matrix in light of equation \n       .\n     \n      Matrix multiplication has some properties that are unfamiliar to us as the next\n       activity illustrates.\n     \n        Let  ,\n         ,\n         ,\n          and  .\n       \n              Find the indicated products\n              (by hand or using a calculator).\n               \n             \n              Is matrix multiplication commutative?\n              Explain.\n             \n              Is there an identity element for matrix multiplication?\n              In other words,\n              is there a matrix   for which   for any matrix  ?\n              Explain.\n             \n              If   and   are real numbers with  ,\n              then we know that either   or  .\n              Is this same property true with matrix multiplication?\n              Explain.\n             \n              If  ,  , and   are real numbers with\n                and  , we know that  .\n              Is this same property true with matrix multiplication?\n              Explain.\n             commute \n      There is an alternative method of calculating a matrix product that we will often use \n      that we illustrate in the next activity.\n      This alternate version depends on the product of a row matrix with a vector.\n      Suppose   is a   matrix and\n        \n      is an   vector.\n      Then the product   is the   vector\n       .\n     \n      In this situation, we usually identify the\n        matrix with its scalar entry and write\n       .\n     scalar dot \n        Let   and  .\n       \n        Let   be the  th row of   and   the \n         th column of  .\n        For example,\n          and \n         .\n       \n        Calculate the entries of the matrix  , where\n         ,\n        where   refers to the scalar product of row \n          of   with column   of  . \n        Recall from  \n        of  \n        that the scalar product   of a   matrix\n          and an   vector\n          is  .\n          Compare your result with the result of   calculated via the product \n        of   with the columns of  .\n       \n       \n      shows that these is an alternate way to calculate a matrix product.\n      To see how this works in general,\n      let   be a   matrix and\n        an   matrix.\n      We know that\n       .\n     \n      Now let  ,  ,  ,\n        be the rows of   so that \n       .\n      First we argue that if  , then\n       .\n     scalar product dot product \n      To show that this definition gives the same result as the linear combination \n      definition of matrix-vector product,\n      we first let  ,\n      where  ,  ,\n       ,   are the columns of  .\n      By our linear combination definition of the matrix-vector product,\n      we obtain\n       .\n     \n      Therefore, the above work shows that both linear combination and scalar product \n      definitions give the same matrix-vector product.\n     \n      Applying this to the matrix product   defined in terms of the matrix-vector product,\n      we see that\n       .\n     \n      So the  th entry of the matrix product   is found by taking the \n      scalar product of the  th row of   with the  th column of  .\n      In other words,\n       \n      where   is the  th row of   and   is the \n       th column of  .\n     Properties of Matrix Multiplication arbitrary arbitrary \n      Similar arguments can be used to show the following properties of matrix multiplication.\n     associative distributes over matrix addition distributes over matrix addition (multiplicative) identity matrix identity matrix identity matrix \n      We also write the matrix   as\n       .\n     \n      The matrix   has the property that for any   matrix  ,\n       .\n      so   is a multiplicative identity in the set of all   matrices.\n      More generally, for an   matrix  ,\n       .\n     The Transpose of a Matrix \n    One additional operation on matrices is the transpose.\n    The transpose of a matrix occurs in many useful formulas in linear algebra and in \n    applications of linear algebra.\n   matrix transpose transpose \n    Written out, the transpose of the   matrix\n     \n    is the   matrix\n     .\n   diagonal \n            Find the transpose of each of the indicated matrices.\n             \n           \n            Find the transpose of the new matrix for each part above.\n            What can you conjecture based on your results?\n            There are certain special types of matrices that are given names.\n           \n            There are certain special types of matrices that are given names.\n           \n                Let   be a square matrix whose  th entry is  .\n                 \n                     \n                      The matrix   is a  diagonal matrix \n                         diagonal matrix \n                      if   whenever  .\n                     \n                   \n                     \n                      The matrix   is a  symmetric \n                         symmetric matrix \n                      matrix if  .\n                     \n                   \n                     \n                      The matrix   is an  upper triangular \n                         upper triangular matrix \n                      if   whenever  .\n                     \n                   \n                     \n                      The matrix   is a  lower triangular \n                         lower triangular matrix \n                      if   whenever  .\n                     \n                   \n               \n          Find an example of a diagonal matrix  .\n          What can you say about  ?\n         \n          Find an example of a non-diagonal symmetric matrix  .\n          If  , must   be a square matrix?\n         \n          Find an example of an upper triangular matrix  .\n          What kind of a matrix is  ?\n         \n    We will see later that diagonal matrices are important in that their powers are easy to \n    calculate.\n    Symmetric matrices arise frequently in applications such as in graph theory as \n    adjacency matrices and in quantum mechanics as observables,\n    and have many useful properties including being diagonalizable and having real eigenvalues,\n    as we will also see later.\n   Properties of the Matrix Transpose \n    As with every other operation,\n    we want to understand what properties the matrix transpose has.\n    Properties of transposes are shown in the following theorem.\n   \n        Let   and   be matrices of the appropriate sizes and let\n           be a scalar.\n        Then\n         \n             \n               \n             \n           \n             \n               \n             \n           \n             \n               \n             \n           \n             \n               \n             \n           \n       \n    The one property that might seem strange is the third one.\n    To understand this property,\n    suppose   is an   matrix and   an\n      matrix so that the product   is defined.\n    We will argue that   by comparing the \n     th entry of each side.\n     \n         \n          First notice that the  th entry of\n            is the  th entry of  .\n          The  th entry of   is found by taking the scalar\n           product of the  th row of   with the  th column of  .\n          Thus, the  th entry of\n            is the scalar product of the  th row of \n            with the  th column of  .\n         \n       \n         \n          The  th entry of   is the scalar product \n          of the  th row of   with the  th column of  .\n          But the  th row of   is the  th column of \n            and the  th column of   is the  th row of  .\n          So the  th entry of\n            is the scalar product of the  th row of \n            with the  th column of  .\n         \n       \n   \n    Since the two matrices   and\n      have the same size and same corresponding entries,\n    they are the same matrix.\n   Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Let\n         .\n       \n        Determine the results of the following operations, if defined.\n        If not defined, explain why.\n       \n               \n             \n              Since   is a   matrix and   is a   matrix,\n              the number of columns of   equals the number of rows of   and the \n              matrix produce   is defined.\n              Recall that if  ,\n              where  ,  ,\n                are the columns of  ,\n              then  .\n              Recall also that   is the linear combination of the columns of \n                with weights from  , so\n               ,\n               ,\n              and\n               .\n              So  .\n              Alternatively,\n              if  ,\n              then the matrix product   is the matrix whose   entry is \n               .\n              Using this method we have\n               .\n              Now\n               ,\n              so  .\n             \n               \n             \n              Since   is a   matrix but   is  ,\n              the number of columns of   is not equal to the number of rows of  .\n              We conclude that   is not defined.\n             \n               \n             \n              Since   is a   matrix and   is  ,\n              the number of columns of   is equal to the number of rows of  .\n              Thus, the quantity   is defined.\n              First we calculate   using the dot product of the rows of   \n              with the columns of  .\n              Letting   and  ,\n              where  ,  ,\n              and   are the rows of   and  ,\n               ,\n              and   are the columns of  , we have\n               .\n              Now\n               ,\n              so  .\n              If   \n              and  ,\n              where  ,  ,\n              and   are the rows of   and  ,\n               ,\n               , and   are the columns of  , then\n               .\n              Now\n               ,\n              so  .\n             \n               \n             \n              Since   and   are both   matrices,\n              their sum is defined and is a   matrix.\n              Because   is   matrix,\n              the number of columns of   is equal to the number of rows of  .\n              Thus, the quantity   is defined and,\n              using the row-column method of matrix multiplication as earlier,\n               .\n             \n               \n             \n              Since   is a   matrix and   is  ,\n              the number of columns of   is equal to the number of rows of  .\n              Thus,   is defined and\n               .\n             \n               \n             \n              The fact that   is a\n                matrix means that   is a   matrix.\n              Since   is also a   matrix,\n              the sum   is defined.\n              The transpose of any matrix is also defined,\n              so   is defined and\n               .\n             \n        Let   \n        and  .\n       \n              Determine the matrix sum  .\n              Then use this sum to calculate  .\n             \n              Adding corresponding terms shows that \n               .\n              Squaring this sum yields the result \n               .\n             \n              Now calculate   in a different way.\n              Use the fact that matrix multiplication distributes over matrix addition to expand\n              (like foiling)\n                into a sum of matrix products.\n              The calculate each summand and add to find  .\n              You should obtain the same result as part (a).\n              If not, what could be wrong?\n             \n              Expanding  \n              (remember that matrix multiplication is not commutative)\n              gives us\n               \n              just as in part (a).\n              If instead you obtained the matrix\n                \n              you likely made the mistake of equating   with  .\n              These two matrices are not equal in general,\n              because we cannot say that   is equal to  .\n             Summary \n    In this section we defined a matrix sum,\n    scalar multiples of matrices,\n    the matrix product, and the transpose of a matrix.\n     \n         \n          The sum of two   matrices\n            and   is the\n            matrix   whose  th entry is  .\n         \n       \n         \n          If   is an   matrix,\n          the scalar multiple   of   by the scalar   is the\n            matrix whose  th entry is  .\n         \n       \n         \n          If   is a   matrix and\n            is an   matrix,\n          then the matrix product   of the matrices   and   is the \n            matrix\n           .\n          The matrix product is defined in this way so that the matrix of a composite\n            of linear transformations is the product of matrices of \n            and  .\n         \n       \n         \n          An alternate way of calculating the product of an\n            matrix   with rows  ,\n           ,  ,\n            and an   matrix   with columns  ,\n           ,  ,\n            is that the product   is the\n            matrix whose  th entry is  .\n         \n       \n         \n          Matrix multiplication does not behave as the standard multiplication on real numbers.\n          For example,\n          we can have a product of two non-zero matrices equal to the zero matrix and\n          there is no cancellation law for matrix multiplication.\n         \n       \n         \n          The transpose of an   matrix   is the\n            matrix   whose  th entry is  .\n         \n       \n   \n        Calculate   for each of the following matrix pairs by hand in two ways.\n       \n               ,\n               \n             \n               \n             \n               ,\n               \n             \n               .\n             \n        For each of the following   matrices,\n        find all   matrices\n          which \n        commute with the given  .\n        (Two matrices   and   commute with each other if  .)\n       \n                 \n             \n                 \n             \n                \n             \n        Find all possible, if any,\n          matrices satisfying each of the following matrix equations.\n       \n               \n             \n               \n             \n               \n             \n              There is no matrix   with this property.\n             \n               \n             \n               ,\n              where   and   can be any scalars\n             \n        For each of the following   matrices,\n        compute  .\n        Use your results to conjecture a formula for  .\n        Interpret your answer geometrically using the transformation interpretation.\n       \n                 \n             \n                 \n             \n                \n             \n        If   for unknown   matrix and   vector,\n        determine an expression for  ,\n         ,  ,  .\n       \n         \n       \n        If   and  ,\n        find an expression for   in terms of   and  .\n       \n        A matrix   is a  nilpotent  matrix if  ,\n        i.e.,   is the zero matrix,\n        for some positive integer  .\n        Explain why the matrices\n         \n        are nilpotent matrices.\n       \n          and  \n       \n        Suppose   is an   matrix for which  .\n        Show that there is a matrix   for which\n          where   is the identity matrix of size  .\n       \n        Let  ,  , and   be\n          matrices and let   and   be scalars.\n        Verify  .\n        That is, show that\n       \n               \n             \n               \n             \n               \n             \n              The   matrix   whose entries are all 0 has the \n              property that  .\n             \n               \n             \n              The scalar multiple   of the matrix   has the property \n              that  .\n             \n               \n             \n               \n             \n               \n             \n               \n             \n               \n             \n               .\n             \n        Let  ,  ,\n        and   be matrices of the appropriate sizes for all sums and products to be \n        defined and let   be a scalar.\n        Verify the remaining parts of  .\n        That is, show that\n       \n               \n             \n               \n             \n              There is a square matrix   with the property that   or\n                for whichever product is defined.\n             \n               \n             \n        Let   and\n          be matrices of the appropriate sizes,\n        and let   be a scalar.\n        Verify the remaining parts of  .\n        That is, show that\n       \n               \n             \n              Let  .\n              Then   by definition of the transpose.\n              Let  .\n              Then  .\n              So the  th entry of\n                is the same as the  th entry of  ,\n              and we conclude that  .\n             \n               \n             \n               \n             \n              The  th entry of   is  ,\n              so the  th entry of   is  .\n              But   is also the  th entry of  .\n              We conclude that  .\n             matrix exponential matrix exponential \n              Calculate  ,  ,  .\n              Explain why   for any positive integer  .\n             \n              Show that    is equal to\n               .\n             \n              Explain why  .\n             \n        Show that if   and   are   rotation matrices,\n        then   is also a   rotation matrix.\n       \n        If  ,\n         ,\n        then   equals\n         \n       \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n        Throughout, assume that matrices are of the appropriate sizes so that any matrix \n        sums or products are defined.\n       True\/False \n              For any three matrices   with  ,\n                implies  .\n             \n              F\n             True\/False \n              For any three matrices   with  ,\n                implies  .\n             True\/False \n              If   is the zero matrix,\n              then   itself is the zero matrix.\n             \n              F\n             True\/False \n              If   for every   matrix  ,\n              then   is the identity matrix  .\n             True\/False \n              If matrix products   and   are both defined,\n              then   and   are both square matrices of the same size.\n             \n              F\n             True\/False \n              If   is a solution for   (i.e., that\n               ) and   is a solution for  ,\n              then   is a solution for  .\n             True\/False \n              If   is an\n                matrix with two equal columns,\n              then the matrix   has two equal columns for every   matrix.\n             \n              T\n             True\/False \n              If  ,\n              then   or  .\n             Project: Strassen's Algorithm and Partitioned Matrices Strassen's algorithm \n    Strassen's algorithm\n    is an algorithm for matrix multiplication that can be more efficient than the standard \n    row-column method.\n    To understand this method,\n    we begin with the   case which will highlight the essential ideas.\n   \n      We first work with the   case.\n     \n            Let   \n            and  .\n           \n                  Calculate the matrix product  .\n                 \n                  Rather than using eight multiplications to calculate  , Strassen \n                  came up with the idea of using the following seven products:\n                   .\n                  Calculate   through   for the given matrices   and  .\n                  Then calculate the quantities\n                   .\n                  What do you notice?\n                 \n            Now we repeat part (a) in general.\n            Suppose we want to calculate the matrix product   for arbitrary \n              matrices\n              and  .\n            Let\n             .\n            Show that\n             .\n           \n   partitioned matrices \n    The next step is to understand how Strassen's algorithm can be applied to larger matrices.\n    This involves the idea of partitioned\n    (or block)\n    matrices.\n    Recall that the matrix-matrix product of the\n      matrix   and the\n      matrix   is defined as\n     .\n   \n    In this process,\n    we think of   as being partitioned into   columns.\n    We can expand on this idea to partition both   and   when calculating a matrix-matrix product.\n   \n      We illustrate the idea of partitioned matrices with an example.\n      Let  .We can partition   into smaller matrices\n       ,\n      which are indicated by the vertical and horizontal lines.\n      As a shorthand, we can describe this partition of   as\n       ,\n      where  ,\n       ,\n       ,\n      and  .\n      The submatrices   are called  blocks .\n      If   is a matrix such that   is defined,\n      then   must have five rows.\n      As an example,\n        is defined if  .\n      The partition of   breaks   up into blocks with three and two columns,\n      respectively.\n      So if we partition   into blocks with three and two rows,\n      then we can use the blocks to calculate the matrix product  .\n      For example, partition   as\n       .\n     \n      Show that\n       .\n     \n    An advantage to using partitioned matrices is that computations with them can be done \n    in parallel,\n    which lessens the time it takes to do the work.\n    In general, we can multiply partitioned matrices as though the submatrices are scalars.\n    That is,\n     ,\n    where\n     ,\n    provided that all the submatrix products are defined.\n   \n    Now we can apply Strassen's algorithm to larger matrices using partitions.\n    This method is sometimes referred to as divide and conquer.\n   \n      Let   and   be two   matrices.\n      If   is not a power of  ,\n      then pad the rows and columns of   and   with zeros to make them of size\n        for some integer  .\n      (From a practical perspective,\n      we might instead just use unequal block sizes.)\n      Let  .\n      Partition   and   as\n       ,\n      where each submatrix is of size  .\n      Now we use the Strassen algorithm just as in the   case,\n      treating the submatrices as if they were scalars\n      (with the additional constraints of making sure that the dimensions match up so \n      that products are defined,\n      and ensuring we multiply in the correct order).\n      Letting\n       ,\n      then the same algebra as in   shows that\n       .\n     \n      Apply Strassen's algorithm to calculate the matrix product  , where\n       .\n     \n    While Strassen's algorithm can be more efficient,\n    it does not always speed up the process.\n    We investigate this in the next activity.\n   \n      We introduce a little notation to help us describe the efficiency of our calculations.\n      We won't be formal with this notation,\n      rather work with it in an informal way.\n      Big O (the letter\n       O \n      ) notation is used to describe the complexity of an algorithm.\n      Generally speaking,\n      in computer science big O notation can be used to describe the run time of an algorithm,\n      the space used by the algorithm,\n      or the number of computations required.\n      The letter\n       O \n      is used because the behavior described is also called the order.\n      Big O measures the asymptotic time of an algorithm, not its exact time.\n      For example,\n      if it takes   steps to complete an algorithm,\n      then we say that the algorithm grows at the order of  \n      (we ignore the constants and the smaller power terms,\n      since they become insignificant as   increases)\n      and we describe its growth as  .\n      To measure the efficiency of an algorithm to determine a matrix product,\n      we will measure the number of operations it takes to calculate the product.\n     \n            Suppose   and   are   matrices.\n            Explain why the operation of addition\n            (that is, calculating  )\n            is  .\n           \n            Suppose   and   are   matrices.\n            How many multiplications are required to calculate the matrix product  ?\n            Explain.\n           \n            The standard algorithm for calculating a matrix product of two\n              matrices requires   multiplications and a number of additions.\n            Since additions are much less costly in terms of operations,\n            the standard matrix product is  .\n            We won't show it here,\n            but using Strassen's algorithm on a product of   matrices is\n             , where  .\n            That means that Strassen's algorithm applied to an   matrix\n            (where   is a power of  )\n            requires approximately   multiplications.\n            We use this to analyze situations to determine when Strassen's algorithm is \n            computationally more efficient than the standard algorithm.\n           \n                  Suppose   and   are   matrices.\n                  Determine the number of multiplications required to calculate the matrix \n                  product   using the standard matrix product.\n                  Then determine the approximate number of multiplications required to \n                  calculate the matrix product   using Strassen's algorithm.\n                  Which is more efficient? (Remember,\n                  we can only apply Strassen's algorithm to square matrices whose sizes are \n                  powers of  .)\n                 \n                  Repeat part i. with   matrices.\n                  Which method is more efficient?\n                 \n    As a final note, Strassen's algorithm is approximately  .\n    As of 2018, the best algorithm for matrix multiplication,\n    developed by Virginia Williams at Stanford University,\n    is approximately  . \n    V. V. Williams, Multiplying matrices in\n      time, Stanford University, (2014).\n     \n   "
},
{
  "id": "objectives-8",
  "level": "2",
  "url": "chap_matrix_operations.html#objectives-8",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            Under what conditions can we add two matrices and how is the matrix sum defined?\n           \n         \n           \n            Under what conditions can we multiply a matrix by a scalar and how is a scalar \n            multiple of a matrix defined?\n           \n         \n           \n            Under what conditions can we multiply two matrices and how is the matrix product defined?\n           \n         \n           \n            What properties do matrix addition,\n            scalar multiplication of matrices and matrix multiplication satisfy?\n            Are these properties similar to properties that are satisfied by vector operations?\n           \n         \n           \n            What are two properties that make matrix multiplication fundamentally \n            different than our standard product of real numbers?\n           \n         \n           \n            What is the interpretation of matrix multiplication from the perspective \n            of linear transformations?\n           \n         \n           \n            How is the transpose of a matrix defined?\n           \n         "
},
{
  "id": "p-1309",
  "level": "2",
  "url": "chap_matrix_operations.html#p-1309",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "columns size "
},
{
  "id": "pa_2_a",
  "level": "2",
  "url": "chap_matrix_operations.html#pa_2_a",
  "type": "Preview Activity",
  "number": "8.1",
  "title": "",
  "body": "\n            Pick three different varying sizes of pairs of   matrices which can be added.\n            For each pair:\n           \n                  Find the matrices   and  .\n                 \n                  How are the two matrices   and   related?\n                  What does this tell us about matrix addition?\n                 \n            Let  ,\n             ,\n            and  .\n            Determine the entries of the matrix  .\n           \n            Now we turn to multiplication of matrices.\n            Our first goal is to find out what conditions we need on the sizes of \n            matrices   and   if the matrix-matrix product   is \n            defined and what the size of the resulting product matrix is.\n            We know the condition and the size of the result in the special case of   \n            being a vector,\n            i.e., a matrix with one column.\n            So our conjectures for the general case should match what we know in the special case.\n            In each part of this problem,\n            use any appropriate tool (e.g., your calculator, Maple, Mathematica, \n            Wolfram Alpha) to determine the matrix product  ,\n            if it exists.\n            If you obtain a product,\n            write it down and explain how its size is related to the sizes of   and  .\n            If you receive an error,\n            write down the error and guess why the error occurred and\/or what it means.\n           \n                   \n                 \n                   \n                 \n                   \n                 \n                   \n                 \n                  Make a guess for the condition on the sizes of two matrices   for which the product   is defined.\n                  How is the size of the product matrix related to the sizes of   and  ?\n                 \n            The final matrix products, when defined,\n            in  problem \n            might seem unrelated to the individual matrices at first.\n            In this problem,\n            we will uncover this relationship using our knowledge of the matrix-vector product.\n            Let   \n            and  .\n           \n                  Calculate   using any tool.\n                 \n                  Using the matrix-vector product,\n                  calculate   where   is the first column \n                  (i.e., calculate  ),\n                  and then the second column of   \n                  (i.e., calculate  ),\n                  and then the third column of   \n                  (i.e., calculate  ).\n                  Do you notice these output vectors within  ?\n                 \n                  Describe as best you can a definition of   using the matrix-vector \n                  product.\n                 "
},
{
  "id": "thm_matrix_sum_properties",
  "level": "2",
  "url": "chap_matrix_operations.html#thm_matrix_sum_properties",
  "type": "Theorem",
  "number": "8.1",
  "title": "",
  "body": "commutative associative zero matrix additive inverse "
},
{
  "id": "p-1341",
  "level": "2",
  "url": "chap_matrix_operations.html#p-1341",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "vector space "
},
{
  "id": "definition-19",
  "level": "2",
  "url": "chap_matrix_operations.html#definition-19",
  "type": "Definition",
  "number": "8.2",
  "title": "",
  "body": "matrix product matrix product "
},
{
  "id": "act_A2_1_1",
  "level": "2",
  "url": "chap_matrix_operations.html#act_A2_1_1",
  "type": "Activity",
  "number": "8.2",
  "title": "",
  "body": "\n        In this activity,\n        we look for the meaning of the matrix product from a transformation perspective.\n        Let   and   be matrix transformations defined by\n         ,\n        where\n         .\n       \n              What are the domains and codomains of   and  ?\n              Why is the composite transformation   defined?\n              What is the domain of  ?\n              What is the codomain of  ? (Recall that\n                is defined by  ,\n              i.e., we substitute the output   as the input into the \n              transformation  .)\n             \n              Let  .\n              Determine the components of  .\n             \n              Find the components of  .\n             \n              Find a matrix   so that  .\n             \n              Use the definition of composition of transformations and the definitions \n              of the   and   transformations to explain why it is reasonable \n              to define   to be the matrix  .\n              Does the matrix   agree with the\n               \n              you found in   using technology?\n             "
},
{
  "id": "act_A2_1_2",
  "level": "2",
  "url": "chap_matrix_operations.html#act_A2_1_2",
  "type": "Activity",
  "number": "8.3",
  "title": "",
  "body": "\n        Let  ,\n         ,\n         ,\n          and  .\n       \n              Find the indicated products\n              (by hand or using a calculator).\n               \n             \n              Is matrix multiplication commutative?\n              Explain.\n             \n              Is there an identity element for matrix multiplication?\n              In other words,\n              is there a matrix   for which   for any matrix  ?\n              Explain.\n             \n              If   and   are real numbers with  ,\n              then we know that either   or  .\n              Is this same property true with matrix multiplication?\n              Explain.\n             \n              If  ,  , and   are real numbers with\n                and  , we know that  .\n              Is this same property true with matrix multiplication?\n              Explain.\n             "
},
{
  "id": "p-1367",
  "level": "2",
  "url": "chap_matrix_operations.html#p-1367",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "commute "
},
{
  "id": "p-1370",
  "level": "2",
  "url": "chap_matrix_operations.html#p-1370",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "scalar dot "
},
{
  "id": "act_A2_1_3",
  "level": "2",
  "url": "chap_matrix_operations.html#act_A2_1_3",
  "type": "Activity",
  "number": "8.4",
  "title": "",
  "body": "\n        Let   and  .\n       \n        Let   be the  th row of   and   the \n         th column of  .\n        For example,\n          and \n         .\n       \n        Calculate the entries of the matrix  , where\n         ,\n        where   refers to the scalar product of row \n          of   with column   of  . \n        Recall from  \n        of  \n        that the scalar product   of a   matrix\n          and an   vector\n          is  .\n          Compare your result with the result of   calculated via the product \n        of   with the columns of  .\n       "
},
{
  "id": "p-1376",
  "level": "2",
  "url": "chap_matrix_operations.html#p-1376",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "scalar product dot product "
},
{
  "id": "p-1381",
  "level": "2",
  "url": "chap_matrix_operations.html#p-1381",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "arbitrary arbitrary "
},
{
  "id": "thm_matrix_product_properties",
  "level": "2",
  "url": "chap_matrix_operations.html#thm_matrix_product_properties",
  "type": "Theorem",
  "number": "8.3",
  "title": "",
  "body": "associative distributes over matrix addition distributes over matrix addition "
},
{
  "id": "p-1389",
  "level": "2",
  "url": "chap_matrix_operations.html#p-1389",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "(multiplicative) identity matrix "
},
{
  "id": "definition-20",
  "level": "2",
  "url": "chap_matrix_operations.html#definition-20",
  "type": "Definition",
  "number": "8.4",
  "title": "",
  "body": "identity matrix identity matrix "
},
{
  "id": "definition-21",
  "level": "2",
  "url": "chap_matrix_operations.html#definition-21",
  "type": "Definition",
  "number": "8.5",
  "title": "",
  "body": "matrix transpose transpose "
},
{
  "id": "p-1396",
  "level": "2",
  "url": "chap_matrix_operations.html#p-1396",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "diagonal "
},
{
  "id": "act_A2_1_4",
  "level": "2",
  "url": "chap_matrix_operations.html#act_A2_1_4",
  "type": "Activity",
  "number": "8.5",
  "title": "",
  "body": "\n            Find the transpose of each of the indicated matrices.\n             \n           \n            Find the transpose of the new matrix for each part above.\n            What can you conjecture based on your results?\n            There are certain special types of matrices that are given names.\n           \n            There are certain special types of matrices that are given names.\n           \n                Let   be a square matrix whose  th entry is  .\n                 \n                     \n                      The matrix   is a  diagonal matrix \n                         diagonal matrix \n                      if   whenever  .\n                     \n                   \n                     \n                      The matrix   is a  symmetric \n                         symmetric matrix \n                      matrix if  .\n                     \n                   \n                     \n                      The matrix   is an  upper triangular \n                         upper triangular matrix \n                      if   whenever  .\n                     \n                   \n                     \n                      The matrix   is a  lower triangular \n                         lower triangular matrix \n                      if   whenever  .\n                     \n                   \n               \n          Find an example of a diagonal matrix  .\n          What can you say about  ?\n         \n          Find an example of a non-diagonal symmetric matrix  .\n          If  , must   be a square matrix?\n         \n          Find an example of an upper triangular matrix  .\n          What kind of a matrix is  ?\n         "
},
{
  "id": "thm_transpose_props",
  "level": "2",
  "url": "chap_matrix_operations.html#thm_transpose_props",
  "type": "Theorem",
  "number": "8.7",
  "title": "",
  "body": "\n        Let   and   be matrices of the appropriate sizes and let\n           be a scalar.\n        Then\n         \n             \n               \n             \n           \n             \n               \n             \n           \n             \n               \n             \n           \n             \n               \n             \n           \n       "
},
{
  "id": "example-15",
  "level": "2",
  "url": "chap_matrix_operations.html#example-15",
  "type": "Example",
  "number": "8.8",
  "title": "",
  "body": "\n        Let\n         .\n       \n        Determine the results of the following operations, if defined.\n        If not defined, explain why.\n       \n               \n             \n              Since   is a   matrix and   is a   matrix,\n              the number of columns of   equals the number of rows of   and the \n              matrix produce   is defined.\n              Recall that if  ,\n              where  ,  ,\n                are the columns of  ,\n              then  .\n              Recall also that   is the linear combination of the columns of \n                with weights from  , so\n               ,\n               ,\n              and\n               .\n              So  .\n              Alternatively,\n              if  ,\n              then the matrix product   is the matrix whose   entry is \n               .\n              Using this method we have\n               .\n              Now\n               ,\n              so  .\n             \n               \n             \n              Since   is a   matrix but   is  ,\n              the number of columns of   is not equal to the number of rows of  .\n              We conclude that   is not defined.\n             \n               \n             \n              Since   is a   matrix and   is  ,\n              the number of columns of   is equal to the number of rows of  .\n              Thus, the quantity   is defined.\n              First we calculate   using the dot product of the rows of   \n              with the columns of  .\n              Letting   and  ,\n              where  ,  ,\n              and   are the rows of   and  ,\n               ,\n              and   are the columns of  , we have\n               .\n              Now\n               ,\n              so  .\n              If   \n              and  ,\n              where  ,  ,\n              and   are the rows of   and  ,\n               ,\n               , and   are the columns of  , then\n               .\n              Now\n               ,\n              so  .\n             \n               \n             \n              Since   and   are both   matrices,\n              their sum is defined and is a   matrix.\n              Because   is   matrix,\n              the number of columns of   is equal to the number of rows of  .\n              Thus, the quantity   is defined and,\n              using the row-column method of matrix multiplication as earlier,\n               .\n             \n               \n             \n              Since   is a   matrix and   is  ,\n              the number of columns of   is equal to the number of rows of  .\n              Thus,   is defined and\n               .\n             \n               \n             \n              The fact that   is a\n                matrix means that   is a   matrix.\n              Since   is also a   matrix,\n              the sum   is defined.\n              The transpose of any matrix is also defined,\n              so   is defined and\n               .\n             "
},
{
  "id": "example-16",
  "level": "2",
  "url": "chap_matrix_operations.html#example-16",
  "type": "Example",
  "number": "8.9",
  "title": "",
  "body": "\n        Let   \n        and  .\n       \n              Determine the matrix sum  .\n              Then use this sum to calculate  .\n             \n              Adding corresponding terms shows that \n               .\n              Squaring this sum yields the result \n               .\n             \n              Now calculate   in a different way.\n              Use the fact that matrix multiplication distributes over matrix addition to expand\n              (like foiling)\n                into a sum of matrix products.\n              The calculate each summand and add to find  .\n              You should obtain the same result as part (a).\n              If not, what could be wrong?\n             \n              Expanding  \n              (remember that matrix multiplication is not commutative)\n              gives us\n               \n              just as in part (a).\n              If instead you obtained the matrix\n                \n              you likely made the mistake of equating   with  .\n              These two matrices are not equal in general,\n              because we cannot say that   is equal to  .\n             "
},
{
  "id": "exercise-76",
  "level": "2",
  "url": "chap_matrix_operations.html#exercise-76",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Calculate   for each of the following matrix pairs by hand in two ways.\n       \n               ,\n               \n             \n               \n             \n               ,\n               \n             \n               .\n             "
},
{
  "id": "exercise-77",
  "level": "2",
  "url": "chap_matrix_operations.html#exercise-77",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        For each of the following   matrices,\n        find all   matrices\n          which \n        commute with the given  .\n        (Two matrices   and   commute with each other if  .)\n       \n                 \n             \n                 \n             \n                \n             "
},
{
  "id": "exercise-78",
  "level": "2",
  "url": "chap_matrix_operations.html#exercise-78",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Find all possible, if any,\n          matrices satisfying each of the following matrix equations.\n       \n               \n             \n               \n             \n               \n             \n              There is no matrix   with this property.\n             \n               \n             \n               ,\n              where   and   can be any scalars\n             "
},
{
  "id": "exercise-79",
  "level": "2",
  "url": "chap_matrix_operations.html#exercise-79",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        For each of the following   matrices,\n        compute  .\n        Use your results to conjecture a formula for  .\n        Interpret your answer geometrically using the transformation interpretation.\n       \n                 \n             \n                 \n             \n                \n             "
},
{
  "id": "exercise-80",
  "level": "2",
  "url": "chap_matrix_operations.html#exercise-80",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        If   for unknown   matrix and   vector,\n        determine an expression for  ,\n         ,  ,  .\n       \n         \n       "
},
{
  "id": "exercise-81",
  "level": "2",
  "url": "chap_matrix_operations.html#exercise-81",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        If   and  ,\n        find an expression for   in terms of   and  .\n       "
},
{
  "id": "exercise-82",
  "level": "2",
  "url": "chap_matrix_operations.html#exercise-82",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        A matrix   is a  nilpotent  matrix if  ,\n        i.e.,   is the zero matrix,\n        for some positive integer  .\n        Explain why the matrices\n         \n        are nilpotent matrices.\n       \n          and  \n       "
},
{
  "id": "exercise-83",
  "level": "2",
  "url": "chap_matrix_operations.html#exercise-83",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Suppose   is an   matrix for which  .\n        Show that there is a matrix   for which\n          where   is the identity matrix of size  .\n       "
},
{
  "id": "exercise-84",
  "level": "2",
  "url": "chap_matrix_operations.html#exercise-84",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        Let  ,  , and   be\n          matrices and let   and   be scalars.\n        Verify  .\n        That is, show that\n       \n               \n             \n               \n             \n               \n             \n              The   matrix   whose entries are all 0 has the \n              property that  .\n             \n               \n             \n              The scalar multiple   of the matrix   has the property \n              that  .\n             \n               \n             \n               \n             \n               \n             \n               \n             \n               \n             \n               .\n             "
},
{
  "id": "exercise-85",
  "level": "2",
  "url": "chap_matrix_operations.html#exercise-85",
  "type": "Exercise",
  "number": "10",
  "title": "",
  "body": "\n        Let  ,  ,\n        and   be matrices of the appropriate sizes for all sums and products to be \n        defined and let   be a scalar.\n        Verify the remaining parts of  .\n        That is, show that\n       \n               \n             \n               \n             \n              There is a square matrix   with the property that   or\n                for whichever product is defined.\n             \n               \n             "
},
{
  "id": "exercise-86",
  "level": "2",
  "url": "chap_matrix_operations.html#exercise-86",
  "type": "Exercise",
  "number": "11",
  "title": "",
  "body": "\n        Let   and\n          be matrices of the appropriate sizes,\n        and let   be a scalar.\n        Verify the remaining parts of  .\n        That is, show that\n       \n               \n             \n              Let  .\n              Then   by definition of the transpose.\n              Let  .\n              Then  .\n              So the  th entry of\n                is the same as the  th entry of  ,\n              and we conclude that  .\n             \n               \n             \n               \n             \n              The  th entry of   is  ,\n              so the  th entry of   is  .\n              But   is also the  th entry of  .\n              We conclude that  .\n             "
},
{
  "id": "ex_2_a_matrix_exponential",
  "level": "2",
  "url": "chap_matrix_operations.html#ex_2_a_matrix_exponential",
  "type": "Exercise",
  "number": "12",
  "title": "",
  "body": "matrix exponential matrix exponential \n              Calculate  ,  ,  .\n              Explain why   for any positive integer  .\n             \n              Show that    is equal to\n               .\n             \n              Explain why  .\n             "
},
{
  "id": "exercise-88",
  "level": "2",
  "url": "chap_matrix_operations.html#exercise-88",
  "type": "Exercise",
  "number": "13",
  "title": "",
  "body": "\n        Show that if   and   are   rotation matrices,\n        then   is also a   rotation matrix.\n       \n        If  ,\n         ,\n        then   equals\n         \n       "
},
{
  "id": "exercise-89",
  "level": "2",
  "url": "chap_matrix_operations.html#exercise-89",
  "type": "Exercise",
  "number": "14",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n        Throughout, assume that matrices are of the appropriate sizes so that any matrix \n        sums or products are defined.\n       True\/False \n              For any three matrices   with  ,\n                implies  .\n             \n              F\n             True\/False \n              For any three matrices   with  ,\n                implies  .\n             True\/False \n              If   is the zero matrix,\n              then   itself is the zero matrix.\n             \n              F\n             True\/False \n              If   for every   matrix  ,\n              then   is the identity matrix  .\n             True\/False \n              If matrix products   and   are both defined,\n              then   and   are both square matrices of the same size.\n             \n              F\n             True\/False \n              If   is a solution for   (i.e., that\n               ) and   is a solution for  ,\n              then   is a solution for  .\n             True\/False \n              If   is an\n                matrix with two equal columns,\n              then the matrix   has two equal columns for every   matrix.\n             \n              T\n             True\/False \n              If  ,\n              then   or  .\n             "
},
{
  "id": "x_pact_Strassen_1",
  "level": "2",
  "url": "chap_matrix_operations.html#x_pact_Strassen_1",
  "type": "Project Activity",
  "number": "8.6",
  "title": "",
  "body": "\n      We first work with the   case.\n     \n            Let   \n            and  .\n           \n                  Calculate the matrix product  .\n                 \n                  Rather than using eight multiplications to calculate  , Strassen \n                  came up with the idea of using the following seven products:\n                   .\n                  Calculate   through   for the given matrices   and  .\n                  Then calculate the quantities\n                   .\n                  What do you notice?\n                 \n            Now we repeat part (a) in general.\n            Suppose we want to calculate the matrix product   for arbitrary \n              matrices\n              and  .\n            Let\n             .\n            Show that\n             .\n           "
},
{
  "id": "project-23",
  "level": "2",
  "url": "chap_matrix_operations.html#project-23",
  "type": "Project Activity",
  "number": "8.7",
  "title": "",
  "body": "\n      We illustrate the idea of partitioned matrices with an example.\n      Let  .We can partition   into smaller matrices\n       ,\n      which are indicated by the vertical and horizontal lines.\n      As a shorthand, we can describe this partition of   as\n       ,\n      where  ,\n       ,\n       ,\n      and  .\n      The submatrices   are called  blocks .\n      If   is a matrix such that   is defined,\n      then   must have five rows.\n      As an example,\n        is defined if  .\n      The partition of   breaks   up into blocks with three and two columns,\n      respectively.\n      So if we partition   into blocks with three and two rows,\n      then we can use the blocks to calculate the matrix product  .\n      For example, partition   as\n       .\n     \n      Show that\n       .\n     "
},
{
  "id": "project-24",
  "level": "2",
  "url": "chap_matrix_operations.html#project-24",
  "type": "Project Activity",
  "number": "8.8",
  "title": "",
  "body": "\n      Let   and   be two   matrices.\n      If   is not a power of  ,\n      then pad the rows and columns of   and   with zeros to make them of size\n        for some integer  .\n      (From a practical perspective,\n      we might instead just use unequal block sizes.)\n      Let  .\n      Partition   and   as\n       ,\n      where each submatrix is of size  .\n      Now we use the Strassen algorithm just as in the   case,\n      treating the submatrices as if they were scalars\n      (with the additional constraints of making sure that the dimensions match up so \n      that products are defined,\n      and ensuring we multiply in the correct order).\n      Letting\n       ,\n      then the same algebra as in   shows that\n       .\n     \n      Apply Strassen's algorithm to calculate the matrix product  , where\n       .\n     "
},
{
  "id": "project-25",
  "level": "2",
  "url": "chap_matrix_operations.html#project-25",
  "type": "Project Activity",
  "number": "8.9",
  "title": "",
  "body": "\n      We introduce a little notation to help us describe the efficiency of our calculations.\n      We won't be formal with this notation,\n      rather work with it in an informal way.\n      Big O (the letter\n       O \n      ) notation is used to describe the complexity of an algorithm.\n      Generally speaking,\n      in computer science big O notation can be used to describe the run time of an algorithm,\n      the space used by the algorithm,\n      or the number of computations required.\n      The letter\n       O \n      is used because the behavior described is also called the order.\n      Big O measures the asymptotic time of an algorithm, not its exact time.\n      For example,\n      if it takes   steps to complete an algorithm,\n      then we say that the algorithm grows at the order of  \n      (we ignore the constants and the smaller power terms,\n      since they become insignificant as   increases)\n      and we describe its growth as  .\n      To measure the efficiency of an algorithm to determine a matrix product,\n      we will measure the number of operations it takes to calculate the product.\n     \n            Suppose   and   are   matrices.\n            Explain why the operation of addition\n            (that is, calculating  )\n            is  .\n           \n            Suppose   and   are   matrices.\n            How many multiplications are required to calculate the matrix product  ?\n            Explain.\n           \n            The standard algorithm for calculating a matrix product of two\n              matrices requires   multiplications and a number of additions.\n            Since additions are much less costly in terms of operations,\n            the standard matrix product is  .\n            We won't show it here,\n            but using Strassen's algorithm on a product of   matrices is\n             , where  .\n            That means that Strassen's algorithm applied to an   matrix\n            (where   is a power of  )\n            requires approximately   multiplications.\n            We use this to analyze situations to determine when Strassen's algorithm is \n            computationally more efficient than the standard algorithm.\n           \n                  Suppose   and   are   matrices.\n                  Determine the number of multiplications required to calculate the matrix \n                  product   using the standard matrix product.\n                  Then determine the approximate number of multiplications required to \n                  calculate the matrix product   using Strassen's algorithm.\n                  Which is more efficient? (Remember,\n                  we can only apply Strassen's algorithm to square matrices whose sizes are \n                  powers of  .)\n                 \n                  Repeat part i. with   matrices.\n                  Which method is more efficient?\n                 "
},
{
  "id": "chap_intro_eigenvals_eigenvects",
  "level": "1",
  "url": "chap_intro_eigenvals_eigenvects.html",
  "type": "Section",
  "number": "9",
  "title": "Introduction to Eigenvalues and Eigenvectors",
  "body": "Introduction to Eigenvalues and Eigenvectors \n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is an eigenvalue of a matrix?\n           \n         \n           \n            What is an eigenvector of a matrix?\n           \n         \n           \n            How do we find eigenvectors of a matrix corresponding to an eigenvalue?\n           \n         \n           \n            How can the action of a matrix on an eigenvector be visualized?\n           \n         \n           \n            Why do we study eigenvalues and eigenvectors?\n           \n         \n           \n            What are discrete dynamical systems and how do we analyze the long-term \n            behavior in them?\n           \n         Application: The Google PageRank Algorithm \n    The World Wide Web is a vast collection of information,\n    searchable via search engines.\n    A search engine looks for pages that are of interest to the user.\n    In order to be effective,\n    a search engine needs to be able to identify those pages that are relevant to the search \n    criteria provided by the user.\n    This involves determining the relative importance of different web pages by ranking the \n    results of thousands or millions of pages fitting the search criteria.\n    For Google, the PageRank algorithm is their method and is\n     the heart of our software \n    as they say.\n    It is this PageRank algorithm that we will learn about later in this section.\n    Eigenvalues and eigenvectors play an important role in this algorithm.\n   Introduction eigenvector eigenvalue \n    Eigenvalues and eigenvectors are used in many applications.\n    Social media like Facebook and Google use eigenvalues to determine the influence of \n    individual members on the network\n    (which can affect advertising)\n    or to rank the importance of web pages.\n    Eigenvalues and eigenvectors appear in quantum physics,\n    where atomic and molecular orbitals can be defined by the eigenvectors of a certain operator.\n    They appear in principal component analysis,\n    used to study large data sets,\n    to diagonalize certain matrices and determine the long term behavior of systems as a result,\n    and in the important singular value decomposition of a matrix.\n    Matrices with real entries can have real or complex eigenvalues,\n    and complex eigenvalues reveal a rotation that is encoded in every real matrix with \n    complex eigenvalues which allows us to better understand certain matrix transformations.\n   eigenvector characteristic vector eigenvalue characteristic value eigenvector characteristic vector eigenvalue characteristic value \n    For example,\n      is an eigenvector of\n      corresponding \n    to the eigenvalue\n      because  ,\n    which is equal to  .\n    On the other hand,\n      is not an eigenvector of\n      because \n     ,\n    which is not a multiple of  .\n   \n            For each of the following parts,\n            use the definition of an eigenvector to determine whether the given vector \n              is an eigenvector for the given matrix  .\n            If it is, determine the corresponding eigenvalue.\n           \n                   ,\n                   \n                 \n                   ,\n                   \n                 \n                   ,\n                   \n                 \n                   ,\n                   \n                 \n            We now consider how we can find the eigenvectors corresponding to an eigenvalue using the definition.\n            Suppose  .\n            We consider whether we can find eigenvectors corresponding to eigenvalues 3, and 5.\n            Effectively,\n            this will help us determine whether 3 and\/or 5 are eigenvalues of  .\n           \n                  Rewrite the vector equation   where\n                    as a vector \n                  equation.\n                 \n                  After writing   as   where   is the identity matrix,\n                  rearrange the variables to turn this vector equation into the homogeneous \n                  matrix equation\n                    where \n                   .\n                  If possible,\n                  find a non-zero (i.e. a non-trivial) solution to  .\n                  Explain what this means about 5 being an eigenvalue of   or not.\n                 \n                  Similarly, determine whether the vector equation\n                    has non-zero solutions.\n                  Using your result,\n                  determine whether 3 is an eigenvalue of   or not.\n                 Eigenvalues and Eigenvectors \n    Eigenvectors are especially useful in understanding the long-term behavior of dynamical systems,\n    an example of which we will see shortly.\n    The long-term behavior of a dynamical system is quite simple when the initial state vector \n    is an eigenvector and this fact helps us analyze the system in general.\n   \n    To find eigenvectors,\n    we are interested in determining the vectors   for which   has the same \n    direction as  .\n    This will happen when\n     \n    for some scalar  .\n    Of course,   when\n      for every   and every  ,\n    but that is uninteresting.\n    So we really want to consider when there is a  non-zero \n    vector   so that  .\n    This prompts the definition of eigenvectors and eigenvalues as in \n     \n   \n   matrix square \n    In order for a matrix   to have an eigenvector,\n    one condition   must satisfy is that   has to be a square matrix,\n    i.e. an   matrix.\n    We will find that each   matrix has only finitely many eigenvalues.\n   \n    The terms eigenvalue and eigenvector seem to come from Hilbert,\n    using the German\n     eigen \n    (roughly translated as\n     own ,\n     proper , or\n     characteristic )\n    to emphasize how eigenvectors and eigenvalues are connected to their matrices.\n    To find the eigenvalues and eigenvectors of an   matrix  ,\n    we need to find the solutions to the equation\n     .\n   \n    In  ,\n    we considered this equation for\n      and  .\n    The homogeneous matrix equation we came up with was\n     .\n   \n    To see the relationship between this homogeneous matrix equation and the \n    eigenvalue-eigenvector equation better,\n    let us consider the eigenvector equation using matrix algebra:\n     ,\n    where   is the   identity matrix.\n    Notice that this description matches the homogenous equation matrix example above since we \n    simply subtracted 5 from the diagonal terms of the matrix  .\n    Hence, to find eigenvalues,\n    we need to find the values of   so that the homogeneous equation\n      has non-trivial solutions.\n   \n          Under what conditions on   will the matrix equation\n            have non-trivial solutions?\n          Describe at least two different but equivalent conditions.\n         \n          The real number 0 is an eigenvalue of \n           .\n          Check that your criteria in the previous part agrees with this result.\n         \n          Determine if 5 is an eigenvalue of the matrix\n            \n          using your criterion above.\n         \n          What are the two eigenvalues of the matrix \n           ?\n         \n    Since an eigenvector of   corresponding to eigenvalue   is a \n    non-trivial solution to the homogeneous equation  ,\n    the eigenvalues   which work are those for which the matrix\n      has linearly dependent columns,\n    or for which the row echelon form of the matrix\n      does not have a pivot in every column.\n    When we need to test if a specific   is an eigenvalue,\n    this method works fine.\n    However, finding which  's will work in general involves row \n    reducing a matrix with  's subtracted on the diagonal algebraically.\n    For certain types of matrices,\n    this method still provides us the eigenvalues quickly.\n    For general matrices though, row reducing algebraically is not efficient.\n    We will later see an algebraic method which uses the determinants to find the eigenvalues.\n   \n            For   to be an eigenvalue of  ,\n            we noted that   must have a non-pivot column.\n            Use this criterion to explain why \n              has eigenvalues\n              and  .\n           \n            Determine the eigenvalues of \n             .\n           \n            Generalize your results from the above parts in the form of a theorem in the\n            most general   case.\n           Dynamical Systems dynamical system discrete dynamical systems dynamical system \n    Discrete dynamical systems can be used in population modeling to provide a simplified \n    model of predator-prey interactions in biology\n    (see  ).\n    Other applications include Markov chains\n    (see  ),\n    age structured population growth models,\n    distillation of a binary ideal mixture of two liquids,\n    cobweb model in economics concerning the interaction of supply and demand for a single good,\n    queuing theory and traffic flow.\n   \n    Eigenvectors can be used to analyze the long-term behavior of dynamical systems.\n   \n            Consider a discrete dynamical system providing a simplified model of predator-prey \n            interactions in biology,\n            such as the system describing the populations of rabbits and foxes in a certain area.\n            Suppose, for example,\n            for a specific area the model is given by the following equations:\n             \n            where   represents the number of rabbits in the area   years after \n            a starting time value,\n            and   represents the number of foxes in year  .\n            We use   to denote the initial population values.\n           \n                  Suppose   and   for one year.\n                  Calculate rabbit and fox population values for the next year.\n                  In other words, find   values.\n                 \n                  Consider the coefficients of the variables\n                    in the the system of equations in  .\n                  Can you explain the reasoning behind the signs and absolute sizes of \n                  the coefficients from the story that it models?\n                 state vector transition matrix \n                  The transition matrix will help us simplify calculations of the population values.\n                  Note that equation   implies that  ,\n                   ,  , and so on.\n                  This is a recursive method to find the population values as each year's \n                  population values depend on the previous year's population values.\n                  Using this approach,\n                  calculate   for   values up to 5 corresponding to the following\n                   three different \n                  initial rabbit-fox population values (all in thousands):\n                   \n                   \n                   \n                  Can you guess the long-term behavior of the population values in each case?\n                  Are they both increasing?\n                  Decreasing?\n                  One increasing, one decreasing?\n                  How do the rabbit and fox populations compare?\n                 \n    A dynamical system is a system of variables whose values change with time.\n    In  ,\n    we considered the discrete dynamical system modeling the rabbit and fox population in an area,\n    which is an example of a predator-prey system.\n    The system was given by the equations from  ,\n    where   represented the number of rabbits in the area   years after a \n    starting time value,\n    and   represented the number of foxes in year  .\n    In this notation,\n      corresponded to the initial population values.\n   \n   state vector \n    As we saw in  ,\n    if we define the state vector\n    as  ,\n    the system of equations representing the dynamical system can be expressed as\n     \n    where   \n    represents the transition matrix.\n    Note that equation   encodes \n    infinitely many equations including  ,\n     ,  , and so on.\n    This is a recursive formula for the population values as each year's population values \n    are expressed in terms of the previous year's population values.\n    If we want to calculate  ,\n    this formula requires first finding the population values for years 1-9.\n    However, we can obtain a non-recursive formula using matrix algebra.\n    If we substitute   into\n      and simplify, we find that\n     .\n   \n    Similarly, substituting   into the formula for   gives\n     .\n   \n    This process can be continued inductively to show that\n     \n    for every   value.\n    So to find the population values at any year  ,\n    we only need to know the initial state vector  .\n   \n      In this activity the matrix   is the transition matrix for the rabbit and \n      fox population model,\n       .\n     \n            Suppose that the initial state vector   is an eigenvector of   \n            corresponding to eigenvalue  .\n            In this case,\n            explain why   and  .\n            Find the formula for   in terms of\n              and   by applying equation \n              iteratively.\n           \n            The initial state vector \n              is an \n            eigenvector of  .\n            Find the corresponding eigenvalue and,\n            using your formula from (a) for   in terms of\n              and  , find the state vector   in this case.\n           \n            The initial state vector \n              \n            is an eigenvector of  .\n            Find the corresponding eigenvalue and,\n            using your formula from (a) for   in terms of\n              and  , find the state vector   in this case.\n           \n            Consider now an initial state vector of the form\n              where   are constants,\n              is an eigenvector corresponding to eigenvalue\n              and   corresponding to eigenvalue\n              (  and   are not necessarily the \n            eigenvectors from parts (b) and (c)).\n            Use matrix algebra and equation   \n            to explain why  .\n           \n            Express the initial state vector\n              \n            as a linear combination of the eigenvectors\n              and use your result from the previous part to find a formula for  .\n            What happens to the population values as  \n           \n    As you discovered in  ,\n    we can use linearly independent eigenvectors of the transition matrix to find a \n    closed formula for the state vector of a dynamical system,\n    as long as the initial state vector can be expressed as a linear combination of \n    the eigenvectors.\n   Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Let  .\n       \n              Find all of the eigenvalues of  .\n             \n              Recall that a scalar   is a eigenvalue for   if there is a \n            nonzero vector   such that\n              or  .\n            For this matrix  , we have\n             .\n            To solve the homogeneous system  ,\n            we row reduce  .\n            To do this, we first interchange rows to get the following matrix that is row \n            equivalent to\n              (we do this to ensure that we have a nonzero entry in the \n            first row and column)\n             .\n            Next we replace row two with\n              times row one minus row two to obtain the row \n            equivalent matrix\n             .\n            There will be a nontrivial solution to\n              if there is a row of zeros in this row echelon form.\n            Thus, we look for values of   that make\n             .\n            Applying a little algebra shows that\n             .\n            So the eigenvalues of   are\n              and  .\n             \n              Find a corresponding eigenvector for each eigenvalue found in part (a).\n             \n              Recall that an eigenvector for the eigenvalue   is a nonzero vector \n                such that  .\n              We consider each eigenvalue in turn.\n          \n               \n                   \n                    When  ,\n                     .\n                    Technology shows that the reduced row echelon form of   is\n                     .\n                    If  ,\n                    then   implies that   is free and  .\n                    Choosing   gives us the eigenvector  .\n                    As a check, note that\n                     .\n                   \n                 \n                   \n                    When  ,\n                     .\n                    Technology shows that the reduced row echelon form of   is\n                     .\n                    If  ,\n                    then   implies that   is free and  .\n                    Choosing   gives us the eigenvector  .\n                    As a check, note that\n                     .\n                   \n                 \n             transition matrix Markov process Markov chain transition matrix \n      Accurately predicting the weather has long been an important task.\n      Meteorologists use science, mathematics,\n      and technology to construct models that help us understand weather patterns.\n      These models are very sophisticated,\n      but we will consider only a simple model.\n      Suppose, for example,\n      we want to learn something about whether it will be wet or dry in Grand Rapids, Michigan.\n      To do this, we might begin by collecting some data about weather conditions in \n      Grand Rapids and then use that to make predictions.\n      Information taken over the course of 2017 from the National Weather Service Climate \n      Data shows that if it was dry\n      (meaning no measurable precipitation, either rain or snow)\n      on a given day in Grand Rapids,\n      it would be dry the next day with a probability of 64% and wet with a probability of 36%. Similarly,\n      if it was wet on a given day it would be dry the next day with a probability of 47% and \n      dry with a probability of 53%. Assuming that this pattern is one that continues in the \n      long run,\n      we can develop a mathematical model to make predictions about the weather.\n     transition matrix state \n            Calculate  .\n            Interpret the meaning of this output.\n           \n            Here we have\n             .\n            This output tells us the different probabilities of whether it will be dry or wet \n            the day following a dry day.\n           \n            Calculate  .\n            Interpret the meaning of this output.\n           \n            Here we have\n             .\n            This output tells us the different probabilities of whether it will be dry or \n            wet the day following a wet day.\n           \n            Calculate  .\n            Interpret the meaning of this output.\n           \n            Here we have\n             .\n            This output tells us there is a 52% chance of it being dry and a 48% chance of it \n            being wet following a day when there is a 30% chance of it being dry and a 70% \n            chance of it being wet.\n           state vector \n                  Starting with  ,\n                  use appropriate technology to calculate   for   values up to 10.\n                  Round to three decimal places.\n                  What do you notice about the entries?\n                 \n                  Technology shows that\n                   \n                  We can see that our vectors   are essentially the same as we \n                  let   increase.\n                 \n                  What does the result of the previous part tell us about eigenvalues of  ?\n                  Explain.\n                 \n                  Since our sequence seems to be converging to a vector   \n                  satisfying  ,\n                  we conclude that 1 is an eigenvalue of  .\n                 \n                  Rewrite   as\n                   .\n                  We do this so we can use exact arithmetic.\n                  Let  .\n                  What is  ? (Use exact arithmetic,\n                  no decimals.) Explain how   is related to the previous \n                  two parts of this problem.\n                  What does the vector   tells us about weather in Grand Rapids?\n                 \n                  A matrix vector multiplication shows that\n                   .\n                  In other words,\n                    is an eigenvector for   with eigenvalue 1.\n                  Notice that\n                   ,\n                  so these fractions give the same results we obtained with our \n                  sequence of vectors  .\n                  These vectors provide a steady-state vector for Grand Rapids weather.\n                  In other words,\n                  if there is a   chance of it being dry on a given day \n                  in Grand Rapids,\n                  then there is a   chance it will be dry again the next day.\n                 \n      This is an example of a Markov process.\n      Markov processes\n      (named after Andrei Andreevich Markov)\n      are widely used to model phenomena in biology, chemistry,\n      business,\n      physics, engineering, the social sciences, and much more.\n      More specifically,\n     Markov process Markov process transition matrix Markov chain Summary \n    We learned about eigenvalues and eigenvectors of a matrix in this section.\n     \n         \n          A scalar   is an eigenvalue\n          (or characteristic value)\n          of a square matrix   if there is a non-zero vector   so that \n           .\n         \n       \n         \n          A non-zero vector   is an eigenvector\n          (or characteristic vector)\n          of a square matrix   if there is a scalar   so that \n           .\n         \n       \n         \n          To find the eigenvectors of an\n            matrix   corresponding to an eigenvalue  ,\n          we determine the non-trivial solutions to\n            where   is the   \n          identity matrix.\n         \n       \n         \n          We study eigenvectors and eigenvalues because the eigenvectors tell us \n          quite a bit about the transformation corresponding to the matrix.\n          These eigenvectors arise in many applications in physics,\n          chemistry, statistics, economics,\n          biology, sociology and other areas,\n          and help understand the long-term behavior of dynamical systems.\n         \n       \n         \n          A dynamical system is a system of variables whose values change with time.\n          In linear dynamical systems,\n          the change in the state vector from one time period to the next is expressed by \n          matrix multiplication by the transition matrix  .\n          The eigenvectors of   provide us a simple method to express the state \n          vector at any given time period in terms of the initial state vector.\n          Specifically,\n          if the initial state vector is   where\n            are eigenvectors corresponding to eigenvalues \n           , we have\n           .\n         \n       \n   \n        For each of the following matrix-vector pairs,\n        determine whether the given vector is an eigenvector of the matrix.\n       \n               ,\n               \n             \n              Eigenvector with eigenvalue  .\n             \n               ,\n               \n             \n              Eigenvector with eigenvalue  .\n             \n               ,\n               \n             \n              Eigenvector with eigenvalue  .\n             \n        For each of the following matrix-eigenvalue pairs,\n        determine an eigenvector of   for the given eigenvalue.\n       \n               ,\n                 \n             \n               ,\n                 \n             \n               ,\n                 \n             \n               ,\n                \n             \n        For each of the following matrix-  pairs,\n        determine whether the given   will work as an eigenvalue.\n        You do not need to find an eigenvector as long you can justify if \n          is a valid eigenvalue or not.\n       \n               ,\n                 \n             \n              Eigenvalue\n             \n               ,\n                 \n             \n              Eigenvalue\n             \n               ,\n                 \n             \n              Not an eigenvalue\n             \n               ,\n                \n             \n              Not an eigenvalue\n             \n        For a matrix   with eigenvector\n          with eigenvalue \n         ,\n        and eigenvector   \n        with eigenvalue  ,\n        determine the value of the following expressions using matrix-vector product properties:\n       \n               \n             \n               \n             \n               \n             Markov chain Markov chain \n              We know that there will be 900,000 students in class on the second day and \n              100,000 students skipping class.\n              On the third day, 90% of the 900,000 students (attenders) and 30% of the \n              100,000 students (skippers) will come back to class.\n              Therefore, 840,000 students will attend class on the third day.\n              On the other hand, 10% of 900,000 students and 70% of 100,000 students \n              skip class on the third day,\n              for a total of 160,000 students skipping class.\n              We can use variables to represent these numbers.\n              Let   represent the number of students attending class   \n              days after first day.\n              So  .\n              Let   represent the students skipping class.\n              So  .\n              Find  .\n             \n                 ,\n               ,  \n             \n              Find a linear expression for   in terms of the previous day values,\n                and  , using the story given in the problem.\n              Similarly, express   in terms of   and  .\n             \n               ,\n               \n             \n              Let   represent the state vector:\n               .\n              It describes the state of the whole system\n              (students attending class and skipping class)\n              in one vector.\n              For example,\n                is the initial state.\n              The state next day is \n               .\n              Using your answer to the previous part,\n              find a matrix   which describes how the system changes from one day \n              to the other so that\n               .\n             \n                equals  \n             \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              The number 0 cannot be an eigenvalue.\n             \n              F\n             True\/False \n              The   vector cannot be an eigenvector.\n             True\/False \n              If   is an eigenvector of  ,\n              then so is  .\n             \n              T\n             True\/False \n              If   is an eigenvector of  ,\n              then it is also an eigenvector of  .\n             True\/False \n              If   and   are eigenvectors of   with the same eigenvalue,\n              then   is also an eigenvector with the same eigenvalue.\n             \n              T\n             True\/False \n              If   is an eigenvalue of  ,\n              then   is an eigenvalue of  .\n             True\/False \n              A projection matrix satisfies  .\n              If   is a projection matrix,\n              then the eigenvalues of   can only be 0 and 1.\n             \n              T\n             True\/False \n              If   is an eigenvalue of an   matrix  ,\n              then   is an eigenvalue of  .\n             True\/False \n              If   is an eigenvalue of two matrices   and   of \n              the same size,\n              then   is an eigenvalue of  .\n             \n              F\n             True\/False \n              If   is an eigenvector of two matrices   and   of the \n              same size,\n              then it is also an eigenvector of  .\n             True\/False \n              A matrix   has 0 as an eigenvalue if and only if   has linearly dependent columns.\n             \n              T\n             Project: Understanding the PageRank Algorithm \n    Sergey Brin and Lawrence Page, the founders of Google,\n    decided that the importance of a web page can be judged by the number of links to \n    it as well as the importance of those pages.\n    It is this idea that leads to the PageRank algorithm. \n    Information for this project was taken from the websites \n      \n    and  .\n      Google uses this algorithm\n    (and others)\n    to order search engine results.\n    According to Google: \n     \n     \n   \n    PageRank works by counting the number and quality of links to a page to determine \n    a rough estimate of how important the website is. The underlying assumption is that more \n    important websites are likely to receive more links from other websites.\n   \n    To rank how\n     important \n    a website is, we need to make some assumptions.\n    We assume that a person visits a page and then surfs the web by selecting a link from \n    that page   all links on a given page are assigned the same probability of being chosen.\n    As an example,\n    assume a small set of seven pages 1, 2, 3, 4, 5, 6, and 7 with links between the pages \n    given by the arrows as shown in  . \n    The Internet is very large and has upwards of 25 billion pages.\n    This would leave us with an enormous transition matrix,\n    even though most of its entries are 0.\n    In fact, studies show that web pages have an average of about 10 links,\n    so on average all but 10 entries of each column are 0.\n    Working with such a large matrix is beyond what we want to do in this project,\n    so we will just amuse ourselves with small examples that illustrate the general points.\n      So, for example,\n    there is a hyperlink from page 4 to page 3, but no hyperlink in the opposite direction.\n    If a web surfer starts on page 5, then there is probability of\n      that this person will surf to page 6 and a probability of\n      that the surfer will move to page 4.\n    If there is no link leaving a page,\n    as in the case of page 3, then the probability of remaining there is 1.\n   A seven page internet transition matrix state vector \n    Since there are links from page 6 to pages 3, 5, and 7, there is a\n      probability that the surfer will next move to one of these pages.\n    That means that at the next step,\n    the state vector   for this user will be\n     .\n\n    Note that\n     .\n\n    As the user continues to surf the internet,\n    the probabilities that the surfer is on a given page after the second,\n    third, and fourth steps are given in the state vectors\n     .\n   \n    In general, the probabilities that the surfer is on a given page after the \n     th step is given by the state vector\n     .\n   Markov process stochastic matrix stochastic \n    In a Markov process,\n    each generation depends only on the preceding generation and there may be a \n    limiting value as we let the process continue indefinitely.\n    We can test to see if that happens for this Markov process defined by   \n    by doing some experimentation.\n   \n      Use appropriate technology to do the following.\n      Choose several different initial state vectors   and calculate the \n      vectors in the sequence\n        for large values of  .\n      (Note that, as state vectors,\n      the entries of   cannot be negative and the sum of the entries of \n        must be  .)\n      Explain the behavior of the sequence\n        as   gets large.\n      Do you notice anything strange?\n      What aspects of our seven page internet do you think explain this behavior?\n      Clearly communicate all of the experimentation that you do.\n      You may use the GeoGebra applet at \n       .\n     steady-state equilibrium \n    Equation   shows that a steady state vector \n      is an eigenvector for   with eigenvalue 1.\n    We can interpret the steady-state vector for   in an important way.\n    Let   be the fraction of time we spend on page   and let \n      be the number of links on page  .\n    Then the fraction of the time that we end up on page   coming from \n    page   is  .\n    If we sum over all the pages linked to page   we have that\n     .\n   \n    Notice that this is essentially the same process we used to obtain   \n    from  ,\n    and so we can interpret the steady-state vector   as telling us what \n    fraction of a random web surfer's time was spent at each web page.\n    If we assume that the time spent at a web page is a measure of its importance,\n    then the steady-state vector tells us the relative importance of each web page.\n    So this steady-state vector provides the page rankings for us.\n    In other words,\n  \n   \n    The importance of a webpage may be measured by the relative size of the \n    corresponding entry in the steady-state vector for an appropriately chosen Markov chain.\n   \n   \n      Show that the limiting vector you found in  \n      is an eigenvector of   with eigenvalue 1.\n     \n     \n    illustrates one problem with our seven page internet.\n    The steady-state vector shows that page 3 is the only important page,\n    but that hardly seems reasonable in the example since there are other pages that \n    must have some importance.\n    The problem is that page 3 is a\n     dangling \n    page and does not lead anywhere.\n    Once a surfer reaches that page,\n    they are stuck there, overemphasizing its importance.\n    So this dangling page acts like a sink,\n    ultimately drawing all surfers to it.\n    To adjust for dangling pages,\n    we make the assumption that if a surfer reaches a dangling page\n    (one with no links emanating from it),\n    the surfer will jump to any page on the web with equal probability.\n    So in our seven page example,\n    once a surfer reaches page 3 the surfer will jump to any page on the web with \n    probability  .\n   \n            Determine the transition matrix for our seven page internet with this adjustment.\n           \n            Approximate the steady-state vector for this adjusted matrix so that the entries are accurate to four decimal places.\n            Use any appropriate technology to row reduce matrices.\n           \n            According to this adjusted model,\n            which web page is now the most important?\n            Why?\n            Does this seem reasonable?\n            Why?\n           \n    There is one more issue to address before we can consider ourselves ready to rank web pages.\n    Consider the example of the five page internet shown in  .\n   A five page internet \n            Explain why\n             .\n            is the transition matrix for this five page internet.\n            (Keep in mind the adjustment we made for dangling pages.)\n           \n            Start with different initial state vectors   \n            and determine if there is a limit to the Markov chain.\n            Explain.\n            You may use the GeoGebra applet at \n             .\n           \n     \n    shows that it is possible to construct an internet so that the corresponding Markov \n    chain does not have a limit,\n    even after adjusting for dangling pages.\n    This is a significant problem if we want to provide a relative ranking of all web pages \n    regardless of where a surfer starts.\n    To fix this problem we need to make one final adjustment to arrive at a type of transition\n    matrix that always provides a limit for our Markov chain.\n   stochastic matrix regular regular \n    Note that the transition matrix from   is not regular.\n    Regular matrices have some especially nice properties,\n    as the following theorem describes.\n    We will not prove this theorem,\n    but use it in the remainder of this project.\n    The theorem shows that if we have a regular transition matrix,\n    then there will a limit of the state vectors  ,\n    and that this limit has a very interesting property.\n   \n        Assume   and that   is a regular   stochastic matrix.\n         \n             \n                exists and is a stochastic matrix.\n             \n           \n             \n              For any vector  ,\n               \n              for the same vector  .\n             \n           \n             \n              The columns of   are the same vector  .\n             \n           \n             \n              The vector   is the unique eigenvector of   whose entries sum to 1.\n             \n           \n             \n              If   is an eigenvalue of   not equal to 1, \n              then  .\n             \n           \n       \n    Having a regular transition matrix   ensures that there is always the same \n    limit   to the sequence\n      for any starting vector  .\n    As mentioned before, the entries in\n      can be interpreted as telling us what \n    fraction of the random surfer's time was spent at each webpage.\n    If we interpret the amount of time a surfer spends at a page as a measure of the page's \n    importance,\n    then this steady-state vector   provides a ranking of the relative importance \n    of each page in the web.\n    This is the essence of Google's PageRank.\n   \n    To make our final adjustment in the transition matrix to be sure that we obtain a \n    regular matrix,\n    we need to deal with the problems of\n     loops \n    in our internet.\n    Loops, as illustrated in  ,\n    can act as sinks just like the dangling pages we saw earlier and condemn a \n    user that enters such a loop to spend his\/her time only on those pages in the loop.\n    Quite boring!\n    To account for this problem, we make a second adjustment.\n   \n    Let   be a number between 0 and 1 (Google supposedly uses  ).\n    Suppose a surfer is on page  .\n    We assume with probability   that the surfer will chose any link on \n    page   with equal probability.\n    We make the additional assumption with probability   that the surfer \n    will select with equal probability any page on the web.\n   \n    If   is a transition matrix,\n    incorporating the method we used to deal with dangling pages,\n    then the adjusted transition matrix  \n    (the Google matrix)\n    is\n     ,\n    where   is the matrix all of whose entries are  ,\n    where   is the number of pages in the internet\n    (  in our seven page example).\n    Since all of the entries of   are positive,\n      is a regular stochastic matrix.\n   \n      Return to the seven page internet in  .\n     \n            Find the Google matrix   for this internet.\n           \n            Approximate, to four decimal places,\n            the steady-state vector for this internet.\n           \n            What is the relative rank of each page in this internet,\n            and approximately what percentage of time does a random user spend on each page.\n           \n    We conclude with two observations.\n    Consider the role of the parameter   in our final adjustment.\n    Notice that if  ,\n    then   and we have the original hyperlink structure of the web.\n    However, if  , then  ,\n    where   is the   identity matrix with   as the \n    number of pages in the web.\n    In this case,\n    every page is linked to every other page and a random surfer spends equal time on any page.\n    Here we have lost all of the character of the linked structure of the web.\n    Choosing   close to 1 retains much of the original hyperlink structure of the web.\n   \n    Finally, the matrices that model the web are HUGE, and so the methods we used in \n    this project to approximate the steady-state vectors are not practical.\n    There are many methods for approximating eigenvectors that are often used in these situations,\n    some of which we discuss in a later section.\n   "
},
{
  "id": "objectives-9",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#objectives-9",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is an eigenvalue of a matrix?\n           \n         \n           \n            What is an eigenvector of a matrix?\n           \n         \n           \n            How do we find eigenvectors of a matrix corresponding to an eigenvalue?\n           \n         \n           \n            How can the action of a matrix on an eigenvector be visualized?\n           \n         \n           \n            Why do we study eigenvalues and eigenvectors?\n           \n         \n           \n            What are discrete dynamical systems and how do we analyze the long-term \n            behavior in them?\n           \n         "
},
{
  "id": "p-1545",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#p-1545",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "eigenvector eigenvalue "
},
{
  "id": "def_eigenvector",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#def_eigenvector",
  "type": "Definition",
  "number": "9.1",
  "title": "",
  "body": "eigenvector characteristic vector eigenvalue characteristic value eigenvector characteristic vector eigenvalue characteristic value "
},
{
  "id": "pa_2_b_1",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#pa_2_b_1",
  "type": "Preview Activity",
  "number": "9.1",
  "title": "",
  "body": "\n            For each of the following parts,\n            use the definition of an eigenvector to determine whether the given vector \n              is an eigenvector for the given matrix  .\n            If it is, determine the corresponding eigenvalue.\n           \n                   ,\n                   \n                 \n                   ,\n                   \n                 \n                   ,\n                   \n                 \n                   ,\n                   \n                 \n            We now consider how we can find the eigenvectors corresponding to an eigenvalue using the definition.\n            Suppose  .\n            We consider whether we can find eigenvectors corresponding to eigenvalues 3, and 5.\n            Effectively,\n            this will help us determine whether 3 and\/or 5 are eigenvalues of  .\n           \n                  Rewrite the vector equation   where\n                    as a vector \n                  equation.\n                 \n                  After writing   as   where   is the identity matrix,\n                  rearrange the variables to turn this vector equation into the homogeneous \n                  matrix equation\n                    where \n                   .\n                  If possible,\n                  find a non-zero (i.e. a non-trivial) solution to  .\n                  Explain what this means about 5 being an eigenvalue of   or not.\n                 \n                  Similarly, determine whether the vector equation\n                    has non-zero solutions.\n                  Using your result,\n                  determine whether 3 is an eigenvalue of   or not.\n                 "
},
{
  "id": "activity-35",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#activity-35",
  "type": "Activity",
  "number": "9.2",
  "title": "",
  "body": "\n          Under what conditions on   will the matrix equation\n            have non-trivial solutions?\n          Describe at least two different but equivalent conditions.\n         \n          The real number 0 is an eigenvalue of \n           .\n          Check that your criteria in the previous part agrees with this result.\n         \n          Determine if 5 is an eigenvalue of the matrix\n            \n          using your criterion above.\n         \n          What are the two eigenvalues of the matrix \n           ?\n         "
},
{
  "id": "activity-36",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#activity-36",
  "type": "Activity",
  "number": "9.3",
  "title": "",
  "body": "\n            For   to be an eigenvalue of  ,\n            we noted that   must have a non-pivot column.\n            Use this criterion to explain why \n              has eigenvalues\n              and  .\n           \n            Determine the eigenvalues of \n             .\n           \n            Generalize your results from the above parts in the form of a theorem in the\n            most general   case.\n           "
},
{
  "id": "p-1572",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#p-1572",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "discrete dynamical systems dynamical system "
},
{
  "id": "pa_2_b_2",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#pa_2_b_2",
  "type": "Activity",
  "number": "9.4",
  "title": "",
  "body": "\n            Consider a discrete dynamical system providing a simplified model of predator-prey \n            interactions in biology,\n            such as the system describing the populations of rabbits and foxes in a certain area.\n            Suppose, for example,\n            for a specific area the model is given by the following equations:\n             \n            where   represents the number of rabbits in the area   years after \n            a starting time value,\n            and   represents the number of foxes in year  .\n            We use   to denote the initial population values.\n           \n                  Suppose   and   for one year.\n                  Calculate rabbit and fox population values for the next year.\n                  In other words, find   values.\n                 \n                  Consider the coefficients of the variables\n                    in the the system of equations in  .\n                  Can you explain the reasoning behind the signs and absolute sizes of \n                  the coefficients from the story that it models?\n                 state vector transition matrix \n                  The transition matrix will help us simplify calculations of the population values.\n                  Note that equation   implies that  ,\n                   ,  , and so on.\n                  This is a recursive method to find the population values as each year's \n                  population values depend on the previous year's population values.\n                  Using this approach,\n                  calculate   for   values up to 5 corresponding to the following\n                   three different \n                  initial rabbit-fox population values (all in thousands):\n                   \n                   \n                   \n                  Can you guess the long-term behavior of the population values in each case?\n                  Are they both increasing?\n                  Decreasing?\n                  One increasing, one decreasing?\n                  How do the rabbit and fox populations compare?\n                 "
},
{
  "id": "act_dynamical_system",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#act_dynamical_system",
  "type": "Activity",
  "number": "9.5",
  "title": "",
  "body": "\n      In this activity the matrix   is the transition matrix for the rabbit and \n      fox population model,\n       .\n     \n            Suppose that the initial state vector   is an eigenvector of   \n            corresponding to eigenvalue  .\n            In this case,\n            explain why   and  .\n            Find the formula for   in terms of\n              and   by applying equation \n              iteratively.\n           \n            The initial state vector \n              is an \n            eigenvector of  .\n            Find the corresponding eigenvalue and,\n            using your formula from (a) for   in terms of\n              and  , find the state vector   in this case.\n           \n            The initial state vector \n              \n            is an eigenvector of  .\n            Find the corresponding eigenvalue and,\n            using your formula from (a) for   in terms of\n              and  , find the state vector   in this case.\n           \n            Consider now an initial state vector of the form\n              where   are constants,\n              is an eigenvector corresponding to eigenvalue\n              and   corresponding to eigenvalue\n              (  and   are not necessarily the \n            eigenvectors from parts (b) and (c)).\n            Use matrix algebra and equation   \n            to explain why  .\n           \n            Express the initial state vector\n              \n            as a linear combination of the eigenvectors\n              and use your result from the previous part to find a formula for  .\n            What happens to the population values as  \n           "
},
{
  "id": "example-17",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#example-17",
  "type": "Example",
  "number": "9.2",
  "title": "",
  "body": "\n        Let  .\n       \n              Find all of the eigenvalues of  .\n             \n              Recall that a scalar   is a eigenvalue for   if there is a \n            nonzero vector   such that\n              or  .\n            For this matrix  , we have\n             .\n            To solve the homogeneous system  ,\n            we row reduce  .\n            To do this, we first interchange rows to get the following matrix that is row \n            equivalent to\n              (we do this to ensure that we have a nonzero entry in the \n            first row and column)\n             .\n            Next we replace row two with\n              times row one minus row two to obtain the row \n            equivalent matrix\n             .\n            There will be a nontrivial solution to\n              if there is a row of zeros in this row echelon form.\n            Thus, we look for values of   that make\n             .\n            Applying a little algebra shows that\n             .\n            So the eigenvalues of   are\n              and  .\n             \n              Find a corresponding eigenvector for each eigenvalue found in part (a).\n             \n              Recall that an eigenvector for the eigenvalue   is a nonzero vector \n                such that  .\n              We consider each eigenvalue in turn.\n          \n               \n                   \n                    When  ,\n                     .\n                    Technology shows that the reduced row echelon form of   is\n                     .\n                    If  ,\n                    then   implies that   is free and  .\n                    Choosing   gives us the eigenvector  .\n                    As a check, note that\n                     .\n                   \n                 \n                   \n                    When  ,\n                     .\n                    Technology shows that the reduced row echelon form of   is\n                     .\n                    If  ,\n                    then   implies that   is free and  .\n                    Choosing   gives us the eigenvector  .\n                    As a check, note that\n                     .\n                   \n                 \n             "
},
{
  "id": "example-18",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#example-18",
  "type": "Example",
  "number": "9.3",
  "title": "",
  "body": "transition matrix Markov process Markov chain transition matrix \n      Accurately predicting the weather has long been an important task.\n      Meteorologists use science, mathematics,\n      and technology to construct models that help us understand weather patterns.\n      These models are very sophisticated,\n      but we will consider only a simple model.\n      Suppose, for example,\n      we want to learn something about whether it will be wet or dry in Grand Rapids, Michigan.\n      To do this, we might begin by collecting some data about weather conditions in \n      Grand Rapids and then use that to make predictions.\n      Information taken over the course of 2017 from the National Weather Service Climate \n      Data shows that if it was dry\n      (meaning no measurable precipitation, either rain or snow)\n      on a given day in Grand Rapids,\n      it would be dry the next day with a probability of 64% and wet with a probability of 36%. Similarly,\n      if it was wet on a given day it would be dry the next day with a probability of 47% and \n      dry with a probability of 53%. Assuming that this pattern is one that continues in the \n      long run,\n      we can develop a mathematical model to make predictions about the weather.\n     transition matrix state \n            Calculate  .\n            Interpret the meaning of this output.\n           \n            Here we have\n             .\n            This output tells us the different probabilities of whether it will be dry or wet \n            the day following a dry day.\n           \n            Calculate  .\n            Interpret the meaning of this output.\n           \n            Here we have\n             .\n            This output tells us the different probabilities of whether it will be dry or \n            wet the day following a wet day.\n           \n            Calculate  .\n            Interpret the meaning of this output.\n           \n            Here we have\n             .\n            This output tells us there is a 52% chance of it being dry and a 48% chance of it \n            being wet following a day when there is a 30% chance of it being dry and a 70% \n            chance of it being wet.\n           state vector \n                  Starting with  ,\n                  use appropriate technology to calculate   for   values up to 10.\n                  Round to three decimal places.\n                  What do you notice about the entries?\n                 \n                  Technology shows that\n                   \n                  We can see that our vectors   are essentially the same as we \n                  let   increase.\n                 \n                  What does the result of the previous part tell us about eigenvalues of  ?\n                  Explain.\n                 \n                  Since our sequence seems to be converging to a vector   \n                  satisfying  ,\n                  we conclude that 1 is an eigenvalue of  .\n                 \n                  Rewrite   as\n                   .\n                  We do this so we can use exact arithmetic.\n                  Let  .\n                  What is  ? (Use exact arithmetic,\n                  no decimals.) Explain how   is related to the previous \n                  two parts of this problem.\n                  What does the vector   tells us about weather in Grand Rapids?\n                 \n                  A matrix vector multiplication shows that\n                   .\n                  In other words,\n                    is an eigenvector for   with eigenvalue 1.\n                  Notice that\n                   ,\n                  so these fractions give the same results we obtained with our \n                  sequence of vectors  .\n                  These vectors provide a steady-state vector for Grand Rapids weather.\n                  In other words,\n                  if there is a   chance of it being dry on a given day \n                  in Grand Rapids,\n                  then there is a   chance it will be dry again the next day.\n                 \n      This is an example of a Markov process.\n      Markov processes\n      (named after Andrei Andreevich Markov)\n      are widely used to model phenomena in biology, chemistry,\n      business,\n      physics, engineering, the social sciences, and much more.\n      More specifically,\n     Markov process Markov process transition matrix Markov chain "
},
{
  "id": "exercise-90",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#exercise-90",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        For each of the following matrix-vector pairs,\n        determine whether the given vector is an eigenvector of the matrix.\n       \n               ,\n               \n             \n              Eigenvector with eigenvalue  .\n             \n               ,\n               \n             \n              Eigenvector with eigenvalue  .\n             \n               ,\n               \n             \n              Eigenvector with eigenvalue  .\n             "
},
{
  "id": "exercise-91",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#exercise-91",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        For each of the following matrix-eigenvalue pairs,\n        determine an eigenvector of   for the given eigenvalue.\n       \n               ,\n                 \n             \n               ,\n                 \n             \n               ,\n                 \n             \n               ,\n                \n             "
},
{
  "id": "exercise-92",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#exercise-92",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        For each of the following matrix-  pairs,\n        determine whether the given   will work as an eigenvalue.\n        You do not need to find an eigenvector as long you can justify if \n          is a valid eigenvalue or not.\n       \n               ,\n                 \n             \n              Eigenvalue\n             \n               ,\n                 \n             \n              Eigenvalue\n             \n               ,\n                 \n             \n              Not an eigenvalue\n             \n               ,\n                \n             \n              Not an eigenvalue\n             "
},
{
  "id": "exercise-93",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#exercise-93",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        For a matrix   with eigenvector\n          with eigenvalue \n         ,\n        and eigenvector   \n        with eigenvalue  ,\n        determine the value of the following expressions using matrix-vector product properties:\n       \n               \n             \n               \n             \n               \n             "
},
{
  "id": "ex_2_b_Markov",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#ex_2_b_Markov",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "Markov chain Markov chain \n              We know that there will be 900,000 students in class on the second day and \n              100,000 students skipping class.\n              On the third day, 90% of the 900,000 students (attenders) and 30% of the \n              100,000 students (skippers) will come back to class.\n              Therefore, 840,000 students will attend class on the third day.\n              On the other hand, 10% of 900,000 students and 70% of 100,000 students \n              skip class on the third day,\n              for a total of 160,000 students skipping class.\n              We can use variables to represent these numbers.\n              Let   represent the number of students attending class   \n              days after first day.\n              So  .\n              Let   represent the students skipping class.\n              So  .\n              Find  .\n             \n                 ,\n               ,  \n             \n              Find a linear expression for   in terms of the previous day values,\n                and  , using the story given in the problem.\n              Similarly, express   in terms of   and  .\n             \n               ,\n               \n             \n              Let   represent the state vector:\n               .\n              It describes the state of the whole system\n              (students attending class and skipping class)\n              in one vector.\n              For example,\n                is the initial state.\n              The state next day is \n               .\n              Using your answer to the previous part,\n              find a matrix   which describes how the system changes from one day \n              to the other so that\n               .\n             \n                equals  \n             "
},
{
  "id": "exercise-95",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#exercise-95",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              The number 0 cannot be an eigenvalue.\n             \n              F\n             True\/False \n              The   vector cannot be an eigenvector.\n             True\/False \n              If   is an eigenvector of  ,\n              then so is  .\n             \n              T\n             True\/False \n              If   is an eigenvector of  ,\n              then it is also an eigenvector of  .\n             True\/False \n              If   and   are eigenvectors of   with the same eigenvalue,\n              then   is also an eigenvector with the same eigenvalue.\n             \n              T\n             True\/False \n              If   is an eigenvalue of  ,\n              then   is an eigenvalue of  .\n             True\/False \n              A projection matrix satisfies  .\n              If   is a projection matrix,\n              then the eigenvalues of   can only be 0 and 1.\n             \n              T\n             True\/False \n              If   is an eigenvalue of an   matrix  ,\n              then   is an eigenvalue of  .\n             True\/False \n              If   is an eigenvalue of two matrices   and   of \n              the same size,\n              then   is an eigenvalue of  .\n             \n              F\n             True\/False \n              If   is an eigenvector of two matrices   and   of the \n              same size,\n              then it is also an eigenvector of  .\n             True\/False \n              A matrix   has 0 as an eigenvalue if and only if   has linearly dependent columns.\n             \n              T\n             "
},
{
  "id": "F_seven_page",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#F_seven_page",
  "type": "Figure",
  "number": "9.5",
  "title": "",
  "body": "A seven page internet "
},
{
  "id": "p-1678",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#p-1678",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "transition matrix "
},
{
  "id": "p-1679",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#p-1679",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "state vector "
},
{
  "id": "p-1682",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#p-1682",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Markov process "
},
{
  "id": "definition-25",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#definition-25",
  "type": "Definition",
  "number": "9.6",
  "title": "",
  "body": "stochastic matrix stochastic "
},
{
  "id": "act_limit",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#act_limit",
  "type": "Project Activity",
  "number": "9.6",
  "title": "",
  "body": "\n      Use appropriate technology to do the following.\n      Choose several different initial state vectors   and calculate the \n      vectors in the sequence\n        for large values of  .\n      (Note that, as state vectors,\n      the entries of   cannot be negative and the sum of the entries of \n        must be  .)\n      Explain the behavior of the sequence\n        as   gets large.\n      Do you notice anything strange?\n      What aspects of our seven page internet do you think explain this behavior?\n      Clearly communicate all of the experimentation that you do.\n      You may use the GeoGebra applet at \n       .\n     "
},
{
  "id": "p-1686",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#p-1686",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "steady-state equilibrium "
},
{
  "id": "project-27",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#project-27",
  "type": "Project Activity",
  "number": "9.7",
  "title": "",
  "body": "\n      Show that the limiting vector you found in  \n      is an eigenvector of   with eigenvalue 1.\n     "
},
{
  "id": "LQ_G2",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#LQ_G2",
  "type": "Project Activity",
  "number": "9.8",
  "title": "",
  "body": "\n            Determine the transition matrix for our seven page internet with this adjustment.\n           \n            Approximate the steady-state vector for this adjusted matrix so that the entries are accurate to four decimal places.\n            Use any appropriate technology to row reduce matrices.\n           \n            According to this adjusted model,\n            which web page is now the most important?\n            Why?\n            Does this seem reasonable?\n            Why?\n           "
},
{
  "id": "F_five_page",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#F_five_page",
  "type": "Figure",
  "number": "9.7",
  "title": "",
  "body": "A five page internet "
},
{
  "id": "Q_no_limit",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#Q_no_limit",
  "type": "Project Activity",
  "number": "9.9",
  "title": "",
  "body": "\n            Explain why\n             .\n            is the transition matrix for this five page internet.\n            (Keep in mind the adjustment we made for dangling pages.)\n           \n            Start with different initial state vectors   \n            and determine if there is a limit to the Markov chain.\n            Explain.\n            You may use the GeoGebra applet at \n             .\n           "
},
{
  "id": "definition-26",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#definition-26",
  "type": "Definition",
  "number": "9.8",
  "title": "",
  "body": "stochastic matrix regular regular "
},
{
  "id": "thm_Google_1",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#thm_Google_1",
  "type": "Theorem",
  "number": "9.9",
  "title": "",
  "body": "\n        Assume   and that   is a regular   stochastic matrix.\n         \n             \n                exists and is a stochastic matrix.\n             \n           \n             \n              For any vector  ,\n               \n              for the same vector  .\n             \n           \n             \n              The columns of   are the same vector  .\n             \n           \n             \n              The vector   is the unique eigenvector of   whose entries sum to 1.\n             \n           \n             \n              If   is an eigenvalue of   not equal to 1, \n              then  .\n             \n           \n       "
},
{
  "id": "project-30",
  "level": "2",
  "url": "chap_intro_eigenvals_eigenvects.html#project-30",
  "type": "Project Activity",
  "number": "9.10",
  "title": "",
  "body": "\n      Return to the seven page internet in  .\n     \n            Find the Google matrix   for this internet.\n           \n            Approximate, to four decimal places,\n            the steady-state vector for this internet.\n           \n            What is the relative rank of each page in this internet,\n            and approximately what percentage of time does a random user spend on each page.\n           "
},
{
  "id": "chap_matrix_inverse",
  "level": "1",
  "url": "chap_matrix_inverse.html",
  "type": "Section",
  "number": "10",
  "title": "The Inverse of a Matrix",
  "body": "The Inverse of a Matrix \n        By the end of this section, you should be able to give precise and thorough\n        answers to the questions listed below. You may want to keep these questions\n        in mind to focus your thoughts as you complete the section.\n       \n           \n            What does it mean for a matrix   to be invertible?\n           \n         \n           \n            How can we tell when an   matrix   is invertible?\n           \n         \n           \n            If an   matrix   is invertible,\n            how do we find the inverse of  ?\n           \n         \n           \n            If   and   are invertible   matrices,\n            why is   invertible and what is  ?\n           \n         \n           \n            How can we use the inverse of a matrix in solving matrix equations?\n           \n         Application: Modeling an Arms Race \n    Lewis Fry Richardson was a Quaker by conviction who was deeply troubled by the major wars \n    that had been fought in his lifetime.\n    Richardson's training as a physicist led him to believe that the causes of war were \n    phenomena that could be quantified, studied,\n    explained, and thus controlled.\n    He collected considerable data on wars and constructed a model to represent an arms race.\n    The equations in his model caused him concern about the future as indicated by the \n    following statement:\n   \n    But it worried him that the equations also showed that the unilateral disarmament of \n    Germany after 1918, enforced by the Allied Powers, combined with the persistent \n    level of armaments of the victor countries would lead to the level of Germany's \n    armaments growing again. In other words, the post-1918 situation was not stable. \n    From the model he concluded that great statesmanship would be needed to prevent an \n    unstable situation from developing, which could only be prevented by a change of policies.\n     \n     Nature  135, 830-831 (18 May 1935)\n     Mathematical Psychology of War \n    (3420).\n     \n    Analyzing Richardson's arms race model utilizes matrix operations,\n    including matrix inverses.\n    We explore the basic ideas in Richardson's model later in this section.\n   Introduction \n    To this point we have solved systems of linear equations with matrix forms\n      by row reducing the augmented matrices  .\n    These linear matrix-vector equations should remind us of linear algebraic equations \n    of the form  ,\n    where   and   are real numbers.\n    Recall that we solved an equation of the form   by dividing both sides by  \n    (provided  ),\n    giving the solution  ,\n    or equivalently  .\n    The important property that the number   has that allows us to solve a \n    linear equation in this way is that  ,\n    so that   is the multiplicative inverse of  .\n    We can solve certain types of matrix equations   in the same way,\n    provided we can find a matrix   with similar properties.\n    We investigate this situation in this section.\n   \n          Before we define the inverse matrix,\n          recall that the identity matrix   (with 1's along the diagonal \n          and 0's everywhere else) is a multiplicative identity in the set of \n            matrices\n          (just like the real number 1 is the multiplicative identity in the set of real number).\n          In particular,\n            for any   matrix  .\n          Now we can generalize the inverse operation to matrices.\n          For an   matrix  ,\n          we define   to be the matrix which when multiplied by   gives \n          us the identity matrix.\n          In other words,  .\n          We can find the inverse of a matrix in a calculator by using the   button.\n         \n          For each of the following matrices,\n          determine if the inverse exists using your calculator or other appropriate technology.\n          If the inverse does exist,\n          write down the inverse and check that it satisfies the defining property of the \n          inverse matrix,\n          that is  .\n          If the inverse doesn't exist,\n          write down any error you received from the technology.\n          Can you guess why the inverse does not exist for these matrices?\n         \n                   \n               \n                   \n               \n                   \n               \n                   \n               \n                   \n               \n                  \n               \n          Now we turn to the question of how to find the inverse of a matrix in general.\n          With this approach,\n          we will be able to determine which matrices have inverses as well.\n          We will consider the   case to make the calculations easier.\n          Suppose   is a   matrix.\n          Our goal is to find a matrix   so that   and  .\n          If such a matrix exists,\n          we will call   the inverse,\n           , of  .\n         \n                What does the equation   tell us about the size of the \n                matrix  ?\n               \n                Now let  .\n                We want to find a  matrix   so that  .\n                Suppose   has columns   and  , i.e.\n                 .\n                Our definition of matrix multiplication shows that\n                 .\n               \n                      If  , what must   and   equal?\n                     \n                      Use the result from part (a) to set up two matrix equations to solve \n                      to find   and  .\n                      Then find   and  .\n                      As a result, find the matrix  .\n                     \n                      When we solve the two systems we have found a matrix   so that  .\n                      Is this enough to say that   is the inverse of  ?\n                      If not, what else do we need to know to verify that   is in fact  ?\n                      Verify that   is  .\n                     \n          A matrix inverse is extremely useful in solving matrix equations and can help us \n          in solving systems of equations.\n          Suppose that   is an invertible matrix,\n          i.e., there exists   such that  .\n         \n                Consider the system  .\n                Use the inverse of   to show that this system has a solution\n                for every   and find an expression for this solution in terms of \n                  and  .\n                (Note that since matrix multiplication is not commutative,\n                we have to pay attention to the order in which we multiply matrices.\n                For example,   while we cannot simplify\n                  to   unless   and   commute.)\n               \n                If  ,\n                 , and   are matrices and  ,\n                then we can subtract the matrix   from both sides to see that  .\n                We saw in  \n                that there is no corresponding general cancellation property for matrix multiplication when we found that   could hold while  .\n                However, we can cancel   from this equation in certain circumstances.\n                Suppose that   and that   is an invertible matrix.\n                Show that we can cancel   in this case and conclude that  .\n                (Note: When simplifying the product of matrices,\n                again keep in mind that matrix multiplication is not commutative.)\n               Invertible Matrices multiplicative inverses \n    Of course, we didn't have to write both products because multiplication \n    of real numbers is a commutative operation.\n    There are a couple of important things to note about multiplicative inverses \n      we can use the inverses of the number   to solve the simple linear equation\n      for   ( ),\n    and not every real number has an inverse.\n    The latter means that the inverse is not defined on the entire set of real numbers.\n    We can extend the idea of inverses to matrices,\n    although we will see that there are many more matrices than just the zero matrix \n    that do not have inverses.\n   \n    To define matrix inverses \n    We usually refer to a multiplicative inverse as just an inverse.\n    Since every matrix has an additive inverse,\n    there is no need to consider the existence of additive inverses.\n      we make an analogy with the property of inverses in the real numbers:\n     .\n   matrix invertible matrix inverse invertible inverse non-singular singular \n            Let  .\n            Calculate   where \n             .\n            Using your result,\n            explain why it is not possible to have  ,\n            showing that   is non-invertible.\n           \n            Calculate   where \n              and \n             .\n            Using your result, explain why the inverse of   doesn't exist.\n           \n    We saw in  \n    why the inverse does not exist for two specific matrices.\n    We will find in the next section an easy criterion for determining when a matrix has an inverse.\n    In short, when the RREF of the matrix has a pivot in every column and row,\n    then the matrix will be invertible.\n    We know that this condition relates to quite a few other linear algebra concepts \n    we have seen so far,\n    such as linear independence of columns and the columns spanning  .\n    We will put these criteria together in one big theorem in the next section.\n   \n      Suppose that   is an invertible   matrix.\n      Hence we have an inverse matrix   for which  .\n      We will see how the inverse is useful in solving matrix equations involving  .\n     \n            Explain why the matrix expressions\n             \n            can all be simplified to  .\n           \n            Use the associative property of matrix multiplication.\n           \n            Suppose the system   has a solution.\n            Explain why then  .\n            What does this equation simplify to?\n           \n            Since we found one single expression for the solution   in \n            equation  ,\n            this implies that the equation has a unique solution.\n            What does this imply about the matrix  ?\n           \n    As we saw in  ,\n    if the   matrix   is invertible,\n    then the equation   is consistent for all   in   \n    and has the unique solution  .\n    This means that   has a pivot in every row and column,\n    which is equivalent to the criterion that   reduces to  ,\n    as we noted above.\n   \n    Even though   is an explicit expression for the solution of the \n    system  ,\n    using the inverse of a matrix is usually not a computationally efficient way to solve a \n    matrix equation.\n    Finding the RREF of a matrix computationally takes fewer steps to solve the matrix equation.\n   Finding the Inverse of a Matrix \n    The next questions for us to address are how to tell when a matrix is invertible \n    and how to find the inverse of an invertible matrix.\n    Consider a   matrix  .\n    To find the inverse matrix   of  ,\n    we have to solve the two matrix-vector equations \n      and\n      to find the columns of  .\n    Since   is the coefficient matrix for both systems,\n    we apply the same row operations on both systems to reduce   to RREF. Thus,\n    instead of solving the two matrix-vector equations separately,\n    we could simply have found the RREF of\n     \n    and done all of the work in one pass.\n    Note that the right hand side of the augmented matrix is now  .\n    So we row reduce  ,\n    and if the systems are consistent,\n    the reduced row echelon form of\n      must be  .\n    You should be able to see that this same process works in any dimension.\n   How to find the inverse of an\n      matrix  \n     \n         \n          Augment   with the identity matrix  .\n         \n       \n         \n          Apply row operations to reduce the augmented matrix  .\n          If the system is consistent,\n          then the reduced row echelon form of   will have the form\n            (by   (d)).\n          If the reduced row echelon form of   is not  ,\n          then this step fails and   is not invertible.\n         \n       \n         \n          If   is row equivalent to  ,\n          then the matrix   in the second step has the property that  .\n          We will show later that the matrix   also satisfies\n            and so   is the inverse of  .\n         \n       \n   \n      Find the inverse of each matrix\n       using the method above , if it exists.\n      Compare the result with the inverse that you get from using appropriate technology \n      to directly calculate the inverse.\n     \n             \n           \n             \n           determinant of  Properties of the Matrix Inverse \n    As we have done with every new operation,\n    we ask what properties the inverse of a matrix has.\n   \n      Consider the following questions about matrix inverses.\n      If two   matrices   and   are invertible,\n      is the product   invertible?\n      If so, what is the inverse of  ?\n      We answer these questions in this activity.\n     \n            Let\n             .\n           \n                  Use formula   to find the inverses of   and  .\n                 \n                  Find the matrix product  .\n                  Is   invertible?\n                  If so, use formula   to find the inverse of  .\n                 \n                  Calculate the products   and  .\n                  What do you notice?\n                 \n            In part (a) we saw that the matrix product\n              was the inverse of the matrix product  .\n            Now we address the question of whether this is true in general.\n            Suppose now that   and   are invertible\n              matrices so that the matrix inverses   and \n              exist.\n           \n                  Use matrix algebra to simplify the matrix product \n                   .\n                  \n                 \n                  What do you know about   and  ?\n                 \n                  Simplify the matrix product\n                    in a manner similar to part i.\n                 \n                  What conclusion can we draw from parts i and ii?\n                  Explain.\n                  What property of matrix multiplication requires us to reverse the order of the product when we create the inverse of  ?\n                 \n     \n    gives us one important property of matrix inverses.\n    The other properties given in the next theorem can be verified similarly.\n   \n        Let   and   be invertible   matrices.\n        Then\n         \n             \n               .\n             \n           \n             \n              The product   is invertible and  .\n             \n           \n             \n              The matrix   is invertible and\n               .\n             \n           \n       Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        For each of the following matrices  ,\n      \n         \n             \n              Use appropriate technology to find the reduced row \n              echelon form of  .\n             \n           \n             \n              Based on the result of part (a), is   invertible?\n              If yes, what is  ?\n              If no, explain why.\n             \n           \n             \n              Let   and  .\n              If   is invertible,\n              solve the matrix equation   using the inverse of  .\n              If   is not invertible,\n              find all solutions, if any,\n              to the equation   using whatever method you choose.\n             \n           \n       \n               \n             \n              With  ,\n              we have the following.\n\n             \n               \n                The reduced row echelon form of   is\n                 .\n               \n             \n               \n                Since   is row equivalent to  ,\n                    we conclude that   is invertible.\n                    The reduced row echelon form of   tells us that\n                     .\n                   \n                 \n                   \n                    The solution to   is given by\n                     .\n                   \n                 \n             \n               \n             \n              With  ,\n            we have the following.\n             \n                 \n                  The reduced row echelon form of   is\n                   .\n                 \n               \n                 \n                  Since   is not row equivalent to  ,\n                  we conclude that   is not invertible.\n                 \n               \n                 \n                  The reduced row echelon form of   is\n                   .\n                  The fact that the augmented column is a pivot column means that the equation \n                    has no solutions.\n                 \n               \n             \n              Let  .\n             \n                    Show that   but  .\n                   \n                    Let  .\n                   \n                    Using technology to calculate   and   we find that   while  .\n                   \n                    Show that   is invertible and find its inverse.\n                    Compare the inverse of   to  .\n                   \n                    Let  .\n                   \n                     For this matrix   we have  .\n                    The reduced row echelon form of   is\n                     ,\n                    so   is invertible and  .\n                    A straightforward matrix calculation also shows that\n                     .\n                   \n              Let   be an arbitrary square matrix such that  .\n              Show that   is invertible and find an inverse for  .\n             \n              We can try to emulate the result of part (a) here.\n              Expanding using matrix operations gives us\n               \n              and\n               .\n              So   is invertible and  .\n              This argument can be generalized to show that if   is a square matrix and   for some positive integer  ,\n              then   is invertible and\n               .\n             Summary \n       \n        If   is an   matrix,\n        then   is invertible if there is a matrix   so that  .\n        The matrix   is called the inverse of   and is denoted  .\n       \n     \n       \n        An   matrix   is invertible if and only if   the \n        reduced row echelon form of   is the\n          identity matrix  .\n       \n     \n       \n        To find the inverse of an invertible   matrix  ,\n        augment   with the identity and row reduce.\n        If  , then  .\n       \n     \n       \n        If   and   are invertible   matrices,\n        then  .\n        Since the inverse of   exists,\n        the product of two invertible matrices is an invertible matrix.\n       \n     \n       \n        We can use the algebraic tools we have developed for matrix operations to solve \n        equations much like we solve equations with real variables.\n        We must be careful, though,\n        to only multiply by inverses of invertible matrices,\n        and remember that matrix multiplication is not commutative.\n       \n     \n        Let   be an invertible   matrix.\n        In this exercise we will prove that the inverse of   is unique.\n        To do so, we assume that both   and   are inverses of  ,\n        that is   and  .\n        By considering the product   simplified in two different ways,\n        show that  , implying that the inverse of   is unique.\n       \n         \n       \n        Let   be an arbitrary   matrix.\n       \n              If   is invertible,\n              perform row operations to determine a row echelon form of  .\n              \n             \n              You may need to consider different cases,\n              e.g., when   and when  .\n             \n              Under certain conditions,\n              we can row reduce   to  , where\n               .\n              Use the row echelon form of   from part (a) to find conditions under which the\n                matrix   is invertible.\n              Then derive the formula for the inverse   of  .\n             \n            For a few different   values,\n            find the inverse of  .\n            From these results,\n            make a conjecture as to what   is in general.\n           \n             \n           \n            Prove your conjecture using the definition of inverse matrix.\n           \n            Row reduce  \n           \n            Find the inverse of  .\n           \n            Row reduce   to see that  .\n           \n      (Note: You can combine the first two parts above by applying the inverse finding algorithm directly on  .)\n       \n        Solve for the matrix   in terms of the others in the following equation:\n         \n        If you need to use an inverse, assume it exists.\n       \n        For which   is the matrix   invertible?\n       \n         \n       \n        For which   is the matrix   invertible?\n       \n        Let   and   be invertible   matrices.\n        Verify the remaining properties of  .\n        That is, show that\n       \n               .\n             \n              The matrix   is invertible and  .\n             \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If   is an invertible matrix,\n              then for any two matrices  ,\n                implies  .\n             \n              T\n             True\/False \n              If   is invertible,\n              then so is   for any matrix  .\n             True\/False \n              If   and   are invertible\n                matrices, then so is  .\n             \n              T\n             True\/False \n              If   is an invertible   matrix,\n              then the equation   is consistent for any   in  .\n             True\/False \n              If   is an invertible   matrix,\n              then the equation   has a unique solution when it is consistent.\n             \n              T\n             True\/False \n              If   is invertible, then so is  .\n             True\/False \n              If   is invertible,\n              then it reduces to the identity matrix.\n             \n              T\n             True\/False \n              If a matrix is invertible, then so is its transpose.\n             True\/False \n              If   and   are invertible   matrices,\n              then   is invertible.\n             \n              F\n             True\/False \n              If  ,\n              then   is invertible.\n             Project: The Richardson Arms Race Model \n    How and why a nation arms itself for defense depends on many factors.\n    Among these factors are the offensive military capabilities a nation deems its enemies have,\n    the resources available for creating military forces and equipment,\n    and many others.\n    To begin to analyze such a situation,\n    we will need some notation and background.\n    In this section we will consider a two nation scenario,\n    but the methods can be extended to any number of nations.\n    In fact, after World War I, Richardson collected data and created a model for the countries Czechoslovakia, China, France, Germany, England, Italy, Japan, Poland,\n    the USA, and the USSR. \n    The Union of Soviet Socialist Republics (USSR), headed by Russia,\n    was a confederation of socialist republics in Eurasia.\n    The USSR disbanded in 1991.\n    Czechoslovakia was a sovereign state in central Europe that peacefully split into the Czech Republic and Slovakia in 1993.\n     \n   armament \n      We continue to analyze a two nation scenario.\n      Let us suppose that our two nations are Iran\n      (nation  )\n      and Iraq\n      (nation  ).\n      In 1980, Iraq invaded Iran resulting in a long and brutal 8 year war.\n      Richardson was interested in analyzing data to see if such wars could be predicted by the changes in armaments of each nation.\n      We construct the two nation model in this activity.\n     fatigue factor \n            Taking the three effects discussed above into consideration,\n            explain why\n             .\n            Then explain why\n             .\n           \n            Write an equation similar to equation   that describes   in terms of the three effects.\n           \n            Let  .\n            Explain why\n             ,\n            where   and  .\n           Military Expenditures of Iran and Iraq 1966-1975 Year Iran Iraq \n      In order to analyze a specific arms race between nations,\n      we need some data to determine values of the   and the  .\n       \n      shows the military expenditures of Iran and Iraq in the years leading up to their war in 1975. (The data is in millions of US dollars,\n      adjusted for inflation and is taken from ``World Military Expenditures and Arms Transfers 1966-1975\" by the U.S. Arms Control and Disarmament Agency.) We can perform regression\n      (we will see how in a later section)\n      on this data to obtain the following linear approximations:\n       \n     \n      (Of course, the data does not restrict itself to only factors between the two countries,\n      so our model will not be as precise as we might like.\n      However, it is a reasonable place to start.) Use the regression equations   and   to explain why\n       \n      for our Iran-Iraq arms race.\n     \n     \n    and  \n    provide the basics to describe the general arms race model due to Richardson.\n    If we have an   nation arms race with\n      and   , then\n     .\n   equilibrium state \n      We can apply matrix algebra to find the equilibrium state vector   under certain conditions.\n     \n            Assuming that   exists,\n            use matrix algebra and  Equation  to show that\n             .\n           \n            Under what conditions can we be assured that there will always be a unique equilibrium state  ?\n            Explain.\n            Under these conditions, how can we find this unique equilibrium state?\n            Write this equilibrium state vector   as a matrix-vector product.\n           \n            Does the arms race model for Iran and Iraq have an equilibrium solution?\n            If so, find it.\n            If not, explain why not.\n            Use technology as appropriate.\n           \n            Assuming an equilibrium exists and that both nations behave in a way that supports the equilibrium,\n            explain what the appropriate entry of the equilibrium state vector   suggests about what Iran and Iraq's policies should be.\n            What does this model say about why there might have been war between these two nations?\n           "
},
{
  "id": "objectives-10",
  "level": "2",
  "url": "chap_matrix_inverse.html#objectives-10",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n        By the end of this section, you should be able to give precise and thorough\n        answers to the questions listed below. You may want to keep these questions\n        in mind to focus your thoughts as you complete the section.\n       \n           \n            What does it mean for a matrix   to be invertible?\n           \n         \n           \n            How can we tell when an   matrix   is invertible?\n           \n         \n           \n            If an   matrix   is invertible,\n            how do we find the inverse of  ?\n           \n         \n           \n            If   and   are invertible   matrices,\n            why is   invertible and what is  ?\n           \n         \n           \n            How can we use the inverse of a matrix in solving matrix equations?\n           \n         "
},
{
  "id": "pa_2_c",
  "level": "2",
  "url": "chap_matrix_inverse.html#pa_2_c",
  "type": "Preview Activity",
  "number": "10.1",
  "title": "",
  "body": "\n          Before we define the inverse matrix,\n          recall that the identity matrix   (with 1's along the diagonal \n          and 0's everywhere else) is a multiplicative identity in the set of \n            matrices\n          (just like the real number 1 is the multiplicative identity in the set of real number).\n          In particular,\n            for any   matrix  .\n          Now we can generalize the inverse operation to matrices.\n          For an   matrix  ,\n          we define   to be the matrix which when multiplied by   gives \n          us the identity matrix.\n          In other words,  .\n          We can find the inverse of a matrix in a calculator by using the   button.\n         \n          For each of the following matrices,\n          determine if the inverse exists using your calculator or other appropriate technology.\n          If the inverse does exist,\n          write down the inverse and check that it satisfies the defining property of the \n          inverse matrix,\n          that is  .\n          If the inverse doesn't exist,\n          write down any error you received from the technology.\n          Can you guess why the inverse does not exist for these matrices?\n         \n                   \n               \n                   \n               \n                   \n               \n                   \n               \n                   \n               \n                  \n               \n          Now we turn to the question of how to find the inverse of a matrix in general.\n          With this approach,\n          we will be able to determine which matrices have inverses as well.\n          We will consider the   case to make the calculations easier.\n          Suppose   is a   matrix.\n          Our goal is to find a matrix   so that   and  .\n          If such a matrix exists,\n          we will call   the inverse,\n           , of  .\n         \n                What does the equation   tell us about the size of the \n                matrix  ?\n               \n                Now let  .\n                We want to find a  matrix   so that  .\n                Suppose   has columns   and  , i.e.\n                 .\n                Our definition of matrix multiplication shows that\n                 .\n               \n                      If  , what must   and   equal?\n                     \n                      Use the result from part (a) to set up two matrix equations to solve \n                      to find   and  .\n                      Then find   and  .\n                      As a result, find the matrix  .\n                     \n                      When we solve the two systems we have found a matrix   so that  .\n                      Is this enough to say that   is the inverse of  ?\n                      If not, what else do we need to know to verify that   is in fact  ?\n                      Verify that   is  .\n                     \n          A matrix inverse is extremely useful in solving matrix equations and can help us \n          in solving systems of equations.\n          Suppose that   is an invertible matrix,\n          i.e., there exists   such that  .\n         \n                Consider the system  .\n                Use the inverse of   to show that this system has a solution\n                for every   and find an expression for this solution in terms of \n                  and  .\n                (Note that since matrix multiplication is not commutative,\n                we have to pay attention to the order in which we multiply matrices.\n                For example,   while we cannot simplify\n                  to   unless   and   commute.)\n               \n                If  ,\n                 , and   are matrices and  ,\n                then we can subtract the matrix   from both sides to see that  .\n                We saw in  \n                that there is no corresponding general cancellation property for matrix multiplication when we found that   could hold while  .\n                However, we can cancel   from this equation in certain circumstances.\n                Suppose that   and that   is an invertible matrix.\n                Show that we can cancel   in this case and conclude that  .\n                (Note: When simplifying the product of matrices,\n                again keep in mind that matrix multiplication is not commutative.)\n               "
},
{
  "id": "p-1742",
  "level": "2",
  "url": "chap_matrix_inverse.html#p-1742",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "multiplicative inverses "
},
{
  "id": "definition-27",
  "level": "2",
  "url": "chap_matrix_inverse.html#definition-27",
  "type": "Definition",
  "number": "10.1",
  "title": "",
  "body": "matrix invertible matrix inverse invertible inverse "
},
{
  "id": "p-1748",
  "level": "2",
  "url": "chap_matrix_inverse.html#p-1748",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "non-singular singular "
},
{
  "id": "act_2_c_1",
  "level": "2",
  "url": "chap_matrix_inverse.html#act_2_c_1",
  "type": "Activity",
  "number": "10.2",
  "title": "",
  "body": "\n            Let  .\n            Calculate   where \n             .\n            Using your result,\n            explain why it is not possible to have  ,\n            showing that   is non-invertible.\n           \n            Calculate   where \n              and \n             .\n            Using your result, explain why the inverse of   doesn't exist.\n           "
},
{
  "id": "act_2_c_2",
  "level": "2",
  "url": "chap_matrix_inverse.html#act_2_c_2",
  "type": "Activity",
  "number": "10.3",
  "title": "",
  "body": "\n      Suppose that   is an invertible   matrix.\n      Hence we have an inverse matrix   for which  .\n      We will see how the inverse is useful in solving matrix equations involving  .\n     \n            Explain why the matrix expressions\n             \n            can all be simplified to  .\n           \n            Use the associative property of matrix multiplication.\n           \n            Suppose the system   has a solution.\n            Explain why then  .\n            What does this equation simplify to?\n           \n            Since we found one single expression for the solution   in \n            equation  ,\n            this implies that the equation has a unique solution.\n            What does this imply about the matrix  ?\n           "
},
{
  "id": "act_2_c_3",
  "level": "2",
  "url": "chap_matrix_inverse.html#act_2_c_3",
  "type": "Activity",
  "number": "10.4",
  "title": "",
  "body": "\n      Find the inverse of each matrix\n       using the method above , if it exists.\n      Compare the result with the inverse that you get from using appropriate technology \n      to directly calculate the inverse.\n     \n             \n           \n             \n           "
},
{
  "id": "p-1767",
  "level": "2",
  "url": "chap_matrix_inverse.html#p-1767",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "determinant of  "
},
{
  "id": "act_2_c_5",
  "level": "2",
  "url": "chap_matrix_inverse.html#act_2_c_5",
  "type": "Activity",
  "number": "10.5",
  "title": "",
  "body": "\n      Consider the following questions about matrix inverses.\n      If two   matrices   and   are invertible,\n      is the product   invertible?\n      If so, what is the inverse of  ?\n      We answer these questions in this activity.\n     \n            Let\n             .\n           \n                  Use formula   to find the inverses of   and  .\n                 \n                  Find the matrix product  .\n                  Is   invertible?\n                  If so, use formula   to find the inverse of  .\n                 \n                  Calculate the products   and  .\n                  What do you notice?\n                 \n            In part (a) we saw that the matrix product\n              was the inverse of the matrix product  .\n            Now we address the question of whether this is true in general.\n            Suppose now that   and   are invertible\n              matrices so that the matrix inverses   and \n              exist.\n           \n                  Use matrix algebra to simplify the matrix product \n                   .\n                  \n                 \n                  What do you know about   and  ?\n                 \n                  Simplify the matrix product\n                    in a manner similar to part i.\n                 \n                  What conclusion can we draw from parts i and ii?\n                  Explain.\n                  What property of matrix multiplication requires us to reverse the order of the product when we create the inverse of  ?\n                 "
},
{
  "id": "thm_inverse_properties",
  "level": "2",
  "url": "chap_matrix_inverse.html#thm_inverse_properties",
  "type": "Theorem",
  "number": "10.2",
  "title": "",
  "body": "\n        Let   and   be invertible   matrices.\n        Then\n         \n             \n               .\n             \n           \n             \n              The product   is invertible and  .\n             \n           \n             \n              The matrix   is invertible and\n               .\n             \n           \n       "
},
{
  "id": "example-19",
  "level": "2",
  "url": "chap_matrix_inverse.html#example-19",
  "type": "Example",
  "number": "10.3",
  "title": "",
  "body": "\n        For each of the following matrices  ,\n      \n         \n             \n              Use appropriate technology to find the reduced row \n              echelon form of  .\n             \n           \n             \n              Based on the result of part (a), is   invertible?\n              If yes, what is  ?\n              If no, explain why.\n             \n           \n             \n              Let   and  .\n              If   is invertible,\n              solve the matrix equation   using the inverse of  .\n              If   is not invertible,\n              find all solutions, if any,\n              to the equation   using whatever method you choose.\n             \n           \n       \n               \n             \n              With  ,\n              we have the following.\n\n             \n               \n                The reduced row echelon form of   is\n                 .\n               \n             \n               \n                Since   is row equivalent to  ,\n                    we conclude that   is invertible.\n                    The reduced row echelon form of   tells us that\n                     .\n                   \n                 \n                   \n                    The solution to   is given by\n                     .\n                   \n                 \n             \n               \n             \n              With  ,\n            we have the following.\n             \n                 \n                  The reduced row echelon form of   is\n                   .\n                 \n               \n                 \n                  Since   is not row equivalent to  ,\n                  we conclude that   is not invertible.\n                 \n               \n                 \n                  The reduced row echelon form of   is\n                   .\n                  The fact that the augmented column is a pivot column means that the equation \n                    has no solutions.\n                 \n               \n             "
},
{
  "id": "example-20",
  "level": "2",
  "url": "chap_matrix_inverse.html#example-20",
  "type": "Example",
  "number": "10.4",
  "title": "",
  "body": "\n              Let  .\n             \n                    Show that   but  .\n                   \n                    Let  .\n                   \n                    Using technology to calculate   and   we find that   while  .\n                   \n                    Show that   is invertible and find its inverse.\n                    Compare the inverse of   to  .\n                   \n                    Let  .\n                   \n                     For this matrix   we have  .\n                    The reduced row echelon form of   is\n                     ,\n                    so   is invertible and  .\n                    A straightforward matrix calculation also shows that\n                     .\n                   \n              Let   be an arbitrary square matrix such that  .\n              Show that   is invertible and find an inverse for  .\n             \n              We can try to emulate the result of part (a) here.\n              Expanding using matrix operations gives us\n               \n              and\n               .\n              So   is invertible and  .\n              This argument can be generalized to show that if   is a square matrix and   for some positive integer  ,\n              then   is invertible and\n               .\n             "
},
{
  "id": "ex_2_c_unique_inverse",
  "level": "2",
  "url": "chap_matrix_inverse.html#ex_2_c_unique_inverse",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Let   be an invertible   matrix.\n        In this exercise we will prove that the inverse of   is unique.\n        To do so, we assume that both   and   are inverses of  ,\n        that is   and  .\n        By considering the product   simplified in two different ways,\n        show that  , implying that the inverse of   is unique.\n       \n         \n       "
},
{
  "id": "ex_2_c_2by2_inverse",
  "level": "2",
  "url": "chap_matrix_inverse.html#ex_2_c_2by2_inverse",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Let   be an arbitrary   matrix.\n       \n              If   is invertible,\n              perform row operations to determine a row echelon form of  .\n              \n             \n              You may need to consider different cases,\n              e.g., when   and when  .\n             \n              Under certain conditions,\n              we can row reduce   to  , where\n               .\n              Use the row echelon form of   from part (a) to find conditions under which the\n                matrix   is invertible.\n              Then derive the formula for the inverse   of  .\n             "
},
{
  "id": "exercise-98",
  "level": "2",
  "url": "chap_matrix_inverse.html#exercise-98",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n            For a few different   values,\n            find the inverse of  .\n            From these results,\n            make a conjecture as to what   is in general.\n           \n             \n           \n            Prove your conjecture using the definition of inverse matrix.\n           \n            Row reduce  \n           \n            Find the inverse of  .\n           \n            Row reduce   to see that  .\n           \n      (Note: You can combine the first two parts above by applying the inverse finding algorithm directly on  .)\n       "
},
{
  "id": "exercise-99",
  "level": "2",
  "url": "chap_matrix_inverse.html#exercise-99",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Solve for the matrix   in terms of the others in the following equation:\n         \n        If you need to use an inverse, assume it exists.\n       "
},
{
  "id": "exercise-100",
  "level": "2",
  "url": "chap_matrix_inverse.html#exercise-100",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        For which   is the matrix   invertible?\n       \n         \n       "
},
{
  "id": "exercise-101",
  "level": "2",
  "url": "chap_matrix_inverse.html#exercise-101",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        For which   is the matrix   invertible?\n       "
},
{
  "id": "exercise-102",
  "level": "2",
  "url": "chap_matrix_inverse.html#exercise-102",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        Let   and   be invertible   matrices.\n        Verify the remaining properties of  .\n        That is, show that\n       \n               .\n             \n              The matrix   is invertible and  .\n             "
},
{
  "id": "exercise-103",
  "level": "2",
  "url": "chap_matrix_inverse.html#exercise-103",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If   is an invertible matrix,\n              then for any two matrices  ,\n                implies  .\n             \n              T\n             True\/False \n              If   is invertible,\n              then so is   for any matrix  .\n             True\/False \n              If   and   are invertible\n                matrices, then so is  .\n             \n              T\n             True\/False \n              If   is an invertible   matrix,\n              then the equation   is consistent for any   in  .\n             True\/False \n              If   is an invertible   matrix,\n              then the equation   has a unique solution when it is consistent.\n             \n              T\n             True\/False \n              If   is invertible, then so is  .\n             True\/False \n              If   is invertible,\n              then it reduces to the identity matrix.\n             \n              T\n             True\/False \n              If a matrix is invertible, then so is its transpose.\n             True\/False \n              If   and   are invertible   matrices,\n              then   is invertible.\n             \n              F\n             True\/False \n              If  ,\n              then   is invertible.\n             "
},
{
  "id": "p-1850",
  "level": "2",
  "url": "chap_matrix_inverse.html#p-1850",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "armament "
},
{
  "id": "act_arms_race_1",
  "level": "2",
  "url": "chap_matrix_inverse.html#act_arms_race_1",
  "type": "Project Activity",
  "number": "10.6",
  "title": "",
  "body": "\n      We continue to analyze a two nation scenario.\n      Let us suppose that our two nations are Iran\n      (nation  )\n      and Iraq\n      (nation  ).\n      In 1980, Iraq invaded Iran resulting in a long and brutal 8 year war.\n      Richardson was interested in analyzing data to see if such wars could be predicted by the changes in armaments of each nation.\n      We construct the two nation model in this activity.\n     fatigue factor \n            Taking the three effects discussed above into consideration,\n            explain why\n             .\n            Then explain why\n             .\n           \n            Write an equation similar to equation   that describes   in terms of the three effects.\n           \n            Let  .\n            Explain why\n             ,\n            where   and  .\n           "
},
{
  "id": "T_Expenditures",
  "level": "2",
  "url": "chap_matrix_inverse.html#T_Expenditures",
  "type": "Table",
  "number": "10.5",
  "title": "Military Expenditures of Iran and Iraq 1966-1975",
  "body": "Military Expenditures of Iran and Iraq 1966-1975 Year Iran Iraq "
},
{
  "id": "act_arms_race_2",
  "level": "2",
  "url": "chap_matrix_inverse.html#act_arms_race_2",
  "type": "Project Activity",
  "number": "10.7",
  "title": "",
  "body": "\n      In order to analyze a specific arms race between nations,\n      we need some data to determine values of the   and the  .\n       \n      shows the military expenditures of Iran and Iraq in the years leading up to their war in 1975. (The data is in millions of US dollars,\n      adjusted for inflation and is taken from ``World Military Expenditures and Arms Transfers 1966-1975\" by the U.S. Arms Control and Disarmament Agency.) We can perform regression\n      (we will see how in a later section)\n      on this data to obtain the following linear approximations:\n       \n     \n      (Of course, the data does not restrict itself to only factors between the two countries,\n      so our model will not be as precise as we might like.\n      However, it is a reasonable place to start.) Use the regression equations   and   to explain why\n       \n      for our Iran-Iraq arms race.\n     "
},
{
  "id": "project-33",
  "level": "2",
  "url": "chap_matrix_inverse.html#project-33",
  "type": "Project Activity",
  "number": "10.8",
  "title": "",
  "body": "equilibrium state \n      We can apply matrix algebra to find the equilibrium state vector   under certain conditions.\n     \n            Assuming that   exists,\n            use matrix algebra and  Equation  to show that\n             .\n           \n            Under what conditions can we be assured that there will always be a unique equilibrium state  ?\n            Explain.\n            Under these conditions, how can we find this unique equilibrium state?\n            Write this equilibrium state vector   as a matrix-vector product.\n           \n            Does the arms race model for Iran and Iraq have an equilibrium solution?\n            If so, find it.\n            If not, explain why not.\n            Use technology as appropriate.\n           \n            Assuming an equilibrium exists and that both nations behave in a way that supports the equilibrium,\n            explain what the appropriate entry of the equilibrium state vector   suggests about what Iran and Iraq's policies should be.\n            What does this model say about why there might have been war between these two nations?\n           "
},
{
  "id": "chap_IMT",
  "level": "1",
  "url": "chap_IMT.html",
  "type": "Section",
  "number": "11",
  "title": "The Invertible Matrix Theorem",
  "body": "The Invertible Matrix Theorem \n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What does it mean for two statements to be equivalent?\n           \n         \n           \n            How can we efficiently prove that a string of statements are all equivalent?\n           \n         \n           \n            What is the Invertible Matrix Theorem and why is it important?\n           \n         \n           \n            What are the equivalent conditions to a matrix being invertible?\n           \n         Introduction \n    This section is different than others in this book in that it contains only one long proof of the equivalence of statements that we have already discussed.\n    As such, this is a theoretical section and there is no application connected to it.\n   equivalent \n      Let   be an   matrix.\n      In this activity we endeavor to understand why the two statements\n       \n           \n            The matrix   is invertible.\n           \n         \n           \n            The matrix   is invertible.\n           \n         \n      are equivalent.\n      To demonstrate that statements I and II are equivalent,\n      we need to argue that if statement I is true,\n      then so is statement II, and if statement II is true then so is statement I.\n       \n            Let's first show that if statement I is true,\n            then so is statement II. So we assume statement I. That is,\n            we assume that   is an invertible matrix.\n            So we know that there is an\n              matrix   such that  ,\n            where   is the   identity matrix.\n            To demonstrate that statement II must also be true,\n            we need to verify that   is an invertible matrix.\n           \n                  What is  ?\n                 \n                  Take the transpose of both sides of the equation\n                    and use the properties of the transpose to write\n                    in terms of   and  .\n                 \n                  Take the transpose of both sides of the equation\n                    and use the properties of the transpose to write\n                    in terms of   and  .\n                 \n                  Explain how the previous two parts show that   is the inverse of  ,\n                  so that   is invertible.\n                  So we have shown that if statement I is true, so is statement II. \n                  Note that statement I does not have to be true.\n                  We are only assuming that IF statement I is true,\n                  then statement II must also be true.\n                   \n                 \n            Now we prove that if statement II is true,\n            then so is statement I. So we assume statement II. That is,\n            we assume that the matrix   is invertible.\n            We could do this in the same manner as part (a),\n            or we could be a bit clever.\n            Let's try to be clever.\n           \n                  What matrix is  ?\n                 \n                  Why can we use the result of part (a) with   in place of   to conclude that   is invertible?\n                  As a consequence,\n                  we have demonstrated that   is invertible if   is invertible.\n                  This concludes our argument that statements I and II are equivalent.\n                 The Invertible Matrix Theorem \n    We have been introduced to many statements about existence and uniqueness of solutions to systems of linear equations,\n    linear independence of columns of coefficient matrices,\n    onto linear transformations, and many other items.\n    In this section we will analyze these statements in light of how they are related to invertible matrices,\n    with the main goal to understand the Invertible Matrix Theorem.\n   \n    Recall that an   matrix   is invertible if there is an\n      matrix   such that  ,\n    where   is the   identity matrix.\n    The Invertible Matrix Theorem is an important theorem in that it provides us with a wealth of statements that are all equivalent to the statement that an\n      matrix   is invertible,\n    and connects many of the topics we have been discussing so far this semester into one big picture.\n   The Invertible Matrix Theorem Invertible Matrix Theorem    \n        Let   be an   matrix.\n        The following statements are equivalent:\n         \n             \n                is an invertible matrix.\n             \n           \n             \n              The equation   has only the trivial solution.\n             \n           \n             \n                has   pivot columns.\n             \n           \n             \n              The columns of   span  .\n             \n           \n             \n                is row equivalent to the identity matrix  .\n             \n           \n             \n              The columns of   are linearly independent.\n             \n           \n             \n              The columns of   form a basis for  .\n             \n           \n             \n              The matrix transformation   from   to   defined by   is one-to-one.\n             \n           \n             \n              The matrix equation   has exactly one solution for each vector   in  .\n             \n           \n             \n              The matrix transformation   from   to   defined by   is onto.\n             \n           \n             \n              There is an   matrix   so that  .\n             \n           \n             \n              There is an   matrix   so that  .\n             \n           \n             \n              The scalar 0 is not an eigenvalue of  .\n             \n           \n             \n                is invertible.\n             \n           \n       equivalent \n    The Invertible Matrix Theorem, however,\n    provides a long list of statements that are equivalent.\n    It would be inefficient to prove,\n    one by one, that each pair of statements is equivalent.\n    (There are   such pairs.)\n    Fortunately, there is a shorter method that we can use.\n   \n      In this activity,\n      we will consider certain parts of the Invertible Matrix Theorem and show that one implies another in a specific order.\n      For all parts in this activity,\n      we assume   is an   matrix.\n     \n            Consider the following implication:\n             : \n            The symbol   is the implication symbol,\n            so   is read to mean that statement   of the theorem implies statement  .\n               If the equation\n              has only the trivial solution,\n            then the columns of   are linearly independent.\n            This shows that part 2 of the IMT implies part 6 of the IMT. Justify this implication as if it is a T\/F problem.\n           \n            Justify the following implication:\n             : If the columns of   are linearly independent,\n            then the matrix equation   has exactly one solution for each vector   in  .\n           \n            Justify the following implication:\n             : If the equation\n              has exactly one solution for every vector   in  ,\n            then the columns of   span  .\n           \n            Justify the following implication:\n             : If the columns of   span  ,\n            then the equation   has only the trivial solution.\n           \n            Using the above implications you proved,\n            explain why we can conclude the following implication must also be true:\n             : If the equation\n              has only the trivial solution,\n            then the matrix equation   has exactly one solution for each vector   in  .\n           \n            Using the above implications you proved,\n            explain why any one of the implications  ,  ,\n             , and   implies any of the others.\n           \n    Using a similar ordering of circular implications as in  ,\n    we can prove the Invertible Matrix Theorem by showing that each statement in the list implies the next statement,\n    and that the last statement implies the first.\n   Proof of the Invertible Matrix Theorem \n     \n         Statement (1) implies Statement (2) \n         \n          This follows from work done in  .\n         \n       \n         Statement (2) implies Statement (3) \n         \n          This was done in  .\n         \n       \n         Statement (3) implies Statement (4) \n         \n          Suppose that every column of   is a pivot column.\n          The fact that   is square means that every row of   contains a pivot,\n          and hence the columns of   span  .\n         \n       \n         Statement (4) implies Statement (5) \n         \n          Since the columns of   span  ,\n          it must be the case that every row of   contains a pivot.\n          This means that   must be row equivalent to  .\n         \n       \n         Statement (5) implies Statement (6) \n         \n          If   is row equivalent to  ,\n          there must be a pivot in every column,\n          which means that the columns of   are linearly independent.\n         \n       \n         Statement (6) implies Statement (7) \n         \n          If the columns of   are linearly independent,\n          then there is a pivot in every column.\n          Since   is a square matrix,\n          there is a pivot in every row as well.\n          So the columns of   span  .\n          Since they are also linearly independent,\n          the columns form a minimal spanning set,\n          which is a basis of  .\n         \n       \n         Statement (7) implies Statement (8) \n         \n          If the columns form a basis of  ,\n          then the columns are linearly independent.\n          This means that each column is a pivot column,\n          which also implies   has a unique solution and that   is one-to-one.\n         \n       \n         Statement (8) implies Statement (9) \n         \n          If   is one-to-one, then   has a pivot in every column.\n          Since   is square, every row of   contains a pivot.\n          Therefore, the system   is consistent for every\n            and has a unique solution.\n         \n       \n         Statement (9) implies Statement (10) \n         \n          If   has a unique solution for every  ,\n          then the transformation   is onto since\n            has a solution for every  .\n         \n       \n         Statement (10) implies Statement (11) \n         \n          Assume that   defined by   is onto.\n          For each  , let   be the  th column of the\n            identity matrix  .\n          That is,   is the vector in   with 1 in the  th component and 0 everywhere else.\n          Since   is onto,\n          for each   there is a vector   such that  .\n          Let  .\n          Then\n           .\n         \n       \n         Statement (11) implies Statement (12) \n         \n          Assume   is an   matrix so that  .\n          First we show that the matrix equation\n            has only the trivial solution.\n          Suppose  .\n          Then multiplying both sides on the left by   gives us\n           .\n          Simplifying this equation using  ,\n          we find  .\n          Since   has only the trivial solution,\n          every column of   must be a pivot column.\n          Since   is an   matrix,\n          it follows that every row of   contains a pivot position.\n          Thus, the matrix equation   is consistent and has a unique solution for every   in  .\n          Let   be the vector in   satisfying\n            for each   between 1 and   and let  .\n          Then  .\n          Now we show that  .\n          Since\n           \n          we can multiply both sides on the left by   to see that\n           .\n          Now we multiply both sides on the right by   and obtain\n           .\n          Using the associative property of matrix multiplication and the fact that   shows that\n           .\n          Thus, if   and   are   matrices and\n           , then  .\n          So we have proved our implication with  \n         \n       \n         Statement (12) implies Statement (13) \n         \n          Assume that there is an   matrix   so that  .\n          Suppose  .\n          Then multiplying both sides by   on the left, we find that\n           .\n          So the equation   has only the trivial solution and   is not an eigenvalue for  .\n         \n       \n         Statement (13) implies Statement (14) \n         \n          If 0 is not an eigenvalue of  ,\n          then the equation   has only the trivial solution.\n          Since  statement \n          implies  statement ,\n          there is an   matrix   such that  .\n          The proof that  statement \n          implies  statement \n          shows that   as well.\n          So   is invertible.\n          By taking the transpose of both sides of the equation\n            (remembering  ) we find\n           .\n          Therefore,   is the inverse of   by definition of the inverse.\n         \n       \n         Statement (14) implies Statement (1) \n         \n          Since statement (1) implies statement (14),\n          we proved  If   is invertible,\n          then   is invertible. ' Using this implication with   replaced by  ,\n          we find that  If   is invertible,\n          then   is invertible. ' But  ,\n          which proves that statement (14) implies statement (1).\n         \n       \n   \n    This concludes our proof of the Invertible Matrix Theorem.\n   Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Let  .\n       \n              Without doing any calculations, is   invertible?\n              Explain your response.\n             \n              The third column of   is twice the first,\n              so the columns of   are not linearly independent.\n              We conclude that   is not invertible.\n             \n              Is the equation   consistent for every   in  ?\n              Explain.\n             \n              The equation   is not consistent for every   in  .\n              If it was, then the columns of   would span   and,\n              since there are exactly four columns,\n              the columns of   would be a basis for  .\n              Thus,   would have to be invertible, which it is not.\n             \n              Is the equation   consistent?\n              If so, how many solutions does this equation have?\n              Explain.\n             \n              The homogeneous system is always consistent.\n              Since the columns of   are linearly dependent,\n              the equation   has infinitely many solutions.\n             \n              Is it possible to find a   matrix   such that  ?\n              Explain.\n             \n              It is not possible to find a\n                matrix   such that  .\n              Otherwise   would have to be invertible.\n             \n        Let   be an   matrix whose eigenvalues are all nonzero.\n       \n              Let  .\n              Is the equation   consistent?\n              If yes, explain why and find all solutions in terms of   and  .\n              If no, explain why.\n             \n              Since   is not an eigenvalue of  ,\n              we know that   is invertible.\n              Therefore, the equation   has the unique solution  .\n             \n              Let   be the matrix transformation defined by  .\n              Suppose   for some vectors   and   in  .\n              Must there be any relationship between   and  ?\n              If yes, explain the relationship.\n              If no, explain why.\n             \n              The fact that   is invertible implies that   is one-to-one.\n              So if  ,\n              then it must be the case that  .\n             \n              Let  ,  ,\n               ,   be the columns of  .\n              In how many ways can we write the zero vector as a linear combination of  ,\n               ,  ,  ?\n              Explain.\n             \n              Because   is invertible,\n              the columns of   are linearly independent.\n              Therefore, there is only the trivial solution to the equation\n               .\n             Summary \n       \n        Two statements are equivalent if,\n        whenever one of the statements is true,\n        then the other must also be true.\n       \n     \n       \n        To efficiently prove that a string of statements are all equivalent,\n        we can prove that each statement in the list implies the next statement,\n        and that the last statement implies the first.\n       \n     \n       \n        The Invertible Matrix Theorem gives us many conditions that are equivalent to an\n          matrix being invertible.\n        This theorem is important because it connects many of the concepts we have been studying.\n       \n     \n        Consider the matrix  .\n        Use the Invertible Matrix Theorem to geometrically describe the vectors\n          which make   invertible without doing any calculations.\n       \n          is invertible as long as\n          is \n        not in the plane in   through the origin and the points   \n        and  \n       \n        Suppose   is an invertible   matrix.\n        Let   be the matrix transformation defined by\n          for   in  .\n        Show that the matrix transformation   defined by\n          is the inverse of the transformation   (i.e.,   is the inverse function to   when the transformations are considered as functions).\n       \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If   is invertible,\n              then   is invertible.\n             \n              T\n             True\/False \n              If   and   are square matrices with   invertible,\n              then   and   are invertible.\n             True\/False \n              If the columns of an\n                matrix   span  ,\n              then the equation   has a unique solution.\n             \n              T\n             True\/False \n              If the columns of   and columns of   form a basis of  ,\n              then so do the columns of  .\n             True\/False \n              If the columns of a matrix   form a basis of  ,\n              then so do the rows of  .\n             \n              T\n             True\/False \n              If the matrix transformation   defined by\n                is one-to-one for an   matrix  ,\n              then the columns of   are linearly independent.\n             True\/False \n              If the columns of an\n                matrix   span  ,\n              then so do the rows of  .\n             \n              T\n             True\/False \n              If there are two\n                matrices   and   such that  ,\n              then the matrix transformation defined by   is one-to-one.\n             "
},
{
  "id": "objectives-11",
  "level": "2",
  "url": "chap_IMT.html#objectives-11",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What does it mean for two statements to be equivalent?\n           \n         \n           \n            How can we efficiently prove that a string of statements are all equivalent?\n           \n         \n           \n            What is the Invertible Matrix Theorem and why is it important?\n           \n         \n           \n            What are the equivalent conditions to a matrix being invertible?\n           \n         "
},
{
  "id": "p-1874",
  "level": "2",
  "url": "chap_IMT.html#p-1874",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "equivalent "
},
{
  "id": "pa_2_d",
  "level": "2",
  "url": "chap_IMT.html#pa_2_d",
  "type": "Preview Activity",
  "number": "11.1",
  "title": "",
  "body": "\n      Let   be an   matrix.\n      In this activity we endeavor to understand why the two statements\n       \n           \n            The matrix   is invertible.\n           \n         \n           \n            The matrix   is invertible.\n           \n         \n      are equivalent.\n      To demonstrate that statements I and II are equivalent,\n      we need to argue that if statement I is true,\n      then so is statement II, and if statement II is true then so is statement I.\n       \n            Let's first show that if statement I is true,\n            then so is statement II. So we assume statement I. That is,\n            we assume that   is an invertible matrix.\n            So we know that there is an\n              matrix   such that  ,\n            where   is the   identity matrix.\n            To demonstrate that statement II must also be true,\n            we need to verify that   is an invertible matrix.\n           \n                  What is  ?\n                 \n                  Take the transpose of both sides of the equation\n                    and use the properties of the transpose to write\n                    in terms of   and  .\n                 \n                  Take the transpose of both sides of the equation\n                    and use the properties of the transpose to write\n                    in terms of   and  .\n                 \n                  Explain how the previous two parts show that   is the inverse of  ,\n                  so that   is invertible.\n                  So we have shown that if statement I is true, so is statement II. \n                  Note that statement I does not have to be true.\n                  We are only assuming that IF statement I is true,\n                  then statement II must also be true.\n                   \n                 \n            Now we prove that if statement II is true,\n            then so is statement I. So we assume statement II. That is,\n            we assume that the matrix   is invertible.\n            We could do this in the same manner as part (a),\n            or we could be a bit clever.\n            Let's try to be clever.\n           \n                  What matrix is  ?\n                 \n                  Why can we use the result of part (a) with   in place of   to conclude that   is invertible?\n                  As a consequence,\n                  we have demonstrated that   is invertible if   is invertible.\n                  This concludes our argument that statements I and II are equivalent.\n                 "
},
{
  "id": "theorem-23",
  "level": "2",
  "url": "chap_IMT.html#theorem-23",
  "type": "Theorem",
  "number": "11.1",
  "title": "The Invertible Matrix Theorem.",
  "body": "The Invertible Matrix Theorem Invertible Matrix Theorem    \n        Let   be an   matrix.\n        The following statements are equivalent:\n         \n             \n                is an invertible matrix.\n             \n           \n             \n              The equation   has only the trivial solution.\n             \n           \n             \n                has   pivot columns.\n             \n           \n             \n              The columns of   span  .\n             \n           \n             \n                is row equivalent to the identity matrix  .\n             \n           \n             \n              The columns of   are linearly independent.\n             \n           \n             \n              The columns of   form a basis for  .\n             \n           \n             \n              The matrix transformation   from   to   defined by   is one-to-one.\n             \n           \n             \n              The matrix equation   has exactly one solution for each vector   in  .\n             \n           \n             \n              The matrix transformation   from   to   defined by   is onto.\n             \n           \n             \n              There is an   matrix   so that  .\n             \n           \n             \n              There is an   matrix   so that  .\n             \n           \n             \n              The scalar 0 is not an eigenvalue of  .\n             \n           \n             \n                is invertible.\n             \n           \n       "
},
{
  "id": "p-1903",
  "level": "2",
  "url": "chap_IMT.html#p-1903",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "equivalent "
},
{
  "id": "act_2_d_1",
  "level": "2",
  "url": "chap_IMT.html#act_2_d_1",
  "type": "Activity",
  "number": "11.2",
  "title": "",
  "body": "\n      In this activity,\n      we will consider certain parts of the Invertible Matrix Theorem and show that one implies another in a specific order.\n      For all parts in this activity,\n      we assume   is an   matrix.\n     \n            Consider the following implication:\n             : \n            The symbol   is the implication symbol,\n            so   is read to mean that statement   of the theorem implies statement  .\n               If the equation\n              has only the trivial solution,\n            then the columns of   are linearly independent.\n            This shows that part 2 of the IMT implies part 6 of the IMT. Justify this implication as if it is a T\/F problem.\n           \n            Justify the following implication:\n             : If the columns of   are linearly independent,\n            then the matrix equation   has exactly one solution for each vector   in  .\n           \n            Justify the following implication:\n             : If the equation\n              has exactly one solution for every vector   in  ,\n            then the columns of   span  .\n           \n            Justify the following implication:\n             : If the columns of   span  ,\n            then the equation   has only the trivial solution.\n           \n            Using the above implications you proved,\n            explain why we can conclude the following implication must also be true:\n             : If the equation\n              has only the trivial solution,\n            then the matrix equation   has exactly one solution for each vector   in  .\n           \n            Using the above implications you proved,\n            explain why any one of the implications  ,  ,\n             , and   implies any of the others.\n           "
},
{
  "id": "proof-4",
  "level": "2",
  "url": "chap_IMT.html#proof-4",
  "type": "Proof",
  "number": "1",
  "title": "Proof of the Invertible Matrix Theorem.",
  "body": "Proof of the Invertible Matrix Theorem \n     \n         Statement (1) implies Statement (2) \n         \n          This follows from work done in  .\n         \n       \n         Statement (2) implies Statement (3) \n         \n          This was done in  .\n         \n       \n         Statement (3) implies Statement (4) \n         \n          Suppose that every column of   is a pivot column.\n          The fact that   is square means that every row of   contains a pivot,\n          and hence the columns of   span  .\n         \n       \n         Statement (4) implies Statement (5) \n         \n          Since the columns of   span  ,\n          it must be the case that every row of   contains a pivot.\n          This means that   must be row equivalent to  .\n         \n       \n         Statement (5) implies Statement (6) \n         \n          If   is row equivalent to  ,\n          there must be a pivot in every column,\n          which means that the columns of   are linearly independent.\n         \n       \n         Statement (6) implies Statement (7) \n         \n          If the columns of   are linearly independent,\n          then there is a pivot in every column.\n          Since   is a square matrix,\n          there is a pivot in every row as well.\n          So the columns of   span  .\n          Since they are also linearly independent,\n          the columns form a minimal spanning set,\n          which is a basis of  .\n         \n       \n         Statement (7) implies Statement (8) \n         \n          If the columns form a basis of  ,\n          then the columns are linearly independent.\n          This means that each column is a pivot column,\n          which also implies   has a unique solution and that   is one-to-one.\n         \n       \n         Statement (8) implies Statement (9) \n         \n          If   is one-to-one, then   has a pivot in every column.\n          Since   is square, every row of   contains a pivot.\n          Therefore, the system   is consistent for every\n            and has a unique solution.\n         \n       \n         Statement (9) implies Statement (10) \n         \n          If   has a unique solution for every  ,\n          then the transformation   is onto since\n            has a solution for every  .\n         \n       \n         Statement (10) implies Statement (11) \n         \n          Assume that   defined by   is onto.\n          For each  , let   be the  th column of the\n            identity matrix  .\n          That is,   is the vector in   with 1 in the  th component and 0 everywhere else.\n          Since   is onto,\n          for each   there is a vector   such that  .\n          Let  .\n          Then\n           .\n         \n       \n         Statement (11) implies Statement (12) \n         \n          Assume   is an   matrix so that  .\n          First we show that the matrix equation\n            has only the trivial solution.\n          Suppose  .\n          Then multiplying both sides on the left by   gives us\n           .\n          Simplifying this equation using  ,\n          we find  .\n          Since   has only the trivial solution,\n          every column of   must be a pivot column.\n          Since   is an   matrix,\n          it follows that every row of   contains a pivot position.\n          Thus, the matrix equation   is consistent and has a unique solution for every   in  .\n          Let   be the vector in   satisfying\n            for each   between 1 and   and let  .\n          Then  .\n          Now we show that  .\n          Since\n           \n          we can multiply both sides on the left by   to see that\n           .\n          Now we multiply both sides on the right by   and obtain\n           .\n          Using the associative property of matrix multiplication and the fact that   shows that\n           .\n          Thus, if   and   are   matrices and\n           , then  .\n          So we have proved our implication with  \n         \n       \n         Statement (12) implies Statement (13) \n         \n          Assume that there is an   matrix   so that  .\n          Suppose  .\n          Then multiplying both sides by   on the left, we find that\n           .\n          So the equation   has only the trivial solution and   is not an eigenvalue for  .\n         \n       \n         Statement (13) implies Statement (14) \n         \n          If 0 is not an eigenvalue of  ,\n          then the equation   has only the trivial solution.\n          Since  statement \n          implies  statement ,\n          there is an   matrix   such that  .\n          The proof that  statement \n          implies  statement \n          shows that   as well.\n          So   is invertible.\n          By taking the transpose of both sides of the equation\n            (remembering  ) we find\n           .\n          Therefore,   is the inverse of   by definition of the inverse.\n         \n       \n         Statement (14) implies Statement (1) \n         \n          Since statement (1) implies statement (14),\n          we proved  If   is invertible,\n          then   is invertible. ' Using this implication with   replaced by  ,\n          we find that  If   is invertible,\n          then   is invertible. ' But  ,\n          which proves that statement (14) implies statement (1).\n         \n       \n   "
},
{
  "id": "example-21",
  "level": "2",
  "url": "chap_IMT.html#example-21",
  "type": "Example",
  "number": "11.2",
  "title": "",
  "body": "\n        Let  .\n       \n              Without doing any calculations, is   invertible?\n              Explain your response.\n             \n              The third column of   is twice the first,\n              so the columns of   are not linearly independent.\n              We conclude that   is not invertible.\n             \n              Is the equation   consistent for every   in  ?\n              Explain.\n             \n              The equation   is not consistent for every   in  .\n              If it was, then the columns of   would span   and,\n              since there are exactly four columns,\n              the columns of   would be a basis for  .\n              Thus,   would have to be invertible, which it is not.\n             \n              Is the equation   consistent?\n              If so, how many solutions does this equation have?\n              Explain.\n             \n              The homogeneous system is always consistent.\n              Since the columns of   are linearly dependent,\n              the equation   has infinitely many solutions.\n             \n              Is it possible to find a   matrix   such that  ?\n              Explain.\n             \n              It is not possible to find a\n                matrix   such that  .\n              Otherwise   would have to be invertible.\n             "
},
{
  "id": "example-22",
  "level": "2",
  "url": "chap_IMT.html#example-22",
  "type": "Example",
  "number": "11.3",
  "title": "",
  "body": "\n        Let   be an   matrix whose eigenvalues are all nonzero.\n       \n              Let  .\n              Is the equation   consistent?\n              If yes, explain why and find all solutions in terms of   and  .\n              If no, explain why.\n             \n              Since   is not an eigenvalue of  ,\n              we know that   is invertible.\n              Therefore, the equation   has the unique solution  .\n             \n              Let   be the matrix transformation defined by  .\n              Suppose   for some vectors   and   in  .\n              Must there be any relationship between   and  ?\n              If yes, explain the relationship.\n              If no, explain why.\n             \n              The fact that   is invertible implies that   is one-to-one.\n              So if  ,\n              then it must be the case that  .\n             \n              Let  ,  ,\n               ,   be the columns of  .\n              In how many ways can we write the zero vector as a linear combination of  ,\n               ,  ,  ?\n              Explain.\n             \n              Because   is invertible,\n              the columns of   are linearly independent.\n              Therefore, there is only the trivial solution to the equation\n               .\n             "
},
{
  "id": "exercise-104",
  "level": "2",
  "url": "chap_IMT.html#exercise-104",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Consider the matrix  .\n        Use the Invertible Matrix Theorem to geometrically describe the vectors\n          which make   invertible without doing any calculations.\n       \n          is invertible as long as\n          is \n        not in the plane in   through the origin and the points   \n        and  \n       "
},
{
  "id": "exercise-105",
  "level": "2",
  "url": "chap_IMT.html#exercise-105",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Suppose   is an invertible   matrix.\n        Let   be the matrix transformation defined by\n          for   in  .\n        Show that the matrix transformation   defined by\n          is the inverse of the transformation   (i.e.,   is the inverse function to   when the transformations are considered as functions).\n       "
},
{
  "id": "exercise-106",
  "level": "2",
  "url": "chap_IMT.html#exercise-106",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If   is invertible,\n              then   is invertible.\n             \n              T\n             True\/False \n              If   and   are square matrices with   invertible,\n              then   and   are invertible.\n             True\/False \n              If the columns of an\n                matrix   span  ,\n              then the equation   has a unique solution.\n             \n              T\n             True\/False \n              If the columns of   and columns of   form a basis of  ,\n              then so do the columns of  .\n             True\/False \n              If the columns of a matrix   form a basis of  ,\n              then so do the rows of  .\n             \n              T\n             True\/False \n              If the matrix transformation   defined by\n                is one-to-one for an   matrix  ,\n              then the columns of   are linearly independent.\n             True\/False \n              If the columns of an\n                matrix   span  ,\n              then so do the rows of  .\n             \n              T\n             True\/False \n              If there are two\n                matrices   and   such that  ,\n              then the matrix transformation defined by   is one-to-one.\n             "
},
{
  "id": "chap_R_n",
  "level": "1",
  "url": "chap_R_n.html",
  "type": "Section",
  "number": "12",
  "title": "The Structure of <span class=\"process-math\">\\(\\R^n\\)<\/span>",
  "body": "The Structure of  \n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What properties make   a vector space?\n           \n         \n           \n            What is a subspace of  ?\n           \n         \n           \n            What properties do we need to verify to show that a set of vectors is a subspace of  ?\n            Why?\n           \n         \n           \n            What important structure does the span of a set of vectors in   have?\n           \n         Application: Connecting GDP and Consumption in Romania least squares consumption \n    As we can see from the scatterplot,\n    the relationship between the GDP and consumption is not exactly linear,\n    but looks to be very close.\n    To make correlations between GDP and consumption as the authors did,\n    we need to understand how they determined their approximate linear relationship between the variables.\n    With a good approximation function we can then compare the variables,\n    extrapolate from the data,\n    and make predictions or interpolate and estimate between data points.\n    For example,\n    we could use our approximation function to predict,\n    as the authors did, how changes in consumption affect GDP\n    (or vice versa).\n    Later in this section we will see how to find the least squares line to fit this data   the best linear approximation to the data.\n    This involves finding a vector in a certain subspace of   that is closest to a given vector.\n    Linear least squares approximation is a special case of a more general process that we will encounter in later sections where we learn how to project sets onto subspaces.\n   Introduction vector space subspaces \n    Recall that the span of a set of vectors\n      in   is the set of all linear combinations of the vectors.\n    For example,\n    if   and  ,\n    then a linear combination of these two vectors is of the form\n     .\n   \n    One linear combination can be obtained by letting  ,\n    which gives the vector  .\n    All such linear combinations form the span of the vectors   and  .\n    In this case,\n    these vectors will form a plane through the origin in  .\n   \n    Now we will investigate if the span of two vectors form a subspace,\n    i.e. if it has the same structure as a vector space.\n   \n      Let   and   be two vectors in  .\n      Let  .\n     \n            For   to be a subspace of  ,\n            the sum of any two vectors in   must also be in  .\n           \n                  Pick two specific examples of vectors   in  \n                  (keeping   unknown\/general vectors).\n                  For example, one specific   would be\n                    as we used in the above example.\n                  Find the sum of  .\n                  Is the sum also in  ?\n                  Explain.\n                  \n                 \n                  What does it mean for a vector to be in  ?\n                 \n                  Now let   and   be arbitrary vectors in  .\n                  Explain why   is in  .\n                 \n            For   to be a subspace of  ,\n            any scalar multiple of any vector in   must also be in  .\n           \n                  Pick a specific example   in  .\n                  Explain why   are all also in  .\n                 \n                  Now let   be an arbitrary scalar and let   be an arbitrary vector in  .\n                  Explain why the vector   is in  .\n                 \n            For   to be a subspace of  ,\n            the zero vector must also be in  .\n            Explain why the zero vector is in  .\n           \n            Does vector addition being commutative for vectors in   imply that vector addition is also commutative for vectors in  ?\n            Explain your reasoning.\n           \n            Suppose we have an arbitrary   in  .\n            There is an additive inverse of   in  .\n            In other words,\n            there is a   such that  .\n            Should this   be also in  ?\n            If so, explain why.\n            If not, give a counterexample.\n           \n            Look at the other properties of vector addition and scalar multiplication of vectors in   listed in  \n            in  .\n            Which of these properties should also hold for vectors in  ?\n           Vector Spaces vector space vector space vector space closed commutative associative additive identity zero vector additive inverse closed multiplication by scalars distributes over scalar addition multiplication by scalars distributes over addition in  subspaces subspace of  subspace \n    The following example illustrates the process for demonstrating that a subset of   is a subspace of  .\n   \n        There are many subsets of   that are themselves vector spaces.\n        Consider as an example the set   of vectors in   defined by\n         .\n       \n        In other words,\n          is the set of vectors in   whose second component is 0.\n        To see that   is itself a vector space,\n        we need to demonstrate that   satisfies all of the properties listed in  .\n       arbitrary \n        Since the second component of\n          is 0, it follows that   is in  .\n        Thus, the set   is closed under addition.\n       \n        For the second property, that addition is commutative in  ,\n        we can just use the fact that if   and   are in  ,\n        they are also vectors in   and\n          is satisfied in  .\n        So the property also holds in  .\n       \n        A similar argument can be made for property (3).\n       \n        Property (4) states the existence of the additive identity in  .\n        Note that   is an additive identity in   and if it is also an element in  ,\n        then it will automatically be the additive identity of  .\n        Since the zero vector can be written as   with  ,\n          is in  .\n        Thus,   satisfies property 4.\n       \n        We will postpone property (5) for a bit since we can show that other properties imply property (5).\n       \n        Property (6) is a closure property, just like property (1).\n        We need to verify that  any \n        scalar multiple of  any \n        vector in   is again in  .\n        Consider an arbitrary vector   and an arbitrary scalar  .\n        Now\n         .\n       \n        Since the vector   has a 0 as its second component,\n        we see that   is in  .\n        Thus,   is closed under scalar multiplication.\n       \n        Properties (7), (8), (9) and (10) only depend on the operations of addition and multiplication by scalars in  .\n        Since these properties depend on the operations and not the vectors,\n        these properties will transfer to  .\n       \n        We still have to justify property (5) though.\n        Note that since   in real numbers,\n        by applying property (7) with  ,  ,\n        we find that\n         .\n       \n        Therefore,   is an additive inverse for  .\n        Therefore, to show that the additive inverse of any   in   is also in  ,\n        we simply note that any multiple of   is also in   and hence   must also be in  .\n       subspace \n     \n    and our work  \n    bring out some important ideas.\n    When checking that a subset   of a vector space   is also a vector space,\n    we can use the fact that all of the properties of the operations in   are transferred to any closed subset  .\n    This implies that properties (2), (3), (7)-(10) are all automatically satisfied for   as well.\n    Property (5) follows from the others.\n    So we only need to check properties (1), (4) and (6).\n    In fact, as we argued in the above example,\n    property (4) also needs to be checked by simply checking that   of   is in  .\n    We summarize this result in the following theorem.\n   closed closed \n    The next activity provides some practice using  .\n   \n      Use  \n      to answer the following questions.\n      Justify your responses.\n      For sets which lie inside  ,\n      sketch a pictorial representation of the set and explain why your picture confirms your answer.\n     \n            Is the set   a subspace of  ?\n           \n            Is the set   a subspace of  ?\n           \n            Is the set   a subspace of  ?\n           \n            Is the set   a subspace of  ?\n           \n            Is the set   a subspace of  ?\n           \n            Is the set   a subspace of  ?\n           \n            Is the set   a subspace of  ?\n            Note that   is the unit sphere (a.k.a. unit ball) in  .\n           \n            Is the set   a subspace of  ?\n           \n    There are several important points that we can glean from  .\n     \n         \n          A subspace is a vector space within a larger vector space,\n          similar to a subset being a set within a larger set.\n         \n       \n         \n          The set containing the zero vector in   is a subspace of  ,\n          and it is the only finite subspace of  .\n         \n       \n         \n          Every subspace of   must contain the zero vector.\n         \n       \n         \n          No nonzero subspace is bounded   since a subspace must include all scalar multiples of its vectors,\n          a subspace cannot be contained in a finite sphere or box.\n         \n       \n         \n          Since vectors in   have   components,\n          vectors in   are not contained in   when  .\n          However, if  ,\n          then we can think of   as containing a  copy \n          (what we call an isomorphic image)\n          of   as the set of vectors with zeros as the last   components.\n         \n       \n   The Subspace Spanned by a Set of Vectors \n    One of the most convenient ways to represent a subspace of   is as the span of a set of vectors.\n    In  \n    we saw that the span of two vectors is a subspace of  .\n    In the next theorem we verify this result for the span of an arbitrary number of vectors,\n    extending the ideas you used in  .\n    Expressing a set of vectors as the span of some number of vectors is a quick way of justifying that this set is a subspace and it also provides us a geometric intuition for the set of vectors.\n   \n        Let  ,  ,\n         ,   be vectors in  .\n        Then   is a subspace of  .\n       \n      Let  ,  ,\n       ,   be vectors in  .\n      Let  .\n      To show that   is a subspace of   we need to show that   is closed under addition and multiplication by scalars and that   is in  .\n     \n      First we show that   is closed under addition.\n      Let   and   be vectors in  .\n      This means that   and   are linear combinations of  ,\n       ,  ,  .\n      So there are scalars  ,\n       ,  ,\n        and  ,  ,\n       ,   so that\n       .\n     \n      To demonstrate that   is in  ,\n      we need to show that   is a linear combination of  ,\n       ,  ,  .\n      Using the properties of vector addition and scalar multiplication, we find\n       .\n     \n      Thus   is a linear combination of  ,\n       ,  ,\n        and   is closed under vector addition.\n     \n      Next we show that   is closed under scalar multiplication.\n      Let   be in   and   be a scalar.\n      Then\n       \n      and   is a linear combination of  ,\n       ,  ,\n        and   is closed under multiplication by scalars.\n     \n      Finally, we show that   is in  .\n      Since\n       ,\n     \n        is in  .\n     \n      Since   satisfies all of the properties of a subspace as given in definition of a subspace,\n      we conclude that   is a subspace of  .\n     subspace of   spanned by  \n            Describe geometrically as best as you can the subspaces of   spanned by the following sets of vectors.\n               \n           \n            Express the following set of vectors as the span of some vectors to show that this set is a subspace.\n            Can you give a geometric description of the set?\n             \n           \n    One additional conclusion we can draw from  \n    and  \n    is that subspaces of   are made up of\n     flat \n    subsets.\n    The span of a single nonzero vector is a line\n    (which is flat),\n    and the span of a set of two distinct nonzero vectors is a plane\n    (which is also flat).\n    So subspaces of   are linear (or\n     flat ) subsets of  .\n    That is why we can recognize that the non-flat parabola in  \n    is not a subspace of  .\n   Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Let  .\n       \n              Show that   is a subspace of  .\n             \n              Every vector in   has the form\n               \n              for some real numbers  ,  , and  .\n              Thus,\n               .\n              As a span of a set of vectors,\n              we know that   is a subspace of  .\n             \n              Describe in detail the geometry of the subspace   (e.g., is it a line,\n              a union of lines, a plane, a union of planes, etc.)\n             \n              Let  ,\n               ,\n              and  .\n              The reduced row echelon form of\n                is  .\n              The pivot columns of   form a linearly independent set with the same span as  , So\n                and   forms the plane in   through the origin and the points   and  .\n             \n              Let   and let  .\n              That is,   is the  -axis and   the  -axis in three-space.\n              Let\n               .\n             \n                    Is   in  ?\n                    Justify your answer.\n                   \n                    We let   and  . T\n                   \n                    Let  ,\n                     ,\n                    and  .\n                    Since   with   and\n                      we conclude that  .\n                   \n                    Is   in  ?\n                    Justify your answer.\n                   \n                    We let   and  . T\n                   \n                    Every vector in   has the form   for some scalar   (where  ,\n                    and every vector in   has the form   for some scalar  \n                    (where  ).\n                    So every vector in   is of the form  .\n                    Since the vector   does not have a   in the third component,\n                    we conclude that in   is not in  .\n                   \n                    Assume that   is a subspace of  .\n                    Describe in detail the geometry of this subspace.\n                   \n                    We let   and  . T\n                   \n                    As we just argued,\n                    every vector in   has the form  .\n                    So  ,\n                    which is the  -plane in  .\n                   subspace sum sum \n              To see why the set   is a subspace of  ,\n              suppose that   and   are in  .\n              Then   and   for some\n                in   and some   in  .\n              Then\n               .\n              Since   is a subspace of   it follows that  .\n              Similarly,  .\n              This makes   an element of  .\n              Also, suppose that   is a scalar.\n              Then\n               .\n              Since   is a subspace of   it follows that  .\n              Similarly,  .\n              This makes   an element of  .\n              Finally, since   is in both   and  ,\n              and  ,\n              it follows that   is an element of  .\n              We conclude that   is a subspace of  .\n             Summary \n       closed commutative associative additive identity zero vector additive inverse closed multiplication by scalars distributes over scalar addition multiplication by scalars distributes over addition in  \n     \n       \n        For every  ,   is a vector space.\n       \n     \n       \n        A subset   of   is a subspace of   if   is a vector space using the same operations as in  .\n       \n     \n       closed closed \n     \n       \n        The span of any set of vectors in   is a subspace of  .\n       \n     \n        Each of the following regions or graphs determines a subset   of  .\n        For each region,\n        discuss each of the subspace properties of Theorem 12.4 and explain with justification if the set   satisfies each property or not.\n       \n           \n         \n          Closed under addition,\n          not closed under multiplication by scalars, contains the zero vector.\n         \n           \n         \n          Not closed under addition,\n          closed under multiplication by scalars, contains the zero vector.\n         \n           \n         \n          Not closed under addition,\n          not closed under multiplication by scalars,\n          does not contain the zero vector.\n         \n           \n         \n          Not closed under addition,\n          closed under multiplication by scalars, contains the zero vector.\n         \n        Determine which of the following sets   is a subspace of   for the indicated value of  .\n        Justify your answer.\n       \n               \n             \n               \n             \n               \n             \n               \n             \n        Find a subset of   that is closed under addition and scalar multiplication,\n        but that does not contain the zero vector,\n        or explain why no such subset exists.\n       \n        The only such set is the empty set.\n       \n        Let   be a vector in  .\n        What is the smallest subspace of   that contains  ?\n        Explain.\n        Describe this space geometrically.\n       \n        What is the smallest subspace of   containing the first quadrant?\n        Justify your answer.\n       \n         .\n       \n        Let  ,  ,\n        and   be vectors in   with  .\n        Let   and  .\n       \n              If   is in  ,\n              must   be in  ?\n              Explain.\n             \n              If   is in  ,\n              must   be in  ?\n              Explain.\n             \n              What is the relationship between\n                and  ?\n              Be specific.\n             \n        Let   and   be positive integers,\n        and let   be in  .\n        Let  .\n       \n              As an example,\n              let   in   with  .\n             \n                    Show that the vector   is in   by finding a matrix   that places   in  .\n                   \n                    As an example, let   in  .\n                   \n                     \n                   \n                    Show that the the vector   is in   by finding a matrix   that places   in  .\n                   \n                    As an example, let   in  .\n                   \n                     \n                   \n                    Show that the vector   is in   by finding a matrix   that places   in  .\n                   \n                    As an example, let   in  .\n                   \n                     \n                   \n                    Show that  .\n                   \n                    As an example, let   in  .\n                   \n                     \n                   \n              Show that, regardless of the vector   selected,\n                is a subspace of  .\n             \n               ,\n               ,  .\n             \n              Characterize all of the possibilities for what the subspace   can be.\n              \n             \n              There is more than one possibility.\n             \n               ,  \n             \n        Let   and   be subsets of   such that  .\n        Must it be the case that   and   contain at least one vector in common?\n        Justify your answer.\n       \n        Assume   and   are two subspaces of  .\n        Is   also a subspace of  ?\n        Is   also a subspace of  ?\n        Justify your answer.\n        (Note: The notation   refers to the vectors common to both  ,\n        while the notation   refers to the vectors that are in at least one of  .)\n       \n          is a subspace of  ,\n          is not in general a subspace of  \n       \n        Determine whether the plane defined by the equation\n          is a subspace in  .\n       \n        If   is a subspace of   and   is a vector in   not in  ,\n        determine whether\n         \n        is a subspace of  .\n       \n        Not in general a subspace of  .\n       \n        Two students are talking about examples of subspaces.\n         \n          Student 1: The  -axis in   is a subspace. It is generated by the vector  .\n         \n         \n          Student 2: Similarly   is a subspace of  .\n         \n         \n          Student 1: I'm not sure if that will work. Can we fit   inside  ? Don't we need   to be a subset of   if it is a subspace of  ?\n         \n         \n          Student 2: Of course we can fit   inside  . We can think of   as vectors  . That's the  -plane.\n         \n         \n          Student 1: I don't know. The vector   is not exactly same as  .\n         \n         \n          Student 2: Well,   is a plane and so is the  -plane. So they must be equal, shouldn't they?\n         \n         \n          Student 1: But there are infinitely many planes in  . They can't all be equal to  . They all\n           look like  but I don't think we can say they are equal.\n         \n        Which student is correct?\n        Is   a subspace of  , or not?\n        Justify your answer.\n       subspace sum \n        Given two subspaces   of  , define\n         .\n        Show that   is a subspace of   containing both   as subspaces.\n        The space   is the sum\n        of the subspaces   and  .\n       \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              Any line in   is a subspace in  .\n             \n              F\n             True\/False \n              Any line through the origin in   is a subspace in  .\n             True\/False \n              Any plane through the origin in   is a subspace in  .\n             \n              T\n             True\/False \n              In  ,\n              the points satisfying   form a subspace.\n             True\/False \n              In  ,\n              the points satisfying   form a subspace.\n             \n              T\n             True\/False \n              Any two nonzero vectors generate a plane subspace in  .\n             True\/False \n              The space   is a subspace of  .\n             \n              F\n             True\/False \n              If   is a subspace of   and   is in  ,\n              then the line through the origin and   is in  .\n             True\/False \n              There are four types of subspaces in  :\n               , line through origin,\n              plane through origin and the whole space  .\n             \n              T\n             True\/False \n              There are four types of subspaces in  :\n               , line through origin,\n              plane through origin and the whole space  .\n             True\/False \n              The vectors  ,\n                and\n                form a subspace in  .\n             \n              F\n             True\/False \n              The vectors   and\n                form a basis of a subspace in  .\n             Project: Least Squares Linear Approximation \n    We return to the problem of finding the least squares line to fit the GDP-consumption data. We will start our work in a more general setting, determining the  method for fitting a linear function to fit any data set, like the GDP-consumption data, in the least squares sense. Then we will apply our result to the GDP-consumption data.\n   \n        Suppose we want to fit a linear function   to our data. For the sake of our argument, let us assume the general case where we have  data points labeled as  ,  ,  ,   ,  . (In the GDP-consumption data  .) In the unlikely event that the graph of   actually passes through these data points, then we would have the system of equations\n\n         \n\n        in the unknowns   and  .\n       \n          As a small example to illustrate, write the system (\\ref{eq:LS_system}) using the threepoints  ,  , and  . Identify the unknowns and then write this system in the form  . Explicitly identify thematrix   and the the vectors   and  .\n         \n          Identify the specific matrix   and the specific vectors   and   using the data inTable \\ref{T:GDP_consumption}. Explain why the system   is inconsistent.(Remember, we are treating consumption as the independent variable and GDP as the dependentvariable.) What does the result tell us about the data? \n         \n      shows that the GDP-consumption data does not lie on a line. So instead of attempting to find coefficients   and   that give a solution to this system, which may be impossible, we instead look for a vector   that provides us with something that is  close  to a solution.\n   \n    If we could find   and   that give a solution to the system  , then   would be zero. If we can't make   exactly equal to the vector  , we could instead try to minimize   in some way. One way is to minimize the length   of the vector  .\n   \n    If we minimize the quantity  , then we will have minimized a function given by a sum of squares. That is,   is calculated to be\n\n     .\n    \n    This is why the method we will derive is called the method of least squares. This method provides us with a vector  solution  in a subspace that is related to  . We can visualize   as in  . In this figure the data points are shown along with a linear approximation (not the best for illustrative purposes). The lengths of the vertical line segments are the summands   in  . So we are trying to minimize the sum of the squares of these line segments. \n   Error in the linear approximation. \n    Suppose that   minimizes  . Then the vector   is the vector that is closest to   of all of the vectors of the form  . The fact that the vectors of the form   make a subspace will be useful in what follows. We verify that fact in the next project activity.\n   \n        Let   be an arbitrary   matrix. Explain why the set   is a subspace of  . \n       \n      shows us that even though the GDP-consumption system   does not have a solution, we can find a vector that is close to a solution in the subspace  . That is, find a vector   in   such that   is as close (in the least squares sense) to   as we can get. In other words, the error   is as small as possible. In the following activity we see how to find  . \n   \n        Let \n         ,\n        the quantity we want to minimize. The variables in   are   and  , so we can think of   as a function of the two independent variables   and  . The square root makes calculations more complicated, so it is helpful to notice that   will be a minimum when   is a minimum. Since   is also function of the two variables   and  , the minimum value of   will occur when the partial derivatives of   with respect to   and   are both   (if you haven't yet taken a multivariable calculus course, you can just assume that this is correct). This yields the equations \n         .\n\n        In this activity we solve equations   and   for the unknowns   and  . (Do this in a general setting without using specific values for the   and  .)\n       \n          Let  ,  ,  , and  . Show that the equations   and   can be written in the form\n           .\n          Note that this is a system of two linear equations in the unknowns   and  .\n         \n          Write the system from part (a) in matrix form  . Then use techniques from linear algebra to solve the linear system to show that\n           \n          and\n           \n          \n         \n    Use the formulas   and   to find the values of   and   for the regression line to fit the GDP-consumption data in  . You may use the fact that the sum of the GDP data is  , the sum of the consumption data is  , the sum of the squares of the consumption data is  , and the sum of the products of the GDP and consumption data is  . Compare to the results the authors obtained in the paper   A Statistical Analysis of GDP and Final Consumption Using Simple Linear Regression, the Case of Romania 1990-2010 . \n   "
},
{
  "id": "objectives-12",
  "level": "2",
  "url": "chap_R_n.html#objectives-12",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What properties make   a vector space?\n           \n         \n           \n            What is a subspace of  ?\n           \n         \n           \n            What properties do we need to verify to show that a set of vectors is a subspace of  ?\n            Why?\n           \n         \n           \n            What important structure does the span of a set of vectors in   have?\n           \n         "
},
{
  "id": "p-1972",
  "level": "2",
  "url": "chap_R_n.html#p-1972",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "least squares "
},
{
  "id": "p-1973",
  "level": "2",
  "url": "chap_R_n.html#p-1973",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "consumption "
},
{
  "id": "p-1975",
  "level": "2",
  "url": "chap_R_n.html#p-1975",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "vector space "
},
{
  "id": "p-1976",
  "level": "2",
  "url": "chap_R_n.html#p-1976",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "subspaces "
},
{
  "id": "pa_3_a",
  "level": "2",
  "url": "chap_R_n.html#pa_3_a",
  "type": "Preview Activity",
  "number": "12.1",
  "title": "",
  "body": "\n      Let   and   be two vectors in  .\n      Let  .\n     \n            For   to be a subspace of  ,\n            the sum of any two vectors in   must also be in  .\n           \n                  Pick two specific examples of vectors   in  \n                  (keeping   unknown\/general vectors).\n                  For example, one specific   would be\n                    as we used in the above example.\n                  Find the sum of  .\n                  Is the sum also in  ?\n                  Explain.\n                  \n                 \n                  What does it mean for a vector to be in  ?\n                 \n                  Now let   and   be arbitrary vectors in  .\n                  Explain why   is in  .\n                 \n            For   to be a subspace of  ,\n            any scalar multiple of any vector in   must also be in  .\n           \n                  Pick a specific example   in  .\n                  Explain why   are all also in  .\n                 \n                  Now let   be an arbitrary scalar and let   be an arbitrary vector in  .\n                  Explain why the vector   is in  .\n                 \n            For   to be a subspace of  ,\n            the zero vector must also be in  .\n            Explain why the zero vector is in  .\n           \n            Does vector addition being commutative for vectors in   imply that vector addition is also commutative for vectors in  ?\n            Explain your reasoning.\n           \n            Suppose we have an arbitrary   in  .\n            There is an additive inverse of   in  .\n            In other words,\n            there is a   such that  .\n            Should this   be also in  ?\n            If so, explain why.\n            If not, give a counterexample.\n           \n            Look at the other properties of vector addition and scalar multiplication of vectors in   listed in  \n            in  .\n            Which of these properties should also hold for vectors in  ?\n           "
},
{
  "id": "p-1992",
  "level": "2",
  "url": "chap_R_n.html#p-1992",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "vector space "
},
{
  "id": "def_3_a_1",
  "level": "2",
  "url": "chap_R_n.html#def_3_a_1",
  "type": "Definition",
  "number": "12.3",
  "title": "",
  "body": "vector space vector space closed commutative associative additive identity zero vector additive inverse closed multiplication by scalars distributes over scalar addition multiplication by scalars distributes over addition in  "
},
{
  "id": "p-2004",
  "level": "2",
  "url": "chap_R_n.html#p-2004",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "subspaces "
},
{
  "id": "def_3_a_subspaces",
  "level": "2",
  "url": "chap_R_n.html#def_3_a_subspaces",
  "type": "Definition",
  "number": "12.4",
  "title": "",
  "body": "subspace of  subspace "
},
{
  "id": "ex_3_a_1",
  "level": "2",
  "url": "chap_R_n.html#ex_3_a_1",
  "type": "Example",
  "number": "12.5",
  "title": "",
  "body": "\n        There are many subsets of   that are themselves vector spaces.\n        Consider as an example the set   of vectors in   defined by\n         .\n       \n        In other words,\n          is the set of vectors in   whose second component is 0.\n        To see that   is itself a vector space,\n        we need to demonstrate that   satisfies all of the properties listed in  .\n       arbitrary \n        Since the second component of\n          is 0, it follows that   is in  .\n        Thus, the set   is closed under addition.\n       \n        For the second property, that addition is commutative in  ,\n        we can just use the fact that if   and   are in  ,\n        they are also vectors in   and\n          is satisfied in  .\n        So the property also holds in  .\n       \n        A similar argument can be made for property (3).\n       \n        Property (4) states the existence of the additive identity in  .\n        Note that   is an additive identity in   and if it is also an element in  ,\n        then it will automatically be the additive identity of  .\n        Since the zero vector can be written as   with  ,\n          is in  .\n        Thus,   satisfies property 4.\n       \n        We will postpone property (5) for a bit since we can show that other properties imply property (5).\n       \n        Property (6) is a closure property, just like property (1).\n        We need to verify that  any \n        scalar multiple of  any \n        vector in   is again in  .\n        Consider an arbitrary vector   and an arbitrary scalar  .\n        Now\n         .\n       \n        Since the vector   has a 0 as its second component,\n        we see that   is in  .\n        Thus,   is closed under scalar multiplication.\n       \n        Properties (7), (8), (9) and (10) only depend on the operations of addition and multiplication by scalars in  .\n        Since these properties depend on the operations and not the vectors,\n        these properties will transfer to  .\n       \n        We still have to justify property (5) though.\n        Note that since   in real numbers,\n        by applying property (7) with  ,  ,\n        we find that\n         .\n       \n        Therefore,   is an additive inverse for  .\n        Therefore, to show that the additive inverse of any   in   is also in  ,\n        we simply note that any multiple of   is also in   and hence   must also be in  .\n       subspace "
},
{
  "id": "thm_3_a_subspace_Rn",
  "level": "2",
  "url": "chap_R_n.html#thm_3_a_subspace_Rn",
  "type": "Theorem",
  "number": "12.6",
  "title": "",
  "body": "closed closed "
},
{
  "id": "act_3_a_1",
  "level": "2",
  "url": "chap_R_n.html#act_3_a_1",
  "type": "Activity",
  "number": "12.2",
  "title": "",
  "body": "\n      Use  \n      to answer the following questions.\n      Justify your responses.\n      For sets which lie inside  ,\n      sketch a pictorial representation of the set and explain why your picture confirms your answer.\n     \n            Is the set   a subspace of  ?\n           \n            Is the set   a subspace of  ?\n           \n            Is the set   a subspace of  ?\n           \n            Is the set   a subspace of  ?\n           \n            Is the set   a subspace of  ?\n           \n            Is the set   a subspace of  ?\n           \n            Is the set   a subspace of  ?\n            Note that   is the unit sphere (a.k.a. unit ball) in  .\n           \n            Is the set   a subspace of  ?\n           "
},
{
  "id": "thm_3_a_span_subspace",
  "level": "2",
  "url": "chap_R_n.html#thm_3_a_span_subspace",
  "type": "Theorem",
  "number": "12.7",
  "title": "",
  "body": "\n        Let  ,  ,\n         ,   be vectors in  .\n        Then   is a subspace of  .\n       \n      Let  ,  ,\n       ,   be vectors in  .\n      Let  .\n      To show that   is a subspace of   we need to show that   is closed under addition and multiplication by scalars and that   is in  .\n     \n      First we show that   is closed under addition.\n      Let   and   be vectors in  .\n      This means that   and   are linear combinations of  ,\n       ,  ,  .\n      So there are scalars  ,\n       ,  ,\n        and  ,  ,\n       ,   so that\n       .\n     \n      To demonstrate that   is in  ,\n      we need to show that   is a linear combination of  ,\n       ,  ,  .\n      Using the properties of vector addition and scalar multiplication, we find\n       .\n     \n      Thus   is a linear combination of  ,\n       ,  ,\n        and   is closed under vector addition.\n     \n      Next we show that   is closed under scalar multiplication.\n      Let   be in   and   be a scalar.\n      Then\n       \n      and   is a linear combination of  ,\n       ,  ,\n        and   is closed under multiplication by scalars.\n     \n      Finally, we show that   is in  .\n      Since\n       ,\n     \n        is in  .\n     \n      Since   satisfies all of the properties of a subspace as given in definition of a subspace,\n      we conclude that   is a subspace of  .\n     "
},
{
  "id": "p-2052",
  "level": "2",
  "url": "chap_R_n.html#p-2052",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "subspace of   spanned by  "
},
{
  "id": "act_3_a_2",
  "level": "2",
  "url": "chap_R_n.html#act_3_a_2",
  "type": "Activity",
  "number": "12.3",
  "title": "",
  "body": "\n            Describe geometrically as best as you can the subspaces of   spanned by the following sets of vectors.\n               \n           \n            Express the following set of vectors as the span of some vectors to show that this set is a subspace.\n            Can you give a geometric description of the set?\n             \n           "
},
{
  "id": "example-24",
  "level": "2",
  "url": "chap_R_n.html#example-24",
  "type": "Example",
  "number": "12.8",
  "title": "",
  "body": "\n        Let  .\n       \n              Show that   is a subspace of  .\n             \n              Every vector in   has the form\n               \n              for some real numbers  ,  , and  .\n              Thus,\n               .\n              As a span of a set of vectors,\n              we know that   is a subspace of  .\n             \n              Describe in detail the geometry of the subspace   (e.g., is it a line,\n              a union of lines, a plane, a union of planes, etc.)\n             \n              Let  ,\n               ,\n              and  .\n              The reduced row echelon form of\n                is  .\n              The pivot columns of   form a linearly independent set with the same span as  , So\n                and   forms the plane in   through the origin and the points   and  .\n             "
},
{
  "id": "example-25",
  "level": "2",
  "url": "chap_R_n.html#example-25",
  "type": "Example",
  "number": "12.9",
  "title": "",
  "body": "\n              Let   and let  .\n              That is,   is the  -axis and   the  -axis in three-space.\n              Let\n               .\n             \n                    Is   in  ?\n                    Justify your answer.\n                   \n                    We let   and  . T\n                   \n                    Let  ,\n                     ,\n                    and  .\n                    Since   with   and\n                      we conclude that  .\n                   \n                    Is   in  ?\n                    Justify your answer.\n                   \n                    We let   and  . T\n                   \n                    Every vector in   has the form   for some scalar   (where  ,\n                    and every vector in   has the form   for some scalar  \n                    (where  ).\n                    So every vector in   is of the form  .\n                    Since the vector   does not have a   in the third component,\n                    we conclude that in   is not in  .\n                   \n                    Assume that   is a subspace of  .\n                    Describe in detail the geometry of this subspace.\n                   \n                    We let   and  . T\n                   \n                    As we just argued,\n                    every vector in   has the form  .\n                    So  ,\n                    which is the  -plane in  .\n                   subspace sum sum \n              To see why the set   is a subspace of  ,\n              suppose that   and   are in  .\n              Then   and   for some\n                in   and some   in  .\n              Then\n               .\n              Since   is a subspace of   it follows that  .\n              Similarly,  .\n              This makes   an element of  .\n              Also, suppose that   is a scalar.\n              Then\n               .\n              Since   is a subspace of   it follows that  .\n              Similarly,  .\n              This makes   an element of  .\n              Finally, since   is in both   and  ,\n              and  ,\n              it follows that   is an element of  .\n              We conclude that   is a subspace of  .\n             "
},
{
  "id": "p-2074",
  "level": "2",
  "url": "chap_R_n.html#p-2074",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "closed commutative associative additive identity zero vector additive inverse closed multiplication by scalars distributes over scalar addition multiplication by scalars distributes over addition in  "
},
{
  "id": "p-2087",
  "level": "2",
  "url": "chap_R_n.html#p-2087",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "closed closed "
},
{
  "id": "exercise-107",
  "level": "2",
  "url": "chap_R_n.html#exercise-107",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Each of the following regions or graphs determines a subset   of  .\n        For each region,\n        discuss each of the subspace properties of Theorem 12.4 and explain with justification if the set   satisfies each property or not.\n       \n           \n         \n          Closed under addition,\n          not closed under multiplication by scalars, contains the zero vector.\n         \n           \n         \n          Not closed under addition,\n          closed under multiplication by scalars, contains the zero vector.\n         \n           \n         \n          Not closed under addition,\n          not closed under multiplication by scalars,\n          does not contain the zero vector.\n         \n           \n         \n          Not closed under addition,\n          closed under multiplication by scalars, contains the zero vector.\n         "
},
{
  "id": "exercise-108",
  "level": "2",
  "url": "chap_R_n.html#exercise-108",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Determine which of the following sets   is a subspace of   for the indicated value of  .\n        Justify your answer.\n       \n               \n             \n               \n             \n               \n             \n               \n             "
},
{
  "id": "exercise-109",
  "level": "2",
  "url": "chap_R_n.html#exercise-109",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Find a subset of   that is closed under addition and scalar multiplication,\n        but that does not contain the zero vector,\n        or explain why no such subset exists.\n       \n        The only such set is the empty set.\n       "
},
{
  "id": "exercise-110",
  "level": "2",
  "url": "chap_R_n.html#exercise-110",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Let   be a vector in  .\n        What is the smallest subspace of   that contains  ?\n        Explain.\n        Describe this space geometrically.\n       "
},
{
  "id": "exercise-111",
  "level": "2",
  "url": "chap_R_n.html#exercise-111",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        What is the smallest subspace of   containing the first quadrant?\n        Justify your answer.\n       \n         .\n       "
},
{
  "id": "exercise-112",
  "level": "2",
  "url": "chap_R_n.html#exercise-112",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Let  ,  ,\n        and   be vectors in   with  .\n        Let   and  .\n       \n              If   is in  ,\n              must   be in  ?\n              Explain.\n             \n              If   is in  ,\n              must   be in  ?\n              Explain.\n             \n              What is the relationship between\n                and  ?\n              Be specific.\n             "
},
{
  "id": "exercise-113",
  "level": "2",
  "url": "chap_R_n.html#exercise-113",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        Let   and   be positive integers,\n        and let   be in  .\n        Let  .\n       \n              As an example,\n              let   in   with  .\n             \n                    Show that the vector   is in   by finding a matrix   that places   in  .\n                   \n                    As an example, let   in  .\n                   \n                     \n                   \n                    Show that the the vector   is in   by finding a matrix   that places   in  .\n                   \n                    As an example, let   in  .\n                   \n                     \n                   \n                    Show that the vector   is in   by finding a matrix   that places   in  .\n                   \n                    As an example, let   in  .\n                   \n                     \n                   \n                    Show that  .\n                   \n                    As an example, let   in  .\n                   \n                     \n                   \n              Show that, regardless of the vector   selected,\n                is a subspace of  .\n             \n               ,\n               ,  .\n             \n              Characterize all of the possibilities for what the subspace   can be.\n              \n             \n              There is more than one possibility.\n             \n               ,  \n             "
},
{
  "id": "exercise-114",
  "level": "2",
  "url": "chap_R_n.html#exercise-114",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Let   and   be subsets of   such that  .\n        Must it be the case that   and   contain at least one vector in common?\n        Justify your answer.\n       "
},
{
  "id": "exercise-115",
  "level": "2",
  "url": "chap_R_n.html#exercise-115",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        Assume   and   are two subspaces of  .\n        Is   also a subspace of  ?\n        Is   also a subspace of  ?\n        Justify your answer.\n        (Note: The notation   refers to the vectors common to both  ,\n        while the notation   refers to the vectors that are in at least one of  .)\n       \n          is a subspace of  ,\n          is not in general a subspace of  \n       "
},
{
  "id": "exercise-116",
  "level": "2",
  "url": "chap_R_n.html#exercise-116",
  "type": "Exercise",
  "number": "10",
  "title": "",
  "body": "\n        Determine whether the plane defined by the equation\n          is a subspace in  .\n       "
},
{
  "id": "exercise-117",
  "level": "2",
  "url": "chap_R_n.html#exercise-117",
  "type": "Exercise",
  "number": "11",
  "title": "",
  "body": "\n        If   is a subspace of   and   is a vector in   not in  ,\n        determine whether\n         \n        is a subspace of  .\n       \n        Not in general a subspace of  .\n       "
},
{
  "id": "exercise-118",
  "level": "2",
  "url": "chap_R_n.html#exercise-118",
  "type": "Exercise",
  "number": "12",
  "title": "",
  "body": "\n        Two students are talking about examples of subspaces.\n         \n          Student 1: The  -axis in   is a subspace. It is generated by the vector  .\n         \n         \n          Student 2: Similarly   is a subspace of  .\n         \n         \n          Student 1: I'm not sure if that will work. Can we fit   inside  ? Don't we need   to be a subset of   if it is a subspace of  ?\n         \n         \n          Student 2: Of course we can fit   inside  . We can think of   as vectors  . That's the  -plane.\n         \n         \n          Student 1: I don't know. The vector   is not exactly same as  .\n         \n         \n          Student 2: Well,   is a plane and so is the  -plane. So they must be equal, shouldn't they?\n         \n         \n          Student 1: But there are infinitely many planes in  . They can't all be equal to  . They all\n           look like  but I don't think we can say they are equal.\n         \n        Which student is correct?\n        Is   a subspace of  , or not?\n        Justify your answer.\n       "
},
{
  "id": "ex_3_a_sum",
  "level": "2",
  "url": "chap_R_n.html#ex_3_a_sum",
  "type": "Exercise",
  "number": "13",
  "title": "",
  "body": "subspace sum \n        Given two subspaces   of  , define\n         .\n        Show that   is a subspace of   containing both   as subspaces.\n        The space   is the sum\n        of the subspaces   and  .\n       "
},
{
  "id": "exercise-120",
  "level": "2",
  "url": "chap_R_n.html#exercise-120",
  "type": "Exercise",
  "number": "14",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              Any line in   is a subspace in  .\n             \n              F\n             True\/False \n              Any line through the origin in   is a subspace in  .\n             True\/False \n              Any plane through the origin in   is a subspace in  .\n             \n              T\n             True\/False \n              In  ,\n              the points satisfying   form a subspace.\n             True\/False \n              In  ,\n              the points satisfying   form a subspace.\n             \n              T\n             True\/False \n              Any two nonzero vectors generate a plane subspace in  .\n             True\/False \n              The space   is a subspace of  .\n             \n              F\n             True\/False \n              If   is a subspace of   and   is in  ,\n              then the line through the origin and   is in  .\n             True\/False \n              There are four types of subspaces in  :\n               , line through origin,\n              plane through origin and the whole space  .\n             \n              T\n             True\/False \n              There are four types of subspaces in  :\n               , line through origin,\n              plane through origin and the whole space  .\n             True\/False \n              The vectors  ,\n                and\n                form a subspace in  .\n             \n              F\n             True\/False \n              The vectors   and\n                form a basis of a subspace in  .\n             "
},
{
  "id": "act_ls_no_line",
  "level": "2",
  "url": "chap_R_n.html#act_ls_no_line",
  "type": "Project Activity",
  "number": "12.4",
  "title": "",
  "body": "\n        Suppose we want to fit a linear function   to our data. For the sake of our argument, let us assume the general case where we have  data points labeled as  ,  ,  ,   ,  . (In the GDP-consumption data  .) In the unlikely event that the graph of   actually passes through these data points, then we would have the system of equations\n\n         \n\n        in the unknowns   and  .\n       \n          As a small example to illustrate, write the system (\\ref{eq:LS_system}) using the threepoints  ,  , and  . Identify the unknowns and then write this system in the form  . Explicitly identify thematrix   and the the vectors   and  .\n         \n          Identify the specific matrix   and the specific vectors   and   using the data inTable \\ref{T:GDP_consumption}. Explain why the system   is inconsistent.(Remember, we are treating consumption as the independent variable and GDP as the dependentvariable.) What does the result tell us about the data? \n         "
},
{
  "id": "F_GDP_error",
  "level": "2",
  "url": "chap_R_n.html#F_GDP_error",
  "type": "Figure",
  "number": "12.10",
  "title": "",
  "body": "Error in the linear approximation. "
},
{
  "id": "act_ls_subspace",
  "level": "2",
  "url": "chap_R_n.html#act_ls_subspace",
  "type": "Project Activity",
  "number": "12.5",
  "title": "",
  "body": "\n        Let   be an arbitrary   matrix. Explain why the set   is a subspace of  . \n       "
},
{
  "id": "act_ls_minimum",
  "level": "2",
  "url": "chap_R_n.html#act_ls_minimum",
  "type": "Project Activity",
  "number": "12.6",
  "title": "",
  "body": "\n        Let \n         ,\n        the quantity we want to minimize. The variables in   are   and  , so we can think of   as a function of the two independent variables   and  . The square root makes calculations more complicated, so it is helpful to notice that   will be a minimum when   is a minimum. Since   is also function of the two variables   and  , the minimum value of   will occur when the partial derivatives of   with respect to   and   are both   (if you haven't yet taken a multivariable calculus course, you can just assume that this is correct). This yields the equations \n         .\n\n        In this activity we solve equations   and   for the unknowns   and  . (Do this in a general setting without using specific values for the   and  .)\n       \n          Let  ,  ,  , and  . Show that the equations   and   can be written in the form\n           .\n          Note that this is a system of two linear equations in the unknowns   and  .\n         \n          Write the system from part (a) in matrix form  . Then use techniques from linear algebra to solve the linear system to show that\n           \n          and\n           \n          \n         "
},
{
  "id": "project-37",
  "level": "2",
  "url": "chap_R_n.html#project-37",
  "type": "Project Activity",
  "number": "12.7",
  "title": "",
  "body": "\n    Use the formulas   and   to find the values of   and   for the regression line to fit the GDP-consumption data in  . You may use the fact that the sum of the GDP data is  , the sum of the consumption data is  , the sum of the squares of the consumption data is  , and the sum of the products of the GDP and consumption data is  . Compare to the results the authors obtained in the paper   A Statistical Analysis of GDP and Final Consumption Using Simple Linear Regression, the Case of Romania 1990-2010 . \n   "
},
{
  "id": "chap_null_space",
  "level": "1",
  "url": "chap_null_space.html",
  "type": "Section",
  "number": "13",
  "title": "The Null Space and Column Space of a Matrix",
  "body": "The Null Space and Column Space of a Matrix \n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is the null space of a matrix?\n           \n         \n           \n            What is the column space of a matrix?\n           \n         \n           \n            What important structure do the null space and column space of a matrix have?\n           \n         \n           \n            What is the kernel of a matrix transformation?\n           \n         \n           \n            How is the kernel of a matrix transformation   defined by\n              related to the null space of  ?\n           \n         \n           \n            What is the range of a matrix transformation?\n           \n         \n           \n            How is the range of a matrix transformation   defined by\n              related to the column space of  ?\n           \n         \n           \n            How do we find a basis for  ?\n           \n         \n           \n            How do we find a basis for  ?\n           \n         Application: The Lights Out Game \n    Lights Out (LO) is a commercial game released by Tiger Toys in 1995\n    (later bought out by Hasbro).\n    The game consists of a   grid in which each square is either lit or unlit.\n    Pressing a square changes the status of the square itself and all the squares to the left, right,\n    up, or down.\n    The player's job is to turn all the lights out.\n    You can play a sample game at  . There is a method to solve any solvable Lights Out game that can be uncovered through linear algebra that we will uncover later in this section.\n    Column spaces and null spaces play important roles in this method.\n   Introduction closed closed \n    Given a matrix  ,\n    there are several subspaces that are connected to  .\n    Two specific such subspaces are the null space of   and the column space of  .\n    We will see that these subspaces provide answers to the big questions we have been considering since the beginning of the semester, such as\n     Do columns of   span  ? \n     Are the columns of   linearly independent? \n     Is the transformation   defined by matrix multiplication by   one-to-one? \n     Is the transformation   onto? \n   null space \n            Let  .\n           \n                  Find the general solution to the homogeneous equation  .\n                  Write your solutions in parametric vector form.\n                  (Recall that the parametric vector form expresses the solutions to an equation as linear combinations of vectors with free variables as the weights.\n                  An example would be  .)\n                 \n                  Find two specific solutions   and   to the homogeneous equation  .\n                  Is   a solution to  ?\n                  Explain.\n                 \n                  Is   a solution to  ?\n                  Explain.\n                 \n                  Is   a solution to  ?\n                 \n                  What does the above seem to indicate about the set of solutions to the homogeneous system  ?\n                 null space arbitrary \n                  The null space of an   matrix is a subset of   for some integer  .\n                  What is  ?\n                 \n                  Now suppose   and   are two vectors in  .\n                  By definition, that means  ,  .\n                  Use properties of the matrix-vector product to show that\n                    is also in  .\n                 \n                  Now suppose   is a vector in   and   is a scalar.\n                  Explain why   is also in  .\n                 \n                  Explain why   is a subspace of  .\n                 The Null Space of a Matrix and the Kernel of a Matrix Transformation null space \n    Let   be an   matrix.\n    In  \n    we defined the null space of a matrix  \n    (see  )\n    as the set of solutions to the matrix equation  .\n    Note that the null space of an\n      matrix is a subset of  .\n    We saw that the null space of   is closed under addition and scalar multiplication   that is,\n    if   and   are in   and   and   are any scalars,\n    then   and   are also in  .\n    Since the zero vector is always in  ,\n    we can conclude that the null space of   is a subspace of  .\n   \n    There is a connection between the null space of a matrix and the matrix transformation it defines.\n    Recall that any   matrix   defines a matrix transformation   from   to   by  .\n    The null space of   is then the collection of vectors   in   so that  .\n    So if   is a matrix transformation from   to  ,\n    then the set\n     \n    is a subspace of   equal to the null space of  .\n    This set is is given a special name.\n   kernel \n      If   is a matrix transformation defined by a matrix  ,\n      then there is a convenient way to determine if   is one-to-one.\n     \n            Let   be the matrix transformation defined by  , where\n             .\n            Find all of the vectors in  .\n            If   contains more than one vector,\n            can   be one-to-one?\n            Why?\n           \n            Let   be the matrix transformation defined by  , where\n             .\n            Find all of the vectors in  .\n            Is   one-to-one?\n            Why?\n           \n            To find the vectors in the null space of a matrix   we solve the system  .\n            Since a homogeneous system is always consistent,\n            there are two possibilities for  :\n            either   or   contains infinitely many vectors.\n           \n                  Under what conditions on   is  ?\n                  What does that mean about   being one-to-one or not?\n                  Explain.\n                 \n                  Under what conditions is   infinite?\n                  What does that mean about   being one-to-one or not?\n                  Explain.\n                 \n                  Is is possible for   to be the whole space  ?\n                  If so, give an example.\n                  If not, explain why not.\n                 \n    Recall that for a function to be one-to-one,\n    each output must come from exactly one input.\n    Since a matrix transformation   defined by\n      always maps the zero vector to the zero vector,\n    for   to be one-to-one it must be the case that the zero vector is the only vector that   maps to the zero vector.\n    This means that the null space of   must be  .\n     \n    demonstrates that if the matrix   that defines the transformation   has a pivot in every column,\n    then   will have exactly one solution for each   in the range of  .\n    So a trivial null space is enough to characterize a one-to-one matrix transformation.\n   \n        A matrix transformation   from   to   defined by\n          is one-to-one if and only if\n         .\n       The Column Space of a Matrix and the Range of a Matrix Transformation \n    Given an   matrix  ,\n    we have seen that the matrix-vector product   is a linear combination of the columns of   with weights from  .\n    It follows that the equation\n      has a solution if and only if   is a linear combination of the columns of  .\n    So the span of the columns of   tells us for which vectors the equation   is consistent.\n    We give the span of the columns of a matrix   a special name.\n   column space column space \n    We denote the column space of   as  .\n    Given that   is a linear combination of the columns of  ,\n    we can also write the column space of an   matrix   as\n     .\n   \n    For the matrix transformation   defined by  ,\n    the set of all vectors of the form   is also the range of the transformation  .\n    So for a matrix transformation   with matrix   we have  .\n   \n      As a span of a set of vectors,\n      we know that   is a subspace of   for an appropriate value of  .\n     \n            Let  .\n            The space   is a subspace of   for some positive integer  .\n            What is   in this case?\n           \n            If   is an   matrix,\n            then   is a subspace of   for some positive integer  .\n            What is   in this case?\n           \n            Recall that a matrix transformation   given by   where   is an\n              matrix is onto if for every   in   there exists a   in   for which  .\n            How does   being onto relate to the  ?\n           \n    As you saw in  ,\n    a matrix transformation   defined by\n      is onto if the column space of  ,\n    which consists of the image vectors under the transformation  ,\n    is equal to  .\n    In other words, we want the\n      to equal  .\n   \n        A matrix transformation   from   to   defined by\n          is onto if and only if\n         .\n       The Row Space of a Matrix \n    As you might expect,\n    if there is a column space for a matrix then there is also a row space for a matrix.\n    The row space is defined just as the column space as the span of the rows of a matrix.\n   row space row space \n    There is really nothing new here, though.\n    Since the rows of   are the columns of  ,\n    it follows that  .\n    So if we want to learn anything about the row space of  ,\n    we can just translate all of our questions to the column space of  .\n   Bases for   and  \n    When confronted with a subspace of  ,\n    we will usually want to find a minimal spanning set   a smallest spanning set   for the space.\n    Recall that a minimal spanning set is also called a basis for the space.\n    So a basis for a space must span that space,\n    and to be a minimal spanning set we have seen that a basis must also be linearly independent.\n    So to prove that a set is a basis for a subspace of   we need to demonstrate two things:\n    that the set is linearly independent,\n    and that the set spans the subspace.\n   \n      In this activity we see how to find a basis for   and   for a specific matrix  .\n      Let\n       .\n     \n      Assume that the reduced row echelon form of   is\n       .\n     \n            First we examine  .\n            Recall that to find a minimal spanning set of a set of vectors\n              in   we just select the pivot columns of the matrix  .\n           \n                  Find a basis for  .\n                 \n                  Does   equal  ?\n                  Explain.\n                 \n            Now we look at  .\n           \n                  Write the general solution to the homogeneous system   in vector form.\n                 \n                  Find a spanning set for  .\n                 \n                  Find a basis for  .\n                  Explain how you know you have a basis.\n                 \n    You should have noticed that  \n    (a) provides a process for finding a basis for     the pivot columns of   form a basis for  .\n    Similarly,  \n    (b) shows us that we can find a basis for   by writing the general solution to the homogeneous equation\n      as a linear combination of vectors whose weights are the variables corresponding to the non-pivot columns of     and these vectors form a basis for  .\n    As we will argue next,\n    these process always give us bases for   and  .\n   \n    Let   be an   matrix,\n    and let   be the reduced row echelon form of  .\n    Suppose   has   non-pivot columns and   pivot columns.\n    We can rearrange the columns so that the non-pivot columns of   are the last   columns\n    (this just amounts to relabeling the unknowns in the system).\n   Basis for  \n          Here we argue that the method described following  \n          to find a spanning set for the null space always yields a basis for the null space.\n          First note that  ,\n          since the system   has the same solution set as  .\n          So it is enough to find a basis for  .\n          If every column of   is a pivot column,\n          then   has only the trivial solution and the null space of   is  .\n          Let us now consider the case where   contains non-pivot columns.\n          If we let  ,\n          and if   then we can write  ,\n           ,  ,\n            in terms of  ,\n           ,  , and  .\n          From these equations we can write   as a linear combination of some vectors  ,\n           ,  ,\n            with  ,\n           ,  ,   as weights.\n          By construction, each of the vectors  ,\n           ,  ,\n            has a component that is 1 with the corresponding component as 0 in all the other  .\n          Therefore, the vectors  ,  ,\n           ,\n            are linearly independent and span  \n          (and  ).\n          In other words,\n          the method we have developed to find the general solution to\n            always produces a basis for  .\n         Basis for  \n          Here we explain why the pivot columns of   form a basis for  .\n          Recall that the product   expresses a linear combination of the columns of   with weights from  ,\n          and every such linear combination is matched with a product   giving a linear combination of the columns of  \n           using the same weights .\n          So if a set of columns of   is linearly independent\n          (or dependent),\n          then the set of corresponding columns in   is linearly independent\n          (or dependent)\n          and vice versa.\n          Since each pivot column of   is a vector with 1 in one entry (a different entry for different pivot columns) and zeros elsewhere,\n          the pivot columns of   are clearly linearly independent.\n          It follows that the pivot columns of   are linearly independent.\n          All that remains is to explain why the pivot columns of   span  .\n          Let  ,  ,  ,\n            be the columns of   so that  ,\n          and let  ,  ,  ,\n            be the columns of   so that  .\n          Suppose   is a non-pivot column for   and   the corresponding non-pivot column in  .\n          Each pivot column is composed of a single 1 with the rest of its entries 0.\n          Also, if a non-pivot column contains a nonzero entry,\n          then there is a corresponding pivot column that contains a 1 in the corresponding position.\n          So   is a linear combination of  ,\n           ,\n           ,\n              the pivot columns of  .\n          Thus,\n           \n          for some scalars  ,  ,\n           ,  .\n          Let  ,\n          where the   is in position  .\n          Then   and so  .\n          Thus,\n           \n          and   is a linear combination of the pivot columns of  .\n          So every non-pivot column of   is in the span of   and we conclude that the pivot columns of   form a basis for  .\n         IMPORTANT POINT \n    It is the pivot columns of   that form a basis for  ,\n    not the pivot columns of the reduced row echelon form   of  .\n    In general,  .\n   \n    We can incorporate the ideas of this section to expand the Invertible Matrix Theorem.\n   The Invertible Matrix Theorem \n        Let   be an   matrix.\n        The following statements are equivalent.\n         \n             \n              The matrix   is an invertible matrix.\n             \n           \n             \n              The matrix equation   has only the trivial solution.\n             \n           \n             \n              The matrix   has   pivot columns.\n             \n           \n             \n              Every row of   contains a pivot.\n             \n           \n             \n              The columns of   span  .\n             \n           \n             \n              The matrix   is row equivalent to the identity matrix  .\n             \n           \n             \n              The columns of   are linearly independent.\n             \n           \n             \n              The columns of   form a basis for  .\n             \n           \n             \n              The matrix transformation   from   to   defined by   is one-to-one.\n             \n           \n             \n              The matrix equation   has exactly one solution for each vector   in  .\n             \n           \n             \n              The matrix transformation   from   to   defined by   is onto.\n             \n           \n             \n              There is an   matrix   so that  .\n             \n           \n             \n              There is an   matrix   so that  .\n             \n           \n             \n              The scalar 0 is not an eigenvalue of  .\n             \n           \n             \n              The matrix   is invertible.\n             \n           \n             \n               .\n             \n           \n             \n               .\n             \n           \n       Examples \n    What follows are worked examples that use the concepts from this section.\n   \n              Let  .\n             \n                    Find a basis for  .\n                   \n                    Technology shows that the reduced row echelon form of   is\n                     .\n                    The first two columns of   are the pivot columns of  .\n                    Since the pivot columns of   form a basis for  ,\n                    a basis for   is\n                     .\n                   \n                    We use  .\n                   \n                    Let   and  .\n                    Since neither   nor   is a scalar multiple of the other,\n                    we see that   is the span of two linearly independent vectors in  .\n                    Thus, we conclude that   is the plane in   through the origin and the points   and  .\n                   \n                    Describe   geometrically (e.g., as a line, a plane,\n                    a union of lines, etc.) in the appropriate larger space.\n                   \n                    \n                   \n              Let  .\n             \n                    Find a basis for  .\n                   \n                    We use  .\n                   \n                    Technology shows that the reduced row echelon form of   is\n                     .\n                    To find a basis for  ,\n                    we must solve the homogeneous equation  .\n                    If   and  ,\n                    the reduced row echelon form of   shows that   is free,\n                     , and  .\n                    So\n                     .\n                    Thus, a basis for   is\n                     .\n                   \n                    Describe   geometrically (e.g., as a line, a plane,\n                    a union of lines, etc.) in the appropriate larger space.\n                   \n                    We use  .\n                   \n                    Since   is the span of one nonzero vector in  ,\n                    we conclude that   is the line in   through the origin and the point  .\n                   \n        Let  ,\n        and let   be the matrix transformation defined by  .\n       \n              What are the domain and codomain of  ?\n              Why?\n             \n              Recall that   is a linear combination of the columns of   with weights from  .\n              So   is defined only when the number of components of   is equal to the number of columns of  .\n              This explains why the domain of   is  .\n              Also, since each output of   is a linear combination of the columns of  ,\n              the codomain of   is  .\n             \n              Find all vectors   such that  .\n              How is this set of vectors related to  ?\n              Explain.\n             \n              The set of vectors   such that\n                is the same as  .\n              The reduced row echelon form of   is\n               .\n              Since both columns of   are pivot columns,\n              the columns of   are linearly independent.\n              This implies that   has only the trivial solution.\n              Therefore, the only vector   such that\n                is the zero vector in  .\n             \n              Is   one-to-one?\n              Explain.\n             \n              The previous part shows that  .\n              This means that   is one-to-one by  .\n             \n              Is   onto?\n              If yes, explain why.\n              If no, find a basis for the range of  .\n             \n              Recall that the range of   is the same as  .\n              The reduced row echelon form of   has a row of zeros,\n              so   is not consistent for every   in  .\n              We conclude that   is not onto.\n              To find a basis for the range of  ,\n              we just need to find a basis for  .\n              The pivot columns of   form such a basis,\n              so a basis for the range of   is\n               .\n             Summary \n       \n        The null space of an   matrix   is the set of vectors   in   so that  .\n        In set notation\n         .\n       \n     \n       \n        The column space of a matrix   is the span of the columns of  .\n       \n     \n       closed closed \n     \n       \n        The null space of an   matrix is a subspace of   while the column space of   is a subspace of  .\n       \n     \n       \n        The span of any set of vectors in   is a subspace of  .\n       \n     \n       \n        The kernel of a matrix transformation   is the set\n         .\n       \n     \n       \n        The kernel of a matrix transformation   defined by\n          is the same set as  .\n       \n     \n       \n        The range of a matrix transformation   is the set\n         .\n       \n     \n       \n        The range of a matrix transformation   defined by\n          is the same set as  .\n       \n     \n       \n        A basis for the null space of a matrix   can be found by writing the general solution to the homogeneous equation\n          as a linear combination of vectors whose weights are the variables corresponding to the non-pivot columns of  .\n        The number of vectors in a basis for   is the number of non-pivot columns of  .\n       \n     \n       \n        The pivot columns of a matrix   form a basis for the column space of  .\n       \n     \n        Find a basis for the null space and column space of the matrix\n         .\n        Of which spaces are the null and column spaces of   subspaces?\n       \n        A basis for   is  ,\n        a basis for   is  .\n       \n        If the column space of   has basis\n         ,\n        what must   be?\n       \n        If the null space of   has basis  ,\n        what must   and   be?\n       \n          and  \n       \n        Find a matrix with at least four non-zero and distinct columns for which the column space has basis  .\n       \n        Find a matrix with at least two rows whose null space has basis  .\n       \n         \n       \n        Find a matrix whose column space has basis\n          and whose null space has basis  .\n       \n        If possible,\n        find a   matrix whose column space does not equal   but whose null space equals  .\n        Explain your answer.\n        If not possible, explain why not.\n       \n        Not possible\n       \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              For a   matrix,\n              the null space contains vectors other than the zero vector.\n             \n              T\n             True\/False \n              For a   matrix,\n              the null space contains vectors other than the zero vector.\n             True\/False \n              If Nul   is not the zero subspace,\n              then the transformation   is not one-to-one.\n             \n              T\n             True\/False \n              If the transformation\n                is onto where   is an\n                matrix, then Col  .\n             True\/False \n              For a   matrix  , Col   cannot equal  .\n             \n              T\n             True\/False \n              For a   matrix  , Col   cannot equal  .\n             True\/False \n              The null space of the matrix\n                consists of the two vectors\n                and  .\n             \n              F\n             True\/False \n              A basis for the null space of the matrix\n                consists of the two vectors\n                and  .\n             True\/False \n              There does not exist a matrix whose null space equals its column space.\n             \n              F\n             True\/False \n              The column space of every\n                matrix is   and its null space is  .\n             Project: Solving the Lights Out Game configuration \n    The numbers 0 and 1 in   will be the only numbers we use when playing the Lights Out game,\n    so all of our matrix entries will be in   and all of our calculations are done in  .\n   \n    There are two ways we can view a Lights Out game.\n     \n         \n          We can view each configuration as a   matrix.\n          In this situation,\n          we label the entries in the grid as shown in  .\n          Each entry in the grid will be assigned a 0 or 1 according to whether the light in that entry is off or on.\n         \n       \n         \n          For our purposes a better way to visualize a Lights Out configuration is as a   vector.\n          The components in this vector correspond to the entries in the\n            grid with the correspondence given by the numbering demonstrated in  \n          (for the sake of space, this array is shown in a row instead of a column).\n          Again, each component is assigned a 0 or 1 according to whether the light for that entry is off or on.\n          In this view,\n          each configuration is a vector with 25 components in  .\n         \n       \n   Two representations of the Lights Out game. \n    We will take the latter perspective and view the Lights Out game as if it is played on a\n      board with entries in  .\n    The space of all of these Lights Out configurations is denoted as  \n    (similar to  ,\n    but with entries in   rather than  ).\n    Since   is a field,\n    the space   is a vector space just as   is.\n    This is the environment in which we will play the Lights Out game.\n   \n    If we think about the game as played on a   board,\n    then pressing a square correlates to selecting one of the 25 components of a configuration vector.\n    Each time we press a square,\n    we make a move that changes the status of that square and all the squares vertically or horizontally adjacent to it from the   board.\n    Recalling that adding 1 to a square has the effect of changing its status\n    (from on to off or off to on),\n    and each move that we make in the game can be represented as a\n      vector that is added to a configuration.\n    For example,\n    the move of pressing the first square is given by adding the vector\n     \n    to a configuration vector and the move of pressing the second square is represented by adding the vector\n     .\n   \n      Let   be the move of pressing the  th square for   from 1 to 25.\n     \n            Find vector representations for   and  .\n           \n            Let  .\n            Explain why   for each   and  .\n            In other words, explain why  .\n            (Such a matrix is called a  symmetric  matrix.)\n           \n    The goal of the Lights Out game is to begin with an initial configuration   (a vector in\n     ) and determine if we can apply a sequence of moves to obtain the configuration in which all the entries are 0\n    (or all the lights are off).\n    The vector in   of all 0s is the zero vector in\n      and we will denote it as  .\n    Some basic algebra of vector addition in  \n    (or mod 2)\n    will help us understand the strategy.\n   \n    Start with a configuration  .\n    If we press the  th square,\n    then we obtain the new configuration  \n    (where each move   is also in  ).\n   \n            What happens if we press the  th square twice in a row?\n            Explain in terms of the action and the game and verify using vector addition.\n           \n            Explain why applying move   then move   is the same as applying move  ,\n            then  .\n           \n            Explain how the answers to the previous two questions show that to play the game we only need to determine which buttons to press\n            (and only once each)\n            without worrying about the order in which the buttons are pressed.\n           \n    What we have seen is that to play the game we are really looking for scalars  ,\n     ,  ,   in  \n    (in other words, either 0 or 1)\n    so that\n     .\n   \n      Explain why   has the equivalent matrix equation\n       ,\n      where  , or\n       .\n     \n      Explicitly identify the vector  .\n      Also, explain why   is on the right side of this equation.\n     \n    To solve a Lights Out game now,\n    all we need do is determine a solution,\n    if one exists, to the matrix equation  .\n   \n      For this activity you may use the fact that the reduced row echelon form of the matrix  \n      (using algebra in  )\n      is as shown below.\n     \n            Find a basis for the column space of  .\n           \n            Explain why not every Lights Out puzzle can be solved.\n            That is, explain why there are some initial configurations of lights on and off for which it is not possible to turn out all the lights\n            (without turning off the game).\n            Relate this to the column space of  .\n           \n    The reduced row echelon form of the matrix   (using algebra in  ):\n     .\n   \n    To find conditions under which a Lights Out game is not solvable,\n    we will demonstrate that if   is an   matrix,\n    then the scalar product of any vector in\n      with any column of   is  .\n    Let   be an\n      matrix with columns  ,\n     ,  ,  .\n    Represent the entries in the  th column as\n      for each   between 1 and  .\n    Note that   is also the  th row of  .\n    Also, let   be a vector in  .\n    Then  .\n    Using the row-column method of multiplying a matrix by a vector,\n    when we multiply the  th row of   with   we obtain\n     .\n   \n    This equation is valid for each   between 1 and  .\n    Recall that the sum in   is the scalar product of   and   and is denoted  .\n    That is,\n     .\n   orthogonal \n        Let   be an   matrix.\n        If   is any vector in   and   is any vector in  ,\n        then  .\n       \n    With  \n    in mind we can return to our analysis of the Lights Out game,\n    applying this result to the matrix  .\n   \n            Find a basis for the null space of  . (Recall that  ,\n            so you can use the reduced row echelon form of  \n            (using algebra in  )\n            given earlier.)\n           \n            Use   \n            to show that if   is an initial Lights Out configuration that is solvable,\n            then   must be orthogonal to each of the vectors in a basis for  .\n            Then show that if   is a solvable initial Lights Out configuration,\n              must satisfy\n             \n            and\n             .\n            Be very specific in your explanation.\n           \n      Now that we know which Lights Out games can be solved,\n      let   be an initial configuration to a solvable Lights Out game.\n      Explain how to find a solution to this game.\n      Will the solution be unique?\n      Explain.\n     \n    Now that we have a strategy for solving the Lights Out game,\n    use it to solve random puzzles at \n     , or create your own game to solve.\n   "
},
{
  "id": "objectives-13",
  "level": "2",
  "url": "chap_null_space.html#objectives-13",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is the null space of a matrix?\n           \n         \n           \n            What is the column space of a matrix?\n           \n         \n           \n            What important structure do the null space and column space of a matrix have?\n           \n         \n           \n            What is the kernel of a matrix transformation?\n           \n         \n           \n            How is the kernel of a matrix transformation   defined by\n              related to the null space of  ?\n           \n         \n           \n            What is the range of a matrix transformation?\n           \n         \n           \n            How is the range of a matrix transformation   defined by\n              related to the column space of  ?\n           \n         \n           \n            How do we find a basis for  ?\n           \n         \n           \n            How do we find a basis for  ?\n           \n         "
},
{
  "id": "p-2185",
  "level": "2",
  "url": "chap_null_space.html#p-2185",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "closed closed "
},
{
  "id": "p-2190",
  "level": "2",
  "url": "chap_null_space.html#p-2190",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "null space "
},
{
  "id": "pa_3_b",
  "level": "2",
  "url": "chap_null_space.html#pa_3_b",
  "type": "Preview Activity",
  "number": "13.1",
  "title": "",
  "body": "\n            Let  .\n           \n                  Find the general solution to the homogeneous equation  .\n                  Write your solutions in parametric vector form.\n                  (Recall that the parametric vector form expresses the solutions to an equation as linear combinations of vectors with free variables as the weights.\n                  An example would be  .)\n                 \n                  Find two specific solutions   and   to the homogeneous equation  .\n                  Is   a solution to  ?\n                  Explain.\n                 \n                  Is   a solution to  ?\n                  Explain.\n                 \n                  Is   a solution to  ?\n                 \n                  What does the above seem to indicate about the set of solutions to the homogeneous system  ?\n                 null space arbitrary \n                  The null space of an   matrix is a subset of   for some integer  .\n                  What is  ?\n                 \n                  Now suppose   and   are two vectors in  .\n                  By definition, that means  ,  .\n                  Use properties of the matrix-vector product to show that\n                    is also in  .\n                 \n                  Now suppose   is a vector in   and   is a scalar.\n                  Explain why   is also in  .\n                 \n                  Explain why   is a subspace of  .\n                 "
},
{
  "id": "p-2204",
  "level": "2",
  "url": "chap_null_space.html#p-2204",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "null space "
},
{
  "id": "definition-31",
  "level": "2",
  "url": "chap_null_space.html#definition-31",
  "type": "Definition",
  "number": "13.2",
  "title": "",
  "body": "kernel "
},
{
  "id": "act_3_b_1",
  "level": "2",
  "url": "chap_null_space.html#act_3_b_1",
  "type": "Activity",
  "number": "13.2",
  "title": "",
  "body": "\n      If   is a matrix transformation defined by a matrix  ,\n      then there is a convenient way to determine if   is one-to-one.\n     \n            Let   be the matrix transformation defined by  , where\n             .\n            Find all of the vectors in  .\n            If   contains more than one vector,\n            can   be one-to-one?\n            Why?\n           \n            Let   be the matrix transformation defined by  , where\n             .\n            Find all of the vectors in  .\n            Is   one-to-one?\n            Why?\n           \n            To find the vectors in the null space of a matrix   we solve the system  .\n            Since a homogeneous system is always consistent,\n            there are two possibilities for  :\n            either   or   contains infinitely many vectors.\n           \n                  Under what conditions on   is  ?\n                  What does that mean about   being one-to-one or not?\n                  Explain.\n                 \n                  Under what conditions is   infinite?\n                  What does that mean about   being one-to-one or not?\n                  Explain.\n                 \n                  Is is possible for   to be the whole space  ?\n                  If so, give an example.\n                  If not, explain why not.\n                 "
},
{
  "id": "thm_3_b_one_to_one_kernel",
  "level": "2",
  "url": "chap_null_space.html#thm_3_b_one_to_one_kernel",
  "type": "Theorem",
  "number": "13.3",
  "title": "",
  "body": "\n        A matrix transformation   from   to   defined by\n          is one-to-one if and only if\n         .\n       "
},
{
  "id": "definition-32",
  "level": "2",
  "url": "chap_null_space.html#definition-32",
  "type": "Definition",
  "number": "13.4",
  "title": "",
  "body": "column space column space "
},
{
  "id": "act_3_b_2",
  "level": "2",
  "url": "chap_null_space.html#act_3_b_2",
  "type": "Activity",
  "number": "13.3",
  "title": "",
  "body": "\n      As a span of a set of vectors,\n      we know that   is a subspace of   for an appropriate value of  .\n     \n            Let  .\n            The space   is a subspace of   for some positive integer  .\n            What is   in this case?\n           \n            If   is an   matrix,\n            then   is a subspace of   for some positive integer  .\n            What is   in this case?\n           \n            Recall that a matrix transformation   given by   where   is an\n              matrix is onto if for every   in   there exists a   in   for which  .\n            How does   being onto relate to the  ?\n           "
},
{
  "id": "thm_3_b_onto_range",
  "level": "2",
  "url": "chap_null_space.html#thm_3_b_onto_range",
  "type": "Theorem",
  "number": "13.5",
  "title": "",
  "body": "\n        A matrix transformation   from   to   defined by\n          is onto if and only if\n         .\n       "
},
{
  "id": "definition-33",
  "level": "2",
  "url": "chap_null_space.html#definition-33",
  "type": "Definition",
  "number": "13.6",
  "title": "",
  "body": "row space row space "
},
{
  "id": "act_3_b_3",
  "level": "2",
  "url": "chap_null_space.html#act_3_b_3",
  "type": "Activity",
  "number": "13.4",
  "title": "",
  "body": "\n      In this activity we see how to find a basis for   and   for a specific matrix  .\n      Let\n       .\n     \n      Assume that the reduced row echelon form of   is\n       .\n     \n            First we examine  .\n            Recall that to find a minimal spanning set of a set of vectors\n              in   we just select the pivot columns of the matrix  .\n           \n                  Find a basis for  .\n                 \n                  Does   equal  ?\n                  Explain.\n                 \n            Now we look at  .\n           \n                  Write the general solution to the homogeneous system   in vector form.\n                 \n                  Find a spanning set for  .\n                 \n                  Find a basis for  .\n                  Explain how you know you have a basis.\n                 "
},
{
  "id": "theorem-28",
  "level": "2",
  "url": "chap_null_space.html#theorem-28",
  "type": "Theorem",
  "number": "13.7",
  "title": "The Invertible Matrix Theorem.",
  "body": "The Invertible Matrix Theorem \n        Let   be an   matrix.\n        The following statements are equivalent.\n         \n             \n              The matrix   is an invertible matrix.\n             \n           \n             \n              The matrix equation   has only the trivial solution.\n             \n           \n             \n              The matrix   has   pivot columns.\n             \n           \n             \n              Every row of   contains a pivot.\n             \n           \n             \n              The columns of   span  .\n             \n           \n             \n              The matrix   is row equivalent to the identity matrix  .\n             \n           \n             \n              The columns of   are linearly independent.\n             \n           \n             \n              The columns of   form a basis for  .\n             \n           \n             \n              The matrix transformation   from   to   defined by   is one-to-one.\n             \n           \n             \n              The matrix equation   has exactly one solution for each vector   in  .\n             \n           \n             \n              The matrix transformation   from   to   defined by   is onto.\n             \n           \n             \n              There is an   matrix   so that  .\n             \n           \n             \n              There is an   matrix   so that  .\n             \n           \n             \n              The scalar 0 is not an eigenvalue of  .\n             \n           \n             \n              The matrix   is invertible.\n             \n           \n             \n               .\n             \n           \n             \n               .\n             \n           \n       "
},
{
  "id": "example-26",
  "level": "2",
  "url": "chap_null_space.html#example-26",
  "type": "Example",
  "number": "13.8",
  "title": "",
  "body": "\n              Let  .\n             \n                    Find a basis for  .\n                   \n                    Technology shows that the reduced row echelon form of   is\n                     .\n                    The first two columns of   are the pivot columns of  .\n                    Since the pivot columns of   form a basis for  ,\n                    a basis for   is\n                     .\n                   \n                    We use  .\n                   \n                    Let   and  .\n                    Since neither   nor   is a scalar multiple of the other,\n                    we see that   is the span of two linearly independent vectors in  .\n                    Thus, we conclude that   is the plane in   through the origin and the points   and  .\n                   \n                    Describe   geometrically (e.g., as a line, a plane,\n                    a union of lines, etc.) in the appropriate larger space.\n                   \n                    \n                   \n              Let  .\n             \n                    Find a basis for  .\n                   \n                    We use  .\n                   \n                    Technology shows that the reduced row echelon form of   is\n                     .\n                    To find a basis for  ,\n                    we must solve the homogeneous equation  .\n                    If   and  ,\n                    the reduced row echelon form of   shows that   is free,\n                     , and  .\n                    So\n                     .\n                    Thus, a basis for   is\n                     .\n                   \n                    Describe   geometrically (e.g., as a line, a plane,\n                    a union of lines, etc.) in the appropriate larger space.\n                   \n                    We use  .\n                   \n                    Since   is the span of one nonzero vector in  ,\n                    we conclude that   is the line in   through the origin and the point  .\n                   "
},
{
  "id": "example-27",
  "level": "2",
  "url": "chap_null_space.html#example-27",
  "type": "Example",
  "number": "13.9",
  "title": "",
  "body": "\n        Let  ,\n        and let   be the matrix transformation defined by  .\n       \n              What are the domain and codomain of  ?\n              Why?\n             \n              Recall that   is a linear combination of the columns of   with weights from  .\n              So   is defined only when the number of components of   is equal to the number of columns of  .\n              This explains why the domain of   is  .\n              Also, since each output of   is a linear combination of the columns of  ,\n              the codomain of   is  .\n             \n              Find all vectors   such that  .\n              How is this set of vectors related to  ?\n              Explain.\n             \n              The set of vectors   such that\n                is the same as  .\n              The reduced row echelon form of   is\n               .\n              Since both columns of   are pivot columns,\n              the columns of   are linearly independent.\n              This implies that   has only the trivial solution.\n              Therefore, the only vector   such that\n                is the zero vector in  .\n             \n              Is   one-to-one?\n              Explain.\n             \n              The previous part shows that  .\n              This means that   is one-to-one by  .\n             \n              Is   onto?\n              If yes, explain why.\n              If no, find a basis for the range of  .\n             \n              Recall that the range of   is the same as  .\n              The reduced row echelon form of   has a row of zeros,\n              so   is not consistent for every   in  .\n              We conclude that   is not onto.\n              To find a basis for the range of  ,\n              we just need to find a basis for  .\n              The pivot columns of   form such a basis,\n              so a basis for the range of   is\n               .\n             "
},
{
  "id": "p-2290",
  "level": "2",
  "url": "chap_null_space.html#p-2290",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "closed closed "
},
{
  "id": "exercise-121",
  "level": "2",
  "url": "chap_null_space.html#exercise-121",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Find a basis for the null space and column space of the matrix\n         .\n        Of which spaces are the null and column spaces of   subspaces?\n       \n        A basis for   is  ,\n        a basis for   is  .\n       "
},
{
  "id": "exercise-122",
  "level": "2",
  "url": "chap_null_space.html#exercise-122",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        If the column space of   has basis\n         ,\n        what must   be?\n       "
},
{
  "id": "exercise-123",
  "level": "2",
  "url": "chap_null_space.html#exercise-123",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        If the null space of   has basis  ,\n        what must   and   be?\n       \n          and  \n       "
},
{
  "id": "exercise-124",
  "level": "2",
  "url": "chap_null_space.html#exercise-124",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Find a matrix with at least four non-zero and distinct columns for which the column space has basis  .\n       "
},
{
  "id": "exercise-125",
  "level": "2",
  "url": "chap_null_space.html#exercise-125",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Find a matrix with at least two rows whose null space has basis  .\n       \n         \n       "
},
{
  "id": "exercise-126",
  "level": "2",
  "url": "chap_null_space.html#exercise-126",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Find a matrix whose column space has basis\n          and whose null space has basis  .\n       "
},
{
  "id": "exercise-127",
  "level": "2",
  "url": "chap_null_space.html#exercise-127",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        If possible,\n        find a   matrix whose column space does not equal   but whose null space equals  .\n        Explain your answer.\n        If not possible, explain why not.\n       \n        Not possible\n       "
},
{
  "id": "exercise-128",
  "level": "2",
  "url": "chap_null_space.html#exercise-128",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              For a   matrix,\n              the null space contains vectors other than the zero vector.\n             \n              T\n             True\/False \n              For a   matrix,\n              the null space contains vectors other than the zero vector.\n             True\/False \n              If Nul   is not the zero subspace,\n              then the transformation   is not one-to-one.\n             \n              T\n             True\/False \n              If the transformation\n                is onto where   is an\n                matrix, then Col  .\n             True\/False \n              For a   matrix  , Col   cannot equal  .\n             \n              T\n             True\/False \n              For a   matrix  , Col   cannot equal  .\n             True\/False \n              The null space of the matrix\n                consists of the two vectors\n                and  .\n             \n              F\n             True\/False \n              A basis for the null space of the matrix\n                consists of the two vectors\n                and  .\n             True\/False \n              There does not exist a matrix whose null space equals its column space.\n             \n              F\n             True\/False \n              The column space of every\n                matrix is   and its null space is  .\n             "
},
{
  "id": "p-2329",
  "level": "2",
  "url": "chap_null_space.html#p-2329",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "configuration "
},
{
  "id": "F_LO",
  "level": "2",
  "url": "chap_null_space.html#F_LO",
  "type": "Figure",
  "number": "13.10",
  "title": "",
  "body": "Two representations of the Lights Out game. "
},
{
  "id": "project-38",
  "level": "2",
  "url": "chap_null_space.html#project-38",
  "type": "Project Activity",
  "number": "13.5",
  "title": "",
  "body": "\n      Let   be the move of pressing the  th square for   from 1 to 25.\n     \n            Find vector representations for   and  .\n           \n            Let  .\n            Explain why   for each   and  .\n            In other words, explain why  .\n            (Such a matrix is called a  symmetric  matrix.)\n           "
},
{
  "id": "act_LO_1",
  "level": "2",
  "url": "chap_null_space.html#act_LO_1",
  "type": "Project Activity",
  "number": "13.6",
  "title": "",
  "body": "\n            What happens if we press the  th square twice in a row?\n            Explain in terms of the action and the game and verify using vector addition.\n           \n            Explain why applying move   then move   is the same as applying move  ,\n            then  .\n           \n            Explain how the answers to the previous two questions show that to play the game we only need to determine which buttons to press\n            (and only once each)\n            without worrying about the order in which the buttons are pressed.\n           "
},
{
  "id": "project-40",
  "level": "2",
  "url": "chap_null_space.html#project-40",
  "type": "Project Activity",
  "number": "13.7",
  "title": "",
  "body": "\n      Explain why   has the equivalent matrix equation\n       ,\n      where  , or\n       .\n     \n      Explicitly identify the vector  .\n      Also, explain why   is on the right side of this equation.\n     "
},
{
  "id": "act_LO_2",
  "level": "2",
  "url": "chap_null_space.html#act_LO_2",
  "type": "Project Activity",
  "number": "13.8",
  "title": "",
  "body": "\n      For this activity you may use the fact that the reduced row echelon form of the matrix  \n      (using algebra in  )\n      is as shown below.\n     \n            Find a basis for the column space of  .\n           \n            Explain why not every Lights Out puzzle can be solved.\n            That is, explain why there are some initial configurations of lights on and off for which it is not possible to turn out all the lights\n            (without turning off the game).\n            Relate this to the column space of  .\n           "
},
{
  "id": "p-2357",
  "level": "2",
  "url": "chap_null_space.html#p-2357",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "orthogonal "
},
{
  "id": "thm_LO_Nul_Col",
  "level": "2",
  "url": "chap_null_space.html#thm_LO_Nul_Col",
  "type": "Theorem",
  "number": "13.11",
  "title": "",
  "body": "\n        Let   be an   matrix.\n        If   is any vector in   and   is any vector in  ,\n        then  .\n       "
},
{
  "id": "project-42",
  "level": "2",
  "url": "chap_null_space.html#project-42",
  "type": "Project Activity",
  "number": "13.9",
  "title": "",
  "body": "\n            Find a basis for the null space of  . (Recall that  ,\n            so you can use the reduced row echelon form of  \n            (using algebra in  )\n            given earlier.)\n           \n            Use   \n            to show that if   is an initial Lights Out configuration that is solvable,\n            then   must be orthogonal to each of the vectors in a basis for  .\n            Then show that if   is a solvable initial Lights Out configuration,\n              must satisfy\n             \n            and\n             .\n            Be very specific in your explanation.\n           "
},
{
  "id": "project-43",
  "level": "2",
  "url": "chap_null_space.html#project-43",
  "type": "Project Activity",
  "number": "13.10",
  "title": "",
  "body": "\n      Now that we know which Lights Out games can be solved,\n      let   be an initial configuration to a solvable Lights Out game.\n      Explain how to find a solution to this game.\n      Will the solution be unique?\n      Explain.\n     "
},
{
  "id": "chap_eigenspaces",
  "level": "1",
  "url": "chap_eigenspaces.html",
  "type": "Section",
  "number": "14",
  "title": "Eigenspaces of a Matrix",
  "body": "Eigenspaces of a Matrix \n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is an eigenspace of a matrix?\n           \n         \n           \n            How do we find a basis for an eigenspace of a matrix?\n           \n         \n           \n            What is true about any set of eigenvectors for a matrix that correspond to different eigenvalues?\n           \n         Application: Population Dynamics \n    The study of population dynamics   how and why people move from one place to another   is important to economists.\n    The movement of people corresponds to the movement of money,\n    and money makes the economy go.\n    As an example,\n    we might consider a simple model of population migration to and from the state of Michigan.\n   \n    According to the\n     \n     \n    Michigan Department of Technology, Management, and Budget, from 2011 to 2012,\n    approximately 0.05% of the U.S. population outside of Michigan moved to the state of Michigan,\n    while approximately 2% of Michigan's population moved out of Michigan.\n    A reasonable question to ask about this situation is,\n    if these numbers don't change,\n    what is the long-term distribution of the US population inside and outside of Michigan\n    (under the assumption that the total US population doesn't change.).\n    The answer to this question involves eigenvalues and eigenvectors of a matrix.\n    More details can be found later in this section.\n   Introduction \n      Consider the matrix transformation   from   to   defined by  , where\n       .\n     \n      We are interested in understanding what this matrix transformation does to vectors in  .\n      The matrix   has eigenvalues   and\n        with corresponding eigenvectors\n        and  .\n     \n            Explain why   and   are linearly independent.\n           \n            Explain why any vector   in   can be written uniquely as a linear combination of   and  .\n           \n            We now consider the action of the matrix transformation   on a linear combination of   and  .\n            Explain why\n             .\n           \n    Equation   illustrates that it would be convenient to view the action of   in the coordinate system where\n      serves as the  -axis and\n      as the  -axis.\n    In this case,\n    we can visualize that when we apply the transformation   to a vector\n      in   the result is an output vector is scaled by a factor of   in the   direction and by a factor of   in the   direction.\n    For example, consider the box with vertices at  ,\n     ,  ,\n    and   as shown at left in  .\n    The transformation   stretches this box by a factor of   in the   direction and a factor of   in the   direction as illustrated at right in  .\n    In this situation,\n    the eigenvalues and eigenvectors provide the most convenient perspective through which to visualize the action of the transformation  .\n    Here,   and\n      are the eigenspaces of the matrix  .\n   A box and a transformed box. \n    This geometric perspective illustrates how each the span of each eigenvalue of   tells us something important about  .\n    In this section we explore the idea of eigenvalues and spaces defined by eigenvectors in more detail.\n   Eigenspaces of Matrix \n    Recall that the eigenvectors of an\n      matrix   satisfy the equation\n     \n    for some scalar  .\n    Equivalently,\n    the eigenvectors of   with eigenvalue   satisfy the equation\n     .\n   eigenspaces eigenspace eigenspace \n      The matrix   has two distinct eigenvalues.\n     \n            Find a basis for the eigenspace of   corresponding to the eigenvalue  .\n            In other words, find a basis for  .\n           \n            Find a basis for the eigenspace of   corresponding to the eigenvalue  .\n           \n            Is it true that if   and   are two distinct eigenvectors for  ,\n            that   and   are linearly independent?\n            Explain.\n           \n            Is it possible to have two linearly independent eigenvectors corresponding to the same eigenvalue?\n           \n            Is it true that if   and   are two distinct eigenvectors corresponding to different eigenvalues for  ,\n            that   and   are linearly independent?\n            Explain.\n           \n    If we know an eigenvalue   of an   matrix  ,\n     \n    shows us how to find a basis for the corresponding eigenspace   just row reduce\n      to find a basis for  .\n    To this point we have always been given eigenvalues for our matrices,\n    and have not seen how to find these eigenvalues.\n    That process will come a bit later.\n    For now, we just want to become more familiar with eigenvalues and eigenvectors.\n    The next activity should help connect eigenvalues to ideas we have discussed earlier.\n   \n      Let   be an   matrix with eigenvalue  .\n     \n            How many solutions does the equation   have?\n            Explain.\n           \n            Can   have a pivot in every column?\n            Why or why not?\n           \n            Can   have a pivot in every row?\n            Why or why not?\n           \n            Can the columns of   be linearly independent?\n            Why or why not?\n           Linearly Independent Eigenvectors \n    An important question we will want to answer about a matrix is how many linearly independent eigenvectors the matrix has.\n     \n    shows that eigenvectors for the same eigenvalue may be linearly dependent or independent,\n    but all of our examples so far seem to indicate that eigenvectors corresponding to different eigenvalues are linearly independent.\n    This turns out to be universally true as our next theorem demonstrates.\n    The next activity should help prepare us for the proof of this theorem\n   \n      Let   and   be distinct eigenvalues of a matrix   with corresponding eigenvectors   and  .\n      The goal of this activity is to demonstrate that   and   are linearly independent.\n      To prove that   and   are linearly independent,\n      suppose that\n       .\n     \n            Multiply both sides of equation   on the left by the matrix   and show that\n             .\n           \n            Now multiply both sides of equation   by the scalar   and show that\n             .\n           \n            Combine equations   and   to obtain the equation\n             .\n           \n            Explain how we can conclude that  .\n            Why does it follow that  ?\n            What does this tell us about   and  ?\n           \n     \n    contains the basic elements of the proof of the next theorem.\n   \n        Let  ,  ,  ,\n          be   distinct eigenvalues for a matrix   and for each   between 1 and   let   be an eigenvector of   with eigenvalue  .\n        Then the vectors  ,\n         ,\n         ,   are linearly independent.\n       \n      Let   be a matrix with   distinct eigenvalues  ,\n       ,  ,\n        and corresponding eigenvectors  ,\n       ,\n       ,  .\n      To understand why  ,\n       ,\n       ,   are linearly independent,\n      we will argue by contradiction and suppose that the vectors  ,\n       ,\n       ,   are linearly dependent.\n      Note that   cannot be the zero vector (why?), so the set\n        is linearly independent.\n      If we include   into this set,\n      the set   may be linearly independent or dependent.\n      If   is linearly independent,\n      then the set   may be linearly independent or dependent.\n      We can continue adding additional vectors until we reach the set\n        which we are assuming is linearly dependent.\n      So there must be a smallest integer\n        such that the set   is linearly dependent while   is linearly independent.\n      Since   is linearly dependent,\n      there is a linear combination of  ,\n       ,  ,\n        with weights not all 0 that is the zero vector.\n      Let  ,  ,\n       ,   be such weights, not all zero, so that\n       \n     \n      If we multiply both sides of   on the left by the matrix   we obtain\n       .\n     \n      If we multiply both sides of   by\n        we obtain the equation\n       .\n     \n      Subtracting corresponding sides of equation   from   gives us\n       .\n     \n      Recall that   is a linearly independent set,\n      so the only way a linear combination of vectors in   can be   is if all of the weights are 0.\n      Therefore, we must have\n       .\n     \n      Since the eigenvalues are all distinct, this can only happen if\n       .\n     \n      But equation   then implies that   and so all of the weights  ,\n       ,\n       ,   are 0.\n      However, when we assumed that the eigenvectors  ,\n       ,\n       ,   were linearly dependent,\n      this led to having at least one of the weights  ,\n       ,\n       ,   be nonzero.\n      This cannot happen,\n      so our assumption that the eigenvectors  ,\n       ,  ,\n        were linearly dependent must be false and we conclude that the eigenvectors  ,\n       ,\n       ,   are linearly independent.\n     Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Let   and let   be the matrix transformation defined by  .\n       \n              Show that   is an eigenvalue for   and find a basis for the corresponding eigenspace of  .\n             \n              Recall that   is an eigenvalue of   if   is not invertible.\n              To show that   is an eigenvalue for   we row reduce the matrix\n               \n              to  .\n              Since the third column of   is not a pivot column,\n              the matrix   is not invertible.\n              We conclude that   is an eigenvalue of  .\n              The eigenspace of   for the eigenvalue   is  .\n              The reduced row echelon form of   shows that if\n                and  ,\n              then   is free,  , and  .\n              Thus,\n               .\n              Therefore,   is a basis for the eigenspace of   corresponding to the eigenvalue  .\n             \n              Geometrically describe the eigenspace of   corresponding to the eigenvalue  .\n              Explain what the transformation   does to this eigenspace.\n             \n              Since the eigenspace of   corresponding to the eigenvalue   is the span of a single nonzero vector  ,\n              this eigenspace is the line in   through the origin and the point  .\n              Any vector in this eigenspace has the form   for some scalar  .\n              Notice that\n               ,\n              so   expands any vector in this eigenspace by a factor of 4.\n             \n              Show that   is an eigenvalue for   and find a basis for the corresponding eigenspace of  .\n             \n              To show that   is an eigenvalue for   we row reduce the matrix\n               \n              to  .\n              Since the third column of   is not a pivot column,\n              the matrix   is not invertible.\n              We conclude that   is an eigenvalue of  .\n              The eigenspace of   for the eigenvalue   is  .\n              The reduced row echelon form of   shows that if\n                and  ,\n              then   and   are free, and  .\n              Thus,\n               .\n              Therefore,   is a basis for the eigenspace of   corresponding to the eigenvalue  .\n             \n              Geometrically describe the eigenspace of   corresponding to the eigenvalue  .\n              Explain what the transformation   does to this eigenspace.\n             \n              Since the eigenspace of   corresponding to the eigenvalue   is the span of two linearly independent vectors\n                and  ,\n              this eigenspace is the plane in   through the origin and the points   and  .\n              Any vector in this eigenspace has the form\n                for some scalars   and  .\n              Notice that\n               ,\n              so   fixes every vector in this plane.\n             \n              Let  .\n              Note that the vector   satisfies  .\n             \n                    Show that   is an eigenvector of  .\n                    What is the corresponding eigenvalue?\n                   \n                    We use the fact that   is an eigenvector of the matrix   with eigenvalue  .\n                   \n                    We have that\n                     .\n                    So   is an eigenvector of   with eigenvalue  .\n                   \n                    Show that   is an eigenvector of  .\n                    What is the corresponding eigenvalue?\n                   \n                    We use the fact that   is an eigenvector of the matrix   with eigenvalue  .\n                   \n                    We have that\n                     .\n                    So   is an eigenvector of   with eigenvalue  .\n                   \n                    Show that   is an eigenvector of  .\n                    What is the corresponding eigenvalue?\n                   \n                    We use the fact that   is an eigenvector of the matrix   with eigenvalue  .\n                   \n                    We have that\n                     .\n                    So   is an eigenvector of   with eigenvalue  .\n                   \n                    If   is a positive integer,\n                    do you expect that   is an eigenvector of  ?\n                    If so, what do you think is the corresponding eigenvalue?\n                   \n                    We use the fact that   is an eigenvector of the matrix   with eigenvalue  .\n                   \n                    The results of the previous parts of this example indicate that  ,\n                    or that   is an eigenvector of   with corresponding eigenvalue  .\n                   \n              The result of part (a) is true in general.\n              Let   be an   matrix with eigenvalue   and corresponding eigenvector  .\n             \n                    Show that   is an eigenvalue of   with eigenvector  .\n                   \n                    Let   be an   matrix with eigenvalue   and corresponding eigenvector  .\n                   \n                    We have that\n                     .\n                    So   is an eigenvector of   with eigenvalue  .\n                   \n                    Show that   is an eigenvalue of   with eigenvector  .\n                   \n                    Let   be an   matrix with eigenvalue   and corresponding eigenvector  .\n                   \n                    We have that\n                     .\n                    So   is an eigenvector of   with eigenvalue  .\n                   \n                    Suppose that   is an eigenvalue of   with eigenvector   for some integer  .\n                    Show then that   is an eigenvalue of   with eigenvector  .\n                    This argument shows that   is an eigenvalue of   with eigenvector   for any positive integer  .\n                   \n                    Let   be an   matrix with eigenvalue   and corresponding eigenvector  .\n                   \n                    Assume that  .\n                    Then\n                     .\n                    So   is an eigenvector of   with eigenvalue  .\n                   \n              We now investigate the eigenvalues of a special type of matrix.\n             matrix nilpotent nilpotent \n                    Now we investigate a special type of matrix.\n                   \n                    Straightforward calculations show that  .\n                    Since   is an upper triangular matrix,\n                    the eigenvalues of   are the entries on the diagonal.\n                    That is, the only eigenvalue of   is  .\n                   \n                    Show that the only eigenvalue of a nilpotent matrix is  .\n                   \n                    Assume that   is a nilpotent matrix.\n                    Suppose that   is an eigenvalue of   with corresponding eigenvector  .\n                    Since   is a nilpotent matrix,\n                    there is a positive integer   such that  .\n                    But   is an eigenvalue of   with eigenvector  .\n                    The only eigenvalue of the zero matrix is  , so  .\n                    This implies that  .\n                    We conclude that the only eigenvalue of a nilpotent matrix is  .\n                   Summary \n       \n        An eigenspace of an   matrix   corresponding to an eigenvalue   of   is the null space of  .\n       \n     \n       \n        To find a basis for an eigenspace of a matrix   corresponding to an eigenvalue  ,\n        we row reduce   and find a basis for  .\n       \n     \n       \n        Eigenvectors corresponding to different eigenvalues are always linearly independent.\n       \n     \n        For each of the following,\n        find a basis for the eigenspace of the indicated matrix corresponding to the given eigenvalue.\n       \n                with eigenvalue 3\n             \n               \n             \n                with eigenvalue 2\n             \n               \n             \n                with eigenvalue 1\n             \n               \n             \n                with eigenvalue 2\n             \n               \n             \n                with eigenvalue 1\n             \n               \n             \n                with eigenvalue 0\n             \n               \n             \n        Suppose   is an invertible matrix.\n       \n              Use the definition of an eigenvalue and an eigenvector to algebraically explain why if   is an eigenvalue of  ,\n              then   is an eigenvalue of  .\n             \n              To provide an alternative explanation to the result in the previous part,\n              let   be an eigenvector of   corresponding to  .\n              Consider the matrix transformation   corresponding to   and\n                corresponding to  .\n              Considering what happens to   if   and then   are applied,\n              describe why this justifies   is also an eigenvector of  .\n             \n        If   has two eigenvalues 4 and 6, what are the values of   and  ?\n       \n         ,  \n       \n              What are the eigenvalues of the identity matrix  ?\n              Describe each eigenspace.\n             \n              Now let   be a positive integer.\n              What are the eigenvalues of the identity matrix  ?\n              Describe each eigenspace.\n             \n              What are the eigenvalues of the\n                zero matrix (the matrix all of whose entries are 0)?\n              Describe each eigenspace.\n             \n              Eigenvalue  , eigenspace  \n             \n              Now let   be a positive integer.\n              What are the eigenvalues of the   zero matrix?\n              Describe each eigenspace.\n             \n              Eigenvalue  , eigenspace  \n             \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If  ,\n              then   is an eigenvalue of   with eigenvector  .\n             \n              F\n             True\/False \n              The scalar   is an eigenvalue of a square matrix   if and only if the equation\n                has a nontrivial solution.\n             True\/False \n              If   is an eigenvalue of a matrix  ,\n              then there is only one nonzero vector   with  .\n             \n              F\n             True\/False \n              The eigenspace of an eigenvalue of an\n                matrix   is the same as  .\n             True\/False \n              If   and   are eigenvectors of a matrix   corresponding to the same eigenvalue  ,\n              then   is also an eigenvector of  .\n             \n              T\n             True\/False \n              If   and   are eigenvectors of a matrix  ,\n              then   is also an eigenvector of  .\n             True\/False \n              If   is an eigenvector of an invertible matrix  ,\n              then   is also an eigenvector of  .\n             \n              T\n             Project: Modeling Population Migration \n    As introduced earlier,\n    data from the Michigan Department of Technology, Management,\n    and Budget shows that from 2011 to 2012,\n    approximately 0.05% of the U.S. population outside of Michigan moved to the state of Michigan,\n    while approximately 2% of Michigan's population moved out of Michigan.\n    We are interested in determining the long-term distribution of population in Michigan.\n   \n    Let   be the\n      vector where   is the population of Michigan and   is the U.S. population outside of Michigan in year  .\n    Assume that we start our analysis at generation 0 and  .\n   \n            Explain how the data above shows that\n             \n           \n            Identify the matrix   such that  .\n           \n    One we have the equation  ,\n    we can extend it to subsequent years:\n     \n    for each  .\n   \n    This example illustrates the general nature of what is called a  Markov process \n    (see  ).\n    Recall that the matrix   that provides the link from one generation to the next is called the transition matrix.\n   \n    In situations like these,\n    we are interested in determining if there is a steady-state vector,\n    that is a vector that satisfies\n     .\n   \n    Such a vector would show us the long-term population of Michigan provided the population dynamics do not change.\n   \n            Explain why a steady-state solution to   is an eigenvector of  .\n            What is the corresponding eigenvalue?\n           \n            Consider again the transition matrix   from  .\n            Recall that the solutions to equation   are all the vectors in  .\n            In other words,\n            the eigenvectors of   for this eigenvalue are the nonzero vectors in  .\n            Find a basis for the eigenspace of   corresponding to this eigenvalue.\n            Use whatever technology is appropriate.\n           \n            Once we know a basis for the eigenspace of the transition matrix  ,\n            we can use it to estimate the steady-state population of Michigan\n            (assuming the stated migration trends are valid long-term).\n            According to the  US Census Bureau ,\n            the resident US population on December 1, 2019 was 330,073,471.\n            Assuming no population growth in the U.S., what would the long-term population of Michigan be?\n            How realistic do you think this is?\n           "
},
{
  "id": "objectives-14",
  "level": "2",
  "url": "chap_eigenspaces.html#objectives-14",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is an eigenspace of a matrix?\n           \n         \n           \n            How do we find a basis for an eigenspace of a matrix?\n           \n         \n           \n            What is true about any set of eigenvectors for a matrix that correspond to different eigenvalues?\n           \n         "
},
{
  "id": "pa_3_c_1",
  "level": "2",
  "url": "chap_eigenspaces.html#pa_3_c_1",
  "type": "Preview Activity",
  "number": "14.1",
  "title": "",
  "body": "\n      Consider the matrix transformation   from   to   defined by  , where\n       .\n     \n      We are interested in understanding what this matrix transformation does to vectors in  .\n      The matrix   has eigenvalues   and\n        with corresponding eigenvectors\n        and  .\n     \n            Explain why   and   are linearly independent.\n           \n            Explain why any vector   in   can be written uniquely as a linear combination of   and  .\n           \n            We now consider the action of the matrix transformation   on a linear combination of   and  .\n            Explain why\n             .\n           "
},
{
  "id": "F_3_c_1",
  "level": "2",
  "url": "chap_eigenspaces.html#F_3_c_1",
  "type": "Figure",
  "number": "14.1",
  "title": "",
  "body": "A box and a transformed box. "
},
{
  "id": "p-2378",
  "level": "2",
  "url": "chap_eigenspaces.html#p-2378",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "eigenspaces "
},
{
  "id": "definition-34",
  "level": "2",
  "url": "chap_eigenspaces.html#definition-34",
  "type": "Definition",
  "number": "14.2",
  "title": "",
  "body": "eigenspace eigenspace "
},
{
  "id": "act_3_c_1",
  "level": "2",
  "url": "chap_eigenspaces.html#act_3_c_1",
  "type": "Activity",
  "number": "14.2",
  "title": "",
  "body": "\n      The matrix   has two distinct eigenvalues.\n     \n            Find a basis for the eigenspace of   corresponding to the eigenvalue  .\n            In other words, find a basis for  .\n           \n            Find a basis for the eigenspace of   corresponding to the eigenvalue  .\n           \n            Is it true that if   and   are two distinct eigenvectors for  ,\n            that   and   are linearly independent?\n            Explain.\n           \n            Is it possible to have two linearly independent eigenvectors corresponding to the same eigenvalue?\n           \n            Is it true that if   and   are two distinct eigenvectors corresponding to different eigenvalues for  ,\n            that   and   are linearly independent?\n            Explain.\n           "
},
{
  "id": "act_3_c_2",
  "level": "2",
  "url": "chap_eigenspaces.html#act_3_c_2",
  "type": "Activity",
  "number": "14.3",
  "title": "",
  "body": "\n      Let   be an   matrix with eigenvalue  .\n     \n            How many solutions does the equation   have?\n            Explain.\n           \n            Can   have a pivot in every column?\n            Why or why not?\n           \n            Can   have a pivot in every row?\n            Why or why not?\n           \n            Can the columns of   be linearly independent?\n            Why or why not?\n           "
},
{
  "id": "act_3_c_3",
  "level": "2",
  "url": "chap_eigenspaces.html#act_3_c_3",
  "type": "Activity",
  "number": "14.4",
  "title": "",
  "body": "\n      Let   and   be distinct eigenvalues of a matrix   with corresponding eigenvectors   and  .\n      The goal of this activity is to demonstrate that   and   are linearly independent.\n      To prove that   and   are linearly independent,\n      suppose that\n       .\n     \n            Multiply both sides of equation   on the left by the matrix   and show that\n             .\n           \n            Now multiply both sides of equation   by the scalar   and show that\n             .\n           \n            Combine equations   and   to obtain the equation\n             .\n           \n            Explain how we can conclude that  .\n            Why does it follow that  ?\n            What does this tell us about   and  ?\n           "
},
{
  "id": "thm_4_b_lin_indep_evects",
  "level": "2",
  "url": "chap_eigenspaces.html#thm_4_b_lin_indep_evects",
  "type": "Theorem",
  "number": "14.3",
  "title": "",
  "body": "\n        Let  ,  ,  ,\n          be   distinct eigenvalues for a matrix   and for each   between 1 and   let   be an eigenvector of   with eigenvalue  .\n        Then the vectors  ,\n         ,\n         ,   are linearly independent.\n       \n      Let   be a matrix with   distinct eigenvalues  ,\n       ,  ,\n        and corresponding eigenvectors  ,\n       ,\n       ,  .\n      To understand why  ,\n       ,\n       ,   are linearly independent,\n      we will argue by contradiction and suppose that the vectors  ,\n       ,\n       ,   are linearly dependent.\n      Note that   cannot be the zero vector (why?), so the set\n        is linearly independent.\n      If we include   into this set,\n      the set   may be linearly independent or dependent.\n      If   is linearly independent,\n      then the set   may be linearly independent or dependent.\n      We can continue adding additional vectors until we reach the set\n        which we are assuming is linearly dependent.\n      So there must be a smallest integer\n        such that the set   is linearly dependent while   is linearly independent.\n      Since   is linearly dependent,\n      there is a linear combination of  ,\n       ,  ,\n        with weights not all 0 that is the zero vector.\n      Let  ,  ,\n       ,   be such weights, not all zero, so that\n       \n     \n      If we multiply both sides of   on the left by the matrix   we obtain\n       .\n     \n      If we multiply both sides of   by\n        we obtain the equation\n       .\n     \n      Subtracting corresponding sides of equation   from   gives us\n       .\n     \n      Recall that   is a linearly independent set,\n      so the only way a linear combination of vectors in   can be   is if all of the weights are 0.\n      Therefore, we must have\n       .\n     \n      Since the eigenvalues are all distinct, this can only happen if\n       .\n     \n      But equation   then implies that   and so all of the weights  ,\n       ,\n       ,   are 0.\n      However, when we assumed that the eigenvectors  ,\n       ,\n       ,   were linearly dependent,\n      this led to having at least one of the weights  ,\n       ,\n       ,   be nonzero.\n      This cannot happen,\n      so our assumption that the eigenvectors  ,\n       ,  ,\n        were linearly dependent must be false and we conclude that the eigenvectors  ,\n       ,\n       ,   are linearly independent.\n     "
},
{
  "id": "example-28",
  "level": "2",
  "url": "chap_eigenspaces.html#example-28",
  "type": "Example",
  "number": "14.4",
  "title": "",
  "body": "\n        Let   and let   be the matrix transformation defined by  .\n       \n              Show that   is an eigenvalue for   and find a basis for the corresponding eigenspace of  .\n             \n              Recall that   is an eigenvalue of   if   is not invertible.\n              To show that   is an eigenvalue for   we row reduce the matrix\n               \n              to  .\n              Since the third column of   is not a pivot column,\n              the matrix   is not invertible.\n              We conclude that   is an eigenvalue of  .\n              The eigenspace of   for the eigenvalue   is  .\n              The reduced row echelon form of   shows that if\n                and  ,\n              then   is free,  , and  .\n              Thus,\n               .\n              Therefore,   is a basis for the eigenspace of   corresponding to the eigenvalue  .\n             \n              Geometrically describe the eigenspace of   corresponding to the eigenvalue  .\n              Explain what the transformation   does to this eigenspace.\n             \n              Since the eigenspace of   corresponding to the eigenvalue   is the span of a single nonzero vector  ,\n              this eigenspace is the line in   through the origin and the point  .\n              Any vector in this eigenspace has the form   for some scalar  .\n              Notice that\n               ,\n              so   expands any vector in this eigenspace by a factor of 4.\n             \n              Show that   is an eigenvalue for   and find a basis for the corresponding eigenspace of  .\n             \n              To show that   is an eigenvalue for   we row reduce the matrix\n               \n              to  .\n              Since the third column of   is not a pivot column,\n              the matrix   is not invertible.\n              We conclude that   is an eigenvalue of  .\n              The eigenspace of   for the eigenvalue   is  .\n              The reduced row echelon form of   shows that if\n                and  ,\n              then   and   are free, and  .\n              Thus,\n               .\n              Therefore,   is a basis for the eigenspace of   corresponding to the eigenvalue  .\n             \n              Geometrically describe the eigenspace of   corresponding to the eigenvalue  .\n              Explain what the transformation   does to this eigenspace.\n             \n              Since the eigenspace of   corresponding to the eigenvalue   is the span of two linearly independent vectors\n                and  ,\n              this eigenspace is the plane in   through the origin and the points   and  .\n              Any vector in this eigenspace has the form\n                for some scalars   and  .\n              Notice that\n               ,\n              so   fixes every vector in this plane.\n             "
},
{
  "id": "example-29",
  "level": "2",
  "url": "chap_eigenspaces.html#example-29",
  "type": "Example",
  "number": "14.5",
  "title": "",
  "body": "\n              Let  .\n              Note that the vector   satisfies  .\n             \n                    Show that   is an eigenvector of  .\n                    What is the corresponding eigenvalue?\n                   \n                    We use the fact that   is an eigenvector of the matrix   with eigenvalue  .\n                   \n                    We have that\n                     .\n                    So   is an eigenvector of   with eigenvalue  .\n                   \n                    Show that   is an eigenvector of  .\n                    What is the corresponding eigenvalue?\n                   \n                    We use the fact that   is an eigenvector of the matrix   with eigenvalue  .\n                   \n                    We have that\n                     .\n                    So   is an eigenvector of   with eigenvalue  .\n                   \n                    Show that   is an eigenvector of  .\n                    What is the corresponding eigenvalue?\n                   \n                    We use the fact that   is an eigenvector of the matrix   with eigenvalue  .\n                   \n                    We have that\n                     .\n                    So   is an eigenvector of   with eigenvalue  .\n                   \n                    If   is a positive integer,\n                    do you expect that   is an eigenvector of  ?\n                    If so, what do you think is the corresponding eigenvalue?\n                   \n                    We use the fact that   is an eigenvector of the matrix   with eigenvalue  .\n                   \n                    The results of the previous parts of this example indicate that  ,\n                    or that   is an eigenvector of   with corresponding eigenvalue  .\n                   \n              The result of part (a) is true in general.\n              Let   be an   matrix with eigenvalue   and corresponding eigenvector  .\n             \n                    Show that   is an eigenvalue of   with eigenvector  .\n                   \n                    Let   be an   matrix with eigenvalue   and corresponding eigenvector  .\n                   \n                    We have that\n                     .\n                    So   is an eigenvector of   with eigenvalue  .\n                   \n                    Show that   is an eigenvalue of   with eigenvector  .\n                   \n                    Let   be an   matrix with eigenvalue   and corresponding eigenvector  .\n                   \n                    We have that\n                     .\n                    So   is an eigenvector of   with eigenvalue  .\n                   \n                    Suppose that   is an eigenvalue of   with eigenvector   for some integer  .\n                    Show then that   is an eigenvalue of   with eigenvector  .\n                    This argument shows that   is an eigenvalue of   with eigenvector   for any positive integer  .\n                   \n                    Let   be an   matrix with eigenvalue   and corresponding eigenvector  .\n                   \n                    Assume that  .\n                    Then\n                     .\n                    So   is an eigenvector of   with eigenvalue  .\n                   \n              We now investigate the eigenvalues of a special type of matrix.\n             matrix nilpotent nilpotent \n                    Now we investigate a special type of matrix.\n                   \n                    Straightforward calculations show that  .\n                    Since   is an upper triangular matrix,\n                    the eigenvalues of   are the entries on the diagonal.\n                    That is, the only eigenvalue of   is  .\n                   \n                    Show that the only eigenvalue of a nilpotent matrix is  .\n                   \n                    Assume that   is a nilpotent matrix.\n                    Suppose that   is an eigenvalue of   with corresponding eigenvector  .\n                    Since   is a nilpotent matrix,\n                    there is a positive integer   such that  .\n                    But   is an eigenvalue of   with eigenvector  .\n                    The only eigenvalue of the zero matrix is  , so  .\n                    This implies that  .\n                    We conclude that the only eigenvalue of a nilpotent matrix is  .\n                   "
},
{
  "id": "exercise-129",
  "level": "2",
  "url": "chap_eigenspaces.html#exercise-129",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        For each of the following,\n        find a basis for the eigenspace of the indicated matrix corresponding to the given eigenvalue.\n       \n                with eigenvalue 3\n             \n               \n             \n                with eigenvalue 2\n             \n               \n             \n                with eigenvalue 1\n             \n               \n             \n                with eigenvalue 2\n             \n               \n             \n                with eigenvalue 1\n             \n               \n             \n                with eigenvalue 0\n             \n               \n             "
},
{
  "id": "exercise-130",
  "level": "2",
  "url": "chap_eigenspaces.html#exercise-130",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Suppose   is an invertible matrix.\n       \n              Use the definition of an eigenvalue and an eigenvector to algebraically explain why if   is an eigenvalue of  ,\n              then   is an eigenvalue of  .\n             \n              To provide an alternative explanation to the result in the previous part,\n              let   be an eigenvector of   corresponding to  .\n              Consider the matrix transformation   corresponding to   and\n                corresponding to  .\n              Considering what happens to   if   and then   are applied,\n              describe why this justifies   is also an eigenvector of  .\n             "
},
{
  "id": "exercise-131",
  "level": "2",
  "url": "chap_eigenspaces.html#exercise-131",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        If   has two eigenvalues 4 and 6, what are the values of   and  ?\n       \n         ,  \n       "
},
{
  "id": "exercise-132",
  "level": "2",
  "url": "chap_eigenspaces.html#exercise-132",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n              What are the eigenvalues of the identity matrix  ?\n              Describe each eigenspace.\n             \n              Now let   be a positive integer.\n              What are the eigenvalues of the identity matrix  ?\n              Describe each eigenspace.\n             "
},
{
  "id": "exercise-133",
  "level": "2",
  "url": "chap_eigenspaces.html#exercise-133",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n              What are the eigenvalues of the\n                zero matrix (the matrix all of whose entries are 0)?\n              Describe each eigenspace.\n             \n              Eigenvalue  , eigenspace  \n             \n              Now let   be a positive integer.\n              What are the eigenvalues of the   zero matrix?\n              Describe each eigenspace.\n             \n              Eigenvalue  , eigenspace  \n             "
},
{
  "id": "exercise-134",
  "level": "2",
  "url": "chap_eigenspaces.html#exercise-134",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If  ,\n              then   is an eigenvalue of   with eigenvector  .\n             \n              F\n             True\/False \n              The scalar   is an eigenvalue of a square matrix   if and only if the equation\n                has a nontrivial solution.\n             True\/False \n              If   is an eigenvalue of a matrix  ,\n              then there is only one nonzero vector   with  .\n             \n              F\n             True\/False \n              The eigenspace of an eigenvalue of an\n                matrix   is the same as  .\n             True\/False \n              If   and   are eigenvectors of a matrix   corresponding to the same eigenvalue  ,\n              then   is also an eigenvector of  .\n             \n              T\n             True\/False \n              If   and   are eigenvectors of a matrix  ,\n              then   is also an eigenvector of  .\n             True\/False \n              If   is an eigenvector of an invertible matrix  ,\n              then   is also an eigenvector of  .\n             \n              T\n             "
},
{
  "id": "act_Mi_pop_1",
  "level": "2",
  "url": "chap_eigenspaces.html#act_Mi_pop_1",
  "type": "Project Activity",
  "number": "14.5",
  "title": "",
  "body": "\n            Explain how the data above shows that\n             \n           \n            Identify the matrix   such that  .\n           "
},
{
  "id": "project-45",
  "level": "2",
  "url": "chap_eigenspaces.html#project-45",
  "type": "Project Activity",
  "number": "14.6",
  "title": "",
  "body": "\n            Explain why a steady-state solution to   is an eigenvector of  .\n            What is the corresponding eigenvalue?\n           \n            Consider again the transition matrix   from  .\n            Recall that the solutions to equation   are all the vectors in  .\n            In other words,\n            the eigenvectors of   for this eigenvalue are the nonzero vectors in  .\n            Find a basis for the eigenspace of   corresponding to this eigenvalue.\n            Use whatever technology is appropriate.\n           \n            Once we know a basis for the eigenspace of the transition matrix  ,\n            we can use it to estimate the steady-state population of Michigan\n            (assuming the stated migration trends are valid long-term).\n            According to the  US Census Bureau ,\n            the resident US population on December 1, 2019 was 330,073,471.\n            Assuming no population growth in the U.S., what would the long-term population of Michigan be?\n            How realistic do you think this is?\n           "
},
{
  "id": "chap_bases_dimension",
  "level": "1",
  "url": "chap_bases_dimension.html",
  "type": "Section",
  "number": "15",
  "title": "Bases and Dimension",
  "body": "Bases and Dimension \n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section\n         \n           \n            What is the dimension of a subspace of  ?\n            What property of bases makes the dimension a well-defined number?\n           \n         \n           \n            If   is a subspace of   with dimension  ,\n            what must be true about any linearly independent subset   of   that contains exactly   vectors?\n           \n         \n           \n            If   is a subspace of   with dimension  ,\n            what must be true about any subset   of   that contains exactly   vectors and spans  ?\n           \n         \n           \n            What is the rank of a matrix?\n           \n         \n           \n            What does the Rank-Nullity Theorem say?\n           \n         Application: Lattice Based Cryptography \n    When you use your credit card,\n    you expect that the information that is transmitted is protected so others can't use your card.\n    Similarly, when you create a password for your computer or other devices,\n    you do so with the intention that it will be difficult for others to decipher.\n   \n    Cryptology is the study of methods to maintain secure communication in the presence of other parties (cryptography),\n    along with the study of breaking codes\n    (cryptanalysis).\n    In essence, cryptology is the art of keeping and breaking secrets.\n    The creation of secure codes (cryptography) can provide confidentiality\n    (ensure that information is available only to the intended recipients),\n    data integrity\n    (prevent data from being altered between the sender and recipient),\n    and authentication\n    (making sure that the information is from the correct source).\n   \n    Modern cryptology uses mathematical theory that can be implemented with computer hardware and algorithms.\n    The security of public key systems is largely based on mathematical problems that are very difficult to solve.\n    For example,\n    the security of the RSA system relies on the fact that it is computationally difficult to find prime factors of very large numbers,\n    and elliptic curve cryptography relies on the difficulty of the discrete logarithm problem for elliptic curves.\n    However, the continual increase in the power of computers threatens the security of these systems,\n    and so cryptographic systems have to keep adapting to the newest technology.\n    For example, Shor's Algorithm\n    (which could run on a quantum computer)\n    can solve the public key cryptographic systems that rely on the integer factorization problem or the discrete logarithm problem.\n    So if a working quantum computer was ever developed,\n    it would threaten the existing cryptographic systems.\n    Lattice-based cryptography is a potential source of systems that may be secure even in such an environment.\n    The security of these systems is dependent on the fact that the average case of the difficulty of certain problems in lattice theory is higher than the worst case problems that underpin current cryptosystems.\n    As we will see later in this section,\n    lattices are built on bases for subspace of  .\n   Introduction \n    A basis provides a system in which we can uniquely represent every vector in the space we are considering.\n    More specifically,\n    every vector in the space can be expressed as a linear combination of the vectors in the basis in a unique way.\n    In order to be able to cover every point in the space,\n    the basis has to span the space.\n    In order to be able to provide a unique coordinate for each point,\n    there should not be any extra vectors in the basis,\n    which is achieved by linear independence of the vectors.\n    For practical reasons,\n    a basis simplifies many problems because we only need to solve the problem for each of the basis vectors.\n    Solutions of the other cases usually follow because every vector in the space can be expressed as a unique linear combination of the basis vectors.\n   \n    Recall that a basis for a subspace   of   is a set of vectors which are linearly independent and which span  .\n   \n            For each of the following sets of vectors,\n            determine whether the vectors form a basis of  .\n            Use any appropriate technology for your computations.\n           \n                   \n                 \n                   \n                 \n                   \n                 \n                   \n                 \n            In problem (1) we should have noticed that a space can have more than one basis,\n            but that any two bases contain the same number of elements.\n            This is a critically important idea that we investigate in more detail in this problem in one specific case.\n            Assume that   is a subspace of   that has a basis\n              with two basis vectors.\n            We want to see if any other basis for   can have a different number of elements.\n            Let us now consider a set   of three vectors in  .\n            Our goal is to determine if   can be a basis for  .\n            Since   is a basis for  ,\n            any vector in   can be written as a linear combination of the vectors in  .\n            So we can write\n             \n            for some scalars  .\n            If   were to be a basis for  ,\n            then   would have to be a linearly independent set.\n            To determine the independence or dependence of   we consider the vector equation\n             \n            for scalars  ,  , and  .\n           \n                  Substitute for  ,  ,\n                  and   from  ,  ,\n                  and   into   and perform some vector algebra to show that\n                   .\n                 \n                  Recall that   is a basis.\n                  What does that tell us about the weights in the linear combination  ?\n                  Explain why  ,\n                  where   and  .\n                 \n                  With   as in part (b),\n                  how many solutions does the system   have?\n                  Explain.\n                  \n                  What does this tell us about the independence or dependence of the set  ?\n                  Why?\n                 \n                  Consider the number of rows and columns of  .\n                 \n                  Can   be a basis for  ?\n                  Explain.\n                 The Dimension of a Subspace of  dimension \n    Let   be a subspace of   that has a basis   of   vectors.\n    Since we have been calling bases minimal spanning sets,\n    we should expect that any two bases for the same subspace have the same number of elements\n    (otherwise one of the two bases would not be minimal).\n    Our goal in this section is to prove that result   that any other basis of   contains exactly   vectors.\n    The approach will be the same as was used in  .\n    We will let   be a set of vectors in   with   and demonstrate that   is a linearly dependent set.\n    To argue linear dependence, let  ,\n     ,  ,   be scalars so that\n     .\n   \n    For each   there exist scalars   so that\n     .\n   \n    Substituting into   yields\n     .\n   \n    Since   is a basis, the vectors  ,  ,\n     ,   are linearly independent.\n    So each coefficient in   is 0 and\n      is a solution to the homogeneous system  ,\n    where  .\n    Now   is a   matrix with  ,\n    so not every column of   is a pivot column.\n    This means that   has a nontrivial solution.\n    It follows that the vector equation   has a nontrivial solution and so the   vectors  ,\n     ,\n     ,   are linearly dependent.\n    We summarize this in the following theorem.\n   \n        Let   be a subspace of   containing a basis with   vectors.\n        If  ,\n        then any set of   vectors in   is linearly dependent.\n       \n    One consequence of   is that,\n    in addition to being a minimal spanning set,\n    a basis is also a maximal linearly independent set.\n   \n      Now let's return to the question of the number of elements in a basis for a subspace of  .\n      Recall that we are assuming that   has a basis\n        of   vectors in  .\n      Suppose that   is another basis for   containing   vectors.\n     \n            Given the fact that   is a basis for  ,\n            what does  \n            tell us about the relationship between   and  ?\n           \n            Given the fact that   is a basis for  ,\n            what does  \n            tell us about the relationship between   and  ?\n           \n            What do the results of (a) and (b) tell us about the relationship between   and  ?\n            What can we conclude about any basis for  ?\n           \n    The result of  \n    is summarized in the following theorem.\n    Recall that the trivial space is the single element set  .\n   \n        If a nontrivial subspace   of   has a basis of   vectors,\n        then every basis of   contains exactly   vectors.\n       invariant dimension subspace of  dimension \n    We denote the dimension of a subspace   of   by  .\n    As we will see later,\n    any two vector spaces of the same dimension are basically the same vector space.\n    So the dimension of a vector space is an important number that essentially tells us all we need to know about the structure of the space.\n   \n      Find the dimensions of each of the indicated subspaces of   for the appropriate  .\n      Explain your method.\n     \n             \n           \n             -plane in  \n           \n             \n           \n             \n           Conditions for a Basis of a Subspace of  \n    There are two items we need to confirm before we can state that a subset   of a subspace   of   is a basis for  :\n    the set   must be linearly independent and span  .\n    However, if we have the right number (namely,\n    the dimension) of vectors in our set  ,\n    then either one of these conditions will imply the other.\n   \n      Let   be a subspace of   with  .\n      We know that every basis of   contains exactly   vectors.\n     \n            Suppose that   is a subset of   that contains   vectors and is linearly independent.\n            In this part of the activity we will show that   must span  .\n           \n                  Suppose that   does not span  .\n                  Explain why this implies that   contains a set of   linearly independent vectors.\n                 \n                  Explain why the result of part i. tells us that   is a basis for  .\n                 \n            Now suppose that   is a subset of   with   vectors that spans  .\n            In this part of the activity we will show that   must be linearly independent.\n           \n                  Suppose that   is not linearly independent.\n                  Explain why we can then find a proper subset of   that is linearly independent but has the same span as  .\n                 \n                  Explain why the result of part i. tells us that   is a basis for  .\n                 \n    The result of  \n    is the following important theorem.\n   \n        Let   be a subspace of   of dimension   and let   be a subset of   containing exactly   vectors.\n         \n             \n              If   is linearly independent,\n              then   is a basis for  .\n             \n           \n             \n              If   spans  , then   is a basis for  .\n             \n           \n       Finding a Basis for a Subspace \n    Since every vector in a subspace of   can be written uniquely as a linear combination of vectors in a basis for the subspace,\n    a basis provides us with the most efficient and convenient way to represent vectors in the subspace.\n    Until now we have been given a set of vectors and have been asked to find a basis from that set,\n    so an important question to address is how we can find a basis for a subspace   of   starting from scratch.\n    Here is one way.\n    If  ,\n    then the dimension of   is 0 and   has no basis.\n    So suppose  .\n    Start by choosing any nonzero vector   in  .\n    Let  .\n    If   spans  ,\n    then   is a basis for  .\n    If not, there is a vector   in   that is not in  .\n    Then   is a linearly independent set.\n    If  ,\n    then   is a basis for   and we are done.\n    If not, repeat the process.\n    Since any basis for   can contain at most   vectors,\n    we know the process must stop at some point.\n    This process also allows us to construct a basis for a vector space that contains a given nonzero vector.\n   \n      Find a basis for   that contains the vector  .\n      When constructing your basis,\n      how do you know when to stop?\n     Rank of a Matrix \n    In this section,\n    we define the rank of a matrix and review conditions to add to our Invertible Matrix Theorem.\n   \n      Let  .\n     \n            Without performing any calculations, find  .\n            Explain.\n           \n            Without performing any calculations, find  .\n            Explain.\n           \n            There is a connection between  ,\n              and the size of  .\n            Find this connection and explain it.\n           rank nullity nullity The Rank-Nullity Theorem \n        Let   be an   matrix.\n        Then\n         .\n       \n    There is also a row space of a matrix  ,\n    which we define to be the span of the rows of  .\n    We can find the row space of   by finding the column space of  ,\n    so the row space is really nothing new.\n    As it turns out,\n    the dimension of the row space of   is always equal to the dimension of the column space of  ,\n    and justification for this statement is in the exercises.\n   \n    The Rank-Nullity Theorem allows us to add extra conditions to the Invertible Matrix Theorem.\n   The Invertible Matrix Theorem \n        Let   be an   matrix.\n        The following statements are equivalent.\n         \n             \n              The matrix   is an invertible matrix.\n             \n           \n             \n              The matrix equation   has only the trivial solution.\n             \n           \n             \n              The matrix   has   pivot columns.\n             \n           \n             \n              Every row of   contains a pivot.\n             \n           \n             \n              The columns of   span  .\n             \n           \n             \n              The matrix   is row equivalent to the identity matrix  .\n             \n           \n             \n              The columns of   are linearly independent.\n             \n           \n             \n              The columns of   form a basis for  .\n             \n           \n             \n              The matrix transformation   from   to   defined by   is one-to-one.\n             \n           \n             \n              The matrix equation   has exactly one solution for each vector   in  .\n             \n           \n             \n              The matrix transformation   from   to   defined by   is onto.\n             \n           \n             \n              There is an   matrix   so that  .\n             \n           \n             \n              There is an   matrix   so that  .\n             \n           \n             \n              The scalar 0 is not an eigenvalue of  .\n             \n           \n             \n              The matrix   is invertible.\n             \n           \n             \n               .\n             \n           \n             \n               .\n             \n           \n             \n               \n             \n           \n             \n               \n             \n           \n             \n               \n             \n           \n       Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Let  .\n       \n              Explain why   is a subspace of  .\n             \n              We can write any vector in   in the form\n               ,\n              so\n               .\n              As a span of a set of vectors in  ,\n                is a subspace of  .\n             \n              Find a basis for   and determine the dimension of  .\n             \n              Let  .\n              To find a basis for  ,\n              we note that the reduced row echelon form of   is  .\n              Since the pivot columns of   form a basis for  ,\n              we conclude that\n               \n              is a basis for  .\n              Therefore,  .\n             \n        Find a basis and the dimension of the solution set to the system\n         .\n       \n        The coefficient matrix of this system is\n         ,\n        and the solution set to the system is  .\n        To find a basis for   we row reduce   to\n         .\n       \n        The general solution to the system has the form\n         ,\n        so   is a basis for   and  .\n       Summary dimension dimension \n        Let  .\n       \n              Find a basis for  .\n              What is the dimension of  ?\n              What, then, is the dimension of  ?\n             \n               ,\n               ,  \n             \n              Find a basis for   and verify the dimension you found in part (a).\n             \n               \n             \n        Let  .\n        The eigenvalues of   are   and  .\n        Find the dimension of each eigenspace of  .\n       \n        Let  .\n       \n              Find a basis for  .\n              What is the rank of  ?\n             \n               ,\n               \n             \n              Find a basis for  .\n              What is the nullity of  .\n             \n               ,\n               \n             \n              Verify the Rank-Nullity Theorem for  .\n             \n               \n             \n              The row space of   is the span of the rows of  .\n              Find a basis for the row space of   and the dimension of the row space of  .\n             \n               ,\n              dimension  \n             \n        Let   be an   matrix with   pivots,\n        where   is less than or equal to both  .\n        Fill in the blanks.\n       \n              The null space of   is a subspace of  .\n             \n              The column space of   is a subspace of  .\n             \n              Suppose  .\n              Then there is a pivot in every   and    .\n             \n              Suppose  .\n              Then there is a pivot in every   and  .\n             \n              If   has 3 pivots, then the rank of   is  .\n             \n              If   has 3 pivots,\n              then the number of free variables in the system   is  .\n             \n              The dimension of   is equal to the number of  , i.e.\n               .\n             \n              The dimension of   is equal to the number of  , i.e.\n               .\n             \n                 .\n             \n              Suppose the columns of   span  .\n              Then rank   is  .\n             \n              Suppose the columns of   are linearly independent.\n              Then     and the dimension of   is  .\n             \n        Prove the remaining parts of the Invertible Matrix Theorem\n        ( ).\n        Let   be an   matrix.\n       \n              Prove that   is invertible if and only if  .\n             \n              What are the solutions to  ?\n             \n              Prove that   is invertible if and only if  .\n             \n              Use part (a) and the Rank-Nullity Theorem.\n             \n        We can convert the language of the Rank-Nullity Theorem to matrix transformation language,\n        as we show in this exercise.\n        Let   be the matrix transformation defined by the matrix  .\n       \n              How is the kernel of   related to  ?\n             \n              How is the range of   related to  ?\n             \n              How is the domain of   related to  ?\n             \n              Explain why the Rank-Nullity Theorem says that  .\n             \n        Let   be a subspace of  .\n        What are possible values for the dimension of  ?\n        Explain.\n        What are the geometric descriptions of   in each case?\n       \n          can be  ,  ,  ,\n         , or   corresponding to  ,\n        a line through the origin in  ,\n        a plane containing the origin in  ,\n        a copy of   through the origin in  ,\n         \n       \n        Is it possible to find two subspaces   and   in   such that\n          and  ?\n        If possible,\n        give an example and justify that they satisfy the conditions.\n        If not possible, explain why not.\n       \n        Dimension two leads to two linearly independent vectors in each of  .\n       \n        Determine the dimensions of the column space and null space of  .\n       \n         ,  \n       \n        If possible,\n        find a   matrix whose column space has dimension 3 and null space has dimension 1.\n        Explain how you  found  the matrix in addition to explaining why your answer works.\n        If not possible,\n        explain why it is not possible to find such a matrix.\n       \n              If possible,\n              find a   matrix whose column space has the same dimension as its null space.\n              Explain how you  found  the matrix in addition to explaining why your answer works.\n              If not possible,\n              explain why it is not possible to find such a matrix.\n             \n              Not possible.\n             \n              If possible,\n              find a matrix   so that  .\n              Explain how you  found  the matrix in addition to explaining why your answer works.\n              If not possible,\n              explain why it is not possible to find such a matrix.\n             \n               \n             \n        In this exercise we examine why the dimension of a row space of a matrix is the same as the dimension of the column space of the matrix.\n        Let   be an   matrix.\n       \n              Explain why row operations do not change the row space of a matrix.\n              Then explain why if   is the reduced row echelon form of  ,\n              then  ,\n              where   is the row space of the matrix  .\n             \n              Explain why the rows of   that contain pivots form a basis for  ,\n              and also of  .\n             \n              Explain why   is the number of pivots in the matrix  .\n              Then explain why  .\n             \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              The dimension of the column space of a   matrix can be three.\n             \n              F\n             True\/False \n              There exists a\n                matrix whose column space has equal dimension as the null space.\n             True\/False \n              If a set of vectors spans a subspace,\n              then that set is a basis of this subspace.\n             \n              F\n             True\/False \n              If a linearly independent set of vectors spans a subspace,\n              then that set is a basis of this subspace.\n             True\/False \n              The dimension of a space is the minimum number of vectors needed to span that space.\n             \n              T\n             True\/False \n              The dimension of the null space of a\n                matrix can at most be 2.\n             True\/False \n              Any basis of   contains 4 vectors.\n             \n              T\n             True\/False \n              If   vectors span  ,\n              then these vectors form a basis of  .\n             True\/False \n              Every line in   is a one-dimensional subspace of  .\n             \n              F\n             True\/False \n              Every plane through origin in   is a two-dimensional subspace of  .\n             True\/False \n              In   any   linearly independent vectors form a basis.\n             \n              T\n             Project: The GGH Cryptosystem plaintext ciphertext decrypted \n    The Goldreich-Goldwasser-Halevi (GGH) public key cryptosystem \n    Published in 1997 by Oded Goldreich, Shafi Goldwasser,\n    and Shai Halevi.\n      uses lattices to encrypt plaintext.\n    The security of the system depends on the fact that the Closest Vector Problem (CVP) is,\n    in general, a very hard problem.\n    To begin to understand these cryptosystems, we begin with lattices.\n   full-rank basis \n      We explore lattices in more detail in this activity.\n     \n            Let  .\n           \n                  Find five distinct vectors in  .\n                 \n                  Is the vector   in  ?\n                  Justify your answer.\n                 \n                  We can draw pictures of lattices by plotting the terminal points of the lattice vectors.\n                  Draw all of the lattice points in\n                    on the square with vertices  ,\n                   ,\n                   , and  .\n                 \n            Now let  .\n            A picture of   is shown in  \n            with the basis vectors highlighted.\n            As we have seen,\n              is not the entire space  .\n            Is  ?\n            Justify your answer.\n             The lattice  . \n           \n     \n    shows that even if   is a basis for  ,\n    it does not follow that   is all of  .\n    So latices can be complicated,\n    and problems in lattice theory can be very difficult.\n   \n    The GGH cryptosystem relies on the fact that we can convert\n     good \n    bases for lattices into\n     bad \n    bases.\n    We will not delve into the details of what separates a\n     good \n    basis from a\n     bad \n    one, but suffice it to say that a good basis is one in which the basis vectors are close to being perpendicular \n    This is also a good property in vector spaces.\n    We will see in a later section that perpendicular basis vectors make calculations in vector spaces relatively easy.\n    A similar thing is true in lattices,\n    where we are able to solve certain variants of closest vector problem very efficiently.\n      and are all short\n    (that is, they have small norms),\n    while any other basis is a bad basis.\n    An example of a good basis is the basis   for   in  ,\n    and we will see later that\n      is a bad basis for the same lattice.\n    You should draw a picture of the vectors   and\n      to convince yourself that this is a bad basis for its lattice.\n   keys \n    First we encrypt the message,\n    which can be done by anyone who has the public key  .\n     \n         \n          Create the message vector\n           \n          that is in the lattice using the bad basis  .\n         \n       \n         \n          Choose a small error   to add to   to move   off the lattice\n          (small enough so that   does not pass by another lattice point).\n          This is an important step that will make the message difficult to decrypt without the key.\n          Let  .\n          The vector   is the ciphertext that is to be transmitted to the receiver.\n         \n       \n   \n    Only someone who knows the basis   can decode the ciphertext.\n    This is done as follows.\n     \n         \n          Find the vector   in the good basis   that is closest to  .\n         \n       \n         \n          We interpret the vector   as being the encoded vector without the error.\n          So to recreate the original message vector we need to undo the encrypting using the bad basis  .\n          That is, we need to find the weights  ,  ,\n           ,   such that\n           .\n          We can do this by as  .\n         \n       \n   \n    There are several items to address before we can implement this algorithm.\n    One is how we create a bad basis   from   that produces the same lattice.\n    Another is how we find the vector in\n      closest to a given vector.\n    The latter problem is called the Closest Vector Problem (CVP) and is,\n    in general, a very difficult problem.\n    This is what makes lattice-based cryptosystems secure.\n    We address the first of these items in the next activity,\n    and the second a bit later.\n   \n      Consider again the basis   from  ,\n      and let   be the matrix whose columns are the vectors in  .\n     \n            Let   be the triangle with vertices  ,\n             , and  .\n            Show that this triangle is a right triangle,\n            and conclude that the vectors  ,\n            and   are perpendicular.\n           \n            Let  .\n            Let   be the set whose vectors are the columns of the matrix  .\n            Show that  .\n           \n    The two bases   and\n      from  \n    are shown in  .\n    This figure illustrates how the matrix   transforms the basis  ,\n    in which the vectors are perpendicular and short,\n    to one in which the vectors are nearly parallel and significantly longer.\n    So the matrix   converts the\n     good \n    basis   into a\n     bad \n    basis  , keeping the lattice intact.\n    This is a key idea in the GGH cryptosystem.\n    What makes this work is the fact that both   and   have integer entries.\n    The reason for this is that,\n    for a   matrix  ,\n    we know that  .\n    If   has integer entries and  ,\n    then   will also have integer entries.\n    The number   is called the\n     determinant  of  ,\n    and matrices with determinant of   or   are called  unimodular .\n    That what happened in  \n    happens in the general case is the focus of the next activity.\n   The lattices   and  . \n      We will restrict ourselves to\n        matrices in this activity,\n      but the results generalize to   matrices.\n      Let   and\n        be bases for   with integer entries,\n      and let   and\n        be the matrices associated to these bases.\n      Show that if   for some unimodular matrix   with integer entries,\n      then  .\n     \n     \n    is the part we need for our lattice-based cryptosysystem.\n    Although we won't show it here,\n    the converse of the statement in   is also true.\n    That is, if   and   generate the same lattice,\n    then   for some unimodular matrix   with integer entries.\n   \n    There is one more item to address before we implement the GGH cryptosystem.\n    That item is how to solve the Closest Vector Problem.\n    There are some algorithms for approximating the closest vector in a basis.\n    One is Babai's Closest Vector algorithm.\n    This algorithm works in the following way.\n    Consider a lattice with basis  .\n    To approximate the closest vector in the lattice to a vector  ,\n    find the weights  ,  ,  ,\n      in   such that  .\n    Then round the coefficients to the nearest integer.\n    This algorithm works well for a good basis,\n    but is unlikely to return a lattice point that is close to   if the basis is a bad one.\n   \n    Now we put this all together to illustrate the GGH algorithm.\n   Decrypting an encrypted message. \n      Let   be the private key,\n      and let   be the matrix whose columns are the vectors in  .\n      Let   be the unimodular matrix  .\n      Let   be our message and let   be our error vector.\n     \n            Use the unimodular matrix   to create the bad basis  .\n           \n            Determine the ciphertext message  .\n           \n            A picture of the message vector   and the ciphertext vector   are shown in  .\n            Although the closest vector in the lattice to   can be determined by the figure,\n            actual messages are constructed in high dimensional spaces where a visual approach is not practical.\n            Use Babai's algorithm to find the vector in\n              that is closest to   and compare to  .\n           \n            The final step in the GGH scheme is to recover the original message.\n            Complete the GGH algorithm to find this message.\n           \n            The GGH cryptosystem works because the CVP can be reasonable solved using a good basis.\n            That is, Babai's algorithm works if our basis is a good basis.\n            To illustrate that a bad basis will not allow us to reproduce the original message vector,\n            show that Babai's algorithm does not return the closest vector to   using the bad basis  .\n           "
},
{
  "id": "objectives-15",
  "level": "2",
  "url": "chap_bases_dimension.html#objectives-15",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section\n         \n           \n            What is the dimension of a subspace of  ?\n            What property of bases makes the dimension a well-defined number?\n           \n         \n           \n            If   is a subspace of   with dimension  ,\n            what must be true about any linearly independent subset   of   that contains exactly   vectors?\n           \n         \n           \n            If   is a subspace of   with dimension  ,\n            what must be true about any subset   of   that contains exactly   vectors and spans  ?\n           \n         \n           \n            What is the rank of a matrix?\n           \n         \n           \n            What does the Rank-Nullity Theorem say?\n           \n         "
},
{
  "id": "pa_3_d",
  "level": "2",
  "url": "chap_bases_dimension.html#pa_3_d",
  "type": "Preview Activity",
  "number": "15.1",
  "title": "",
  "body": "\n            For each of the following sets of vectors,\n            determine whether the vectors form a basis of  .\n            Use any appropriate technology for your computations.\n           \n                   \n                 \n                   \n                 \n                   \n                 \n                   \n                 \n            In problem (1) we should have noticed that a space can have more than one basis,\n            but that any two bases contain the same number of elements.\n            This is a critically important idea that we investigate in more detail in this problem in one specific case.\n            Assume that   is a subspace of   that has a basis\n              with two basis vectors.\n            We want to see if any other basis for   can have a different number of elements.\n            Let us now consider a set   of three vectors in  .\n            Our goal is to determine if   can be a basis for  .\n            Since   is a basis for  ,\n            any vector in   can be written as a linear combination of the vectors in  .\n            So we can write\n             \n            for some scalars  .\n            If   were to be a basis for  ,\n            then   would have to be a linearly independent set.\n            To determine the independence or dependence of   we consider the vector equation\n             \n            for scalars  ,  , and  .\n           \n                  Substitute for  ,  ,\n                  and   from  ,  ,\n                  and   into   and perform some vector algebra to show that\n                   .\n                 \n                  Recall that   is a basis.\n                  What does that tell us about the weights in the linear combination  ?\n                  Explain why  ,\n                  where   and  .\n                 \n                  With   as in part (b),\n                  how many solutions does the system   have?\n                  Explain.\n                  \n                  What does this tell us about the independence or dependence of the set  ?\n                  Why?\n                 \n                  Consider the number of rows and columns of  .\n                 \n                  Can   be a basis for  ?\n                  Explain.\n                 "
},
{
  "id": "p-2518",
  "level": "2",
  "url": "chap_bases_dimension.html#p-2518",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "dimension "
},
{
  "id": "thm_3_d_1",
  "level": "2",
  "url": "chap_bases_dimension.html#thm_3_d_1",
  "type": "Theorem",
  "number": "15.1",
  "title": "",
  "body": "\n        Let   be a subspace of   containing a basis with   vectors.\n        If  ,\n        then any set of   vectors in   is linearly dependent.\n       "
},
{
  "id": "act_3_d_1",
  "level": "2",
  "url": "chap_bases_dimension.html#act_3_d_1",
  "type": "Activity",
  "number": "15.2",
  "title": "",
  "body": "\n      Now let's return to the question of the number of elements in a basis for a subspace of  .\n      Recall that we are assuming that   has a basis\n        of   vectors in  .\n      Suppose that   is another basis for   containing   vectors.\n     \n            Given the fact that   is a basis for  ,\n            what does  \n            tell us about the relationship between   and  ?\n           \n            Given the fact that   is a basis for  ,\n            what does  \n            tell us about the relationship between   and  ?\n           \n            What do the results of (a) and (b) tell us about the relationship between   and  ?\n            What can we conclude about any basis for  ?\n           "
},
{
  "id": "thm_3_d_basis",
  "level": "2",
  "url": "chap_bases_dimension.html#thm_3_d_basis",
  "type": "Theorem",
  "number": "15.2",
  "title": "",
  "body": "\n        If a nontrivial subspace   of   has a basis of   vectors,\n        then every basis of   contains exactly   vectors.\n       "
},
{
  "id": "p-2531",
  "level": "2",
  "url": "chap_bases_dimension.html#p-2531",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "invariant "
},
{
  "id": "definition-35",
  "level": "2",
  "url": "chap_bases_dimension.html#definition-35",
  "type": "Definition",
  "number": "15.3",
  "title": "",
  "body": "dimension subspace of  dimension "
},
{
  "id": "act_3_d_2",
  "level": "2",
  "url": "chap_bases_dimension.html#act_3_d_2",
  "type": "Activity",
  "number": "15.3",
  "title": "",
  "body": "\n      Find the dimensions of each of the indicated subspaces of   for the appropriate  .\n      Explain your method.\n     \n             \n           \n             -plane in  \n           \n             \n           \n             \n           "
},
{
  "id": "act_3_d_3",
  "level": "2",
  "url": "chap_bases_dimension.html#act_3_d_3",
  "type": "Activity",
  "number": "15.4",
  "title": "",
  "body": "\n      Let   be a subspace of   with  .\n      We know that every basis of   contains exactly   vectors.\n     \n            Suppose that   is a subset of   that contains   vectors and is linearly independent.\n            In this part of the activity we will show that   must span  .\n           \n                  Suppose that   does not span  .\n                  Explain why this implies that   contains a set of   linearly independent vectors.\n                 \n                  Explain why the result of part i. tells us that   is a basis for  .\n                 \n            Now suppose that   is a subset of   with   vectors that spans  .\n            In this part of the activity we will show that   must be linearly independent.\n           \n                  Suppose that   is not linearly independent.\n                  Explain why we can then find a proper subset of   that is linearly independent but has the same span as  .\n                 \n                  Explain why the result of part i. tells us that   is a basis for  .\n                 "
},
{
  "id": "thm_3_d_basis_properties",
  "level": "2",
  "url": "chap_bases_dimension.html#thm_3_d_basis_properties",
  "type": "Theorem",
  "number": "15.4",
  "title": "",
  "body": "\n        Let   be a subspace of   of dimension   and let   be a subset of   containing exactly   vectors.\n         \n             \n              If   is linearly independent,\n              then   is a basis for  .\n             \n           \n             \n              If   spans  , then   is a basis for  .\n             \n           \n       "
},
{
  "id": "act_3_d_4",
  "level": "2",
  "url": "chap_bases_dimension.html#act_3_d_4",
  "type": "Activity",
  "number": "15.5",
  "title": "",
  "body": "\n      Find a basis for   that contains the vector  .\n      When constructing your basis,\n      how do you know when to stop?\n     "
},
{
  "id": "act_3_d_5",
  "level": "2",
  "url": "chap_bases_dimension.html#act_3_d_5",
  "type": "Activity",
  "number": "15.6",
  "title": "",
  "body": "\n      Let  .\n     \n            Without performing any calculations, find  .\n            Explain.\n           \n            Without performing any calculations, find  .\n            Explain.\n           \n            There is a connection between  ,\n              and the size of  .\n            Find this connection and explain it.\n           "
},
{
  "id": "p-2558",
  "level": "2",
  "url": "chap_bases_dimension.html#p-2558",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "rank nullity nullity "
},
{
  "id": "thm_3_d_rank_nullity",
  "level": "2",
  "url": "chap_bases_dimension.html#thm_3_d_rank_nullity",
  "type": "Theorem",
  "number": "15.5",
  "title": "The Rank-Nullity Theorem.",
  "body": "The Rank-Nullity Theorem \n        Let   be an   matrix.\n        Then\n         .\n       "
},
{
  "id": "thm_3_d_IMT",
  "level": "2",
  "url": "chap_bases_dimension.html#thm_3_d_IMT",
  "type": "Theorem",
  "number": "15.6",
  "title": "The Invertible Matrix Theorem.",
  "body": "The Invertible Matrix Theorem \n        Let   be an   matrix.\n        The following statements are equivalent.\n         \n             \n              The matrix   is an invertible matrix.\n             \n           \n             \n              The matrix equation   has only the trivial solution.\n             \n           \n             \n              The matrix   has   pivot columns.\n             \n           \n             \n              Every row of   contains a pivot.\n             \n           \n             \n              The columns of   span  .\n             \n           \n             \n              The matrix   is row equivalent to the identity matrix  .\n             \n           \n             \n              The columns of   are linearly independent.\n             \n           \n             \n              The columns of   form a basis for  .\n             \n           \n             \n              The matrix transformation   from   to   defined by   is one-to-one.\n             \n           \n             \n              The matrix equation   has exactly one solution for each vector   in  .\n             \n           \n             \n              The matrix transformation   from   to   defined by   is onto.\n             \n           \n             \n              There is an   matrix   so that  .\n             \n           \n             \n              There is an   matrix   so that  .\n             \n           \n             \n              The scalar 0 is not an eigenvalue of  .\n             \n           \n             \n              The matrix   is invertible.\n             \n           \n             \n               .\n             \n           \n             \n               .\n             \n           \n             \n               \n             \n           \n             \n               \n             \n           \n             \n               \n             \n           \n       "
},
{
  "id": "example-30",
  "level": "2",
  "url": "chap_bases_dimension.html#example-30",
  "type": "Example",
  "number": "15.7",
  "title": "",
  "body": "\n        Let  .\n       \n              Explain why   is a subspace of  .\n             \n              We can write any vector in   in the form\n               ,\n              so\n               .\n              As a span of a set of vectors in  ,\n                is a subspace of  .\n             \n              Find a basis for   and determine the dimension of  .\n             \n              Let  .\n              To find a basis for  ,\n              we note that the reduced row echelon form of   is  .\n              Since the pivot columns of   form a basis for  ,\n              we conclude that\n               \n              is a basis for  .\n              Therefore,  .\n             "
},
{
  "id": "example-31",
  "level": "2",
  "url": "chap_bases_dimension.html#example-31",
  "type": "Example",
  "number": "15.8",
  "title": "",
  "body": "\n        Find a basis and the dimension of the solution set to the system\n         .\n       \n        The coefficient matrix of this system is\n         ,\n        and the solution set to the system is  .\n        To find a basis for   we row reduce   to\n         .\n       \n        The general solution to the system has the form\n         ,\n        so   is a basis for   and  .\n       "
},
{
  "id": "p-2592",
  "level": "2",
  "url": "chap_bases_dimension.html#p-2592",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "dimension dimension "
},
{
  "id": "exercise-135",
  "level": "2",
  "url": "chap_bases_dimension.html#exercise-135",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Let  .\n       \n              Find a basis for  .\n              What is the dimension of  ?\n              What, then, is the dimension of  ?\n             \n               ,\n               ,  \n             \n              Find a basis for   and verify the dimension you found in part (a).\n             \n               \n             "
},
{
  "id": "exercise-136",
  "level": "2",
  "url": "chap_bases_dimension.html#exercise-136",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Let  .\n        The eigenvalues of   are   and  .\n        Find the dimension of each eigenspace of  .\n       "
},
{
  "id": "exercise-137",
  "level": "2",
  "url": "chap_bases_dimension.html#exercise-137",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Let  .\n       \n              Find a basis for  .\n              What is the rank of  ?\n             \n               ,\n               \n             \n              Find a basis for  .\n              What is the nullity of  .\n             \n               ,\n               \n             \n              Verify the Rank-Nullity Theorem for  .\n             \n               \n             \n              The row space of   is the span of the rows of  .\n              Find a basis for the row space of   and the dimension of the row space of  .\n             \n               ,\n              dimension  \n             "
},
{
  "id": "exercise-138",
  "level": "2",
  "url": "chap_bases_dimension.html#exercise-138",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Let   be an   matrix with   pivots,\n        where   is less than or equal to both  .\n        Fill in the blanks.\n       \n              The null space of   is a subspace of  .\n             \n              The column space of   is a subspace of  .\n             \n              Suppose  .\n              Then there is a pivot in every   and    .\n             \n              Suppose  .\n              Then there is a pivot in every   and  .\n             \n              If   has 3 pivots, then the rank of   is  .\n             \n              If   has 3 pivots,\n              then the number of free variables in the system   is  .\n             \n              The dimension of   is equal to the number of  , i.e.\n               .\n             \n              The dimension of   is equal to the number of  , i.e.\n               .\n             \n                 .\n             \n              Suppose the columns of   span  .\n              Then rank   is  .\n             \n              Suppose the columns of   are linearly independent.\n              Then     and the dimension of   is  .\n             "
},
{
  "id": "exercise-139",
  "level": "2",
  "url": "chap_bases_dimension.html#exercise-139",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Prove the remaining parts of the Invertible Matrix Theorem\n        ( ).\n        Let   be an   matrix.\n       \n              Prove that   is invertible if and only if  .\n             \n              What are the solutions to  ?\n             \n              Prove that   is invertible if and only if  .\n             \n              Use part (a) and the Rank-Nullity Theorem.\n             "
},
{
  "id": "exercise-140",
  "level": "2",
  "url": "chap_bases_dimension.html#exercise-140",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        We can convert the language of the Rank-Nullity Theorem to matrix transformation language,\n        as we show in this exercise.\n        Let   be the matrix transformation defined by the matrix  .\n       \n              How is the kernel of   related to  ?\n             \n              How is the range of   related to  ?\n             \n              How is the domain of   related to  ?\n             \n              Explain why the Rank-Nullity Theorem says that  .\n             "
},
{
  "id": "exercise-141",
  "level": "2",
  "url": "chap_bases_dimension.html#exercise-141",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        Let   be a subspace of  .\n        What are possible values for the dimension of  ?\n        Explain.\n        What are the geometric descriptions of   in each case?\n       \n          can be  ,  ,  ,\n         , or   corresponding to  ,\n        a line through the origin in  ,\n        a plane containing the origin in  ,\n        a copy of   through the origin in  ,\n         \n       "
},
{
  "id": "exercise-142",
  "level": "2",
  "url": "chap_bases_dimension.html#exercise-142",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Is it possible to find two subspaces   and   in   such that\n          and  ?\n        If possible,\n        give an example and justify that they satisfy the conditions.\n        If not possible, explain why not.\n       \n        Dimension two leads to two linearly independent vectors in each of  .\n       "
},
{
  "id": "exercise-143",
  "level": "2",
  "url": "chap_bases_dimension.html#exercise-143",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        Determine the dimensions of the column space and null space of  .\n       \n         ,  \n       "
},
{
  "id": "exercise-144",
  "level": "2",
  "url": "chap_bases_dimension.html#exercise-144",
  "type": "Exercise",
  "number": "10",
  "title": "",
  "body": "\n        If possible,\n        find a   matrix whose column space has dimension 3 and null space has dimension 1.\n        Explain how you  found  the matrix in addition to explaining why your answer works.\n        If not possible,\n        explain why it is not possible to find such a matrix.\n       "
},
{
  "id": "exercise-145",
  "level": "2",
  "url": "chap_bases_dimension.html#exercise-145",
  "type": "Exercise",
  "number": "11",
  "title": "",
  "body": "\n              If possible,\n              find a   matrix whose column space has the same dimension as its null space.\n              Explain how you  found  the matrix in addition to explaining why your answer works.\n              If not possible,\n              explain why it is not possible to find such a matrix.\n             \n              Not possible.\n             \n              If possible,\n              find a matrix   so that  .\n              Explain how you  found  the matrix in addition to explaining why your answer works.\n              If not possible,\n              explain why it is not possible to find such a matrix.\n             \n               \n             "
},
{
  "id": "exercise-146",
  "level": "2",
  "url": "chap_bases_dimension.html#exercise-146",
  "type": "Exercise",
  "number": "12",
  "title": "",
  "body": "\n        In this exercise we examine why the dimension of a row space of a matrix is the same as the dimension of the column space of the matrix.\n        Let   be an   matrix.\n       \n              Explain why row operations do not change the row space of a matrix.\n              Then explain why if   is the reduced row echelon form of  ,\n              then  ,\n              where   is the row space of the matrix  .\n             \n              Explain why the rows of   that contain pivots form a basis for  ,\n              and also of  .\n             \n              Explain why   is the number of pivots in the matrix  .\n              Then explain why  .\n             "
},
{
  "id": "exercise-147",
  "level": "2",
  "url": "chap_bases_dimension.html#exercise-147",
  "type": "Exercise",
  "number": "13",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              The dimension of the column space of a   matrix can be three.\n             \n              F\n             True\/False \n              There exists a\n                matrix whose column space has equal dimension as the null space.\n             True\/False \n              If a set of vectors spans a subspace,\n              then that set is a basis of this subspace.\n             \n              F\n             True\/False \n              If a linearly independent set of vectors spans a subspace,\n              then that set is a basis of this subspace.\n             True\/False \n              The dimension of a space is the minimum number of vectors needed to span that space.\n             \n              T\n             True\/False \n              The dimension of the null space of a\n                matrix can at most be 2.\n             True\/False \n              Any basis of   contains 4 vectors.\n             \n              T\n             True\/False \n              If   vectors span  ,\n              then these vectors form a basis of  .\n             True\/False \n              Every line in   is a one-dimensional subspace of  .\n             \n              F\n             True\/False \n              Every plane through origin in   is a two-dimensional subspace of  .\n             True\/False \n              In   any   linearly independent vectors form a basis.\n             \n              T\n             "
},
{
  "id": "p-2668",
  "level": "2",
  "url": "chap_bases_dimension.html#p-2668",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "plaintext ciphertext decrypted "
},
{
  "id": "p-2670",
  "level": "2",
  "url": "chap_bases_dimension.html#p-2670",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "full-rank basis "
},
{
  "id": "act_lattices_1",
  "level": "2",
  "url": "chap_bases_dimension.html#act_lattices_1",
  "type": "Project Activity",
  "number": "15.7",
  "title": "",
  "body": "\n      We explore lattices in more detail in this activity.\n     \n            Let  .\n           \n                  Find five distinct vectors in  .\n                 \n                  Is the vector   in  ?\n                  Justify your answer.\n                 \n                  We can draw pictures of lattices by plotting the terminal points of the lattice vectors.\n                  Draw all of the lattice points in\n                    on the square with vertices  ,\n                   ,\n                   , and  .\n                 \n            Now let  .\n            A picture of   is shown in  \n            with the basis vectors highlighted.\n            As we have seen,\n              is not the entire space  .\n            Is  ?\n            Justify your answer.\n             The lattice  . \n           "
},
{
  "id": "p-2679",
  "level": "2",
  "url": "chap_bases_dimension.html#p-2679",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "keys "
},
{
  "id": "act_lattices_2",
  "level": "2",
  "url": "chap_bases_dimension.html#act_lattices_2",
  "type": "Project Activity",
  "number": "15.8",
  "title": "",
  "body": "\n      Consider again the basis   from  ,\n      and let   be the matrix whose columns are the vectors in  .\n     \n            Let   be the triangle with vertices  ,\n             , and  .\n            Show that this triangle is a right triangle,\n            and conclude that the vectors  ,\n            and   are perpendicular.\n           \n            Let  .\n            Let   be the set whose vectors are the columns of the matrix  .\n            Show that  .\n           "
},
{
  "id": "F_Lattice_3",
  "level": "2",
  "url": "chap_bases_dimension.html#F_Lattice_3",
  "type": "Figure",
  "number": "15.10",
  "title": "",
  "body": "The lattices   and  . "
},
{
  "id": "act_lattices_3",
  "level": "2",
  "url": "chap_bases_dimension.html#act_lattices_3",
  "type": "Project Activity",
  "number": "15.9",
  "title": "",
  "body": "\n      We will restrict ourselves to\n        matrices in this activity,\n      but the results generalize to   matrices.\n      Let   and\n        be bases for   with integer entries,\n      and let   and\n        be the matrices associated to these bases.\n      Show that if   for some unimodular matrix   with integer entries,\n      then  .\n     "
},
{
  "id": "F_lattices_GGH_ex",
  "level": "2",
  "url": "chap_bases_dimension.html#F_lattices_GGH_ex",
  "type": "Figure",
  "number": "15.11",
  "title": "",
  "body": "Decrypting an encrypted message. "
},
{
  "id": "act_lattices_GGH",
  "level": "2",
  "url": "chap_bases_dimension.html#act_lattices_GGH",
  "type": "Project Activity",
  "number": "15.10",
  "title": "",
  "body": "\n      Let   be the private key,\n      and let   be the matrix whose columns are the vectors in  .\n      Let   be the unimodular matrix  .\n      Let   be our message and let   be our error vector.\n     \n            Use the unimodular matrix   to create the bad basis  .\n           \n            Determine the ciphertext message  .\n           \n            A picture of the message vector   and the ciphertext vector   are shown in  .\n            Although the closest vector in the lattice to   can be determined by the figure,\n            actual messages are constructed in high dimensional spaces where a visual approach is not practical.\n            Use Babai's algorithm to find the vector in\n              that is closest to   and compare to  .\n           \n            The final step in the GGH scheme is to recover the original message.\n            Complete the GGH algorithm to find this message.\n           \n            The GGH cryptosystem works because the CVP can be reasonable solved using a good basis.\n            That is, Babai's algorithm works if our basis is a good basis.\n            To illustrate that a bad basis will not allow us to reproduce the original message vector,\n            show that Babai's algorithm does not return the closest vector to   using the bad basis  .\n           "
},
{
  "id": "chap_coordinate_vectors",
  "level": "1",
  "url": "chap_coordinate_vectors.html",
  "type": "Section",
  "number": "16",
  "title": "Coordinate Vectors and Change of Basis",
  "body": "Coordinate Vectors and Change of Basis \n        By the end of this section, you should be able to give precise and thorough answers to the\n        questions listed below. You may want to keep these questions in mind to focus your thoughts as\n        you complete the section.\n       \n           \n            How do we find the coordinate vector of a vector   with respect to a basis  ?\n           \n         \n           \n            What is a change of basis matrix?\n           \n         \n           \n            Why is a change of basis useful?\n           \n         Application: Describing Orbits of Planets \n    Consider a planet orbiting the sun\n    (or an object like a satellite orbiting the Earth).\n    According to Kepler's Laws, we assume an elliptical orbit.\n    There are many different ways to describe this orbit,\n    and which description we use depends on our perspective and the application.\n    One important perspective is to make the description of the orbit as simple as possible for earth-based observations.\n    Two problems arise.\n    One is that the earth's orbit and the orbit of the planet do not lie in the same plane.\n    A second problem is that it is complicated to describe the orbit of a planet using the perspective of the plane of the earth's orbit.\n    A reasonable approach,\n    then, is to establish two different coordinate systems,\n    one for the earth's orbit and one for the planet's orbit.\n    We can then use a change of basis to move back and forth from these two perspectives.\n   Introduction \n    In this section we will investigate how a basis in   provides a coordinate system in which each vector in   has a unique set of coordinates.\n    In this way,\n    each basis will provide us with a new perspective to visualize  .\n    Then we will see how coordinate vectors can allow us to find change of basis matrices that we can use to easily switch between coordinate systems.\n    We begin our analysis of coordinate systems by looking at how a basis in   gives us a different view of  .\n   \n      Two vectors   and   are shown in  .\n     Vectors   and  . \n          Explain why   is a basis for  .\n         \n          Draw the vector   in  .\n          Explain your process.\n         coordinate vector \n          Since   is a basis for  ,\n          any vector   in   has a coordinate vector in the coordinate system defined by   and  .\n          But we also need to make sure that each vector has a unique coordinate vector.\n          Explain why there is no vector in   which has two different coordinate vectors.\n         \n          We can think of the vectors   and   as defining a coordinate system,\n          with   as the\n           \n          -axis and   as the\n           \n          -axis.\n          Any vector   in   can be written uniquely in the form\n           \n          and the weights serve as the coordinates of   in the  ,\n            coordinate system.\n          In this case the coordinate vector\n            of   with respect to the basis\n            is written as  .\n          Let  .\n         \n                Show that   is a basis for  .\n               \n                Find   if  .\n                Draw a picture to illustrate how   is expressed as a linear combination of the vectors in  .\n               Bases as Coordinate Systems in  \n    Bases are useful for many reasons.\n    A basis provides us with a unique representation of the elements in   as linear combinations of the basis vectors in the coordinate system defined by the vectors.\n   \n    As we saw in  ,\n    we can think of a basis   of   as determining a coordinate system of  .\n    For example, let   where\n      and  .\n    The vector   can be written as  .\n     \n    shows that if we plot the point that is   units in the   direction\n    (where a\n     unit \n    is a copy of  )\n    and   units in the   direction,\n    then the result is the vector from the origin to point defined by  .\n   A Coordinate System Defined by a Basis. coordinate vector \n    This is actually a familiar idea,\n    one we have used for years.\n    The standard coordinates of a vector\n      in   are just the coordinates of   with respect to the standard basis   of  .\n   \n    While we can draw pictures in  ,\n    there is no reason to restrict this idea to  .\n   coordinate vector with respect to a basis in  coordinates with respect to a basis in  coordinate vector coordinates of the vector  \n        with respect to the basis \n    Recall that there is exactly one way to write a vector as a linear combination of basis vectors,\n    so there is only one coordinate vector of a given vector with respect to a basis.\n    Therefore, the coordinate vector of any vector with respect to a basis is well-defined.\n   \n      Let   and  .\n      Assume that   and   are bases for  .\n      Find   and  .\n      Note that the coordinate vector depends on the basis that is used.\n     IMPORTANT NOTE \n    We have defined the coordinate vector of a vector   in   with respect to a basis\n      as the vector   if\n     .\n   \n    Until now we have listed a basis as a set without regard to the order in which the basis elements are written.\n    That is, the set   is the same as the set  .\n    Notice, however,\n    that if we change the order of the vectors in our basis,\n    say from   to  ,\n    then the coordinate vector of   with respect to   will be different.\n    To avoid this problem,\n    when discussing coordinate vectors we will consider our bases to be  ordered bases ,\n    so that the order in which we write the elements in our basis is fixed.\n    So, for example, the ordered basis\n      is different than the ordered basis  .\n   \n    Coordinate vectors behave nicely with respect to addition and multiplication by scalars.\n    The next activity illustrates this in  .\n   \n      Let   be a basis for  .\n      Let   and   be vectors in   with\n        and  .\n     \n            Determine the components of the vector  .\n            How is   related to\n              and  ?\n           \n            Let   be any scalar.\n            Determine the components of the vector  .\n            How is   related to   and  \n           \n     \n    suggests that the following theorem is true for coordinate vectors.\n    The verification is left to the exercises.\n   \n        Let   be a positive integer and let   be a basis for  .\n        If   and   are in   and   and   are any scalars, then\n         .\n       Change of Basis in  \n    In calculus we change coordinates,\n    from rectangular to polar, for example,\n    to make certain calculations easier.\n    In order for us to be able to work effectively in different coordinate systems,\n    and to easily change back and forth as needed,\n    we will want to have a way to effectively transition from one coordinate system to another.\n    In other words, if we have two different bases for  ,\n    we want a straightforward way to translate between the coordinate vectors of any given vector in   with respect to the two bases.\n   \n            Let  ,\n             ,\n             ,\n             ,\n            and let   and  .\n           \n                  Show that   and   are bases for  .\n                 \n                  Let  .\n                  What is  ?\n                 \n                  Since   is also a basis for  ,\n                  there is also a coordinate vector for   with respect to  ,\n                  and it is reasonable to ask how\n                    is related to  .\n                  Recall that coordinate vectors respect linear combinations   that is\n                   \n                  for any vectors   and   in    with basis  ,\n                  and any scalars   and  .\n                  Use the fact that   and the linearity of the coordinate transformation with respect to the basis   to express\n                    in terms of   and\n                    (don't actually calculate\n                    and   yet,\n                  just leave your result in terms of the symbols\n                    and  .)\n                 \n                  The result of part (c) can be expressed as a matrix-vector product of the form\n                   .\n                  Describe how the columns of the matrix   are related to\n                    and  .\n                 \n                  Now calculate  ,\n                   , and  .\n                  Determine the entries of the matrix   and verify in this example that  .\n                 change of basis matrix \n                  What are   and  ?\n                  Why?\n                 \n                  If   is an   matrix and  ,\n                   ,  ,\n                    are the standard unit vectors in  \n                  (that is,   is the  th column of the   identity matrix),\n                  then what does the product   tell us about the matrix  ?\n                 \n                  Combine the results of parts (a) and (b) and the equation\n                    to explain why  .\n                 The Change of Basis Matrix in  change of basis matrix in  \n        \n    Suppose we have two different finite bases   and   for  .\n    In  \n    we learned how to translate between the two bases in the 2-dimensional case   if\n      and  ,\n    then the change of basis matrix from   to   is the matrix  .\n    This result in the 2-dimensional case generalizes to the  -dimensional case,\n    and we can determine a straightforward method for calculating a change of basis matrix.\n    The essential idea was introduced in  .\n   \n    Let   and\n      be two bases for  .\n    If   is in  ,\n    we have defined the coordinate vectors   and\n      for   with respect to   and  ,\n    respectively.\n    Recall that   if\n     .\n   \n    To see how to convert from the coordinates of   with respect to   to coordinates of   with respect to  ,\n    note that\n     .\n   \n    So we can convert from coordinates with respect to the basis   to coordinates with respect to the basis   by multiplying\n      on the left by the matrix\n     .\n   change of basis matrix change of basis matrix in  change of basis matrix \n    The change of basis matrix allows us to convert from coordinates with respect to one basis to coordinates with respect to another.\n    The result is summarized in the following theorem.\n   \n        Let   be a positive integer and let   and\n          be two bases for  .\n        Then\n         \n        for any vector   in  .\n       \n    One way to find a change of basis matrix is to utilize a basis in which computations are straightforward.\n    The following activity illustrates the process.\n   \n      Let   be the standard basis for  .\n      Let   and  ,\n      where  ,\n       ,\n       ,\n      and  .\n      You may assume that\n       .\n     \n            Find  ,\n             ,  , and  .\n           \n            Row reduce  .\n            What do you notice?\n           \n    In general, as   suggests,\n    we can use the standard basis to do our work to find a change of basis matrix\n     \n    from a basis   of   to a basis   of  .\n    Recall that we need to write   as a linear combination of the vectors in  .\n    That is, we need to find weights  ,\n     ,  ,   so that\n     .\n   \n    The weights in equation   are also the weights that satisfy the equation\n     \n    where   is any basis for  .\n    So to find these weights, we choose a convenient basis  \n    (often the standard basis, if one exists, is a good choice)\n    and then row reduce the matrix\n     .\n   \n    The row operations we will apply to row reduce the coefficient matrix\n     \n    will be the same regardless of the augmented column,\n    so we can solve all of the systems at one time by row reducing the matrix\n     .\n   \n    The result of the row reduction will be the matrix\n     .\n   \n    In particular,\n    if we use the standard basis for   as our basis  ,\n    then   for any vector  .\n    Our change of basis matrix can then be realized by row reducing the matrix\n     .\n   Properties of the Change of Basis Matrix \n    The are many different bases for  ,\n    so it is natural to ask how change of bases matrices might be related to one another.\n   \n      The sets   and\n        are bases for  .\n     \n            Find the change of basis matrix\n              from the basis   to the basis  .\n           \n            Let  .\n            Find   and  .\n           \n            Verify by matrix multiplication that  .\n           \n            Find the change of basis matrix\n              from the basis   to the basis  .\n           \n            Verify by matrix multiplication that  .\n           \n            How, specifically, are the matrices\n              and   related?\n            \n           \n            If you don't see a relationship right away,\n            what is the product of these two matrices?\n           \n     \n    seems to indicate that the inverse of a change of basis matrix is also a change of basis matrix,\n    which assumes that a change of basis matrix is always invertible.\n    The following theorem provides some properties about change of basis matrices.\n    The proofs are left for the exercises.\n   \n        Let   be a positive integer, and let  ,\n         , and   be bases for  .\n        Then\n         \n             \n              the change of basis matrix   is invertible,\n             \n           \n             \n               ,\n             \n           \n             \n               .\n             \n           \n       Examples \n    What follows are worked examples that use the concepts from this section.\n   \n              Find the coordinate vector of   with respect to the ordered basis   in the indicated space.\n             \n                      in   with  \n                   \n                    Find the coordinate vector of   with respect to the ordered basis   in the indicated vector space.\n                   \n                    We need to write   as a linear combination of\n                      and  .\n                    If  ,\n                    then equating coefficients of like power terms yields the equations\n                      and  .\n                    The solution to this system is   and  ,\n                    so  .\n                   \n                      in   with  \n                   \n                    Find the coordinate vector of   with respect to the ordered basis   in the indicated vector space.\n                   \n                    We need to write   as a linear combination of the vectors in  .\n                    If\n                     ,\n                    equating corresponding components produces the system\n                     \n                    The solution to this system is  ,\n                     ,  ,\n                    and  , so  .\n                   \n              Find the vector   given the basis   and the coordinate vector  .\n             \n                     ,\n                     \n                   \n                    Find the vector   given the basis   of   and the coordinate vector  .\n                   \n                    Since  , it follows that\n                     .\n                   \n                      in\n                      with  \n                   \n                    Find the vector   given the basis   of   and the coordinate vector  .\n                   \n                    Since  , it follows that\n                     .\n                   \n        Let   and  ,\n        where  ,\n         ,\n         ,\n         ,\n         ,\n        and  .\n       \n              Find the change of basis matrix  .\n             \n              To find   we row reduce   and get the matrix\n               .\n              So\n               .\n             \n              Use the change of basis matrix to find  .\n             \n              Since  , it follows that\n               .\n              A quick check shows that  .\n             \n              If   and   are vectors in   with\n                and  ,\n              find  .\n             \n              Properties of the matrix-vector product show that\n               .\n             \n              Find the change of basis matrix  .\n             \n              Since  ,\n              technology shows that\n               .\n             Summary \n    The key ideas in this section are the coordinate vector with respect to a basis and the change of basis matrix.\n     \n         \n          If   is a basis for  ,\n          then the coordinate vector of   with respect to   is the vector\n           ,\n          where\n           .\n         \n       \n         \n          If   and\n            are two bases for  ,\n          then the change of basis matrix from   to   is the matrix\n           \n          that satisfies\n           \n          for any vector   in  .\n         \n       \n         \n          Change of basis matrices allow us to effectively and efficiently transition from one coordinate system to another.\n         \n       \n   \n        Let   be a basis of the subspace defined by the equation  .\n        Find the coordinates of the vector\n          with respect to the basis  .\n       \n         .\n       \n        Let  .\n        Assume that   is a basis of  .\n       \n              For which vector   is  ?\n             \n              Determine coordinates of   with respect to the basis  .\n             \n        Find two different bases   and   of   so that  ,\n        where  .\n       \n         ,\n         .\n       \n        If   and\n          with respect to some basis   of  ,\n        where   and  ,\n        what are the coordinates of\n          with respect to  ?\n       \n        If   and\n          with respect to some basis  ,\n        where   and  ,\n        what are the vectors in  ?\n       \n         ,  \n       \n        Let   be a basis for  .\n        Describe how the coordinates of a vector with respect to   will change if   is replaced with  .\n       \n        Let  .\n       \n              Show that   is a basis for  .\n             \n              Row reduce an appropriate matrix.\n             \n              Let  ,\n               ,\n              and  .\n              Find  ,\n               , and  .\n             \n               ,\n               ,\n               \n             \n        Let   be a basis for  ,\n        and let   be a subset of  .\n        Let   in  .\n       \n              Show that if   is linearly independent in  ,\n              then   is linearly independent in  .\n             \n              Is the converse of part (a) true?\n              That is, if   is linearly independent in  ,\n              must   be linearly independent in  ?\n              Justify your answer.\n             \n              Repeat parts (a) and (b), replacing  linearly independent  with\n               linearly dependent .\n             \n        Verify  .\n        That is, let   be a positive integer and let   be a basis for  .\n        Show that if   and   are in   and   and   are any scalars, then\n         .\n       \n        Write   and   in terms of the basis vectors.\n       \n        Calculate the change of basis matrix\n          in each of the following cases.\n       \n                and   in  .\n             \n                and   in  .\n             \n        We can view the matrix transformation that performs a counterclockwise rotation by an angle   around the origin in   as a change of basis matrix.\n        Let   be the standard basis for  ,\n        and let  ,\n        where   and  .\n        Note that   is a vector rotated counterclockwise from the positive  -axis by the angle  ,\n        and   is a vector rotated counterclockwise from the positive  -axis by the angle  .\n       \n              Use necessary trigonometric identities to show that the change of basis matrix from   to   is\n               .\n              Then find the change of basis matrix from   to  .\n             \n              Use the trigonometric identities\n                and  .\n             \n              Let   in  .\n              Find  .\n              Then find  ,\n              where   with  .\n              Draw a picture to illustrate how the components of\n                determine coordinates of   in the coordinate system with axes   and  .\n             \n              Find   and then row reduce an appropriate matrix.\n             \n              Let   be the vector such that  .\n              Find  .\n              Draw a picture to illustrate how the components of\n                determine coordinates of   in the coordinate system with axes   and  .\n             \n               \n             permutation matrix \n        Let   be a basis for  .\n        Suppose   is another basis for   and\n         .\n        Find the vectors in the basis  .\n       \n         ,\n         ,  \n       \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              The coordinates of a non-zero vector cannot be the same in the coordinate systems defined by two different bases.\n             \n              F\n             True\/False \n              The coordinate vector of the zero vector with respect to any basis is always the zero vector.\n             True\/False \n              If   is a   dimensional subspace of  ,\n              and   is a basis of  ,\n              then   is a vector in   for any   in  .\n             \n              F\n             True\/False \n              The order of vectors in a basis do not affect the coordinates of vectors with respect to this basis.\n             True\/False \n              If   is a basis for  ,\n              then the vector   is unique to  .\n             \n              T\n             True\/False \n              If   is a basis for   and   is a vector in  ,\n              there is a vector   in   such that  .\n             True\/False \n              If   is a basis for   and   is a vector in  ,\n              then the additive inverse of\n                is the coordinate vector of the additive inverse of  .\n             \n              T\n             True\/False \n              If a coordinate vector of   in   is\n                with respect to some basis,\n              then the coordinate vector of   is\n                with respect to the same basis.\n             True\/False \n              If   and   are bases for  ,\n              then the columns of   span  .\n             \n              T\n             True\/False \n              If   and   are bases for  ,\n              then the rows of   span  .\n             True\/False \n              If   and   are bases for  ,\n              then the columns of   are linearly independent.\n             \n              T\n             True\/False \n              The matrix\n                row reduces to  .\n             Project: Planetary Orbits and Change of Basis \n    We are interested in determining the orbit of planet that orbits the sun.\n    Finding the equation of such an orbit is not difficult,\n    but just having an equation is not enough.\n    For many purposes,\n    it is important to know where the planet is fro the perspective or earth observation.\n    This is a more complicated question,\n    one we can address through change of bases matrices. \n    This project is based on the paper\n     Planetary Orbits: Change of Basis in R , Donald Teets,\n     Teaching Mathematics and its Applications: An International Journal of the IMA , Volume 17, Issue 2, 1 June 1998, Pages 66-68.\n     \n   \n      Since planetary orbits are elliptical, not circular,\n      we need to understand ellipses.\n      An ellipse is a shape like a flattened circle.\n      More specifically,\n      while a circle is the set of points equidistant from a fixed point,\n      and ellipse is a set of points so that the sum of the distances from a point on the ellipse to two fixed points\n      (called foci)\n      is a constant.\n      We can use this definition to derive an equation for an ellipse.\n      We will simplify by rotating and translating an ellipse so that its foci are at points   and  ,\n      and the constant sum is  .\n      Let   be a point on the ellipse as illustrated in  .\n      Use the fact that the sum of the distances from   to the foci is   to show that   satisfies the equation\n       ,\n      where the points   and   are the   intercepts of the ellipse.\n     An ellipse. eccentricity \n    Now we assume we have a planet\n    (different from the earth)\n    orbiting the sun and we establish how to convert back and forth from the coordinate system of earth's orbit to the coordinate system of the planet's orbit.\n    To do so we need to establish some coordinate systems.\n    We assume the orbit of earth is in the standard   plane,\n    with the sun\n    (one of the foci)\n    at the origin.\n    The elliptical orbit of the planet is in some other plane with coordinate axes   and  .\n    The two orbital planes intersect in a line.\n    Let this line be the   axis and let   be the angle the positive   axis makes with the positive   axis.\n    We can represent the elliptical orbit of the planet in the   plane,\n    but the   and   axes are not likely to be the best axes for this orbit.\n    So we define a third coordinate system   in the   plane so that the origin\n    (the position of the sun)\n    is at one focus of the planet's orbit and the   axis is the major axis of the orbit and the   axis is the minor axis of the orbit of the planet.\n    The unit vectors  ,\n     ,\n    and   in the positive  ,  ,\n    and   directions define a basis   for  ,\n    the unit vectors  ,\n     ,\n      in the positive  ,  ,\n    and   directions define a basis   for  ,\n    and the unit vectors  ,\n     ,\n      in the positive  ,  ,\n    and   directions define a basis   for  .\n    See   for illustrations.\n   Left: The planet's orbit in the   system. Right: The planes of the planet and earth orbits. \n    Finally, let   be the angle between the positive   axis and the positive   axis as shown at left in  .\n    Our first step is to find the change of basis matrix from   to  .\n   \n      Explain why the change of basis matrix   is given by\n       .\n     \n    More complicated is the change of basis matrix from   to  .\n   \n      Now we look for  .\n      Assume that the plane   in which the planet's orbit lies has equation  .\n     \n            Explain why  .\n           \n            The   axis is the intersection of the plane   with the plane  ,\n            so the equation of   axis in terms of   and   is  .\n            Now we determine the coordinates of   in terms of the basis  .\n           \n                  Explain why the vector   lies on the   axis.\n                  We take this vector to point in the positive   direction.\n                  This gives us another representation of     namely that  .\n                 \n                  Explain why a vector in the plane\n                    orthogonal to   is  .\n                 \n                  From the previous part we have\n                   .\n                  Let   be the terminal point of the projection of   onto the   plane.\n                  Show that   is orthogonal to  .\n                 \n                  Let   be the angle between the plane   and the   plane as illustrated at right in  .\n                  Explain why  .\n                  Then explain why\n                   .\n                  \n                 \n                  Use the trigonometric identities\n                    and  .\n                 \n            Finally, we find  .\n            The cross product of   and   is a vector orthogonal to   and  , so\n             .\n            Let   be the terminal point of the projection of   onto the   plane as illustrated at right in  .\n           \n                  Explain why the angle between   and\n                    is  .\n                 \n                  Explain why  . (Hint: Use the trigonometric identity  .)\n                 \n                  Since the angle from   to   is negative,\n                  this angle is  .\n                  Use this angle and the previous information to find the coordinates of the point   and,\n                  consequently,\n                  explain why\n                   .\n                  \n                 \n                  Use the trigonometric identities\n                    and  .\n                 \n            Explain why the change of basis matrix\n              from   to   is\n             .\n           Points on the ellipse in terms of angles. \n    With the change of basis matrices we can convert from any one coordinate system to the other.\n    Note that all of the change of basis matrices are written in terms of angles,\n    so it will be convenient to have a way to express points on our ellipses using angles as well.\n    Given any point on an ellipse\n    (or any point in the plane),\n    we can represent the coordinates of that point in terms of the angle   the vector through the origin and the point makes with the positive  -axis and the distance   from the origin to the point as shown in  .\n    In this representation we have\n      and  .\n   \n    So we can start in the   coordinate system with the coordinate vector of a point  .\n    Then to view this point in the   system,\n    we apply the change of basis matrices\n     .\n   \n    Of course we can also covert from   coordinates to   coordinates by applying the inverses of our change of basis matrices.\n   "
},
{
  "id": "objectives-16",
  "level": "2",
  "url": "chap_coordinate_vectors.html#objectives-16",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n        By the end of this section, you should be able to give precise and thorough answers to the\n        questions listed below. You may want to keep these questions in mind to focus your thoughts as\n        you complete the section.\n       \n           \n            How do we find the coordinate vector of a vector   with respect to a basis  ?\n           \n         \n           \n            What is a change of basis matrix?\n           \n         \n           \n            Why is a change of basis useful?\n           \n         "
},
{
  "id": "pa_3_e",
  "level": "2",
  "url": "chap_coordinate_vectors.html#pa_3_e",
  "type": "Preview Activity",
  "number": "16.1",
  "title": "",
  "body": "\n      Two vectors   and   are shown in  .\n     Vectors   and  . \n          Explain why   is a basis for  .\n         \n          Draw the vector   in  .\n          Explain your process.\n         coordinate vector \n          Since   is a basis for  ,\n          any vector   in   has a coordinate vector in the coordinate system defined by   and  .\n          But we also need to make sure that each vector has a unique coordinate vector.\n          Explain why there is no vector in   which has two different coordinate vectors.\n         \n          We can think of the vectors   and   as defining a coordinate system,\n          with   as the\n           \n          -axis and   as the\n           \n          -axis.\n          Any vector   in   can be written uniquely in the form\n           \n          and the weights serve as the coordinates of   in the  ,\n            coordinate system.\n          In this case the coordinate vector\n            of   with respect to the basis\n            is written as  .\n          Let  .\n         \n                Show that   is a basis for  .\n               \n                Find   if  .\n                Draw a picture to illustrate how   is expressed as a linear combination of the vectors in  .\n               "
},
{
  "id": "F_3_e_1",
  "level": "2",
  "url": "chap_coordinate_vectors.html#F_3_e_1",
  "type": "Figure",
  "number": "16.2",
  "title": "",
  "body": "A Coordinate System Defined by a Basis. "
},
{
  "id": "p-2717",
  "level": "2",
  "url": "chap_coordinate_vectors.html#p-2717",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "coordinate vector "
},
{
  "id": "definition-36",
  "level": "2",
  "url": "chap_coordinate_vectors.html#definition-36",
  "type": "Definition",
  "number": "16.3",
  "title": "",
  "body": "coordinate vector with respect to a basis in  coordinates with respect to a basis in  coordinate vector coordinates of the vector  \n        with respect to the basis "
},
{
  "id": "act_3_e_1",
  "level": "2",
  "url": "chap_coordinate_vectors.html#act_3_e_1",
  "type": "Activity",
  "number": "16.2",
  "title": "",
  "body": "\n      Let   and  .\n      Assume that   and   are bases for  .\n      Find   and  .\n      Note that the coordinate vector depends on the basis that is used.\n     "
},
{
  "id": "act_3_e_coor_vect_props",
  "level": "2",
  "url": "chap_coordinate_vectors.html#act_3_e_coor_vect_props",
  "type": "Activity",
  "number": "16.3",
  "title": "",
  "body": "\n      Let   be a basis for  .\n      Let   and   be vectors in   with\n        and  .\n     \n            Determine the components of the vector  .\n            How is   related to\n              and  ?\n           \n            Let   be any scalar.\n            Determine the components of the vector  .\n            How is   related to   and  \n           "
},
{
  "id": "thm_3_e_coord_vector",
  "level": "2",
  "url": "chap_coordinate_vectors.html#thm_3_e_coord_vector",
  "type": "Theorem",
  "number": "16.4",
  "title": "",
  "body": "\n        Let   be a positive integer and let   be a basis for  .\n        If   and   are in   and   and   are any scalars, then\n         .\n       "
},
{
  "id": "pa_3_e_2",
  "level": "2",
  "url": "chap_coordinate_vectors.html#pa_3_e_2",
  "type": "Activity",
  "number": "16.4",
  "title": "",
  "body": "\n            Let  ,\n             ,\n             ,\n             ,\n            and let   and  .\n           \n                  Show that   and   are bases for  .\n                 \n                  Let  .\n                  What is  ?\n                 \n                  Since   is also a basis for  ,\n                  there is also a coordinate vector for   with respect to  ,\n                  and it is reasonable to ask how\n                    is related to  .\n                  Recall that coordinate vectors respect linear combinations   that is\n                   \n                  for any vectors   and   in    with basis  ,\n                  and any scalars   and  .\n                  Use the fact that   and the linearity of the coordinate transformation with respect to the basis   to express\n                    in terms of   and\n                    (don't actually calculate\n                    and   yet,\n                  just leave your result in terms of the symbols\n                    and  .)\n                 \n                  The result of part (c) can be expressed as a matrix-vector product of the form\n                   .\n                  Describe how the columns of the matrix   are related to\n                    and  .\n                 \n                  Now calculate  ,\n                   , and  .\n                  Determine the entries of the matrix   and verify in this example that  .\n                 change of basis matrix \n                  What are   and  ?\n                  Why?\n                 \n                  If   is an   matrix and  ,\n                   ,  ,\n                    are the standard unit vectors in  \n                  (that is,   is the  th column of the   identity matrix),\n                  then what does the product   tell us about the matrix  ?\n                 \n                  Combine the results of parts (a) and (b) and the equation\n                    to explain why  .\n                 "
},
{
  "id": "p-2747",
  "level": "2",
  "url": "chap_coordinate_vectors.html#p-2747",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "change of basis matrix "
},
{
  "id": "definition-37",
  "level": "2",
  "url": "chap_coordinate_vectors.html#definition-37",
  "type": "Definition",
  "number": "16.5",
  "title": "",
  "body": "change of basis matrix in  change of basis matrix "
},
{
  "id": "thm_3_e_COB",
  "level": "2",
  "url": "chap_coordinate_vectors.html#thm_3_e_COB",
  "type": "Theorem",
  "number": "16.6",
  "title": "",
  "body": "\n        Let   be a positive integer and let   and\n          be two bases for  .\n        Then\n         \n        for any vector   in  .\n       "
},
{
  "id": "act_3_e_COB_standard_basis",
  "level": "2",
  "url": "chap_coordinate_vectors.html#act_3_e_COB_standard_basis",
  "type": "Activity",
  "number": "16.5",
  "title": "",
  "body": "\n      Let   be the standard basis for  .\n      Let   and  ,\n      where  ,\n       ,\n       ,\n      and  .\n      You may assume that\n       .\n     \n            Find  ,\n             ,  , and  .\n           \n            Row reduce  .\n            What do you notice?\n           "
},
{
  "id": "act_3_e_COB_properties",
  "level": "2",
  "url": "chap_coordinate_vectors.html#act_3_e_COB_properties",
  "type": "Activity",
  "number": "16.6",
  "title": "",
  "body": "\n      The sets   and\n        are bases for  .\n     \n            Find the change of basis matrix\n              from the basis   to the basis  .\n           \n            Let  .\n            Find   and  .\n           \n            Verify by matrix multiplication that  .\n           \n            Find the change of basis matrix\n              from the basis   to the basis  .\n           \n            Verify by matrix multiplication that  .\n           \n            How, specifically, are the matrices\n              and   related?\n            \n           \n            If you don't see a relationship right away,\n            what is the product of these two matrices?\n           "
},
{
  "id": "thm_3_e_COB_properties",
  "level": "2",
  "url": "chap_coordinate_vectors.html#thm_3_e_COB_properties",
  "type": "Theorem",
  "number": "16.7",
  "title": "",
  "body": "\n        Let   be a positive integer, and let  ,\n         , and   be bases for  .\n        Then\n         \n             \n              the change of basis matrix   is invertible,\n             \n           \n             \n               ,\n             \n           \n             \n               .\n             \n           \n       "
},
{
  "id": "example-32",
  "level": "2",
  "url": "chap_coordinate_vectors.html#example-32",
  "type": "Example",
  "number": "16.8",
  "title": "",
  "body": "\n              Find the coordinate vector of   with respect to the ordered basis   in the indicated space.\n             \n                      in   with  \n                   \n                    Find the coordinate vector of   with respect to the ordered basis   in the indicated vector space.\n                   \n                    We need to write   as a linear combination of\n                      and  .\n                    If  ,\n                    then equating coefficients of like power terms yields the equations\n                      and  .\n                    The solution to this system is   and  ,\n                    so  .\n                   \n                      in   with  \n                   \n                    Find the coordinate vector of   with respect to the ordered basis   in the indicated vector space.\n                   \n                    We need to write   as a linear combination of the vectors in  .\n                    If\n                     ,\n                    equating corresponding components produces the system\n                     \n                    The solution to this system is  ,\n                     ,  ,\n                    and  , so  .\n                   \n              Find the vector   given the basis   and the coordinate vector  .\n             \n                     ,\n                     \n                   \n                    Find the vector   given the basis   of   and the coordinate vector  .\n                   \n                    Since  , it follows that\n                     .\n                   \n                      in\n                      with  \n                   \n                    Find the vector   given the basis   of   and the coordinate vector  .\n                   \n                    Since  , it follows that\n                     .\n                   "
},
{
  "id": "example-33",
  "level": "2",
  "url": "chap_coordinate_vectors.html#example-33",
  "type": "Example",
  "number": "16.9",
  "title": "",
  "body": "\n        Let   and  ,\n        where  ,\n         ,\n         ,\n         ,\n         ,\n        and  .\n       \n              Find the change of basis matrix  .\n             \n              To find   we row reduce   and get the matrix\n               .\n              So\n               .\n             \n              Use the change of basis matrix to find  .\n             \n              Since  , it follows that\n               .\n              A quick check shows that  .\n             \n              If   and   are vectors in   with\n                and  ,\n              find  .\n             \n              Properties of the matrix-vector product show that\n               .\n             \n              Find the change of basis matrix  .\n             \n              Since  ,\n              technology shows that\n               .\n             "
},
{
  "id": "exercise-148",
  "level": "2",
  "url": "chap_coordinate_vectors.html#exercise-148",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Let   be a basis of the subspace defined by the equation  .\n        Find the coordinates of the vector\n          with respect to the basis  .\n       \n         .\n       "
},
{
  "id": "exercise-149",
  "level": "2",
  "url": "chap_coordinate_vectors.html#exercise-149",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Let  .\n        Assume that   is a basis of  .\n       \n              For which vector   is  ?\n             \n              Determine coordinates of   with respect to the basis  .\n             "
},
{
  "id": "exercise-150",
  "level": "2",
  "url": "chap_coordinate_vectors.html#exercise-150",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Find two different bases   and   of   so that  ,\n        where  .\n       \n         ,\n         .\n       "
},
{
  "id": "exercise-151",
  "level": "2",
  "url": "chap_coordinate_vectors.html#exercise-151",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        If   and\n          with respect to some basis   of  ,\n        where   and  ,\n        what are the coordinates of\n          with respect to  ?\n       "
},
{
  "id": "exercise-152",
  "level": "2",
  "url": "chap_coordinate_vectors.html#exercise-152",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        If   and\n          with respect to some basis  ,\n        where   and  ,\n        what are the vectors in  ?\n       \n         ,  \n       "
},
{
  "id": "exercise-153",
  "level": "2",
  "url": "chap_coordinate_vectors.html#exercise-153",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Let   be a basis for  .\n        Describe how the coordinates of a vector with respect to   will change if   is replaced with  .\n       "
},
{
  "id": "exercise-154",
  "level": "2",
  "url": "chap_coordinate_vectors.html#exercise-154",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        Let  .\n       \n              Show that   is a basis for  .\n             \n              Row reduce an appropriate matrix.\n             \n              Let  ,\n               ,\n              and  .\n              Find  ,\n               , and  .\n             \n               ,\n               ,\n               \n             "
},
{
  "id": "exercise-155",
  "level": "2",
  "url": "chap_coordinate_vectors.html#exercise-155",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Let   be a basis for  ,\n        and let   be a subset of  .\n        Let   in  .\n       \n              Show that if   is linearly independent in  ,\n              then   is linearly independent in  .\n             \n              Is the converse of part (a) true?\n              That is, if   is linearly independent in  ,\n              must   be linearly independent in  ?\n              Justify your answer.\n             \n              Repeat parts (a) and (b), replacing  linearly independent  with\n               linearly dependent .\n             "
},
{
  "id": "exercise-156",
  "level": "2",
  "url": "chap_coordinate_vectors.html#exercise-156",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        Verify  .\n        That is, let   be a positive integer and let   be a basis for  .\n        Show that if   and   are in   and   and   are any scalars, then\n         .\n       \n        Write   and   in terms of the basis vectors.\n       "
},
{
  "id": "exercise-157",
  "level": "2",
  "url": "chap_coordinate_vectors.html#exercise-157",
  "type": "Exercise",
  "number": "10",
  "title": "",
  "body": "\n        Calculate the change of basis matrix\n          in each of the following cases.\n       \n                and   in  .\n             \n                and   in  .\n             "
},
{
  "id": "exercise-158",
  "level": "2",
  "url": "chap_coordinate_vectors.html#exercise-158",
  "type": "Exercise",
  "number": "11",
  "title": "",
  "body": "\n        We can view the matrix transformation that performs a counterclockwise rotation by an angle   around the origin in   as a change of basis matrix.\n        Let   be the standard basis for  ,\n        and let  ,\n        where   and  .\n        Note that   is a vector rotated counterclockwise from the positive  -axis by the angle  ,\n        and   is a vector rotated counterclockwise from the positive  -axis by the angle  .\n       \n              Use necessary trigonometric identities to show that the change of basis matrix from   to   is\n               .\n              Then find the change of basis matrix from   to  .\n             \n              Use the trigonometric identities\n                and  .\n             \n              Let   in  .\n              Find  .\n              Then find  ,\n              where   with  .\n              Draw a picture to illustrate how the components of\n                determine coordinates of   in the coordinate system with axes   and  .\n             \n              Find   and then row reduce an appropriate matrix.\n             \n              Let   be the vector such that  .\n              Find  .\n              Draw a picture to illustrate how the components of\n                determine coordinates of   in the coordinate system with axes   and  .\n             \n               \n             "
},
{
  "id": "exercise-159",
  "level": "2",
  "url": "chap_coordinate_vectors.html#exercise-159",
  "type": "Exercise",
  "number": "12",
  "title": "",
  "body": "permutation matrix "
},
{
  "id": "exercise-160",
  "level": "2",
  "url": "chap_coordinate_vectors.html#exercise-160",
  "type": "Exercise",
  "number": "13",
  "title": "",
  "body": "\n        Let   be a basis for  .\n        Suppose   is another basis for   and\n         .\n        Find the vectors in the basis  .\n       \n         ,\n         ,  \n       "
},
{
  "id": "exercise-161",
  "level": "2",
  "url": "chap_coordinate_vectors.html#exercise-161",
  "type": "Exercise",
  "number": "14",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              The coordinates of a non-zero vector cannot be the same in the coordinate systems defined by two different bases.\n             \n              F\n             True\/False \n              The coordinate vector of the zero vector with respect to any basis is always the zero vector.\n             True\/False \n              If   is a   dimensional subspace of  ,\n              and   is a basis of  ,\n              then   is a vector in   for any   in  .\n             \n              F\n             True\/False \n              The order of vectors in a basis do not affect the coordinates of vectors with respect to this basis.\n             True\/False \n              If   is a basis for  ,\n              then the vector   is unique to  .\n             \n              T\n             True\/False \n              If   is a basis for   and   is a vector in  ,\n              there is a vector   in   such that  .\n             True\/False \n              If   is a basis for   and   is a vector in  ,\n              then the additive inverse of\n                is the coordinate vector of the additive inverse of  .\n             \n              T\n             True\/False \n              If a coordinate vector of   in   is\n                with respect to some basis,\n              then the coordinate vector of   is\n                with respect to the same basis.\n             True\/False \n              If   and   are bases for  ,\n              then the columns of   span  .\n             \n              T\n             True\/False \n              If   and   are bases for  ,\n              then the rows of   span  .\n             True\/False \n              If   and   are bases for  ,\n              then the columns of   are linearly independent.\n             \n              T\n             True\/False \n              The matrix\n                row reduces to  .\n             "
},
{
  "id": "act_orbits_ellipse",
  "level": "2",
  "url": "chap_coordinate_vectors.html#act_orbits_ellipse",
  "type": "Project Activity",
  "number": "16.7",
  "title": "",
  "body": "\n      Since planetary orbits are elliptical, not circular,\n      we need to understand ellipses.\n      An ellipse is a shape like a flattened circle.\n      More specifically,\n      while a circle is the set of points equidistant from a fixed point,\n      and ellipse is a set of points so that the sum of the distances from a point on the ellipse to two fixed points\n      (called foci)\n      is a constant.\n      We can use this definition to derive an equation for an ellipse.\n      We will simplify by rotating and translating an ellipse so that its foci are at points   and  ,\n      and the constant sum is  .\n      Let   be a point on the ellipse as illustrated in  .\n      Use the fact that the sum of the distances from   to the foci is   to show that   satisfies the equation\n       ,\n      where the points   and   are the   intercepts of the ellipse.\n     An ellipse. "
},
{
  "id": "p-2858",
  "level": "2",
  "url": "chap_coordinate_vectors.html#p-2858",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "eccentricity "
},
{
  "id": "F_orbits",
  "level": "2",
  "url": "chap_coordinate_vectors.html#F_orbits",
  "type": "Figure",
  "number": "16.11",
  "title": "",
  "body": "Left: The planet's orbit in the   system. Right: The planes of the planet and earth orbits. "
},
{
  "id": "act_orbits_COB_1",
  "level": "2",
  "url": "chap_coordinate_vectors.html#act_orbits_COB_1",
  "type": "Project Activity",
  "number": "16.8",
  "title": "",
  "body": "\n      Explain why the change of basis matrix   is given by\n       .\n     "
},
{
  "id": "act_orbits_COB_2",
  "level": "2",
  "url": "chap_coordinate_vectors.html#act_orbits_COB_2",
  "type": "Project Activity",
  "number": "16.9",
  "title": "",
  "body": "\n      Now we look for  .\n      Assume that the plane   in which the planet's orbit lies has equation  .\n     \n            Explain why  .\n           \n            The   axis is the intersection of the plane   with the plane  ,\n            so the equation of   axis in terms of   and   is  .\n            Now we determine the coordinates of   in terms of the basis  .\n           \n                  Explain why the vector   lies on the   axis.\n                  We take this vector to point in the positive   direction.\n                  This gives us another representation of     namely that  .\n                 \n                  Explain why a vector in the plane\n                    orthogonal to   is  .\n                 \n                  From the previous part we have\n                   .\n                  Let   be the terminal point of the projection of   onto the   plane.\n                  Show that   is orthogonal to  .\n                 \n                  Let   be the angle between the plane   and the   plane as illustrated at right in  .\n                  Explain why  .\n                  Then explain why\n                   .\n                  \n                 \n                  Use the trigonometric identities\n                    and  .\n                 \n            Finally, we find  .\n            The cross product of   and   is a vector orthogonal to   and  , so\n             .\n            Let   be the terminal point of the projection of   onto the   plane as illustrated at right in  .\n           \n                  Explain why the angle between   and\n                    is  .\n                 \n                  Explain why  . (Hint: Use the trigonometric identity  .)\n                 \n                  Since the angle from   to   is negative,\n                  this angle is  .\n                  Use this angle and the previous information to find the coordinates of the point   and,\n                  consequently,\n                  explain why\n                   .\n                  \n                 \n                  Use the trigonometric identities\n                    and  .\n                 \n            Explain why the change of basis matrix\n              from   to   is\n             .\n           "
},
{
  "id": "F_ellipse_polar",
  "level": "2",
  "url": "chap_coordinate_vectors.html#F_ellipse_polar",
  "type": "Figure",
  "number": "16.12",
  "title": "",
  "body": "Points on the ellipse in terms of angles. "
},
{
  "id": "chap_determinants",
  "level": "1",
  "url": "chap_determinants.html",
  "type": "Section",
  "number": "17",
  "title": "The Determinant",
  "body": "The Determinant \n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            How do we calculate the determinant of an   matrix?\n           \n         \n           \n            What is one important fact the determinant tells us about a matrix?\n           \n         Application: Area and Volume \n    Consider the problem of finding the area of a parallelogram determined by two vectors   and  ,\n    as illustrated at left in  .\n   A parallelogram and a parallelepiped. parallelepiped \n    It is quite a bit more difficult to break this parallelepiped into subregions whose volumes are easy to compute.\n    However, all of these computations can be made quickly by using determinants.\n    The details are later in this section.\n   Introduction \n    We know that a non-zero vector   is an eigenvector of an   matrix   if\n      for some scalar  .\n    Note that this equation can be written as  .\n    Until now, we were given eigenvalues of matrices and have used the eigenvalues to find the eigenvectors.\n    In this section we will learn an algebraic technique to find the eigenvalues ourselves.\n    We will also be able to justify why an\n      matrix has at most   eigenvalues.\n   \n    A scalar   is an eigenvalue of   if\n      has a non-trivial solution  ,\n    which happens if and only if   is not invertible.\n    In this section we will find a scalar whose value will tell us when a matrix is invertible and when it is not,\n    and use this scalar to find the eigenvalues of a matrix.\n   \n      In this activity, we will focus on   matrices.\n      Let   be a   matrix.\n      To see if   is invertible,\n      we row reduce   by replacing row 2 with  (row 2)  (row 1):\n       .\n     determinant \n      We now consider how we can use the determinant to find eigenvalues and other information about the invertibility of a matrix.\n     \n            Let  .\n            Find   by hand.\n            What does this mean about the matrix  ?\n            Can you confirm this with other methods?\n           \n            One of the eigenvalues of   is  .\n            Recall that we can rewrite the matrix equation\n              in the form  .\n            What must be true about   in order for 4 to be an eigenvalue of  ?\n            How does this relate to  ?\n           \n            Another eigenvalue of   is  .\n            What must be true about   in order for   to be an eigenvalue of  ?\n            How does this relate to  ?\n           \n            To find the eigenvalues of the matrix  ,\n            we rewrite the equation   as  .\n            The coefficient matrix of this last system has the form  .\n            The determinant of this matrix is a quadratic expression in  .\n            Since the eigenvalues will occur when the determinant is 0, we need to solve a quadratic equation.\n            Find the resulting eigenvalues.\n            (Note: One of the eigenvalues is 2.)\n           \n            Can you explain why a   matrix can have at most two eigenvalues?\n           The Determinant of a Square Matrix \n    Around 1900 or so determinants were deemed much more important than they are today.\n    In fact, determinants were used even before matrices.\n    According to Tucker \n    Tucker, Alan. (1993).\n    The Growing Importance of Linear Algebra in Undergraduate Mathematics.\n     The College Mathematics Journal , 1, 3-9.\n      determinants\n    (not matrices)\n    developed out of the study of coefficients of systems of linear equations and were used by Leibniz 150 years before the term matrix was coined by J. J. Sylvester in 1848.\n    Even though determinants are not as important as they once were,\n    the determinant of a matrix is still a useful quantity.\n    We saw in  \n    that the determinant of a matrix tells us if the matrix is invertible and how it can help us find eigenvalues.\n    In this section,\n    we will see how to find the determinant of any size matrix and how to use this determinant to find the eigenvalues.\n   \n    The determinant of a   matrix\n      is  .\n    The matrix   is invertible if and only if  .\n    We will use a recursive approach to find the determinants of larger size matrices building from the   determinants.\n    We present the result in the\n      case here   a more detailed analysis can be found at the end of this section.\n   \n    To find the determinant of a\n      matrix  ,\n    we will use the determinants of three   matrices.\n    More specifically, the determinant of  ,\n    denoted   is the quantity\n     .\n   cofactor expansion \n    We will use the specific matrix\n     \n    as an example in illustrating the cofactor expansion method in general.\n     \n         \n          We first pick a row or column of  .\n          We will pick the first row of   for this example.\n         \n       \n         \n          For each entry in the row\n          (or column)\n          we choose, in this case the first row,\n          we will calculate the determinant of a smaller matrix obtained by removing the row and the column the entry is in.\n          Let   be the smaller matrix found by deleting the  th row and  th column of  .\n          For entry  ,\n          we find the matrix   obtained by removing first row and first column:\n           .\n          For entry  , we find\n           .\n          Finally, for entry  , we find\n           .\n         \n       \n         \n          Notice that in the   determinant formula in   above,\n          the middle term had a (-) sign.\n          The signs of the terms in the cofactor expansion alternate within each row and each column.\n          More specifically,\n          the sign of a term in the  th row and  th column is  .\n          We then obtain the following pattern of the signs within each row and column:\n           \n          In particular,\n          the sign factor for   is  ,\n          for   is  ,\n          and for   is  .\n         \n       \n         \n          For each entry   in the row\n          (or column)\n          of   we chose,\n          we multiply the entry   by the determinant of   and the sign  .\n          In this case, we obtain the following numbers\n           \n           \n           \n          Note that in the last calculation,\n          since  , we did not have to evaluate the rest of the terms.\n         \n       \n         \n          Finally, we find the determinant by adding all these values:\n           .\n         \n       \n   Cofactors matrix minor cofactor minor cofactor determinant determinant cofactor expansion Laplace expansion \n    Note that when finding a cofactor expansion,\n    choosing a row or column with many zeros makes calculations easier.\n   \n            Let  .\n            Use the cofactor expansion along the first row to calculate the determinant of   by hand.\n           \n            Calculate   by using a cofactor expansion along the second row where  .\n           \n            Calculate the determinant of  .\n           \n            Which determinant property can be used to calculate the determinant in part (c)?\n            Explain how.\n            (Determinant properties are included below for easy reference.)\n           \n            Consider the matrix  .\n            Let   be the matrix which results when   times row 1 is added to row 2 of  .\n            Evaluate the determinant of   by hand to check that it is equal to the determinant of  ,\n            which verifies one other determinant property\n            (in a specific case).\n           \n    As with any new idea, like the determinant,\n    we must ask what properties are satisfied.\n    We state the following theorem without proof for the time being.\n    For the interested reader,\n    the proof of many of these properties is given in   and others in the exercises.\n   \n        Given   matrices  , the following hold:\n         \n             \n               ,\n              and in particular   for any positive integer  .\n             \n           \n             \n               .\n             \n           \n             \n                is invertible if and only if  .\n             \n           \n             \n              If   is invertible, then  .\n             \n           \n             \n              For a   matrix\n               ,\n               .\n             \n           \n             \n              If   is upper\/lower triangular,\n              then   is the product of the entries on the diagonal.\n             \n           \n             \n              The determinant of a matrix is the product of the eigenvalues,\n              with each eigenvalue repeated as many times as its multiplicity.\n             \n           \n             \n              Effect of row operations:\n               \n                   \n                    Adding a multiple of a row to another does NOT change the determinant of the matrix.\n                   \n                 \n                   \n                    Multiplying a row by a constant multiplies the determinant by the same constant.\n                   \n                 \n                   \n                    Row swapping multiplies the determinant by  .\n                   \n                 \n             \n           \n             \n              If the row echelon form   of   is obtained by adding multiples of one row to another,\n              and row swapping,\n              then   is equal to\n                multiplied by   where   is the number of row swappings done during the row reduction.\n             \n           \n       \n    Note that if we were to find the determinant of a\n      matrix using the cofactor method,\n    we will calculate determinants of 4 matrices of size  ,\n    each of which will require 3 determinant calculations again.\n    So, we will need a total of 12 calculations of determinants of   matrices.\n    That is a lot of calculations.\n    There are other, more efficient,\n    methods for calculating determinants.\n    For example, we can row reduce the matrix,\n    keeping track of the effect that each row operation has on the determinant.\n   The Determinant of a   Matrix \n    Earlier we defined the determinant of a   matrix.\n    In this section we endeavor to understand the motivation behind that definition.\n   \n    We will repeat the process we went through in the\n      case to see how to define the determinant of a   matrix.\n    Let\n     .\n   \n    To find the inverse of   we augment   by the   identity matrix\n     \n    and row reduce the matrix\n    (using appropriate technology)\n    to obtain\n     ,\n    where\n     \n   \n    In this case, we can see that the inverse of the\n      matrix   will be defined if and only if  .\n    So, in the   case the determinant of   will be given by the value of   in Equation  .\n    What remains is for us to see how this is related to determinants of\n      sub-matrices of  .\n   \n    To start, we collect all terms involving   in  .\n    A little algebra shows that\n     .\n   \n    Now let's collect the remaining terms involving  :\n     .\n   \n    Finally, we collect the terms involving  :\n     .\n   \n    Now we can connect the determinant of   to determinants of\n      sub-matrices of  .\n     \n         \n          Notice that\n           \n          is the determinant of the   matrix\n            obtained from   by deleting the first row and first column.\n         \n       \n         \n          Similarly, the expression\n           \n          is the determinant of the   matrix\n            obtained from   by deleting the first row and second column.\n         \n       \n         \n          Finally, the expression\n           \n          is the determinant of the   matrix\n            obtained from   by deleting the first row and third column.\n         \n       \n   \n    Putting this all together gives us formula   for the determinant of a\n      matrix as we defined earlier.\n   Two Devices for Remembering Determinants \n    There are useful ways to remember how to calculate the formulas for determinants of\n      and   matrices.\n    In the   case of  ,\n    we saw that\n     .\n   \n    This makes   the product of the diagonal elements   and   minus the product of the off-diagonal elements   and  .\n    We can visualize this in an array by drawing arrows across the diagonal and off-diagonal,\n    with a plus sign on the diagonal arrow indicting that we add the product of the diagonal elements and a minus sign on the off-diagonal arrow indicating that we subtract the product of the off-diagonal elements as shown in  .\n   A diagram to remember the   determinant. \n    We can do a similar thing for the determinant of a   matrix.\n    In this case, we extend the   array to a\n      array by adjoining the first two columns onto the matrix.\n    We then add the products along the diagonals going from left to right and subtract the products along the diagonals going from right to left as indicated in  .\n   A diagram to remember the   determinant. Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        For each of the following\n         \n             \n              Identify the sub-matrices  \n             \n           \n             \n              Determine the cofactors  .\n             \n           \n             \n              Use the cofactor expansion to calculate the determinant.\n             \n           \n       \n               \n             \n              With a   matrix,\n              we will find the sub-matrices  ,\n               , and  .\n              Recall that   is the sub-matrix of   obtained by deleting the  th row and  th column of  .\n              Thus,\n               .\n              The  th cofactor is  , so\n               .\n              Then\n               .\n             \n               \n             \n              With a   matrix,\n              we will find the sub-matrices  ,  ,\n               , and  .\n              We see that\n               .\n              To calculate the  th cofactor  ,\n              we need to calculate the determinants of the  .\n              Using the device for calculating the determinant of a   matrix we have that\n               ,\n               ,\n               ,\n              and\n               .\n              Then\n               \n              and so\n               .\n             \n        Show that for any   matrices   and  ,\n         .\n       \n        Let   and  .\n        Then\n         .\n       \n        So\n         .\n       \n        Also,\n         .\n       \n        We conclude that   if   and   are   matrices.\n       Summary \n       cofactor \n     \n       \n        The matrix   is invertible if and only if  .\n       \n     \n        Use the cofactor expansion to explain why multiplying each of the entries of a\n          matrix   by 2 multiplies the determinant of   by 8.\n       \n         .\n       \n        Use the determinant criterion to determine for which   the matrix   is invertible.\n       \n        Let   be a square matrix.\n       \n              Explain why  \n             \n               .\n             \n              Expand on the argument from (a) to explain why\n                for any positive integer  .\n             \n               .\n             \n              Suppose that   is an invertible matrix and   is a positive integer.\n              Must   be an invertible matrix?\n              Why or why not?\n             \n              Yes.\n             \n        Let   be an invertible matrix.\n        Explain why   using determinant properties.\n       \n        Simplify the following determinant expression using determinant properties:\n         \n       \n         \n       \n        Find the eigenvalues of the following matrices.\n        Find a basis for and the dimension of each eigenspace.\n       \n               \n             \n               \n             \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              For any two\n                matrices   and  ,\n               .\n             \n              F\n             True\/False \n              For any square matrix  ,\n               .\n             True\/False \n              For any square matrix  ,  .\n             \n              F\n             True\/False \n              The determinant of a square matrix with all non-zero entries is non-zero.\n             True\/False \n              If the determinant of   is non-zero,\n              then so is the determinant of  .\n             \n              T\n             True\/False \n              If the determinant of a matrix   is 0, then one of the rows of   is a linear combination of the other rows.\n             True\/False \n              For any square matrix  ,  .\n             \n              F\n             True\/False \n              If   and   are\n                matrices and   is invertible,\n              then   and   are invertible.\n             True\/False \n              If   is the zero matrix,\n              then the only eigenvalue of   is 0.\n             \n              T\n             True\/False \n              If 0 is an eigenvalue of  ,\n              then 0 is an eigenvalue of   for any   of the same size as  .\n             True\/False \n              Suppose   is a   matrix.\n              Then any three eigenvectors of   will form a basis of  .\n             \n              F\n             Project: Area and Volume Using Determinants \n    The approach we will take to connecting area (volume) to the determinant will help shed light on properties of the determinant that we will discuss from an algebraic perspective in a later section.\n    First, we mention some basic properties of area\n    (we focus on area for now,\n    but these same properties are valid for volumes as well). volume).\n    As a shorthand,\n    we denote the area of a region   by  .\n     \n         \n          Area cannot be negative.\n         \n       \n         \n          If two regions   and   don't overlap,\n          then the area of the union of the regions is equal to the sum of the areas of the regions.\n          That is, if  ,\n          then  .\n         \n       \n         \n          Area is invariant under translation.\n          That is, if we move a geometric region by the same amount uniformly in a given direction,\n          the area of the original region and the area of the transformed region are the same.\n          A translation of a region is done by just adding a fixed vector to each vector in the region.\n          That is, a translation by a vector   is a function   such that the image\n            of a region   is defined as\n           .\n          Since area is translation invariant,\n           .\n         \n       \n         \n          The area of a one-dimensional object like a line segment is  .\n         \n       \n   \n    Now we turn our attention to areas of parallelograms.\n    Let   and   be vectors in  .\n    The parallelogram   defined by   and   with point   as basepoint is the set\n     .\n   \n    An illustration of such a parallelogram is shown at left in  .\n   A parallelogram and a translated, rotated parallelogram. \n    If   and  ,\n    then we will also represent\n      as  .\n   \n    Since area is translation and rotation invariant,\n    we can translate our parallelogram by\n      to place its basepoint at the origin,\n    then rotate by an angle   (as shown at left in  .\n    This transforms the vector   to a vector   and the vector   to a vector   as shown at right in  .\n    With this in mind we can always assume that our parallelograms have one vertex at the origin,\n    with   along the  -axis,\n    and   in standard position.\n    Now we can investigate how to calculate the area of a parallelogram.\n   \n      There are two situations to consider when we want to find the area of a parallelogram determined by vectors   and  ,\n      both shown in  .\n      The parallelogram will be determined by the lengths of these vectors.\n     Parallelograms formed by   and  \n          In the situation depicted at left in  ,\n          use geometry to explain why  .\n          \n         \n          What can we say about the triangles   and  ?\n         \n          In the situation depicted at right in  ,\n          use geometry to again explain why\n           . (Hint: What can we say about\n            and  ?)\n         \n    The result of  \n    is that the area of   is given by  ,\n    where   is the height of the parallelogram determined by dropping a perpendicular from the terminal point of   to the line determined by the vector  .\n   \n    Now we turn to the question of how the determinant is related to area of a parallelogram.\n    Our approach will use some properties of the area of  .\n   \n      Let   and   be vectors that determine a parallelogram in  .\n     Parallelograms formed by   and   and by   and  . \n          Explain why\n           \n         \n          If   is any scalar,\n          then   either stretches or compresses  .\n          Use this idea,\n          and the result of  ,\n          to explain why\n           \n          for any real number  .\n          A representative picture of this situation is shown at left in   for a value of  .\n          You will also need to consider what happens when  .\n         \n          Finally, use the result of   to explain why\n           \n          for any real number  .\n          A representative picture is shown at right in  .\n         \n    Properties   and   will allow us to calculate the area of the parallelogram determined by vectors   and  .\n   \n      Let   and  .\n      We will now demonstrate that\n       .\n     \n      Before we begin,\n      note that if both   and   are  ,\n      then   and   are parallel.\n      This makes   a line segment and so  .\n      But if  , it is also the case that\n       \n      as well.\n      So we can assume that at least one of  ,\n        is not  .\n      Since  ,\n      we can assume without loss of generality that  .\n     \n            Explain using properties   and    as appropriate why\n             .\n           \n            Let  .\n            Recall that our alternate representation of   allows us to write\n             .\n            This should seem very suggestive.\n            We are essentially applying the process of Gaussian elimination to our parallelogram matrix to reduce it to a diagonal matrix.\n            From there, we can calculate the area.\n            The matrix form should indicate the next step    applying an operation to eliminate the entry in the first row and second column.\n            To do this, we need to consider what happens if\n              and if  .\n           \n                  Assume that  .\n                  Explain why  .\n                  Then explain why  .\n                 \n                  Now we consider the case when  .\n                  Complete the process as in part (a),\n                  using properties   and   \n                  (compare to Gaussian elimination)\n                  to continue to reduce the problem of calculating\n                    to one of calculating  .\n                  Use this process to conclude that\n                   .\n                 \n    We can apply the same arguments as above using rotations,\n    translations,\n    shearings,\n    and scalings to show that the properties of area given above work in any dimension.\n    Given vectors  ,\n     ,  ,   in  , we let\n     .\n   \n    If  ,\n    then   is the parallelogram determined by   and   with basepoint  .\n    If  ,\n    then   is the parallelepiped with basepoint   determined by  ,\n     , and  .\n    In higher dimensions the sets\n      are called parallelotopes,\n    and we use the notation   for their volume.\n    The  -dimensional volumes of these paralleotopes satisfy the following properties:\n     \n    for any   and  .\n     \n    for any real number   and any  .\n     \n    for any real number   and any distinct   and  .\n   \n      We now show that   is the absolute value of the determinant of  .\n      For easier notation, let  ,\n       ,\n      and  .\n      As we argued in the 2-dimensional case,\n      we can assume that all terms that we need to be nonzero are nonzero,\n      and we can do so without verification.\n     \n            Explain how property   shows that   is equal to\n             .\n            \n           \n            Think about how these properties are related to row operations.\n           \n            Now let   and   .\n            Explain how property   shows that   is equal to\n             ,\n            where\n             .\n           \n            Just as we saw in the 2-dimensional case,\n            we can proceed to use the diagonal entries to eliminate the entries above the diagonal without changing the volume to see that\n             .\n            Complete the process, applying appropriate properties to explain why\n             \n            for some constant  .\n            Find the constant and, as a result,\n            find a specific expression for\n              involving a determinant.\n           \n    Properties  ,  ,\n    and   involve the analogs of row operations on matrices,\n    and we will prove algebraically that the determinant exhibits the same properties.\n    In fact, the determinant can be uniquely defined by these properties.\n    So in a sense, the determinant is an area or volume function.\n   "
},
{
  "id": "objectives-17",
  "level": "2",
  "url": "chap_determinants.html#objectives-17",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            How do we calculate the determinant of an   matrix?\n           \n         \n           \n            What is one important fact the determinant tells us about a matrix?\n           \n         "
},
{
  "id": "F_det_area",
  "level": "2",
  "url": "chap_determinants.html#F_det_area",
  "type": "Figure",
  "number": "17.1",
  "title": "",
  "body": "A parallelogram and a parallelepiped. "
},
{
  "id": "p-2884",
  "level": "2",
  "url": "chap_determinants.html#p-2884",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "parallelepiped "
},
{
  "id": "pa_4_a",
  "level": "2",
  "url": "chap_determinants.html#pa_4_a",
  "type": "Preview Activity",
  "number": "17.1",
  "title": "",
  "body": "\n      In this activity, we will focus on   matrices.\n      Let   be a   matrix.\n      To see if   is invertible,\n      we row reduce   by replacing row 2 with  (row 2)  (row 1):\n       .\n     determinant \n      We now consider how we can use the determinant to find eigenvalues and other information about the invertibility of a matrix.\n     \n            Let  .\n            Find   by hand.\n            What does this mean about the matrix  ?\n            Can you confirm this with other methods?\n           \n            One of the eigenvalues of   is  .\n            Recall that we can rewrite the matrix equation\n              in the form  .\n            What must be true about   in order for 4 to be an eigenvalue of  ?\n            How does this relate to  ?\n           \n            Another eigenvalue of   is  .\n            What must be true about   in order for   to be an eigenvalue of  ?\n            How does this relate to  ?\n           \n            To find the eigenvalues of the matrix  ,\n            we rewrite the equation   as  .\n            The coefficient matrix of this last system has the form  .\n            The determinant of this matrix is a quadratic expression in  .\n            Since the eigenvalues will occur when the determinant is 0, we need to solve a quadratic equation.\n            Find the resulting eigenvalues.\n            (Note: One of the eigenvalues is 2.)\n           \n            Can you explain why a   matrix can have at most two eigenvalues?\n           "
},
{
  "id": "p-2899",
  "level": "2",
  "url": "chap_determinants.html#p-2899",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "cofactor expansion "
},
{
  "id": "p-2906",
  "level": "2",
  "url": "chap_determinants.html#p-2906",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "minor cofactor "
},
{
  "id": "definition-38",
  "level": "2",
  "url": "chap_determinants.html#definition-38",
  "type": "Definition",
  "number": "17.2",
  "title": "",
  "body": "determinant determinant "
},
{
  "id": "p-2911",
  "level": "2",
  "url": "chap_determinants.html#p-2911",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "cofactor expansion Laplace expansion "
},
{
  "id": "act_4_a_1",
  "level": "2",
  "url": "chap_determinants.html#act_4_a_1",
  "type": "Activity",
  "number": "17.2",
  "title": "",
  "body": "\n            Let  .\n            Use the cofactor expansion along the first row to calculate the determinant of   by hand.\n           \n            Calculate   by using a cofactor expansion along the second row where  .\n           \n            Calculate the determinant of  .\n           \n            Which determinant property can be used to calculate the determinant in part (c)?\n            Explain how.\n            (Determinant properties are included below for easy reference.)\n           \n            Consider the matrix  .\n            Let   be the matrix which results when   times row 1 is added to row 2 of  .\n            Evaluate the determinant of   by hand to check that it is equal to the determinant of  ,\n            which verifies one other determinant property\n            (in a specific case).\n           "
},
{
  "id": "thm_determinant_properties",
  "level": "2",
  "url": "chap_determinants.html#thm_determinant_properties",
  "type": "Theorem",
  "number": "17.3",
  "title": "",
  "body": "\n        Given   matrices  , the following hold:\n         \n             \n               ,\n              and in particular   for any positive integer  .\n             \n           \n             \n               .\n             \n           \n             \n                is invertible if and only if  .\n             \n           \n             \n              If   is invertible, then  .\n             \n           \n             \n              For a   matrix\n               ,\n               .\n             \n           \n             \n              If   is upper\/lower triangular,\n              then   is the product of the entries on the diagonal.\n             \n           \n             \n              The determinant of a matrix is the product of the eigenvalues,\n              with each eigenvalue repeated as many times as its multiplicity.\n             \n           \n             \n              Effect of row operations:\n               \n                   \n                    Adding a multiple of a row to another does NOT change the determinant of the matrix.\n                   \n                 \n                   \n                    Multiplying a row by a constant multiplies the determinant by the same constant.\n                   \n                 \n                   \n                    Row swapping multiplies the determinant by  .\n                   \n                 \n             \n           \n             \n              If the row echelon form   of   is obtained by adding multiples of one row to another,\n              and row swapping,\n              then   is equal to\n                multiplied by   where   is the number of row swappings done during the row reduction.\n             \n           \n       "
},
{
  "id": "F_2by2_determinant",
  "level": "2",
  "url": "chap_determinants.html#F_2by2_determinant",
  "type": "Figure",
  "number": "17.4",
  "title": "",
  "body": "A diagram to remember the   determinant. "
},
{
  "id": "F_3by3_determinant",
  "level": "2",
  "url": "chap_determinants.html#F_3by3_determinant",
  "type": "Figure",
  "number": "17.5",
  "title": "",
  "body": "A diagram to remember the   determinant. "
},
{
  "id": "example-34",
  "level": "2",
  "url": "chap_determinants.html#example-34",
  "type": "Example",
  "number": "17.6",
  "title": "",
  "body": "\n        For each of the following\n         \n             \n              Identify the sub-matrices  \n             \n           \n             \n              Determine the cofactors  .\n             \n           \n             \n              Use the cofactor expansion to calculate the determinant.\n             \n           \n       \n               \n             \n              With a   matrix,\n              we will find the sub-matrices  ,\n               , and  .\n              Recall that   is the sub-matrix of   obtained by deleting the  th row and  th column of  .\n              Thus,\n               .\n              The  th cofactor is  , so\n               .\n              Then\n               .\n             \n               \n             \n              With a   matrix,\n              we will find the sub-matrices  ,  ,\n               , and  .\n              We see that\n               .\n              To calculate the  th cofactor  ,\n              we need to calculate the determinants of the  .\n              Using the device for calculating the determinant of a   matrix we have that\n               ,\n               ,\n               ,\n              and\n               .\n              Then\n               \n              and so\n               .\n             "
},
{
  "id": "example-35",
  "level": "2",
  "url": "chap_determinants.html#example-35",
  "type": "Example",
  "number": "17.7",
  "title": "",
  "body": "\n        Show that for any   matrices   and  ,\n         .\n       \n        Let   and  .\n        Then\n         .\n       \n        So\n         .\n       \n        Also,\n         .\n       \n        We conclude that   if   and   are   matrices.\n       "
},
{
  "id": "p-2962",
  "level": "2",
  "url": "chap_determinants.html#p-2962",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "cofactor "
},
{
  "id": "exercise-162",
  "level": "2",
  "url": "chap_determinants.html#exercise-162",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Use the cofactor expansion to explain why multiplying each of the entries of a\n          matrix   by 2 multiplies the determinant of   by 8.\n       \n         .\n       "
},
{
  "id": "exercise-163",
  "level": "2",
  "url": "chap_determinants.html#exercise-163",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Use the determinant criterion to determine for which   the matrix   is invertible.\n       "
},
{
  "id": "exercise-164",
  "level": "2",
  "url": "chap_determinants.html#exercise-164",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Let   be a square matrix.\n       \n              Explain why  \n             \n               .\n             \n              Expand on the argument from (a) to explain why\n                for any positive integer  .\n             \n               .\n             \n              Suppose that   is an invertible matrix and   is a positive integer.\n              Must   be an invertible matrix?\n              Why or why not?\n             \n              Yes.\n             "
},
{
  "id": "exercise-165",
  "level": "2",
  "url": "chap_determinants.html#exercise-165",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Let   be an invertible matrix.\n        Explain why   using determinant properties.\n       "
},
{
  "id": "exercise-166",
  "level": "2",
  "url": "chap_determinants.html#exercise-166",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Simplify the following determinant expression using determinant properties:\n         \n       \n         \n       "
},
{
  "id": "exercise-167",
  "level": "2",
  "url": "chap_determinants.html#exercise-167",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Find the eigenvalues of the following matrices.\n        Find a basis for and the dimension of each eigenspace.\n       \n               \n             \n               \n             "
},
{
  "id": "exercise-168",
  "level": "2",
  "url": "chap_determinants.html#exercise-168",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              For any two\n                matrices   and  ,\n               .\n             \n              F\n             True\/False \n              For any square matrix  ,\n               .\n             True\/False \n              For any square matrix  ,  .\n             \n              F\n             True\/False \n              The determinant of a square matrix with all non-zero entries is non-zero.\n             True\/False \n              If the determinant of   is non-zero,\n              then so is the determinant of  .\n             \n              T\n             True\/False \n              If the determinant of a matrix   is 0, then one of the rows of   is a linear combination of the other rows.\n             True\/False \n              For any square matrix  ,  .\n             \n              F\n             True\/False \n              If   and   are\n                matrices and   is invertible,\n              then   and   are invertible.\n             True\/False \n              If   is the zero matrix,\n              then the only eigenvalue of   is 0.\n             \n              T\n             True\/False \n              If 0 is an eigenvalue of  ,\n              then 0 is an eigenvalue of   for any   of the same size as  .\n             True\/False \n              Suppose   is a   matrix.\n              Then any three eigenvectors of   will form a basis of  .\n             \n              F\n             "
},
{
  "id": "F_parallelograms",
  "level": "2",
  "url": "chap_determinants.html#F_parallelograms",
  "type": "Figure",
  "number": "17.8",
  "title": "",
  "body": "A parallelogram and a translated, rotated parallelogram. "
},
{
  "id": "act_parallelogram_area",
  "level": "2",
  "url": "chap_determinants.html#act_parallelogram_area",
  "type": "Project Activity",
  "number": "17.3",
  "title": "",
  "body": "\n      There are two situations to consider when we want to find the area of a parallelogram determined by vectors   and  ,\n      both shown in  .\n      The parallelogram will be determined by the lengths of these vectors.\n     Parallelograms formed by   and  \n          In the situation depicted at left in  ,\n          use geometry to explain why  .\n          \n         \n          What can we say about the triangles   and  ?\n         \n          In the situation depicted at right in  ,\n          use geometry to again explain why\n           . (Hint: What can we say about\n            and  ?)\n         "
},
{
  "id": "act_P_area_properties",
  "level": "2",
  "url": "chap_determinants.html#act_P_area_properties",
  "type": "Project Activity",
  "number": "17.4",
  "title": "",
  "body": "\n      Let   and   be vectors that determine a parallelogram in  .\n     Parallelograms formed by   and   and by   and  . \n          Explain why\n           \n         \n          If   is any scalar,\n          then   either stretches or compresses  .\n          Use this idea,\n          and the result of  ,\n          to explain why\n           \n          for any real number  .\n          A representative picture of this situation is shown at left in   for a value of  .\n          You will also need to consider what happens when  .\n         \n          Finally, use the result of   to explain why\n           \n          for any real number  .\n          A representative picture is shown at right in  .\n         "
},
{
  "id": "act_det_area",
  "level": "2",
  "url": "chap_determinants.html#act_det_area",
  "type": "Project Activity",
  "number": "17.5",
  "title": "",
  "body": "\n      Let   and  .\n      We will now demonstrate that\n       .\n     \n      Before we begin,\n      note that if both   and   are  ,\n      then   and   are parallel.\n      This makes   a line segment and so  .\n      But if  , it is also the case that\n       \n      as well.\n      So we can assume that at least one of  ,\n        is not  .\n      Since  ,\n      we can assume without loss of generality that  .\n     \n            Explain using properties   and    as appropriate why\n             .\n           \n            Let  .\n            Recall that our alternate representation of   allows us to write\n             .\n            This should seem very suggestive.\n            We are essentially applying the process of Gaussian elimination to our parallelogram matrix to reduce it to a diagonal matrix.\n            From there, we can calculate the area.\n            The matrix form should indicate the next step    applying an operation to eliminate the entry in the first row and second column.\n            To do this, we need to consider what happens if\n              and if  .\n           \n                  Assume that  .\n                  Explain why  .\n                  Then explain why  .\n                 \n                  Now we consider the case when  .\n                  Complete the process as in part (a),\n                  using properties   and   \n                  (compare to Gaussian elimination)\n                  to continue to reduce the problem of calculating\n                    to one of calculating  .\n                  Use this process to conclude that\n                   .\n                 "
},
{
  "id": "act_det_vol",
  "level": "2",
  "url": "chap_determinants.html#act_det_vol",
  "type": "Project Activity",
  "number": "17.6",
  "title": "",
  "body": "\n      We now show that   is the absolute value of the determinant of  .\n      For easier notation, let  ,\n       ,\n      and  .\n      As we argued in the 2-dimensional case,\n      we can assume that all terms that we need to be nonzero are nonzero,\n      and we can do so without verification.\n     \n            Explain how property   shows that   is equal to\n             .\n            \n           \n            Think about how these properties are related to row operations.\n           \n            Now let   and   .\n            Explain how property   shows that   is equal to\n             ,\n            where\n             .\n           \n            Just as we saw in the 2-dimensional case,\n            we can proceed to use the diagonal entries to eliminate the entries above the diagonal without changing the volume to see that\n             .\n            Complete the process, applying appropriate properties to explain why\n             \n            for some constant  .\n            Find the constant and, as a result,\n            find a specific expression for\n              involving a determinant.\n           "
},
{
  "id": "chap_characteristic_equation",
  "level": "1",
  "url": "chap_characteristic_equation.html",
  "type": "Section",
  "number": "18",
  "title": "The Characteristic Equation",
  "body": "The Characteristic Equation \n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is the characteristic polynomial of a matrix?\n           \n         \n           \n            What is the characteristic equation of a matrix?\n           \n         \n           \n            How and why is the characteristic equation of a matrix useful?\n           \n         \n           \n            How many different eigenvalues can an   matrix have?\n           \n         \n           \n            How large can the dimension of the eigenspace corresponding to an eigenvalue be?\n           \n         Application: Modeling the Second Law of Thermodynamics \n    Pour cream into your cup of coffee and the cream spreads out;\n    straighten up your room and it soon becomes messy again;\n    when gasoline is mixed with air in a car's cylinders,\n    it explodes if a spark is introduced.\n    In each of these cases a transition from a low energy state\n    (your room is straightened up)\n    to a higher energy state (a messy,\n    disorganized room) occurs.\n    This can be described by entropy   a measure of the energy in a system.\n    Low energy is organized\n    (like ice cubes)\n    and higher energy is not\n    (like water vapor).\n    It is a fundamental property of energy\n    (as described by the second law of thermodynamics)\n    that the entropy of a system cannot decrease.\n    In other words, in the absence of any external intervention,\n    things never become more organized.\n   \n    The Ehrenfest model \n    named after Paul and Tatiana Ehrenfest who introduced it in\n     \n    ber zwei bekannte Einwnde gegen das Boltzmannsche H-Theorem, \n     Physikalishce Zeitschrift ,\n    vol. 8 (1907), pp. 311-314)\n      is a Markov process proposed to explain the statistical interpretation of the second law of thermodynamics using the diffusion of gas molecules.\n    This process can be modeled as a problem of balls and bins,\n    as we will do later in this section.\n    The characteristic polynomial of the transition matrix will help us find the eigenvalues and allow us to analyze our model.\n   Introduction \n    We have seen that the eigenvalues of an\n      matrix   are the scalars   so that\n      has a nontrivial null space.\n    Since a matrix has a nontrivial null space if and only if the matrix is not invertible,\n    we can also say that   is an eigenvalue of   if\n     .\n   characteristic equation algebraic \n    In this activity,\n    our goal will be to use the characteristic equation to obtain information about eigenvalues and eigenvectors of a matrix with real entries.\n   \n            For each of the following parts,\n            use the characteristic equation to determine the eigenvalues of  .\n            Then, for each eigenvalue  ,\n            find a basis of the corresponding eigenspace,\n            i.e.,  .\n            You might want to recall how to find a basis for the null space of a matrix from  .\n            Also, make sure that your eigenvalue candidate   yields nonzero eigenvectors in\n              for otherwise   will not be an eigenvalue.\n           \n                     \n                 \n                     \n                 \n                    \n                 \n            Use your eigenvalue and eigenvector calculations of the above problem as a guidance to answer the following questions about a matrix with real entries.\n           \n                  At most how many eigenvalues can a   matrix have?\n                  Is it possible to have no eigenvalues?\n                  Is it possible to have only one eigenvalue?\n                  Explain.\n                 \n                  If a matrix is an upper-triangular matrix (i.e., all entries below the diagonal are 0's, as in the first two matrices of the previous problem),\n                  what can you say about its eigenvalues?\n                  Explain.\n                 \n                  How many linearly independent eigenvectors can be found for a   matrix?\n                  Is it possible to have a matrix without 2 linearly independent eigenvectors?\n                  Explain.\n                 \n            Using the characteristic equation,\n            determine which matrices have 0 as an eigenvalue.\n           The Characteristic Equation \n    Until now, we have been given eigenvalues or eigenvectors of a matrix and determined eigenvectors and eigenvalues from the known information.\n    In this section we use determinants to find\n    (or approximate)\n    the eigenvalues of a matrix.\n    From there we can find\n    (or approximate)\n    the corresponding eigenvectors.\n    The tool we will use is a polynomial equation,\n    the  characteristic equation ,\n    of a square matrix whose roots are the eigenvalues of the matrix.\n    The characteristic equation will then provide us with an  algebraic \n    way of finding the eigenvalues of a square matrix.\n   \n    We have seen that the eigenvalues of a square matrix   are the scalars   so that\n      has a nontrivial null space.\n    Since a matrix has a nontrivial null space if and only if the matrix is not invertible,\n    we can also say that   is an eigenvalue of   if\n     .\n   \n    Note that if   is an   matrix,\n    then   is a polynomial of degree  .\n    Furthermore, if   has real entries,\n    the polynomial has real coefficients.\n    This polynomial,\n    and the equation   are given special names.\n   characteristic polynomial characteristic equation characteristic polynomial characteristic equation \n    So the characteristic equation of   gives us an algebraic way of finding the eigenvalues of  .\n   \n            Find the characteristic polynomial of the matrix  ,\n            and use the characteristic polynomial to find all of the eigenvalues of  .\n           \n            Verify that 1 and 2 are the only eigenvalues of the matrix  .\n           (algebraic) multiplicity multiplicity algebraic (algebraic) multiplicity \n    Thus, in  \n    (b) the eigenvalue 1 has multiplicity 3 and the eigenvalue 2 has multiplicity 1.\n    Notice that if we count the eigenvalues of an\n      matrix with their multiplicities,\n    the total will always be  .\n   \n    If   is a matrix with real entries,\n    then the characteristic polynomial will have real coefficients.\n    It is possible that the characteristic polynomial can have complex roots,\n    and that the matrix   has complex eigenvalues.\n    The Fundamental Theorem of Algebra shows us that if a real matrix has complex eigenvalues,\n    then those eigenvalues will appear in conjugate pairs,\n    i.e., if   is an eigenvalue of  ,\n    then   is another eigenvalue of  .\n    Furthermore, for an odd degree polynomial,\n    since the complex eigenvalues will come in conjugate pairs,\n    we will be able to find at least one real eigenvalue.\n   \n    We now summarize the information we have so far about eigenvalues of an   real matrix:\n   \n        Let   be an   matrix with real entries.\n        Then\n         \n             \n              There are at most   eigenvalues of  .\n              If each eigenvalue\n              (including complex eigenvalues)\n              is counted with its multiplicity,\n              there are exactly   eigenvalues.\n             \n           \n             \n              If   has a complex eigenvalue  ,\n              the complex conjugate of   is also an eigenvalue of  .\n             \n           \n             \n              If   is odd,   has at least one real eigenvalue.\n             \n           \n             \n              If   is upper or lower-triangular,\n              the eigenvalues are the entries on the diagonal.\n             \n           \n       Eigenspaces, A Geometric Example \n    Recall that for each eigenvalue   of an   matrix  ,\n    the eigenspace of   corresponding to the eigenvalue   is  .\n    These eigenspaces can tell us important information about the matrix transformation defined by  .\n    For example,\n    consider the matrix transformation   from   to   defined by  , where\n     .\n   \n    We are interested in understanding what this matrix transformation does to vectors in  .\n    First we note that   has eigenvalues\n      and  ,\n    with   having multiplicity  .\n    There is a pair   and\n      of linearly independent eigenvectors for   corresponding to the eigenvalue   and an eigenvector\n      for   corresponding to the eigenvalue  .\n    Note that the vectors  ,\n     , and   are linearly independent\n    (recall from Theorem that eigenvectors corresponding to different eigenvalues are always linearly independent).\n    So any vector   in   can be written uniquely as a linear combination of  ,\n     , and  .\n    Let's now consider the action of the matrix transformation   on a linear combination of  ,\n     , and  .\n    Note that\n     .\n   \n    Equation   illustrates that it is most convenient to view the action of   in the coordinate system where\n      serves as the  -axis,\n      serves as the  -axis,\n    and   as the  -axis.\n    In this case,\n    we can visualize that when we apply the transformation   to a vector\n      in   the result is an output vector that is unchanged in the  -  plane and scaled by a factor of   in the   direction.\n    For example,\n    consider the box whose sides are determined by the vectors  ,\n     ,\n    and   as shown in  .\n    The transformation   stretches this box by a factor of   in the   direction and leaves everything else alone,\n    as illustrated in  .\n    So the entire   is unchanged by  ,\n    but   is scaled by  .\n    In this situation,\n    the eigenvalues and eigenvectors provide the most convenient perspective through which to visualize the action of the transformation  .\n   A box and a transformed box. \n    This geometric perspective illustrates how each eigenvalue and the corresponding eigenspace of   tells us something important about  .\n    So it behooves us to learn a little more about eigenspaces.\n   Dimensions of Eigenspaces \n    There is a connection between the dimension of the eigenspace of a matrix corresponding to an eigenvalue and the multiplicity of that eigenvalue as a root of the characteristic polynomial.\n    Recall that the dimension of a subspace of   is the number of vectors in a basis for the eigenspace.\n    We investigate the connection between dimension and multiplicity in the next activity.\n   \n            Find the dimension of the eigenspace for each eigenvalue of matrix\n              from   (a).\n           \n            Find the dimension of the eigenspace for each eigenvalue of matrix\n              from   (b).\n           \n            Consider now a   matrix with 3 distinct eigenvalues  .\n           \n                  Recall that a polynomial of degree can have at most three distinct roots.\n                  What does that say about the multiplicities of  ?\n                 \n                  Use the fact that eigenvectors corresponding to distinct eigenvalues are linearly independent to find the dimensions of the eigenspaces for  .\n                 \n    The examples in  \n    all provide instances of the principle that the dimension of an eigenspace corresponding to an eigenvalue   cannot exceed the multiplicity of  .\n    Specifically:\n   \n        If   is an eigenvalue of  ,\n        the dimension of the eigenspace corresponding to   is less than or equal to the multiplicity of  .\n       \n    The examples we have seen raise another important point.\n    The matrix   from our geometric example has two eigenvalues   and  ,\n    with the eigenvalue 1 having multiplicity 2.\n    If we let   represent the eigenspace of   corresponding to the eigenvalue  ,\n    then   and  .\n    If we change this matrix slightly to the matrix\n      we see that   has two eigenvalues   and  ,\n    with the eigenvalue 1 having multiplicity 2.\n    However, in this case we have\n      (like the example in from  \n    (a) and   (a)).\n    In this case the vector   forms a basis for   and the vector\n      forms a basis for  .\n    We can visualize the action of   on the square formed by   and   in the  -plane as a scaling by 2 in the   direction as shown in  ,\n    but since we do not have a third linearly independent eigenvector,\n    the action of   in the direction of   is not so clear.\n   A box and a transformed box. \n    So the action of a matrix transformation can be more easily visualized if the dimension of each eigenspace is equal to the multiplicity of the corresponding eigenvalue.\n    This geometric perspective leads us to define the geometric multiplicity of an eigenvalue.\n   multiplicity geometric geometric multiplicity Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Let  .\n       \n              Find the characteristic polynomial of  .\n             \n              The characteristic polynomial of   is\n               .\n             \n              Factor the characteristic polynomial and find the eigenvalues of  .\n             \n              The eigenvalues of   are the solutions to the characteristic equation.\n              Since\n               \n              implies   or  ,\n              the eigenvalues of   are   and  .\n             \n              Find a basis for each eigenspace of  .\n             \n              To find a basis for the eigenspace of   corresponding to the eigenvalue  ,\n              we find a basis for  .\n              The reduced row echelon form of\n                is  .\n              If  ,\n              then   has general solution\n               .\n              Therefore,   is a basis for the eigenspace of   corresponding to the eigenvalue  .\n              To find a basis for the eigenspace of   corresponding to the eigenvalue  ,\n              we find a basis for  .\n              The reduced row echelon form of\n                is  .\n              If  ,\n              then   has general solution\n               .\n              Therefore, a basis for the eigenspace of   corresponding to the eigenvalue   is  .\n             \n              Is it possible to find a basis for   consisting of eigenvectors of  ?\n              Explain.\n             \n              Let  ,\n               ,\n              and  .\n              Since eigenvectors corresponding to different eigenvalues are linearly independent,\n              and since neither   nor   is a scalar multiple of the other,\n              we can conclude that the set\n                is a linearly independent set with   vectors.\n              Therefore,   is a basis for   consisting of eigenvectors of  .\n             \n        Find a   matrix   that has an eigenvector\n          with corresponding eigenvalue  ,\n        an eigenvector   with corresponding eigenvalue  ,\n        and an eigenvector   with corresponding eigenvalue  .\n        Explain your process.\n       \n        We are looking for a\n          matrix   such that  ,\n          and  .\n        Since  ,  ,\n        and   are eigenvectors corresponding to different eigenvalues,\n         ,\n         , and   are linearly independent.\n        So the matrix   is invertible.\n        It follows that\n         .\n       Summary \n    In this section we studied the characteristic polynomial of a matrix and similar matrices.\n     \n         \n          If   is an   matrix,\n          the characteristic polynomial of   is the polynomial\n           ,\n          where   is the   identity matrix.\n         \n       \n         \n          If   is an   matrix,\n          the characteristic equation of   is the equation\n           .\n         \n       \n         \n          The characteristic equation of a square matrix provides us an algebraic method to find the eigenvalues of the matrix.\n         \n       \n         \n          The eigenvalues of an upper or lower-triangular matrix are the entries on the diagonal.\n         \n       \n         \n          There are at most   eigenvalues of an   matrix.\n         \n       \n         \n          For a real matrix  ,\n          if an eigenvalue   of   is complex,\n          then the complex conjugate of   is also an eigenvalue.\n         \n       \n         \n          The algebraic multiplicity of an eigenvalue   is the multiplicity of   as a root of the characteristic equation.\n         \n       \n         \n          The dimension of the eigenspace corresponding to an eigenvalue   is less than or equal to the algebraic multiplicity of  .\n         \n       \n   \n        There is a useful relationship between the determinant and eigenvalues of a matrix   that we explore in this exercise.\n       \n              Let  .\n              Find the determinant of   and the eigenvalues of  ,\n              and compare   to the eigenvalues of  .\n             \n               , eigenvalues   and  ,\n                is the product of the eigenvalues\n             \n              Let   be an   matrix.\n              In this part of the exercise we argue the general case illustrated in the previous part   that   is the product of the eigenvalues of  .\n              Let   be the characteristic polynomial of  .\n             \n                    Let  ,  ,\n                     ,   be the eigenvalues of  \n                    (note that these eigenvalues may not all be distinct).\n                    Recall that if   is a root of a polynomial  ,\n                    then   is a factor of  .\n                    Use this idea to explain why\n                     .\n                   \n                    Each   is a root of the characteristic polynomial.\n                   \n                    Explain why  .\n                   \n                    Evaluate   at  .\n                   \n                    Why is   also equal to  .\n                    Explain how we have shown that   is the product of the eigenvalues of  .\n                   \n                    Use the fact that  .\n                   \n        Find the eigenvalues of the following matrices.\n        For each eigenvalue,\n        determine its algebraic and geometric multiplicity.\n       \n               \n             \n               \n             \n        Let   be an   matrix.\n        Use the characteristic equation to explain why   and   have the same eigenvalues.\n       \n        Explain why  .\n       \n        Find three   matrices whose eigenvalues are 2 and 3, and for which the dimensions of the eigenspaces for\n          and   are different.\n       \n        Suppose   is an   matrix and   is an invertible   matrix.\n        Explain why the characteristic polynomial of   is the same as the characteristic polynomial of\n         , and hence, as a result,\n        the eigenvalues of   and   are the same.\n       \n        Write   as   and then expand  .\n       \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If the determinant of a\n                matrix   is positive,\n              then   has two distinct real eigenvalues.\n             \n              F\n             True\/False \n              If two\n                matrices have the same eigenvalues,\n              then the have the same eigenvectors.\n             True\/False \n              The characteristic polynomial of an\n                matrix has degree  .\n             \n              T\n             True\/False \n              If   is the reduced row echelon form of an   matrix  ,\n              then   and   have the same eigenvalues.\n             True\/False \n              If   is the reduced row echelon form of an   matrix  ,\n              and   is an eigenvector of  ,\n              then   is an eigenvector of  .\n             \n              F\n             True\/False \n              Let   and   be\n                matrices with characteristic polynomials\n                and  , respectively.\n              If  , then  .\n             True\/False \n              Every matrix has at least one eigenvalue.\n             \n              T\n             True\/False \n              Suppose   is a\n                matrix with three distinct eigenvalues.\n              Then any three eigenvectors,\n              one for each eigenvalue, will form a basis of  .\n             True\/False \n              If an eigenvalue   is repeated 3 times among the eigenvalues of a matrix,\n              then there are at most 3 linearly independent eigenvectors corresponding to  .\n             \n              T\n             Project: The Ehrenfest Model \n    To realistically model the diffusion of gas molecules we would need to consider a system with a large number of balls as substitutes for the gas molecules.\n    However, the main idea can be seen in a model with a much smaller number of balls,\n    as we will do now.\n    Suppose we have two bins that contain a total of   balls between them.\n    Label the bins as Bin 1 and Bin 2.\n    In this case we can think of entropy as the number of different possible ways the balls can be arranged in the system.\n    For example,\n    there is only   way for all of the balls to be in Bin 1\n    (low entropy),\n    but there are   ways that we can have one ball in Bin 1\n    (choose any one of the four different balls,\n    which can be distinguished from each other)\n    and   balls in Bin 2\n    (higher entropy).\n    The highest entropy state has the balls equally distributed between the bins\n    (with   different ways to do this).\n   state States probability distributions \n      We begin by analyzing the ways that a state can change.\n      For example,\n    \n     \n         \n          Suppose there are   balls in Bin 1. (In our probability distribution  ,\n          this happens with probability  .) Then there are four balls in Bin 2.\n          The only way for a ball to change bins is if one of the four balls moves from Bin 2 to Bin 1, putting us in State 1.\n          Regardless of which ball moves,\n          we will always be put in State 1, so this happens with a probability of  .\n          In other words,\n          if the probability that Bin 1 contains   balls is  ,\n          then there is a probability of   that Bin 1 will contain 1 ball after the move.\n         \n       \n         \n          Suppose we have 1 ball in Bin 1.\n          There are four ways this can happen\n          (since there are four balls,\n          and the one in Bin 1 is selected at random from the four balls),\n          so the probability of a given ball being in Bin 1 is  .\n         \n      \n         \n             \n              If the ball in Bin 1 moves, that move puts us in State  .\n              In other words,\n              if the probability that Bin 1 contains 1 ball is  ,\n              then there is a probability of\n                that Bin 1 will contain   balls after a move.\n             \n           \n             \n              If any of the   balls in Bin 2 moves\n              (each moves with probability  ),\n              that move puts us in State 2.\n              In other words,\n              if the probability that Bin 1 contains 1 ball is  ,\n              then there is a probability of\n                that Bin 1 will contain   balls after a move.\n             \n           \n       \n     \n          Complete this analysis to explain the probabilities if there are  ,\n           , or   balls in Bin 1.\n         \n          Explain how the results of part (a) show that\n           \n          \n         transition matrix \n    Subsequent moves give probability distribution vectors\n     .\n   \n    This example is an example of a Markov process\n    (see  ).\n    There are several questions we can ask about this model.\n    For example, what is the long-term behavior of this system,\n    and how does this model relate to entropy?\n    That is, given an initial probability distribution vector  ,\n    the system will have probability distribution vectors  ,\n     ,   after subsequent moves.\n    What happens to the vectors   as   goes to infinity,\n    and what does this tell us about entropy?\n    To answer these questions,\n    we will first explore the sequence   numerically,\n    and then use the eigenvalues and eigenvectors of   to analyze the sequence  .\n   \n      Use appropriate technology to do the following.\n     \n            Suppose we begin with a probability distribution vector  .\n            Calculate vectors   for enough values of   so that you can identify the long term behavior of the sequence.\n            Describe this behavior.\n           \n            Repeat part (a) with\n           \n                   \n                 \n                   \n                 \n                   \n                 \n          Describe the long term behavior of the sequence   in each case.\n         \n    In what follows, we investigate the behavior of the sequence\n      that we uncovered in  .\n   \n      We use the characteristic polynomial to find the eigenvalues of  .\n     \n            Find the characteristic polynomial of  .\n            Factor the characteristic polynomial into a product of linear polynomials to show that the eigenvalues of   are  ,\n             ,\n             ,   and  .\n           \n            As we will see a bit later,\n            certain eigenvectors for   will describe the end behavior of the sequence  .\n            Find eigenvectors for   corresponding to the eigenvalues   and  .\n            Explain how the eigenvector for   corresponding to the eigenvalue   explains the behavior of one of the sequences was saw in  .\n            (Any eigenvector of   with eigenvalue   is called an  equilibrium \n            or  steady state  vector.)\n           \n    Now we can analyze the behavior of the sequence  .\n   \n      To make the notation easier,\n      we will let   be an eigenvector of   corresponding to the eigenvalue  ,\n        an eigenvector of   corresponding to the eigenvalue  ,\n        an eigenvector of   corresponding to the eigenvalue  ,\n        an eigenvector of   corresponding to the eigenvalue  ,\n      and   an eigenvector of   corresponding to the eigenvalue  .\n     \n            Explain why   is a basis of  .\n           \n            Let   be any initial probability distribution vector.\n            Explain why we can write   as\n             \n            for some scalars  ,\n             ,  ,  , and  .\n           \n    We can now use the eigenvalues and eigenvectors of   to write the vectors   in a convenient form.\n    Let  ,  ,  ,\n     ,\n    and  .\n    Notice that\n     .\n   \n    Similarly\n     .\n   \n    We can continue in this manner to ultimately show that for each positive integer   we have\n     \n    when  .\n   \n      Recall that we are interested in understanding the behavior of the sequence\n        as   goes to infinity.\n     \n          Equation   shows that we need to know\n            for each   in order to analyze  .\n          Calculate or describe these limits.\n         \n          Use the result of part (a), Equation  ,\n          and  \n          (b) to explain why the sequence\n            is either eventually fixed or oscillates between two states.\n          Compare to the results from  .\n          How are these results related to entropy?\n          You may use the facts that\n        \n         \n             \n                is an eigenvector for   corresponding to the eigenvalue  ,\n             \n           \n             \n                is an eigenvector for   corresponding to the eigenvalue  ,\n             \n           \n             \n                is an eigenvector for   corresponding to the eigenvalue  ,\n             \n           \n             \n                is an eigenvector for   corresponding to the eigenvalue  ,\n             \n           \n             \n                is an eigenvector for   corresponding to the eigenvalue  .\n             \n           \n         "
},
{
  "id": "objectives-18",
  "level": "2",
  "url": "chap_characteristic_equation.html#objectives-18",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is the characteristic polynomial of a matrix?\n           \n         \n           \n            What is the characteristic equation of a matrix?\n           \n         \n           \n            How and why is the characteristic equation of a matrix useful?\n           \n         \n           \n            How many different eigenvalues can an   matrix have?\n           \n         \n           \n            How large can the dimension of the eigenspace corresponding to an eigenvalue be?\n           \n         "
},
{
  "id": "p-3043",
  "level": "2",
  "url": "chap_characteristic_equation.html#p-3043",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "characteristic equation algebraic "
},
{
  "id": "pa_4_b",
  "level": "2",
  "url": "chap_characteristic_equation.html#pa_4_b",
  "type": "Preview Activity",
  "number": "18.1",
  "title": "",
  "body": "\n            For each of the following parts,\n            use the characteristic equation to determine the eigenvalues of  .\n            Then, for each eigenvalue  ,\n            find a basis of the corresponding eigenspace,\n            i.e.,  .\n            You might want to recall how to find a basis for the null space of a matrix from  .\n            Also, make sure that your eigenvalue candidate   yields nonzero eigenvectors in\n              for otherwise   will not be an eigenvalue.\n           \n                     \n                 \n                     \n                 \n                    \n                 \n            Use your eigenvalue and eigenvector calculations of the above problem as a guidance to answer the following questions about a matrix with real entries.\n           \n                  At most how many eigenvalues can a   matrix have?\n                  Is it possible to have no eigenvalues?\n                  Is it possible to have only one eigenvalue?\n                  Explain.\n                 \n                  If a matrix is an upper-triangular matrix (i.e., all entries below the diagonal are 0's, as in the first two matrices of the previous problem),\n                  what can you say about its eigenvalues?\n                  Explain.\n                 \n                  How many linearly independent eigenvectors can be found for a   matrix?\n                  Is it possible to have a matrix without 2 linearly independent eigenvectors?\n                  Explain.\n                 \n            Using the characteristic equation,\n            determine which matrices have 0 as an eigenvalue.\n           "
},
{
  "id": "definition-39",
  "level": "2",
  "url": "chap_characteristic_equation.html#definition-39",
  "type": "Definition",
  "number": "18.1",
  "title": "",
  "body": "characteristic polynomial characteristic equation characteristic polynomial characteristic equation "
},
{
  "id": "act_4_b_1",
  "level": "2",
  "url": "chap_characteristic_equation.html#act_4_b_1",
  "type": "Activity",
  "number": "18.2",
  "title": "",
  "body": "\n            Find the characteristic polynomial of the matrix  ,\n            and use the characteristic polynomial to find all of the eigenvalues of  .\n           \n            Verify that 1 and 2 are the only eigenvalues of the matrix  .\n           "
},
{
  "id": "p-3061",
  "level": "2",
  "url": "chap_characteristic_equation.html#p-3061",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "(algebraic) multiplicity "
},
{
  "id": "definition-40",
  "level": "2",
  "url": "chap_characteristic_equation.html#definition-40",
  "type": "Definition",
  "number": "18.2",
  "title": "",
  "body": "multiplicity algebraic (algebraic) multiplicity "
},
{
  "id": "theorem-40",
  "level": "2",
  "url": "chap_characteristic_equation.html#theorem-40",
  "type": "Theorem",
  "number": "18.3",
  "title": "",
  "body": "\n        Let   be an   matrix with real entries.\n        Then\n         \n             \n              There are at most   eigenvalues of  .\n              If each eigenvalue\n              (including complex eigenvalues)\n              is counted with its multiplicity,\n              there are exactly   eigenvalues.\n             \n           \n             \n              If   has a complex eigenvalue  ,\n              the complex conjugate of   is also an eigenvalue of  .\n             \n           \n             \n              If   is odd,   has at least one real eigenvalue.\n             \n           \n             \n              If   is upper or lower-triangular,\n              the eigenvalues are the entries on the diagonal.\n             \n           \n       "
},
{
  "id": "F_4_b_1",
  "level": "2",
  "url": "chap_characteristic_equation.html#F_4_b_1",
  "type": "Figure",
  "number": "18.4",
  "title": "",
  "body": "A box and a transformed box. "
},
{
  "id": "act_4_b_3",
  "level": "2",
  "url": "chap_characteristic_equation.html#act_4_b_3",
  "type": "Activity",
  "number": "18.3",
  "title": "",
  "body": "\n            Find the dimension of the eigenspace for each eigenvalue of matrix\n              from   (a).\n           \n            Find the dimension of the eigenspace for each eigenvalue of matrix\n              from   (b).\n           \n            Consider now a   matrix with 3 distinct eigenvalues  .\n           \n                  Recall that a polynomial of degree can have at most three distinct roots.\n                  What does that say about the multiplicities of  ?\n                 \n                  Use the fact that eigenvectors corresponding to distinct eigenvalues are linearly independent to find the dimensions of the eigenspaces for  .\n                 "
},
{
  "id": "theorem-41",
  "level": "2",
  "url": "chap_characteristic_equation.html#theorem-41",
  "type": "Theorem",
  "number": "18.5",
  "title": "",
  "body": "\n        If   is an eigenvalue of  ,\n        the dimension of the eigenspace corresponding to   is less than or equal to the multiplicity of  .\n       "
},
{
  "id": "F_4_b_2",
  "level": "2",
  "url": "chap_characteristic_equation.html#F_4_b_2",
  "type": "Figure",
  "number": "18.6",
  "title": "",
  "body": "A box and a transformed box. "
},
{
  "id": "definition-41",
  "level": "2",
  "url": "chap_characteristic_equation.html#definition-41",
  "type": "Definition",
  "number": "18.7",
  "title": "",
  "body": "multiplicity geometric geometric multiplicity "
},
{
  "id": "example-36",
  "level": "2",
  "url": "chap_characteristic_equation.html#example-36",
  "type": "Example",
  "number": "18.8",
  "title": "",
  "body": "\n        Let  .\n       \n              Find the characteristic polynomial of  .\n             \n              The characteristic polynomial of   is\n               .\n             \n              Factor the characteristic polynomial and find the eigenvalues of  .\n             \n              The eigenvalues of   are the solutions to the characteristic equation.\n              Since\n               \n              implies   or  ,\n              the eigenvalues of   are   and  .\n             \n              Find a basis for each eigenspace of  .\n             \n              To find a basis for the eigenspace of   corresponding to the eigenvalue  ,\n              we find a basis for  .\n              The reduced row echelon form of\n                is  .\n              If  ,\n              then   has general solution\n               .\n              Therefore,   is a basis for the eigenspace of   corresponding to the eigenvalue  .\n              To find a basis for the eigenspace of   corresponding to the eigenvalue  ,\n              we find a basis for  .\n              The reduced row echelon form of\n                is  .\n              If  ,\n              then   has general solution\n               .\n              Therefore, a basis for the eigenspace of   corresponding to the eigenvalue   is  .\n             \n              Is it possible to find a basis for   consisting of eigenvectors of  ?\n              Explain.\n             \n              Let  ,\n               ,\n              and  .\n              Since eigenvectors corresponding to different eigenvalues are linearly independent,\n              and since neither   nor   is a scalar multiple of the other,\n              we can conclude that the set\n                is a linearly independent set with   vectors.\n              Therefore,   is a basis for   consisting of eigenvectors of  .\n             "
},
{
  "id": "example-37",
  "level": "2",
  "url": "chap_characteristic_equation.html#example-37",
  "type": "Example",
  "number": "18.9",
  "title": "",
  "body": "\n        Find a   matrix   that has an eigenvector\n          with corresponding eigenvalue  ,\n        an eigenvector   with corresponding eigenvalue  ,\n        and an eigenvector   with corresponding eigenvalue  .\n        Explain your process.\n       \n        We are looking for a\n          matrix   such that  ,\n          and  .\n        Since  ,  ,\n        and   are eigenvectors corresponding to different eigenvalues,\n         ,\n         , and   are linearly independent.\n        So the matrix   is invertible.\n        It follows that\n         .\n       "
},
{
  "id": "ex_determinant_eigenvalues",
  "level": "2",
  "url": "chap_characteristic_equation.html#ex_determinant_eigenvalues",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        There is a useful relationship between the determinant and eigenvalues of a matrix   that we explore in this exercise.\n       \n              Let  .\n              Find the determinant of   and the eigenvalues of  ,\n              and compare   to the eigenvalues of  .\n             \n               , eigenvalues   and  ,\n                is the product of the eigenvalues\n             \n              Let   be an   matrix.\n              In this part of the exercise we argue the general case illustrated in the previous part   that   is the product of the eigenvalues of  .\n              Let   be the characteristic polynomial of  .\n             \n                    Let  ,  ,\n                     ,   be the eigenvalues of  \n                    (note that these eigenvalues may not all be distinct).\n                    Recall that if   is a root of a polynomial  ,\n                    then   is a factor of  .\n                    Use this idea to explain why\n                     .\n                   \n                    Each   is a root of the characteristic polynomial.\n                   \n                    Explain why  .\n                   \n                    Evaluate   at  .\n                   \n                    Why is   also equal to  .\n                    Explain how we have shown that   is the product of the eigenvalues of  .\n                   \n                    Use the fact that  .\n                   "
},
{
  "id": "exercise-170",
  "level": "2",
  "url": "chap_characteristic_equation.html#exercise-170",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Find the eigenvalues of the following matrices.\n        For each eigenvalue,\n        determine its algebraic and geometric multiplicity.\n       \n               \n             \n               \n             "
},
{
  "id": "exercise-171",
  "level": "2",
  "url": "chap_characteristic_equation.html#exercise-171",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Let   be an   matrix.\n        Use the characteristic equation to explain why   and   have the same eigenvalues.\n       \n        Explain why  .\n       "
},
{
  "id": "exercise-172",
  "level": "2",
  "url": "chap_characteristic_equation.html#exercise-172",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Find three   matrices whose eigenvalues are 2 and 3, and for which the dimensions of the eigenspaces for\n          and   are different.\n       "
},
{
  "id": "exercise-173",
  "level": "2",
  "url": "chap_characteristic_equation.html#exercise-173",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Suppose   is an   matrix and   is an invertible   matrix.\n        Explain why the characteristic polynomial of   is the same as the characteristic polynomial of\n         , and hence, as a result,\n        the eigenvalues of   and   are the same.\n       \n        Write   as   and then expand  .\n       "
},
{
  "id": "exercise-174",
  "level": "2",
  "url": "chap_characteristic_equation.html#exercise-174",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If the determinant of a\n                matrix   is positive,\n              then   has two distinct real eigenvalues.\n             \n              F\n             True\/False \n              If two\n                matrices have the same eigenvalues,\n              then the have the same eigenvectors.\n             True\/False \n              The characteristic polynomial of an\n                matrix has degree  .\n             \n              T\n             True\/False \n              If   is the reduced row echelon form of an   matrix  ,\n              then   and   have the same eigenvalues.\n             True\/False \n              If   is the reduced row echelon form of an   matrix  ,\n              and   is an eigenvector of  ,\n              then   is an eigenvector of  .\n             \n              F\n             True\/False \n              Let   and   be\n                matrices with characteristic polynomials\n                and  , respectively.\n              If  , then  .\n             True\/False \n              Every matrix has at least one eigenvalue.\n             \n              T\n             True\/False \n              Suppose   is a\n                matrix with three distinct eigenvalues.\n              Then any three eigenvectors,\n              one for each eigenvalue, will form a basis of  .\n             True\/False \n              If an eigenvalue   is repeated 3 times among the eigenvalues of a matrix,\n              then there are at most 3 linearly independent eigenvectors corresponding to  .\n             \n              T\n             "
},
{
  "id": "p-3141",
  "level": "2",
  "url": "chap_characteristic_equation.html#p-3141",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "state "
},
{
  "id": "F_Ehrenfest",
  "level": "2",
  "url": "chap_characteristic_equation.html#F_Ehrenfest",
  "type": "Figure",
  "number": "18.10",
  "title": "",
  "body": "States "
},
{
  "id": "act_Eherenfest_model",
  "level": "2",
  "url": "chap_characteristic_equation.html#act_Eherenfest_model",
  "type": "Project Activity",
  "number": "18.4",
  "title": "",
  "body": "probability distributions \n      We begin by analyzing the ways that a state can change.\n      For example,\n    \n     \n         \n          Suppose there are   balls in Bin 1. (In our probability distribution  ,\n          this happens with probability  .) Then there are four balls in Bin 2.\n          The only way for a ball to change bins is if one of the four balls moves from Bin 2 to Bin 1, putting us in State 1.\n          Regardless of which ball moves,\n          we will always be put in State 1, so this happens with a probability of  .\n          In other words,\n          if the probability that Bin 1 contains   balls is  ,\n          then there is a probability of   that Bin 1 will contain 1 ball after the move.\n         \n       \n         \n          Suppose we have 1 ball in Bin 1.\n          There are four ways this can happen\n          (since there are four balls,\n          and the one in Bin 1 is selected at random from the four balls),\n          so the probability of a given ball being in Bin 1 is  .\n         \n      \n         \n             \n              If the ball in Bin 1 moves, that move puts us in State  .\n              In other words,\n              if the probability that Bin 1 contains 1 ball is  ,\n              then there is a probability of\n                that Bin 1 will contain   balls after a move.\n             \n           \n             \n              If any of the   balls in Bin 2 moves\n              (each moves with probability  ),\n              that move puts us in State 2.\n              In other words,\n              if the probability that Bin 1 contains 1 ball is  ,\n              then there is a probability of\n                that Bin 1 will contain   balls after a move.\n             \n           \n       \n     \n          Complete this analysis to explain the probabilities if there are  ,\n           , or   balls in Bin 1.\n         \n          Explain how the results of part (a) show that\n           \n          \n         "
},
{
  "id": "p-3150",
  "level": "2",
  "url": "chap_characteristic_equation.html#p-3150",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "transition matrix "
},
{
  "id": "act_Ehrenfest_numeric",
  "level": "2",
  "url": "chap_characteristic_equation.html#act_Ehrenfest_numeric",
  "type": "Project Activity",
  "number": "18.5",
  "title": "",
  "body": "\n      Use appropriate technology to do the following.\n     \n            Suppose we begin with a probability distribution vector  .\n            Calculate vectors   for enough values of   so that you can identify the long term behavior of the sequence.\n            Describe this behavior.\n           \n            Repeat part (a) with\n           \n                   \n                 \n                   \n                 \n                   \n                 \n          Describe the long term behavior of the sequence   in each case.\n         "
},
{
  "id": "act_Ehrenfest_eigenvalues",
  "level": "2",
  "url": "chap_characteristic_equation.html#act_Ehrenfest_eigenvalues",
  "type": "Project Activity",
  "number": "18.6",
  "title": "",
  "body": "\n      We use the characteristic polynomial to find the eigenvalues of  .\n     \n            Find the characteristic polynomial of  .\n            Factor the characteristic polynomial into a product of linear polynomials to show that the eigenvalues of   are  ,\n             ,\n             ,   and  .\n           \n            As we will see a bit later,\n            certain eigenvectors for   will describe the end behavior of the sequence  .\n            Find eigenvectors for   corresponding to the eigenvalues   and  .\n            Explain how the eigenvector for   corresponding to the eigenvalue   explains the behavior of one of the sequences was saw in  .\n            (Any eigenvector of   with eigenvalue   is called an  equilibrium \n            or  steady state  vector.)\n           "
},
{
  "id": "act_Ehrenfest_basis",
  "level": "2",
  "url": "chap_characteristic_equation.html#act_Ehrenfest_basis",
  "type": "Project Activity",
  "number": "18.7",
  "title": "",
  "body": "\n      To make the notation easier,\n      we will let   be an eigenvector of   corresponding to the eigenvalue  ,\n        an eigenvector of   corresponding to the eigenvalue  ,\n        an eigenvector of   corresponding to the eigenvalue  ,\n        an eigenvector of   corresponding to the eigenvalue  ,\n      and   an eigenvector of   corresponding to the eigenvalue  .\n     \n            Explain why   is a basis of  .\n           \n            Let   be any initial probability distribution vector.\n            Explain why we can write   as\n             \n            for some scalars  ,\n             ,  ,  , and  .\n           "
},
{
  "id": "act_Ehrenfest_entropy",
  "level": "2",
  "url": "chap_characteristic_equation.html#act_Ehrenfest_entropy",
  "type": "Project Activity",
  "number": "18.8",
  "title": "",
  "body": "\n      Recall that we are interested in understanding the behavior of the sequence\n        as   goes to infinity.\n     \n          Equation   shows that we need to know\n            for each   in order to analyze  .\n          Calculate or describe these limits.\n         \n          Use the result of part (a), Equation  ,\n          and  \n          (b) to explain why the sequence\n            is either eventually fixed or oscillates between two states.\n          Compare to the results from  .\n          How are these results related to entropy?\n          You may use the facts that\n        \n         \n             \n                is an eigenvector for   corresponding to the eigenvalue  ,\n             \n           \n             \n                is an eigenvector for   corresponding to the eigenvalue  ,\n             \n           \n             \n                is an eigenvector for   corresponding to the eigenvalue  ,\n             \n           \n             \n                is an eigenvector for   corresponding to the eigenvalue  ,\n             \n           \n             \n                is an eigenvector for   corresponding to the eigenvalue  .\n             \n           \n         "
},
{
  "id": "chap_diagonalization",
  "level": "1",
  "url": "chap_diagonalization.html",
  "type": "Section",
  "number": "19",
  "title": "Diagonalization",
  "body": "Diagonalization \n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is a diagonal matrix?\n           \n         \n           \n            What does it mean to diagonalize a matrix?\n           \n         \n           \n            What does it mean for two matrices to be similar?\n           \n         \n           \n            What important properties do similar matrices share?\n           \n         \n           \n            Under what conditions is a matrix diagonalizable?\n           \n         \n           \n            When a matrix   is diagonalizable,\n            what is the structure of a matrix   that diagonalizes  ?\n           \n         \n           \n            Why is diagonalization useful?\n           \n         Application: The Fibonacci Numbers Fibonacci sequence \n    In 1202 Leonardo of Pisa\n    (better known as Fibonacci)\n    published  Liber Abaci \n    (roughly translated as  The Book of Calculation ),\n    in which he constructed a mathematical model of the growth of a rabbit population.\n    The problem Fibonacci considered is that of determining the number of pairs of rabbits produced in a given time period beginning with an initial pair of rabbits.\n    Fibonacci made the assumptions that each pair of rabbits more than one month old produces a new pair of rabbits each month,\n    and that no rabbits die.\n    (We ignore any issues about that might arise concerning the gender of the offspring.)\n    If we let   represent the number of rabbits in month  , Fibonacci produced the model\n     ,\n    for   where   and  .\n    The resulting sequence\n     \n    is a very famous sequence in mathematics and is called the Fibonacci sequence.\n    This sequence is thought to model many natural phenomena such as number of seeds in a sunflower and anything which grows in a spiral form.\n    It is so famous in fact that it has a journal devoted entirely to it.\n    As a note, while Fibonacci's work  Liber Abaci \n    introduced this sequence to the western world,\n    it had been described earlier Sanskrit texts going back as early as the sixth century.\n   \n    By definition, the Fibonacci numbers are calculated by recursion.\n    This is a very ineffective way to determine entries   for large  .\n    Later in this section we will derive a fascinating and unexpected formula for the Fibonacci numbers using the idea of diagonalization.\n   Introduction \n    As we have seen when studying Markov processes,\n    each state is dependent on the previous state.\n    If   is the initial state and   is the transition matrix,\n    then the  th state is found by  .\n    In these situations, and others,\n    it is valuable to be able to quickly and easily calculate powers of a matrix.\n    We explore a way to do that in this section.\n   \n      Consider a very simplified weather forecast.\n      Let us assume there are two possible states for the weather:\n      rainy ( ) or sunny( ).\n      Let us also assume that the weather patterns are stable enough that we can reasonably predict the weather tomorrow based on the weather today.\n      If is is sunny today,\n      then there is a 70% chance that it will be sunny tomorrow,\n      and if it is rainy today then there is a 40% chance that it will be rainy tomorrow.\n      If   is a state vector that indicates a probability   that it is sunny and probability   that it is rainy on day  , then\n       \n      tells us the likelihood of it being sunny or rainy on day 1.\n      Let  .\n       \n            Suppose it is sunny today,\n            that is  .\n            Calculate   and explain how this matrix-vector product tells us the probability that it will be sunny tomorrow.\n           \n            Calculate   and interpret the meaning of each component of the product.\n           \n            Explain why  .\n            Then explain in general why  .\n           \n            The previous result demonstrates that to determine the long-term probability of a sunny or rainy day,\n            we want to be able to easily calculate powers of the matrix  .\n            Use a computer algebra system (e.g., Maple, Mathematica, Wolfram Alpha) to calculate the entries of  ,\n             , and  .\n            Based on this data,\n            what do you expect the long term probability of any day being a sunny one?\n           Diagonalization \n    In  \n    we saw how if we can powers of a matrix we can make predictions about the long-term behavior of some systems.\n    In general, calculating powers of a matrix can be a very difficult thing,\n    but there are times when the process is straightforward.\n   \n      Let  .\n     \n            Show that  .\n           \n            Show that  .\n             \n                 .\n               \n           \n            Explain in general why   for any positive integer  .\n           \n     \n    illustrates that calculating powers of square matrices whose only nonzero entries are along the diagonal is rather simple.\n    In general, if\n     ,\n    then\n     \n    for any positive integer  .\n    Recall that a diagonal matrix is a matrix whose only nonzero elements are along the diagonal\n    (see  ).\n    In this section we will see that matrices that are similar to diagonal matrices have some very nice properties,\n    and that diagonal matrices are useful in calculations of powers of matrices.\n   \n    We can utilize the method of calculating powers of diagonal matrices to also easily calculate powers of other types of matrices.\n   \n      Let   be any matrix,\n        an invertible matrix,\n      and let  .\n     \n            Show that  .\n           \n            Show that  .\n           \n            Explain in general why   for positive integers  .\n           \n    As   illustrates,\n    to calculate the powers of a matrix of the form\n      we only need determine the powers of the matrix  .\n    If   is a diagonal matrix,\n    this is especially straightforward.\n   Similar Matrices \n    Similar matrices play an important role in certain calculations.\n    For example,  \n    showed that if we can write a square matrix   in the form\n      for some invertible matrix   and diagonal matrix  ,\n    then finding the powers of   is straightforward.\n    As we will see, the relation\n      will imply that the matrices   and   share many properties.\n   similar matrices similar \n      Let   and  .\n      Assume that   is similar to   via the matrix  .\n     \n            Calculate   and  .\n            What do you notice?\n           \n            Find the characteristic polynomials of   and  .\n            What do you notice?\n           \n            What can you say about the eigenvalues of   and  ?\n            Explain.\n           \n            Explain why   is an eigenvector for   with eigenvalue 2.\n            Is   an eigenvector for   with eigenvalue 2?\n            Why or why not?\n           \n     \n    suggests that similar matrices share some, but not all,\n    properties.\n    Note that if  ,\n    then   with  .\n    So if   is similar to  ,\n    then   is similar to  .\n    Similarly (no pun intended), since  \n    (where   is the identity matrix),\n    then any square matrix is similar to itself.\n    Also, if   and  ,\n    then  .\n    So if   is similar to   and   is similar to  ,\n    then   is similar to  .\n    If you have studied relations,\n    these three properties show that similarity is an equivalence relation on the set of all   matrices.\n    This is one reason why similar matrices share many important traits,\n    as the next activity highlights.\n   \n      Let   and   be similar matrices with  .\n     \n            Use the multiplicative property of the determinant to explain why  .\n            So similar matrices have the same determinants.\n           \n            Use the fact that   to show that\n              is similar to  .\n           \n            Explain why it follows from (a) and (b) that\n             .\n            So similar matrices have the same characteristic polynomial,\n            and the same eigenvalues.\n           \n    We summarize some properties of similar matrices in the following theorem.\n   \n        Let   and   be similar\n          matrices and   the   identity matrix.\n        Then\n         \n             \n               ,\n             \n           \n             \n                is similar to  ,\n             \n           \n             \n                and   have the same characteristic polynomial,\n             \n           \n             \n                and   have the same eigenvalues.\n             \n           \n       Similarity and Matrix Transformations \n    When a matrix is similar to a diagonal matrix,\n    we can gain insight into the action of the corresponding matrix transformation.\n    As an example,\n    consider the matrix transformation   from   to   defined by  , where\n     .\n   \n    We are interested in understanding what this matrix transformation does to vectors in  .\n    First we note that   has eigenvalues   and\n      with corresponding eigenvectors\n      and  .\n    If we let  , then you can check that\n     \n    and\n     ,\n    where\n     .\n   \n    Thus,\n     .\n   \n    A simple calculation shows that\n     .\n   \n    Let us apply   to the unit square whose sides are formed by the vectors   and\n      as shown in the first picture in  .\n   \n    To apply   we first multiply   and   by  .\n    This gives us\n     .\n   \n    So   transforms the standard coordinate system into a coordinate system in which columns of   determine the axes,\n    as illustrated in the second picture in  .\n    Applying   to the output scales by 2 in the first component and by   in the second component as depicted in the third picture in  .\n    Finally, we apply   to translate back into the standard   coordinate system as shown in the last picture in  .\n    In this case,\n    we can visualize that when we apply the transformation   to a vector in this system it is just scaled in the\n      system by the matrix  .\n    Then the matrix   translates everything back to the standard   coordinate system.\n   The matrix transformation. \n       \n       \n     \n    This geometric perspective provides another example of how having a matrix similar to a diagonal matrix informs us about the situation.\n    In what follows we determine the conditions that determine when a matrix is similar to a diagonal matrix.\n   Diagonalization in General \n    In  \n    and in the matrix transformation example we found that a matrix   was similar to a diagonal matrix whose columns were eigenvectors of  .\n    This will work for a general\n      matrix   as long as we can find an invertible matrix   whose columns are eigenvectors of  .\n    More specifically, suppose   is an\n      matrix with   linearly independent eigenvectors  ,\n     ,  ,\n      with corresponding eigenvalues  ,\n     ,\n     ,  \n    (not necessarily distinct).\n    Let\n     .\n   \n    Then\n     .\n    where\n     .\n   \n    Since the columns of   are linearly independent,\n    we know   is invertible, and so\n     .\n   matrix diagonalizable diagonalizable \n    In other words,\n    a matrix   is diagonalizable if   is similar to a diagonal matrix.\n   IMPORTANT NOTE diagonalize \n      Find an invertible matrix   that diagonalizes  .\n     \n             \n           \n             .\n            \n           \n            The eigenvalues of   are 8 and  .\n           \n    It should be noted that there are square matrices that are not diagonalizable.\n    For example,\n    the matrix   has 1 as its only eigenvalue and the dimension of the eigenspace of   corresponding to the eigenvalue is one.\n    Therefore, it will be impossible to find two linearly independent eigenvectors for  .\n   \n    We showed previously that eigenvectors corresponding to distinct eigenvalue are always linearly independent,\n    so if an   matrix   has   distinct eigenvalues then   is diagonalizable.\n     \n    (b) shows that it is possible to diagonalize an\n      matrix even if the matrix does not have   distinct eigenvalues.\n    In general, we can diagonalize a matrix as long as the dimension of each eigenspace is equal to the multiplicity of the corresponding eigenvalue.\n    In other words,\n    a matrix is diagonalizable if the geometric multiplicity is the same is the algebraic multiplicity for each eigenvalue.\n   \n    At this point we might ask one final question.\n    We argued that if an   matrix   has   linearly independent eigenvectors,\n    then   is diagonalizable.\n    It is reasonable to wonder if the converse is true   that is,\n    if   is diagonalizable,\n    must   have   linearly independent eigenvectors?\n    The answer is yes,\n    and you are asked to show this in  .\n    We summarize the result in the following theorem.\n   The Diagonalization Theorem \n        An   matrix   is diagonalizable if and only if   has   linearly independent eigenvectors.\n        If   is diagonalizable and has linearly independent eigenvectors  ,\n         ,  ,\n          with   for each  ,\n        then   matrix\n          whose columns are linearly independent eigenvectors of   satisfies  ,\n        where   is the diagonal matrix with diagonal entries   for each  .\n       Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Let   and  .\n        You should use appropriate technology to calculate determinants,\n        perform any row reductions, or solve any polynomial equations.\n       \n              Determine if   is diagonalizable.\n              If diagonalizable, find a matrix   that diagonalizes  .\n             \n              Technology shows that the characteristic polynomial of   is\n               .\n              The eigenvalues of   are the solutions to the characteristic equation  .\n              Thus, the eigenvalues of   are   and  .\n              To find a basis for the eigenspace of   corresponding to the eigenvalue  ,\n              we find the general solution to the homogeneous system  .\n              Using technology we see that the reduced row echelon form of\n                is  .\n              So if  ,\n              then the general solution to   is\n               .\n              So a basis for the eigenspace of   corresponding to the eigenvalue   is\n               .\n              To find a basis for the eigenspace of   corresponding to the eigenvalue  ,\n              we find the general solution to the homogeneous system  .\n              Using technology we see that the reduced row echelon form of\n                is  .\n              So if  ,\n              then the general solution to   is\n               .\n              So a basis for the eigenspace of   corresponding to the eigenvalue   is\n               .\n              Eigenvectors corresponding to different eigenvalues are linearly independent,\n              so the set\n               \n              is a basis for  .\n              Since we can find a basis for   consisting of eigenvectors of  ,\n              we conclude that   is diagonalizable.\n              Letting\n               \n              gives us\n               .\n             \n              Determine if   is diagonalizable.\n              If diagonalizable, find a matrix   that diagonalizes  .\n             \n              Technology shows that the characteristic polynomial of   is\n               .\n              The eigenvalues of   are the solutions to the characteristic equation  .\n              Thus, the eigenvalues of   are   and  .\n              To find a basis for the eigenspace of   corresponding to the eigenvalue  ,\n              we find the general solution to the homogeneous system  .\n              Using technology we see that the reduced row echelon form of\n                is  .\n              So if  ,\n              then the general solution to   is\n               .\n              So a basis for the eigenspace of   corresponding to the eigenvalue   is\n               .\n              To find a basis for the eigenspace of   corresponding to the eigenvalue  ,\n              we find the general solution to the homogeneous system  .\n              Using technology we see that the reduced row echelon form of\n                is  .\n              So if  ,\n              then the general solution to   is\n               .\n              So a basis for the eigenspace of   corresponding to the eigenvalue   is\n               .\n              Since each eigenspace is one-dimensional,\n              we cannot find a basis for   consisting of eigenvectors of  .\n              We conclude that   is not diagonalizable.\n             \n              Is it possible for two matrices   and   to have the same eigenvalues with the same algebraic multiplicities,\n              but one matrix is diagonalizable and the other is not?\n              Explain.\n             \n              Yes it is possible for two matrices   and   to have the same eigenvalues with the same multiplicities,\n              but one matrix is diagonalizable and the other is not.\n              An example is given by the matrices   and   in this problem.\n             \n              Is it possible to find diagonalizable matrices   and   such that   is not diagonalizable?\n              If yes, provide an example.\n              If no, explain why.\n             \n              Let   and  .\n              Since   and   are both diagonal matrices,\n              their eigenvalues are their diagonal entries.\n              With   distinct eigenvalues,\n              both   and   are diagonalizable.\n              In this case we have  ,\n              whose only eigenvector is  .\n              The reduced row echelon form of\n                is  .\n              So a basis for the eigenspace of   is  .\n              Since there is no basis for   consisting of eigenvectors of  ,\n              we conclude that   is not diagonalizable.\n             \n              Is it possible to find diagonalizable matrices   and   such that   is not diagonalizable?\n              If yes, provide an example.\n              If no, explain why.\n             \n              Let   and  .\n              Since   and   are both diagonal matrices,\n              their eigenvalues are their diagonal entries.\n              With   distinct eigenvalues,\n              both   and   are diagonalizable.\n              In this case we have  ,\n              whose only eigenvector is  .\n              The reduced row echelon form of\n                is  .\n              So a basis for the eigenspace of   is  .\n              Since there is no basis for   consisting of eigenvectors of  ,\n              we conclude that   is not diagonalizable.\n             \n              Is it possible to find a diagonalizable matrix   such that   is not diagonalizable?\n              If yes, provide an example.\n              If no, explain why.\n             \n              It is not possible to find a diagonalizable matrix   such that   is not diagonalizable.\n              To see why, suppose that matrix   is diagonalizable.\n              That is, there exists a matrix   such that  ,\n              where   is a diagonal matrix.\n              Recall that  .\n              So\n               .\n              Letting  , we conclude that\n               .\n              Therefore,   diagonalizes  .\n             \n              Is it possible to find an invertible diagonalizable matrix   such that   is not diagonalizable?\n              If yes, provide an example.\n              If no, explain why.\n             \n              It is not possible to find an invertible diagonalizable matrix   such that   is not diagonalizable.\n              To see why, suppose that matrix   is diagonalizable.\n              That is, there exists a matrix   such that  ,\n              where   is a diagonal matrix.\n              Thus,  .\n              Since   is invertible,  .\n              It follows that  .\n              So none of the diagonal entries of   can be  .\n              Thus,   is invertible and   is a diagonal matrix.\n              Then\n               \n              and so   diagonalizes  .\n             Summary \n       \n        A matrix   is a diagonal matrix if\n          whenever  .\n       \n     \n       \n        A matrix   is diagonalizable if there is an invertible matrix   so that\n          is a diagonal matrix.\n       \n     \n       \n        Two matrices   and   are similar if there is an invertible matrix   so that\n         .\n       \n     \n       \n        Similar matrices have the same determinants,\n        same characteristic polynomials, and same eigenvalues.\n        Note that similar matrices do not necessarily have the same eigenvectors corresponding to the same eigenvalues.\n       \n     \n       \n        An   matrix   is diagonalizable if and only if   has   linearly independent eigenvectors.\n       \n     \n       \n        When an   matrix   is diagonalizable,\n        then   is invertible and   is diagonal,\n        where  ,  ,  ,\n          are   linearly independent eigenvectors for  .\n       \n     \n       \n        One use for diagonalization is that once we have diagonalized a matrix   we can quickly and easily compute powers of  .\n        Diagonalization can also help us understand the actions of matrix transformations.\n       \n     \n        Determine if each of the following matrices is diagonalizable or not.\n        For diagonalizable matrices,\n        clearly identify a matrix   which diagonalizes the matrix,\n        and what the resulting diagonal matrix is.\n       \n               \n             \n              Not diagonalizable.\n             \n               \n             \n              Diagonalizable by  \n             \n        The   matrix   has two eigenvalues\n          and  .\n        The vectors  ,\n         ,\n        and   are eigenvectors for  ,\n        while the vectors   are eigenvectors for  .\n        Find the matrix  .\n       \n        Find a   non-diagonal matrix   and two different pairs of   and   matrices for which  .\n       \n         ,\n         ,\n        we have  ,\n         ,\n         \n       \n        Find a   non-diagonal matrix   and two different   matrices for which\n          with the same  .\n       \n        Suppose a   matrix   has eigenvalues 2, 3 and 5 and the eigenspace for the eigenvalue 3 has dimension 2.\n        Do we have enough information to determine if   is diagonalizable?\n        Explain.\n       \n        Yes\n       \n        Let   be a diagonalizable   matrix.\n        Show that   has   linearly independent eigenvectors.\n       \n              Let   and  .\n              Find the eigenvalues and eigenvectors of   and  .\n              Conclude that it is possible for two different\n                matrices   and   to have exactly the same eigenvectors and corresponding eigenvalues.\n             \n              Eigenvalues, 1; eigenvectors :  \n             \n              A natural question to ask is if there are any conditions under which\n                matrices that have exactly the same eigenvectors and corresponding eigenvalues must be equal.\n              Determine the answer to this question if   and   are both diagonalizable.\n             \n              Diagonalizable\n             \n              Show that if   and   are\n                diagonal matrices, then  .\n             \n              Show that if   and   are\n                matrices and   is an invertible\n                matrix such that   and\n                with   and   diagonal matrices,\n              then  .\n             \n         \n        in  \n        shows that the determinant of a matrix is the product of its eigenvalues.\n        In this exercise we show that the trace of a diagonalizable matrix is the sum of its eigenvalues. \n        This result is true for any matrix,\n        but the argument is more complicated.\n          First we define the trace of a matrix.\n         trace trace \n              Show that if   and\n                are   matrices,\n              then  .\n             \n              The   entry of   is  .\n              The   entry of   is  .\n              Sum as   and   go from   to  .\n             \n              Let   be a diagonalizable   matrix,\n              and let   be the characteristic polynomial of  .\n              Let   be an invertible matrix such that  ,\n              where   is the diagonal matrix whose diagonal entries are  ,\n               ,  ,\n               , the eigenvalues of  \n              (note that these eigenvalues may not all be distinct).\n             \n                    Explain why  .\n                   \n                     \n                   \n                    Show that the trace of an   diagonalizable matrix is the sum of the eigenvalues of the matrix.\n                   \n                     \n                   \n        In this exercise we generalize the result of  \n        in  \n        to arbitrary diagonalizable matrices.\n       \n              Show that if\n               ,\n              then\n               .\n             \n              Now suppose that an   matrix   is diagonalizable,\n              with   equal to a diagonal matrix  .\n              Show that  .\n             \n        Let   and let  .\n       \n              Use the result of   to calculate  .\n             \n               \n             \n              Calculate  .\n              \n             \n              Explain why   is not diagonalizable.\n             \n               .\n             \n              Use the result of   to calculate  .\n             \n               \n             \n              The real exponential function satisfies some familiar properties.\n              For example,   and\n                for any real numbers   and  .\n              Does the matrix exponential satisfy the corresponding properties.\n              That is, if   and   are   matrices,\n              must   and  ?\n              Explain.\n             \n              No\n             \n        In  \n        we see that we cannot conclude that   for\n          matrices   and  .\n        However, a more limited property is true.\n       >\n             \n              Follow the steps indicated to show that if   is an\n                matrix and   and   are any scalars,\n              then  .\n              (Although we will not use it,\n              you may assume that the series for   converges for any square matrix  .)\n             \n                    Use the definition to show that\n                     .\n                   \n                    Relabel and reorder terms with   to show that\n                     .\n                   \n                    Complete the problem using the Binomial Theorem that says\n                     .\n                   \n              Use the result of part (a) to show that   is an invertible matrix for any   matrix  .\n             \n        There is an interesting connection between the determinant of a matrix exponential and the trace of the matrix.\n        Let   be a diagonalizable\n          matrix with real entries.\n        Let   for some invertible matrix  ,\n        where   is the diagonal matrix with entries  ,\n         ,\n         ,   the eigenvalues of  .\n       \n              Show that  .\n             \n               .\n             \n              Use   to show that\n               .\n             \n               .\n             \n        There is interesting relationship between a matrix and its characteristic equation that we explore in this exercise.\n       \n              We first illustrate with an example.\n              Let  .\n             \n                    Show that   is the characteristic polynomial for  .\n                   \n                    Calculate  .\n                    Then compute  .\n                    What do you get?\n                   \n              The first part of this exercise presents an example of a matrix that satisfies its own characteristic equation.\n              Show that if   is an\n                 diagonalizable \n              matrix with characteristic polynomial  , then  . \n              This result is known as the Cayley-Hamilton Theorem and is one of the fascinating results in linear algebra.\n              This result is true for any square matrix.\n                That is,\n              if  ,\n              then  . (Hint: If\n                for some diagonal matrix  ,\n              show that  .\n              Then determine  .)\n             \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If matrix   is diagonalizable,\n              then so is  .\n             \n              T\n             True\/False \n              If matrix   is diagonalizable,\n              then   is invertible.\n             True\/False \n              If an   matrix   is diagonalizable,\n              then   has   distinct eigenvalues.\n             \n              F\n             True\/False \n              If matrix   is invertible and diagonalizable,\n              then so is  .\n             True\/False \n              If an   matrix   is diagonalizable,\n              then there exists a basis of   consisting of the eigenvectors of  .\n             \n              T\n             True\/False \n              An   matrix with   distinct eigenvalues is diagonalizable.\n             True\/False \n              If   is an\n                diagonalizable matrix,\n              then there is a unique diagonal matrix such that\n                for some invertible matrix  .\n             \n              F\n             True\/False \n              If   is an\n                matrix with eigenvalue  ,\n              then the dimension of the eigenspace of   corresponding to the eigenvalue   is  .\n             True\/False \n              If   is an eigenvalue of an   matrix  ,\n              then   is an eigenvalue of  .\n              (See  \n              in  \n              for information on the matrix exponential.)\n             \n              T\n             Project: Binet's Formula for the Fibonacci Numbers recurrence relation \n      The recurrence relation  gives the equations\n       .\n     \n      Let   for  .\n      Explain how the equations   and   can be described with the matrix equation\n       ,\n      where  .\n     \n    The matrix equation   shows us how to find the vectors   using powers of the matrix  :\n     .\n   \n    So if we can somehow easily find the powers of the matrix  ,\n    then we can find a convenient formula for  .\n    As we have seen,\n    we know how to do this if   is diagonalizable\n   \n      Let  .\n     \n            Show that the eigenvalues of   are\n              and  .\n           \n            Find bases for each eigenspace of  .\n           \n    Now that we have the eigenvalues and know corresponding eigenvectors for  ,\n    we can return to the problem of diagonalizing  .\n   \n          Why do we know that   is diagonalizable?\n         \n          Find a matrix   such that\n            is a diagonal matrix.\n          What is the diagonal matrix?\n         \n    Now we can find a formula for the  th Fibonacci number.\n   \n      Since  ,\n      where   is a diagonal matrix,\n      we also have  .\n      Recall that when  ,\n      it follows that  .\n      Use the equation   to show that\n       .\n     \n        We just need to calculate the second component of  .\n       \n   Binet's formula \n    Formula   is called\n     Binet's formula .\n    It is a very surprising formula in the fact that the expression on the right hand side of   is an integer for each positive integer  .\n    Note that with Binet's formula we can quickly compute   for very large values of  .\n    For example,\n     .\n   golden mean golden ratio \n      You might wonder what happens if we use negative integer exponents in Binet's formula.\n      In other words, are there negatively indexed Fibonacci numbers?\n      For any integer  , including negative integers, let\n       \n     \n      There is a specific relationship between   and  .\n      Find it and verify it.\n     "
},
{
  "id": "objectives-19",
  "level": "2",
  "url": "chap_diagonalization.html#objectives-19",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is a diagonal matrix?\n           \n         \n           \n            What does it mean to diagonalize a matrix?\n           \n         \n           \n            What does it mean for two matrices to be similar?\n           \n         \n           \n            What important properties do similar matrices share?\n           \n         \n           \n            Under what conditions is a matrix diagonalizable?\n           \n         \n           \n            When a matrix   is diagonalizable,\n            what is the structure of a matrix   that diagonalizes  ?\n           \n         \n           \n            Why is diagonalization useful?\n           \n         "
},
{
  "id": "pa_4_c",
  "level": "2",
  "url": "chap_diagonalization.html#pa_4_c",
  "type": "Preview Activity",
  "number": "19.1",
  "title": "",
  "body": "\n      Consider a very simplified weather forecast.\n      Let us assume there are two possible states for the weather:\n      rainy ( ) or sunny( ).\n      Let us also assume that the weather patterns are stable enough that we can reasonably predict the weather tomorrow based on the weather today.\n      If is is sunny today,\n      then there is a 70% chance that it will be sunny tomorrow,\n      and if it is rainy today then there is a 40% chance that it will be rainy tomorrow.\n      If   is a state vector that indicates a probability   that it is sunny and probability   that it is rainy on day  , then\n       \n      tells us the likelihood of it being sunny or rainy on day 1.\n      Let  .\n       \n            Suppose it is sunny today,\n            that is  .\n            Calculate   and explain how this matrix-vector product tells us the probability that it will be sunny tomorrow.\n           \n            Calculate   and interpret the meaning of each component of the product.\n           \n            Explain why  .\n            Then explain in general why  .\n           \n            The previous result demonstrates that to determine the long-term probability of a sunny or rainy day,\n            we want to be able to easily calculate powers of the matrix  .\n            Use a computer algebra system (e.g., Maple, Mathematica, Wolfram Alpha) to calculate the entries of  ,\n             , and  .\n            Based on this data,\n            what do you expect the long term probability of any day being a sunny one?\n           "
},
{
  "id": "act_4_c_0",
  "level": "2",
  "url": "chap_diagonalization.html#act_4_c_0",
  "type": "Activity",
  "number": "19.2",
  "title": "",
  "body": "\n      Let  .\n     \n            Show that  .\n           \n            Show that  .\n             \n                 .\n               \n           \n            Explain in general why   for any positive integer  .\n           "
},
{
  "id": "act_4_c_0_5",
  "level": "2",
  "url": "chap_diagonalization.html#act_4_c_0_5",
  "type": "Activity",
  "number": "19.3",
  "title": "",
  "body": "\n      Let   be any matrix,\n        an invertible matrix,\n      and let  .\n     \n            Show that  .\n           \n            Show that  .\n           \n            Explain in general why   for positive integers  .\n           "
},
{
  "id": "definition-42",
  "level": "2",
  "url": "chap_diagonalization.html#definition-42",
  "type": "Definition",
  "number": "19.1",
  "title": "",
  "body": "similar matrices similar "
},
{
  "id": "act_4_c_1",
  "level": "2",
  "url": "chap_diagonalization.html#act_4_c_1",
  "type": "Activity",
  "number": "19.4",
  "title": "",
  "body": "\n      Let   and  .\n      Assume that   is similar to   via the matrix  .\n     \n            Calculate   and  .\n            What do you notice?\n           \n            Find the characteristic polynomials of   and  .\n            What do you notice?\n           \n            What can you say about the eigenvalues of   and  ?\n            Explain.\n           \n            Explain why   is an eigenvector for   with eigenvalue 2.\n            Is   an eigenvector for   with eigenvalue 2?\n            Why or why not?\n           "
},
{
  "id": "act_4_c_2",
  "level": "2",
  "url": "chap_diagonalization.html#act_4_c_2",
  "type": "Activity",
  "number": "19.5",
  "title": "",
  "body": "\n      Let   and   be similar matrices with  .\n     \n            Use the multiplicative property of the determinant to explain why  .\n            So similar matrices have the same determinants.\n           \n            Use the fact that   to show that\n              is similar to  .\n           \n            Explain why it follows from (a) and (b) that\n             .\n            So similar matrices have the same characteristic polynomial,\n            and the same eigenvalues.\n           "
},
{
  "id": "theorem-42",
  "level": "2",
  "url": "chap_diagonalization.html#theorem-42",
  "type": "Theorem",
  "number": "19.2",
  "title": "",
  "body": "\n        Let   and   be similar\n          matrices and   the   identity matrix.\n        Then\n         \n             \n               ,\n             \n           \n             \n                is similar to  ,\n             \n           \n             \n                and   have the same characteristic polynomial,\n             \n           \n             \n                and   have the same eigenvalues.\n             \n           \n       "
},
{
  "id": "F_4_c_1",
  "level": "2",
  "url": "chap_diagonalization.html#F_4_c_1",
  "type": "Figure",
  "number": "19.3",
  "title": "",
  "body": "The matrix transformation. \n       \n       \n     "
},
{
  "id": "definition-43",
  "level": "2",
  "url": "chap_diagonalization.html#definition-43",
  "type": "Definition",
  "number": "19.4",
  "title": "",
  "body": "matrix diagonalizable diagonalizable "
},
{
  "id": "p-3239",
  "level": "2",
  "url": "chap_diagonalization.html#p-3239",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "diagonalize "
},
{
  "id": "act_4_c_3",
  "level": "2",
  "url": "chap_diagonalization.html#act_4_c_3",
  "type": "Activity",
  "number": "19.6",
  "title": "",
  "body": "\n      Find an invertible matrix   that diagonalizes  .\n     \n             \n           \n             .\n            \n           \n            The eigenvalues of   are 8 and  .\n           "
},
{
  "id": "theorem-43",
  "level": "2",
  "url": "chap_diagonalization.html#theorem-43",
  "type": "Theorem",
  "number": "19.5",
  "title": "The Diagonalization Theorem.",
  "body": "The Diagonalization Theorem \n        An   matrix   is diagonalizable if and only if   has   linearly independent eigenvectors.\n        If   is diagonalizable and has linearly independent eigenvectors  ,\n         ,  ,\n          with   for each  ,\n        then   matrix\n          whose columns are linearly independent eigenvectors of   satisfies  ,\n        where   is the diagonal matrix with diagonal entries   for each  .\n       "
},
{
  "id": "example-38",
  "level": "2",
  "url": "chap_diagonalization.html#example-38",
  "type": "Example",
  "number": "19.6",
  "title": "",
  "body": "\n        Let   and  .\n        You should use appropriate technology to calculate determinants,\n        perform any row reductions, or solve any polynomial equations.\n       \n              Determine if   is diagonalizable.\n              If diagonalizable, find a matrix   that diagonalizes  .\n             \n              Technology shows that the characteristic polynomial of   is\n               .\n              The eigenvalues of   are the solutions to the characteristic equation  .\n              Thus, the eigenvalues of   are   and  .\n              To find a basis for the eigenspace of   corresponding to the eigenvalue  ,\n              we find the general solution to the homogeneous system  .\n              Using technology we see that the reduced row echelon form of\n                is  .\n              So if  ,\n              then the general solution to   is\n               .\n              So a basis for the eigenspace of   corresponding to the eigenvalue   is\n               .\n              To find a basis for the eigenspace of   corresponding to the eigenvalue  ,\n              we find the general solution to the homogeneous system  .\n              Using technology we see that the reduced row echelon form of\n                is  .\n              So if  ,\n              then the general solution to   is\n               .\n              So a basis for the eigenspace of   corresponding to the eigenvalue   is\n               .\n              Eigenvectors corresponding to different eigenvalues are linearly independent,\n              so the set\n               \n              is a basis for  .\n              Since we can find a basis for   consisting of eigenvectors of  ,\n              we conclude that   is diagonalizable.\n              Letting\n               \n              gives us\n               .\n             \n              Determine if   is diagonalizable.\n              If diagonalizable, find a matrix   that diagonalizes  .\n             \n              Technology shows that the characteristic polynomial of   is\n               .\n              The eigenvalues of   are the solutions to the characteristic equation  .\n              Thus, the eigenvalues of   are   and  .\n              To find a basis for the eigenspace of   corresponding to the eigenvalue  ,\n              we find the general solution to the homogeneous system  .\n              Using technology we see that the reduced row echelon form of\n                is  .\n              So if  ,\n              then the general solution to   is\n               .\n              So a basis for the eigenspace of   corresponding to the eigenvalue   is\n               .\n              To find a basis for the eigenspace of   corresponding to the eigenvalue  ,\n              we find the general solution to the homogeneous system  .\n              Using technology we see that the reduced row echelon form of\n                is  .\n              So if  ,\n              then the general solution to   is\n               .\n              So a basis for the eigenspace of   corresponding to the eigenvalue   is\n               .\n              Since each eigenspace is one-dimensional,\n              we cannot find a basis for   consisting of eigenvectors of  .\n              We conclude that   is not diagonalizable.\n             \n              Is it possible for two matrices   and   to have the same eigenvalues with the same algebraic multiplicities,\n              but one matrix is diagonalizable and the other is not?\n              Explain.\n             \n              Yes it is possible for two matrices   and   to have the same eigenvalues with the same multiplicities,\n              but one matrix is diagonalizable and the other is not.\n              An example is given by the matrices   and   in this problem.\n             "
},
{
  "id": "example-39",
  "level": "2",
  "url": "chap_diagonalization.html#example-39",
  "type": "Example",
  "number": "19.7",
  "title": "",
  "body": "\n              Is it possible to find diagonalizable matrices   and   such that   is not diagonalizable?\n              If yes, provide an example.\n              If no, explain why.\n             \n              Let   and  .\n              Since   and   are both diagonal matrices,\n              their eigenvalues are their diagonal entries.\n              With   distinct eigenvalues,\n              both   and   are diagonalizable.\n              In this case we have  ,\n              whose only eigenvector is  .\n              The reduced row echelon form of\n                is  .\n              So a basis for the eigenspace of   is  .\n              Since there is no basis for   consisting of eigenvectors of  ,\n              we conclude that   is not diagonalizable.\n             \n              Is it possible to find diagonalizable matrices   and   such that   is not diagonalizable?\n              If yes, provide an example.\n              If no, explain why.\n             \n              Let   and  .\n              Since   and   are both diagonal matrices,\n              their eigenvalues are their diagonal entries.\n              With   distinct eigenvalues,\n              both   and   are diagonalizable.\n              In this case we have  ,\n              whose only eigenvector is  .\n              The reduced row echelon form of\n                is  .\n              So a basis for the eigenspace of   is  .\n              Since there is no basis for   consisting of eigenvectors of  ,\n              we conclude that   is not diagonalizable.\n             \n              Is it possible to find a diagonalizable matrix   such that   is not diagonalizable?\n              If yes, provide an example.\n              If no, explain why.\n             \n              It is not possible to find a diagonalizable matrix   such that   is not diagonalizable.\n              To see why, suppose that matrix   is diagonalizable.\n              That is, there exists a matrix   such that  ,\n              where   is a diagonal matrix.\n              Recall that  .\n              So\n               .\n              Letting  , we conclude that\n               .\n              Therefore,   diagonalizes  .\n             \n              Is it possible to find an invertible diagonalizable matrix   such that   is not diagonalizable?\n              If yes, provide an example.\n              If no, explain why.\n             \n              It is not possible to find an invertible diagonalizable matrix   such that   is not diagonalizable.\n              To see why, suppose that matrix   is diagonalizable.\n              That is, there exists a matrix   such that  ,\n              where   is a diagonal matrix.\n              Thus,  .\n              Since   is invertible,  .\n              It follows that  .\n              So none of the diagonal entries of   can be  .\n              Thus,   is invertible and   is a diagonal matrix.\n              Then\n               \n              and so   diagonalizes  .\n             "
},
{
  "id": "exercise-175",
  "level": "2",
  "url": "chap_diagonalization.html#exercise-175",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Determine if each of the following matrices is diagonalizable or not.\n        For diagonalizable matrices,\n        clearly identify a matrix   which diagonalizes the matrix,\n        and what the resulting diagonal matrix is.\n       \n               \n             \n              Not diagonalizable.\n             \n               \n             \n              Diagonalizable by  \n             "
},
{
  "id": "exercise-176",
  "level": "2",
  "url": "chap_diagonalization.html#exercise-176",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        The   matrix   has two eigenvalues\n          and  .\n        The vectors  ,\n         ,\n        and   are eigenvectors for  ,\n        while the vectors   are eigenvectors for  .\n        Find the matrix  .\n       "
},
{
  "id": "exercise-177",
  "level": "2",
  "url": "chap_diagonalization.html#exercise-177",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Find a   non-diagonal matrix   and two different pairs of   and   matrices for which  .\n       \n         ,\n         ,\n        we have  ,\n         ,\n         \n       "
},
{
  "id": "exercise-178",
  "level": "2",
  "url": "chap_diagonalization.html#exercise-178",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Find a   non-diagonal matrix   and two different   matrices for which\n          with the same  .\n       "
},
{
  "id": "exercise-179",
  "level": "2",
  "url": "chap_diagonalization.html#exercise-179",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Suppose a   matrix   has eigenvalues 2, 3 and 5 and the eigenspace for the eigenvalue 3 has dimension 2.\n        Do we have enough information to determine if   is diagonalizable?\n        Explain.\n       \n        Yes\n       "
},
{
  "id": "ex_4_c_diagonal_converse",
  "level": "2",
  "url": "chap_diagonalization.html#ex_4_c_diagonal_converse",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Let   be a diagonalizable   matrix.\n        Show that   has   linearly independent eigenvectors.\n       "
},
{
  "id": "exercise-181",
  "level": "2",
  "url": "chap_diagonalization.html#exercise-181",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n              Let   and  .\n              Find the eigenvalues and eigenvectors of   and  .\n              Conclude that it is possible for two different\n                matrices   and   to have exactly the same eigenvectors and corresponding eigenvalues.\n             \n              Eigenvalues, 1; eigenvectors :  \n             \n              A natural question to ask is if there are any conditions under which\n                matrices that have exactly the same eigenvectors and corresponding eigenvalues must be equal.\n              Determine the answer to this question if   and   are both diagonalizable.\n             \n              Diagonalizable\n             "
},
{
  "id": "exercise-182",
  "level": "2",
  "url": "chap_diagonalization.html#exercise-182",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n              Show that if   and   are\n                diagonal matrices, then  .\n             \n              Show that if   and   are\n                matrices and   is an invertible\n                matrix such that   and\n                with   and   diagonal matrices,\n              then  .\n             "
},
{
  "id": "ex_trace_eigenvalues",
  "level": "2",
  "url": "chap_diagonalization.html#ex_trace_eigenvalues",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n         \n        in  \n        shows that the determinant of a matrix is the product of its eigenvalues.\n        In this exercise we show that the trace of a diagonalizable matrix is the sum of its eigenvalues. \n        This result is true for any matrix,\n        but the argument is more complicated.\n          First we define the trace of a matrix.\n         trace trace \n              Show that if   and\n                are   matrices,\n              then  .\n             \n              The   entry of   is  .\n              The   entry of   is  .\n              Sum as   and   go from   to  .\n             \n              Let   be a diagonalizable   matrix,\n              and let   be the characteristic polynomial of  .\n              Let   be an invertible matrix such that  ,\n              where   is the diagonal matrix whose diagonal entries are  ,\n               ,  ,\n               , the eigenvalues of  \n              (note that these eigenvalues may not all be distinct).\n             \n                    Explain why  .\n                   \n                     \n                   \n                    Show that the trace of an   diagonalizable matrix is the sum of the eigenvalues of the matrix.\n                   \n                     \n                   "
},
{
  "id": "ex_4_c_matrix_exponential",
  "level": "2",
  "url": "chap_diagonalization.html#ex_4_c_matrix_exponential",
  "type": "Exercise",
  "number": "10",
  "title": "",
  "body": "\n        In this exercise we generalize the result of  \n        in  \n        to arbitrary diagonalizable matrices.\n       \n              Show that if\n               ,\n              then\n               .\n             \n              Now suppose that an   matrix   is diagonalizable,\n              with   equal to a diagonal matrix  .\n              Show that  .\n             "
},
{
  "id": "ex_4_c_matrix_exponential_examples",
  "level": "2",
  "url": "chap_diagonalization.html#ex_4_c_matrix_exponential_examples",
  "type": "Exercise",
  "number": "11",
  "title": "",
  "body": "\n        Let   and let  .\n       \n              Use the result of   to calculate  .\n             \n               \n             \n              Calculate  .\n              \n             \n              Explain why   is not diagonalizable.\n             \n               .\n             \n              Use the result of   to calculate  .\n             \n               \n             \n              The real exponential function satisfies some familiar properties.\n              For example,   and\n                for any real numbers   and  .\n              Does the matrix exponential satisfy the corresponding properties.\n              That is, if   and   are   matrices,\n              must   and  ?\n              Explain.\n             \n              No\n             "
},
{
  "id": "exercise-186",
  "level": "2",
  "url": "chap_diagonalization.html#exercise-186",
  "type": "Exercise",
  "number": "12",
  "title": "",
  "body": "\n        In  \n        we see that we cannot conclude that   for\n          matrices   and  .\n        However, a more limited property is true.\n       >\n             \n              Follow the steps indicated to show that if   is an\n                matrix and   and   are any scalars,\n              then  .\n              (Although we will not use it,\n              you may assume that the series for   converges for any square matrix  .)\n             \n                    Use the definition to show that\n                     .\n                   \n                    Relabel and reorder terms with   to show that\n                     .\n                   \n                    Complete the problem using the Binomial Theorem that says\n                     .\n                   \n              Use the result of part (a) to show that   is an invertible matrix for any   matrix  .\n             "
},
{
  "id": "exercise-187",
  "level": "2",
  "url": "chap_diagonalization.html#exercise-187",
  "type": "Exercise",
  "number": "13",
  "title": "",
  "body": "\n        There is an interesting connection between the determinant of a matrix exponential and the trace of the matrix.\n        Let   be a diagonalizable\n          matrix with real entries.\n        Let   for some invertible matrix  ,\n        where   is the diagonal matrix with entries  ,\n         ,\n         ,   the eigenvalues of  .\n       \n              Show that  .\n             \n               .\n             \n              Use   to show that\n               .\n             \n               .\n             "
},
{
  "id": "ex_Cayley-Hamilton",
  "level": "2",
  "url": "chap_diagonalization.html#ex_Cayley-Hamilton",
  "type": "Exercise",
  "number": "14",
  "title": "",
  "body": "\n        There is interesting relationship between a matrix and its characteristic equation that we explore in this exercise.\n       \n              We first illustrate with an example.\n              Let  .\n             \n                    Show that   is the characteristic polynomial for  .\n                   \n                    Calculate  .\n                    Then compute  .\n                    What do you get?\n                   \n              The first part of this exercise presents an example of a matrix that satisfies its own characteristic equation.\n              Show that if   is an\n                 diagonalizable \n              matrix with characteristic polynomial  , then  . \n              This result is known as the Cayley-Hamilton Theorem and is one of the fascinating results in linear algebra.\n              This result is true for any square matrix.\n                That is,\n              if  ,\n              then  . (Hint: If\n                for some diagonal matrix  ,\n              show that  .\n              Then determine  .)\n             "
},
{
  "id": "exercise-189",
  "level": "2",
  "url": "chap_diagonalization.html#exercise-189",
  "type": "Exercise",
  "number": "15",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If matrix   is diagonalizable,\n              then so is  .\n             \n              T\n             True\/False \n              If matrix   is diagonalizable,\n              then   is invertible.\n             True\/False \n              If an   matrix   is diagonalizable,\n              then   has   distinct eigenvalues.\n             \n              F\n             True\/False \n              If matrix   is invertible and diagonalizable,\n              then so is  .\n             True\/False \n              If an   matrix   is diagonalizable,\n              then there exists a basis of   consisting of the eigenvectors of  .\n             \n              T\n             True\/False \n              An   matrix with   distinct eigenvalues is diagonalizable.\n             True\/False \n              If   is an\n                diagonalizable matrix,\n              then there is a unique diagonal matrix such that\n                for some invertible matrix  .\n             \n              F\n             True\/False \n              If   is an\n                matrix with eigenvalue  ,\n              then the dimension of the eigenspace of   corresponding to the eigenvalue   is  .\n             True\/False \n              If   is an eigenvalue of an   matrix  ,\n              then   is an eigenvalue of  .\n              (See  \n              in  \n              for information on the matrix exponential.)\n             \n              T\n             "
},
{
  "id": "p-3342",
  "level": "2",
  "url": "chap_diagonalization.html#p-3342",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "recurrence relation "
},
{
  "id": "project-62",
  "level": "2",
  "url": "chap_diagonalization.html#project-62",
  "type": "Project Activity",
  "number": "19.7",
  "title": "",
  "body": "\n      The recurrence relation  gives the equations\n       .\n     \n      Let   for  .\n      Explain how the equations   and   can be described with the matrix equation\n       ,\n      where  .\n     "
},
{
  "id": "project-63",
  "level": "2",
  "url": "chap_diagonalization.html#project-63",
  "type": "Project Activity",
  "number": "19.8",
  "title": "",
  "body": "\n      Let  .\n     \n            Show that the eigenvalues of   are\n              and  .\n           \n            Find bases for each eigenspace of  .\n           "
},
{
  "id": "project-64",
  "level": "2",
  "url": "chap_diagonalization.html#project-64",
  "type": "Project Activity",
  "number": "19.9",
  "title": "",
  "body": "\n          Why do we know that   is diagonalizable?\n         \n          Find a matrix   such that\n            is a diagonal matrix.\n          What is the diagonal matrix?\n         "
},
{
  "id": "project-65",
  "level": "2",
  "url": "chap_diagonalization.html#project-65",
  "type": "Project Activity",
  "number": "19.10",
  "title": "",
  "body": "\n      Since  ,\n      where   is a diagonal matrix,\n      we also have  .\n      Recall that when  ,\n      it follows that  .\n      Use the equation   to show that\n       .\n     \n        We just need to calculate the second component of  .\n       "
},
{
  "id": "p-3357",
  "level": "2",
  "url": "chap_diagonalization.html#p-3357",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "golden mean golden ratio "
},
{
  "id": "project-66",
  "level": "2",
  "url": "chap_diagonalization.html#project-66",
  "type": "Project Activity",
  "number": "19.11",
  "title": "",
  "body": "\n      You might wonder what happens if we use negative integer exponents in Binet's formula.\n      In other words, are there negatively indexed Fibonacci numbers?\n      For any integer  , including negative integers, let\n       \n     \n      There is a specific relationship between   and  .\n      Find it and verify it.\n     "
},
{
  "id": "chap_approx_eigenvalues",
  "level": "1",
  "url": "chap_approx_eigenvalues.html",
  "type": "Section",
  "number": "20",
  "title": "Approximating Eigenvalues and Eigenvectors",
  "body": "Approximating Eigenvalues and Eigenvectors \n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is the power method for?\n           \n         \n           \n            How does the power method work?\n           \n         \n           \n            How can we use the inverse power method to approximate any eigenvalue\/eigenvector pair?\n           \n         Application: Leslie Matrices and Population Modeling \n    The Leslie Matrix\n    (also called the Leslie Model)\n    is a powerful model for describing an age distributed growth of a population that is closed to migration.\n    In a Leslie model, it is usually the case that only one gender\n    (most often female)\n    is considered.\n    As an example,\n    we will later consider a population of sheep that is being grown commercially.\n    A natural question that we will address is how we can harvest the population to build a sustainable environment.\n   \n    When working with populations,\n    the matrices we use are often large.\n    For large matrices,\n    using the characteristic polynomial to calculate eigenvalues is too time and resource consuming to be practical,\n    and we generally cannot find the exact values of the eigenvalues.\n    As a result, approximation techniques are very important.\n    In this section we will explore a method for approximating eigenvalues.\n    The eigenvalues of a Leslie matrix are important because they describe the limiting or steady-state behavior of a population.\n    The matrix and model were introduced by Patrick H. Leslie in\n     On the Use of Matrices in Certain Population Mathematics , Leslie, P.H.,  Biometrika , Volume XXXIII, November 1945, pp. 183-212.\n   Introduction power method \n      Let  .\n      Our goal is to find a scalar   and a nonzero vector   so that  .\n     \n            If we have no prior knowledge of the eigenvalues and eigenvectors of this matrix,\n            we might just begin with a guess.\n            Let   be such a guess for an eigenvector.\n            Calculate  .\n            Is   an eigenvector of  ?\n            Explain.\n           \n            If   is not a good approximation to an eigenvector of  ,\n            then we need to make a better guess.\n            We have little to work with other than just random guessing,\n            but we can use   as another guess.\n            We calculated   in part 1.\n            Is   an eigenvector for  ?\n            Explain.\n           \n            In parts (a) and (b) you might have noticed that in some sense   is closer to being an eigenvector of   than   was.\n            So maybe continuing this process will get us closer to an eigenvector of  .\n            In other words,\n            for each positive integer   we define   as  .\n            Before we proceed, however,\n            we should note that as we calculate the vectors  ,\n             ,  ,\n             , the entries in the vectors get large very quickly.\n            So it will be useful to scale the entries so that they stay at a reasonable size,\n            which makes it easier to interpret the output.\n            One way to do this is to divide each vector   by its largest component in absolute value so that all of the entries stay between   and  . \n            There are several other ways to scale,\n            but we won't consider them here.\n              So in our example we have  ,\n             ,\n            and  .\n            Explain why scaling our vectors will not affect our search for an eigenvector.\n           \n            Use an appropriate technological tool to find the vectors   up to  .\n            What do you think the limiting vector   is?\n            Is this limiting vector an eigenvector of  ?\n            If so, what is the corresponding eigenvalue?\n           The Power Method power method \n    In  ,\n    we saw an example of a matrix\n      so that the sequence  ,\n    where  ,\n    converged to a dominant eigenvector of   for an initial guess vector  .\n    The vectors   for   from   to  \n    (with scaling)\n    are approximately\n\n     .\n   \n    Numerically we can see that the sequence\n      approaches the vector  ,\n    and  \n    illustrates this geometrically as well.\n   The power method. power method dominant eigenvalue dominant eigenvector \n    We have seen that for each positive integer   we can write   as\n     .\n   \n    With this representation of   we can now see why the power method approximates a dominant eigenvector of  .\n   \n      Assume as above that   is an arbitrary\n        matrix with two linearly independent eigenvectors   and   and corresponding eigenvalues\n        and  , respectively.\n      (We are assuming that we don't know these eigenvectors,\n      but we can assume that they exist.)\n      Assume that   is the dominant eigenvalue for  ,\n        is some initial guess to an eigenvector for  ,\n      that  ,\n      and that   for  .\n     \n            We divide both sides of equation   by  \n            (since   is the dominant eigenvalue,\n            we know that   is not  )\n            to obtain\n             .\n            Recall that   is the dominant eigenvalue for  .\n            What happens to   as  ?\n            Explain what happens to the right hand side of equation   as  .\n           \n            Explain why the previous result tells us that the vectors   are approaching a vector in the  direction \n            of   or   as  ,\n            assuming  . (Why do we need  ?\n            What happens if  ?)\n           \n            What does all of this tell us about the sequence\n              as  ?\n           seed \n    Then\n     \n    and\n     .\n   \n    Notice that we are not actually calculating the vectors   here   this is a theoretical argument and we don't know\n      and are not performing any scaling like we did in  .\n    We are assuming that   is the dominant eigenvalue of  , though,\n    so for each   the terms\n      converge to   as   goes to infinity.\n    Thus,\n     \n    for large values of  ,\n    which makes the sequence   converge to a vector in the direction of a dominant eigenvector   provided  .\n    So we need to be careful enough to choose a seed that has a nonzero component in the direction of  .\n    Of course, we generally don't know that our matrix is diagonalizable before we make these calculations,\n    but for many matrices the sequence\n      will approach a dominant eigenvector.\n   \n    The power method approximates a dominant eigenvector,\n    and there are ways that we can approximate the dominant eigenvalue.\n     \n    presents one way   by keeping track of the components of the   that have the largest absolute values,\n    and the next activity shows another.\n   \n      Let   be an   matrix with eigenvalue   and corresponding eigenvector  .\n     \n            Explain why  .\n           \n            Use the result of part (a) to explain why  .\n           Rayleigh quotients \n    To summarize,\n    the procedure for applying the power method for approximating a dominant eigenvector and dominant eigenvalue of a matrix   is as follows.\n     \n         Step 1 \n         \n          Select an arbitrary nonzero vector   as an initial guess to a dominant eigenvector.\n         \n       \n         Step 2 \n         \n          Let  .\n          Let  .\n         \n       \n         Step 3 \n         \n          To avoid having the magnitudes of successive approximations become excessively large,\n          scale this approximation  .\n          That is, find the entry   of   that is largest in absolute value.\n          Then replace   by  .\n         \n       \n         Step 4 \n         \n          Calculate the Rayleigh quotient  .\n         \n       \n         Step 5 \n         \n          Let let  .\n          Increase   by   and repeat Steps 3 through 5.\n         \n       \n   \n    If the sequence   converges to a dominant eigenvector of  ,\n    then the sequence   converges to the dominant eigenvalue of  .\n   sparse The Inverse Power Method \n    The power method only allows us to approximate the dominant eigenvalue and a dominant eigenvector for a matrix  .\n    It is possible to modify this method to approximate other eigenvectors and eigenvalues under certain conditions.\n    We consider an example in the next activity to motivate the general situation.\n   \n      Let   be the matrix from  .\n      Recall that   is an eigenvalue for  ,\n      and a quick calculation can show that   is the other eigenvalue of  .\n      Consider the matrix  .\n     \n            Show that   and\n              are the eigenvalues of  .\n           \n            Recall that   is an eigenvector of   corresponding to the eigenvalue   and assume that\n              is an eigenvector for   corresponding to the eigenvalue  .\n            Calculate the products   and  .\n            How do the products relate to the results of part (a)?\n           \n     \n    provides evidence that we can translate the matrix   having a dominant eigenvalue to a different matrix   with the same eigenvectors as   and with a dominant eigenvalue of our choosing.\n    To see why, let   be an\n      matrix with eigenvalues  ,\n     ,\n     ,  ,\n    and let   be any real number distinct from the eigenvalues.\n    Let  .\n    In our example in   the numbers\n     \n    were the eigenvalues of  ,\n    and that if   is an eigenvector for   corresponding to the eigenvalue  ,\n    then   is an eigenvector of   corresponding to the eigenvalue  .\n    To see why, let   be an eigenvalue of an\n      matrix   with corresponding eigenvector  .\n    Let   be a scalar that is not an eigenvalue of  ,\n    and let  .\n    Now\n     .\n   \n    So   is an eigenvalue of   with eigenvector  .\n   \n    Now suppose that   is an\n      matrix with eigenvalues  ,\n     ,\n     ,  ,\n    and that we want to approximate an eigenvector and corresponding eigenvalue   of  .\n    If we can somehow find a value of   so that\n      for all  ,\n    then   for any  .\n    Thus, the matrix   has\n      as its dominant eigenvalue and we can use the power method to approximate an eigenvector and the Rayleigh quotient to approximate the eigenvalue  ,\n    and hence approximate  .\n   \n      Let  .\n     \n            Apply the power method to the matrix   with initial vector\n              to fill in  \n            (to four decimal places).\n            Use this information to estimate an eigenvalue for   and a corresponding eigenvector.\n             Applying the power method to  \n           \n            Applying the power method to the matrix   with initial vector\n              yields the information in  \n            (to four decimal places).\n            Use this information to estimate an eigenvalue for   and a corresponding eigenvector.\n             Applying the power method to  \n           \n            Applying the power method to the matrix   with initial vector\n              yields the information in  \n            (to four decimal places).\n            Use this information to estimate an eigenvalue for   and a corresponding eigenvector.\n             Applying the power method to  \n           Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Let  .\n       \n              Approximate the dominant eigenvalue of   accurate to two decimal places using the power method.\n              Use technology as appropriate.\n             \n              We use technology to calculate the scaled vectors\n                for values of   until the components don't change in the second decimal place.\n              We start with the seed  .\n              For example, to two decimal places we have\n                for  .\n              So we suspect that   is close to a dominant eigenvector for  .\n              For the dominant eigenvalue,\n              we can calculate the Rayleigh quotients\n                until they do not change to two decimal places.\n              For  , our Rayleigh quotients are all\n              (to two decimal places)\n              equal to  .\n              So we expect that the dominant eigenvalue of   is close to  .\n              Notice that\n               ,\n              which is not far off from  .\n             \n              Find the characteristic polynomial   of  .\n              Then find the the root of   farthest from the origin.\n              Compare to the result of part (a).\n              Use technology as appropriate.\n             \n              The characteristic polynomial of   is\n               .\n              The quadratic formula gives the nonzero roots of   as\n               .\n              The roots farthest from the origin is approximately  ,\n              as was also calculated in part (a).\n             \n        Let  .\n       \n              Use the power method to approximate the dominant eigenvalue and a corresponding eigenvector\n              (using scaling)\n              accurate to two decimal places.\n              Use   as the seed.\n             \n              We use technology to calculate the scaled vectors\n                for values of   until the components don't change in the second decimal place.\n              For example,\n              to two decimal places we have   for  .\n              So we suspect that   is a dominant eigenvector for  .\n              For the dominant eigenvalue,\n              we can calculate the Rayleigh quotients\n                until they do not change to two decimal places.\n              For  , our Rayleigh quotients are all\n              (to two decimal places)\n              equal to  .\n              So we expect that the dominant eigenvalue of   is  .\n              We could also use the fact that\n               \n              to see that   is a dominant eigenvector for   with eigenvalue  .\n             \n              Determine the exact value of the dominant eigenvalue of   and compare to your result from part (a).\n             \n              Technology shows that the characteristic polynomial of   is\n               .\n              We can see from the characteristic polynomial that   is the dominant eigenvalue of  .\n             \n              Approximate the remaining eigenvalues of   using the inverse power method.\n             \n              Try   and  .\n             \n              Applying the power method to   with seed\n                gives   for  ,\n              with Rayleigh quotients of  \n              (to several decimal places).\n              So   is the dominant eigenvalue of  .\n              But   is also the dominant eigenvalue of  ,\n              where   is the corresponding eigenvalue of  . . So to find  ,\n              we note that   implies that\n                is an eigenvalue of  .\n              Now applying the power method to\n                with seed   gives\n                for large enough  ,\n              with Rayleigh quotients of  \n              (to several decimal places).\n              To find the corresponding eigenvalue   for  ,\n              we note that  ,\n              or   is an eigenvalue of  .\n              Admittedly, this method is very limited.\n              Finding good choices for   often depends on having some information about the eigenvalues of  .\n              Choosing   close to an eigenvalue provides the best chance of obtaining that eigenvalue.\n             Summary \n       \n        The power method is an iterative method that can be used to approximate the dominant eigenvalue of an\n          matrix   that has   linearly independent eigenvectors and a dominant eigenvalue.\n       \n     \n       \n        To use the power method we start with a seed   and then calculate the sequence   of vectors,\n        where  .\n        If   is chosen well,\n        then the sequence   converges to a dominant eigenvector of  .\n       \n     \n       \n        If   is an   matrix with eigenvalues  ,\n         ,\n         ,  ,\n        to approximate an eigenvector of   corresponding to the eigenvalue  ,\n        we apply the power method to the matrix  ,\n        where   is not a eigenvalue of   and\n          for any  .\n       \n     \n        Let  .\n        Let  .\n       \n              Find the eigenvalues and corresponding eigenvectors for  .\n             \n              Eigenvalues:\n                and  ; T  is an eigenvector for   with eigenvalue   and\n                is an eigenvector for   with eigenvalue  \n             \n              Use appropriate technology to calculate\n                for   up to 10.\n              Compare to a dominant eigenvector for  .\n             \n              As   increases, the vectors\n                are approaching the vector  ,\n              which is a dominant eigenvector of  .\n             \n              Use the eigenvectors from part (b) to approximate the dominant eigenvalue for  .\n              Compare to the exact value of the dominant eigenvalue of  .\n             \n              The Rayleigh quotients   approach the dominant eigenvalue  .\n             \n              Assume that the other eigenvalue for   is close to  .\n              Apply the inverse power method and compare the results to the remaining eigenvalue and eigenvectors for  .\n             \n              Apply the power method to  .\n              As   increases, the vectors\n                are approaching the vector  ,\n              which is an eigenvector of  .\n              The Rayleigh quotients approach the other eigenvalue   of  .\n             \n        Let  .\n        Use the power method to approximate a dominant eigenvector for  .\n        Use   as the seed.\n        Then approximate the dominant eigenvalue of  .\n       \n        Let  .\n        Use the power method starting with  .\n        Explain why the method fails in this case to approximate a dominant eigenvector,\n        and how you could adjust the seed to make the process work.\n       \n          is an eigenvector for   with eigenvalue  ,\n        so the vectors   are all equal to  .\n        We can adjust the seed to a non-eigenvector.\n       \n        Let  .\n       \n              Find the eigenvalues and an eigenvector for each eigenvalue.\n             \n              Apply the power method with an initial starting vector  .\n              What is the resulting sequence?\n             \n              Use equation   to explain the sequence you found in part (b).\n             \n        Let  .\n        Fill in the entries in  ,\n        where   is the  th approximation to a dominant eigenvector using the power method,\n        starting with the seed  .\n        Compare the results of this table to the eigenvalues of   and  .\n        What do you notice?\n         Values of the Rayleigh quotient \n       \n            is the dominant eigenvalue of  \n         \n        Let  .\n        The power method will approximate the dominant eigenvalue  .\n        In this exercise we explore what happens if we apply the power method to  .\n       \n              Apply the power method to   to approximate the dominant eigenvalue of  .\n              Use   as the seed.\n              How is this eigenvalue related to an eigenvalue of  ?\n             \n              Explain in general why applying the power method to the inverse of an invertible matrix   might give an approximation to an eigenvalue of   of smallest magnitude.\n              When might this not work?\n             \n        There are other algebraic methods that do not rely on the determinant of a matrix that can be used to find eigenvalues of a matrix.\n        We examine one such method in this exercise.\n        Let   be any   matrix,\n        and let   be any vector in  .\n       \n              Explain why the vectors\n               \n              are linearly dependent.\n             \n              Since   has dimension  ,\n              it follows that any set of   vectors is linearly dependent.\n             \n              Let  ,  ,\n               ,   be scalars, not all 0, so that\n               .\n              Explain why there must be a smallest positive integer   so that there are scalars  ,\n               ,\n               ,   with  . such that\n               .\n             \n              Proceed down the list  ,\n               , etc., until you reach a weight that is non-zero.\n             \n              Let\n               .\n              Then\n               \n              and\n               .\n              Suppose the polynomial   has a linear factor,\n              say   for some degree   polynomial  .\n              Explain why, if   is non-zero,\n                is an eigenvalue of   with eigenvector  .\n             \n               \n             \n              This method allows us to find certain eigenvalues and eigenvectors,\n              the roots of the polynomial  .\n              Any other eigenvector must lie outside the eigenspaces we have already found,\n              so repeating the process with a vector   not in any of the known eigenspaces will produce different eigenvalues and eigenvectors.\n              Let  .\n             \n                    Find the polynomial  .\n                    Use  .\n                   \n                     \n                   \n                    Find all of the roots of  .\n                   \n                     ,  , and  \n                   \n                    For each root   of  ,\n                    find the polynomial   and use this polynomial to determine an eigenvector of  .\n                    Verify your work.\n                   \n                    For  , we have  ,\n                    and   an eigenvector for   with eigenvalue  .\n                    For   we have  ,\n                    and   is an eigenvector for   with eigenvalue  .\n                    For   we have  ,\n                    and   is an eigenvector for   with eigenvalue  .\n                   \n        We have seen that the Rayleigh quotients approximate the dominant eigenvalue of a matrix  .\n        As an alternative to using Rayleigh quotients,\n        we can keep track of the scaling factors.\n        Recall that the scaling in the power method can be used to make the magnitudes of the successive approximations smaller and easier to work with.\n        Let   be an   matrix and begin with a non-zero seed  .\n        We now want to keep track of the scaling factors,\n        so let   be the component of   with largest absolute value and let  .\n        For  , let  ,\n        let   be the component of   with largest absolute value and let  .\n       \n              Let  .\n              Use   as the seed and calculate\n                for   from   to  .\n              Compare to the dominant eigenvalue of  .\n             \n              Assume that for large   the vectors   approach a dominant eigenvector with dominant eigenvalue  .\n              Show now in general that the sequence of scaling factors\n                approaches  .\n             \n        Let   be an   matrix and let   be a scalar that is not an eigenvalue of  .\n        Suppose that   is an eigenvector of\n          with eigenvalue  .\n        Find an eigenvalue of   in terms of   and   with corresponding eigenvector  .\n       \n        If   is an eigenvalue of   with eigenvector  ,\n        then   is an eigenvalue of   with eigenvector  .\n       \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              The largest eigenvalue of a matrix is a dominant eigenvalue.\n             \n              F\n             True\/False \n              If an   matrix   has   linearly independent eigenvectors and a dominant eigenvalue,\n              then the sequence   converges to a dominant eigenvector of   for any initial vector  .\n             True\/False \n              If   is an eigenvalue of an\n                matrix   and   is not an eigenvalue of  ,\n              then   is an eigenvalue of  .\n             \n              T\n             True\/False \n              Every square matrix has a dominant eigenvalue.\n             Project: Managing a Sheep Herd \n    Sheep farming is a significant industry in New Zealand.\n    New Zealand is reported to have the highest density of sheep in the world.\n    Sheep can begin to reproduce after one year,\n    and give birth only once per year.\n      gives Birth and Survival Rates for Female New Zealand Sheep (from G. Caughley,\n     Parameters for Seasonally Breeding Populations, \n     Ecology ,  48 , (1967), 834-839).\n    Since sheep hardly ever live past 12 years,\n    we will only consider the population through 12 years.\n   New Zealand female sheep data by age group Age (years) Birth Rate Survival Rate 0-1 0.000 0.845 1-2 0.045 0.975 2-3 0.391 0.965 3-4 0.472 0.950 4-5 0.484 0.926 5-6 0.546 0.895 6-7 0.543 0.850 7-8 0.502 0.786 8-9 0.468 0.691 9-10 0.459 0.561 10-11 0.433 0.370 11-12 0.421 0.000 fecundity fecund Life cycle with four age classes. \n    To model the sheep population, we need a few variables.\n    Let   be the number of sheep in age group 0-1,\n      the number in age group 1-2,   the number in age group 2-3 and,\n    in general,\n      the number of sheep in age group  -  at some initial time\n    (time  ),\n    and let\n     .\n   \n    We wish to determine the populations in the different groups after one year.\n    Let\n     ,\n    where   denotes the number of sheep in age group 0-1,\n      the number of sheep in age group 1-2 and,\n    in general,\n      the number of tilapia in age group  -  after one year.\n   \n        shows that, on average,\n      each female in age group 1-2 produces   female offspring in a year.\n      Since there are   females in age group 1-2, the lamb population increases by   in a year.\n     \n            Continue this analysis to explain why\n             .\n           \n            Explain why  .\n           \n            Now explain why\n             ,\n            where   is the matrix\n             .\n           Leslie matrix dominant eigenvalue strictly dominant eigenvalue \n    We can use these properties to determine the long-term behavior of the sheep herd.\n   \n      Assume that   is defined by  , and let\n       ,\n      where   denotes the number of sheep in age group 0-1,\n        the number of sheep in age group 1-2 and,\n      in general,\n        the number of sheep in age group  -  after   years.\n     \n            Assume that  .\n            Use appropriate technology to calculate  ,\n             ,  , and  .\n            Round to the nearest whole number.\n            What do you notice about the sheep population?\n            You may use the GeoGebra applet at  .\n           \n            We can use the third and fourth properties of Leslie matrices to better understand the long-term behavior of the sheep population.\n            Since successive entries in the first row of the Leslie matrix in   are positive,\n            our Leslie matrix has a strictly dominant eigenvalue  .\n            Given the dimensions of our Leslie matrix,\n            finding this dominant eigenvalue through algebraic means is not feasible.\n            Use the power method to approximate the dominant eigenvalue\n              of the Leslie matrix in   to five decimal places.\n            Explain your process.\n            Then explain how this dominant eigenvalue tells us that,\n            unchecked,\n            the sheep population grows at a rate that is roughly exponential.\n            What is the growth rate of this exponential growth?\n            You may use the GeoGebra applet at  .\n           \n      indicates that,\n    unchecked, the sheep population will grow without bound,\n    roughly exponentially with ratio equal to the dominant eigenvalue of our Leslie matrix  .\n    Of course, a sheep farmer cannot provide the physical environment or the resources to support an unlimited population of sheep.\n    In addition,\n    most sheep farmers cannot support themselves only by shearing sheep for the wool.\n    Consequently,\n    some harvesting of the sheep population each year for meat and skin is necessary.\n    A sustainable harvesting policy allows for the regular harvesting of some sheep while maintaining the population at a stable level.\n    It is necessary for the farmer to find an optimal harvesting rate to attain this stable population and the following activity leads us through an analysis of how such a harvesting rate can be determined.\n   \n      The Leslie model can be modified to consider harvesting.\n      It is possible to harvest different age groups at different rates,\n      and to harvest only some age groups and not others.\n      In the case of sheep,\n      it might make sense to only harvest from the youngest population since lamb is more desirable than mutton and the lamb population grows the fastest.\n      Assume that this is our harvesting strategy and that we harvest our sheep from only the youngest age group at the start of each year.\n      Let   be the fraction of sheep we harvest from the youngest age group each year after considering growth.\n     \n            If we begin with an initial population  ,\n            then the state vector after births and expected deaths is  .\n            Now we harvest.\n            Explain why if we harvest a fraction   from the youngest age group after considering growth,\n            then the state vector after 1 year will be\n             ,\n            where\n             .\n           \n            Our goal is to find a harvesting rate that will lead to a steady state in which the sheep population remains the same each year.\n            In other words, we want to find a value of  ,\n            if one exists, that satisfies\n             .\n            Show that   is equivalent to the matrix equation\n             .\n           \n            Use appropriate technology to experiment numerically with different values of   to find the value you think gives the best uniform harvest rate.\n            Explain your reasoning.\n            You may use the GeoGebra applet at  .\n           \n            Now we will use some algebra to find an equation that explicitly gives us the harvest rate in the general setting.\n            This will take a bit of work,\n            but none of it is too difficult.\n            To simplify our work but yet illustrate the overall idea,\n            let us consider the general\n              case with arbitrary Leslie matrix\n             .\n            Recall that we want to find a value of   that satisfies   with  .\n            Let  .\n           \n                  Calculate the matrix product  .\n                  Explain why this product is again a Leslie matrix and why\n                    will have a dominant eigenvalue of 1.\n                 \n                  Now calculate   and set it equal to  .\n                  Write down the resulting system of 4 equations that must be satisfied.\n                  Be sure that your first equation is\n                   .\n                 \n                  Equation   as written depends on the entries of the vector  ,\n                  but we should be able to arrive at a result that is independent of  .\n                  To see how we do this,\n                  we assume the population of the youngest group is never 0, so we can divide both sides of   by   to obtain\n                   .\n                  Now we need to write the fractions\n                   ,    ,\n                  and   so that they do not involve the  .\n                  Use the remaining equations in your system to show that\n                   .\n                 net reproduction rate of the population \n            Extend   to the 12 age group case of the sheep herd.\n            Calculate the value of   for this sheep herd and then find the value of  .\n            Compare this   to the value you obtained through experimentation earlier.\n            Find the fraction of the lambs that should be harvested each year and explain what the stable population state vector   tells us about the sheep population for this harvesting policy.\n           "
},
{
  "id": "objectives-20",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#objectives-20",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is the power method for?\n           \n         \n           \n            How does the power method work?\n           \n         \n           \n            How can we use the inverse power method to approximate any eigenvalue\/eigenvector pair?\n           \n         "
},
{
  "id": "p-3366",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#p-3366",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "power method "
},
{
  "id": "pa_4_d",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#pa_4_d",
  "type": "Preview Activity",
  "number": "20.1",
  "title": "",
  "body": "\n      Let  .\n      Our goal is to find a scalar   and a nonzero vector   so that  .\n     \n            If we have no prior knowledge of the eigenvalues and eigenvectors of this matrix,\n            we might just begin with a guess.\n            Let   be such a guess for an eigenvector.\n            Calculate  .\n            Is   an eigenvector of  ?\n            Explain.\n           \n            If   is not a good approximation to an eigenvector of  ,\n            then we need to make a better guess.\n            We have little to work with other than just random guessing,\n            but we can use   as another guess.\n            We calculated   in part 1.\n            Is   an eigenvector for  ?\n            Explain.\n           \n            In parts (a) and (b) you might have noticed that in some sense   is closer to being an eigenvector of   than   was.\n            So maybe continuing this process will get us closer to an eigenvector of  .\n            In other words,\n            for each positive integer   we define   as  .\n            Before we proceed, however,\n            we should note that as we calculate the vectors  ,\n             ,  ,\n             , the entries in the vectors get large very quickly.\n            So it will be useful to scale the entries so that they stay at a reasonable size,\n            which makes it easier to interpret the output.\n            One way to do this is to divide each vector   by its largest component in absolute value so that all of the entries stay between   and  . \n            There are several other ways to scale,\n            but we won't consider them here.\n              So in our example we have  ,\n             ,\n            and  .\n            Explain why scaling our vectors will not affect our search for an eigenvector.\n           \n            Use an appropriate technological tool to find the vectors   up to  .\n            What do you think the limiting vector   is?\n            Is this limiting vector an eigenvector of  ?\n            If so, what is the corresponding eigenvalue?\n           "
},
{
  "id": "p-3372",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#p-3372",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "power method "
},
{
  "id": "F_4_e_1",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#F_4_e_1",
  "type": "Figure",
  "number": "20.1",
  "title": "",
  "body": "The power method. "
},
{
  "id": "p-3375",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#p-3375",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "power method "
},
{
  "id": "p-3376",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#p-3376",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "dominant eigenvalue dominant eigenvector "
},
{
  "id": "act_4_e_1",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#act_4_e_1",
  "type": "Activity",
  "number": "20.2",
  "title": "",
  "body": "\n      Assume as above that   is an arbitrary\n        matrix with two linearly independent eigenvectors   and   and corresponding eigenvalues\n        and  , respectively.\n      (We are assuming that we don't know these eigenvectors,\n      but we can assume that they exist.)\n      Assume that   is the dominant eigenvalue for  ,\n        is some initial guess to an eigenvector for  ,\n      that  ,\n      and that   for  .\n     \n            We divide both sides of equation   by  \n            (since   is the dominant eigenvalue,\n            we know that   is not  )\n            to obtain\n             .\n            Recall that   is the dominant eigenvalue for  .\n            What happens to   as  ?\n            Explain what happens to the right hand side of equation   as  .\n           \n            Explain why the previous result tells us that the vectors   are approaching a vector in the  direction \n            of   or   as  ,\n            assuming  . (Why do we need  ?\n            What happens if  ?)\n           \n            What does all of this tell us about the sequence\n              as  ?\n           "
},
{
  "id": "p-3383",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#p-3383",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "seed "
},
{
  "id": "act_4_d_Rayleigh",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#act_4_d_Rayleigh",
  "type": "Activity",
  "number": "20.3",
  "title": "",
  "body": "\n      Let   be an   matrix with eigenvalue   and corresponding eigenvector  .\n     \n            Explain why  .\n           \n            Use the result of part (a) to explain why  .\n           "
},
{
  "id": "p-3390",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#p-3390",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Rayleigh quotients "
},
{
  "id": "p-3398",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#p-3398",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "sparse "
},
{
  "id": "act_4_e_2",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#act_4_e_2",
  "type": "Activity",
  "number": "20.4",
  "title": "",
  "body": "\n      Let   be the matrix from  .\n      Recall that   is an eigenvalue for  ,\n      and a quick calculation can show that   is the other eigenvalue of  .\n      Consider the matrix  .\n     \n            Show that   and\n              are the eigenvalues of  .\n           \n            Recall that   is an eigenvector of   corresponding to the eigenvalue   and assume that\n              is an eigenvector for   corresponding to the eigenvalue  .\n            Calculate the products   and  .\n            How do the products relate to the results of part (a)?\n           "
},
{
  "id": "act_4_e_3",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#act_4_e_3",
  "type": "Activity",
  "number": "20.5",
  "title": "",
  "body": "\n      Let  .\n     \n            Apply the power method to the matrix   with initial vector\n              to fill in  \n            (to four decimal places).\n            Use this information to estimate an eigenvalue for   and a corresponding eigenvector.\n             Applying the power method to  \n           \n            Applying the power method to the matrix   with initial vector\n              yields the information in  \n            (to four decimal places).\n            Use this information to estimate an eigenvalue for   and a corresponding eigenvector.\n             Applying the power method to  \n           \n            Applying the power method to the matrix   with initial vector\n              yields the information in  \n            (to four decimal places).\n            Use this information to estimate an eigenvalue for   and a corresponding eigenvector.\n             Applying the power method to  \n           "
},
{
  "id": "example-40",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#example-40",
  "type": "Example",
  "number": "20.5",
  "title": "",
  "body": "\n        Let  .\n       \n              Approximate the dominant eigenvalue of   accurate to two decimal places using the power method.\n              Use technology as appropriate.\n             \n              We use technology to calculate the scaled vectors\n                for values of   until the components don't change in the second decimal place.\n              We start with the seed  .\n              For example, to two decimal places we have\n                for  .\n              So we suspect that   is close to a dominant eigenvector for  .\n              For the dominant eigenvalue,\n              we can calculate the Rayleigh quotients\n                until they do not change to two decimal places.\n              For  , our Rayleigh quotients are all\n              (to two decimal places)\n              equal to  .\n              So we expect that the dominant eigenvalue of   is close to  .\n              Notice that\n               ,\n              which is not far off from  .\n             \n              Find the characteristic polynomial   of  .\n              Then find the the root of   farthest from the origin.\n              Compare to the result of part (a).\n              Use technology as appropriate.\n             \n              The characteristic polynomial of   is\n               .\n              The quadratic formula gives the nonzero roots of   as\n               .\n              The roots farthest from the origin is approximately  ,\n              as was also calculated in part (a).\n             "
},
{
  "id": "example-41",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#example-41",
  "type": "Example",
  "number": "20.6",
  "title": "",
  "body": "\n        Let  .\n       \n              Use the power method to approximate the dominant eigenvalue and a corresponding eigenvector\n              (using scaling)\n              accurate to two decimal places.\n              Use   as the seed.\n             \n              We use technology to calculate the scaled vectors\n                for values of   until the components don't change in the second decimal place.\n              For example,\n              to two decimal places we have   for  .\n              So we suspect that   is a dominant eigenvector for  .\n              For the dominant eigenvalue,\n              we can calculate the Rayleigh quotients\n                until they do not change to two decimal places.\n              For  , our Rayleigh quotients are all\n              (to two decimal places)\n              equal to  .\n              So we expect that the dominant eigenvalue of   is  .\n              We could also use the fact that\n               \n              to see that   is a dominant eigenvector for   with eigenvalue  .\n             \n              Determine the exact value of the dominant eigenvalue of   and compare to your result from part (a).\n             \n              Technology shows that the characteristic polynomial of   is\n               .\n              We can see from the characteristic polynomial that   is the dominant eigenvalue of  .\n             \n              Approximate the remaining eigenvalues of   using the inverse power method.\n             \n              Try   and  .\n             \n              Applying the power method to   with seed\n                gives   for  ,\n              with Rayleigh quotients of  \n              (to several decimal places).\n              So   is the dominant eigenvalue of  .\n              But   is also the dominant eigenvalue of  ,\n              where   is the corresponding eigenvalue of  . . So to find  ,\n              we note that   implies that\n                is an eigenvalue of  .\n              Now applying the power method to\n                with seed   gives\n                for large enough  ,\n              with Rayleigh quotients of  \n              (to several decimal places).\n              To find the corresponding eigenvalue   for  ,\n              we note that  ,\n              or   is an eigenvalue of  .\n              Admittedly, this method is very limited.\n              Finding good choices for   often depends on having some information about the eigenvalues of  .\n              Choosing   close to an eigenvalue provides the best chance of obtaining that eigenvalue.\n             "
},
{
  "id": "exercise-190",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#exercise-190",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Let  .\n        Let  .\n       \n              Find the eigenvalues and corresponding eigenvectors for  .\n             \n              Eigenvalues:\n                and  ; T  is an eigenvector for   with eigenvalue   and\n                is an eigenvector for   with eigenvalue  \n             \n              Use appropriate technology to calculate\n                for   up to 10.\n              Compare to a dominant eigenvector for  .\n             \n              As   increases, the vectors\n                are approaching the vector  ,\n              which is a dominant eigenvector of  .\n             \n              Use the eigenvectors from part (b) to approximate the dominant eigenvalue for  .\n              Compare to the exact value of the dominant eigenvalue of  .\n             \n              The Rayleigh quotients   approach the dominant eigenvalue  .\n             \n              Assume that the other eigenvalue for   is close to  .\n              Apply the inverse power method and compare the results to the remaining eigenvalue and eigenvectors for  .\n             \n              Apply the power method to  .\n              As   increases, the vectors\n                are approaching the vector  ,\n              which is an eigenvector of  .\n              The Rayleigh quotients approach the other eigenvalue   of  .\n             "
},
{
  "id": "exercise-191",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#exercise-191",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Let  .\n        Use the power method to approximate a dominant eigenvector for  .\n        Use   as the seed.\n        Then approximate the dominant eigenvalue of  .\n       "
},
{
  "id": "exercise-192",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#exercise-192",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Let  .\n        Use the power method starting with  .\n        Explain why the method fails in this case to approximate a dominant eigenvector,\n        and how you could adjust the seed to make the process work.\n       \n          is an eigenvector for   with eigenvalue  ,\n        so the vectors   are all equal to  .\n        We can adjust the seed to a non-eigenvector.\n       "
},
{
  "id": "exercise-193",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#exercise-193",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Let  .\n       \n              Find the eigenvalues and an eigenvector for each eigenvalue.\n             \n              Apply the power method with an initial starting vector  .\n              What is the resulting sequence?\n             \n              Use equation   to explain the sequence you found in part (b).\n             "
},
{
  "id": "exercise-194",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#exercise-194",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Let  .\n        Fill in the entries in  ,\n        where   is the  th approximation to a dominant eigenvector using the power method,\n        starting with the seed  .\n        Compare the results of this table to the eigenvalues of   and  .\n        What do you notice?\n         Values of the Rayleigh quotient \n       \n            is the dominant eigenvalue of  \n         "
},
{
  "id": "exercise-195",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#exercise-195",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Let  .\n        The power method will approximate the dominant eigenvalue  .\n        In this exercise we explore what happens if we apply the power method to  .\n       \n              Apply the power method to   to approximate the dominant eigenvalue of  .\n              Use   as the seed.\n              How is this eigenvalue related to an eigenvalue of  ?\n             \n              Explain in general why applying the power method to the inverse of an invertible matrix   might give an approximation to an eigenvalue of   of smallest magnitude.\n              When might this not work?\n             "
},
{
  "id": "exercise-196",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#exercise-196",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        There are other algebraic methods that do not rely on the determinant of a matrix that can be used to find eigenvalues of a matrix.\n        We examine one such method in this exercise.\n        Let   be any   matrix,\n        and let   be any vector in  .\n       \n              Explain why the vectors\n               \n              are linearly dependent.\n             \n              Since   has dimension  ,\n              it follows that any set of   vectors is linearly dependent.\n             \n              Let  ,  ,\n               ,   be scalars, not all 0, so that\n               .\n              Explain why there must be a smallest positive integer   so that there are scalars  ,\n               ,\n               ,   with  . such that\n               .\n             \n              Proceed down the list  ,\n               , etc., until you reach a weight that is non-zero.\n             \n              Let\n               .\n              Then\n               \n              and\n               .\n              Suppose the polynomial   has a linear factor,\n              say   for some degree   polynomial  .\n              Explain why, if   is non-zero,\n                is an eigenvalue of   with eigenvector  .\n             \n               \n             \n              This method allows us to find certain eigenvalues and eigenvectors,\n              the roots of the polynomial  .\n              Any other eigenvector must lie outside the eigenspaces we have already found,\n              so repeating the process with a vector   not in any of the known eigenspaces will produce different eigenvalues and eigenvectors.\n              Let  .\n             \n                    Find the polynomial  .\n                    Use  .\n                   \n                     \n                   \n                    Find all of the roots of  .\n                   \n                     ,  , and  \n                   \n                    For each root   of  ,\n                    find the polynomial   and use this polynomial to determine an eigenvector of  .\n                    Verify your work.\n                   \n                    For  , we have  ,\n                    and   an eigenvector for   with eigenvalue  .\n                    For   we have  ,\n                    and   is an eigenvector for   with eigenvalue  .\n                    For   we have  ,\n                    and   is an eigenvector for   with eigenvalue  .\n                   "
},
{
  "id": "ex_4_d_dominant_eigenvalue",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#ex_4_d_dominant_eigenvalue",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        We have seen that the Rayleigh quotients approximate the dominant eigenvalue of a matrix  .\n        As an alternative to using Rayleigh quotients,\n        we can keep track of the scaling factors.\n        Recall that the scaling in the power method can be used to make the magnitudes of the successive approximations smaller and easier to work with.\n        Let   be an   matrix and begin with a non-zero seed  .\n        We now want to keep track of the scaling factors,\n        so let   be the component of   with largest absolute value and let  .\n        For  , let  ,\n        let   be the component of   with largest absolute value and let  .\n       \n              Let  .\n              Use   as the seed and calculate\n                for   from   to  .\n              Compare to the dominant eigenvalue of  .\n             \n              Assume that for large   the vectors   approach a dominant eigenvector with dominant eigenvalue  .\n              Show now in general that the sequence of scaling factors\n                approaches  .\n             "
},
{
  "id": "exercise-198",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#exercise-198",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        Let   be an   matrix and let   be a scalar that is not an eigenvalue of  .\n        Suppose that   is an eigenvector of\n          with eigenvalue  .\n        Find an eigenvalue of   in terms of   and   with corresponding eigenvector  .\n       \n        If   is an eigenvalue of   with eigenvector  ,\n        then   is an eigenvalue of   with eigenvector  .\n       "
},
{
  "id": "exercise-199",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#exercise-199",
  "type": "Exercise",
  "number": "10",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              The largest eigenvalue of a matrix is a dominant eigenvalue.\n             \n              F\n             True\/False \n              If an   matrix   has   linearly independent eigenvectors and a dominant eigenvalue,\n              then the sequence   converges to a dominant eigenvector of   for any initial vector  .\n             True\/False \n              If   is an eigenvalue of an\n                matrix   and   is not an eigenvalue of  ,\n              then   is an eigenvalue of  .\n             \n              T\n             True\/False \n              Every square matrix has a dominant eigenvalue.\n             "
},
{
  "id": "T_Sheep",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#T_Sheep",
  "type": "Table",
  "number": "20.8",
  "title": "New Zealand female sheep data by age group",
  "body": "New Zealand female sheep data by age group Age (years) Birth Rate Survival Rate 0-1 0.000 0.845 1-2 0.045 0.975 2-3 0.391 0.965 3-4 0.472 0.950 4-5 0.484 0.926 5-6 0.546 0.895 6-7 0.543 0.850 7-8 0.502 0.786 8-9 0.468 0.691 9-10 0.459 0.561 10-11 0.433 0.370 11-12 0.421 0.000 "
},
{
  "id": "p-3475",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#p-3475",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "fecundity fecund "
},
{
  "id": "F_Life_cycle",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#F_Life_cycle",
  "type": "Figure",
  "number": "20.9",
  "title": "",
  "body": "Life cycle with four age classes. "
},
{
  "id": "Leslie_1",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#Leslie_1",
  "type": "Project Activity",
  "number": "20.6",
  "title": "",
  "body": "\n        shows that, on average,\n      each female in age group 1-2 produces   female offspring in a year.\n      Since there are   females in age group 1-2, the lamb population increases by   in a year.\n     \n            Continue this analysis to explain why\n             .\n           \n            Explain why  .\n           \n            Now explain why\n             ,\n            where   is the matrix\n             .\n           "
},
{
  "id": "p-3482",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#p-3482",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Leslie matrix "
},
{
  "id": "p-3483",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#p-3483",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "dominant eigenvalue strictly dominant eigenvalue "
},
{
  "id": "act_Leslie_2",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#act_Leslie_2",
  "type": "Project Activity",
  "number": "20.7",
  "title": "",
  "body": "\n      Assume that   is defined by  , and let\n       ,\n      where   denotes the number of sheep in age group 0-1,\n        the number of sheep in age group 1-2 and,\n      in general,\n        the number of sheep in age group  -  after   years.\n     \n            Assume that  .\n            Use appropriate technology to calculate  ,\n             ,  , and  .\n            Round to the nearest whole number.\n            What do you notice about the sheep population?\n            You may use the GeoGebra applet at  .\n           \n            We can use the third and fourth properties of Leslie matrices to better understand the long-term behavior of the sheep population.\n            Since successive entries in the first row of the Leslie matrix in   are positive,\n            our Leslie matrix has a strictly dominant eigenvalue  .\n            Given the dimensions of our Leslie matrix,\n            finding this dominant eigenvalue through algebraic means is not feasible.\n            Use the power method to approximate the dominant eigenvalue\n              of the Leslie matrix in   to five decimal places.\n            Explain your process.\n            Then explain how this dominant eigenvalue tells us that,\n            unchecked,\n            the sheep population grows at a rate that is roughly exponential.\n            What is the growth rate of this exponential growth?\n            You may use the GeoGebra applet at  .\n           "
},
{
  "id": "act_Leslie_harvest",
  "level": "2",
  "url": "chap_approx_eigenvalues.html#act_Leslie_harvest",
  "type": "Project Activity",
  "number": "20.8",
  "title": "",
  "body": "\n      The Leslie model can be modified to consider harvesting.\n      It is possible to harvest different age groups at different rates,\n      and to harvest only some age groups and not others.\n      In the case of sheep,\n      it might make sense to only harvest from the youngest population since lamb is more desirable than mutton and the lamb population grows the fastest.\n      Assume that this is our harvesting strategy and that we harvest our sheep from only the youngest age group at the start of each year.\n      Let   be the fraction of sheep we harvest from the youngest age group each year after considering growth.\n     \n            If we begin with an initial population  ,\n            then the state vector after births and expected deaths is  .\n            Now we harvest.\n            Explain why if we harvest a fraction   from the youngest age group after considering growth,\n            then the state vector after 1 year will be\n             ,\n            where\n             .\n           \n            Our goal is to find a harvesting rate that will lead to a steady state in which the sheep population remains the same each year.\n            In other words, we want to find a value of  ,\n            if one exists, that satisfies\n             .\n            Show that   is equivalent to the matrix equation\n             .\n           \n            Use appropriate technology to experiment numerically with different values of   to find the value you think gives the best uniform harvest rate.\n            Explain your reasoning.\n            You may use the GeoGebra applet at  .\n           \n            Now we will use some algebra to find an equation that explicitly gives us the harvest rate in the general setting.\n            This will take a bit of work,\n            but none of it is too difficult.\n            To simplify our work but yet illustrate the overall idea,\n            let us consider the general\n              case with arbitrary Leslie matrix\n             .\n            Recall that we want to find a value of   that satisfies   with  .\n            Let  .\n           \n                  Calculate the matrix product  .\n                  Explain why this product is again a Leslie matrix and why\n                    will have a dominant eigenvalue of 1.\n                 \n                  Now calculate   and set it equal to  .\n                  Write down the resulting system of 4 equations that must be satisfied.\n                  Be sure that your first equation is\n                   .\n                 \n                  Equation   as written depends on the entries of the vector  ,\n                  but we should be able to arrive at a result that is independent of  .\n                  To see how we do this,\n                  we assume the population of the youngest group is never 0, so we can divide both sides of   by   to obtain\n                   .\n                  Now we need to write the fractions\n                   ,    ,\n                  and   so that they do not involve the  .\n                  Use the remaining equations in your system to show that\n                   .\n                 net reproduction rate of the population \n            Extend   to the 12 age group case of the sheep herd.\n            Calculate the value of   for this sheep herd and then find the value of  .\n            Compare this   to the value you obtained through experimentation earlier.\n            Find the fraction of the lambs that should be harvested each year and explain what the stable population state vector   tells us about the sheep population for this harvesting policy.\n           "
},
{
  "id": "chap_complex_eigenvalues",
  "level": "1",
  "url": "chap_complex_eigenvalues.html",
  "type": "Section",
  "number": "21",
  "title": "Complex Eigenvalues",
  "body": "Complex Eigenvalues \n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What properties do complex eigenvalues of a real matrix satisfy?\n           \n         \n           \n            What properties do complex eigenvectors of a real matrix satisfy?\n           \n         \n           \n            What is a rotation-scaling matrix?\n           \n         \n           \n            How do we find a rotation-scaling matrix within a matrix with complex eigenvalues?\n           \n         Application: The Gershgorin Disk Theorem Gershgorin disks Gershgorin disks. \n    We will learn more details about the Gershgorin Disk Theorem at the end of this section.\n   Introduction \n    So far we have worked with real matrices whose eigenvalues are all real.\n    However, the characteristic polynomial of a matrix with real entries can have complex roots.\n    In this section we investigate the properties of these complex roots and their corresponding eigenvectors,\n    how these complex eigenvectors are found,\n    and the geometric interpretation of the transformations defined by matrices with complex eigenvalues.\n    Although we can consider matrices that have complex numbers as entries,\n    we will restrict ourselves to matrices with real entries.\n   \n      Let  .\n     \n            Find the characteristic polynomial of  .\n           \n            Find the eigenvalues of  .\n            You should get two complex numbers.\n            How are these complex numbers related?\n           \n            Find an eigenvector corresponding to each eigenvalue of  .\n            You should obtain vectors with complex entries.\n           Complex Eigenvalues \n    As you noticed in  ,\n    the complex roots of the characteristic equation of a real matrix   come in complex conjugate pairs.\n    This should come as no surprise since we know through our use of the quadratic formula that complex roots of (real) quadratic polynomials come in complex conjugate pairs.\n    More generally,\n    if   is a polynomial with real coefficients and   is a root of this polynomial,\n    meaning  , then\n     .\n\n    Therefore,   is also a root of  .\n   \n      Let  .\n     \n            The matrix transformation   defined by\n              is a rotation transformation.\n            What is the angle of rotation?\n           \n            Find the eigenvalues of  .\n            For each eigenvalue, find an eigenvector.\n           \n    In  \n    and in  ,\n    you found that if   is an eigenvector of   corresponding to  ,\n    then   obtained by taking the complex conjugate of each entry in   is an eigenvector of   corresponding to  .\n    Specifically,\n    if   where both   and   are real vectors is an eigenvector of  ,\n    then so is  .\n    We can justify this property using matrix algebra as follows:\n     .\n   \n    In the first equality,\n    we used the fact that   is a real matrix, so  .\n    In all the other equalities,\n    we used the properties of the conjugation operation in complex numbers.\n   Rotation and Scaling Matrices \n    Recall that a rotation matrix is of the form\n     \n    where the rotation is counterclockwise about the origin by an angle of   radians.\n    In  ,\n    we considered the rotation matrix with angle   in counterclockwise direction.\n    We will soon see that rotation matrices play an important role in the geometry of a matrix transformation for a matrix that has complex eigenvalues.\n    In this activity,\n    we will restrict ourselves to the   case,\n    but similar arguments can be made in higher dimensions.\n   \n      Let  .\n     \n            Explain why   is not a rotation matrix.\n           \n            Although   is not a rotation matrix,\n            there is a rotation matrix   inside  .\n            To find the matrix  ,\n            factor out   from all entries of  .\n            In other words,\n            write   as a product of two matrices in the form\n             .\n           \n            The   matrix is a rotation matrix with an appropriate  .\n            Find this  .\n           \n            If we think about the product of two matrices as applying one transformation after another,\n            describe the effect of the matrix transformation defined by   geometrically.\n           \n    More generally,\n    if we have a matrix   of the form  , then\n     .\n   rotation-scaling matrices Matrices with Complex Eigenvalues \n    Now we will investigate how a general\n      matrix with complex eigenvalues can be seen to be similar\n    (both in a linear algebra and a colloquial meaning)\n    to a rotation-scaling matrix.\n   \n      Let  .\n      The eigenvalues of   are  .\n      An eigenvector for the eigenvalue   is  .\n      We will use this eigenvector to show that   is similar to a rotation-scaling matrix.\n     \n            Any complex vector   can be written as\n              where both   and   are real vectors.\n            What are these real vectors   and   for the eigenvector   above?\n           \n            Let   be the matrix whose first column is the real part of   and whose second column is the imaginary part of  \n            (without the  ).\n            Find  .\n           \n            Express   as a product of a rotation and a scaling matrix.\n            What is the factor of scaling?\n            What is the rotation angle?\n           \n    In  ,\n    we saw that the matrix   with complex eigenvalues   is similar to a rotation-scaling matrix.\n    Specifically  ,\n    where the columns of   are the real and imaginary parts of an eigenvector of  ,\n    is the rotation-scaling matrix with a factor of scaling by\n      and a rotation by angle  .\n   \n    Does a similar decomposition result hold for a general\n      matrix with complex eigenvalues?\n    We investigate this question in the next activity.\n   \n      Let   be a   matrix with complex eigenvalue  ,\n       ,\n      and corresponding complex eigenvector  .\n     \n          Explain why  .\n         \n          Explain why  .\n         \n          Use the previous two results to explain why\n         \n             \n                and\n             \n           \n             \n               .\n             \n           \n          Let  .\n          We will now show that   where  .\n         \n                Without any calculation, explain why\n                 .\n               \n                Recall that if   is an\n                  matrix and   is an   vector,\n                then the matrix product   is a linear combination of the columns of   with weights the corresponding entries of the vector  .\n                Use this idea to show that\n                 .\n               \n                Now explain why  .\n               \n                Assume for the moment that   is an invertible matrix.\n                Show that  .\n               \n    Your work in  \n    shows that any   matrix is similar to a rotation-scaling matrix with a factor of scaling by\n      and a rotation by angle   if  ,\n    and   if  .\n    Geometrically, this means that every\n      real matrix with complex eigenvalues is just a scaled rotation ( ) with respect to the basis   formed by   and   from the complex eigenvector  .\n    Multiplying by   and   simply provides the change of basis from the standard basis to the basis  ,\n    as we will see in detail when we learn about linear transformations.\n   \n        Let   be a real   matrix with complex eigenvalue   and corresponding eigenvector  .\n        Then\n         .\n       \n    The one fact that we have not yet addressed is why the matrix   is invertible.\n    We do that now to complete the argument.\n   \n    Let   be a real   matrix with  ,\n    where  ,\n      and  \n    (where   and   are in  )\n    with  .\n    From   we know that\n     .\n   \n    To show that   and   are linearly independent,\n    we need to show that no nontrivial linear combination of   and   can be the zero vector.\n    Suppose\n     \n    for some scalars   and  .\n    We will show that  .\n    Assume to the contrary that one of   is not zero.\n    First, assume  .\n    Then  .\n    Let  .\n    From this we have\n     .\n   \n    Since   we must have  .\n    A little algebra shows that  .\n    Since  , we conclude that  ,\n    which is impossible for a real constant  .\n    Therefore, we cannot have  .\n    A similar argument\n    (left to the reader)\n    shows that  .\n    Thus we can conclude that   and   are linearly independent.\n   Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Let  .\n       \n              Without doing any computations,\n              explain why not all of the eigenvalues of   can be complex.\n             \n              Since complex eigenvalues occur in conjugate pairs,\n              the complex eigenvalues with nonzero imaginary parts occur in pairs.\n              Since   can have at most 3 different eigenvalues,\n              at most two of them can have nonzero imaginary parts.\n              So at least one eigenvalue of   is real.\n             \n              Find all of the eigenvalues of  .\n             \n              For this matrix   we have  .\n              Using a cofactor expansion along the first row gives us\n               .\n              The roots of the characteristic polynomial are   and\n               .\n             \n        Let  .\n        Find a rotation scaling matrix   that is similar to  .\n        Identify the rotation and scaling factor.\n       \n        The eigenvalues of   are the roots of the characteristic polynomial\n         .\n       \n        The quadratic formula shows that the roots of   are\n         .\n       \n        To find an eigenvector for   with eigenvalue  ,\n        we row reduce\n         \n        to\n         .\n       \n        An eigenvector for   with eigenvalue   is then\n         .\n       \n        Letting  , we have\n         .\n       \n        The scaling is determined by the determinant of   which is  ,\n        and the angle   of rotation satisfies  .\n        This makes   radians or approximately   counterclockwise.\n       Summary \n       \n        For a real matrix, complex eigenvalues appear in conjugate pairs.\n        Specifically,\n        if   is an eigenvalue of a real matrix  ,\n        then   is also an eigenvalue of  .\n       \n     \n       \n        For a real matrix,\n        if a   is an eigenvector corresponding to  ,\n        then the vector   obtained by taking the complex conjugate of each entry in   is an eigenvector corresponding to  .\n       \n     \n       \n        The rotation-scaling matrix   can be written as\n         .\n        This decomposition geometrically means that the transformation corresponding to   can be viewed as a rotation by angle   if  ,\n        or   if  ,\n        followed by a scaling by factor  .\n       \n     \n       \n        If   is a real   matrix with complex eigenvalue   and corresponding eigenvector  ,\n        then   is similar to the rotation-scaling matrix  .\n        More specifically,\n         .\n       \n     \n        Find eigenvalues and eigenvectors of each of the following matrices.\n       \n               \n             \n              Eigenvalues:  and  ; Eigenvectors:\n                and  \n             \n               \n             \n              Eigenvalues:\n                and  ; Eigenvectors:\n               ,  \n             \n               \n             \n              Eigenvalues:\n                and  ; Eigenvectors:\n                and  \n             \n        Find a rotation-scaling matrix where the rotation angle is\n          and scaling factor is less than 1.\n       \n        Determine which rotation-scaling matrices have determinant equal to 1.\n        Be as specific as possible.\n       \n        Just the rotation matrices\n       \n        Determine the rotation-scaling matrix inside the matrix  .\n       \n        Find a real   matrix with eigenvalue  .\n       \n         \n       \n        Find a real   matrix which is not a rotation-scaling matrix with eigenvalue  .\n       \n        We have seen how to find the characteristic polynomial of an   matrix.\n        In this exercise we consider the reverse question.\n        That is, given a polynomial   of degree  ,\n        can we find an   matrix whose characteristic polynomial is  ?\n       \n              Find the characteristic polynomial of the\n                matrix  .\n              Use this result to find a real valued matrix whose eigenvalues are   and  .\n             \n              Characteristic polynomial  ;\n               \n             \n              Repeat part (a) by showing that\n                is the characteristic polynomial of the\n                matrix  .\n             \n                \n             companion matrix companion matrix \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If   is an eigenvalue of a real matrix,\n              then so is  .\n             \n              T\n             True\/False \n              If   is an eigenvalue of a\n                real matrix  ,\n              then   has three distinct eigenvalues.\n             True\/False \n              Every   real matrix with complex eigenvalues is a rotation-scaling matrix.\n             \n              F\n             True\/False \n              Every square matrix with real entries has real number eigenvalues.\n             True\/False \n              If   is a\n                matrix with complex eigenvalues similar to a rotation-scaling matrix  ,\n              the eigenvalues of   and   are the same.\n             \n              T\n             True\/False \n              If   is a real matrix with complex eigenvalues,\n              all eigenvectors of   must be non-real.\n             Project: Understanding the Gershgorin Disk Theorem \n    To understand the Gershgorin Disk Theorem,\n    we need to recall how to visualize a complex number in the plane.\n    Recall that a complex number   is a number of the form\n      where   and   are real numbers and  .\n    The number   is the real part of  ,\n    denoted as  ,\n    and   is the imaginary part of  ,\n    denoted  .\n    The set of all complex numbers is denoted  .\n    We define addition and multiplication on   as follows.\n    For  ,\n     .\n   \n    Note that the product is what we would expect if we\n     expanded \n    the product in the normal way and used the fact that  .\n    The set of complex numbers forms a field   that is,\n      satisfies all of the same properties as   as stated in  .\n   \n    We can visualize the complex number   in the plane as the point  .\n    Here we are viewing the horizontal axis as the real axis and the vertical axis as the imaginary axis.\n    The length (or magnitude) of the complex number  ,\n    which we denote as  ,\n    is the distance from the origin to  .\n    So by the Pythagorean Theorem we have  .\n    Note that the magnitude of\n      can be written as a complex product\n     .\n   complex conjugate \n    Using these facts we can show that the triangle inequality is true for complex numbers.\n    That is,\n     .\n   \n    To see why, notice that\n     .\n   \n    Since  ,  ,\n    and   are all non-negative,\n    taking square roots of both sides gives us   as desired.\n    We can extend this triangle inequality to any number of complex numbers.\n    That is, if  ,  ,\n     ,   are complex numbers, then\n     .\n   \n    We can prove Equation   by mathematical induction.\n    We have already done the   case and so we assume that Equation   is true for any sum of   complex numbers.\n    Now suppose that  ,  ,\n     ,  ,   are complex numbers.\n    Then\n     .\n   \n    To prove the Gershgorin Disk Theorem,\n    we will use the Levy-Desplanques Theorem,\n    which gives conditions that guarantee that a matrix is invertible.\n    We illustrate with an example in the following activity.\n   \n      Let  .\n      Since  , we know that   is an invertible matrix.\n      Let us assume for a moment that we don't know that   is invertible and try to determine if 0 is an eigenvalue of  .\n      In other words,\n      we want to know if there is a nonzero vector   so that  .\n      Assuming the existence of such a vector  ,\n      for   to be   it must be the case that\n       .\n     \n      Since the vector   is not the zero vector,\n      at least one of  ,   is not zero.\n      Note that if one of  ,\n        is zero, the so is the other.\n      So we can assume that   and   are nonzero.\n     \n            Use the fact that   to show that  .\n           \n            Use the fact that   to show that  .\n            What conclusion can we draw about whether 0 is an eigenvalue of  ?\n            Why does this mean that   is invertible?\n           \n    What makes the arguments work in  \n    is that   and  .\n    This argument can be extended to larger matrices,\n    as described in the following theorem.\n   Levy-Desplanques Theorem \n        Any square matrix   satisfying\n          for all   is invertible.\n       \n      Let   be an\n        matrix satisfying   for all  .\n      Let us assume that   is not invertible,\n      that is that there is a vector\n        such that  .\n      Let   and   be between 1 and   so that   for all  .\n      That is, choose   to be the component of   with the largest absolute value.\n     \n      Expanding the product   using the row-column product along the  th row shows that\n       .\n     \n      Solving for the   term gives us\n       .\n     \n      Then\n       .\n     \n      Since  , we cancel the   term to conclude that\n       .\n     \n      But this contradicts the condition that   for all  .\n      We conclude that 0 is not an eigenvalue for   and   is invertible.\n     \n    Any matrix   satisfying the condition of the Levy-Desplanques Theorem is given a special name.\n   strictly diagonally dominant matrix strictly diagonally dominant \n    So any strictly diagonally dominant matrix is invertible.\n    A quick glance can show that a matrix is strictly diagonally dominant.\n    For example, since  ,\n     , and  , the matrix\n     \n    is strictly diagonally dominant and therefore invertible.\n    However, just because a matrix is not strictly diagonally dominant,\n    it does not follow that the matrix is non-invertible.\n    For example,\n    the matrix   is invertible,\n    but not strictly diagonally dominant.\n   \n    Now we can address the Gershgorin Disk Theorem.\n   Gershgorin Disk Theorem \n      Let   be an arbitrary   matrix and assume that   is an eigenvalue of  .\n     \n            Explain why the matrix   is singular.\n           \n            What does the Levy-Desplanques Theorem tell us about the matrix  ?\n           \n            Explain how we can conclude the Gershgorin Disk Theorem.\n             Gershgorin Disk Theorem \n                  Let   be an\n                    matrix with complex entries.\n                  Then every eigenvalue of   lies in one of the Gershgorin discs\n                   ,\n                  where  .\n                 \n            Based on this theorem,\n            we define a Gershgorin disk to be  ,\n            where  .\n           \n            Use the Gershgorin Disk Theorem to give estimates on the locations of the eigenvalues of the matrix  .\n           \n    The Gershgorin Disk Theorem has a consequence that gives additional information about the eigenvalues if some of the Gershgorin disks do not overlap.\n   \n        If   is a union of   Gershgorin disks of a matrix   such that   does not intersect any other Gershgorin disk,\n        then   contains exactly   eigenvalues\n        (counting multiplicities)\n        of  .\n       \n      Most proofs of this theorem require some results from topology.\n      For that reason,\n      we will not present a completely rigorous proof but rather give the highlights.\n      Let   be an   matrix.\n      Let   be a collection of Gershgorin disks of   for   such that\n        does not intersect any other Gershgorin disk of  ,\n      and let   be the union of the Gershgorin disks of   that are different from the  .\n      Note that  .\n      Let   be the matrix whose  th column is  ,\n      that is   is the diagonal matrix whose diagonal entries are the corresponding diagonal entries of  .\n      Note that the eigenvalues of   are   and the Gershgorin disks of   are just the points  .\n      So our theorem is true for this matrix  .\n      To prove the result,\n      we build a continuum of matrices from   to   as follows:\n      let  \n      (so that   is the matrix whose off-diagonal entries are those of   and whose diagonal entries are 0),\n      and let   for   in the interval  .\n      Note that  .\n      Since the diagonal entries of   are the same as those of  ,\n      the Gershgorin disks of   have the same centers as the corresponding Gershgorin disks of  ,\n      while the radii of the Gershgorin disks of   are those of   but scaled by  .\n      So the Gershgorin disks of   increase from points\n      (the  )\n      to the Gershgorin disks of   as   increases from 0 to 1.\n      While the centers of the disks all remain fixed,\n      it is important to recognize that the eigenvalues of   move as   changes.\n      An illustration of this is shown in  \n      with the eigenvalues as the black points and the changing Gershgorin disks dashed in magenta,\n      using the matrix  .\n      We can learn about how the eigenvalues move with the characteristic polynomial.\n     How eigenvalues move. \n      Let   be the characteristic polynomial of  .\n      Note that these characteristic polynomials are functions of both   and  .\n      Since polynomials are continuous functions,\n      their roots (the eigenvalues of  ) are continuous for   as well.\n      Let   be an eigenvalue of  .\n      Note that   is an eigenvalue of  ,\n      and   is one of the   and is therefore in  .\n      We will argue that   is in   for every value of   in  .\n      Let   be the radius of   and let   be the Gershgorin disk of   with the same center as   and radius  .\n      Let  .\n      Since  ,\n      it follows that   and so   as well.\n      From topology,\n      we know that since the disks   are closed,\n      the union   of these disks is also closed.\n      Similarly,   and   are closed.\n      Thus,   is continuous in a closed set and so does not leave the set.\n      Thus,   is in   for every value of   in  .\n     "
},
{
  "id": "objectives-21",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#objectives-21",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What properties do complex eigenvalues of a real matrix satisfy?\n           \n         \n           \n            What properties do complex eigenvectors of a real matrix satisfy?\n           \n         \n           \n            What is a rotation-scaling matrix?\n           \n         \n           \n            How do we find a rotation-scaling matrix within a matrix with complex eigenvalues?\n           \n         "
},
{
  "id": "p-3508",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#p-3508",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Gershgorin disks "
},
{
  "id": "F_Gershgorin_1",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#F_Gershgorin_1",
  "type": "Figure",
  "number": "21.1",
  "title": "",
  "body": "Gershgorin disks. "
},
{
  "id": "pa_4_e",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#pa_4_e",
  "type": "Preview Activity",
  "number": "21.1",
  "title": "",
  "body": "\n      Let  .\n     \n            Find the characteristic polynomial of  .\n           \n            Find the eigenvalues of  .\n            You should get two complex numbers.\n            How are these complex numbers related?\n           \n            Find an eigenvector corresponding to each eigenvalue of  .\n            You should obtain vectors with complex entries.\n           "
},
{
  "id": "act_4e_1",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#act_4e_1",
  "type": "Activity",
  "number": "21.2",
  "title": "",
  "body": "\n      Let  .\n     \n            The matrix transformation   defined by\n              is a rotation transformation.\n            What is the angle of rotation?\n           \n            Find the eigenvalues of  .\n            For each eigenvalue, find an eigenvector.\n           "
},
{
  "id": "act_4e_2",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#act_4e_2",
  "type": "Activity",
  "number": "21.3",
  "title": "",
  "body": "\n      Let  .\n     \n            Explain why   is not a rotation matrix.\n           \n            Although   is not a rotation matrix,\n            there is a rotation matrix   inside  .\n            To find the matrix  ,\n            factor out   from all entries of  .\n            In other words,\n            write   as a product of two matrices in the form\n             .\n           \n            The   matrix is a rotation matrix with an appropriate  .\n            Find this  .\n           \n            If we think about the product of two matrices as applying one transformation after another,\n            describe the effect of the matrix transformation defined by   geometrically.\n           "
},
{
  "id": "p-3528",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#p-3528",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "rotation-scaling matrices "
},
{
  "id": "act_4e_3",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#act_4e_3",
  "type": "Activity",
  "number": "21.4",
  "title": "",
  "body": "\n      Let  .\n      The eigenvalues of   are  .\n      An eigenvector for the eigenvalue   is  .\n      We will use this eigenvector to show that   is similar to a rotation-scaling matrix.\n     \n            Any complex vector   can be written as\n              where both   and   are real vectors.\n            What are these real vectors   and   for the eigenvector   above?\n           \n            Let   be the matrix whose first column is the real part of   and whose second column is the imaginary part of  \n            (without the  ).\n            Find  .\n           \n            Express   as a product of a rotation and a scaling matrix.\n            What is the factor of scaling?\n            What is the rotation angle?\n           "
},
{
  "id": "act_4e_4",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#act_4e_4",
  "type": "Activity",
  "number": "21.5",
  "title": "",
  "body": "\n      Let   be a   matrix with complex eigenvalue  ,\n       ,\n      and corresponding complex eigenvector  .\n     \n          Explain why  .\n         \n          Explain why  .\n         \n          Use the previous two results to explain why\n         \n             \n                and\n             \n           \n             \n               .\n             \n           \n          Let  .\n          We will now show that   where  .\n         \n                Without any calculation, explain why\n                 .\n               \n                Recall that if   is an\n                  matrix and   is an   vector,\n                then the matrix product   is a linear combination of the columns of   with weights the corresponding entries of the vector  .\n                Use this idea to show that\n                 .\n               \n                Now explain why  .\n               \n                Assume for the moment that   is an invertible matrix.\n                Show that  .\n               "
},
{
  "id": "theorem-44",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#theorem-44",
  "type": "Theorem",
  "number": "21.2",
  "title": "",
  "body": "\n        Let   be a real   matrix with complex eigenvalue   and corresponding eigenvector  .\n        Then\n         .\n       "
},
{
  "id": "example-42",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#example-42",
  "type": "Example",
  "number": "21.3",
  "title": "",
  "body": "\n        Let  .\n       \n              Without doing any computations,\n              explain why not all of the eigenvalues of   can be complex.\n             \n              Since complex eigenvalues occur in conjugate pairs,\n              the complex eigenvalues with nonzero imaginary parts occur in pairs.\n              Since   can have at most 3 different eigenvalues,\n              at most two of them can have nonzero imaginary parts.\n              So at least one eigenvalue of   is real.\n             \n              Find all of the eigenvalues of  .\n             \n              For this matrix   we have  .\n              Using a cofactor expansion along the first row gives us\n               .\n              The roots of the characteristic polynomial are   and\n               .\n             "
},
{
  "id": "example-43",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#example-43",
  "type": "Example",
  "number": "21.4",
  "title": "",
  "body": "\n        Let  .\n        Find a rotation scaling matrix   that is similar to  .\n        Identify the rotation and scaling factor.\n       \n        The eigenvalues of   are the roots of the characteristic polynomial\n         .\n       \n        The quadratic formula shows that the roots of   are\n         .\n       \n        To find an eigenvector for   with eigenvalue  ,\n        we row reduce\n         \n        to\n         .\n       \n        An eigenvector for   with eigenvalue   is then\n         .\n       \n        Letting  , we have\n         .\n       \n        The scaling is determined by the determinant of   which is  ,\n        and the angle   of rotation satisfies  .\n        This makes   radians or approximately   counterclockwise.\n       "
},
{
  "id": "exercise-200",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#exercise-200",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Find eigenvalues and eigenvectors of each of the following matrices.\n       \n               \n             \n              Eigenvalues:  and  ; Eigenvectors:\n                and  \n             \n               \n             \n              Eigenvalues:\n                and  ; Eigenvectors:\n               ,  \n             \n               \n             \n              Eigenvalues:\n                and  ; Eigenvectors:\n                and  \n             "
},
{
  "id": "exercise-201",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#exercise-201",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Find a rotation-scaling matrix where the rotation angle is\n          and scaling factor is less than 1.\n       "
},
{
  "id": "exercise-202",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#exercise-202",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Determine which rotation-scaling matrices have determinant equal to 1.\n        Be as specific as possible.\n       \n        Just the rotation matrices\n       "
},
{
  "id": "exercise-203",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#exercise-203",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Determine the rotation-scaling matrix inside the matrix  .\n       "
},
{
  "id": "exercise-204",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#exercise-204",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Find a real   matrix with eigenvalue  .\n       \n         \n       "
},
{
  "id": "exercise-205",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#exercise-205",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Find a real   matrix which is not a rotation-scaling matrix with eigenvalue  .\n       "
},
{
  "id": "exercise-206",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#exercise-206",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        We have seen how to find the characteristic polynomial of an   matrix.\n        In this exercise we consider the reverse question.\n        That is, given a polynomial   of degree  ,\n        can we find an   matrix whose characteristic polynomial is  ?\n       \n              Find the characteristic polynomial of the\n                matrix  .\n              Use this result to find a real valued matrix whose eigenvalues are   and  .\n             \n              Characteristic polynomial  ;\n               \n             \n              Repeat part (a) by showing that\n                is the characteristic polynomial of the\n                matrix  .\n             \n                \n             companion matrix companion matrix "
},
{
  "id": "exercise-207",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#exercise-207",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If   is an eigenvalue of a real matrix,\n              then so is  .\n             \n              T\n             True\/False \n              If   is an eigenvalue of a\n                real matrix  ,\n              then   has three distinct eigenvalues.\n             True\/False \n              Every   real matrix with complex eigenvalues is a rotation-scaling matrix.\n             \n              F\n             True\/False \n              Every square matrix with real entries has real number eigenvalues.\n             True\/False \n              If   is a\n                matrix with complex eigenvalues similar to a rotation-scaling matrix  ,\n              the eigenvalues of   and   are the same.\n             \n              T\n             True\/False \n              If   is a real matrix with complex eigenvalues,\n              all eigenvectors of   must be non-real.\n             "
},
{
  "id": "p-3603",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#p-3603",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "complex conjugate "
},
{
  "id": "act_Gershgorin_1",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#act_Gershgorin_1",
  "type": "Project Activity",
  "number": "21.6",
  "title": "",
  "body": "\n      Let  .\n      Since  , we know that   is an invertible matrix.\n      Let us assume for a moment that we don't know that   is invertible and try to determine if 0 is an eigenvalue of  .\n      In other words,\n      we want to know if there is a nonzero vector   so that  .\n      Assuming the existence of such a vector  ,\n      for   to be   it must be the case that\n       .\n     \n      Since the vector   is not the zero vector,\n      at least one of  ,   is not zero.\n      Note that if one of  ,\n        is zero, the so is the other.\n      So we can assume that   and   are nonzero.\n     \n            Use the fact that   to show that  .\n           \n            Use the fact that   to show that  .\n            What conclusion can we draw about whether 0 is an eigenvalue of  ?\n            Why does this mean that   is invertible?\n           "
},
{
  "id": "theorem-45",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#theorem-45",
  "type": "Theorem",
  "number": "21.5",
  "title": "Levy-Desplanques Theorem.",
  "body": "Levy-Desplanques Theorem \n        Any square matrix   satisfying\n          for all   is invertible.\n       \n      Let   be an\n        matrix satisfying   for all  .\n      Let us assume that   is not invertible,\n      that is that there is a vector\n        such that  .\n      Let   and   be between 1 and   so that   for all  .\n      That is, choose   to be the component of   with the largest absolute value.\n     \n      Expanding the product   using the row-column product along the  th row shows that\n       .\n     \n      Solving for the   term gives us\n       .\n     \n      Then\n       .\n     \n      Since  , we cancel the   term to conclude that\n       .\n     \n      But this contradicts the condition that   for all  .\n      We conclude that 0 is not an eigenvalue for   and   is invertible.\n     "
},
{
  "id": "definition-45",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#definition-45",
  "type": "Definition",
  "number": "21.6",
  "title": "",
  "body": "strictly diagonally dominant matrix strictly diagonally dominant "
},
{
  "id": "act_Gershgorin_2",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#act_Gershgorin_2",
  "type": "Activity",
  "number": "21.7",
  "title": "",
  "body": "Gershgorin Disk Theorem \n      Let   be an arbitrary   matrix and assume that   is an eigenvalue of  .\n     \n            Explain why the matrix   is singular.\n           \n            What does the Levy-Desplanques Theorem tell us about the matrix  ?\n           \n            Explain how we can conclude the Gershgorin Disk Theorem.\n             Gershgorin Disk Theorem \n                  Let   be an\n                    matrix with complex entries.\n                  Then every eigenvalue of   lies in one of the Gershgorin discs\n                   ,\n                  where  .\n                 \n            Based on this theorem,\n            we define a Gershgorin disk to be  ,\n            where  .\n           \n            Use the Gershgorin Disk Theorem to give estimates on the locations of the eigenvalues of the matrix  .\n           "
},
{
  "id": "thm_Gersgorin_consequence",
  "level": "2",
  "url": "chap_complex_eigenvalues.html#thm_Gersgorin_consequence",
  "type": "Theorem",
  "number": "21.8",
  "title": "",
  "body": "\n        If   is a union of   Gershgorin disks of a matrix   such that   does not intersect any other Gershgorin disk,\n        then   contains exactly   eigenvalues\n        (counting multiplicities)\n        of  .\n       \n      Most proofs of this theorem require some results from topology.\n      For that reason,\n      we will not present a completely rigorous proof but rather give the highlights.\n      Let   be an   matrix.\n      Let   be a collection of Gershgorin disks of   for   such that\n        does not intersect any other Gershgorin disk of  ,\n      and let   be the union of the Gershgorin disks of   that are different from the  .\n      Note that  .\n      Let   be the matrix whose  th column is  ,\n      that is   is the diagonal matrix whose diagonal entries are the corresponding diagonal entries of  .\n      Note that the eigenvalues of   are   and the Gershgorin disks of   are just the points  .\n      So our theorem is true for this matrix  .\n      To prove the result,\n      we build a continuum of matrices from   to   as follows:\n      let  \n      (so that   is the matrix whose off-diagonal entries are those of   and whose diagonal entries are 0),\n      and let   for   in the interval  .\n      Note that  .\n      Since the diagonal entries of   are the same as those of  ,\n      the Gershgorin disks of   have the same centers as the corresponding Gershgorin disks of  ,\n      while the radii of the Gershgorin disks of   are those of   but scaled by  .\n      So the Gershgorin disks of   increase from points\n      (the  )\n      to the Gershgorin disks of   as   increases from 0 to 1.\n      While the centers of the disks all remain fixed,\n      it is important to recognize that the eigenvalues of   move as   changes.\n      An illustration of this is shown in  \n      with the eigenvalues as the black points and the changing Gershgorin disks dashed in magenta,\n      using the matrix  .\n      We can learn about how the eigenvalues move with the characteristic polynomial.\n     How eigenvalues move. \n      Let   be the characteristic polynomial of  .\n      Note that these characteristic polynomials are functions of both   and  .\n      Since polynomials are continuous functions,\n      their roots (the eigenvalues of  ) are continuous for   as well.\n      Let   be an eigenvalue of  .\n      Note that   is an eigenvalue of  ,\n      and   is one of the   and is therefore in  .\n      We will argue that   is in   for every value of   in  .\n      Let   be the radius of   and let   be the Gershgorin disk of   with the same center as   and radius  .\n      Let  .\n      Since  ,\n      it follows that   and so   as well.\n      From topology,\n      we know that since the disks   are closed,\n      the union   of these disks is also closed.\n      Similarly,   and   are closed.\n      Thus,   is continuous in a closed set and so does not leave the set.\n      Thus,   is in   for every value of   in  .\n     "
},
{
  "id": "chap_det_properties",
  "level": "1",
  "url": "chap_det_properties.html",
  "type": "Section",
  "number": "22",
  "title": "Properties of Determinants",
  "body": "Properties of Determinants \n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            How do elementary row operations change the determinant?\n           \n         \n           \n            How can we represent elementary row operations via matrix multiplication?\n           \n         \n           \n            How can we use elementary row operations to calculate the determinant more efficiently?\n           \n         \n           \n            What is the Cramer's rule for the explicit formula for the inverse of a matrix?\n           \n         \n           \n            How can we interpret determinants from a geometric perspective?\n           \n         \n           \n            What is an   factorization of a matrix and why is such a factorization useful?\n           \n         Introduction \n    This section is different than others in that it contains mainly proofs of previously stated results and only a little new material.\n    Consequently, there is no application attached to this section.\n   \n    We have seen that an important property of the determinant is that it provides an easy criteria for the invertibility of a matrix.\n    As a result,\n    we obtained an algebraic method for finding the eigenvalues of a matrix,\n    using the characteristic equation.\n    In this section,\n    we will investigate other properties of the determinant related to how elementary row operations change the determinant.\n    These properties of the determinant will help us evaluate the determinant in a more efficient way compared to using the cofactor expansion method,\n    which is computationally intensive for large   values due to it being a recursive method.\n    Finally, we will derive a geometrical interpretation of the determinant.\n   \n            We first consider how the determinant changes if we multiply a row of the matrix by a constant.\n           \n                  Let  .\n                  Pick a few different values for the constant   and compare the determinant of   and that of  .\n                  What do you conjecture that the effect of multiplying a row by a constant on the determinant is?\n                 \n                  If we want to make sure our conjecture is valid for any   matrix,\n                  we need to show that for  ,\n                  the relationship between   and the determinant of\n                    follows our conjecture.\n                  We should also check that the relationship between   and the determinant of\n                    follows our conjecture.\n                  Verify this.\n                 \n                  Make a similar conjecture for what happens to the determinant when a row of a\n                    matrix   is multiplied by a constant  ,\n                  and explain why your conjecture is true using the cofactor expansion definition of the determinant.\n                 \n            The second type of elementary row operation we consider is row swapping.\n           \n                  Take a general   matrix\n                    and determine how row swapping effects the determinant.\n                 \n                  Now choose a few different\n                    matrices and see how row swapping changes the determinant in these matrices by evaluating the determinant with a calculator or any other appropriate technology.\n                 \n                  Based on your results so far,\n                  conjecture how row swapping changes the determinant in general.\n                 \n            The last type of elementary row operation is adding a multiple of a row to another.\n            Determine the effect of this operation on a\n              matrix by evaluating the determinant of a general\n              matrix after a multiple of one row is added to the other row.\n           elementary matrices \n                     \n                 \n                     \n                 \n                    \n                 Elementary Row Operations and Their Effects on the Determinant \n    In  ,\n    we conjectured how elementary row operations affect the determinant of a matrix.\n    In the following activity,\n    we prove how the determinant changes when a row is multiplied by a constant using the cofactor expansion definition of the determinant.\n   \n      In this activity,\n      assume that the determinant of   can be determined by a cofactor expansion along any row or column.\n      (We will prove this result independently later in this section.)\n      Consider an arbitrary   matrix  .\n     \n            Write the expression for   using the cofactor expansion along the second row.\n           \n            Let   be obtained by multiplying the second row of   by  .\n            Write the expression for   if the cofactor expansion along the second row is used.\n           \n            Use the expressions you found above,\n            to express   in terms of  .\n           \n            Explain how this method generalizes to prove the relationship between the determinant of a matrix   and that of the matrix obtained by multiplying a row by a constant  .\n           \n    Your work in  \n    proves the first part of the following theorem on how elementary row operations change the determinant of a matrix.\n   \n        Let   be a square matrix.\n         \n             \n              If   is obtained by multiplying a row of   by a constant  ,\n              then  .\n             \n           \n             \n              If   is obtained by swapping two rows of  ,\n              then  .\n             \n           \n             \n              If   is obtained by adding a multiple of a row of   to another,\n              then  .\n             \n           \n       \n    In the next section,\n    we will use elementary matrices to prove the last two properties of  .\n   Elementary Matrices elementary matrices matrix elementary elementary matrix \n    The following elementary matrices correspond, respectively,\n    to an elementary row operation which swaps rows 2 and 4;\n    an elementary row operation which multiplies the third row by 5;\n    and an elementary row operation which adds four times the third row to the first row on any   matrix:\n     .\n   \n    To obtain an elementary matrix corresponding an elementary row operation,\n    we simply perform the elementary row operation on the identity matrix.\n    For example,\n      above is obtained by swapping rows 2 and 4 of the identity matrix.\n   \n    With the use of elementary matrices,\n    we can now prove the result about how the determinant is affected by elementary row operations.\n    We first rewrite  \n    in terms of elementary matrices:\n   \n        Let   be an   matrix.\n        If   is an   elementary matrix,\n        then   where\n         \n       Notes on  \n    An elementary matrix   obtained by multiplying a row by   is a diagonal matrix with one   along the diagonal and the rest 1s, so  .\n    Similarly, an elementary matrix   obtained by adding a multiple of a row to another is a triangular matrix with 1s along the diagonal,\n    so  .\n    The fact that the the determinant of an elementary matrix obtained by swapping two rows is   is a bit more complicated and is verified independently later in this section.\n    Also, the proof of  \n    depends on the fact that the cofactor expansion of a matrix is the same along any two rows.\n    A proof of this can also be found later in this section.\n   Proof of  \n      We will prove the result by induction on  ,\n      the size of the matrix  .\n      We verified these results in  \n      for   using elementary row operations.\n      The elementary matrix versions follow immediately.\n     \n      Now assume the theorem is true for\n        matrices with   and consider an\n        matrix   where  .\n      If   is an   elementary matrix,\n      we want to show that  .\n      Let  .\n      (Although it is an abuse of language,\n      we will refer to both the elementary matrix and the elementary row operation corresponding to it by  .)\n     \n      When finding   we will use a cofactor expansion along a row which is not affected by the elementary row operation  .\n      Since   affects at most two rows and   has   rows,\n      it is possible to find such a row, say row  .\n      The cofactor expansion along row   of   is\n       .\n     \n      Since we chose a row of   which was not affected by the elementary row operation,\n      it follows that   for  .\n      Also, the matrix   obtained by removing row   and column   from matrix   can be obtained from   by an elementary row operation of the same type as  .\n      Hence there is an elementary matrix   of the same type as   with  .\n      Therefore, by induction,\n        and\n        is equal to 1, -1 or   depending on the type of elementary row operation.\n      If we substitute this information into equation  , we obtain\n       \n     \n      This equation proves   for any\n        matrix   where   is the corresponding elementary row operation on the\n        matrices obtained in the cofactor expansion.\n     \n      The proof of the inductive step will be finished if we show that  .\n      This equality follows if we let   in  .\n      Therefore,   is equal to  , or 1, or  ,\n      depending on the type of the elementary row operation   since the same is true of\n        by inductive hypothesis.\n     \n      Therefore, by the principle of induction,\n      the claim is true for every  .\n     \n    As a corollary of this theorem,\n    we can prove the multiplicativity of determinants:\n   \n        Let   and   be   matrices.\n        Then\n         .\n       \n      If   is non-invertible,\n      then   is also non-invertible and both   and\n        are 0, proving the equality in this case.\n     \n      Suppose now that   is invertible.\n      By the Invertible Matrix Theorem,\n      we know that   is row equivalent to  .\n      Expressed in terms of elementary matrices,\n      this means that there are elementary matrices   such that\n       .\n     \n      Therefore, repeatedly applying  ,\n      we find that\n       .\n     \n      If we multiply equation   by   on the right,\n      we obtain\n       .\n     \n      Again, by repeatedly applying  \n      with this product of matrices, we find\n       .\n     \n      From equation  ,\n      the product of  's equals  , so\n       \n      which finishes the proof of the theorem.\n     \n    We can use the multiplicative property of the determinant and the determinants of elementary matrices to calculate the determinant of a matrix in a more efficient way than using the cofactor expansion.\n    The next activity provides an example.\n   \n      Let  .\n     \n            Use elementary row operations to reduce   to a row echelon form.\n            Keep track of the elementary row operation you use.\n           \n            Taking into account how elementary row operations affect the determinant,\n            use the row echelon form of   to calculate  .\n           \n    Your work in  \n    provides an efficient method for calculating the determinant.\n    If   is a square matrix,\n    we use row operations given by elementary matrices  ,\n     ,  ,\n      to row reduce   to row echelon form  .\n    That is\n     .\n   \n    We know   for each  ,\n    and since   is a triangular matrix we can find its determinant.\n    Then\n     .\n   \n    In other words,\n    if we keep track of how the row operations affect the determinant,\n    we can calculate the determinant of a matrix   using row operations.\n   \n       \n      and  \n      can be used to prove the following\n      (part c of  )\n      that   is invertible if and only if  .\n      We see how in this activity.\n      Let   be an   matrix.\n      We can row reduce   to its reduced row echelon form   by elementary matrices  ,\n       ,\n       ,   so that\n       .\n     \n            Suppose   is invertible.\n            What, then, is  ?\n            What is  ?\n            Can the determinant of an elementary matrix ever be  ?\n            How do we conclude that  ?\n           \n            Now suppose that  .\n            What can we conclude about  ?\n            What, then, must   be?\n            How do we conclude that   is invertible?\n           Summary \n    Let   be an   matrix.\n    Suppose we swap rows   times and divide rows by constants\n      while computing a row echelon form   of  .\n    Then  .\n   Geometric Interpretation of the Determinant \n    Determinants have interesting and useful applications from a geometric perspective.\n    To understand the geometric interpretation of the determinant of an   matrix  ,\n    we consider the image of the unit square under the transformation\n      and see how its area changes based on  .\n   \n            Let  .\n            Start with the unit square in   with corners at the origin and at  .\n            In other words,\n            the unit square we are considering consists of all vectors   where\n              and  ,\n            visualized as points in the plane.\n           \n                  Consider the collection of image vectors   obtained by multiplying  's by  .\n                  Sketch the rectangle formed by these image vectors.\n                 \n                  Explain how the area of this image rectangle and the unit square is related via  .\n                 \n                  Does the relationship you found above generalize to an arbitrary  ?\n                  If not, modify the relationship to hold for all diagonal matrices.\n                 \n            Let  .\n           \n                  Sketch the image of the unit square under the transformation  .\n                  To make the sketching easier,\n                  find the images of the vectors\n                    as points first and then connect these images to find the image of the unit square.\n                 \n                  Check that the area of the parallelogram you obtained in the above part is equal to  .\n                 \n                  Does the relationship between the area and   still hold if  ?\n                  If not, how will you modify the relationship?\n                 \n    It can be shown that for all\n      matrices a similar relationship holds.\n   \n        For a   matrix  ,\n        the area of the image of the unit square under the transformation\n          is equal to  .\n        This is equivalent to saying that\n          is equal to the area of the parallelogram defined by the columns of  .\n        The area of the parallelogram is also equal to the lengths of the column vectors of   multiplied by\n          where   is the angle between the two column vectors.\n       \n    There is a similar geometric interpretation of the determinant of a\n      matrix in terms of volume.\n   \n        For a   matrix  ,\n        the volume of the image of the unit cube under the transformation\n          is equal to  .\n        This is equivalent to saying that\n          is equal to the volume of the parallelepiped defined by the columns of  .\n       \n    The sign of   can be interpreted in terms of the orientation of the column vectors of  .\n    See the project in   for details.\n   An Explicit Formula for the Inverse and Cramer's Rule \n    In  \n    we found the inverse   using row reduction of the matrix obtained by augmenting   with  .\n    However, in theoretical applications,\n    having an explicit formula for   can be handy.\n    Such an explicit formula provides us with an algebraic expression for   in terms of the entries of  .\n    A consequence of the formula we develop is Cramer's Rule,\n    which can be used to provide formulas that give solutions to certain linear systems.\n   \n    We begin with an interesting connection between a square matrix and the matrix of its cofactors that we explore in the next activity.\n   \n      Let  .\n     \n            Calculate the  ,\n             , and   cofactors of  .\n           matrix adjugate adjugate \n            What do you notice about the product  ?\n            How is this product related to  ?\n           \n    The result of   is rather surprising,\n    but it is valid in general.\n    That is, if   is an invertible\n      matrix and   is the   cofactor of  ,\n    then  .\n    In other words,\n      and so\n     .\n   \n    This gives us another formulation of the inverse of a matrix.\n    To see why  ,\n    we use the row-column version of the matrix product to find the  th entry of\n      as indicated by the shaded row and column\n     \n     \n     \n     .\n   \n    Thus the  th entry of   is\n     .\n   \n    Notice that if  ,\n    then expression   is the cofactor expansion of   along the  th row.\n    So the  th entry of   is  .\n    It remains to show that the  th entry of\n      is 0 when  .\n   \n    When  ,\n    the expression   is the cofactor expansion of the matrix\n     \n     \n     \n     \n    along the  th row.\n    This matrix is the one obtained by replacing the  th row of   with the  th row of  .\n    Since this matrix has two identical rows,\n    it is not row equivalent to the identity matrix and is therefore not invertible.\n    Thus, when   expression   is 0.\n    This makes  .\n   \n    One consequence of the formula   is Cramer's rule,\n    which describes the solution to the equation  .\n   \n      Let  ,\n      and let  .\n     \n            Solve the equation   using the inverse of  .\n           \n            Let  ,\n            the matrix obtained by replacing the first column of   with  .\n            Calculate   and compare to your solution from part (a).\n            What do you notice?\n           \n            Now let  ,\n            the matrix obtained by replacing the second column of   with  .\n            Calculate   and compare to your solution from part (a).\n            What do you notice?\n           \n    The result from   may seem a bit strange,\n    but turns out to be true in general.\n    The result is called  Cramer's Rule .\n   Cramer's Rule \n        Let   be an   invertible matrix.\n        For any   in  ,\n        the solution   of   has entries\n         \n        where   represents the matrix formed by replacing  th column of   with  .\n             Cramer's Rule \n       \n    To see why Cramer's Rule works in general,\n    let   be an   invertible matrix and  .\n    The solution to   is\n     .\n   \n    Expanding the product gives us\n     .\n   \n    The expression\n     \n    is the cofactor expansion of the matrix\n     \n     \n     \n     \n    along the  th column, giving us the formula in Cramer's Rule.\n   \n    Cramer's Rule is not a computationally efficient method.\n    To find a solution to a linear system of   equations in   unknowns using Cramer's Rule requires calculating   determinants of\n      matrices   quite inefficient when   is 3 or greater.\n    Our standard method of solving systems using Gaussian elimination is much more efficient.\n    However, Cramer's Rule does provide a formula for the solution to\n      as long as   is invertible.\n   The Determinant of the Transpose \n    In this section we establish the fact that the determinant of a square matrix is the same as the determinant of its transpose.\n   \n    The result is easily verified for   matrices,\n    so we will proceed by induction and assume that the determinant of the transpose of any\n      matrix is the same as the determinant of its transpose.\n    Suppose   is an   matrix.\n    By definition,\n     \n    and\n     .\n   \n    Note that the only terms in either determinant that contains   is  .\n    This term is the same in both determinants,\n    so we proceed to examine other elements.\n    Let us consider all terms in the cofactor expansion for\n      that contain  .\n    The only summand that contains   is  .\n    Letting   be the sub-matrix of   obtained by deleting the  th row and  th column,\n    we see that  .\n    Now let's examine the sub-matrix  :\n     \n     \n     \n     \n   \n    When we expand along the first row to calculate  ,\n    the only term that will involve   is\n     ,\n    where   denotes the sub-matrix of   obtained by deleting rows   and   and columns   and   from  .\n    So the term that contains   in the cofactor expansion for   is\n     .\n   \n    Now we examine the cofactor expansion for   to find the terms that contain  .\n    The quantity   only appears in the cofactor expansion as\n     .\n   \n    Now let's examine the sub-matrix  :\n     \n     \n     \n     \n   \n    Here is where we use the induction hypothesis.\n    Since   is an   matrix,\n    its determinant can be found with a cofactor expansion down the first column.\n    The only term in this cofactor expansion that will involve   is\n     .\n   \n    So the term that contains   in the cofactor expansion for   is\n     .\n   \n    Since the quantities in   and   are equal,\n    we conclude that the terms in the two cofactor expansions are the same and\n     .\n   Row Swaps and Determinants \n    In this section we determine the effect of row swaps to the determinant.\n    Let   be the elementary matrix that swaps rows   and   in the\n      matrix  .\n    Applying   to a\n      matrix  ,\n    we see that\n     .\n   \n    So swapping rows in a   matrix multiplies the determinant by  .\n    Suppose that row swapping on any\n      matrix multiplies the determinant by  \n    (in other words,\n    we are proving our statement by mathematical induction).\n    Now suppose   is an\n      matrix and let  .\n    We first consider the case that     that we swap adjacent rows.\n    We consider two cases,   and  .\n    First let us suppose that  .\n    Let   be the   cofactor of   and   the   cofactor of  .\n    We have\n     \n    and\n     .\n   \n    Since  ,it follows that   for every  .\n    For each   the sub-matrix   obtained from   by deleting the  th row and  th column is the same matrix as obtained from   by swapping rows   and  .\n    So by our induction hypothesis,\n    we have   for each  .\n    Then\n     .\n   \n    Now we consider the case where  ,\n    where   is the matrix obtained from   by swapping the first and second rows.\n    Here we will use the fact that\n      which allows us to calculate   and   with the cofactor expansions down the first column.\n    In this case we have\n     \n    and\n     .\n   \n    For each  ,\n    the sub-matrix   is just   with rows 1 and 2 swapped.\n    So we have   by our induction hypothesis.\n    Since we swapped rows 1 and 2, we have\n      and  .\n    Thus,\n     \n    and\n     .\n   \n    Putting this all together gives us\n     .\n   \n    So we have shown that if   is obtained from   by interchanging two adjacent rows,\n    then  .\n    Now we consider the general case.\n    Suppose   is obtained from   by interchanging rows   and  ,\n    with  .\n    We can perform this single row interchange through a sequence of adjacent row interchanges.\n    First we swap rows   and  ,\n    then rows   and  ,\n    and continue until we swap rows   and  .\n    This places the original row   into the row   position,\n    and the process involved   adjacent row interchanges.\n    Each of these interchanges multiplies the determinant by a factor of  .\n    At the end of this sequence of row swaps,\n    the original row   is now row  .\n    So it will take one fewer adjacent row interchanges to move this row to be row  .\n    This sequence of   row interchanges produces the matrix  .\n    Thus,\n     ,\n    and interchanging any two rows multiplies the determinant by  .\n   Cofactor Expansions \n    We have stated that the determinant of a matrix can be calculated by using a cofactor expansion along any row or column.\n    We use the result that swapping rows introduces a factor of   in the determinant to verify that result in this section.\n    Note that in proving that  ,\n    we have already shown that the cofactor expansion along the first column is the same as the cofactor expansion along the first row.\n    If we can prove that the cofactor expansion along any row is the same,\n    then the fact that   will imply that the cofactor expansion along any column is the same as well.\n   \n    Now we demonstrate that the cofactor expansions along the first row and the  th row are the same.\n    Let   be an   matrix.\n    The cofactor expansion of   along the first row is\n     \n    and the cofactor expansion along the  th row is\n     .\n   \n    Let   be the matrix obtained by swapping row   with previous rows so that row   becomes the first row and the order of the remaining rows is preserved.\n     \n     \n     \n     \n   \n    Then\n     .\n   \n    So, letting   be the   cofactor of   we have\n     .\n   \n    Notice that for each   we have  .\n    So\n     .\n   The LU Factorization of a Matrix \n    There are many instances where we have a number of systems to solve of the form  ,\n    all with the same coefficient matrix.\n    The system may evolve over time so that we do not know the constant vectors   in the system all at once,\n    but only determine them as time progresses.\n    Each time we obtain a new vector  ,\n    we have to apply the same row operations to reduce the coefficient matrix to solve the new system.\n    This is time repetitive and time consuming.\n    Instead, we can keep track of the row operations in one row reduction and save ourselves a significant amount of time.\n    One way of doing this is the  -factorization\n    (or decomposition).\n   \n    To illustrate,\n    suppose we can write the matrix   as a product  , where\n     .\n   \n    Let   and  ,\n    and consider the linear system  .\n    If  , then  .\n    We can solve this system without applying row operations as follows.\n    Let  , where  .\n    We can solve   by using forward substitution.\n   \n    The equation   is equivalent to the system\n     .\n   \n    The first equation shows that  .\n    Substituting into the second equation gives us  .\n    Using this information in the third equation yields  ,\n    and then the fourth equation shows that  .\n    To return to the original system, since  ,\n    we now solve this system to find the solution vector  .\n    In this case,\n    since   is upper triangular, we use back substitution.\n    The equation   is equivalent to the system\n     .\n   -factorization -decomposition \n    We can use elementary matrices to obtain a factorization of certain matrices into products of lower triangular (the\n     L \n    in LU) and upper triangular (the\n     U \n    in LU) matrices.\n    We illustrate with an example.\n    Let\n     .\n   \n    Our goal is to find an upper triangular matrix   and a lower triangular matrix   so that  .\n    We begin by row reducing   to an upper triangular matrix,\n    keeping track of the elementary matrices used to perform the row operations.\n    We start by replacing the entries below the   entry in   with zeros.\n    The elementary matrices that perform these operations are\n     ,\n    and\n     .\n   \n    We next zero out the entries below the   entry as\n     ,\n    where\n     .\n   \n    The product   is an upper triangular matrix  .\n    So we have\n     \n    and\n     ,\n    where\n     \n    is a lower triangular matrix  .\n    So we have decomposed the matrix   into a product  ,\n    where   is lower triangular and   is upper triangular.\n    Since every matrix is row equivalent to a matrix in row echelon form,\n    we can always find an upper triangular matrix   in this way.\n    However, we may not always obtain a corresponding lower triangular matrix,\n    as the next example illustrates.\n   \n    Suppose we change the problem slightly and consider the matrix\n     .\n   \n    Using the same elementary matrices  ,  ,\n    and   as earlier, we have\n     .\n   \n    To reduce   to row-echelon form now requires a row interchange.\n    Letting\n     \n    brings us to\n     .\n   \n    So in this case we have  , but\n     \n    is not lower triangular.\n    The difference in this latter example is that we needed a row swap to obtain the upper triangular form.\n   Examples \n    What follows are worked examples that use the concepts from this section.\n   \n              If  ,   are   matrices with\n                and  ,\n              evaluate the following determinant values.\n              Briefly justify.\n             \n                     \n                   \n                    Assume that   and  .\n                   \n                    Since  , we know that   is invertible.\n                    Since  ,\n                    it follows that  .\n                   \n                     \n                   \n                    Assume that   and  .\n                   \n                    We know that  , so\n                     .\n                   \n                     \n                   \n                    Assume that   and  .\n                   \n                    Using properties of determinants gives us\n                     .\n                   \n              If the determinant of   is  ,\n              find the determinant of each of the following matrices.\n             \n                     \n                   \n                    Assume that  .\n                   \n                    Multiplying a row by a scalar multiples the determinant by that scalar, so\n                     .\n                   \n                     \n                   \n                    Assume that  .\n                   \n                    Interchanging two rows multiples the determinant by  .\n                    It takes two row swaps in the original matrix to obtain this one, so\n                     .\n                   \n                     \n                   \n                    Assume that  .\n                   \n                    Adding a multiple of a row to another does not change the determinant of the matrix.\n                    Since there is a row swap needed to get this matrix from the original we have\n                     .\n                   \n        Let  .\n       \n              Find an LU factorization for  .\n             \n              We row reduce   to an upper triangular matrix by applying elementary matrices.\n              First notice that if  , then\n               .\n              Letting   gives us\n               .\n              Finally, when   we have\n               .\n              This gives us  , so we can take\n               .\n             \n              Use the LU factorization with forward substitution and back substitution to solve the system  .\n             \n              To solve the system  ,\n              where  ,\n              we use the LU factorization of   and solve  .\n              Let   and let   with\n                so that  .\n              First we solve   to find   using forward substitution.\n              The first row of   shows that\n                and the second row that  .\n              So  .\n              The third row of   gives us  ,\n              so  .\n              Now to find   we solve\n                using back substitution.\n              The third row of   tells us that\n                or that  .\n              The second row of   shows that   or  .\n              Finally, the first row of   gives us  ,\n              or  .\n              So the solution to   is  .\n             Summary \n       \n        The elementary row operations have the following effects on the determinant:\n         \n             \n              If we multiply a row of a matrix by a constant  ,\n              then the determinant is multiplied by  .\n             \n           \n             \n              If we swap two rows of a matrix,\n              then the determinant changes sign.\n             \n           \n             \n              If we add a multiple of a row of a matrix to another,\n              the determinant does not change.\n             \n           \n       \n     \n       \n        Each of the elementary row operations can be achieved by multiplication by elementary matrices.\n        To obtain the elementary matrix corresponding to an elementary row operation,\n        we perform the operation on the identity matrix.\n       \n     \n       \n        Let   be an   invertible matrix.\n        For any   in  ,\n        the solution   of   has entries\n         \n        where   represents the matrix formed by replacing  th column of   with  .\n       \n     \n       adjugate of  \n     \n       \n        For a   matrix  ,\n        the area of the image of the unit square under the transformation\n          is equal to  ,\n        which is also equal to the area of the parallelogram defined by the columns of  .\n       \n     \n       \n        For a   matrix  ,\n        the volume of the image of the unit cube under the transformation\n          is equal to  ,\n        which is also equal to the volume of the parallelepiped defined by the columns of  .\n       \n     \n       \n        An   factorization of a square matrix   consists of a lower triangular matrix   and an upper triangular matrix   so that  .\n       \n     \n       \n        A square matrix   has an   factorization if we can use row operations without row interchanges to row reduce   to an upper triangular matrix  .\n        In this situation the elementary matrices that perform the row operations produce a lower triangular matrix   so that  .\n        If   cannot be reduced to an upper triangular matrix   without row interchanges,\n        then we can factor   in the form  ,\n        where   is a lower triangular matrix,\n          is an upper triangular matrix,\n        and   is obtained from the identity matrix by appropriate row interchanges.\n       \n     \n       \n        There are many instances where we have a number of systems to solve of the form  ,\n        all with the same coefficient matrix but where the vectors   can change.\n        With an   factorization,\n        we can keep track of the row operations in one row reduction and save ourselves a significant amount of time when solving these systems.\n       \n     \n        Find a formula for   in terms of   and  ,\n        where   is an   matrix and   is a scalar.\n        Explain why your formula is valid.\n       \n         \n       \n        Find   by hand using elementary row operations where\n         .\n       \n        Consider the matrix  .\n        We will find   using elementary row operations.\n        (This matrix arises in graph theory,\n        and its determinant gives the number of spanning trees in the complete graph with 5 vertices.\n        This number is also equal to the number of labeled trees with 5 vertices.)\n       \n              Add rows  ,\n                and   to the first row in that order.\n             \n               \n             \n              Then add the new   to rows  ,\n                and   to get a triangular matrix  .\n             \n               \n             \n              Find the determinant of  .\n              Then use   and properties of how elementary row operations affect determinants to find  .\n             \n               \n             \n              Generalize your work to find the determinant of the   matrix\n               .\n             \n               \n             \n        For which matrices  , if any,\n        is  ?\n        Justify your answer.\n       \n        Find the inverse   of\n          using the adjugate matrix.\n       \n         \n       \n        For an invertible   matrix  ,\n        what is the relationship between   and  ?\n        Justify your result.\n       \n        Let  ,\n        and assume that  .\n        Determine the determinants of each of the following.\n       \n               \n             \n              6\n             \n               \n             \n               \n             \n        Find the area of the parallelogram with one vertex at the origin and adjacent vertices at   and  .\n        For which   is the area 0?\n        When does this happen geometrically?\n       \n        Find the volume of the parallelepiped with one vertex at the origin and three adjacent vertices at  ,\n          and   where   is unknown.\n        For which  , is the volume 0?\n        When does this happen geometrically?\n       \n         ,\n        volume is   when the parallelelepiped is two-dimensional.\n       \n        Find an   factorization of each of the following matrices  .\n        Use the   factorization to solve the system\n          for the given vector  .\n       \n               ,\n               \n             \n               ,\n               \n             \n               ,\n               \n             \n        Let  .\n       \n              Find an   decomposition of  .\n             \n              Find a different factorization of   into a product   where   is a lower triangular matrix different from   and   is an upper triangular matrix different from  .\n              Conclude that the   decomposition of a matrix is not unique.\n             \n        Let  .\n       \n              Find an   decomposition of  .\n             \n              Find an   decomposition of   in which the diagonal entries of   are all 1.\n              \n             \n              Continue row reducing.\n             \n              Find an upper triangular matrix   whose diagonal entries are all 1, a lower triangular matrix   whose diagonal entries are all 1, and a diagonal matrix   such that  .\n             \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If two rows are equal in  ,\n              then  .\n             \n              T\n             True\/False \n              If   is a square matrix and   is a row echelon form of  ,\n              then  .\n             True\/False \n              If a matrix   is invertible,\n              then 0 is not an eigenvalue of  .\n             \n              T\n             True\/False \n              If   is a\n                matrix for which the image of the unit square under the transformation   has zero area,\n              then   is non-invertible.\n             True\/False \n              Row operations do not change the determinant of a square matrix.\n             \n              F\n             True\/False \n              If   is the matrix obtained from a square matrix\n                by deleting the  th row and   column of  , then\n               \n              for any   and   between   and  .\n             True\/False \n              If   is an invertible matrix,\n              then  .\n             \n              T\n             "
},
{
  "id": "objectives-22",
  "level": "2",
  "url": "chap_det_properties.html#objectives-22",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            How do elementary row operations change the determinant?\n           \n         \n           \n            How can we represent elementary row operations via matrix multiplication?\n           \n         \n           \n            How can we use elementary row operations to calculate the determinant more efficiently?\n           \n         \n           \n            What is the Cramer's rule for the explicit formula for the inverse of a matrix?\n           \n         \n           \n            How can we interpret determinants from a geometric perspective?\n           \n         \n           \n            What is an   factorization of a matrix and why is such a factorization useful?\n           \n         "
},
{
  "id": "pa_4_f",
  "level": "2",
  "url": "chap_det_properties.html#pa_4_f",
  "type": "Preview Activity",
  "number": "22.1",
  "title": "",
  "body": "\n            We first consider how the determinant changes if we multiply a row of the matrix by a constant.\n           \n                  Let  .\n                  Pick a few different values for the constant   and compare the determinant of   and that of  .\n                  What do you conjecture that the effect of multiplying a row by a constant on the determinant is?\n                 \n                  If we want to make sure our conjecture is valid for any   matrix,\n                  we need to show that for  ,\n                  the relationship between   and the determinant of\n                    follows our conjecture.\n                  We should also check that the relationship between   and the determinant of\n                    follows our conjecture.\n                  Verify this.\n                 \n                  Make a similar conjecture for what happens to the determinant when a row of a\n                    matrix   is multiplied by a constant  ,\n                  and explain why your conjecture is true using the cofactor expansion definition of the determinant.\n                 \n            The second type of elementary row operation we consider is row swapping.\n           \n                  Take a general   matrix\n                    and determine how row swapping effects the determinant.\n                 \n                  Now choose a few different\n                    matrices and see how row swapping changes the determinant in these matrices by evaluating the determinant with a calculator or any other appropriate technology.\n                 \n                  Based on your results so far,\n                  conjecture how row swapping changes the determinant in general.\n                 \n            The last type of elementary row operation is adding a multiple of a row to another.\n            Determine the effect of this operation on a\n              matrix by evaluating the determinant of a general\n              matrix after a multiple of one row is added to the other row.\n           elementary matrices \n                     \n                 \n                     \n                 \n                    \n                 "
},
{
  "id": "act_4_f_1",
  "level": "2",
  "url": "chap_det_properties.html#act_4_f_1",
  "type": "Activity",
  "number": "22.2",
  "title": "",
  "body": "\n      In this activity,\n      assume that the determinant of   can be determined by a cofactor expansion along any row or column.\n      (We will prove this result independently later in this section.)\n      Consider an arbitrary   matrix  .\n     \n            Write the expression for   using the cofactor expansion along the second row.\n           \n            Let   be obtained by multiplying the second row of   by  .\n            Write the expression for   if the cofactor expansion along the second row is used.\n           \n            Use the expressions you found above,\n            to express   in terms of  .\n           \n            Explain how this method generalizes to prove the relationship between the determinant of a matrix   and that of the matrix obtained by multiplying a row by a constant  .\n           "
},
{
  "id": "thm_4_f_1",
  "level": "2",
  "url": "chap_det_properties.html#thm_4_f_1",
  "type": "Theorem",
  "number": "22.1",
  "title": "",
  "body": "\n        Let   be a square matrix.\n         \n             \n              If   is obtained by multiplying a row of   by a constant  ,\n              then  .\n             \n           \n             \n              If   is obtained by swapping two rows of  ,\n              then  .\n             \n           \n             \n              If   is obtained by adding a multiple of a row of   to another,\n              then  .\n             \n           \n       "
},
{
  "id": "p-3677",
  "level": "2",
  "url": "chap_det_properties.html#p-3677",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "elementary matrices "
},
{
  "id": "definition-46",
  "level": "2",
  "url": "chap_det_properties.html#definition-46",
  "type": "Definition",
  "number": "22.2",
  "title": "",
  "body": "matrix elementary elementary matrix "
},
{
  "id": "thm_4_f_2",
  "level": "2",
  "url": "chap_det_properties.html#thm_4_f_2",
  "type": "Theorem",
  "number": "22.3",
  "title": "",
  "body": "\n        Let   be an   matrix.\n        If   is an   elementary matrix,\n        then   where\n         \n       "
},
{
  "id": "proof-9",
  "level": "2",
  "url": "chap_det_properties.html#proof-9",
  "type": "Proof",
  "number": "1",
  "title": "Proof of Theorem22.3.",
  "body": "Proof of  \n      We will prove the result by induction on  ,\n      the size of the matrix  .\n      We verified these results in  \n      for   using elementary row operations.\n      The elementary matrix versions follow immediately.\n     \n      Now assume the theorem is true for\n        matrices with   and consider an\n        matrix   where  .\n      If   is an   elementary matrix,\n      we want to show that  .\n      Let  .\n      (Although it is an abuse of language,\n      we will refer to both the elementary matrix and the elementary row operation corresponding to it by  .)\n     \n      When finding   we will use a cofactor expansion along a row which is not affected by the elementary row operation  .\n      Since   affects at most two rows and   has   rows,\n      it is possible to find such a row, say row  .\n      The cofactor expansion along row   of   is\n       .\n     \n      Since we chose a row of   which was not affected by the elementary row operation,\n      it follows that   for  .\n      Also, the matrix   obtained by removing row   and column   from matrix   can be obtained from   by an elementary row operation of the same type as  .\n      Hence there is an elementary matrix   of the same type as   with  .\n      Therefore, by induction,\n        and\n        is equal to 1, -1 or   depending on the type of elementary row operation.\n      If we substitute this information into equation  , we obtain\n       \n     \n      This equation proves   for any\n        matrix   where   is the corresponding elementary row operation on the\n        matrices obtained in the cofactor expansion.\n     \n      The proof of the inductive step will be finished if we show that  .\n      This equality follows if we let   in  .\n      Therefore,   is equal to  , or 1, or  ,\n      depending on the type of the elementary row operation   since the same is true of\n        by inductive hypothesis.\n     \n      Therefore, by the principle of induction,\n      the claim is true for every  .\n     "
},
{
  "id": "thm_determinant_product",
  "level": "2",
  "url": "chap_det_properties.html#thm_determinant_product",
  "type": "Theorem",
  "number": "22.4",
  "title": "",
  "body": "\n        Let   and   be   matrices.\n        Then\n         .\n       \n      If   is non-invertible,\n      then   is also non-invertible and both   and\n        are 0, proving the equality in this case.\n     \n      Suppose now that   is invertible.\n      By the Invertible Matrix Theorem,\n      we know that   is row equivalent to  .\n      Expressed in terms of elementary matrices,\n      this means that there are elementary matrices   such that\n       .\n     \n      Therefore, repeatedly applying  ,\n      we find that\n       .\n     \n      If we multiply equation   by   on the right,\n      we obtain\n       .\n     \n      Again, by repeatedly applying  \n      with this product of matrices, we find\n       .\n     \n      From equation  ,\n      the product of  's equals  , so\n       \n      which finishes the proof of the theorem.\n     "
},
{
  "id": "act_4_f_2",
  "level": "2",
  "url": "chap_det_properties.html#act_4_f_2",
  "type": "Activity",
  "number": "22.3",
  "title": "",
  "body": "\n      Let  .\n     \n            Use elementary row operations to reduce   to a row echelon form.\n            Keep track of the elementary row operation you use.\n           \n            Taking into account how elementary row operations affect the determinant,\n            use the row echelon form of   to calculate  .\n           "
},
{
  "id": "act_4_f_2_b",
  "level": "2",
  "url": "chap_det_properties.html#act_4_f_2_b",
  "type": "Activity",
  "number": "22.4",
  "title": "",
  "body": "\n       \n      and  \n      can be used to prove the following\n      (part c of  )\n      that   is invertible if and only if  .\n      We see how in this activity.\n      Let   be an   matrix.\n      We can row reduce   to its reduced row echelon form   by elementary matrices  ,\n       ,\n       ,   so that\n       .\n     \n            Suppose   is invertible.\n            What, then, is  ?\n            What is  ?\n            Can the determinant of an elementary matrix ever be  ?\n            How do we conclude that  ?\n           \n            Now suppose that  .\n            What can we conclude about  ?\n            What, then, must   be?\n            How do we conclude that   is invertible?\n           "
},
{
  "id": "act_4_f_3",
  "level": "2",
  "url": "chap_det_properties.html#act_4_f_3",
  "type": "Activity",
  "number": "22.5",
  "title": "",
  "body": "\n            Let  .\n            Start with the unit square in   with corners at the origin and at  .\n            In other words,\n            the unit square we are considering consists of all vectors   where\n              and  ,\n            visualized as points in the plane.\n           \n                  Consider the collection of image vectors   obtained by multiplying  's by  .\n                  Sketch the rectangle formed by these image vectors.\n                 \n                  Explain how the area of this image rectangle and the unit square is related via  .\n                 \n                  Does the relationship you found above generalize to an arbitrary  ?\n                  If not, modify the relationship to hold for all diagonal matrices.\n                 \n            Let  .\n           \n                  Sketch the image of the unit square under the transformation  .\n                  To make the sketching easier,\n                  find the images of the vectors\n                    as points first and then connect these images to find the image of the unit square.\n                 \n                  Check that the area of the parallelogram you obtained in the above part is equal to  .\n                 \n                  Does the relationship between the area and   still hold if  ?\n                  If not, how will you modify the relationship?\n                 "
},
{
  "id": "theorem-51",
  "level": "2",
  "url": "chap_det_properties.html#theorem-51",
  "type": "Theorem",
  "number": "22.5",
  "title": "",
  "body": "\n        For a   matrix  ,\n        the area of the image of the unit square under the transformation\n          is equal to  .\n        This is equivalent to saying that\n          is equal to the area of the parallelogram defined by the columns of  .\n        The area of the parallelogram is also equal to the lengths of the column vectors of   multiplied by\n          where   is the angle between the two column vectors.\n       "
},
{
  "id": "theorem-52",
  "level": "2",
  "url": "chap_det_properties.html#theorem-52",
  "type": "Theorem",
  "number": "22.6",
  "title": "",
  "body": "\n        For a   matrix  ,\n        the volume of the image of the unit cube under the transformation\n          is equal to  .\n        This is equivalent to saying that\n          is equal to the volume of the parallelepiped defined by the columns of  .\n       "
},
{
  "id": "act_4_f_4",
  "level": "2",
  "url": "chap_det_properties.html#act_4_f_4",
  "type": "Activity",
  "number": "22.6",
  "title": "",
  "body": "\n      Let  .\n     \n            Calculate the  ,\n             , and   cofactors of  .\n           matrix adjugate adjugate \n            What do you notice about the product  ?\n            How is this product related to  ?\n           "
},
{
  "id": "act_4_f_5",
  "level": "2",
  "url": "chap_det_properties.html#act_4_f_5",
  "type": "Activity",
  "number": "22.7",
  "title": "",
  "body": "\n      Let  ,\n      and let  .\n     \n            Solve the equation   using the inverse of  .\n           \n            Let  ,\n            the matrix obtained by replacing the first column of   with  .\n            Calculate   and compare to your solution from part (a).\n            What do you notice?\n           \n            Now let  ,\n            the matrix obtained by replacing the second column of   with  .\n            Calculate   and compare to your solution from part (a).\n            What do you notice?\n           "
},
{
  "id": "theorem-53",
  "level": "2",
  "url": "chap_det_properties.html#theorem-53",
  "type": "Theorem",
  "number": "22.7",
  "title": "Cramer's Rule.",
  "body": "Cramer's Rule \n        Let   be an   invertible matrix.\n        For any   in  ,\n        the solution   of   has entries\n         \n        where   represents the matrix formed by replacing  th column of   with  .\n             Cramer's Rule \n       "
},
{
  "id": "p-3773",
  "level": "2",
  "url": "chap_det_properties.html#p-3773",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "-factorization -decomposition "
},
{
  "id": "example-44",
  "level": "2",
  "url": "chap_det_properties.html#example-44",
  "type": "Example",
  "number": "22.8",
  "title": "",
  "body": "\n              If  ,   are   matrices with\n                and  ,\n              evaluate the following determinant values.\n              Briefly justify.\n             \n                     \n                   \n                    Assume that   and  .\n                   \n                    Since  , we know that   is invertible.\n                    Since  ,\n                    it follows that  .\n                   \n                     \n                   \n                    Assume that   and  .\n                   \n                    We know that  , so\n                     .\n                   \n                     \n                   \n                    Assume that   and  .\n                   \n                    Using properties of determinants gives us\n                     .\n                   \n              If the determinant of   is  ,\n              find the determinant of each of the following matrices.\n             \n                     \n                   \n                    Assume that  .\n                   \n                    Multiplying a row by a scalar multiples the determinant by that scalar, so\n                     .\n                   \n                     \n                   \n                    Assume that  .\n                   \n                    Interchanging two rows multiples the determinant by  .\n                    It takes two row swaps in the original matrix to obtain this one, so\n                     .\n                   \n                     \n                   \n                    Assume that  .\n                   \n                    Adding a multiple of a row to another does not change the determinant of the matrix.\n                    Since there is a row swap needed to get this matrix from the original we have\n                     .\n                   "
},
{
  "id": "example-45",
  "level": "2",
  "url": "chap_det_properties.html#example-45",
  "type": "Example",
  "number": "22.9",
  "title": "",
  "body": "\n        Let  .\n       \n              Find an LU factorization for  .\n             \n              We row reduce   to an upper triangular matrix by applying elementary matrices.\n              First notice that if  , then\n               .\n              Letting   gives us\n               .\n              Finally, when   we have\n               .\n              This gives us  , so we can take\n               .\n             \n              Use the LU factorization with forward substitution and back substitution to solve the system  .\n             \n              To solve the system  ,\n              where  ,\n              we use the LU factorization of   and solve  .\n              Let   and let   with\n                so that  .\n              First we solve   to find   using forward substitution.\n              The first row of   shows that\n                and the second row that  .\n              So  .\n              The third row of   gives us  ,\n              so  .\n              Now to find   we solve\n                using back substitution.\n              The third row of   tells us that\n                or that  .\n              The second row of   shows that   or  .\n              Finally, the first row of   gives us  ,\n              or  .\n              So the solution to   is  .\n             "
},
{
  "id": "p-3814",
  "level": "2",
  "url": "chap_det_properties.html#p-3814",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "adjugate of  "
},
{
  "id": "ex_4_f_det_multiple",
  "level": "2",
  "url": "chap_det_properties.html#ex_4_f_det_multiple",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Find a formula for   in terms of   and  ,\n        where   is an   matrix and   is a scalar.\n        Explain why your formula is valid.\n       \n         \n       "
},
{
  "id": "exercise-209",
  "level": "2",
  "url": "chap_det_properties.html#exercise-209",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Find   by hand using elementary row operations where\n         .\n       "
},
{
  "id": "exercise-210",
  "level": "2",
  "url": "chap_det_properties.html#exercise-210",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Consider the matrix  .\n        We will find   using elementary row operations.\n        (This matrix arises in graph theory,\n        and its determinant gives the number of spanning trees in the complete graph with 5 vertices.\n        This number is also equal to the number of labeled trees with 5 vertices.)\n       \n              Add rows  ,\n                and   to the first row in that order.\n             \n               \n             \n              Then add the new   to rows  ,\n                and   to get a triangular matrix  .\n             \n               \n             \n              Find the determinant of  .\n              Then use   and properties of how elementary row operations affect determinants to find  .\n             \n               \n             \n              Generalize your work to find the determinant of the   matrix\n               .\n             \n               \n             "
},
{
  "id": "exercise-211",
  "level": "2",
  "url": "chap_det_properties.html#exercise-211",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        For which matrices  , if any,\n        is  ?\n        Justify your answer.\n       "
},
{
  "id": "exercise-212",
  "level": "2",
  "url": "chap_det_properties.html#exercise-212",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Find the inverse   of\n          using the adjugate matrix.\n       \n         \n       "
},
{
  "id": "exercise-213",
  "level": "2",
  "url": "chap_det_properties.html#exercise-213",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        For an invertible   matrix  ,\n        what is the relationship between   and  ?\n        Justify your result.\n       "
},
{
  "id": "exercise-214",
  "level": "2",
  "url": "chap_det_properties.html#exercise-214",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        Let  ,\n        and assume that  .\n        Determine the determinants of each of the following.\n       \n               \n             \n              6\n             \n               \n             \n               \n             "
},
{
  "id": "exercise-215",
  "level": "2",
  "url": "chap_det_properties.html#exercise-215",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Find the area of the parallelogram with one vertex at the origin and adjacent vertices at   and  .\n        For which   is the area 0?\n        When does this happen geometrically?\n       "
},
{
  "id": "exercise-216",
  "level": "2",
  "url": "chap_det_properties.html#exercise-216",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        Find the volume of the parallelepiped with one vertex at the origin and three adjacent vertices at  ,\n          and   where   is unknown.\n        For which  , is the volume 0?\n        When does this happen geometrically?\n       \n         ,\n        volume is   when the parallelelepiped is two-dimensional.\n       "
},
{
  "id": "exercise-217",
  "level": "2",
  "url": "chap_det_properties.html#exercise-217",
  "type": "Exercise",
  "number": "10",
  "title": "",
  "body": "\n        Find an   factorization of each of the following matrices  .\n        Use the   factorization to solve the system\n          for the given vector  .\n       \n               ,\n               \n             \n               ,\n               \n             \n               ,\n               \n             "
},
{
  "id": "exercise-218",
  "level": "2",
  "url": "chap_det_properties.html#exercise-218",
  "type": "Exercise",
  "number": "11",
  "title": "",
  "body": "\n        Let  .\n       \n              Find an   decomposition of  .\n             \n              Find a different factorization of   into a product   where   is a lower triangular matrix different from   and   is an upper triangular matrix different from  .\n              Conclude that the   decomposition of a matrix is not unique.\n             "
},
{
  "id": "exercise-219",
  "level": "2",
  "url": "chap_det_properties.html#exercise-219",
  "type": "Exercise",
  "number": "12",
  "title": "",
  "body": "\n        Let  .\n       \n              Find an   decomposition of  .\n             \n              Find an   decomposition of   in which the diagonal entries of   are all 1.\n              \n             \n              Continue row reducing.\n             \n              Find an upper triangular matrix   whose diagonal entries are all 1, a lower triangular matrix   whose diagonal entries are all 1, and a diagonal matrix   such that  .\n             "
},
{
  "id": "exercise-220",
  "level": "2",
  "url": "chap_det_properties.html#exercise-220",
  "type": "Exercise",
  "number": "13",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If two rows are equal in  ,\n              then  .\n             \n              T\n             True\/False \n              If   is a square matrix and   is a row echelon form of  ,\n              then  .\n             True\/False \n              If a matrix   is invertible,\n              then 0 is not an eigenvalue of  .\n             \n              T\n             True\/False \n              If   is a\n                matrix for which the image of the unit square under the transformation   has zero area,\n              then   is non-invertible.\n             True\/False \n              Row operations do not change the determinant of a square matrix.\n             \n              F\n             True\/False \n              If   is the matrix obtained from a square matrix\n                by deleting the  th row and   column of  , then\n               \n              for any   and   between   and  .\n             True\/False \n              If   is an invertible matrix,\n              then  .\n             \n              T\n             "
},
{
  "id": "chap_dot_product",
  "level": "1",
  "url": "chap_dot_product.html",
  "type": "Section",
  "number": "23",
  "title": "The Dot Product in <span class=\"process-math\">\\(\\R^n\\)<\/span>",
  "body": "The Dot Product in  \n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is the dot product of two vectors?\n            Under what conditions is the dot product defined?\n           \n         \n           \n            How do we find the angle between two nonzero vectors in  ?\n           \n         \n           \n            How does the dot product tell us if two vectors are orthogonal?\n           \n         \n           \n            How do we define the length of a vector in any dimension and how can the dot product be used to calculate the length?\n           \n         \n           \n            How do we define the distance between two vectors?\n           \n         \n           \n            What is the orthogonal projection of a vector   in the direction of the vector   and how do we find it?\n           \n         \n           \n            What is the orthogonal complement of a subspace   of  ?\n           \n         Application: Hidden Figures in Computer Graphics \n    In video games,\n    the speed at which a computer can render changing graphics views is vitally important.\n    To increase a computer's ability to render a scene,\n    programs often try to identify those parts of the images a viewer could see and those parts the viewer could not see.\n    For example, in a scene involving buildings,\n    the viewer could not see any images blocked by a solid building.\n    In the mathematical world, this can be visualized by graphing surfaces.\n    In  \n    we see a crude image of a house made up of small polygons\n    (this is how programs generally represent surfaces).\n    On the left in  \n    we see all of the polygons that are needed to construct the entire surface,\n    even those polygons that lie behind others which we could not see if the surface was solid.\n    On the right in  \n    we have hidden the parts of the polygons that we cannot see from our view.\n   Images of a house. \n    We also see this idea in mathematics when we graph surfaces.\n     \n    shows the graph of the surface defined by\n      that is made up of polygons.\n    At left we see all of the polygons and at right only those parts that would be visible from our viewing perspective.\n   Graphs of  . \n    By eliminating the parts of the polygons we cannot see from our viewing perspective,\n    the computer program can more quickly render the viewing image.\n    Later in this section we will explore one method for how programs remove the hidden portions of images.\n    This process involves the dot product of vectors.\n   Introduction inner products \n    We will illustrate the dot product in  ,\n    but the process we go through will translate to any dimension.\n    Recall that we can represent the vector\n      as the directed line segment\n    (or arrow)\n    from the origin to the point   in  ,\n    as illustrated in  .\n    Using the Pythagorean Theorem we can then define the length\n    (or magnitude or norm)\n    of the vector   in   as\n     .\n   \n    We can also write this norm as\n     .\n   \n    The expression under the square root is an important one and we extend it and give it a special name.\n   A vector in   from the origin to a point. dot product \n    The definition of the dot product translates naturally to  \n    (see  \n    in  ).\n   dot product scalar product dot product scalar product \n    The dot product then allows us to define the norm\n    (or magnitude or length)\n    of any vector in  .\n   vector norm in  norm magnitude length \n    We can also realize the dot product as a matrix product.\n    If   and  , then\n     Technically,   is a   matrix and not a scalar, but we usually think of   matrices as scalars. . \n   IMPORTANT NOTE \n    The dot product is only defined between two vectors with the\n     same number of components .\n   \n            Find   if\n              and   in  .\n           \n            The dot product satisfies some useful properties as given in the next theorem.\n           commutative distributes over vector addition \n            Verification of some of these properties is left to the exercises.\n            Let   and   be vectors in   with  ,\n              and  .\n           \n                  Use property (c) of  \n                  to determine the value of  .\n                 \n                  Use property (b) of  \n                  to determine the value of  .\n                 \n                  Use whatever properties of  \n                  that are needed to determine the value of  .\n                 \n            At times we will want to find vectors in the direction of a given vector that have a certain magnitude.\n            Let   in  .\n           \n                  What is  ?\n                 \n                  Show that  .\n                 unit vector \n                  We can use unit vectors to find vectors of a given length in the direction of a given vector.\n                  Let   be a positive scalar and   a vector in  .\n                  Use properties from  \n                  to show that the magnitude of the vector   is  .\n                 The Distance Between Vectors \n    Finding optimal solutions to systems is an important problem in applied mathematics.\n    It is often the case that we cannot find an exact solution that satisfies certain constraints,\n    so we look instead for the\n     best \n    solution that satisfies the constraints.\n    An example of this is fitting a least squares line to a set of data.\n    As we will see, the dot product will allow us to find\n     best \n    solutions to certain types of problems,\n    where we measure accuracy using the notion of a distance between vectors.\n    Geometrically,\n    we can represent a vector   as a directed line segment from the origin to the point defined by  .\n    If we have two vectors   and  ,\n    we can think of the length of the difference\n      as a measure of how far apart the two vectors are from each other.\n    It is natural,\n    then to define the distance between vectors as follows.\n   distance between vectors distance \n    As   illustrates,\n    if vectors   and   emanate from the same initial point,\n    and   and   are the terminal points of   and  ,\n    respectively,\n    then the difference   is the standard Euclidean distance between the points   and  .\n   . The Angle Between Two Vectors \n    Determining a\n     best \n    solution to a problem often involves finding a solution that minimizes a distance.\n    We generally accomplish a minimization through orthogonality   which depends on the angle between vectors.\n    Given two vectors   and   in  ,\n    we position the vectors so that they emanate from the same initial point.\n    If the vectors are nonzero, then they determine a plane in  .\n    In that plane there are two angles that these vectors create.\n    We will define the angle between the vectors to be the smaller of these two angles.\n    The dot product will tell us how to find the angle between vectors.\n    Let   and   be vectors in   and   the angle between them as illustrated in  .\n   The angle between   and  \n    Using the Law of Cosines, we have\n     .\n   \n    Rearranging, we obtain\n     .\n   \n     angle between vectors \n    So the angle   between two nonzero vectors   and   in   satisfies the equation\n     .\n   orthogonal \n            The vectors   and\n              are perpendicular in  .\n            What is  ?\n           \n            Now let   and   be any vectors in  .\n           \n                  Suppose the angle between nonzero vectors   and   is  .\n                  What does Equation   tell us about  ?\n                 \n                  Now suppose that  .\n                  What does Equation   tell us about the angle between   and  ?\n                  Why?\n                 orthogonal \n                  According to  ,\n                  to which vectors is   orthogonal?\n                  Does this make sense to you intuitively?\n                  Explain.\n                 \n          Find the angle between the two vectors\n            and  .\n         \n          Find, if possible,\n          two non-parallel vectors orthogonal to  .\n         Orthogonal Projections \n    When running a sprint, the racers may be aided or slowed by the wind.\n    The wind assistance is a measure of the wind speed that is helping push the runners down the track.\n    It is much easier to run a very fast race if the wind is blowing hard in the direction of the race.\n    So that world records aren't dependent on the weather conditions,\n    times are only recorded as record times if the wind aiding the runners is less than or equal to 2 meters per second.\n    Wind speed for a race is recorded by a wind gauge that is set up close to the track.\n    It is important to note, however,\n    that weather is not always as cooperative as we might like.\n    The wind does not always blow exactly in the direction of the track,\n    so the gauge must account for the angle the wind makes with the track.\n    If the wind is blowing in the direction of the vector   in  \n    and the track is in the direction of the vector   in  ,\n    then only part of the total wind vector is actually working to help the runners.\n    This part is called the orthogonal projection of the vector   onto the vector   and is denoted  .\n    The next activity shows how to find this projection.\n   The orthogonal projection of   onto  . \n      Since the orthogonal projection\n        is in the direction of  ,\n      there exists a constant   such that  .\n      If we determine the value of  ,\n      we can find  .\n     \n            The wind component that acts perpendicular to the direction of   is called the projection of   orthogonal to   and is denoted\n              as shown in  .\n            Write an equation that involves  ,\n             , and  .\n            Then solve that equation for  .\n           \n            Given that   and   are orthogonal,\n            what does that tell us about  ?\n            Combine this fact with the result of part (a) and that\n              to obtain an equation involving  ,\n             ,\n            and  .\n           \n            Solve for   using the equation you found in the previous step.\n           \n            Use your value of   to identify  .\n           \n    To summarize:\n   orthogonal projection in the direction of a vector projection orthogonal to a vector orthogonal projection projection \n      Let   and  .\n      Find   and\n        and draw a picture to illustrate.\n     \n    The orthogonal projection of a vector   onto a vector   is really a projection of the vector   onto the space  .\n    The vector   is the best approximation to   of all the vectors in   in the sense that\n      is the closest to   among all vectors in\n     , as we will prove later.\n   Orthogonal Complements normal orthogonal complement orthogonal complement orthogonal complement \n      Let   in  .\n      Completely describe all vectors in\n        both algebraically and geometrically.\n     orthogonal complement \n      We have seen another example of orthogonal complements.\n      Let   be an   matrix with rows  ,\n        ,  ,   in order.\n      Consider the three spaces  ,  ,\n      and   related to  ,\n      where  \n      (that is,   is the span of the rows of  ).\n      Let   be a vector in  .\n     \n            What does it mean for   to be in  ?\n           \n            Now let   be a vector in  .\n            Use the result of part (a) and the fact that\n              to explain why  .\n            Explain how this verifies  .\n           \n            Calculate   using scalar products of rows of   with  .\n           \n            Use   in place of   in the result of the previous part to show  .\n           \n    The activity proves the following theorem:\n   \n        Let   be an   matrix.\n        Then\n         .\n       \n    To show that a vector is in the orthogonal complement of a subspace,\n    it is not necessary to demonstrate that the vector is orthogonal to every vector in the subspace.\n    If we have a basis for the subspace,\n    it suffices to show that the vector is orthogonal to every vector in that basis for the subspace,\n    as the next theorem demonstrates.\n   \n        Let   be a basis for a subspace   of  .\n        A vector   in   is orthogonal to every vector in   if and only if   is orthogonal to every vector in  .\n       \n      Let   be a basis for a subspace   of   and let   be a vector in  .\n      Our theorem is a biconditional,\n      so we need to prove both implications.\n      Since  ,\n      it follows that if   is orthogonal to every vector in  ,\n      then   is orthogonal to every vector in  .\n      This proves the forward implication.\n      Now we assume that   is orthogonal to every vector in   and show that   is orthogonal to every vector in  .\n      Let   be a vector in  .\n      Then\n       \n      for some scalars  ,  ,\n       ,  .\n      Then\n       .\n     \n      Thus,   is orthogonal to   and   is orthogonal to every vector in  .\n     \n      Let  .\n      Find all vectors in  .\n     \n    We will work more closely with projections and orthogonal complements in later sections.\n   Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Let   be the line defined by the equation\n          with in   and let\n          be a point in the plane.\n        In this example we will learn how to find the distance from   to  .\n       \n              Show that   is orthogonal to the line  .\n              That is,   is orthogonal to any vector on the line  .\n             \n              Any vector on the line   is a  vector between two points on the line.\n              Let   and\n                be points on the line  .\n              Then   is a vector on line  .\n              Since   and   are on the line,\n              we know that   and  .\n              So   and\n               .\n              Thus,   is orthogonal to every vector on the line  .\n             \n              Let   be any point on line  .\n              Draw a representative picture of  ,\n                with its initial point at  ,\n              along with   and  .\n              Explain how to use a projection to determine the distance from   to  .\n             \n              A picture of the situation is shown in  .\n              If  ,\n              then the distance from point   to line   is given by  .\n               Distance from a point to a line. \n             \n              Use the idea from part (b) to show that the distance   from   to   satisfies\n               .\n             \n              Recall that   and  .\n              Since  , we have\n               .\n              So\n               .\n             \n              Use Equation   to find the distance from the point   to the line  .\n             \n              Here we have  ,\n              and the equation of our line is  .\n              So  ,  , and  .\n              Thus, the distance from   to the line is\n               .\n             \n        Let  ,  ,\n        and   be scalars with  , and let\n         .\n       \n              Find two vectors that span  ,\n              showing that   is a subspace of  .\n              (In fact,   is a plane through the origin in  .)\n             \n              The coefficient matrix for the system\n                is  .\n              The first column is a pivot column and the others are not.\n              So   and   are free variables and\n               .\n              So  .\n             \n              Find a vector   that is orthogonal to the two vectors you found in part (a).\n             \n              If we let  , then\n               .\n              Thus,   is orthogonal to both\n                and  .\n             \n              Explain why   is a basis for  .\n             \n              Let   and  .\n              Every vector in   has the form\n                for some scalars   and  , and\n               .\n              So  .\n              Now we need to verify that   spans  .\n              Let   be in  .\n              Then   for every  .\n              In particular,\n                or  ,\n              and   or  .\n              Equivalently,\n              we have   and  .\n              So\n               .\n              So every vector in   is a multiple of  ,\n              and   spans  .\n              We conclude that   is a basis for  .\n              Thus, the vector   is a normal vector to the plane   if  .\n              The same reasoning works if at least one of  ,\n               ,or   is nonzero,\n              so we can say in every case that\n                is a normal vector to the plane  .\n             Summary \n       \n        The dot product of vectors   and\n          in   is the scalar\n         .\n       \n     \n       \n        The angle   between two nonzero vectors   and   in   satisfies the equation\n         \n        and  .\n       \n     \n       \n        Two vectors   and   are orthogonal if  .\n       \n     \n       \n        The length, or norm,\n        of the vector   can be found as  .\n       \n     \n       \n        The distance between the vectors   and   in   is  ,\n        which is the length of the difference  .\n       \n     \n       \n        Let   and   be vectors in  .\n         \n             \n              The orthogonal projection of   onto   is the vector\n               .\n             \n           \n             \n              The projection of   perpendicular to   is the vector\n               .\n             \n           \n       \n     \n       \n        The orthogonal complement of the subspace   of   is the set\n         .\n       \n     \n        For each of the following pairs of vectors,\n        find  ,\n        calculate the angle between   and  ,\n        determine if   and   are orthogonal,\n        find   and  ,\n        calculate the distance between   and  ,\n        and determine the orthogonal projection of   onto  .\n       \n               ,  \n             \n              The angle between   and   is  .\n              The distance between   and   is  .\n              The orthogonal projection of   onto   is  .\n             \n               ,  \n             \n              The angle between   and   is  .\n              The distance between   and   is  .\n               .\n             \n               ,  \n             \n              The angle between   and   is approximately  .\n              The distance between   and   is  .\n               .\n             \n               ,\n               \n             \n              The angle between   and   is  .\n              The orthogonal projection of   onto   is  .\n              The distance between   and   is  .\n             \n               ,\n               \n             \n              The angle between   and   is approximately  .\n              The distance between   and   is  .\n               .\n             \n        Given  ,\n        find a vector   so that the angle between   and   is\n          and the orthogonal projection of   onto   has length 2.\n       \n        For which value(s) of   is the angle between   and\n          equal to  ?\n       \n         \n       \n        Let   be a\n          matrix with rows  ,\n         ,  ,  ,\n        and let   be an\n          matrix with columns  ,  ,\n         ,  .\n        Show that we can write the matrix product   in a shorthand way as  .\n       \n        Let   be an  ,\n          a vector in   and   a vector in  .\n        Show that\n         .\n       \n        Write the dot product as a matrix-vector product.\n       \n        Let  ,\n         , and   be vectors in  .\n        Show that\n       distributes over vector addition \n              If   is an arbitrary constant,\n              then  \n             \n        The Pythagorean Theorem states that if   and   are the lengths of the legs of a right triangle whose hypotenuse has length  ,\n        then  .\n        If we think of the legs as defining vectors   and  ,\n        then the hypotenuse is the vector   and we can restate the Pythagorean Theorem Pythagorean Theorem in   as\n         .\n        In this exercise we show that this result holds in any dimension.\n       \n              Let   and   be orthogonal vectors in  .\n              Show that  .\n              \n             \n              Rewrite   using the dot product.\n             \n              Must it be true that if   and   are vectors in   with  ,\n              then   and   are orthogonal?\n              If not, provide a counterexample.\n              If true, verify the statement.\n             \n              From part (a), what can we say about  ?\n             Cauchy-Schwarz inequality in  \n        The Cauchy-Schwarz inequality,\n         \n        for any vectors   and   in  ,\n        is considered one of the most important inequalities in mathematics.\n        We verify the Cauchy-Schwarz inequality in this exercise.\n        Let   and   be vectors in  .\n       \n              Explain why the inequality   is true if either   or   is the zero vector.\n              As a consequence,\n              we assume that   and   are nonzero vectors for the remainder of this exercise.\n             \n              Let   and let  .\n              We know that  .\n              Use   of this section to show that\n               .\n             \n              Now show that  .\n             \n              Combine parts (b) and (c) to explain why equation   is true.\n             triangle inequality in  Triangle Inequality \n        Consider cases of   separately.\n        Then write the norm as a dot product.\n       \n        Let   be a subspace of   for some  .\n        Show that   is also a subspace of  .\n       \n        Let   be a subspace of  .\n        Show that   is a subspace of  .\n       \n        Use the definition of  .\n       \n        If   is a subspace of   for some  ,\n        what is  ?\n        Verify your answer.\n       \n        Suppose   are two subspaces of  .\n        Show that  .\n       \n        If   and  ,\n        in what other set is  ?\n       \n        What are   and   in  ?\n        Justify your answers.\n       \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              The dot product is defined between any two vectors.\n             \n              F\n             True\/False \n              If   and   are vectors in  ,\n              then   is another vector in  .\n             True\/False \n              If   and   are vectors in  ,\n              then   is always non-negative.\n             \n              F\n             True\/False \n              If   is a vector in  ,\n              then   is never negative.\n             True\/False \n              If   and   are vectors in   and  ,\n              then  .\n             \n              F\n             True\/False \n              If   is a vector in   and\n               , then  .\n             True\/False \n              The norm of the sum of vectors is the sum of the norms of the vectors.\n             \n              F\n             True\/False \n              If   and   are vectors in  ,\n              then   is a vector in the same direction as  .\n             True\/False \n              The only subspace   of   for which   is  .\n             \n              T\n             True\/False \n              If a vector   is orthogonal to   and  ,\n              then   is also orthogonal to  .\n             True\/False \n              If a vector   is orthogonal to   and  ,\n              then   is also orthogonal to all linear combinations of   and  .\n             \n              T\n             True\/False \n              If   and   are parallel,\n              then the orthogonal projection of   onto   equals  .\n             True\/False \n              If   and   are orthogonal,\n              then the orthogonal projection of   onto   equals  .\n             \n              F\n             True\/False \n              For any vector   and  ,\n               .\n             True\/False \n              Given an   matrix,\n               .\n             \n              T\n             True\/False \n              If   is a square matrix,\n              then the columns of   are orthogonal to the vectors in  .\n             True\/False \n              The vectors in the null space of an\n                matrix are orthogonal to vectors in the row space of  .\n             \n              T\n             Project: Back-Face Culling back-face culling back face culling cross product normal vector \n      Let   and\n        be two vectors in  .\n      If   and   are linearly independent,\n      then   and   determine a polygon as shown in  .\n      Our goal is to find a vector   that is orthogonal to both   and  .\n      Let   be another vector in   and let\n        be the matrix whose rows are  ,\n       , and  .\n      Let   be the  th cofactor of  ,\n      that is   is   times the determinant of the submatrix of   obtained by deleting the  th row and  th column of  .\n      Now define the vector   as follows:\n       .\n     cross product Normal vector to a polygon. \n          Show that\n           .\n         \n          Use a cofactor expansion of   along the first row and properties of the dot product to show that\n           .\n         \n          Use the result of part (b) and properties of the determinant to calculate\n            and  .\n          Explain why   is orthogonal to both   and   and is therefore a normal vector to the polygon determined by   and  .\n         \n     \n    shows how we can find a normal vector to a parallelogram   take two vectors   and   between the vertices of the parallelogram and calculate their cross products.\n    Such a normal vector can define a direction for the parallelogram.\n    There is still a problem, however.\n   \n      Let   and\n        be any vectors in  .\n      There is a relationship between\n        and  .\n      Find and verify this relationship.\n     \n     \n    shows that the cross product is anticommutative,\n    so we get different directions if we switch the order in which we calculate the cross product.\n    To fix a direction,\n    we establish the convention that we always label the vertices of our parallelogram in the counterclockwise direction as shown in  .\n    This way we always use   as the vector from vertex   to vertex   rather than the reverse.\n    With this convention established,\n    we can now define the direction of a parallelogram as the direction of its normal vector.\n   \n    Once we have a normal vector established for each polygon,\n    we can now determine which polygons are back-face and which are front-face.\n     \n    at left provides the gist of the idea,\n    where we represent the polygons with line segments to illustrate.\n    If the viewer's eye is at point   and views the figures,\n    the normal vectors of the visible polygons point in a direction toward the viewer (front-face) and the normal vectors of the polygons hidden from the viewer point away from the viewer\n    (back-face).\n    What remains is to determine an effective computational way to identify the front and back facing polygons.\n   Left: Hidden faces. Right: Back face culling. \n      Consider the situation as depicted at right in  .\n      Assume that   and   are polygons\n      (rendered one dimensionally here)\n      with normal vectors   at their centers as shown.\n      The viewer's eye is at point   and the viewer's line of vision to the centers   and   are indicated by the vectors  .\n      Each vector   makes an angle   with the normal to the polygon.\n     \n            What can be said about the angle   for a front-facing polygon?\n            What must be true about   for a front-facing polygon?\n            Why?\n           \n            What can be said about the angle   for a back-facing polygon?\n            What must be true about   for a back-facing polygon?\n            Why?\n           \n            The dot product then provides us with a simple computational tool for identifying back-facing polygons\n            (assuming we have already calculated all of the normal vectors).\n            We can then create an algorithm to cull the back-facing polygons.\n            Assuming that we the viewpoint   and the coordinates of the polygons of the surface,\n            complete the pseudo-code for a back-face culling algorithm:\n           \n          for all polygons on the surface do\n            calculate the normal vector n using the ______ product for the current polygon\n            calculate the center C of the current polygon\n            calculate the viewing vector ______\n              if ______ then\n                render the current polygon\n            end if\n          end for\n           \n    As a final comment,\n    back-face culling generally reduces the number of polygons to be rendered by half.\n    This algorithm is not perfect and does not always do what we want it to do (e.g., it may not remove all parts of a polygon that we don't see),\n    so there are other algorithms to use in concert with back-face culling to correctly render objects.\n   "
},
{
  "id": "objectives-23",
  "level": "2",
  "url": "chap_dot_product.html#objectives-23",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is the dot product of two vectors?\n            Under what conditions is the dot product defined?\n           \n         \n           \n            How do we find the angle between two nonzero vectors in  ?\n           \n         \n           \n            How does the dot product tell us if two vectors are orthogonal?\n           \n         \n           \n            How do we define the length of a vector in any dimension and how can the dot product be used to calculate the length?\n           \n         \n           \n            How do we define the distance between two vectors?\n           \n         \n           \n            What is the orthogonal projection of a vector   in the direction of the vector   and how do we find it?\n           \n         \n           \n            What is the orthogonal complement of a subspace   of  ?\n           \n         "
},
{
  "id": "F_House",
  "level": "2",
  "url": "chap_dot_product.html#F_House",
  "type": "Figure",
  "number": "23.1",
  "title": "",
  "body": "Images of a house. "
},
{
  "id": "F_Surface",
  "level": "2",
  "url": "chap_dot_product.html#F_Surface",
  "type": "Figure",
  "number": "23.2",
  "title": "",
  "body": "Graphs of  . "
},
{
  "id": "p-3879",
  "level": "2",
  "url": "chap_dot_product.html#p-3879",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "inner products "
},
{
  "id": "F_6_a_Vector_norm",
  "level": "2",
  "url": "chap_dot_product.html#F_6_a_Vector_norm",
  "type": "Figure",
  "number": "23.3",
  "title": "",
  "body": "A vector in   from the origin to a point. "
},
{
  "id": "p-3883",
  "level": "2",
  "url": "chap_dot_product.html#p-3883",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "dot product "
},
{
  "id": "def_6_a_dot_product",
  "level": "2",
  "url": "chap_dot_product.html#def_6_a_dot_product",
  "type": "Definition",
  "number": "23.4",
  "title": "",
  "body": "dot product scalar product dot product scalar product "
},
{
  "id": "def_6_a_length_Rn",
  "level": "2",
  "url": "chap_dot_product.html#def_6_a_length_Rn",
  "type": "Definition",
  "number": "23.5",
  "title": "",
  "body": "vector norm in  norm "
},
{
  "id": "p-3888",
  "level": "2",
  "url": "chap_dot_product.html#p-3888",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "magnitude length "
},
{
  "id": "pa_6_a",
  "level": "2",
  "url": "chap_dot_product.html#pa_6_a",
  "type": "Preview Activity",
  "number": "23.1",
  "title": "",
  "body": "\n            Find   if\n              and   in  .\n           \n            The dot product satisfies some useful properties as given in the next theorem.\n           commutative distributes over vector addition \n            Verification of some of these properties is left to the exercises.\n            Let   and   be vectors in   with  ,\n              and  .\n           \n                  Use property (c) of  \n                  to determine the value of  .\n                 \n                  Use property (b) of  \n                  to determine the value of  .\n                 \n                  Use whatever properties of  \n                  that are needed to determine the value of  .\n                 \n            At times we will want to find vectors in the direction of a given vector that have a certain magnitude.\n            Let   in  .\n           \n                  What is  ?\n                 \n                  Show that  .\n                 unit vector \n                  We can use unit vectors to find vectors of a given length in the direction of a given vector.\n                  Let   be a positive scalar and   a vector in  .\n                  Use properties from  \n                  to show that the magnitude of the vector   is  .\n                 "
},
{
  "id": "def_6_a_distance",
  "level": "2",
  "url": "chap_dot_product.html#def_6_a_distance",
  "type": "Definition",
  "number": "23.8",
  "title": "",
  "body": "distance between vectors distance "
},
{
  "id": "F_6_a_vector_difference",
  "level": "2",
  "url": "chap_dot_product.html#F_6_a_vector_difference",
  "type": "Figure",
  "number": "23.9",
  "title": "",
  "body": ". "
},
{
  "id": "F_Angle",
  "level": "2",
  "url": "chap_dot_product.html#F_Angle",
  "type": "Figure",
  "number": "23.10",
  "title": "",
  "body": "The angle between   and  "
},
{
  "id": "p-3916",
  "level": "2",
  "url": "chap_dot_product.html#p-3916",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "orthogonal "
},
{
  "id": "act_6_a_orthogonality",
  "level": "2",
  "url": "chap_dot_product.html#act_6_a_orthogonality",
  "type": "Activity",
  "number": "23.2",
  "title": "",
  "body": "\n            The vectors   and\n              are perpendicular in  .\n            What is  ?\n           \n            Now let   and   be any vectors in  .\n           \n                  Suppose the angle between nonzero vectors   and   is  .\n                  What does Equation   tell us about  ?\n                 \n                  Now suppose that  .\n                  What does Equation   tell us about the angle between   and  ?\n                  Why?\n                 orthogonal \n                  According to  ,\n                  to which vectors is   orthogonal?\n                  Does this make sense to you intuitively?\n                  Explain.\n                 "
},
{
  "id": "activity-86",
  "level": "2",
  "url": "chap_dot_product.html#activity-86",
  "type": "Activity",
  "number": "23.3",
  "title": "",
  "body": "\n          Find the angle between the two vectors\n            and  .\n         \n          Find, if possible,\n          two non-parallel vectors orthogonal to  .\n         "
},
{
  "id": "F_Projection",
  "level": "2",
  "url": "chap_dot_product.html#F_Projection",
  "type": "Figure",
  "number": "23.12",
  "title": "",
  "body": "The orthogonal projection of   onto  . "
},
{
  "id": "activity-87",
  "level": "2",
  "url": "chap_dot_product.html#activity-87",
  "type": "Activity",
  "number": "23.4",
  "title": "",
  "body": "\n      Since the orthogonal projection\n        is in the direction of  ,\n      there exists a constant   such that  .\n      If we determine the value of  ,\n      we can find  .\n     \n            The wind component that acts perpendicular to the direction of   is called the projection of   orthogonal to   and is denoted\n              as shown in  .\n            Write an equation that involves  ,\n             , and  .\n            Then solve that equation for  .\n           \n            Given that   and   are orthogonal,\n            what does that tell us about  ?\n            Combine this fact with the result of part (a) and that\n              to obtain an equation involving  ,\n             ,\n            and  .\n           \n            Solve for   using the equation you found in the previous step.\n           \n            Use your value of   to identify  .\n           "
},
{
  "id": "definition-52",
  "level": "2",
  "url": "chap_dot_product.html#definition-52",
  "type": "Definition",
  "number": "23.13",
  "title": "",
  "body": "orthogonal projection in the direction of a vector projection orthogonal to a vector orthogonal projection projection "
},
{
  "id": "activity-88",
  "level": "2",
  "url": "chap_dot_product.html#activity-88",
  "type": "Activity",
  "number": "23.5",
  "title": "",
  "body": "\n      Let   and  .\n      Find   and\n        and draw a picture to illustrate.\n     "
},
{
  "id": "p-3938",
  "level": "2",
  "url": "chap_dot_product.html#p-3938",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "normal "
},
{
  "id": "p-3939",
  "level": "2",
  "url": "chap_dot_product.html#p-3939",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "orthogonal complement "
},
{
  "id": "def_6_a_orth_complement",
  "level": "2",
  "url": "chap_dot_product.html#def_6_a_orth_complement",
  "type": "Definition",
  "number": "23.14",
  "title": "",
  "body": "orthogonal complement orthogonal complement "
},
{
  "id": "pa_6_a_2",
  "level": "2",
  "url": "chap_dot_product.html#pa_6_a_2",
  "type": "Preview Activity",
  "number": "23.6",
  "title": "",
  "body": "\n      Let   in  .\n      Completely describe all vectors in\n        both algebraically and geometrically.\n     "
},
{
  "id": "p-3942",
  "level": "2",
  "url": "chap_dot_product.html#p-3942",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "orthogonal complement "
},
{
  "id": "activity-89",
  "level": "2",
  "url": "chap_dot_product.html#activity-89",
  "type": "Activity",
  "number": "23.7",
  "title": "",
  "body": "\n      We have seen another example of orthogonal complements.\n      Let   be an   matrix with rows  ,\n        ,  ,   in order.\n      Consider the three spaces  ,  ,\n      and   related to  ,\n      where  \n      (that is,   is the span of the rows of  ).\n      Let   be a vector in  .\n     \n            What does it mean for   to be in  ?\n           \n            Now let   be a vector in  .\n            Use the result of part (a) and the fact that\n              to explain why  .\n            Explain how this verifies  .\n           \n            Calculate   using scalar products of rows of   with  .\n           \n            Use   in place of   in the result of the previous part to show  .\n           "
},
{
  "id": "thm_6_a_orthogonal_subspaces",
  "level": "2",
  "url": "chap_dot_product.html#thm_6_a_orthogonal_subspaces",
  "type": "Theorem",
  "number": "23.15",
  "title": "",
  "body": "\n        Let   be an   matrix.\n        Then\n         .\n       "
},
{
  "id": "thm_6_a_dot_pd_orth_complement_basis",
  "level": "2",
  "url": "chap_dot_product.html#thm_6_a_dot_pd_orth_complement_basis",
  "type": "Theorem",
  "number": "23.16",
  "title": "",
  "body": "\n        Let   be a basis for a subspace   of  .\n        A vector   in   is orthogonal to every vector in   if and only if   is orthogonal to every vector in  .\n       \n      Let   be a basis for a subspace   of   and let   be a vector in  .\n      Our theorem is a biconditional,\n      so we need to prove both implications.\n      Since  ,\n      it follows that if   is orthogonal to every vector in  ,\n      then   is orthogonal to every vector in  .\n      This proves the forward implication.\n      Now we assume that   is orthogonal to every vector in   and show that   is orthogonal to every vector in  .\n      Let   be a vector in  .\n      Then\n       \n      for some scalars  ,  ,\n       ,  .\n      Then\n       .\n     \n      Thus,   is orthogonal to   and   is orthogonal to every vector in  .\n     "
},
{
  "id": "activity-90",
  "level": "2",
  "url": "chap_dot_product.html#activity-90",
  "type": "Activity",
  "number": "23.8",
  "title": "",
  "body": "\n      Let  .\n      Find all vectors in  .\n     "
},
{
  "id": "example-46",
  "level": "2",
  "url": "chap_dot_product.html#example-46",
  "type": "Example",
  "number": "23.17",
  "title": "",
  "body": "\n        Let   be the line defined by the equation\n          with in   and let\n          be a point in the plane.\n        In this example we will learn how to find the distance from   to  .\n       \n              Show that   is orthogonal to the line  .\n              That is,   is orthogonal to any vector on the line  .\n             \n              Any vector on the line   is a  vector between two points on the line.\n              Let   and\n                be points on the line  .\n              Then   is a vector on line  .\n              Since   and   are on the line,\n              we know that   and  .\n              So   and\n               .\n              Thus,   is orthogonal to every vector on the line  .\n             \n              Let   be any point on line  .\n              Draw a representative picture of  ,\n                with its initial point at  ,\n              along with   and  .\n              Explain how to use a projection to determine the distance from   to  .\n             \n              A picture of the situation is shown in  .\n              If  ,\n              then the distance from point   to line   is given by  .\n               Distance from a point to a line. \n             \n              Use the idea from part (b) to show that the distance   from   to   satisfies\n               .\n             \n              Recall that   and  .\n              Since  , we have\n               .\n              So\n               .\n             \n              Use Equation   to find the distance from the point   to the line  .\n             \n              Here we have  ,\n              and the equation of our line is  .\n              So  ,  , and  .\n              Thus, the distance from   to the line is\n               .\n             "
},
{
  "id": "example-47",
  "level": "2",
  "url": "chap_dot_product.html#example-47",
  "type": "Example",
  "number": "23.19",
  "title": "",
  "body": "\n        Let  ,  ,\n        and   be scalars with  , and let\n         .\n       \n              Find two vectors that span  ,\n              showing that   is a subspace of  .\n              (In fact,   is a plane through the origin in  .)\n             \n              The coefficient matrix for the system\n                is  .\n              The first column is a pivot column and the others are not.\n              So   and   are free variables and\n               .\n              So  .\n             \n              Find a vector   that is orthogonal to the two vectors you found in part (a).\n             \n              If we let  , then\n               .\n              Thus,   is orthogonal to both\n                and  .\n             \n              Explain why   is a basis for  .\n             \n              Let   and  .\n              Every vector in   has the form\n                for some scalars   and  , and\n               .\n              So  .\n              Now we need to verify that   spans  .\n              Let   be in  .\n              Then   for every  .\n              In particular,\n                or  ,\n              and   or  .\n              Equivalently,\n              we have   and  .\n              So\n               .\n              So every vector in   is a multiple of  ,\n              and   spans  .\n              We conclude that   is a basis for  .\n              Thus, the vector   is a normal vector to the plane   if  .\n              The same reasoning works if at least one of  ,\n               ,or   is nonzero,\n              so we can say in every case that\n                is a normal vector to the plane  .\n             "
},
{
  "id": "exercise-221",
  "level": "2",
  "url": "chap_dot_product.html#exercise-221",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        For each of the following pairs of vectors,\n        find  ,\n        calculate the angle between   and  ,\n        determine if   and   are orthogonal,\n        find   and  ,\n        calculate the distance between   and  ,\n        and determine the orthogonal projection of   onto  .\n       \n               ,  \n             \n              The angle between   and   is  .\n              The distance between   and   is  .\n              The orthogonal projection of   onto   is  .\n             \n               ,  \n             \n              The angle between   and   is  .\n              The distance between   and   is  .\n               .\n             \n               ,  \n             \n              The angle between   and   is approximately  .\n              The distance between   and   is  .\n               .\n             \n               ,\n               \n             \n              The angle between   and   is  .\n              The orthogonal projection of   onto   is  .\n              The distance between   and   is  .\n             \n               ,\n               \n             \n              The angle between   and   is approximately  .\n              The distance between   and   is  .\n               .\n             "
},
{
  "id": "exercise-222",
  "level": "2",
  "url": "chap_dot_product.html#exercise-222",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Given  ,\n        find a vector   so that the angle between   and   is\n          and the orthogonal projection of   onto   has length 2.\n       "
},
{
  "id": "exercise-223",
  "level": "2",
  "url": "chap_dot_product.html#exercise-223",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        For which value(s) of   is the angle between   and\n          equal to  ?\n       \n         \n       "
},
{
  "id": "exercise-224",
  "level": "2",
  "url": "chap_dot_product.html#exercise-224",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Let   be a\n          matrix with rows  ,\n         ,  ,  ,\n        and let   be an\n          matrix with columns  ,  ,\n         ,  .\n        Show that we can write the matrix product   in a shorthand way as  .\n       "
},
{
  "id": "exercise-225",
  "level": "2",
  "url": "chap_dot_product.html#exercise-225",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Let   be an  ,\n          a vector in   and   a vector in  .\n        Show that\n         .\n       \n        Write the dot product as a matrix-vector product.\n       "
},
{
  "id": "exercise-226",
  "level": "2",
  "url": "chap_dot_product.html#exercise-226",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Let  ,\n         , and   be vectors in  .\n        Show that\n       distributes over vector addition \n              If   is an arbitrary constant,\n              then  \n             "
},
{
  "id": "ex_Pyth_Thm",
  "level": "2",
  "url": "chap_dot_product.html#ex_Pyth_Thm",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        The Pythagorean Theorem states that if   and   are the lengths of the legs of a right triangle whose hypotenuse has length  ,\n        then  .\n        If we think of the legs as defining vectors   and  ,\n        then the hypotenuse is the vector   and we can restate the Pythagorean Theorem Pythagorean Theorem in   as\n         .\n        In this exercise we show that this result holds in any dimension.\n       \n              Let   and   be orthogonal vectors in  .\n              Show that  .\n              \n             \n              Rewrite   using the dot product.\n             \n              Must it be true that if   and   are vectors in   with  ,\n              then   and   are orthogonal?\n              If not, provide a counterexample.\n              If true, verify the statement.\n             \n              From part (a), what can we say about  ?\n             "
},
{
  "id": "ex_Cauchy_Schwarz",
  "level": "2",
  "url": "chap_dot_product.html#ex_Cauchy_Schwarz",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "Cauchy-Schwarz inequality in  \n        The Cauchy-Schwarz inequality,\n         \n        for any vectors   and   in  ,\n        is considered one of the most important inequalities in mathematics.\n        We verify the Cauchy-Schwarz inequality in this exercise.\n        Let   and   be vectors in  .\n       \n              Explain why the inequality   is true if either   or   is the zero vector.\n              As a consequence,\n              we assume that   and   are nonzero vectors for the remainder of this exercise.\n             \n              Let   and let  .\n              We know that  .\n              Use   of this section to show that\n               .\n             \n              Now show that  .\n             \n              Combine parts (b) and (c) to explain why equation   is true.\n             "
},
{
  "id": "exercise-229",
  "level": "2",
  "url": "chap_dot_product.html#exercise-229",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "triangle inequality in  Triangle Inequality \n        Consider cases of   separately.\n        Then write the norm as a dot product.\n       "
},
{
  "id": "exercise-230",
  "level": "2",
  "url": "chap_dot_product.html#exercise-230",
  "type": "Exercise",
  "number": "10",
  "title": "",
  "body": "\n        Let   be a subspace of   for some  .\n        Show that   is also a subspace of  .\n       "
},
{
  "id": "exercise-231",
  "level": "2",
  "url": "chap_dot_product.html#exercise-231",
  "type": "Exercise",
  "number": "11",
  "title": "",
  "body": "\n        Let   be a subspace of  .\n        Show that   is a subspace of  .\n       \n        Use the definition of  .\n       "
},
{
  "id": "exercise-232",
  "level": "2",
  "url": "chap_dot_product.html#exercise-232",
  "type": "Exercise",
  "number": "12",
  "title": "",
  "body": "\n        If   is a subspace of   for some  ,\n        what is  ?\n        Verify your answer.\n       "
},
{
  "id": "exercise-233",
  "level": "2",
  "url": "chap_dot_product.html#exercise-233",
  "type": "Exercise",
  "number": "13",
  "title": "",
  "body": "\n        Suppose   are two subspaces of  .\n        Show that  .\n       \n        If   and  ,\n        in what other set is  ?\n       "
},
{
  "id": "exercise-234",
  "level": "2",
  "url": "chap_dot_product.html#exercise-234",
  "type": "Exercise",
  "number": "14",
  "title": "",
  "body": "\n        What are   and   in  ?\n        Justify your answers.\n       "
},
{
  "id": "exercise-235",
  "level": "2",
  "url": "chap_dot_product.html#exercise-235",
  "type": "Exercise",
  "number": "15",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              The dot product is defined between any two vectors.\n             \n              F\n             True\/False \n              If   and   are vectors in  ,\n              then   is another vector in  .\n             True\/False \n              If   and   are vectors in  ,\n              then   is always non-negative.\n             \n              F\n             True\/False \n              If   is a vector in  ,\n              then   is never negative.\n             True\/False \n              If   and   are vectors in   and  ,\n              then  .\n             \n              F\n             True\/False \n              If   is a vector in   and\n               , then  .\n             True\/False \n              The norm of the sum of vectors is the sum of the norms of the vectors.\n             \n              F\n             True\/False \n              If   and   are vectors in  ,\n              then   is a vector in the same direction as  .\n             True\/False \n              The only subspace   of   for which   is  .\n             \n              T\n             True\/False \n              If a vector   is orthogonal to   and  ,\n              then   is also orthogonal to  .\n             True\/False \n              If a vector   is orthogonal to   and  ,\n              then   is also orthogonal to all linear combinations of   and  .\n             \n              T\n             True\/False \n              If   and   are parallel,\n              then the orthogonal projection of   onto   equals  .\n             True\/False \n              If   and   are orthogonal,\n              then the orthogonal projection of   onto   equals  .\n             \n              F\n             True\/False \n              For any vector   and  ,\n               .\n             True\/False \n              Given an   matrix,\n               .\n             \n              T\n             True\/False \n              If   is a square matrix,\n              then the columns of   are orthogonal to the vectors in  .\n             True\/False \n              The vectors in the null space of an\n                matrix are orthogonal to vectors in the row space of  .\n             \n              T\n             "
},
{
  "id": "p-4048",
  "level": "2",
  "url": "chap_dot_product.html#p-4048",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "back face culling "
},
{
  "id": "act_bf_normal",
  "level": "2",
  "url": "chap_dot_product.html#act_bf_normal",
  "type": "Project Activity",
  "number": "23.9",
  "title": "",
  "body": "cross product normal vector \n      Let   and\n        be two vectors in  .\n      If   and   are linearly independent,\n      then   and   determine a polygon as shown in  .\n      Our goal is to find a vector   that is orthogonal to both   and  .\n      Let   be another vector in   and let\n        be the matrix whose rows are  ,\n       , and  .\n      Let   be the  th cofactor of  ,\n      that is   is   times the determinant of the submatrix of   obtained by deleting the  th row and  th column of  .\n      Now define the vector   as follows:\n       .\n     cross product Normal vector to a polygon. \n          Show that\n           .\n         \n          Use a cofactor expansion of   along the first row and properties of the dot product to show that\n           .\n         \n          Use the result of part (b) and properties of the determinant to calculate\n            and  .\n          Explain why   is orthogonal to both   and   and is therefore a normal vector to the polygon determined by   and  .\n         "
},
{
  "id": "act_bf_normal_2",
  "level": "2",
  "url": "chap_dot_product.html#act_bf_normal_2",
  "type": "Project Activity",
  "number": "23.10",
  "title": "",
  "body": "\n      Let   and\n        be any vectors in  .\n      There is a relationship between\n        and  .\n      Find and verify this relationship.\n     "
},
{
  "id": "F_Hidden",
  "level": "2",
  "url": "chap_dot_product.html#F_Hidden",
  "type": "Figure",
  "number": "23.21",
  "title": "",
  "body": "Left: Hidden faces. Right: Back face culling. "
},
{
  "id": "act_bf_dot_product",
  "level": "2",
  "url": "chap_dot_product.html#act_bf_dot_product",
  "type": "Project Activity",
  "number": "23.11",
  "title": "",
  "body": "\n      Consider the situation as depicted at right in  .\n      Assume that   and   are polygons\n      (rendered one dimensionally here)\n      with normal vectors   at their centers as shown.\n      The viewer's eye is at point   and the viewer's line of vision to the centers   and   are indicated by the vectors  .\n      Each vector   makes an angle   with the normal to the polygon.\n     \n            What can be said about the angle   for a front-facing polygon?\n            What must be true about   for a front-facing polygon?\n            Why?\n           \n            What can be said about the angle   for a back-facing polygon?\n            What must be true about   for a back-facing polygon?\n            Why?\n           \n            The dot product then provides us with a simple computational tool for identifying back-facing polygons\n            (assuming we have already calculated all of the normal vectors).\n            We can then create an algorithm to cull the back-facing polygons.\n            Assuming that we the viewpoint   and the coordinates of the polygons of the surface,\n            complete the pseudo-code for a back-face culling algorithm:\n           \n          for all polygons on the surface do\n            calculate the normal vector n using the ______ product for the current polygon\n            calculate the center C of the current polygon\n            calculate the viewing vector ______\n              if ______ then\n                render the current polygon\n            end if\n          end for\n           "
},
{
  "id": "chap_orthogonal_basis",
  "level": "1",
  "url": "chap_orthogonal_basis.html",
  "type": "Section",
  "number": "24",
  "title": "Orthogonal and Orthonormal Bases in <span class=\"process-math\">\\(\\R^n\\)<\/span>",
  "body": "Orthogonal and Orthonormal Bases in  \n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            What is an orthogonal set in  ?\n            What is one useful fact about orthogonal subsets of  ?\n           \n         \n           \n            What is an orthogonal basis for a subspace of  ?\n            What is an orthonormal basis?\n           \n         \n           \n            How does orthogonality help us find the weights to write a vector as a linear combination of vectors in an orthogonal basis?\n           \n         \n           \n            What is an orthogonal matrix and why are orthogonal matrices useful?\n           \n         Application: Rotations in 3D roll pitch yaw Introduction orthogonal orthonormal \n    Recall that two nonzero vectors   and   in   are orthogonal if  .\n    We can extend this idea to an entire set.\n    For example, the standard basis\n      for   has the property that any two distinct vectors in   are orthogonal to each other.\n    The basis vectors in   make a very nice coordinate system for  ,\n    where the basis vectors provide the directions for the coordinate axes.\n    We could rotate this standard basis,\n    or multiply any of the vectors in the basis by a nonzero constant,\n    and retain a basis in which all distinct vectors are orthogonal to each other (e.g.,  ).\n    We define this idea of having all vectors be orthogonal to each other for sets,\n    and then for bases.\n   orthogonal \n            Determine if the set   is an orthogonal set.\n           orthogonal basis \n                  Explain why   is an orthogonal basis for  .\n                 \n                  Suppose   has coordinates\n                    with respect to the basis  , i.e.\n                   .\n                  Substitute this expression for   in\n                    and use the orthogonality property of the basis   to show that  .\n                  Then determine   and   similarly.\n                  Finally, calculate the values of  ,\n                   , and   if  .\n                 \n                  Find components of   by reducing the augmented matrix  .\n                  Does this result agree with your work from the previous part?\n                 Orthogonal Sets \n    We defined orthogonal sets in   and bases of subspaces of   in  Definitions \n    and  .\n    We saw that the standard basis in   is an orthogonal set and an orthogonal basis of     there are many other examples as well.\n   \n      Let  ,\n       ,\n      and  .\n      In the same manner as in  ,\n      we can show that the set   is an orthogonal subset of  .\n     \n            Is the set   an orthogonal subset of  ?\n           \n            Suppose a vector   is a vector so that\n              is an orthogonal subset of  .\n            Then   for each  .\n            Explain why this implies that   is in  ,\n            where  .\n           \n            Assuming that the reduced row echelon form of the matrix   is  ,\n            explain why it is not possible to find a nonzero vector   so that\n              is an orthogonal subset of  .\n           \n    The example from  \n    suggests that we can have three orthogonal nonzero vectors in  ,\n    but no more.\n    Orthogonal vectors are, in a sense,\n    as far apart as they can be.\n    So we might expect that there is no linear relationship between orthogonal vectors.\n    The following theorem makes this clear.\n   \n        Let   be a set of nonzero orthogonal vectors in  .\n        Then the vectors   are linearly independent.\n       \n      Let   be a set of nonzero orthogonal vectors in  .\n      To show that  ,  ,\n       ,   are linearly independent, assume that\n       \n      for some scalars  ,  ,\n       ,  .\n      We will show that   for each   from 1 to  .\n      Since the vectors in   are orthogonal to each other,\n      we know that   whenever  .\n      Fix an index   between 1 and  .\n      We evaluate the dot product of both sides of   with   and simplify using the dot product properties:\n       .\n     \n      Now all of the dot products on the left are 0 except for  ,\n      so   becomes\n       .\n     \n      We assumed that   and since  ,\n      we conclude that  .\n      We chose   arbitrarily,\n      so we have shown that   for each   between 1 and  .\n      Therefore, the only solution to equation   is the trivial solution with\n        and the set   is linearly independent.\n     Properties of Orthogonal Bases \n    Orthogonality is a useful and important property for a basis to have.\n    In  \n    we saw that if a vector   in the span of an orthogonal basis\n      could be written as a linear combination of the basis vectors as  ,\n    then  .\n    If we continued that same argument we could show that\n     .\n   \n    We can apply this idea in general to see how the orthogonality of an orthogonal basis allows us to quickly and easily determine the weights to write a given vector as a linear combination of orthogonal basis vectors.\n    To see why, let   be an orthogonal basis for a subspace   of   and let   be any vector in  .\n    We know that\n     \n    for some scalars  ,  ,\n     ,  .\n    Let  .\n    Then, using orthogonality of vectors  , we have\n     .\n   \n    So\n     .\n   \n    Thus, we can calculate each weight individually with two simple dot products.\n    We summarize this discussion in the next theorem.\n   \n        Let   be an orthogonal basis for a subspace of  .\n        Let   be a vector in  .\n        Then\n         .\n       \n      Let  ,\n       ,\n      and  .\n      The set   is a basis for  .\n      Let  .\n      Calculate\n       .\n     \n      Compare to  .\n      Does this violate  ?\n      Explain.\n     Orthonormal Bases \n    The decomposition   is even simpler if\n      for each  , that is,\n    if   is a unit vector for each  .\n    In this case,\n    the denominators are all 1 and we don't even need to consider them.\n    We have a familiar example of such a basis for  ,\n    namely the standard basis  .\n   \n    Recall that\n     ,\n    so the condition   implies that the vector   has norm 1.\n    An orthogonal basis with this additional condition is a very nice basis and is given a special name.\n   orthonormal basis in  orthonormal basis \n    In other words,\n    an orthonormal basis is an orthogonal basis in which every basis vector is a unit vector.\n    A good question to ask here is how we can construct an orthonormal basis from an orthogonal basis.\n   \n          Let   and   be orthogonal vectors.\n          Explain how we can obtain unit vectors   in the direction of   and   in the direction of  .\n         \n          Show that   and   from the previous part are orthogonal vectors.\n         \n          Use the ideas from this problem to construct an orthonormal basis for   from the orthogonal basis  .\n         \n    In general, we can construct an orthonormal basis\n      from an orthogonal basis\n      by normalizing each vector in  \n    (that is, dividing each vector by its norm).\n   Orthogonal Matrices \n    We have seen in the diagonalization process that we diagonalize a matrix   with a matrix   whose columns are linearly independent eigenvectors of  .\n    In general, calculating the inverse of the matrix whose columns are eigenvectors of   in the diagonalization process can be time consuming,\n    but if the columns form an orthonormal set,\n    then the calculation is very straightforward.\n   \n      Let  ,\n       ,\n      and  .\n      It is not difficult to see that the set\n        is an orthonormal basis for  .\n      Let\n       . \n     \n            Use the definition of the matrix-matrix product to find the entries of the second row of the matrix product  .\n            Why should you have expected the result?\n            \n           \n            How are the rows of   related to the columns of  ?\n           \n            With the result of part (a) in mind,\n            what is the matrix product  ?\n            What does this tell us about the relationship between   and  ?\n            Use technology to calculate   and confirm your answer.\n           \n            Suppose   is an   matrix whose columns form an orthonormal basis for  .\n            Explain why  .\n           \n    The result of  \n    is that if the columns of a square matrix   form an orthonormal set,\n    then  .\n    This makes calculating   very easy.\n    Note, however,\n    that this only works if the columns of   form an orthonormal basis for  .\n    You should also note that if   is an\n      matrix satisfying  ,\n    then the columns of   must form an orthonormal set.\n    Matrices like this appear quite often and are given a special name.\n   matrix orthogonal orthogonal \n      As a special case,\n      we apply the result of  \n      to a   rotation matrix  .\n     \n            Show that the columns of   form an orthonormal set.\n           \n            Use the fact that   to find  .\n            Explain how this shows that the inverse of a rotation matrix by an angle   is just another rotation matrix but by the angle  .\n           \n    Orthogonal matrices are useful because they satisfy some special properties.\n    For example, if   is an orthogonal\n      matrix and  , then\n     .\n   isometry \n    Thus   also preserves angles.\n    Transformations defined by orthogonal matrices are very well behaved transformations.\n    To summarize,\n   \n        Let   be an   orthogonal matrix and let  .\n        Then\n         \n             \n               ,\n             \n           \n             \n               , and\n             \n           \n             \n                if   and   are nonzero.\n             \n           \n       \n    We have discussed orthogonal and orthonormal bases for subspaces of   in this section.\n    There are reasonable questions that follow, such as\n     \n         \n          Can we always find an orthogonal\n          (or orthonormal)\n          basis for any subspace of  ?\n         \n       \n         \n          Given a vector   in  ,\n          can we find an orthogonal basis of   that contain  ?\n         \n       \n\n    We will answer these questions in subsequent sections.\n   Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Let  ,\n        where  ,\n         ,\n        and  .\n       \n              Show that   is an orthogonal set.\n             \n              Using the dot product formula,\n              we see that  ,\n               ,\n              and  .\n              Thus, the set   is an orthogonal set.\n             \n              Create an orthonormal set   from the vectors in  .\n             \n              To make an orthonormal set   from  ,\n              we divide each vector in   by its magnitude.\n              This gives us\n               .\n             \n              Just by calculating dot products,\n              write the vector   as a linear combination of the vectors in  .\n             \n              Since   is an orthonormal basis for  ,\n              we know that\n               .\n             \n        Let  ,\n         ,\n        and  .\n        Let  .\n       \n              Show that   is an orthonormal basis for  .\n             \n              Using the dot product formula,\n              we see that   if\n                and that   for each  .\n              Since orthogonal vectors are linearly independent,\n              the set   is a linearly independent set with   vectors in a  -dimensional space.\n              It follows that   is an orthonormal basis for  .\n             \n              Let  .\n              Find  .\n             \n              Since   is an orthonormal basis for  ,\n              we know that\n               .\n              Therefore,\n               .\n             \n              Calculate   and  .\n              What do you notice?\n             \n              Using the definition of the norm of a vector we have\n               .\n              So in this case we have  .\n             \n              Show that the result of part (c) is true in general.\n              That is, if   is an orthonormal basis for  ,\n              and if  , then\n               .\n             \n              Let   be an orthonormal basis for  ,\n              and suppose that  .\n              Then\n               .\n              Since   is an orthonormal basis for  ,\n              it follows that   if\n                and  .\n              Expanding the dot product in  ,\n              the only terms that won't be zero are the ones that involve  .\n              This leaves us with\n               .\n             Summary \n       \n        A subset   of   is an orthogonal set if\n          for every pair of distinct vector   and   in  .\n       \n     \n       \n        Any orthogonal set of nonzero vectors is linearly independent.\n       \n     \n       \n        A basis   for a subspace   of   is an orthogonal basis if   is also an orthogonal set.\n       \n     \n       \n        An orthogonal basis   for a subspace   of   is an orthonormal basis if each vector in   has unit length.\n       \n     \n       \n        If   is an orthogonal basis for a subspace   of   and   is any vector in  , then\n         \n        where  .\n       \n     \n       \n        An   matrix   is an orthogonal matrix if  .\n        Orthogonal matrices are important, in part,\n        because the matrix transformations they define are isometries.\n       \n     \n        Find an orthogonal basis for the subspace   of  .\n       \n         \n       \n        Let   be an orthogonal basis for   and,\n        for some   between 1 and  ,\n        let  .\n        Show that   is a basis for  .\n       \n        Let   be a subspace of   for some  ,\n        and let   be an orthogonal basis for  .\n        Let   be a vector in  . and define   as\n         .\n       \n              Explain why   is in  .\n             \n              Where are  ,  ,\n               ,  ?\n             \n              Let  .\n              Show that   is in  .\n             \n              Take the dot product of   with  .\n             \n              Explain why   can be written as a sum of vectors,\n              one in   and one in  .\n             \n              Use parts (a) and (b).\n             \n              Suppose   and  ,\n              where   and   are in   and   and   are in  .\n              Show that   and  ,\n              so that the representation of   as a sum of a vector in   and a vector in   is unique.\n             \n              Collect terms in   and in  .\n             \n        Use the result of problem   above and that   to show that\n          for a subspace   of  .\n        (See  \n        in  \n        for the definition of the sum of subspaces.)\n       \n        Let   be an   matrix.\n        We showed that if   is an orthogonal matrix,\n        then   for any vectors   and   in  .\n        Now we ask if the converse of this statement is true.\n        That is, determine the validity of the following statement:\n        if   for any vectors   and   in  ,\n        then   is an orthogonal matrix?\n        Verify your answer.\n       \n        Consider\n          where   is the  th standard basis vector for  .\n       \n        In this exercise we examine reflection matrices.\n        In the following exercise we will show that the reflection and rotation matrices are the only   orthogonal matrices.\n        We will determine how to represent the reflection across a line through the origin in   as a matrix transformation.\n        The setup is as follows.\n        Let   be the line through the origin in   that makes an angle   with the positive  -axis as illustrated in  .\n       Reflecting across a line  . \n              Find a unit vector   in the direction of the line  .\n             \n              Let   be an arbitrary vector in   as represented in  .\n              Determine the components of the vectors\n                and  .\n              Reproduce  \n              and draw the vectors   and   in your figure.\n             \n              The vector labeled   is the reflection of the vector   across the line  .\n              Write   in terms of   and  .\n              Clearly explain your method.\n             reflection \n        In this exercise we will show that the only orthogonal\n          matrices are the rotation matrices\n          and the reflection matrices  \n        (see  ).\n        Throughout this exercise let  ,  ,  ,\n        and   be real numbers such that\n          is an orthogonal   matrix.\n        Let   and\n          be the columns of  .\n       \n              Explain why the terminal point of   in standard position lies on the unit circle.\n              Then explain why there is an angle   such that\n                and  .\n              What angle, specifically, is  ?\n              Draw a picture to illustrate.\n             \n              Think polar coordinates.\n             \n              A similar argument to (b) shows that there is an angle   such that  .\n              Given that   is an orthogonal matrix,\n              how must   be related to  ?\n              Use this result to find the two possibilities for   as a vector in terms of\n                and  .\n              (You will likely want to look up some trigonometric identities for this part of the problem.)\n             \n              What properties do the columns of an orthogonal matrix have?\n             \n              By considering the two possibilities from part (c),\n              show that   is either a rotation matrix or a reflection matrix.\n              Conclude that the only   orthogonal matrices are the reflection and rotation matrices.\n             \n              What happens if   and if  ?\n             \n        Suppose   are orthogonal matrices of the same size.\n       \n              Show that   is also an orthogonal matrix.\n             \n              Show that   is also an orthogonal matrix.\n             \n              Show that   is also an orthogonal matrix.\n             \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              Any orthogonal subset of   is linearly independent.\n             \n              F\n             True\/False \n              Every single vector set is an orthogonal set.\n             True\/False \n              If   is an orthogonal set in   with exactly   nonzero vectors,\n              then   is a basis for  .\n             \n              T\n             True\/False \n              Every set of three linearly independent vectors in   is an orthogonal set.\n             True\/False \n              If   and   are   orthogonal matrices,\n              then   must also be an orthogonal matrix.\n             \n              F\n             True\/False \n              If the set\n                is an orthogonal set in  ,\n              then so is the set   for any scalars  ,\n               ,  ,  .\n             True\/False \n              If   is an orthogonal basis of  ,\n              then so is  ,\n               ,  ,\n                for any nonzero scalars  ,  ,\n               ,  .\n             \n              T\n             True\/False \n              If   is an   orthogonal matrix,\n              the rows of   form an orthonormal basis of  .\n             True\/False \n              If   is an orthogonal matrix,\n              any matrix obtained by interchanging columns of   is also an orthogonal matrix.\n             \n              T\n             Project: Understanding Rotations in 3-Space \n    Recall that a counterclockwise rotation of  -space around the origin by an angle   is accomplished by left multiplication by the matrix  .\n    Notice that the columns of this rotation matrix are orthonormal,\n    so this rotation matrix is an orthogonal matrix.\n    As the next activity shows,\n    rotation matrices in 3D are also orthogonal matrices.\n   \n      Let   be a rotation matrix in 3D. A rotation does not change lengths of vectors,\n      nor does it change angles between vectors.\n      Let  ,\n       ,\n      and   be the standard unit vectors in  .\n     \n            Explain why the columns of   form an orthonormal set.\n            \n           \n                How are  ,  ,\n                and   related to the columns of  ?\n               \n            Explain why   is an orthogonal matrix.\n            What must be true about  ?\n           \n            What is   and what is  ?\n           \n    By  \n    we know that the determinant of any rotation matrix is either   or  .\n    Having a determinant of   preserves orientation,\n    and we will identify these rotations as being counterclockwise,\n    and we will identify the others with determinant of   as being clockwise.\n    We will set the convention that a rotation is always measured counterclockwise\n    (as we did in  ),\n    and so every rotation matrix will have determinant  .\n   \n    Returning to the counterclockwise rotation of  -space around the origin by an angle   determined by left multiplication by the matrix  ,\n    we can think of this rotation in  -space as the rotation that keeps points in the   plane in the   plane,\n    but rotates these points counterclockwise around the   axis.\n    In other words, in the standard   coordinate system,\n    with standard basis  ,\n     ,  ,\n    our rotation matrix   has the property that  .\n    Now   is the third column of  ,\n    so the third column of   is  .\n    Similarly,   is the first column of   and   is the second column of  .\n    Since   is a counterclockwise rotation of the   plane space around the origin by an angle   it follows that this rotation is given by the matrix\n     .\n   normal \n    This idea describes a general rotation matrix\n      in 3D by specifying a normal vector   and an angle  .\n    For example, with roll,\n    a normal vector points from the tail of the aircraft to its tip.\n    It is our goal to understand how we can determine an arbitrary rotation matrix of the form  .\n    We can accomplish this by using the rotation around the   axis and change of basis matrices to find rotation matrices around other axes.\n    Let   be the standard basis for  \n   \n      In this activity we see how to determine the rotation matrix around the   axis using the matrix\n        and a change of basis.\n     \n            Define a new ordered basis   so that our axis of rotation is the third vector.\n            So in this case the third vector in   will be  .\n            The other two vectors need to make   an orthonormal set.\n            So we have plenty of choices.\n            For example, we could set  .\n            Find the change of basis matrix\n              from   to  .\n           \n            Use the change of basis matrix from part (a) to find the change of basis matrix\n              from   to  .\n           \n            To find our rotation matrix around the   axis,\n            we can first change basis from   to  ,\n            then perform a rotation around the new   axis using  ,\n            then changing basis back from   to  .\n            In other words,\n             .\n            Find the entries of this matrix  .\n           IMPORTANT NOTE \n    We could have considered using\n      in  \n    instead of  .\n    Then we would have\n     .\n   \n    The difference between the two options is that,\n    in the first we have   while   in the second.\n    Using   will give clockwise rotations while   gives counterclockwise rotations\n    (this is the difference between a left hand system and a right hand system).\n    So it is important to ensure that our change of basis matrix is one with determinant  .\n   \n      Repeat  \n      to find the 3D rotation around the   axis.\n     \n    We do one more example to illustrate the process before tackling the general case.\n   \n      In this activity we find the rotation around the axis given by the line  .\n      This line is in the direction of the vector  .\n      So we start with making a unit vector in the direction of   as the third vector in an ordered basis  .\n      The other two vectors need to make   an orthonormal set with  .\n     \n            Find a unit vector   in the direction of  .\n           \n            Show that   is orthogonal to the vector   from part (a).\n            Then find a unit vector   that is in the same direction as  .\n           \n            Let   be as in the previous part.\n            Now the trick is to find a third unit vector   so that\n              is an orthonormal set.\n            This can be done with the cross product.\n            If   and  ,\n            then the cross product   of   and   is the vector\n             .\n            (You can check that   is an orthogonal set that gives the correct determinant for the change of basis matrix.) Use the cross product to find a unit vector   so that\n              is an orthonormal set.\n           \n            Find the entries of the matrix  .\n           \n    In the next activity we summarize the general process to find a 3D rotation matrix\n      for any normal vector  .\n    There is a GeoGebra applet at \n      that allows you to visualize rotation matrices in 3D.\n   \n      Let   be a normal vector (nonzero) for our rotation.\n      We need to create an orthonormal basis\n        where   is a unit vector in the direction of   so that the change of basis matrix\n        has determinant  .\n     \n            Find, by inspection,\n            a vector   that is orthogonal to  .\n            \n           \n            You may need to consider some cases to ensure that   is not the zero vector.\n           \n            Once we have a normal vector   and a vector   orthogonal to  ,\n            the vector   gives us an orthogonal set  .\n            We then normalize each vector to create our orthonormal basis  .\n            Use this process to find the matrix that produces a\n              counterclockwise rotation around the normal vector  .\n           "
},
{
  "id": "objectives-24",
  "level": "2",
  "url": "chap_orthogonal_basis.html#objectives-24",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            What is an orthogonal set in  ?\n            What is one useful fact about orthogonal subsets of  ?\n           \n         \n           \n            What is an orthogonal basis for a subspace of  ?\n            What is an orthonormal basis?\n           \n         \n           \n            How does orthogonality help us find the weights to write a vector as a linear combination of vectors in an orthogonal basis?\n           \n         \n           \n            What is an orthogonal matrix and why are orthogonal matrices useful?\n           \n         "
},
{
  "id": "p-4069",
  "level": "2",
  "url": "chap_orthogonal_basis.html#p-4069",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "roll pitch yaw "
},
{
  "id": "p-4070",
  "level": "2",
  "url": "chap_orthogonal_basis.html#p-4070",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "orthogonal orthonormal "
},
{
  "id": "def_6_b_orthogonal_set_Rn",
  "level": "2",
  "url": "chap_orthogonal_basis.html#def_6_b_orthogonal_set_Rn",
  "type": "Definition",
  "number": "24.1",
  "title": "",
  "body": "orthogonal "
},
{
  "id": "pa_6_b",
  "level": "2",
  "url": "chap_orthogonal_basis.html#pa_6_b",
  "type": "Preview Activity",
  "number": "24.1",
  "title": "",
  "body": "\n            Determine if the set   is an orthogonal set.\n           orthogonal basis \n                  Explain why   is an orthogonal basis for  .\n                 \n                  Suppose   has coordinates\n                    with respect to the basis  , i.e.\n                   .\n                  Substitute this expression for   in\n                    and use the orthogonality property of the basis   to show that  .\n                  Then determine   and   similarly.\n                  Finally, calculate the values of  ,\n                   , and   if  .\n                 \n                  Find components of   by reducing the augmented matrix  .\n                  Does this result agree with your work from the previous part?\n                 "
},
{
  "id": "act_6_b_orthog",
  "level": "2",
  "url": "chap_orthogonal_basis.html#act_6_b_orthog",
  "type": "Activity",
  "number": "24.2",
  "title": "",
  "body": "\n      Let  ,\n       ,\n      and  .\n      In the same manner as in  ,\n      we can show that the set   is an orthogonal subset of  .\n     \n            Is the set   an orthogonal subset of  ?\n           \n            Suppose a vector   is a vector so that\n              is an orthogonal subset of  .\n            Then   for each  .\n            Explain why this implies that   is in  ,\n            where  .\n           \n            Assuming that the reduced row echelon form of the matrix   is  ,\n            explain why it is not possible to find a nonzero vector   so that\n              is an orthogonal subset of  .\n           "
},
{
  "id": "thm_6_b_Orth_li",
  "level": "2",
  "url": "chap_orthogonal_basis.html#thm_6_b_Orth_li",
  "type": "Theorem",
  "number": "24.3",
  "title": "",
  "body": "\n        Let   be a set of nonzero orthogonal vectors in  .\n        Then the vectors   are linearly independent.\n       \n      Let   be a set of nonzero orthogonal vectors in  .\n      To show that  ,  ,\n       ,   are linearly independent, assume that\n       \n      for some scalars  ,  ,\n       ,  .\n      We will show that   for each   from 1 to  .\n      Since the vectors in   are orthogonal to each other,\n      we know that   whenever  .\n      Fix an index   between 1 and  .\n      We evaluate the dot product of both sides of   with   and simplify using the dot product properties:\n       .\n     \n      Now all of the dot products on the left are 0 except for  ,\n      so   becomes\n       .\n     \n      We assumed that   and since  ,\n      we conclude that  .\n      We chose   arbitrarily,\n      so we have shown that   for each   between 1 and  .\n      Therefore, the only solution to equation   is the trivial solution with\n        and the set   is linearly independent.\n     "
},
{
  "id": "thm_6_b_orth_dcomp",
  "level": "2",
  "url": "chap_orthogonal_basis.html#thm_6_b_orth_dcomp",
  "type": "Theorem",
  "number": "24.4",
  "title": "",
  "body": "\n        Let   be an orthogonal basis for a subspace of  .\n        Let   be a vector in  .\n        Then\n         .\n       "
},
{
  "id": "activity-92",
  "level": "2",
  "url": "chap_orthogonal_basis.html#activity-92",
  "type": "Activity",
  "number": "24.3",
  "title": "",
  "body": "\n      Let  ,\n       ,\n      and  .\n      The set   is a basis for  .\n      Let  .\n      Calculate\n       .\n     \n      Compare to  .\n      Does this violate  ?\n      Explain.\n     "
},
{
  "id": "def_6_b_orthonormal_basis",
  "level": "2",
  "url": "chap_orthogonal_basis.html#def_6_b_orthonormal_basis",
  "type": "Definition",
  "number": "24.5",
  "title": "",
  "body": "orthonormal basis in  orthonormal basis "
},
{
  "id": "activity-93",
  "level": "2",
  "url": "chap_orthogonal_basis.html#activity-93",
  "type": "Activity",
  "number": "24.4",
  "title": "",
  "body": "\n          Let   and   be orthogonal vectors.\n          Explain how we can obtain unit vectors   in the direction of   and   in the direction of  .\n         \n          Show that   and   from the previous part are orthogonal vectors.\n         \n          Use the ideas from this problem to construct an orthonormal basis for   from the orthogonal basis  .\n         "
},
{
  "id": "act_6_b_orthogonal_inverse",
  "level": "2",
  "url": "chap_orthogonal_basis.html#act_6_b_orthogonal_inverse",
  "type": "Activity",
  "number": "24.5",
  "title": "",
  "body": "\n      Let  ,\n       ,\n      and  .\n      It is not difficult to see that the set\n        is an orthonormal basis for  .\n      Let\n       . \n     \n            Use the definition of the matrix-matrix product to find the entries of the second row of the matrix product  .\n            Why should you have expected the result?\n            \n           \n            How are the rows of   related to the columns of  ?\n           \n            With the result of part (a) in mind,\n            what is the matrix product  ?\n            What does this tell us about the relationship between   and  ?\n            Use technology to calculate   and confirm your answer.\n           \n            Suppose   is an   matrix whose columns form an orthonormal basis for  .\n            Explain why  .\n           "
},
{
  "id": "definition-57",
  "level": "2",
  "url": "chap_orthogonal_basis.html#definition-57",
  "type": "Definition",
  "number": "24.6",
  "title": "",
  "body": "matrix orthogonal orthogonal "
},
{
  "id": "activity-95",
  "level": "2",
  "url": "chap_orthogonal_basis.html#activity-95",
  "type": "Activity",
  "number": "24.6",
  "title": "",
  "body": "\n      As a special case,\n      we apply the result of  \n      to a   rotation matrix  .\n     \n            Show that the columns of   form an orthonormal set.\n           \n            Use the fact that   to find  .\n            Explain how this shows that the inverse of a rotation matrix by an angle   is just another rotation matrix but by the angle  .\n           "
},
{
  "id": "p-4116",
  "level": "2",
  "url": "chap_orthogonal_basis.html#p-4116",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "isometry "
},
{
  "id": "theorem-59",
  "level": "2",
  "url": "chap_orthogonal_basis.html#theorem-59",
  "type": "Theorem",
  "number": "24.7",
  "title": "",
  "body": "\n        Let   be an   orthogonal matrix and let  .\n        Then\n         \n             \n               ,\n             \n           \n             \n               , and\n             \n           \n             \n                if   and   are nonzero.\n             \n           \n       "
},
{
  "id": "example-48",
  "level": "2",
  "url": "chap_orthogonal_basis.html#example-48",
  "type": "Example",
  "number": "24.8",
  "title": "",
  "body": "\n        Let  ,\n        where  ,\n         ,\n        and  .\n       \n              Show that   is an orthogonal set.\n             \n              Using the dot product formula,\n              we see that  ,\n               ,\n              and  .\n              Thus, the set   is an orthogonal set.\n             \n              Create an orthonormal set   from the vectors in  .\n             \n              To make an orthonormal set   from  ,\n              we divide each vector in   by its magnitude.\n              This gives us\n               .\n             \n              Just by calculating dot products,\n              write the vector   as a linear combination of the vectors in  .\n             \n              Since   is an orthonormal basis for  ,\n              we know that\n               .\n             "
},
{
  "id": "example-49",
  "level": "2",
  "url": "chap_orthogonal_basis.html#example-49",
  "type": "Example",
  "number": "24.9",
  "title": "",
  "body": "\n        Let  ,\n         ,\n        and  .\n        Let  .\n       \n              Show that   is an orthonormal basis for  .\n             \n              Using the dot product formula,\n              we see that   if\n                and that   for each  .\n              Since orthogonal vectors are linearly independent,\n              the set   is a linearly independent set with   vectors in a  -dimensional space.\n              It follows that   is an orthonormal basis for  .\n             \n              Let  .\n              Find  .\n             \n              Since   is an orthonormal basis for  ,\n              we know that\n               .\n              Therefore,\n               .\n             \n              Calculate   and  .\n              What do you notice?\n             \n              Using the definition of the norm of a vector we have\n               .\n              So in this case we have  .\n             \n              Show that the result of part (c) is true in general.\n              That is, if   is an orthonormal basis for  ,\n              and if  , then\n               .\n             \n              Let   be an orthonormal basis for  ,\n              and suppose that  .\n              Then\n               .\n              Since   is an orthonormal basis for  ,\n              it follows that   if\n                and  .\n              Expanding the dot product in  ,\n              the only terms that won't be zero are the ones that involve  .\n              This leaves us with\n               .\n             "
},
{
  "id": "exercise-236",
  "level": "2",
  "url": "chap_orthogonal_basis.html#exercise-236",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Find an orthogonal basis for the subspace   of  .\n       \n         \n       "
},
{
  "id": "exercise-237",
  "level": "2",
  "url": "chap_orthogonal_basis.html#exercise-237",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Let   be an orthogonal basis for   and,\n        for some   between 1 and  ,\n        let  .\n        Show that   is a basis for  .\n       "
},
{
  "id": "problem_orthog_decomp",
  "level": "2",
  "url": "chap_orthogonal_basis.html#problem_orthog_decomp",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Let   be a subspace of   for some  ,\n        and let   be an orthogonal basis for  .\n        Let   be a vector in  . and define   as\n         .\n       \n              Explain why   is in  .\n             \n              Where are  ,  ,\n               ,  ?\n             \n              Let  .\n              Show that   is in  .\n             \n              Take the dot product of   with  .\n             \n              Explain why   can be written as a sum of vectors,\n              one in   and one in  .\n             \n              Use parts (a) and (b).\n             \n              Suppose   and  ,\n              where   and   are in   and   and   are in  .\n              Show that   and  ,\n              so that the representation of   as a sum of a vector in   and a vector in   is unique.\n             \n              Collect terms in   and in  .\n             "
},
{
  "id": "exercise-239",
  "level": "2",
  "url": "chap_orthogonal_basis.html#exercise-239",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Use the result of problem   above and that   to show that\n          for a subspace   of  .\n        (See  \n        in  \n        for the definition of the sum of subspaces.)\n       "
},
{
  "id": "exercise-240",
  "level": "2",
  "url": "chap_orthogonal_basis.html#exercise-240",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Let   be an   matrix.\n        We showed that if   is an orthogonal matrix,\n        then   for any vectors   and   in  .\n        Now we ask if the converse of this statement is true.\n        That is, determine the validity of the following statement:\n        if   for any vectors   and   in  ,\n        then   is an orthogonal matrix?\n        Verify your answer.\n       \n        Consider\n          where   is the  th standard basis vector for  .\n       "
},
{
  "id": "ex_Reflection_matrices",
  "level": "2",
  "url": "chap_orthogonal_basis.html#ex_Reflection_matrices",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        In this exercise we examine reflection matrices.\n        In the following exercise we will show that the reflection and rotation matrices are the only   orthogonal matrices.\n        We will determine how to represent the reflection across a line through the origin in   as a matrix transformation.\n        The setup is as follows.\n        Let   be the line through the origin in   that makes an angle   with the positive  -axis as illustrated in  .\n       Reflecting across a line  . \n              Find a unit vector   in the direction of the line  .\n             \n              Let   be an arbitrary vector in   as represented in  .\n              Determine the components of the vectors\n                and  .\n              Reproduce  \n              and draw the vectors   and   in your figure.\n             \n              The vector labeled   is the reflection of the vector   across the line  .\n              Write   in terms of   and  .\n              Clearly explain your method.\n             reflection "
},
{
  "id": "exercise-242",
  "level": "2",
  "url": "chap_orthogonal_basis.html#exercise-242",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        In this exercise we will show that the only orthogonal\n          matrices are the rotation matrices\n          and the reflection matrices  \n        (see  ).\n        Throughout this exercise let  ,  ,  ,\n        and   be real numbers such that\n          is an orthogonal   matrix.\n        Let   and\n          be the columns of  .\n       \n              Explain why the terminal point of   in standard position lies on the unit circle.\n              Then explain why there is an angle   such that\n                and  .\n              What angle, specifically, is  ?\n              Draw a picture to illustrate.\n             \n              Think polar coordinates.\n             \n              A similar argument to (b) shows that there is an angle   such that  .\n              Given that   is an orthogonal matrix,\n              how must   be related to  ?\n              Use this result to find the two possibilities for   as a vector in terms of\n                and  .\n              (You will likely want to look up some trigonometric identities for this part of the problem.)\n             \n              What properties do the columns of an orthogonal matrix have?\n             \n              By considering the two possibilities from part (c),\n              show that   is either a rotation matrix or a reflection matrix.\n              Conclude that the only   orthogonal matrices are the reflection and rotation matrices.\n             \n              What happens if   and if  ?\n             "
},
{
  "id": "exercise-243",
  "level": "2",
  "url": "chap_orthogonal_basis.html#exercise-243",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Suppose   are orthogonal matrices of the same size.\n       \n              Show that   is also an orthogonal matrix.\n             \n              Show that   is also an orthogonal matrix.\n             \n              Show that   is also an orthogonal matrix.\n             "
},
{
  "id": "exercise-244",
  "level": "2",
  "url": "chap_orthogonal_basis.html#exercise-244",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              Any orthogonal subset of   is linearly independent.\n             \n              F\n             True\/False \n              Every single vector set is an orthogonal set.\n             True\/False \n              If   is an orthogonal set in   with exactly   nonzero vectors,\n              then   is a basis for  .\n             \n              T\n             True\/False \n              Every set of three linearly independent vectors in   is an orthogonal set.\n             True\/False \n              If   and   are   orthogonal matrices,\n              then   must also be an orthogonal matrix.\n             \n              F\n             True\/False \n              If the set\n                is an orthogonal set in  ,\n              then so is the set   for any scalars  ,\n               ,  ,  .\n             True\/False \n              If   is an orthogonal basis of  ,\n              then so is  ,\n               ,  ,\n                for any nonzero scalars  ,  ,\n               ,  .\n             \n              T\n             True\/False \n              If   is an   orthogonal matrix,\n              the rows of   form an orthonormal basis of  .\n             True\/False \n              If   is an orthogonal matrix,\n              any matrix obtained by interchanging columns of   is also an orthogonal matrix.\n             \n              T\n             "
},
{
  "id": "act_rot_det",
  "level": "2",
  "url": "chap_orthogonal_basis.html#act_rot_det",
  "type": "Project Activity",
  "number": "24.7",
  "title": "",
  "body": "\n      Let   be a rotation matrix in 3D. A rotation does not change lengths of vectors,\n      nor does it change angles between vectors.\n      Let  ,\n       ,\n      and   be the standard unit vectors in  .\n     \n            Explain why the columns of   form an orthonormal set.\n            \n           \n                How are  ,  ,\n                and   related to the columns of  ?\n               \n            Explain why   is an orthogonal matrix.\n            What must be true about  ?\n           \n            What is   and what is  ?\n           "
},
{
  "id": "p-4202",
  "level": "2",
  "url": "chap_orthogonal_basis.html#p-4202",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "normal "
},
{
  "id": "act_rot_x",
  "level": "2",
  "url": "chap_orthogonal_basis.html#act_rot_x",
  "type": "Project Activity",
  "number": "24.8",
  "title": "",
  "body": "\n      In this activity we see how to determine the rotation matrix around the   axis using the matrix\n        and a change of basis.\n     \n            Define a new ordered basis   so that our axis of rotation is the third vector.\n            So in this case the third vector in   will be  .\n            The other two vectors need to make   an orthonormal set.\n            So we have plenty of choices.\n            For example, we could set  .\n            Find the change of basis matrix\n              from   to  .\n           \n            Use the change of basis matrix from part (a) to find the change of basis matrix\n              from   to  .\n           \n            To find our rotation matrix around the   axis,\n            we can first change basis from   to  ,\n            then perform a rotation around the new   axis using  ,\n            then changing basis back from   to  .\n            In other words,\n             .\n            Find the entries of this matrix  .\n           "
},
{
  "id": "act_rot_y",
  "level": "2",
  "url": "chap_orthogonal_basis.html#act_rot_y",
  "type": "Project Activity",
  "number": "24.9",
  "title": "",
  "body": "\n      Repeat  \n      to find the 3D rotation around the   axis.\n     "
},
{
  "id": "act_rot_ex",
  "level": "2",
  "url": "chap_orthogonal_basis.html#act_rot_ex",
  "type": "Project Activity",
  "number": "24.10",
  "title": "",
  "body": "\n      In this activity we find the rotation around the axis given by the line  .\n      This line is in the direction of the vector  .\n      So we start with making a unit vector in the direction of   as the third vector in an ordered basis  .\n      The other two vectors need to make   an orthonormal set with  .\n     \n            Find a unit vector   in the direction of  .\n           \n            Show that   is orthogonal to the vector   from part (a).\n            Then find a unit vector   that is in the same direction as  .\n           \n            Let   be as in the previous part.\n            Now the trick is to find a third unit vector   so that\n              is an orthonormal set.\n            This can be done with the cross product.\n            If   and  ,\n            then the cross product   of   and   is the vector\n             .\n            (You can check that   is an orthogonal set that gives the correct determinant for the change of basis matrix.) Use the cross product to find a unit vector   so that\n              is an orthonormal set.\n           \n            Find the entries of the matrix  .\n           "
},
{
  "id": "act_rot_general",
  "level": "2",
  "url": "chap_orthogonal_basis.html#act_rot_general",
  "type": "Project Activity",
  "number": "24.11",
  "title": "",
  "body": "\n      Let   be a normal vector (nonzero) for our rotation.\n      We need to create an orthonormal basis\n        where   is a unit vector in the direction of   so that the change of basis matrix\n        has determinant  .\n     \n            Find, by inspection,\n            a vector   that is orthogonal to  .\n            \n           \n            You may need to consider some cases to ensure that   is not the zero vector.\n           \n            Once we have a normal vector   and a vector   orthogonal to  ,\n            the vector   gives us an orthogonal set  .\n            We then normalize each vector to create our orthonormal basis  .\n            Use this process to find the matrix that produces a\n              counterclockwise rotation around the normal vector  .\n           "
},
{
  "id": "chap_gram_schmidt",
  "level": "1",
  "url": "chap_gram_schmidt.html",
  "type": "Section",
  "number": "25",
  "title": "Projections onto Subspaces and the Gram-Schmidt Process in <span class=\"process-math\">\\(\\R^n\\)<\/span>",
  "body": "Projections onto Subspaces and the Gram-Schmidt Process in  \n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            What is a projection of a vector onto a subspace and why are such projections important?\n           \n         \n           \n            What is a projection of a vector orthogonal to a subspace and why are such orthogonal projections important?\n           \n         \n           \n            What is the Gram-Schmidt process and why is it useful?\n           \n         \n           \n            What is the QR-factorization of a matrix and why is it useful?\n           \n         Application: MIMO Systems \n    MIMO (Multiple Input Multiple Output) systems are widely used to increase data transmission rates and improve the quality of wireless communication systems.\n    MIMO is one of several forms of smart antenna technology,\n    and it makes use of multiple antennas at the transmitter to send signals and multiple antennas at the receiver to accept the signals.\n    MIMO systems transmit the same data on multiple streams,\n    which introduces redundancy into the system.\n    MIMO systems can utilize bounced and reflected signals to actually improve signal strength.\n    This is very useful in urban environments in the presence of large buildings and the absence of direct line-of-sight transmission.\n    MIMO systems can transmit several information streams in parallel\n    (known as spatial multiplexing),\n    allowing for increased data transformation rates.\n    The presence of multiple receiver antennas allows for greater reliability in the system.\n   A MIMO system. fading noise multipath fading Introduction \n    In many situations (least squares approximations, for example) want to find a vector   in a subspace   of   that is the  best  approximation to a vector   not in the subspace. A natural measure of  best  is to find a vector  in   if one exists, such that the distance from   to   is a minimum. This means we want to find   so that the quantity\n     \n    is as small as possible over all vectors   in   To do this, we will need to find a suitable projection of the vector   onto the subspace   We have already done this in the case that   is the span of a single vector -- we can project the vector   in the direction of  . Our goal is to generalize this to project a vector   onto an entire subspace in a useful way. \n   \n        Let   be a basis for a subspace   of   where   and   Note that   is an orthonormal basis for   Let   Note that   is the  plane and that   is not in   as illustrated in  .\n       The space   and vectors  ,  , and  . \n          Find the projection  of  onto   \n         \n           See Equation  .\n         \n          Find the projection  of  onto  \n         \n          Calculate the distance between  and  and  and   Which of  and  is closer to  ?\n         \n          Show that the vector  is in  and find the distance between  and  \n         \n          Part (4) shows that neither vector  nor  is the vector in  that is closest to   We should probably expect this neither projection uses the fact that the other vector might contribute to the closest vector. Let us instead consider the sum   Calculate the components of this vector  and determine the distance between  and   Which of the three vectors     and  in  is closest to  ?  \n         \n          A picture of       and  is shown in  . Draw in     and   Draw a picture of the vector  in  that appears to be closest to   Does this vector seem to have any relationship to     or  ? If yes, what relationship do you see?\n         Projections onto Subspaces and Orthogonal Projections \n     \n    gives an indication of how we can project a vector   in   onto a subspace   of  .\n    If we have an orthogonal basis for  ,\n    we can just add the projections of   onto each basis vector.\n    The resulting vector is called the projection of   onto the subspace  .\n    As we did with projections onto vectors,\n    we can also define the projection of   orthogonal to  .\n    Note that to make this all work out properly,\n    we will see that we need an orthogonal basis for  .\n   projection onto a subspace in  projection orthogonal to a subspace projection of the vector   in   onto  projection of   orthogonal to  \n    The notation   indicates that we expect this vector to be orthogonal to every vector in  .\n    We address this in the following activity.\n   \n      Let   in  ,\n      with   and  ,\n      and   as in  .\n      Recall that  .\n      Find the projection of   orthogonal to   and show directly that\n        is orthogonal to the basis vectors for  \n      (and hence to every vector in  ).\n     \n     \n    indicates that the vector   is in fact orthogonal to every vector in  .\n    To see that this is true in general,\n    let   be an orthogonal basis for a subspace   of   and let   be a vector in  .\n    Let\n     .\n   \n    Then   is the projection of   orthogonal to  .\n    We will show that   is orthogonal to every basis vector for  .\n    Since   is an orthogonal basis for  ,\n    we know that   for  .\n    So if   is between 1 and   then\n     .\n   \n    So the vector   is orthogonal to every basis vector for  ,\n    and therefore to every vector in  \n    ( ).\n    Because   is a basis for  ,\n    if   is in  , then\n     \n    for some scalars  ,  ,\n     ,  .\n    So\n     ,\n    and so   is orthogonal to every vector in  .\n   Best Approximations \n    We have seen that   is orthogonal to every vector in  ,\n    which suggests that   is in fact the vector in   that is closest to  .\n    We now verify this fact and conclude that\n      is the vector in   closest to   and therefore the best approximation of   by a vector in  .\n   \n        Let   be an orthogonal basis for a subspace   of   and let   be a vector in  .\n        Then\n         \n        for every vector   different from   in  .\n       \n      Let   be an orthogonal basis for a subspace   of   and let   be a vector in  .\n      Let   be a vector in  .\n      The vector   is in  ,\n      so is orthogonal to  .\n      Thus, the dotted triangle whose vertices are the tips of the vectors  ,\n       ,\n      and   in   is a right triangle.\n     The best approximation to   in  \n      The Pythagorean theorem then shows that\n       .\n     \n      Now  ,\n      so  .\n      This implies\n       \n      and it follows that\n       .\n     \n    This theorem shows that the distance from\n      to   is less than the distance from any other vector in   to  .\n    So   is the best approximation to   of all the vectors in  .\n   \n    If   and  ,\n    then the square of the error in approximating   by   is given by\n     .\n   least squares approximation \n      Let   and let   in  .\n      Find the best approximation in   to the vector   in  .\n     The Gram-Schmidt Process \n    We have seen that orthogonal bases make computations very convenient.\n    However, until now we had no convenient way to construct orthogonal bases.\n    The problem we address in this section is how to create an orthogonal basis from any basis.\n   \n      Let   in  ,\n      where  ,\n       ,\n      and  .\n      Our goal in this preview activity is to begin to understand how we can find an orthogonal set\n        in   so that  .\n      To begin, we could start by letting  .\n     \n            Now we want to find a vector in   that is orthogonal to  .\n            Let  .\n            Explain why   is in   and is orthogonal to  .\n            Then calculate the vector  .\n           \n            Next we need to find a third vector   that is in   and is orthogonal to both   and  .\n            Let  .\n            Explain why   is in   and is orthogonal to both   and  .\n            Then calculate the vector  .\n           \n            Explain why the set   is an orthogonal basis for  .\n           \n     \n    shows the first steps of the Gram-Schmidt process to construct an orthogonal basis from any basis of a subspace in  .\n    To understand why the process works in general,\n    let   be a basis for a subspace   of  .\n    Let   and let  .\n    Since   we have that  .\n   \n    The vectors   and   are possibly not orthogonal,\n    but we know the orthogonal projection of   onto\n      is orthogonal to  .\n    Let\n     .\n   \n    Then   is an orthogonal set.\n    Note that  ,\n    and the fact that   implies that  .\n    So the set   is linearly independent,\n    being a set of non-zero orthogonal vectors.\n    Now the question is whether  .\n    Note that   is a linear combination of   and  ,\n    so   is in  .\n    Since   is a 2-dimensional subspace of the 2-dimensional space  ,\n    it must be true that  .\n   \n    Now we take the next step, adding   into the mix.\n    The vector\n     \n    is orthogonal to both   and   and,\n    by construction,\n      is a linear combination of  ,\n     , and  .\n    So   is in  .\n    The fact that   implies that   and\n      is a linearly independent set.\n    Since   is a 3-dimensional subspace of the 3-dimensional space  ,\n    we conclude that  .\n   \n    We continue inductively in this same manner.\n    If we have constructed a set  ,\n     ,  ,  ,\n      of   orthogonal vectors such that\n     ,\n    then we let\n     ,\n    where\n     .\n   \n    We know that   is orthogonal to  ,\n     ,\n     ,  .\n    Since  ,  ,\n     ,  ,\n    and   are all in\n      we see that   is also in  .\n    Since   implies that   and\n      is a linearly independent set.\n    Then   is a  -dimensional subspace of the  -dimensional space  ,\n    so it follows that\n     .\n   \n    This process will end when we have an orthogonal set  ,\n     ,  ,\n     ,\n      with  ,  ,\n     ,  ,   =  .\n   \n    We summarize the process in the following theorem.\n   The Gram-Schmidt Process Gram-Schmidt Process in  \n        Let   be a basis for a subspace   of  .\n        The set   defined by\n         \n             \n               ,\n             \n           \n             \n               ,\n             \n           \n             \n               ,\n               \n             \n           \n             \n               .\n             \n           \n       \n        is an orthogonal basis for  .\n        Moreover,\n         \n        for  .\n       \n    The Gram-Schmidt process builds an orthogonal basis\n      for us from a given basis.\n    To make an orthonormal basis  ,\n    all we need do is normalize each basis vector:\n    that is, for each  , we let\n     .\n   \n      Let  .\n     \n            Use the Gram-Schmidt process to find an orthogonal basis for  .\n            \n           \n                Order your vectors carefully to minimize computation.\n               \n            Find the projection of the vector   onto  .\n           The QR Factorization of a Matrix sparse \n      Let  .\n     \n            Find an orthonormal basis   for  .\n            Let   be the matrix whose columns are these orthonormal basis vectors.\n           \n            Write the columns of   as linear combinations of the columns of  .\n            That is, if  ,\n            find   and  .\n            Let  .\n           \n            Find the product   and compare to  .\n           \n     \n    contains the main ideas to find the QR factorization of a matrix.\n    Let\n     \n    be an   matrix with rank \n    Recall that the rank of a matrix   is the dimension of the column space of  .\n       .\n    We can use the Gram-Schmidt process to find an orthonormal basis   for  .\n    Recall also that   for any   between 1 and  .\n    Let\n     .\n   \n    If   is between 1 and  ,\n    then   is in   and\n     \n    for some scalars  ,\n     ,  ,  .\n    Then\n     .\n   \n    If we let   for   from 1 to  , then\n     .\n   \n    This is the QR factorization of   into the product\n     \n    where the columns of   form an orthonormal basis for   and\n     \n    is an upper triangular matrix.\n    Note that   is an   matrix and   is an\n      matrix with   for each  .\n   \n      The set  ,\n      where   and\n        is an orthogonal basis for the column space of a\n        matrix  .\n      Moreover,   and  .\n      What is  ?\n     \n    The QR factorization provides a widely used algorithm\n    (the QR algorithm)\n    for approximating all of the eigenvalues of a matrix.\n    The computer system MATLAB utilizes four versions of the QR algorithm to approximate the eigenvalues of real symmetric matrices,\n    eigenvalues of real nonsymmetric matrices,\n    eigenvalues of pairs of complex matrices,\n    and singular values of general matrices.\n   \n    The algorithm works as follows.\n     \n         \n          Start with an   matrix  .\n          Let  .\n         \n       \n         \n          Find the QR factorization for   and write it as  ,\n          where   is orthogonal and   is upper triangular.\n         \n       \n         \n          Let  .\n          Find the QR factorization of   and write it as  .\n         \n       \n         \n          Let  .\n          Find the QR factorization of   and write it as  .\n         \n       \n         \n          Continue in this manner to obtain a sequence   where\n            and  .\n         \n       \n   \n    Note that   and so all of the matrices   are similar to each other and therefore all have the same eigenvalues.\n    We won't go into the details,\n    but it can be shown that if the eigenvalues of   are real and have distinct absolute values,\n    then the sequence   converges to an upper triangular matrix with the eigenvalues of   as the diagonal entries.\n    If some of the eigenvalues of   are complex,\n    then the sequence   converges to a block upper triangular matrix,\n    where the diagonal blocks are either  \n    (approximating the real eigenvalues of  )\n    or  \n    (which provide a pair of conjugate complex eigenvalues of  ).\n   Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Let   be the subspace of   spanned by  ,\n         ,\n        and  .\n       \n              Use the Gram-Schmidt process to find an orthonormal basis for the subspace   of   spanned by  ,\n               ,\n              and  .\n             \n              First note that  ,\n               , and   are linearly independent.\n              We let   and the Gram-Schmidt process gives us\n               \n              and\n               .\n              So   is an orthogonal basis for  .\n              An orthonormal basis is found by dividing each vector by its magnitude, so\n               \n              is an orthonormal basis for  .\n             \n              Find a QR factorization of the matrix  .\n             \n              Technology shows that the reduced row echelon form of   is  ,\n              so the columns of   are linearly independent and   has rank  .\n              From part (a) we have an orthogonal basis for the span of the first three columns of  .\n              To find a fourth vector to add so that the span is  ,\n              we apply the Gram-Schmidt process one more time with  :\n               .\n              So   is an orthonormal basis for  , where\n               \n              This makes\n               .\n              To find the matrix  ,\n              we write the columns of   in terms of the basis vectors  ,\n               ,\n               , and  .\n              Technology shows that the reduced row echelon form of   is\n               .\n              So\n               .\n             \n        Let  .\n        Find the vector in   that is closest to the vector  .\n        Provide a numeric measure of the error in approximating   by this vector.\n       \n     \n              Our job is to find  .\n              To do this, we need an orthogonal basis of  .\n              Let  ,\n               ,\n              and  .\n              Technology shows that each column of the matrix   is a pivot column,\n              so the set   is a basis for  .\n              We apply the Gram-Schmidt process to this basis to obtain an orthogonal basis   of  .\n              We start with  , then\n               \n              and\n               .\n              Then, letting   we have\n               .\n              The norm of   tells us how well our projection\n                approximates  .\n              Now\n               ,\n              so   is one unit away from  .\n     \n       Summary \n       \n        The projection of the vector   in   onto   is the vector\n         ,\n        where   is the a subspace of   with orthogonal basis  .\n        These projections are important in that\n          is the best approximation of the vector   by a vector in   in the least squares sense.\n       \n     \n       \n        With   as in (a),\n        the projection of   orthogonal to   is the vector\n         .\n        The norm of   provides a measure of how well\n          approximates the vector  .\n       \n     \n       \n        The Gram-Schmidt process produces an orthogonal basis from any basis.\n        It works as follows.\n        Let   be a basis for a subspace   of  .\n        The set  ,  ,\n         ,  ,   defined by\n         \n             \n               ,\n             \n           \n             \n               ,\n             \n           \n             \n               ,\n               \n             \n           \n             \n               .\n             \n           \n      \n      is an orthogonal basis for  .\n      Moreover,\n       \n      for each   between 1 and  .\n       \n     \n       \n        The QR factorization has applications to solving least squares problems and approximating eigenvalues of matrices.\n        The QR factorization writes an\n          matrix with rank   as a product  ,\n        where the columns of   form an orthonormal basis for   and\n         \n        is an upper triangular matrix.\n       \n     \n        Let   in  ,\n        with   and  ,\n        and  .\n       \n              Find   and  \n             \n               ,\n               \n             \n              Find the vector in   that is closest to  .\n              How close is this vector to  ?\n             \n               ,  \n             \n        Let   and let   in  .\n        Find the best approximation in   to the vector\n          in   and give a numeric estimate of how good this approximation is.\n       \n        In this exercise we determine the least-squares line\n        (the line of best fit in the least squares sense)\n        to a set of   data points  ,\n         ,  ,\n          in the plane.\n        In this case,\n        we want to fit a line of the form   to the data.\n        If the data were to lie on a line,\n        then we would have a solution to the system\n         \n        This system can be written as\n         ,\n        where  ,\n         ,\n        and  .\n        If the data does not lie on a line,\n        then the system won't have a solution.\n        Instead, we want to minimize the square of the distance between   and a vector of the form  .\n        That is, minimize\n         .\n        Rephrasing this in terms of projections,\n        we are looking for the vector in\n          that is closest to  .\n        In other words,\n        the values of   and   will occur as the weights when we write\n          as a linear combination of   and  .\n        The one wrinkle in this problem is that we need an orthogonal basis for   to find this projection.\n        Find the least squares line for the data points  ,\n          and   in  .\n       \n         .\n       \n        Each set   is linearly independent.\n        Use the Gram-Schmidt process to create an orthogonal set of vectors with the same span as  .\n        Then find an orthonormal basis for the same span.\n       \n                in  \n             \n                in  \n             \n                in  \n             \n        Let   be a set of linearly dependent vectors in   for some integer  .\n        What is the result if the Gram-Schmidt process is applied to the set  ?\n        Explain.\n       \n        The Gram-Schmidt process will return an orthogonal set with   non-zero vectors and the rest all zero vectors.\n       \n        A fellow student wants to find a QR factorization for a   matrix.\n        What would you tell this student and why?\n       \n        Find the QR factorizations of the given matrices.\n       \n               \n             \n               \n             \n               \n             \n               ,  \n             \n               .\n             \n               ,  \n             \n        Find an orthonormal basis   of   such that  .\n       \n              Let  .\n              A QR factorization of   has   with\n                and  ,\n              and   and  .\n              Calculate the dot products\n                for   and   between   and  .\n              How are these dot products connected to the entries of  ?\n             \n               \n             \n              Explain why the result of part (a) is true in general.\n              That is, if   has QR factorization with\n                and  ,\n              then  .\n             \n              See the discussion after  .\n             \n        Upper triangular matrices play an important role in the QR decomposition.\n        In this exercise we examine two properties of upper triangular matrices.\n       \n              Show that if   and   are upper triangular\n                matrices with positive diagonal entries,\n              then   is also an upper triangular\n                matrices with positive diagonal entries.\n             \n              Show that if   is an invertible upper triangular matrix with positive diagonal entries,\n              then   is also an upper triangular matrix with positive diagonal entries.\n             \n        In this exercise we demonstrate that the QR decomposition of an\n          matrix with linearly independent columns is unique.\n       \n              Suppose that  ,\n              where   is an   matrix with orthogonal columns and   is an\n                upper triangular matrix.\n              Show that   is invertible.\n              \n             \n              What can we say about   if  ?\n             \n              Note that if  , then  . Use the fact that the columns of   are linearly independent.\n             \n              Show that the only   orthogonal upper triangular matrix with positive diagonal entries is the identity matrix  .\n              \n             \n              Let   be an\n                orthogonal upper triangular matrices with positive diagonal entries.\n              What does that tell us about  ?\n             \n              First,   when   and  . Use the fact that   is upper triangular to show that  .\n             \n              Show that if   and   are\n                matrices with orthogonal columns,\n              and   is a matrix such that  ,\n              then   is an orthogonal matrix.\n              \n             \n              Write   in terms of   and  .\n             \n              Use the fact that   to show that  \n             \n              Suppose that   and   are\n                matrices with orthogonal columns and   and   are\n                upper triangular matrices such that\n               .\n              Show that   and  .\n              Conclude that the QR factorization of a matrix is unique.\n              \n             \n              Use the previous parts of this problem along with the results of  .\n             \n              Follow the hint.\n             \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n        Throughout, let   be a vector space.\n       True\/False \n              If   is a basis for a subspace   of  ,\n              then the vector   is the vector in   closest to  .\n             \n              F\n             True\/False \n              If   is a subspace of  ,\n              then the vector   is orthogonal to every vector in  .\n             True\/False \n              If  ,\n               ,   are vectors in  ,\n              then the Gram-Schmidt process constructs an orthogonal set of vectors\n                with the same span as  .\n             \n              T\n             True\/False \n              Any set\n                of orthogonal vectors in   can be extended to an orthogonal basis of  .\n             True\/False \n              If   is an\n                matrix with  ,\n              then the rows of   form an orthogonal set.\n             \n              T\n             True\/False \n              Every nontrivial subspace of   has an orthogonal basis.\n             True\/False \n              If   is a subspace of   satisfying  ,\n              then  .\n             \n              T\n             Project: MIMO Systems and Householder Transformations \n      In a simplified model,\n      a MIMO system will have   transmitting antennas and   receiving antennas.\n      We record a transmitted symbol in a vector  \n      (one   for each transmitting antenna)\n      and the received symbol as a vector  \n      (one   for each received symbol).\n     \n      Between each transmit antenna and each receiver antenna is a fading channel.\n      The MIMO system is the collection of fading channels.\n      If we let   be the fading between the  th transmitter and  th receiver,\n      then we can represent this multipath fading as an\n        matrix  .\n      We assume that there is also some noise in the received signal that we represent by  \n      (for the  th receiving antenna).\n      Our MIMO system is then represented as the matrix-vector equation\n       ,\n      where  .\n      The goal is to reproduce the original signal   from the received signal  .\n      This is where the QR decomposition comes into play.\n     \n        To see why and how the QR decomposition is used in MIMO systems,\n        we begin with an example.\n        Assume that we have two transmitters and three receivers,\n        and suppose  .\n        From this point,\n        to simplify the situation we assume that noise is negligible and consider the system  .\n        Assume that the received signal is  .\n       \n              Show that the system   is inconsistent.\n             \n              When we receive a signal, we need to interpret it,\n              and that is difficult if we cannot find a solution.\n              One reason that the system might not have a solution is that the elements in   have to come from a specified alphabet,\n              and so we are looking for solutions that are in that alphabet,\n              and there may be no direct solution in that alphabet.\n              As a result, we generally have to find a\n               best \n              solution in the alphabet space.\n              That can be done with the QR decomposition.\n              Assume that   has a QR decomposition   with   having orthonormal columns and   a diagonal matrix.\n              Explain how the equation   can be transformed into\n               .\n             \n              Return to our example of  .\n              Assume that\n               .\n              Use   to find   if  .\n             \n      In  \n      we saw that a QR decomposition allows us to replace the system\n        with an upper triangular system   that has a solution.\n      This solution is what is called a least squares solution\n      (which we will discuss in more detail in a later section).\n      The key to all of this is finding an efficient way to calculate a QR decomposition.\n      This is the focus of the remainder of this project.\n     Householder Transformations and the QR Decomposition \n      There are three main ways to compute a QR decomposition.\n       \n           \n            The method discussed in this section uses Gram-Schmidt orthogonalization.\n            This method is not recommended for numerical use as it is inherently numerically unstable.\n            Under certain circumstances,\n            cancellation can destroy the accuracy of the computed vectors and the resulting vectors may not be orthogonal.\n           \n         \n           \n            The method that we will discuss in this project using Householder transformations\n            (developed by Alston S. Householder).\n            In Gaussian elimination,\n            we can zero out entries below the diagonal by multiplying by elementary matrices to perform row operations.\n            Householder transformations do something similar,\n            allowing us to replace columns with columns of mostly zeros,\n            which then allow for more efficient computations.\n           \n         \n           \n            Givens (or Jacobi) rotations\n            (used by W. Givens and originally invented by Jacobi for use with in solving the symmetric eigenvalue problem)\n            is another method that allows us to selectively produce zeros into a matrix.\n            We will focus on Householder transformations in this project.\n           \n         \n     Householder transformation \n        We will discover some properties of Householder transformations in this activity.\n       \n              Let   be any vector and let   be a unit vector.\n              In this part of the exercise we show that,\n              with a suitable choice of  ,\n              the Householder transformation   transforms   into a vector of the same length parallel to  .\n              That is,\n               \n              for some scalar  .\n              \n              A similar argument shows that  .\n              Letting   gives us the following result.\n               \n                    Let   be any vector,\n                    let  , and let  .\n                    Then  .\n                   \n             \n              Let  , and let  .\n              Apply   to  ,\n              factor out  ,\n              and ultimately show that  .\n             \n              The Householder matrix   has two other important properties.\n              Show that   is symmetric and orthogonal.\n             \n      There are many advantages to using Householder matrices.\n      One is that instead of having to store all entries of the matrix  ,\n      all we need to store is the entries of  .\n      Another advantage of Householder transformations is that they can be used to efficiently calculate QR decompositions.\n      The next project activity shows how to use  \n      and Householder transformations to compute a QR decomposition through an example.\n     \n        Assume that we have five receiving antennas and three transmitting antennas, and that\n         .\n       \n        (We use   now instead of   so as to avoid confusion with Householder matrices.) The calculations in this activity can get rather messy,\n        so use appropriate technology and round entries to four decimal places to the right of the decimal.\n       \n              Let   be the first column of  .\n              Identify an appropriate Householder transformation   so that   is a constant multiple of  .\n              Determine the matrix  ,\n              where  . (Special note:\n              when deciding which of   to use to create  ,\n              it is best to use the one whose sign is the same as  \n              (the first entry of  ).\n              We won't go into the details,\n              but this helps prevent problems due to cancellation,)\n             \n              Next we consider just the bottom right   portion\n                of the matrix   found in part (a).\n             \n                    Repeat the process on   to find a Householder matrix\n                      that will make the first column of\n                      have all zeros below its first entry.\n                   \n                    Let\n                     .\n                    Explain what the matrix   is.\n                   \n              As a final step,\n              we consider just the bottom right   portion\n                of the matrix   and repeat the process.\n             \n                    Find a matrix   that produces zeros below the first entry.\n                   \n                    Let\n                     .\n                    What matrix is  ?\n                    Why?\n                   \n              Explain why   is an orthogonal matrix.\n              Then find an upper triangular matrix   such that  .\n             thin "
},
{
  "id": "objectives-25",
  "level": "2",
  "url": "chap_gram_schmidt.html#objectives-25",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            What is a projection of a vector onto a subspace and why are such projections important?\n           \n         \n           \n            What is a projection of a vector orthogonal to a subspace and why are such orthogonal projections important?\n           \n         \n           \n            What is the Gram-Schmidt process and why is it useful?\n           \n         \n           \n            What is the QR-factorization of a matrix and why is it useful?\n           \n         "
},
{
  "id": "F_MIMO",
  "level": "2",
  "url": "chap_gram_schmidt.html#F_MIMO",
  "type": "Figure",
  "number": "25.1",
  "title": "",
  "body": "A MIMO system. "
},
{
  "id": "p-4228",
  "level": "2",
  "url": "chap_gram_schmidt.html#p-4228",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "fading noise multipath fading "
},
{
  "id": "pa_6_e_1",
  "level": "2",
  "url": "chap_gram_schmidt.html#pa_6_e_1",
  "type": "Preview Activity",
  "number": "25.1",
  "title": "",
  "body": "\n        Let   be a basis for a subspace   of   where   and   Note that   is an orthonormal basis for   Let   Note that   is the  plane and that   is not in   as illustrated in  .\n       The space   and vectors  ,  , and  . \n          Find the projection  of  onto   \n         \n           See Equation  .\n         \n          Find the projection  of  onto  \n         \n          Calculate the distance between  and  and  and   Which of  and  is closer to  ?\n         \n          Show that the vector  is in  and find the distance between  and  \n         \n          Part (4) shows that neither vector  nor  is the vector in  that is closest to   We should probably expect this neither projection uses the fact that the other vector might contribute to the closest vector. Let us instead consider the sum   Calculate the components of this vector  and determine the distance between  and   Which of the three vectors     and  in  is closest to  ?  \n         \n          A picture of       and  is shown in  . Draw in     and   Draw a picture of the vector  in  that appears to be closest to   Does this vector seem to have any relationship to     or  ? If yes, what relationship do you see?\n         "
},
{
  "id": "definition-58",
  "level": "2",
  "url": "chap_gram_schmidt.html#definition-58",
  "type": "Definition",
  "number": "25.3",
  "title": "",
  "body": "projection onto a subspace in  projection orthogonal to a subspace projection of the vector   in   onto  projection of   orthogonal to  "
},
{
  "id": "act_6_e_orth_projection",
  "level": "2",
  "url": "chap_gram_schmidt.html#act_6_e_orth_projection",
  "type": "Activity",
  "number": "25.2",
  "title": "",
  "body": "\n      Let   in  ,\n      with   and  ,\n      and   as in  .\n      Recall that  .\n      Find the projection of   orthogonal to   and show directly that\n        is orthogonal to the basis vectors for  \n      (and hence to every vector in  ).\n     "
},
{
  "id": "theorem-60",
  "level": "2",
  "url": "chap_gram_schmidt.html#theorem-60",
  "type": "Theorem",
  "number": "25.4",
  "title": "",
  "body": "\n        Let   be an orthogonal basis for a subspace   of   and let   be a vector in  .\n        Then\n         \n        for every vector   different from   in  .\n       \n      Let   be an orthogonal basis for a subspace   of   and let   be a vector in  .\n      Let   be a vector in  .\n      The vector   is in  ,\n      so is orthogonal to  .\n      Thus, the dotted triangle whose vertices are the tips of the vectors  ,\n       ,\n      and   in   is a right triangle.\n     The best approximation to   in  \n      The Pythagorean theorem then shows that\n       .\n     \n      Now  ,\n      so  .\n      This implies\n       \n      and it follows that\n       .\n     "
},
{
  "id": "p-4253",
  "level": "2",
  "url": "chap_gram_schmidt.html#p-4253",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "least squares approximation "
},
{
  "id": "activity-97",
  "level": "2",
  "url": "chap_gram_schmidt.html#activity-97",
  "type": "Activity",
  "number": "25.3",
  "title": "",
  "body": "\n      Let   and let   in  .\n      Find the best approximation in   to the vector   in  .\n     "
},
{
  "id": "pa_6_d_2_no_inner_product",
  "level": "2",
  "url": "chap_gram_schmidt.html#pa_6_d_2_no_inner_product",
  "type": "Preview Activity",
  "number": "25.4",
  "title": "",
  "body": "\n      Let   in  ,\n      where  ,\n       ,\n      and  .\n      Our goal in this preview activity is to begin to understand how we can find an orthogonal set\n        in   so that  .\n      To begin, we could start by letting  .\n     \n            Now we want to find a vector in   that is orthogonal to  .\n            Let  .\n            Explain why   is in   and is orthogonal to  .\n            Then calculate the vector  .\n           \n            Next we need to find a third vector   that is in   and is orthogonal to both   and  .\n            Let  .\n            Explain why   is in   and is orthogonal to both   and  .\n            Then calculate the vector  .\n           \n            Explain why the set   is an orthogonal basis for  .\n           "
},
{
  "id": "thm_6_d_Gram_Schmidt_noips",
  "level": "2",
  "url": "chap_gram_schmidt.html#thm_6_d_Gram_Schmidt_noips",
  "type": "Theorem",
  "number": "25.6",
  "title": "The Gram-Schmidt Process.",
  "body": "The Gram-Schmidt Process Gram-Schmidt Process in  \n        Let   be a basis for a subspace   of  .\n        The set   defined by\n         \n             \n               ,\n             \n           \n             \n               ,\n             \n           \n             \n               ,\n               \n             \n           \n             \n               .\n             \n           \n       \n        is an orthogonal basis for  .\n        Moreover,\n         \n        for  .\n       "
},
{
  "id": "act_6_d_gs_examples",
  "level": "2",
  "url": "chap_gram_schmidt.html#act_6_d_gs_examples",
  "type": "Activity",
  "number": "25.5",
  "title": "",
  "body": "\n      Let  .\n     \n            Use the Gram-Schmidt process to find an orthogonal basis for  .\n            \n           \n                Order your vectors carefully to minimize computation.\n               \n            Find the projection of the vector   onto  .\n           "
},
{
  "id": "p-4279",
  "level": "2",
  "url": "chap_gram_schmidt.html#p-4279",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "sparse "
},
{
  "id": "act_6_e_QR",
  "level": "2",
  "url": "chap_gram_schmidt.html#act_6_e_QR",
  "type": "Activity",
  "number": "25.6",
  "title": "",
  "body": "\n      Let  .\n     \n            Find an orthonormal basis   for  .\n            Let   be the matrix whose columns are these orthonormal basis vectors.\n           \n            Write the columns of   as linear combinations of the columns of  .\n            That is, if  ,\n            find   and  .\n            Let  .\n           \n            Find the product   and compare to  .\n           "
},
{
  "id": "activity-100",
  "level": "2",
  "url": "chap_gram_schmidt.html#activity-100",
  "type": "Activity",
  "number": "25.7",
  "title": "",
  "body": "\n      The set  ,\n      where   and\n        is an orthogonal basis for the column space of a\n        matrix  .\n      Moreover,   and  .\n      What is  ?\n     "
},
{
  "id": "example-50",
  "level": "2",
  "url": "chap_gram_schmidt.html#example-50",
  "type": "Example",
  "number": "25.7",
  "title": "",
  "body": "\n        Let   be the subspace of   spanned by  ,\n         ,\n        and  .\n       \n              Use the Gram-Schmidt process to find an orthonormal basis for the subspace   of   spanned by  ,\n               ,\n              and  .\n             \n              First note that  ,\n               , and   are linearly independent.\n              We let   and the Gram-Schmidt process gives us\n               \n              and\n               .\n              So   is an orthogonal basis for  .\n              An orthonormal basis is found by dividing each vector by its magnitude, so\n               \n              is an orthonormal basis for  .\n             \n              Find a QR factorization of the matrix  .\n             \n              Technology shows that the reduced row echelon form of   is  ,\n              so the columns of   are linearly independent and   has rank  .\n              From part (a) we have an orthogonal basis for the span of the first three columns of  .\n              To find a fourth vector to add so that the span is  ,\n              we apply the Gram-Schmidt process one more time with  :\n               .\n              So   is an orthonormal basis for  , where\n               \n              This makes\n               .\n              To find the matrix  ,\n              we write the columns of   in terms of the basis vectors  ,\n               ,\n               , and  .\n              Technology shows that the reduced row echelon form of   is\n               .\n              So\n               .\n             "
},
{
  "id": "example-51",
  "level": "2",
  "url": "chap_gram_schmidt.html#example-51",
  "type": "Example",
  "number": "25.8",
  "title": "",
  "body": "\n        Let  .\n        Find the vector in   that is closest to the vector  .\n        Provide a numeric measure of the error in approximating   by this vector.\n       \n     \n              Our job is to find  .\n              To do this, we need an orthogonal basis of  .\n              Let  ,\n               ,\n              and  .\n              Technology shows that each column of the matrix   is a pivot column,\n              so the set   is a basis for  .\n              We apply the Gram-Schmidt process to this basis to obtain an orthogonal basis   of  .\n              We start with  , then\n               \n              and\n               .\n              Then, letting   we have\n               .\n              The norm of   tells us how well our projection\n                approximates  .\n              Now\n               ,\n              so   is one unit away from  .\n     \n       "
},
{
  "id": "exercise-245",
  "level": "2",
  "url": "chap_gram_schmidt.html#exercise-245",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Let   in  ,\n        with   and  ,\n        and  .\n       \n              Find   and  \n             \n               ,\n               \n             \n              Find the vector in   that is closest to  .\n              How close is this vector to  ?\n             \n               ,  \n             "
},
{
  "id": "exercise-246",
  "level": "2",
  "url": "chap_gram_schmidt.html#exercise-246",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Let   and let   in  .\n        Find the best approximation in   to the vector\n          in   and give a numeric estimate of how good this approximation is.\n       "
},
{
  "id": "exercise-247",
  "level": "2",
  "url": "chap_gram_schmidt.html#exercise-247",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        In this exercise we determine the least-squares line\n        (the line of best fit in the least squares sense)\n        to a set of   data points  ,\n         ,  ,\n          in the plane.\n        In this case,\n        we want to fit a line of the form   to the data.\n        If the data were to lie on a line,\n        then we would have a solution to the system\n         \n        This system can be written as\n         ,\n        where  ,\n         ,\n        and  .\n        If the data does not lie on a line,\n        then the system won't have a solution.\n        Instead, we want to minimize the square of the distance between   and a vector of the form  .\n        That is, minimize\n         .\n        Rephrasing this in terms of projections,\n        we are looking for the vector in\n          that is closest to  .\n        In other words,\n        the values of   and   will occur as the weights when we write\n          as a linear combination of   and  .\n        The one wrinkle in this problem is that we need an orthogonal basis for   to find this projection.\n        Find the least squares line for the data points  ,\n          and   in  .\n       \n         .\n       "
},
{
  "id": "exercise-248",
  "level": "2",
  "url": "chap_gram_schmidt.html#exercise-248",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Each set   is linearly independent.\n        Use the Gram-Schmidt process to create an orthogonal set of vectors with the same span as  .\n        Then find an orthonormal basis for the same span.\n       \n                in  \n             \n                in  \n             \n                in  \n             "
},
{
  "id": "exercise-249",
  "level": "2",
  "url": "chap_gram_schmidt.html#exercise-249",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Let   be a set of linearly dependent vectors in   for some integer  .\n        What is the result if the Gram-Schmidt process is applied to the set  ?\n        Explain.\n       \n        The Gram-Schmidt process will return an orthogonal set with   non-zero vectors and the rest all zero vectors.\n       "
},
{
  "id": "exercise-250",
  "level": "2",
  "url": "chap_gram_schmidt.html#exercise-250",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        A fellow student wants to find a QR factorization for a   matrix.\n        What would you tell this student and why?\n       "
},
{
  "id": "exercise-251",
  "level": "2",
  "url": "chap_gram_schmidt.html#exercise-251",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        Find the QR factorizations of the given matrices.\n       \n               \n             \n               \n             \n               \n             \n               ,  \n             \n               .\n             \n               ,  \n             "
},
{
  "id": "exercise-252",
  "level": "2",
  "url": "chap_gram_schmidt.html#exercise-252",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Find an orthonormal basis   of   such that  .\n       "
},
{
  "id": "exercise-253",
  "level": "2",
  "url": "chap_gram_schmidt.html#exercise-253",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n              Let  .\n              A QR factorization of   has   with\n                and  ,\n              and   and  .\n              Calculate the dot products\n                for   and   between   and  .\n              How are these dot products connected to the entries of  ?\n             \n               \n             \n              Explain why the result of part (a) is true in general.\n              That is, if   has QR factorization with\n                and  ,\n              then  .\n             \n              See the discussion after  .\n             "
},
{
  "id": "ex_6_e_upper_triangular_props",
  "level": "2",
  "url": "chap_gram_schmidt.html#ex_6_e_upper_triangular_props",
  "type": "Exercise",
  "number": "10",
  "title": "",
  "body": "\n        Upper triangular matrices play an important role in the QR decomposition.\n        In this exercise we examine two properties of upper triangular matrices.\n       \n              Show that if   and   are upper triangular\n                matrices with positive diagonal entries,\n              then   is also an upper triangular\n                matrices with positive diagonal entries.\n             \n              Show that if   is an invertible upper triangular matrix with positive diagonal entries,\n              then   is also an upper triangular matrix with positive diagonal entries.\n             "
},
{
  "id": "exercise-255",
  "level": "2",
  "url": "chap_gram_schmidt.html#exercise-255",
  "type": "Exercise",
  "number": "11",
  "title": "",
  "body": "\n        In this exercise we demonstrate that the QR decomposition of an\n          matrix with linearly independent columns is unique.\n       \n              Suppose that  ,\n              where   is an   matrix with orthogonal columns and   is an\n                upper triangular matrix.\n              Show that   is invertible.\n              \n             \n              What can we say about   if  ?\n             \n              Note that if  , then  . Use the fact that the columns of   are linearly independent.\n             \n              Show that the only   orthogonal upper triangular matrix with positive diagonal entries is the identity matrix  .\n              \n             \n              Let   be an\n                orthogonal upper triangular matrices with positive diagonal entries.\n              What does that tell us about  ?\n             \n              First,   when   and  . Use the fact that   is upper triangular to show that  .\n             \n              Show that if   and   are\n                matrices with orthogonal columns,\n              and   is a matrix such that  ,\n              then   is an orthogonal matrix.\n              \n             \n              Write   in terms of   and  .\n             \n              Use the fact that   to show that  \n             \n              Suppose that   and   are\n                matrices with orthogonal columns and   and   are\n                upper triangular matrices such that\n               .\n              Show that   and  .\n              Conclude that the QR factorization of a matrix is unique.\n              \n             \n              Use the previous parts of this problem along with the results of  .\n             \n              Follow the hint.\n             "
},
{
  "id": "exercise-256",
  "level": "2",
  "url": "chap_gram_schmidt.html#exercise-256",
  "type": "Exercise",
  "number": "12",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n        Throughout, let   be a vector space.\n       True\/False \n              If   is a basis for a subspace   of  ,\n              then the vector   is the vector in   closest to  .\n             \n              F\n             True\/False \n              If   is a subspace of  ,\n              then the vector   is orthogonal to every vector in  .\n             True\/False \n              If  ,\n               ,   are vectors in  ,\n              then the Gram-Schmidt process constructs an orthogonal set of vectors\n                with the same span as  .\n             \n              T\n             True\/False \n              Any set\n                of orthogonal vectors in   can be extended to an orthogonal basis of  .\n             True\/False \n              If   is an\n                matrix with  ,\n              then the rows of   form an orthogonal set.\n             \n              T\n             True\/False \n              Every nontrivial subspace of   has an orthogonal basis.\n             True\/False \n              If   is a subspace of   satisfying  ,\n              then  .\n             \n              T\n             "
},
{
  "id": "act_QR_example_1",
  "level": "2",
  "url": "chap_gram_schmidt.html#act_QR_example_1",
  "type": "Project Activity",
  "number": "25.8",
  "title": "",
  "body": "\n        To see why and how the QR decomposition is used in MIMO systems,\n        we begin with an example.\n        Assume that we have two transmitters and three receivers,\n        and suppose  .\n        From this point,\n        to simplify the situation we assume that noise is negligible and consider the system  .\n        Assume that the received signal is  .\n       \n              Show that the system   is inconsistent.\n             \n              When we receive a signal, we need to interpret it,\n              and that is difficult if we cannot find a solution.\n              One reason that the system might not have a solution is that the elements in   have to come from a specified alphabet,\n              and so we are looking for solutions that are in that alphabet,\n              and there may be no direct solution in that alphabet.\n              As a result, we generally have to find a\n               best \n              solution in the alphabet space.\n              That can be done with the QR decomposition.\n              Assume that   has a QR decomposition   with   having orthonormal columns and   a diagonal matrix.\n              Explain how the equation   can be transformed into\n               .\n             \n              Return to our example of  .\n              Assume that\n               .\n              Use   to find   if  .\n             "
},
{
  "id": "p-4379",
  "level": "2",
  "url": "chap_gram_schmidt.html#p-4379",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Householder transformation "
},
{
  "id": "project-80",
  "level": "2",
  "url": "chap_gram_schmidt.html#project-80",
  "type": "Project Activity",
  "number": "25.9",
  "title": "",
  "body": "\n        We will discover some properties of Householder transformations in this activity.\n       \n              Let   be any vector and let   be a unit vector.\n              In this part of the exercise we show that,\n              with a suitable choice of  ,\n              the Householder transformation   transforms   into a vector of the same length parallel to  .\n              That is,\n               \n              for some scalar  .\n              \n              A similar argument shows that  .\n              Letting   gives us the following result.\n               \n                    Let   be any vector,\n                    let  , and let  .\n                    Then  .\n                   \n             \n              Let  , and let  .\n              Apply   to  ,\n              factor out  ,\n              and ultimately show that  .\n             \n              The Householder matrix   has two other important properties.\n              Show that   is symmetric and orthogonal.\n             "
},
{
  "id": "act_MIMO_Householder",
  "level": "2",
  "url": "chap_gram_schmidt.html#act_MIMO_Householder",
  "type": "Project Activity",
  "number": "25.10",
  "title": "",
  "body": "\n        Assume that we have five receiving antennas and three transmitting antennas, and that\n         .\n       \n        (We use   now instead of   so as to avoid confusion with Householder matrices.) The calculations in this activity can get rather messy,\n        so use appropriate technology and round entries to four decimal places to the right of the decimal.\n       \n              Let   be the first column of  .\n              Identify an appropriate Householder transformation   so that   is a constant multiple of  .\n              Determine the matrix  ,\n              where  . (Special note:\n              when deciding which of   to use to create  ,\n              it is best to use the one whose sign is the same as  \n              (the first entry of  ).\n              We won't go into the details,\n              but this helps prevent problems due to cancellation,)\n             \n              Next we consider just the bottom right   portion\n                of the matrix   found in part (a).\n             \n                    Repeat the process on   to find a Householder matrix\n                      that will make the first column of\n                      have all zeros below its first entry.\n                   \n                    Let\n                     .\n                    Explain what the matrix   is.\n                   \n              As a final step,\n              we consider just the bottom right   portion\n                of the matrix   and repeat the process.\n             \n                    Find a matrix   that produces zeros below the first entry.\n                   \n                    Let\n                     .\n                    What matrix is  ?\n                    Why?\n                   \n              Explain why   is an orthogonal matrix.\n              Then find an upper triangular matrix   such that  .\n             "
},
{
  "id": "p-4396",
  "level": "2",
  "url": "chap_gram_schmidt.html#p-4396",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "thin "
},
{
  "id": "chap_least_squares",
  "level": "1",
  "url": "chap_least_squares.html",
  "type": "Section",
  "number": "26",
  "title": "Least Squares Approximations",
  "body": "Least Squares Approximations \n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            How, in general,\n            can we find a least squares approximation to a system  ?\n           \n         \n           \n            If the columns of   are linearly independent,\n            how can we find a least squares approximation to\n              using just matrix operations?\n           \n         \n           \n            Why are these approximations called\n             least squares \n            approximations?\n           \n         Application: Fitting Functions to Data \n    Data is all around us.\n    Data is collected on almost everything and it is important to be able to use data to make predictions.\n    However, data is rarely well-behaved and so we generally need to use approximation techniques to estimate from the data.\n    One technique for this is least squares approximations.\n    As we will see,\n    we can use linear algebra to fit a variety of different types of curves to data.\n   Introduction \n    In this section our focus is on fitting linear and polynomial functions to data sets.\n   \n      NBC was awarded the U.S. television broadcast rights to the 2016 and 2020 summer Olympic games.\n        lists the amounts paid\n      (in millions of dollars)\n      by NBC sports for the 2008 through 2012 summer Olympics plus the recently concluded bidding for the 2016 and 2020 Olympics,\n      where year 0 is the year 2008. (We will assume a simple model here,\n      ignoring variations such as value of money due to inflation,\n      viewership data which might affect NBC's expected revenue,\n      etc.)   shows a plot of the data.\n      Our goal in this activity is to find a linear function   defined by\n        that fits the data well.\n     Olympics television broadcast rights. Year Amount 0 894 4 1180 8 1226 12 1418 A plot of the data. \n      If the data were actually linear,\n      then the data would satisfy the system\n       \n     \n      The vector form of this equation is\n       .\n     \n      This equation does not have a solution,\n      so we seek the best approximation to a solution we can find.\n      That is, we want to find   and   so that the line\n        provides a best fit to the data.\n     \n      Letting   and  ,\n      and  ,\n      our vector equation becomes\n       .\n     \n      To make a best fit,\n      we will minimize the square of the distance between   and a vector of the form  .\n      That is, minimize\n       .\n     \n      Rephrasing this in terms of projections,\n      we are looking for the vector in\n        that is closest to  .\n      In other words,\n      the values of   and   will occur as the weights when we write\n        as a linear combination of   and  .\n      The one wrinkle in this problem is that we need an orthogonal basis for   to find this projection.\n      Use appropriate technology throughout this activity.\n     \n            Find an orthogonal basis   for  .\n           \n            Use the basis   to find\n              as illustrated in  .\n            \n           Projecting   onto  . \n            Find the values of   and   that give our best fit line by writing   as a linear combination of   and  .\n           \n            Draw a picture of your line from the previous part on the axes with the data set.\n            How well do you think your line approximates the data?\n            Explain.\n           Least Squares Approximations \n  In   we saw that the projection of a vector   in   onto a subspace   of   is the best approximation to   of all the vectors in  . In fact, if   and  , then the error in approximating   by   is given by \n   .\n  \n  In the context of  , we projected the vector   onto the span of  the vectors   and  . The projection   minimizes the distance between the vectors in   and the vector   (as shown in  ), and also produces a line which minimizes the sums of the squares of the vertical  distances from the line to the data set as illustrated in   with the olympics data. This is why these approximations are called least squares approximations. \n   Least squares linear approximation \n    While we can always solve least squares problems using projections, we can often avoid having to create an orthogonal basis when fitting functions to data. We work in a more general setting, showing how to fit a polynomial of degree   to a set of data points. Our goal is to fit a polynomial   of degree   to   data points  ,  ,  ,  , no two of which have the same   coordinate. In the unlikely event that the polynomial   actually passes through the   points, then we would have the   equations\n\n     \n    in the   unknowns  ,  ,  ,  , and  .\n   \n    The   data points are known in this situation and the coefficients  ,  ,  ,   are the unknowns. To write the system in matrix-vector form, the coefficient matrix   is\n     ,\n    while the vectors   and   are\n     .\n    Letting   and   for  , the vector form of the system is \n     .\n   \n    Of course, it is unlikely that the   data points already lie on a polynomial of degree  , so the system will usually have no solution. So instead of attempting to find coefficients  ,  ,  ,   that give a solution to this system, which may be impossible, we instead look for a vector that is ``close\" to a solution. As we have seen, the vector  , where   is the span of the columns of  , minimizes the sum of the squares of the differences of the components. That is, our desired approximation to a solution to   is the projection of   onto  . Now   is a linear combination of the columns of  , so   for some vector  . This vector   then minimizes  . That is, if we let  , we are minimizing \n     .\n    The expression   measures the error in our approximation.\n   \n    The question we want to answer is how we can find the vector   that minimizes   in a way that is more convenient than computing a projection. We answer this question in a general setting in the next activity. \n\n   \n        Let   be an   matrix and let   be in  . Let  . Then   is in  , so let   be in   such that  . \n       \n          Explain why   is orthogonal to every vector of the form  , for any   in  . That is,   is orthogonal to  . \n         \n          Let   be the  th column of  . Explain why  . From this, explain why  . \n         \n          From the previous part, show that   satisfies the equation\n           .\n          \n         \n    The result of   is that we can now do least squares polynomial approximations with just matrix operations. We summarize this in the following theorem.\n   \n        The least squares solutions to the system   are the solutions to the corresponding system \n         .\n       normal equations \n        Now use the least squares method to find the best polynomial approximations (in the least squares sense) of degrees 2 and 3 for the Olympics data set in  . Which polynomial seems to give the  best  fit? Explain why. Include a discussion of the errors in your approximations. Use your  best  least squares approximation to estimate how much NBC might pay for the television rights to the 2024 Olympic games. Use technology as appropriate.\n       \n    The solution with our Olympics data gave us the situation where   was invertible. This corresponded to a unique least squares solution  . It is reasonable to ask when this happens in general. To conclude this section, we will demonstrate that if the columns of a matrix   are linearly independent, then   is invertible.\n   \n        Let   be an   matrix with linearly independent columns.\n       \n          What must be the relationship between   and  ? Explain. \n         \n          We know that an   matrix   is invertible if and only if the only solution to the homogeneous system   is  . Note that   is an   matrix. Suppose that   for some   in  . \n         \n            Show that  .\n           \n            What is  ?\n           \n            What does   tell us about   in relation to  ? Why?\n           \n            What is  ? Why? What does this tell us about   and then about  ?\n           \n    We summarize the result of   in the following theorem.\n   \n        If the columns of   are linearly independent, then the least squares solution   to the system   is\n         .\n       \n    If the columns of   are linearly dependent, we can still solve the normal equations, but will obtain more than one solution. In a later section we will see that we can also use a pseudoinverse in these situations. \n   Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        According to the  Centers for Disease Control and Prevention ,\n        the average length of a male infant\n        (in centimeters)\n        in the US as it ages\n        (with time in months from 1.5 to 8.5)\n        is given in  .\n       Average lengths of male infants Age (months) 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 Average Length (cm) 56.6 59.6 62.1 64.2 66.1 67.9 69.5 70.9 \n        In this problem we will find the line and the quadratic of best fit in the least squares sense to this data.\n        We treat age in months as the independent variable and length in centimeters as the dependent variable.\n       \n              Find a line that is the best fit to the data in the least squares sense.\n              Draw a picture of your least squares solution against a scatterplot of the data.\n             \n              We assume that a line of the form\n                contains all of the data points.\n              The first data point would satisfy  ,\n              the second  ,\n              and so on, giving us the linear system\n               \n              Letting\n               ,\n              we can write this system in the matrix form   .\n              Neither column of   is a multiple of the other,\n              so the columns of   are linearly independent.\n              The least squares solution   to the system is then found by\n               .\n              Technology shows that\n              (with entries rounded to 3 decimal places),\n                is\n               ,\n              and\n               .\n              So the least squares linear function to the data is  .\n              A graph of   against the data points is shown at left in  .\n               Left: Least squares line. Right: Least squares quadratic. \n             \n              Now find the least squares quadratic of the form   to the data.\n              Draw a picture of your least squares solution against a scatterplot of the data.\n             \n              The first data point would satisfy  ,\n              the second  ,\n              and so on, giving us the linear system\n               \n              Letting\n               ,\n              we can write this system in the matrix form   .\n              Technology shows that every column of the reduced row echelon form of   contains a pivot,\n              so the columns of   are linearly independent.\n              The least squares solution   to the system is then found by\n               .\n              Technology shows that\n              (with entries rounded to 3 decimal places)\n                is\n               ,\n              and\n               .\n              So the least squares quadratic function to the data is   defined by  .\n              A graph of   against the data points is shown at right in  .\n             \n        Least squares solutions can be found through a QR factorization,\n        as we explore in this example.\n        Let   be an   matrix with linearly independent columns and QR factorization  .\n        Suppose that   is not in   so that the system   is inconsistent.\n        We know that the least squares solution   to   is\n         .\n       \n              Replace   by its QR factorization in   to show that\n               .\n             \n                  Use the fact that   is orthogonal and   is invertible.\n                 \n              Replacing   with   and using the fact that   is invertible and   is orthogonal to see that\n               .\n              So if   is a QR factorization of  ,\n              then the least squares solution to\n                is  .\n             \n              Consider the data set in  ,\n              which shows the average life expectance in years in the US for selected years from 1950 to 2010.\n             Life expectancy in the US year 1950 1965 1980 1995 2010 age 68.14 70.21 73.70 75.98 78.49 \n              (Data from  macrotrends .)\n             \n                    Use   to find the least squares linear fit to the data set.\n                   \n                    A linear fit to the data will be provided by the least squares solution to  , where\n                     .\n                    Technology shows that\n                     .\n                   \n                    Use appropriate technology to find the QR factorization of an appropriate matrix  ,\n                    and use the QR decomposition to find the least squares linear fit to the data.\n                    Compare to what you found in part i.\n                   \n                    Technology shows that  , where\n                     .\n                    Then we have that\n                     ,\n                    just as in part i.\n                   Summary \n       \n        A least squares approximation to\n          is found by orthogonally projecting   onto  .\n       \n     \n       \n        If the columns of   are linearly independent,\n        then the least squares approximation to\n          is  .\n       \n     \n       \n        The least squares solution to  ,\n        where   and  ,\n        minimizes the distance  , where\n         .\n        So the least squares solution minimizes a sum of squares.\n       \n     \n        The University of Denver Infant Study Center investigated whether babies take longer to learn to crawl in cold months,\n        when they are often bundled in clothes that restrict their movement,\n        than in warmer months.\n        The study sought a relationship between babies' first crawling age and the average temperature during the month they first try to crawl\n        (about 6 months after birth).\n        Some of the data from the study is in  .\n        Let   represent the temperature in degrees Fahrenheit and   the average crawling age in months.\n       Crawling age 33 37 48 57 33.83 33.35 33.38 32.32 \n              Find the least squares line to fit this data.\n              Plot the data and your line on the same set of axes.\n              (We aren't concerned about whether a linear fit is really a good choice outside of this data set,\n              we just fit a line to it to see what happens.)\n             \n               \n             \n              Use your least squares line to predict the average crawling age when the temperature is 65.\n             \n              Approximately  .\n             \n        The cost, in cents,\n        of a first class postage stamp in years from 1981 to 1995 is shown in  .\n       Cost of postage Year 1981 1985 1988 1991 1995 Cost 20 22 25 29 32 \n              Find the least squares line to fit this data.\n              Plot the data and your line on the same set of axes.\n             \n              Now find the least squares quadratic approximation to this data.\n              Plot the quadratic function on same axes as your linear function.\n             \n              Use your least squares line and quadratic to predict the cost of a postage stamp in this year.\n              Look up the cost of a stamp today and determine how accurate your prediction is.\n              Which function gives a better approximation?\n              Provide reasons for any discrepancies.\n             \n        According to  The Song of Insects \n        by G.W. Pierce (Harvard College Press, 1948) the sound of striped ground crickets chirping,\n        in number of chirps per second,\n        is related to the temperature.\n        So the number of chirps per second could be a predictor of temperature.\n        The data Pierce collected is shown in the table and scatterplot below,\n        where   is the (average) number of chirps per second and   is the temperature in degrees Fahrenheit.\n       20.0 88.6 16.0 71.6 19.8 93.3 18.4 84.3 17.1 80.6 15.5 75.2 14.7 69.7 17.1 82.0 15.4 69.4 16.2 83.3 15.0 79.6 17.2 82.6 16.0 80.6 17.0 83.5 14.4 76.3 \n        The relationship between   and   is not exactly linear,\n        but looks to have a linear pattern.\n        It could be that the relationship is really linear but experimental error causes the data to be slightly inaccurate.\n        Or perhaps the data is not linear,\n        but only approximately linear.\n        Find the least squares linear approximation to the data.\n       \n         \n       \n        We showed that if the columns of   are linearly independent,\n        then   is invertible.\n        Show that the reverse implication is also true.\n        That is, show that if   is invertible,\n        then the columns of   are linearly independent.\n       \n        Consider the small data set of points  .\n       \n              Find a linear system   whose solution would define a least squares linear approximation to the data in set  .\n             \n               \n             \n              Explain what happens when we attempt to find the least squares solution   using the matrix  .\n              Why does this happen?\n             \n                is not invertible\n             \n              Does the system   have a least squares solution?\n              If so, how many and what are they?\n              If not, why not?\n             \n              Infinitely many.\n             \n              Fit a linear function of the form   to the data.\n              Why should you have expected the answer?\n             \n               \n             \n        Let   and   be any matrices such that   is defined.\n        In this exercise we investigate relationships between ranks of various matrices.\n       \n              Show that   is a subspace of  .\n              Use this result to explain why  .\n             \n              Show that  .\n              \n             \n              For part, see Exercise 12 in Section 15.\n             \n              Show that  .\n             \n        We have seen that if the columns of a matrix   are linearly independent, then\n         \n        is a least squares solution to  .\n        What if the columns of   are linearly dependent?\n        From Activity 26.1, a least squares solution to\n          is a solution to the equation  .\n        In this exercise we demonstrate that\n          always has a solution.\n       \n              Explain why it is enough to show that the rank of the augmented matrix\n                is the same as the rank of  .\n             \n              The augmented column of   cannot be a pivot column.\n             \n              Explain why  .\n              \n             \n              See  .\n             \n              Explain why  .\n             \n              Use the definition of the matrix product.\n             \n              Explain why  .\n              \n             \n              See  .\n             \n              Finally, explain why  .\n             \n              Combine parts (b) and (d).\n             projection matrix \n              Show that  .\n             projection matrix orthogonal projection matrix \n              If   is an   orthogonal projection matrix,\n              show that if   is in  ,\n              then   is orthogonal to every vector in  .\n              (Hence,   projects orthogonally onto  .)\n             \n              Recall the projection   of a vector   in the direction of a vector   is give by  .\n              Show that  ,\n              where   is the orthogonal projection matrix  .\n              Illustrate with  .\n             \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If the columns of   are linearly independent,\n              then there is a unique least squares solution to  .\n             \n              T\n             True\/False \n              Let   be an orthonormal basis for  .\n              The least squares solution to\n                is  .\n             True\/False \n              The least squares line to the data points  ,\n               ,\n              and   is  .\n             \n              F\n             True\/False \n              If the columns of a matrix   are not invertible and the vector   is not in  ,\n              then there is no least squares solution to  .\n             True\/False \n              Every matrix equation of the form\n                has a least squares solution.\n             \n              T\n             True\/False \n              If the columns of   are linearly independent,\n              then the least squares solution to\n                is the orthogonal projection of   onto  .\n             Project: Other Least Squares Approximations \n    In this section we learned how to fit a polynomial function to a set of data in the least squares sense.\n    But data takes on many forms,\n    so it is important to be able to fit other types of functions to data sets.\n    We investigate three different types of regression problems in this project.\n   \n      The length of a species of fish is to be represented as a function of the age and water temperature as shown in the table on the next page.   \n      Data from  Mathematical Algorithms for Linear Regression , Helmut Spaeth, Academic Press, 1991, page 305, ISBN 0-12-656460-4.\n        The fish are kept in tanks at 25, 27, 29 and 31 degrees Celsius.\n      After birth,\n      a test specimen is chosen at random every 14 days and its length measured.\n      The data include:\n       \n           \n             , the index;\n           \n         \n           \n             , the age of the fish in days;\n           \n         \n           \n             , the water temperature in degrees Celsius;\n           \n         \n           \n             , the length of the fish.\n           \n         \n     \n      Since there are three variables in the data,\n      we cannot perform a simple linear regression.\n      Instead, we seek a model of the form\n       \n      to fit the data, where   approximates the length.\n      That is, we seek the best fit plane to the data.\n      This is an example of what is called multiple linear regression.\n      A scatterplot of the data, along with the best fit plane,\n      is also shown.\n     \n            As we did when we fit polynomials to data,\n            we start by considering what would happen if all of our data points satisfied our model function.\n            In this case our data points have the form  ,\n             ,  ,  .\n            Explain what system of linear equations would result if the data points actually satisfy our model function  .\n            (You don't need to write 44 different equations,\n            just explain the general form of the system.)\n           \n            Write the system from (a) in the form  ,\n            and specifically identify the matrix   and the vectors   and  .\n           \n            The same derivation as with the polynomial regression models shows that the vector   that minimizes   is found by\n             ,\n            Use this to find the least squares fit of the form   to the data.\n           \n            Provide a numeric measure of how well this model function fits the data.\n            Explain.\n           Index Age Temp ( C) Length 1 14 25 620 2 28 25 1315 3 41 25 2120 4 55 25 2600 5 69 25 3110 6 83 25 3535 7 97 25 3935 8 111 25 4465 9 125 25 4530 10 139 25 4570 11 153 25 4600 12 14 27 625 13 28 27 1215 14 41 27 2110 15 55 27 2805 16 69 27 3255 17 83 27 4015 18 97 27 4315 19 111 27 4495 20 125 27 4535 21 139 27 4600 22 153 27 4600 23 14 29 590 24 28 29 1305 25 41 29 2140 26 55 29 2890 27 69 29 3920 28 83 29 3920 29 97 29 4515 30 111 29 4520 31 125 29 4525 32 139 29 4565 33 153 29 4566 34 14 31 590 35 28 31 1205 36 41 31 1915 37 55 31 2140 38 69 31 2710 39 83 31 3020 40 97 31 3030 41 111 31 3040 42 125 31 3180 43 139 31 3257 44 153 31 3214 \n      Population growth is typically not well modeled by polynomial functions.\n      Populations tend to grow at rates proportional to the population,\n      which implies exponential growth.\n      For example,  \n      shows the approximate population of the United States in years between 1920 and 2000,\n      with the population measured in millions.\n     U.S. population Year 1920 1930 1940 1950 1960 1970 1980 1990 2000 Population 106 123 142 161 189 213 237 259 291 \n      If we assume the population grows exponentially,\n      we would want to find the best fit function   of the form  ,\n      where   and   are constants.\n      However, an exponential function is not linear.\n      So to apply the methods we have developed,\n      we could instead apply the natural logarithm to both sides of\n        to obtain the equation  .\n      We can then find the best fit line to the data in the form\n        to determine the values of   and  .\n      Use this approach to find the best fit exponential function in the least squares sense to the U.S. population data.\n      Then look up the U.S. population in 2010\n      (include your source)\n      and compare to the estimate given by your model function.\n      If your prediction is not very close,\n      give some plausible explanations for the difference.\n     Best fit ellipse. \n      Carl Friedrich Gauss is often credited with inventing the method of least squares.\n      He used the method to find a best-fit ellipse which allowed him to correctly predict the orbit of the asteroid Ceres as it passed behind the sun in 1801. (Adrien-Marie Legendre appears to be the first to publish the method,\n      though.) Here we examine the problem of fitting an ellipse to data.\n     \n      An ellipse is a quadratic equation that can be written in the form\n       \n      for constants  ,  ,\n       ,  , and  , with  .\n      We will find the best-fit ellipse in the least squares sense through the points\n       .\n     \n      A picture of the best fit ellipse is shown in  .\n     \n            Find the system of linear equations that would result if the ellipse   were to exactly pass through the given points.\n           \n            Write the linear system from part (a) in the form  ,\n            where the vector   contains the unknowns in the system.\n            Clearly identify  ,  , and  .\n           \n            Find the least squares ellipse to this set of points.\n            Make sure your method is clear. (Note that we are really fitting a surface of the form\n              to a set of data points in the  -plane.\n            So the error is the sum of the vertical distances from the points in the  -plane to the surface.)\n           "
},
{
  "id": "objectives-26",
  "level": "2",
  "url": "chap_least_squares.html#objectives-26",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            How, in general,\n            can we find a least squares approximation to a system  ?\n           \n         \n           \n            If the columns of   are linearly independent,\n            how can we find a least squares approximation to\n              using just matrix operations?\n           \n         \n           \n            Why are these approximations called\n             least squares \n            approximations?\n           \n         "
},
{
  "id": "pa_LS_1",
  "level": "2",
  "url": "chap_least_squares.html#pa_LS_1",
  "type": "Preview Activity",
  "number": "26.1",
  "title": "",
  "body": "\n      NBC was awarded the U.S. television broadcast rights to the 2016 and 2020 summer Olympic games.\n        lists the amounts paid\n      (in millions of dollars)\n      by NBC sports for the 2008 through 2012 summer Olympics plus the recently concluded bidding for the 2016 and 2020 Olympics,\n      where year 0 is the year 2008. (We will assume a simple model here,\n      ignoring variations such as value of money due to inflation,\n      viewership data which might affect NBC's expected revenue,\n      etc.)   shows a plot of the data.\n      Our goal in this activity is to find a linear function   defined by\n        that fits the data well.\n     Olympics television broadcast rights. Year Amount 0 894 4 1180 8 1226 12 1418 A plot of the data. \n      If the data were actually linear,\n      then the data would satisfy the system\n       \n     \n      The vector form of this equation is\n       .\n     \n      This equation does not have a solution,\n      so we seek the best approximation to a solution we can find.\n      That is, we want to find   and   so that the line\n        provides a best fit to the data.\n     \n      Letting   and  ,\n      and  ,\n      our vector equation becomes\n       .\n     \n      To make a best fit,\n      we will minimize the square of the distance between   and a vector of the form  .\n      That is, minimize\n       .\n     \n      Rephrasing this in terms of projections,\n      we are looking for the vector in\n        that is closest to  .\n      In other words,\n      the values of   and   will occur as the weights when we write\n        as a linear combination of   and  .\n      The one wrinkle in this problem is that we need an orthogonal basis for   to find this projection.\n      Use appropriate technology throughout this activity.\n     \n            Find an orthogonal basis   for  .\n           \n            Use the basis   to find\n              as illustrated in  .\n            \n           Projecting   onto  . \n            Find the values of   and   that give our best fit line by writing   as a linear combination of   and  .\n           \n            Draw a picture of your line from the previous part on the axes with the data set.\n            How well do you think your line approximates the data?\n            Explain.\n           "
},
{
  "id": "F_LS_Olympics2",
  "level": "2",
  "url": "chap_least_squares.html#F_LS_Olympics2",
  "type": "Figure",
  "number": "26.4",
  "title": "",
  "body": "Least squares linear approximation "
},
{
  "id": "act_LS_matrices",
  "level": "2",
  "url": "chap_least_squares.html#act_LS_matrices",
  "type": "Activity",
  "number": "26.2",
  "title": "",
  "body": "\n        Let   be an   matrix and let   be in  . Let  . Then   is in  , so let   be in   such that  . \n       \n          Explain why   is orthogonal to every vector of the form  , for any   in  . That is,   is orthogonal to  . \n         \n          Let   be the  th column of  . Explain why  . From this, explain why  . \n         \n          From the previous part, show that   satisfies the equation\n           .\n          \n         "
},
{
  "id": "thm_6_f_least_squares_1",
  "level": "2",
  "url": "chap_least_squares.html#thm_6_f_least_squares_1",
  "type": "Theorem",
  "number": "26.5",
  "title": "",
  "body": "\n        The least squares solutions to the system   are the solutions to the corresponding system \n         .\n       "
},
{
  "id": "p-4425",
  "level": "2",
  "url": "chap_least_squares.html#p-4425",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "normal equations "
},
{
  "id": "activity-102",
  "level": "2",
  "url": "chap_least_squares.html#activity-102",
  "type": "Activity",
  "number": "26.3",
  "title": "",
  "body": "\n        Now use the least squares method to find the best polynomial approximations (in the least squares sense) of degrees 2 and 3 for the Olympics data set in  . Which polynomial seems to give the  best  fit? Explain why. Include a discussion of the errors in your approximations. Use your  best  least squares approximation to estimate how much NBC might pay for the television rights to the 2024 Olympic games. Use technology as appropriate.\n       "
},
{
  "id": "act_LS_invertible",
  "level": "2",
  "url": "chap_least_squares.html#act_LS_invertible",
  "type": "Activity",
  "number": "26.4",
  "title": "",
  "body": "\n        Let   be an   matrix with linearly independent columns.\n       \n          What must be the relationship between   and  ? Explain. \n         \n          We know that an   matrix   is invertible if and only if the only solution to the homogeneous system   is  . Note that   is an   matrix. Suppose that   for some   in  . \n         \n            Show that  .\n           \n            What is  ?\n           \n            What does   tell us about   in relation to  ? Why?\n           \n            What is  ? Why? What does this tell us about   and then about  ?\n           "
},
{
  "id": "thm_6_f_least_squares_2",
  "level": "2",
  "url": "chap_least_squares.html#thm_6_f_least_squares_2",
  "type": "Theorem",
  "number": "26.6",
  "title": "",
  "body": "\n        If the columns of   are linearly independent, then the least squares solution   to the system   is\n         .\n       "
},
{
  "id": "example-52",
  "level": "2",
  "url": "chap_least_squares.html#example-52",
  "type": "Example",
  "number": "26.7",
  "title": "",
  "body": "\n        According to the  Centers for Disease Control and Prevention ,\n        the average length of a male infant\n        (in centimeters)\n        in the US as it ages\n        (with time in months from 1.5 to 8.5)\n        is given in  .\n       Average lengths of male infants Age (months) 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 Average Length (cm) 56.6 59.6 62.1 64.2 66.1 67.9 69.5 70.9 \n        In this problem we will find the line and the quadratic of best fit in the least squares sense to this data.\n        We treat age in months as the independent variable and length in centimeters as the dependent variable.\n       \n              Find a line that is the best fit to the data in the least squares sense.\n              Draw a picture of your least squares solution against a scatterplot of the data.\n             \n              We assume that a line of the form\n                contains all of the data points.\n              The first data point would satisfy  ,\n              the second  ,\n              and so on, giving us the linear system\n               \n              Letting\n               ,\n              we can write this system in the matrix form   .\n              Neither column of   is a multiple of the other,\n              so the columns of   are linearly independent.\n              The least squares solution   to the system is then found by\n               .\n              Technology shows that\n              (with entries rounded to 3 decimal places),\n                is\n               ,\n              and\n               .\n              So the least squares linear function to the data is  .\n              A graph of   against the data points is shown at left in  .\n               Left: Least squares line. Right: Least squares quadratic. \n             \n              Now find the least squares quadratic of the form   to the data.\n              Draw a picture of your least squares solution against a scatterplot of the data.\n             \n              The first data point would satisfy  ,\n              the second  ,\n              and so on, giving us the linear system\n               \n              Letting\n               ,\n              we can write this system in the matrix form   .\n              Technology shows that every column of the reduced row echelon form of   contains a pivot,\n              so the columns of   are linearly independent.\n              The least squares solution   to the system is then found by\n               .\n              Technology shows that\n              (with entries rounded to 3 decimal places)\n                is\n               ,\n              and\n               .\n              So the least squares quadratic function to the data is   defined by  .\n              A graph of   against the data points is shown at right in  .\n             "
},
{
  "id": "example-53",
  "level": "2",
  "url": "chap_least_squares.html#example-53",
  "type": "Example",
  "number": "26.10",
  "title": "",
  "body": "\n        Least squares solutions can be found through a QR factorization,\n        as we explore in this example.\n        Let   be an   matrix with linearly independent columns and QR factorization  .\n        Suppose that   is not in   so that the system   is inconsistent.\n        We know that the least squares solution   to   is\n         .\n       \n              Replace   by its QR factorization in   to show that\n               .\n             \n                  Use the fact that   is orthogonal and   is invertible.\n                 \n              Replacing   with   and using the fact that   is invertible and   is orthogonal to see that\n               .\n              So if   is a QR factorization of  ,\n              then the least squares solution to\n                is  .\n             \n              Consider the data set in  ,\n              which shows the average life expectance in years in the US for selected years from 1950 to 2010.\n             Life expectancy in the US year 1950 1965 1980 1995 2010 age 68.14 70.21 73.70 75.98 78.49 \n              (Data from  macrotrends .)\n             \n                    Use   to find the least squares linear fit to the data set.\n                   \n                    A linear fit to the data will be provided by the least squares solution to  , where\n                     .\n                    Technology shows that\n                     .\n                   \n                    Use appropriate technology to find the QR factorization of an appropriate matrix  ,\n                    and use the QR decomposition to find the least squares linear fit to the data.\n                    Compare to what you found in part i.\n                   \n                    Technology shows that  , where\n                     .\n                    Then we have that\n                     ,\n                    just as in part i.\n                   "
},
{
  "id": "exercise-257",
  "level": "2",
  "url": "chap_least_squares.html#exercise-257",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        The University of Denver Infant Study Center investigated whether babies take longer to learn to crawl in cold months,\n        when they are often bundled in clothes that restrict their movement,\n        than in warmer months.\n        The study sought a relationship between babies' first crawling age and the average temperature during the month they first try to crawl\n        (about 6 months after birth).\n        Some of the data from the study is in  .\n        Let   represent the temperature in degrees Fahrenheit and   the average crawling age in months.\n       Crawling age 33 37 48 57 33.83 33.35 33.38 32.32 \n              Find the least squares line to fit this data.\n              Plot the data and your line on the same set of axes.\n              (We aren't concerned about whether a linear fit is really a good choice outside of this data set,\n              we just fit a line to it to see what happens.)\n             \n               \n             \n              Use your least squares line to predict the average crawling age when the temperature is 65.\n             \n              Approximately  .\n             "
},
{
  "id": "exercise-258",
  "level": "2",
  "url": "chap_least_squares.html#exercise-258",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        The cost, in cents,\n        of a first class postage stamp in years from 1981 to 1995 is shown in  .\n       Cost of postage Year 1981 1985 1988 1991 1995 Cost 20 22 25 29 32 \n              Find the least squares line to fit this data.\n              Plot the data and your line on the same set of axes.\n             \n              Now find the least squares quadratic approximation to this data.\n              Plot the quadratic function on same axes as your linear function.\n             \n              Use your least squares line and quadratic to predict the cost of a postage stamp in this year.\n              Look up the cost of a stamp today and determine how accurate your prediction is.\n              Which function gives a better approximation?\n              Provide reasons for any discrepancies.\n             "
},
{
  "id": "exercise-259",
  "level": "2",
  "url": "chap_least_squares.html#exercise-259",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        According to  The Song of Insects \n        by G.W. Pierce (Harvard College Press, 1948) the sound of striped ground crickets chirping,\n        in number of chirps per second,\n        is related to the temperature.\n        So the number of chirps per second could be a predictor of temperature.\n        The data Pierce collected is shown in the table and scatterplot below,\n        where   is the (average) number of chirps per second and   is the temperature in degrees Fahrenheit.\n       20.0 88.6 16.0 71.6 19.8 93.3 18.4 84.3 17.1 80.6 15.5 75.2 14.7 69.7 17.1 82.0 15.4 69.4 16.2 83.3 15.0 79.6 17.2 82.6 16.0 80.6 17.0 83.5 14.4 76.3 \n        The relationship between   and   is not exactly linear,\n        but looks to have a linear pattern.\n        It could be that the relationship is really linear but experimental error causes the data to be slightly inaccurate.\n        Or perhaps the data is not linear,\n        but only approximately linear.\n        Find the least squares linear approximation to the data.\n       \n         \n       "
},
{
  "id": "exercise-260",
  "level": "2",
  "url": "chap_least_squares.html#exercise-260",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        We showed that if the columns of   are linearly independent,\n        then   is invertible.\n        Show that the reverse implication is also true.\n        That is, show that if   is invertible,\n        then the columns of   are linearly independent.\n       "
},
{
  "id": "ex_6_f_not_li",
  "level": "2",
  "url": "chap_least_squares.html#ex_6_f_not_li",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Consider the small data set of points  .\n       \n              Find a linear system   whose solution would define a least squares linear approximation to the data in set  .\n             \n               \n             \n              Explain what happens when we attempt to find the least squares solution   using the matrix  .\n              Why does this happen?\n             \n                is not invertible\n             \n              Does the system   have a least squares solution?\n              If so, how many and what are they?\n              If not, why not?\n             \n              Infinitely many.\n             \n              Fit a linear function of the form   to the data.\n              Why should you have expected the answer?\n             \n               \n             "
},
{
  "id": "ex_6_f_ranks",
  "level": "2",
  "url": "chap_least_squares.html#ex_6_f_ranks",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Let   and   be any matrices such that   is defined.\n        In this exercise we investigate relationships between ranks of various matrices.\n       \n              Show that   is a subspace of  .\n              Use this result to explain why  .\n             \n              Show that  .\n              \n             \n              For part, see Exercise 12 in Section 15.\n             \n              Show that  .\n             "
},
{
  "id": "exercise-263",
  "level": "2",
  "url": "chap_least_squares.html#exercise-263",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        We have seen that if the columns of a matrix   are linearly independent, then\n         \n        is a least squares solution to  .\n        What if the columns of   are linearly dependent?\n        From Activity 26.1, a least squares solution to\n          is a solution to the equation  .\n        In this exercise we demonstrate that\n          always has a solution.\n       \n              Explain why it is enough to show that the rank of the augmented matrix\n                is the same as the rank of  .\n             \n              The augmented column of   cannot be a pivot column.\n             \n              Explain why  .\n              \n             \n              See  .\n             \n              Explain why  .\n             \n              Use the definition of the matrix product.\n             \n              Explain why  .\n              \n             \n              See  .\n             \n              Finally, explain why  .\n             \n              Combine parts (b) and (d).\n             "
},
{
  "id": "exercise-264",
  "level": "2",
  "url": "chap_least_squares.html#exercise-264",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "projection matrix \n              Show that  .\n             projection matrix orthogonal projection matrix \n              If   is an   orthogonal projection matrix,\n              show that if   is in  ,\n              then   is orthogonal to every vector in  .\n              (Hence,   projects orthogonally onto  .)\n             \n              Recall the projection   of a vector   in the direction of a vector   is give by  .\n              Show that  ,\n              where   is the orthogonal projection matrix  .\n              Illustrate with  .\n             "
},
{
  "id": "exercise-265",
  "level": "2",
  "url": "chap_least_squares.html#exercise-265",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If the columns of   are linearly independent,\n              then there is a unique least squares solution to  .\n             \n              T\n             True\/False \n              Let   be an orthonormal basis for  .\n              The least squares solution to\n                is  .\n             True\/False \n              The least squares line to the data points  ,\n               ,\n              and   is  .\n             \n              F\n             True\/False \n              If the columns of a matrix   are not invertible and the vector   is not in  ,\n              then there is no least squares solution to  .\n             True\/False \n              Every matrix equation of the form\n                has a least squares solution.\n             \n              T\n             True\/False \n              If the columns of   are linearly independent,\n              then the least squares solution to\n                is the orthogonal projection of   onto  .\n             "
},
{
  "id": "project-82",
  "level": "2",
  "url": "chap_least_squares.html#project-82",
  "type": "Project Activity",
  "number": "26.5",
  "title": "",
  "body": "\n      The length of a species of fish is to be represented as a function of the age and water temperature as shown in the table on the next page.   \n      Data from  Mathematical Algorithms for Linear Regression , Helmut Spaeth, Academic Press, 1991, page 305, ISBN 0-12-656460-4.\n        The fish are kept in tanks at 25, 27, 29 and 31 degrees Celsius.\n      After birth,\n      a test specimen is chosen at random every 14 days and its length measured.\n      The data include:\n       \n           \n             , the index;\n           \n         \n           \n             , the age of the fish in days;\n           \n         \n           \n             , the water temperature in degrees Celsius;\n           \n         \n           \n             , the length of the fish.\n           \n         \n     \n      Since there are three variables in the data,\n      we cannot perform a simple linear regression.\n      Instead, we seek a model of the form\n       \n      to fit the data, where   approximates the length.\n      That is, we seek the best fit plane to the data.\n      This is an example of what is called multiple linear regression.\n      A scatterplot of the data, along with the best fit plane,\n      is also shown.\n     \n            As we did when we fit polynomials to data,\n            we start by considering what would happen if all of our data points satisfied our model function.\n            In this case our data points have the form  ,\n             ,  ,  .\n            Explain what system of linear equations would result if the data points actually satisfy our model function  .\n            (You don't need to write 44 different equations,\n            just explain the general form of the system.)\n           \n            Write the system from (a) in the form  ,\n            and specifically identify the matrix   and the vectors   and  .\n           \n            The same derivation as with the polynomial regression models shows that the vector   that minimizes   is found by\n             ,\n            Use this to find the least squares fit of the form   to the data.\n           \n            Provide a numeric measure of how well this model function fits the data.\n            Explain.\n           "
},
{
  "id": "project-83",
  "level": "2",
  "url": "chap_least_squares.html#project-83",
  "type": "Project Activity",
  "number": "26.6",
  "title": "",
  "body": "\n      Population growth is typically not well modeled by polynomial functions.\n      Populations tend to grow at rates proportional to the population,\n      which implies exponential growth.\n      For example,  \n      shows the approximate population of the United States in years between 1920 and 2000,\n      with the population measured in millions.\n     U.S. population Year 1920 1930 1940 1950 1960 1970 1980 1990 2000 Population 106 123 142 161 189 213 237 259 291 \n      If we assume the population grows exponentially,\n      we would want to find the best fit function   of the form  ,\n      where   and   are constants.\n      However, an exponential function is not linear.\n      So to apply the methods we have developed,\n      we could instead apply the natural logarithm to both sides of\n        to obtain the equation  .\n      We can then find the best fit line to the data in the form\n        to determine the values of   and  .\n      Use this approach to find the best fit exponential function in the least squares sense to the U.S. population data.\n      Then look up the U.S. population in 2010\n      (include your source)\n      and compare to the estimate given by your model function.\n      If your prediction is not very close,\n      give some plausible explanations for the difference.\n     "
},
{
  "id": "F_ellipse",
  "level": "2",
  "url": "chap_least_squares.html#F_ellipse",
  "type": "Figure",
  "number": "26.17",
  "title": "",
  "body": "Best fit ellipse. "
},
{
  "id": "project-84",
  "level": "2",
  "url": "chap_least_squares.html#project-84",
  "type": "Project Activity",
  "number": "26.7",
  "title": "",
  "body": "\n      Carl Friedrich Gauss is often credited with inventing the method of least squares.\n      He used the method to find a best-fit ellipse which allowed him to correctly predict the orbit of the asteroid Ceres as it passed behind the sun in 1801. (Adrien-Marie Legendre appears to be the first to publish the method,\n      though.) Here we examine the problem of fitting an ellipse to data.\n     \n      An ellipse is a quadratic equation that can be written in the form\n       \n      for constants  ,  ,\n       ,  , and  , with  .\n      We will find the best-fit ellipse in the least squares sense through the points\n       .\n     \n      A picture of the best fit ellipse is shown in  .\n     \n            Find the system of linear equations that would result if the ellipse   were to exactly pass through the given points.\n           \n            Write the linear system from part (a) in the form  ,\n            where the vector   contains the unknowns in the system.\n            Clearly identify  ,  , and  .\n           \n            Find the least squares ellipse to this set of points.\n            Make sure your method is clear. (Note that we are really fitting a surface of the form\n              to a set of data points in the  -plane.\n            So the error is the sum of the vertical distances from the points in the  -plane to the surface.)\n           "
},
{
  "id": "chap_orthogonal_diagonalization",
  "level": "1",
  "url": "chap_orthogonal_diagonalization.html",
  "type": "Section",
  "number": "27",
  "title": "Orthogonal Diagonalization",
  "body": "Orthogonal Diagonalization \n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            What does it mean for a matrix to be orthogonally diagonalizable and why is this concept important?\n           \n         \n           \n            What is a symmetric matrix and what important property related to diagonalization does a symmetric matrix have?\n           \n         \n           \n            What is the spectrum of a matrix?\n           \n         Application: The Multivariable Second Derivative Test \n    In single variable calculus,\n    we learn that the second derivative can be used to classify a critical point of the type where the derivative of a function is 0 as a local maximum or minimum.\n   The Second Derivative Test for Single-Variable Functions \n        If   is a critical number of a function   so that   and if   exists, then\n         \n             \n              if  ,\n              then   is a local maximum value of  ,\n             \n           \n             \n              if  ,\n              then   is a local minimum value of  , and\n             \n           \n             \n              if  , this test yields no information.\n             \n           \n       \n    In the two-variable case we have an analogous test,\n    which is usually seen in a multivariable calculus course.\n   The Second Derivative Test for Functions of Two Variables \n        Suppose   is a critical point of the function   for which\n          and  .\n        Let   be the quantity defined by\n         .\n         \n             \n              If   and  ,\n              then   has a local maximum at  .\n             \n           \n             \n              If   and  ,\n              then   has a local minimum at  .\n             \n           \n             \n              If  , then   has a saddle point at  .\n             \n           \n             \n              If  ,\n              then this test yields no information about what happens at  .\n             \n           \n       \n    A proof of this test for two-variable functions is based on Taylor polynomials,\n    and relies on symmetric matrices,\n    eigenvalues, and quadratic forms.\n    The steps for a proof will be found later in this section.\n   Introduction principal axes orthogonal diagonalization orthogonally diagonalizable orthogonally diagonalizes \n            For each matrix   whose eigenvalues and corresponding eigenvectors are given,\n            find a matrix   such that\n              is a diagonal matrix.\n           \n                    with eigenvalues   and 3 and corresponding eigenvectors\n                    and  .\n                 \n                    with eigenvalues   and   and corresponding eigenvectors\n                    and  .\n                 \n                    with eigenvalues  ,\n                   ,\n                  and   and corresponding eigenvectors  ,\n                   ,\n                  and  .\n                 \n            Which matrices in part 1 seem to satisfy the orthogonal diagonalization requirement?\n            Do you notice any common traits among these matrices?\n           Symmetric Matrices \n    As we saw in  ,\n    matrices that are not symmetric need not be orthogonally diagonalizable,\n    but the symmetric matrix examples are orthogonally diagonalizable.\n    We explore that idea in this section.\n   \n    If   is a matrix that orthogonally diagonalizes the matrix  ,\n    then  , where   is a diagonal matrix.\n    Since   and  , we have\n     .\n   \n    Therefore,   and matrices with this property are the only matrices that can be orthogonally diagonalized.\n    Recall that any matrix   satisfying\n      is a symmetric matrix.\n   \n    While we have just shown that the only matrices that can be orthogonally diagonalized are the symmetric matrices,\n    the amazing thing about symmetric matrices is that  every \n    symmetric matrix can be orthogonally diagonalized.\n    We will prove this shortly.\n   \n    Symmetric matrices have useful properties,\n    a few of which are given in the following activity\n    (we will use some of these properties later in this section).\n   \n      Let   be a symmetric\n        matrix and let   and   be vectors in  .\n     \n            Show that  .\n           \n            Show that  .\n           \n            Show that the eigenvalues of a\n              symmetric matrix   are real.\n           \n     \n    (c) shows that a   symmetric matrix has real eigenvalues.\n    This is a general result about real symmetric matrices.\n   \n        Let   be an   symmetric matrix with real entries.\n        Then the eigenvalues of   are real.\n       \n      Let   be an   symmetric matrix with real entries and let   be an eigenvalue of   with eigenvector  .\n      To show that   is real,\n      we will show that  .\n      We know\n       .\n     \n      Since   has real entries,\n      we also know that   is an eigenvalue for   with eigenvector  .\n      Multiply both sides of   on the left by   to obtain\n       .\n     \n      Now\n       \n      and equation   becomes\n       .\n     \n      Since  ,\n      this implies that   and   is real.\n     \n    To orthogonally diagonalize a matrix,\n    it must be the case that eigenvectors corresponding to different eigenvalues are orthogonal.\n    This is an important property and it would be useful to know when it happens.\n   \n      Let   be a real symmetric matrix with eigenvalues   and\n        and corresponding eigenvectors   and  ,\n      respectively.\n     \n            Use  \n            (b) to show that  .\n           \n            Explain why the result of part (a) shows that   and   are orthogonal if  .\n           \n      proves the following theorem.\n   \n        If   is a real symmetric matrix,\n        then eigenvectors corresponding to distinct eigenvalues are orthogonal.\n       \n    Recall that the only matrices that can be orthogonally diagonalized are the symmetric matrices.\n    Now we show that every real symmetric matrix can be orthogonally diagonalized,\n    which completely characterizes the matrices that are orthogonally diagonalizable.\n    The proof of the following theorem proceeds by induction.\n    A reader who has not yet encountered this technique of proof can safely skip the proof of this theorem without loss of continuity.\n   \n        Let   be a real symmetric matrix.\n        Then   is orthogonally diagonalizable.\n       \n      Let   be a real   symmetric matrix.\n      The proof proceeds by induction on  .\n      If  ,\n      then   is diagonal and orthogonally diagonalizable.\n      So assume that any real   symmetric matrix is orthogonally diagonalizable.\n      Assume that   is a real   symmetric matrix.\n      By Theorem 25.4\n      (find reference),\n      the eigenvalues of   are real.\n      Let   be a real eigenvalue of   with corresponding unit eigenvector  .\n      We can use the Gram-Schmidt process to extend\n        to an orthonormal basis   for  .\n      Let  .\n      Then   is an orthogonal matrix.\n      Also,\n       \n      where   is a   vector,\n        is the zero vector in  ,\n      and   is an   matrix.\n      Letting   we have that\n       ,\n      so   is a symmetric matrix.\n      Therefore,   and   is a symmetric matrix.\n      By our induction hypothesis,   is orthogonally diagonalizable.\n      That is, there exists an   orthogonal matrix   such that  ,\n      where   is a diagonal matrix.\n      Now define   by\n       ,\n      where   is the zero vector in  .\n      By construction, the columns of   are orthonormal,\n      so   is an orthogonal matrix.\n      Since   is also an orthogonal matrix,\n       \n      and   is an orthogonal matrix.\n      Finally,\n       .\n     \n      Therefore,   is a diagonal matrix and   orthogonally diagonalizes  .\n      This completes our proof.\n     spectrum The Spectral Theorem for Real Symmetric Matrices \n        Let   be an   symmetric matrix with real entries.\n        Then\n         \n             \n                has   real eigenvalues (counting multiplicities)\n             \n           \n             \n              the dimension of each eigenspace of   is the multiplicity of the corresponding eigenvalue as a root of the characteristic polynomial\n             \n           \n             \n              eigenvectors corresponding to different eigenvalues are orthogonal\n             \n           \n             \n                is orthogonally diagonalizable.\n             \n           \n       \n    So  any  real symmetric matrix is orthogonally diagonalizable.\n    We have seen examples of the orthogonal diagonalization of\n      real symmetric matrices with   distinct eigenvalues,\n    but how do we orthogonally diagonalize a symmetric matrix having eigenvalues of multiplicity greater than 1?\n    The next activity shows us the process.\n   \n      Let  .\n      The eigenvalues of   are 2 and 8, with eigenspace of dimension 2 and dimension 1, respectively.\n     \n            Explain why   can be orthogonally diagonalized.\n           \n            Two linearly independent eigenvectors for   corresponding to the eigenvalue 2 are\n              and  .\n            Note that   are not orthogonal,\n            so cannot be in an orthogonal basis of   consisting of eigenvectors of  .\n            So find a set   of orthogonal eigenvectors of   so that  .\n           \n            The vector   is an eigenvector for   corresponding to the eigenvalue 8.\n            What can you say about the orthogonality relationship between  's and  ?\n           \n            Find a matrix   that orthogonally diagonalizes  .\n            Verify your work.\n           The Spectral Decomposition of a Symmetric Matrix  \n    Let   be an   symmetric matrix with real entries.\n    The Spectral Theorem tells us we can find an orthonormal basis\n      of eigenvectors of  .\n    Let   for each  .\n    If  ,\n    then we know that\n     ,\n    where   is the   diagonal matrix\n     .\n   spectral decomposition \n        Let   be an   symmetric matrix with real entries,\n        and let   be an orthonormal basis of eigenvectors of   with   for each  .\n        For each  , let  .\n        Then\n         \n             \n               ,\n             \n           \n             \n                is a symmetric matrix for each  ,\n             \n           \n             \n                is a rank 1 matrix for each  ,\n             \n           \n             \n                for each  ,\n             \n           \n             \n                if  ,\n             \n           \n             \n                for each  ,\n             \n           \n             \n                if  ,\n             \n           \n             \n              For any vector   in  ,\n               .\n             \n           \n       \n    The consequence of  \n    is that any symmetric matrix can be written as the sum of symmetric,\n    rank 1 matrices.\n    As we will see later,\n    this kind of decomposition contains much information about the matrix product\n      for any matrix  .\n   \n      Let  .\n      Let  ,  ,\n      and   be the eigenvalues of  .\n      A basis for the eigenspace   of   corresponding to the eigenvalue 8 is\n        and a basis for the eigenspace   of   corresponding to the eigenvalue 2 is  .\n      (Compare to  .)\n     \n            Find orthonormal eigenvectors  ,  ,\n            and   of   corresponding to  ,\n             , and  , respectively.\n           \n            Compute  \n           \n            Compute  \n           \n            Compute  \n           \n            Verify that  .\n           Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        For each of the following matrices  ,\n        determine if   is diagonalizable.\n        If   is not diagonalizable, explain why.\n        If   is diagonalizable,\n        find a matrix   so that\n          is a diagonal matrix.\n        If the matrix is diagonalizable,\n        is it orthogonally diagonalizable?\n        If orthogonally diagonalizable,\n        find an orthogonal matrix that diagonalizes  .\n        Use appropriate technology to find eigenvalues and eigenvectors.\n       \n               \n             \n              Recall that an   matrix   is diagonalizable if and only if   has   linearly independent eigenvectors,\n              and   is orthogonally diagonalizable if and only if   is symmetric.\n              Since   is not symmetric,\n                is not orthogonally diagonalizable.\n              Technology shows that the eigenvalues of   are   and   and bases for the corresponding eigenspaces are\n                and  .\n              So   is diagonalizable and if  , then\n               .\n             \n               \n             \n              Since   is not symmetric,\n                is not orthogonally diagonalizable.\n              Technology shows that the eigenvalues of   are   and   and bases for the corresponding eigenspaces are\n                and  .\n              We cannot create a basis of   consisting of eigenvectors of  ,\n              so   is not diagonalizable.\n             \n                \n             \n              Since   is symmetric,   is orthogonally diagonalizable.\n              Technology shows that the eigenvalues of   are   and   and bases for the eigenspaces\n                and  ,\n              respectively.\n              To find an orthogonal matrix that diagonalizes  ,\n              we must find an orthonormal basis of   consisting of eigenvectors of  .\n              To do that, we use the Gram-Schmidt process to obtain an orthogonal basis for the eigenspace of   corresponding to the eigenvalue  .\n              Doing so gives an orthogonal basis  ,\n              where   and\n               .\n              So an orthonormal basis for   of eigenvectors of   is\n               .\n              Therefore,   is orthogonally diagonalizable and if   is the matrix  , then\n               .\n             \n        Let  .\n        Find an orthonormal basis for   consisting of eigenvectors of  .\n       \n        Since   is symmetric,\n        there is an orthogonal matrix   such that   is diagonal.\n        The columns of   will form an orthonormal basis for  .\n        Using a cofactor expansion along the first row shows that\n         .\n       \n        So the eigenvalues of   are   and  .\n        The reduced row echelon forms of   and   are,\n        respectively,\n         .\n       \n        Thus, a basis for the eigenspace   of   is\n          and a basis for the eigenspace   of   is  .\n        The set   is an orthogonal set,\n        so an orthonormal basis for   consisting of eigenvectors of   is\n         .\n       Summary \n       \n        An   matrix   is orthogonally diagonalizable if there is an orthogonal matrix   such that\n          is a diagonal matrix.\n        Orthogonal diagonalizability is useful in that it allows us to find a\n         convenient \n        coordinate system in which to interpret the results of certain matrix transformations.\n        Orthogonal diagonalization also a plays a crucial role in the singular value decomposition of a matrix.\n       \n     \n       \n        An   matrix   is symmetric if  .\n        The symmetric matrices are exactly the matrices that can be orthogonally diagonalized.\n       \n     \n       \n        The spectrum of a matrix is the set of eigenvalues of the matrix.\n       \n     \n        For each of the following matrices,\n        find an orthogonal matrix   so that\n          is a diagonal matrix,\n        or explain why no such matrix exists.\n       \n                \n             \n               \n             \n                \n             \n               \n             \n                \n             \n                is not orthogonally diagonalizable.\n             \n        For each of the following matrices find an orthonormal basis of eigenvectors of  .\n        Then find a spectral decomposition of  .\n       \n                \n             \n                \n             \n                \n             \n                \n             \n        Find a non-diagonal   matrix with eigenvalues 2, 3 and 6 which can be orthogonally diagonalized.\n       \n         \n       \n        Let   be an\n          matrix with columns  ,\n         ,  ,  ,\n        and let   be an\n          matrix  with rows  ,  ,\n         ,  .\n        Show that\n         .\n       \n        Let   be an   symmetric matrix with real entries and let\n          be an orthonormal basis of eigenvectors of  .\n        For each  , let  .\n        Prove     that is,\n        verify each of the following statements.\n       \n              For each  ,   is a symmetric matrix.\n             \n              Determine  \n             \n              For each  ,   is a rank 1 matrix.\n             \n              Show that every column of   is a scalar multiple of  .\n             \n              For each  ,  .\n             \n              Use the orthonormal basis to simplify  .\n             \n              If  , then  .\n             \n              Use the orthonormal basis to simplify  .\n             \n              For each  ,  .\n             \n              Use the orthonormal basis to simplify  .\n             \n              If  , then  .\n             \n              Use the orthonormal basis to simplify  .\n             orthogonal projection matrix \n              Recall that  .\n             \n        Show that if   is an   matrix and\n          for every   in  ,\n        then   is a symmetric matrix.\n        \n       \n        Try   and  .\n       \n        Let   be an   symmetric matrix and assume that   has an orthonormal basis  ,\n         ,  ,\n          of eigenvectors of   so that   for each  .\n        Let   for each  .\n        It is possible that not all of the eigenvalue of   are distinct.\n        In this case,\n        some of the eigenvalues will be repeated in the spectral decomposition of  .\n        If we want only distinct eigenvalues to appear,\n        we might do the following.\n        Let  ,  ,  ,\n          be the distinct eigenvalues of  .\n        For each   between 1 and  ,\n        let   be the sum of all of the   that have   as eigenvalue.\n       \n              The eigenvalues for the matrix\n                are   and  .\n              Find a basis for each eigenspace and determine each  .\n              Then find  ,  ,  ,\n               , and each  .\n             \n              A basis for the eigenspace of   corresponding to the eigenvalue   is\n                and a basis for the eigenspace of   corresponding to the eigenvalue 4 is  .\n               ,\n               ,\n               ,\n               ,\n               ,  ,\n               ,\n              and  .\n             \n              Show in general (not just for the specific example in part (a),\n              that the   satisfy the same properties as the  .\n              That is, verify the following.\n             \n                     \n                   \n                    Collect matrices with the same eigenvalues.\n                   \n                      is a symmetric matrix for each  \n                   \n                    Use the fact that each   is a symmetric matrix.\n                   \n                      for each  \n                   \n                    Use Theorem 31.8.\n                   \n                      when  \n                   \n                    Use Theorem 31.8.\n                   \n                    if   is the eigenspace for   corresponding to the eigenvalue  ,\n                    and if   is in  ,\n                    then  .\n                   \n                    Explain why  ,\n                   ,  ,\n                    is a orthonormal basis for  .\n                   \n              What is the rank of  ?\n              Verify your answer.\n             \n              The rank of   is  .\n             \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              Every real symmetric matrix is diagonalizable.\n             \n              T\n             True\/False \n              If   is a matrix whose columns are eigenvectors of a symmetric matrix,\n              then the columns of   are orthogonal.\n             True\/False \n              If   is a symmetric matrix,\n              then eigenvectors of   corresponding to distinct eigenvalues are orthogonal.\n             \n              F\n             True\/False \n              If   and   are distinct eigenvectors of a symmetric matrix  ,\n              then   and   are orthogonal.\n             True\/False \n              Any symmetric matrix can be written as a sum of symmetric rank 1 matrices.\n             \n              T\n             True\/False \n              If   is a matrix satisfying  ,\n              and   and   are vectors satisfying\n                and  ,\n              then  .\n             True\/False \n              If an   matrix   has   orthogonal eigenvectors,\n              then   is a symmetric matrix.\n             \n              T\n             True\/False \n              If an   matrix has   real eigenvalues\n              (counted with multiplicity),\n              then   is a symmetric matrix.\n             True\/False \n              For each eigenvalue of a symmetric matrix,\n              the algebraic multiplicity equals the geometric multiplicity.\n             \n              T\n             True\/False \n              If   is invertible and orthogonally diagonalizable,\n              then so is  .\n             True\/False \n              If   are orthogonally diagonalizable\n                matrices, then so is  .\n             \n              F\n             Project: The Second Derivative Test for Functions of Two Variables \n    In this project we will verify the Second Derivative Test for functions of two variables. \n    Many thanks to Professor Paul Fishback for sharing his activity on this topic.\n    Much of this project comes from his activity.\n      This test will involve Taylor polynomials and linear algebra.\n    As a quick review,\n    recall that the second order Taylor polynomial for a function   of a single variable   at   is\n     .\n   \n    As with the linearization of a function,\n    the second order Taylor polynomial is a good approximation to   around     that is\n      for   close to  .\n    If   is a critical number for   with  , then\n     .\n   \n    In this situation, if  ,\n    then   for   close to  ,\n    which makes  .\n    This implies that   for   close to  ,\n    which makes   a relative maximum value for  .\n    Similarly, if  ,\n    then   is a relative minimum.\n   \n    We now need a Taylor polynomial for a function of two variables.\n    The complication of the additional independent variable in the two variable case means that the Taylor polynomials will need to contain all of the possible monomials of the indicated degrees.\n    Recall that the linearization\n    (or tangent plane)\n    to a function   at a point   is given by\n     .\n   \n    Note that  ,\n     ,\n    and  .\n    This makes   the best linear approximation to   near the point  .\n    The polynomial   is the first order Taylor polynomial for   at  .\n   \n    Similarly, the second order Taylor polynomial\n      centered at the point   for the function   is\n     .\n   \n      To see that   is the best approximation for   near  ,\n      we need to know that the first and second order partial derivatives of   agree with the corresponding partial derivatives of   at the point  .\n      Verify that this is true.\n     Hessian \n      Use Equation   to compute   for\n        at  .\n     \n    The important idea for us is that if   is a point at which   and   are zero,\n    then   is the zero vector and Equation   reduces to\n     ,\n   \n    To make the connection between the multivariable second derivative test and properties of the Hessian,\n     ,\n    at a critical point of a function   at which  ,\n    we will need to connect the eigenvalues of a matrix to the determinant and the trace.\n   \n    Let   be an   matrix with eigenvalues  ,\n     ,\n     ,  \n    (not necessarily distinct).\n     \n    in   shows that\n     .\n   \n    In other words,\n    the determinant of a matrix is equal to the product of the eigenvalues of the matrix.\n    In addition,  \n    in   shows that\n     .\n    for a diagonalizable matrix,\n    where   is the sum of the diagonal entries of  .\n    Equation   is true for any square matrix,\n    but we don't need the more general result for this project.\n   \n    The fact that the Hessian is a symmetric matrix makes it orthogonally diagonalizable.\n    We denote the eigenvalues of   as\n      and  .\n    Thus there exists an orthogonal matrix   and a diagonal matrix\n      such that  ,\n    or  .\n    Equations  \n    and   show that\n     .\n   \n    Now we have the machinery to verify the Second Derivative Test for Two-Variable Functions.\n    We assume   is a point in the domain of a function   so that  .\n    First we consider the case where  .\n   \n      Explain why if  , then\n       \n      is indefinite.\n      Explain why this implies that   is\n       saddle-shaped \n      near  .\n     \n        Substitute  .\n        What does the graph of   look like in the   and   directions?\n       \n    Now we examine the situation when  .\n   \n      Assume that  .\n     \n            Explain why either both   and\n              are positive or both are negative.\n           \n            If   and  ,\n            explain why   and   must be positive.\n           \n            Explain why, if   and  ,\n            then   is a local minimum value for  .\n           \n    When   and either\n      or   is negative,\n    a slight modification of the preceding argument leads to the fact that   has a local maximum at  \n    (the details are left to the reader).\n    Therefore, we have proved the Second Derivative Test for functions of two variables!\n   \n      Use the Hessian to classify the local maxima,\n      minima, and saddle points of  .\n      Draw a graph of   to illustrate.\n     "
},
{
  "id": "objectives-27",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#objectives-27",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            What does it mean for a matrix to be orthogonally diagonalizable and why is this concept important?\n           \n         \n           \n            What is a symmetric matrix and what important property related to diagonalization does a symmetric matrix have?\n           \n         \n           \n            What is the spectrum of a matrix?\n           \n         "
},
{
  "id": "theorem-64",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#theorem-64",
  "type": "Theorem",
  "number": "27.1",
  "title": "The Second Derivative Test for Single-Variable Functions.",
  "body": "The Second Derivative Test for Single-Variable Functions \n        If   is a critical number of a function   so that   and if   exists, then\n         \n             \n              if  ,\n              then   is a local maximum value of  ,\n             \n           \n             \n              if  ,\n              then   is a local minimum value of  , and\n             \n           \n             \n              if  , this test yields no information.\n             \n           \n       "
},
{
  "id": "theorem-65",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#theorem-65",
  "type": "Theorem",
  "number": "27.2",
  "title": "The Second Derivative Test for Functions of Two Variables.",
  "body": "The Second Derivative Test for Functions of Two Variables \n        Suppose   is a critical point of the function   for which\n          and  .\n        Let   be the quantity defined by\n         .\n         \n             \n              If   and  ,\n              then   has a local maximum at  .\n             \n           \n             \n              If   and  ,\n              then   has a local minimum at  .\n             \n           \n             \n              If  , then   has a saddle point at  .\n             \n           \n             \n              If  ,\n              then this test yields no information about what happens at  .\n             \n           \n       "
},
{
  "id": "p-4548",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#p-4548",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "principal axes "
},
{
  "id": "def_7_a_orthogonal__diagonalization",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#def_7_a_orthogonal__diagonalization",
  "type": "Definition",
  "number": "27.3",
  "title": "",
  "body": "orthogonal diagonalization orthogonally diagonalizable orthogonally diagonalizes "
},
{
  "id": "pa_7_a",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#pa_7_a",
  "type": "Preview Activity",
  "number": "27.1",
  "title": "",
  "body": "\n            For each matrix   whose eigenvalues and corresponding eigenvectors are given,\n            find a matrix   such that\n              is a diagonal matrix.\n           \n                    with eigenvalues   and 3 and corresponding eigenvectors\n                    and  .\n                 \n                    with eigenvalues   and   and corresponding eigenvectors\n                    and  .\n                 \n                    with eigenvalues  ,\n                   ,\n                  and   and corresponding eigenvectors  ,\n                   ,\n                  and  .\n                 \n            Which matrices in part 1 seem to satisfy the orthogonal diagonalization requirement?\n            Do you notice any common traits among these matrices?\n           "
},
{
  "id": "act_7_a_symmetric",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#act_7_a_symmetric",
  "type": "Activity",
  "number": "27.2",
  "title": "",
  "body": "\n      Let   be a symmetric\n        matrix and let   and   be vectors in  .\n     \n            Show that  .\n           \n            Show that  .\n           \n            Show that the eigenvalues of a\n              symmetric matrix   are real.\n           "
},
{
  "id": "thm_7_a_symmetric_eigenvalues",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#thm_7_a_symmetric_eigenvalues",
  "type": "Theorem",
  "number": "27.4",
  "title": "",
  "body": "\n        Let   be an   symmetric matrix with real entries.\n        Then the eigenvalues of   are real.\n       \n      Let   be an   symmetric matrix with real entries and let   be an eigenvalue of   with eigenvector  .\n      To show that   is real,\n      we will show that  .\n      We know\n       .\n     \n      Since   has real entries,\n      we also know that   is an eigenvalue for   with eigenvector  .\n      Multiply both sides of   on the left by   to obtain\n       .\n     \n      Now\n       \n      and equation   becomes\n       .\n     \n      Since  ,\n      this implies that   and   is real.\n     "
},
{
  "id": "act_7_a_symmetric_eigenvalues",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#act_7_a_symmetric_eigenvalues",
  "type": "Activity",
  "number": "27.3",
  "title": "",
  "body": "\n      Let   be a real symmetric matrix with eigenvalues   and\n        and corresponding eigenvectors   and  ,\n      respectively.\n     \n            Use  \n            (b) to show that  .\n           \n            Explain why the result of part (a) shows that   and   are orthogonal if  .\n           "
},
{
  "id": "theorem-67",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#theorem-67",
  "type": "Theorem",
  "number": "27.5",
  "title": "",
  "body": "\n        If   is a real symmetric matrix,\n        then eigenvectors corresponding to distinct eigenvalues are orthogonal.\n       "
},
{
  "id": "theorem-68",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#theorem-68",
  "type": "Theorem",
  "number": "27.6",
  "title": "",
  "body": "\n        Let   be a real symmetric matrix.\n        Then   is orthogonally diagonalizable.\n       \n      Let   be a real   symmetric matrix.\n      The proof proceeds by induction on  .\n      If  ,\n      then   is diagonal and orthogonally diagonalizable.\n      So assume that any real   symmetric matrix is orthogonally diagonalizable.\n      Assume that   is a real   symmetric matrix.\n      By Theorem 25.4\n      (find reference),\n      the eigenvalues of   are real.\n      Let   be a real eigenvalue of   with corresponding unit eigenvector  .\n      We can use the Gram-Schmidt process to extend\n        to an orthonormal basis   for  .\n      Let  .\n      Then   is an orthogonal matrix.\n      Also,\n       \n      where   is a   vector,\n        is the zero vector in  ,\n      and   is an   matrix.\n      Letting   we have that\n       ,\n      so   is a symmetric matrix.\n      Therefore,   and   is a symmetric matrix.\n      By our induction hypothesis,   is orthogonally diagonalizable.\n      That is, there exists an   orthogonal matrix   such that  ,\n      where   is a diagonal matrix.\n      Now define   by\n       ,\n      where   is the zero vector in  .\n      By construction, the columns of   are orthonormal,\n      so   is an orthogonal matrix.\n      Since   is also an orthogonal matrix,\n       \n      and   is an orthogonal matrix.\n      Finally,\n       .\n     \n      Therefore,   is a diagonal matrix and   orthogonally diagonalizes  .\n      This completes our proof.\n     "
},
{
  "id": "p-4580",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#p-4580",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "spectrum "
},
{
  "id": "theorem-69",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#theorem-69",
  "type": "Theorem",
  "number": "27.7",
  "title": "The Spectral Theorem for Real Symmetric Matrices.",
  "body": "The Spectral Theorem for Real Symmetric Matrices \n        Let   be an   symmetric matrix with real entries.\n        Then\n         \n             \n                has   real eigenvalues (counting multiplicities)\n             \n           \n             \n              the dimension of each eigenspace of   is the multiplicity of the corresponding eigenvalue as a root of the characteristic polynomial\n             \n           \n             \n              eigenvectors corresponding to different eigenvalues are orthogonal\n             \n           \n             \n                is orthogonally diagonalizable.\n             \n           \n       "
},
{
  "id": "act_7_a_3",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#act_7_a_3",
  "type": "Activity",
  "number": "27.4",
  "title": "",
  "body": "\n      Let  .\n      The eigenvalues of   are 2 and 8, with eigenspace of dimension 2 and dimension 1, respectively.\n     \n            Explain why   can be orthogonally diagonalized.\n           \n            Two linearly independent eigenvectors for   corresponding to the eigenvalue 2 are\n              and  .\n            Note that   are not orthogonal,\n            so cannot be in an orthogonal basis of   consisting of eigenvectors of  .\n            So find a set   of orthogonal eigenvectors of   so that  .\n           \n            The vector   is an eigenvector for   corresponding to the eigenvalue 8.\n            What can you say about the orthogonality relationship between  's and  ?\n           \n            Find a matrix   that orthogonally diagonalizes  .\n            Verify your work.\n           "
},
{
  "id": "p-4593",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#p-4593",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "spectral decomposition "
},
{
  "id": "thm_7_a_spectral_decomposition",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#thm_7_a_spectral_decomposition",
  "type": "Theorem",
  "number": "27.8",
  "title": "",
  "body": "\n        Let   be an   symmetric matrix with real entries,\n        and let   be an orthonormal basis of eigenvectors of   with   for each  .\n        For each  , let  .\n        Then\n         \n             \n               ,\n             \n           \n             \n                is a symmetric matrix for each  ,\n             \n           \n             \n                is a rank 1 matrix for each  ,\n             \n           \n             \n                for each  ,\n             \n           \n             \n                if  ,\n             \n           \n             \n                for each  ,\n             \n           \n             \n                if  ,\n             \n           \n             \n              For any vector   in  ,\n               .\n             \n           \n       "
},
{
  "id": "act_7_a_4",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#act_7_a_4",
  "type": "Activity",
  "number": "27.5",
  "title": "",
  "body": "\n      Let  .\n      Let  ,  ,\n      and   be the eigenvalues of  .\n      A basis for the eigenspace   of   corresponding to the eigenvalue 8 is\n        and a basis for the eigenspace   of   corresponding to the eigenvalue 2 is  .\n      (Compare to  .)\n     \n            Find orthonormal eigenvectors  ,  ,\n            and   of   corresponding to  ,\n             , and  , respectively.\n           \n            Compute  \n           \n            Compute  \n           \n            Compute  \n           \n            Verify that  .\n           "
},
{
  "id": "example-54",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#example-54",
  "type": "Example",
  "number": "27.9",
  "title": "",
  "body": "\n        For each of the following matrices  ,\n        determine if   is diagonalizable.\n        If   is not diagonalizable, explain why.\n        If   is diagonalizable,\n        find a matrix   so that\n          is a diagonal matrix.\n        If the matrix is diagonalizable,\n        is it orthogonally diagonalizable?\n        If orthogonally diagonalizable,\n        find an orthogonal matrix that diagonalizes  .\n        Use appropriate technology to find eigenvalues and eigenvectors.\n       \n               \n             \n              Recall that an   matrix   is diagonalizable if and only if   has   linearly independent eigenvectors,\n              and   is orthogonally diagonalizable if and only if   is symmetric.\n              Since   is not symmetric,\n                is not orthogonally diagonalizable.\n              Technology shows that the eigenvalues of   are   and   and bases for the corresponding eigenspaces are\n                and  .\n              So   is diagonalizable and if  , then\n               .\n             \n               \n             \n              Since   is not symmetric,\n                is not orthogonally diagonalizable.\n              Technology shows that the eigenvalues of   are   and   and bases for the corresponding eigenspaces are\n                and  .\n              We cannot create a basis of   consisting of eigenvectors of  ,\n              so   is not diagonalizable.\n             \n                \n             \n              Since   is symmetric,   is orthogonally diagonalizable.\n              Technology shows that the eigenvalues of   are   and   and bases for the eigenspaces\n                and  ,\n              respectively.\n              To find an orthogonal matrix that diagonalizes  ,\n              we must find an orthonormal basis of   consisting of eigenvectors of  .\n              To do that, we use the Gram-Schmidt process to obtain an orthogonal basis for the eigenspace of   corresponding to the eigenvalue  .\n              Doing so gives an orthogonal basis  ,\n              where   and\n               .\n              So an orthonormal basis for   of eigenvectors of   is\n               .\n              Therefore,   is orthogonally diagonalizable and if   is the matrix  , then\n               .\n             "
},
{
  "id": "example-55",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#example-55",
  "type": "Example",
  "number": "27.10",
  "title": "",
  "body": "\n        Let  .\n        Find an orthonormal basis for   consisting of eigenvectors of  .\n       \n        Since   is symmetric,\n        there is an orthogonal matrix   such that   is diagonal.\n        The columns of   will form an orthonormal basis for  .\n        Using a cofactor expansion along the first row shows that\n         .\n       \n        So the eigenvalues of   are   and  .\n        The reduced row echelon forms of   and   are,\n        respectively,\n         .\n       \n        Thus, a basis for the eigenspace   of   is\n          and a basis for the eigenspace   of   is  .\n        The set   is an orthogonal set,\n        so an orthonormal basis for   consisting of eigenvectors of   is\n         .\n       "
},
{
  "id": "exercise-266",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#exercise-266",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        For each of the following matrices,\n        find an orthogonal matrix   so that\n          is a diagonal matrix,\n        or explain why no such matrix exists.\n       \n                \n             \n               \n             \n                \n             \n               \n             \n                \n             \n                is not orthogonally diagonalizable.\n             "
},
{
  "id": "exercise-267",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#exercise-267",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        For each of the following matrices find an orthonormal basis of eigenvectors of  .\n        Then find a spectral decomposition of  .\n       \n                \n             \n                \n             \n                \n             \n                \n             "
},
{
  "id": "exercise-268",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#exercise-268",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Find a non-diagonal   matrix with eigenvalues 2, 3 and 6 which can be orthogonally diagonalized.\n       \n         \n       "
},
{
  "id": "ex_7_a_product",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#ex_7_a_product",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Let   be an\n          matrix with columns  ,\n         ,  ,  ,\n        and let   be an\n          matrix  with rows  ,  ,\n         ,  .\n        Show that\n         .\n       "
},
{
  "id": "ex_7_a_spectral_decomposition",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#ex_7_a_spectral_decomposition",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Let   be an   symmetric matrix with real entries and let\n          be an orthonormal basis of eigenvectors of  .\n        For each  , let  .\n        Prove     that is,\n        verify each of the following statements.\n       \n              For each  ,   is a symmetric matrix.\n             \n              Determine  \n             \n              For each  ,   is a rank 1 matrix.\n             \n              Show that every column of   is a scalar multiple of  .\n             \n              For each  ,  .\n             \n              Use the orthonormal basis to simplify  .\n             \n              If  , then  .\n             \n              Use the orthonormal basis to simplify  .\n             \n              For each  ,  .\n             \n              Use the orthonormal basis to simplify  .\n             \n              If  , then  .\n             \n              Use the orthonormal basis to simplify  .\n             orthogonal projection matrix \n              Recall that  .\n             "
},
{
  "id": "exercise-271",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#exercise-271",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Show that if   is an   matrix and\n          for every   in  ,\n        then   is a symmetric matrix.\n        \n       \n        Try   and  .\n       "
},
{
  "id": "exercise-272",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#exercise-272",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        Let   be an   symmetric matrix and assume that   has an orthonormal basis  ,\n         ,  ,\n          of eigenvectors of   so that   for each  .\n        Let   for each  .\n        It is possible that not all of the eigenvalue of   are distinct.\n        In this case,\n        some of the eigenvalues will be repeated in the spectral decomposition of  .\n        If we want only distinct eigenvalues to appear,\n        we might do the following.\n        Let  ,  ,  ,\n          be the distinct eigenvalues of  .\n        For each   between 1 and  ,\n        let   be the sum of all of the   that have   as eigenvalue.\n       \n              The eigenvalues for the matrix\n                are   and  .\n              Find a basis for each eigenspace and determine each  .\n              Then find  ,  ,  ,\n               , and each  .\n             \n              A basis for the eigenspace of   corresponding to the eigenvalue   is\n                and a basis for the eigenspace of   corresponding to the eigenvalue 4 is  .\n               ,\n               ,\n               ,\n               ,\n               ,  ,\n               ,\n              and  .\n             \n              Show in general (not just for the specific example in part (a),\n              that the   satisfy the same properties as the  .\n              That is, verify the following.\n             \n                     \n                   \n                    Collect matrices with the same eigenvalues.\n                   \n                      is a symmetric matrix for each  \n                   \n                    Use the fact that each   is a symmetric matrix.\n                   \n                      for each  \n                   \n                    Use Theorem 31.8.\n                   \n                      when  \n                   \n                    Use Theorem 31.8.\n                   \n                    if   is the eigenspace for   corresponding to the eigenvalue  ,\n                    and if   is in  ,\n                    then  .\n                   \n                    Explain why  ,\n                   ,  ,\n                    is a orthonormal basis for  .\n                   \n              What is the rank of  ?\n              Verify your answer.\n             \n              The rank of   is  .\n             "
},
{
  "id": "exercise-273",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#exercise-273",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              Every real symmetric matrix is diagonalizable.\n             \n              T\n             True\/False \n              If   is a matrix whose columns are eigenvectors of a symmetric matrix,\n              then the columns of   are orthogonal.\n             True\/False \n              If   is a symmetric matrix,\n              then eigenvectors of   corresponding to distinct eigenvalues are orthogonal.\n             \n              F\n             True\/False \n              If   and   are distinct eigenvectors of a symmetric matrix  ,\n              then   and   are orthogonal.\n             True\/False \n              Any symmetric matrix can be written as a sum of symmetric rank 1 matrices.\n             \n              T\n             True\/False \n              If   is a matrix satisfying  ,\n              and   and   are vectors satisfying\n                and  ,\n              then  .\n             True\/False \n              If an   matrix   has   orthogonal eigenvectors,\n              then   is a symmetric matrix.\n             \n              T\n             True\/False \n              If an   matrix has   real eigenvalues\n              (counted with multiplicity),\n              then   is a symmetric matrix.\n             True\/False \n              For each eigenvalue of a symmetric matrix,\n              the algebraic multiplicity equals the geometric multiplicity.\n             \n              T\n             True\/False \n              If   is invertible and orthogonally diagonalizable,\n              then so is  .\n             True\/False \n              If   are orthogonally diagonalizable\n                matrices, then so is  .\n             \n              F\n             "
},
{
  "id": "project-85",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#project-85",
  "type": "Project Activity",
  "number": "27.6",
  "title": "",
  "body": "\n      To see that   is the best approximation for   near  ,\n      we need to know that the first and second order partial derivatives of   agree with the corresponding partial derivatives of   at the point  .\n      Verify that this is true.\n     "
},
{
  "id": "p-4698",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#p-4698",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Hessian "
},
{
  "id": "ex_example_1",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#ex_example_1",
  "type": "Project Activity",
  "number": "27.7",
  "title": "",
  "body": "\n      Use Equation   to compute   for\n        at  .\n     "
},
{
  "id": "project-87",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#project-87",
  "type": "Project Activity",
  "number": "27.8",
  "title": "",
  "body": "\n      Explain why if  , then\n       \n      is indefinite.\n      Explain why this implies that   is\n       saddle-shaped \n      near  .\n     \n        Substitute  .\n        What does the graph of   look like in the   and   directions?\n       "
},
{
  "id": "project-88",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#project-88",
  "type": "Project Activity",
  "number": "27.9",
  "title": "",
  "body": "\n      Assume that  .\n     \n            Explain why either both   and\n              are positive or both are negative.\n           \n            If   and  ,\n            explain why   and   must be positive.\n           \n            Explain why, if   and  ,\n            then   is a local minimum value for  .\n           "
},
{
  "id": "project-89",
  "level": "2",
  "url": "chap_orthogonal_diagonalization.html#project-89",
  "type": "Project Activity",
  "number": "27.10",
  "title": "",
  "body": "\n      Use the Hessian to classify the local maxima,\n      minima, and saddle points of  .\n      Draw a graph of   to illustrate.\n     "
},
{
  "id": "chap_principal_axis_theorem",
  "level": "1",
  "url": "chap_principal_axis_theorem.html",
  "type": "Section",
  "number": "28",
  "title": "Quadratic Forms and the Principal Axis Theorem",
  "body": "Quadratic Forms and the Principal Axis Theorem \n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is a quadratic form on  ?\n           \n         \n           \n            What does the Principal Axis Theorem tell us about quadratic forms?\n           \n         Application: The Tennis Racket Effect \n    Try an experiment with a tennis racket\n    (or a squash racket, or a ping pong paddle).\n    Let us define a 3D coordinate system with the center of the racket as the origin with the head of the racket lying in the  -plane.\n    We let   be the vector in the direction of the handle and   the perpendicular direction\n    (still lying in the plane defined by the head)\n    as illustrated in  .\n    We then let   be a vector perpendicular to the plane of the head.\n    Hold the racket by the handle and spin it to make one rotation around the   axis.\n    This is pretty easy.\n    It is also not difficult to throw the racket so that it rotates around the  .\n    Now toss the racket into the air to make one complete rotation around the axis of the vector   and catch the handle.\n    Repeat this several times.\n    You should notice that in most instances,\n    the racket will also have made a half rotation around the   axis so that the other face of the racket now points up.\n    This is quite different than the rotations around the   and   axes.\n    A good video that illustrates this behavior can be seen at \n     .\n   Two principal axes of a tennis racket. tennis racket effect Introduction \n    We are familiar with quadratic equations in algebra.\n    Examples of quadratic equations include  ,\n     , and  .\n    We don't, however, have to restrict ourselves to two variables.\n    A quadratic equation in   variables is any equation in which the sum of the exponents in any monomial term is 2.\n    So a quadratic equation in the variables  ,  ,\n     ,   is an equation in the form\n     \n    for some constant  .\n    In matrix notation this expression on the left of this equation has the form\n     \n    where   and   is the\n      matrix  .\n    For example,\n    if  ,\n    then we get the quadratic expression  .\n    We should note here that the terms involving   and   are repeated in our sum, but\n     \n    and so we could replace   and   both with\n      without changing the quadratic form.\n    With this alteration in mind,\n    we can then assume that   is a symmetric matrix.\n    So in the previous example,\n    the symmetric matrix   gives the same quadratic expression.\n    This leads to the following definition.\n   quadratic form quadratic form matrix of the quadratic form \n            To get a little more comfortable with quadratic forms,\n            write the quadratic forms in matrix form,\n            explicitly identifying the vector   and the symmetric matrix   of the quadratic form.\n           \n                   \n                 \n                   \n                 \n            Some quadratic forms form equations in   that are very familiar:\n              is an equation of a circle,\n              is an equation of an ellipse,\n            and   is an equation of a hyperbola.\n            Of course, these do not represent all of the quadratic forms in     some contain cross-product terms.\n            We can recognize the equations above because they contain no cross-product terms\n            (terms involving  ).\n            We can more easily recognize the quadratic forms that contain cross-product terms if  we can somehow rewrite the forms in a different format with no cross-product terms.\n            We illustrate how this can be done with the quadratic form   defined by  .\n           \n                  Write   in the form  ,\n                  where   is a   symmetric matrix.\n                 \n                  Since   is a symmetric matrix we can orthogonally diagonalize  .\n                  Given that the eigenvalues of   are   and\n                    with corresponding eigenvectors\n                    and  ,\n                  respectively,\n                  find a matrix   that orthogonally diagonalizes  .\n                 \n                  Define   to satisfy  .\n                  Substitute for   in the quadratic form   to write the quadratic form in terms of   and  .\n                  What kind of graph does the quadratic equation   have?\n                 Equations Involving Quadratic Forms in  \n    When we consider equations of the form  ,\n    where   is a quadratic form in   and   is a constant,\n    we wind up with old friends like  ,\n     , or  .\n    As we saw in  \n    these equations are relatively easy to recognize.\n    However, when we have cross-product terms,\n    like in  ,\n    it is not so easy to identify the curve the equation represents.\n    If there was a way to eliminate the cross-product term   from this form,\n    we might be more easily able to recognize its graph.\n    The discussion in this section will focus on quadratic forms in  ,\n    but we will see later that the arguments work in any number of dimensions.\n    While working in   we will use the standard variables   and   instead of   and  .\n   \n    In general, the equation of the form  ,\n    where   is a quadratic form in   defined by a matrix\n      and   is a constant looks like\n     .\n   \n    The graph of an equation like this is either an ellipse (a circle is a special case of an ellipse),\n    a hyperbola,\n    two non-intersecting lines, a point, or the empty set\n    (see  ).\n    The quadratic forms do not involve linear terms,\n    so we don't consider the cases of parabolas.\n    One way to see into which category one of these quadratic form equations falls is to write the equation in standard form.\n   \n    The standard forms for quadratic equations in   are as follows,\n    where   and   are nonzero constants and   and   are any constants.\n     \n         Lines: \n         \n            or   ( )\n         \n       \n         Ellipse: \n         \n           \n         \n       \n         Hyperbola: \n         \n            or  \n         \n       \n   \n     \n    contains the main tool that we need to convert a quadratic form into one of these standard forms.\n    By this we mean that if we have a quadratic form   in the variables  ,\n     ,  ,  ,\n    we want to find variables  ,\n     ,  ,\n      in terms of  ,\n     ,  ,\n      so that when written in terms of the variables  ,\n     ,  ,\n      the quadratic form   contains no cross terms.\n    In other words, we want to find a vector\n      so that  ,\n    where   is a diagonal matrix.\n    Since every real symmetric matrix is orthogonally diagonalizable,\n    we will always be able to find a matrix   that orthogonally diagonalizes  .\n    The details are as follows.\n   \n    Let   be the quadratic form defined by  ,\n    where   is an   symmetric matrix.\n    As in  ,\n    the fact that   is symmetric means that we can find an orthogonal matrix\n      whose columns are orthonormal eigenvectors of   corresponding to eigenvalues  ,\n     ,  ,\n     , respectively.\n    Letting   give us   and\n     ,\n    where   is the diagonal matrix whose  th diagonal entry is  .\n   \n    Moreover, the set   is an orthonormal basis for   and so defines a coordinate system for  .\n    Note that if  , then\n     .\n   \n    Thus, the coordinate vector of   with respect to   is  ,\n    or  .\n    We summarize in  .\n   Principal Axis Theorem \n        Let   be an   symmetric matrix.\n        There is an orthogonal change of variables\n          so that the quadratic form   defined by\n          is transformed into the quadratic form\n          where   is a diagonal matrix.\n       \n    The columns of the orthogonal matrix   in the Principal Axis Theorem form an orthogonal basis for   and are called the  principal axes \n    for the quadratic form  .\n    Also, the coordinate vector of   with respect to this basis is  .\n   \n      Let   be the quadratic form defined by  ,\n      where   and  .\n     \n            The eigenvalues of   are   and\n              with corresponding eigenvectors\n              and  ,\n            respectively.\n            Find an orthogonal matrix   with determinant 1 that diagonalizes  .\n            Is   unique?\n            Explain.\n            Is there a matrix without determinant 1 that orthogonally diagonalizes  ?\n            Explain.\n           \n            Use the matrix   to write the quadratic form without the cross-product.\n           \n            We can view   as a change of basis matrix from the coordinate system defined by\n              to the standard coordinate system.\n            In other words, in the standard   coordinate system,\n            the quadratic form is written as  ,\n            but in the new coordinate system defined by   the quadratic form is written as  .\n            As a change of basis matrix,   performs a rotation.\n            See if you can recall what we learned about rotation matrices and determine the angle of rotation   defines.\n            Plot the graph of the quadratic equation\n              in the new coordinate system and identify this angle on the graph.\n            Interpret the result.\n           Classifying Quadratic Forms \n    If we draw graphs of equations of the type  ,\n    where   is a quadratic form,\n    we can see that a quadratic form whose matrix does not have 0 as an eigenvalue can take on all positive values\n    (except at  )\n    as shown at left in  ,\n    all negative values\n    (except at  )\n    as shown in the center of  ,\n    or both positive and negative values as depicted at right in  .\n    We can see when these cases happen by analyzing the eigenvalues of the matrix that defines the quadratic form.\n    Let   be a   symmetric matrix with eigenvalues\n      and  ,\n    and let   be a matrix that orthogonally diagonalizes   so that  .\n    If we let  , then\n     .\n   \n    Then   if all of the eigenvalues of   are positive (with\n      when  ) and\n      if all of the eigenvalues of   are negative (with\n      when  ).\n    If one eigenvalue of   is positive and the other negative,\n    then   will take on both positive and negative values.\n    As a result, we classify symmetric matrices\n    (and their corresponding quadratic forms)\n    according to these behaviors.\n   Left: Paraboloid  . Center: Hyperbolic Paraboloid  . Right: Paraboloid  . matrix positive definite matrix positive semidefinite matrix negative definite matrix negative semidefinite matrix indefinite positive definite positive semidefinite negative definite negative semidefinite indefinite \n    For example, the quadratic form\n      at left in   is positive definite\n    (with repeated eigenvalue 1),\n    the quadratic form   at right in   is negative definite\n    (repeated eigenvalue  ),\n    and the hyperbolic paraboloid\n      in the center of   is indefinite\n    (eigenvalues   and  ).\n   \n    So we have argued that a quadratic form\n      is positive definite if   has all positive eigenvalues,\n    negative definite if   has all negative eigenvalues,\n    and indefinite if   has both positive and negative eigenvalues.\n    Similarly, the quadratic form is positive semidefinite if   has all nonnegative eigenvalues and negative semidefinite if   has all nonpositive eigenvalues.\n    Positive definite matrices are important,\n    as we discuss in the next section.\n   Inner Products inner products \n      Define a mapping from   to   by\n       \n      for   and   in  .\n      (The brackets   provide a shorthand way of representing the function.)\n     \n            Calculate  .\n           \n            If   and   are in  , is it true that\n             \n            Verify your answer.\n           \n            If  ,\n             , and   are in  , is it true that\n             \n            Verify your answer.\n           \n            If   and   are in   and   is a scalar,\n            is it true that\n             \n            Verify your answer.\n           \n            If   and   are in  ,\n            must it be the case that  ?\n            When is  ?\n           \n            There is a matrix   such that  .\n            Find this matrix  .\n           inner products inner product inner product \n    The dot product and the example in  \n    provide two examples of inner products.\n    The examples below provide two other important inner products on  .\n     \n         \n          If  ,  ,\n           ,   are positive scalars, then\n           \n          defines an inner product on  .\n         \n       \n         \n          Every invertible   matrix   defines an inner product on   by\n           .\n         \n       \n   \n    As   will demonstrate,\n    every inner product on   can be written in the form\n      for some special type of matrix  .\n   \n      Let   be a symmetric   matrix,\n      and define   by\n       .\n     \n            Explain why it is necessary for   to be positive definite in order for   to define an inner product on  .\n           \n            Show that   defines an inner product on   if   is positive definite.\n           \n            Let   be the mapping from   defined by\n             .\n            Find a matrix   so that   and explain why\n              defines an inner product.\n           Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Write the given quadratic equation in a system in which it has no cross-product terms.\n       \n               \n             \n              We write the quadratic form\n                as  ,\n              where   and  .\n              The eigenvalues for   are   and  ,\n              and bases for the corresponding eigenspaces   and   are\n                and  ,\n              respectively.\n              An orthogonal matrix   that orthogonally diagonalizes   is\n               .\n              If   and we let  ,\n              then we can rewrite the quadratic equation   as\n               .\n              So the quadratic equation   is an ellipse.\n             \n               \n             \n              We write the quadratic form\n                as  ,\n              where   and  .\n              The eigenvalues for   are   and  ,\n              and bases for the corresponding eigenspaces   and   are\n                and  ,\n              respectively.\n              An orthogonal matrix   that orthogonally diagonalizes   is\n               .\n              If   and we let  ,\n              then we can rewrite the quadratic equation   as\n               .\n              So the quadratic equation   is a hyperbola.\n             \n               \n             \n              We write the quadratic form\n                as  ,\n              where   and  .\n              The eigenvalues for   are   and  ,\n              and bases for the corresponding eigenspaces   and   are\n                and  ,\n              respectively.\n              Applying the Gram-Schmidt process to the basis for   gives us an orthogonal basis   of  ,\n              where   and\n               .\n              An orthogonal matrix   that orthogonally diagonalizes   is\n               .\n              If   and we let  ,\n              then we can rewrite the quadratic equation   as\n               .\n              So the quadratic equation   is a ellipsoid.\n             \n        Let   and   be positive definite matrices,\n        and let  .\n       \n              Must   be invertible?\n              Justify your answer.\n             \n              Since   has all positive eigenvalues and   is the product of the eigenvalues of  ,\n              then  .\n              Thus,   is invertible.\n             \n              Must   be positive definite?\n              Justify your answer.\n             \n              The fact that   is positive definite means that   is also symmetric.\n              Recall that  .\n              Since   is symmetric,\n              it follows that   and   is symmetric.\n              The eigenvalues of   are the reciprocals of the eigenvalues of  .\n              Since the eigenvalues of   are all positive,\n              so are the eigenvalues of  .\n              Thus,   is positive definite.\n             \n              Must   be positive definite?\n              Justify your answer.\n             \n              Notice that\n               ,\n              so   is symmetric.\n              The eigenvalues of   are the squares of the eigenvalues of  .\n              Since no eigenvalue of   is  ,\n              the eigenvalues of   are all positive and   is positive definite.\n             \n              Must   be positive definite?\n              Justify your answer.\n             \n              We know that   is symmetric, and\n               ,\n              so   is symmetric.\n              Also, the fact that   and\n                for all   implies that\n               \n              for all  .\n              Thus,   is positive definite.\n             \n              Is   positive definite?\n              Justify your answer.\n             \n              The matrix   is symmetric and\n               .\n              So the eigenvalues of   are\n                and  .\n              Since the eigenvalues of   are both positive,\n                is positive definite.\n             Summary \n       \n        A quadratic form on   is a function   defined by\n         \n        for some   symmetric matrix  .\n       \n     \n       \n        The Principal Axis Theorem tells us that there is a change of variable\n          that will remove the cross-product terms from a quadratic form and allow us to identify the form and determine the principal axes for the form.\n       \n     \n        Find the matrix for each quadratic form.\n       \n                if   is in  \n             \n               \n             \n                if   is in  \n             \n               \n             \n                if   is in  \n             \n               \n             \n        For each quadratic form, identify the matrix   of the form,\n        find a matrix   that orthogonally diagonalizes  ,\n        and make a change of variable that transforms the quadratic form into one with no cross-product terms.\n       \n               \n             \n               \n             \n               \n             \n        One topic in multivariable calculus is constrained optimization.\n        We can use the techniques of this section to solve certain types of constrained optimization problems involving quadratic forms.\n        As an example,\n        we will find the maximum and minimum values of the quadratic form defined by the matrix   on the unit circle.\n       \n              First we determine some bounds on the values of a quadratic form.\n              Let   be the quadratic form defined by the\n                real symmetric matrix  .\n              Let   be the eigenvalues of  ,\n              and let   be a matrix that orthogonally diagonalizes  ,\n              with   as the matrix with diagonal entries  ,\n               ,  ,\n                in order.\n              Let  .\n             \n                    Show that\n                     .\n                   \n                    Let   be a matrix that orthogonally diagonalizes  ,\n                    with  .\n                    Use this to calculate  .\n                   \n                    Use the fact that   for each   and the fact that  \n                    (and  )\n                    is an orthogonal matrix to show that\n                     .\n                   \n                    Substitute in part i.\n                   \n                    Now show that  .\n                   \n                    Make an argument similar to part ii.\n                   \n              Use the result of part (a) to find the maximum and minimum values of the quadratic form defined by the matrix   on the unit circle.\n             \n              The maximum value of   on the unit circle is 1 and it occurs at the input  .\n             \n        In this exercise we characterize the symmetric, positive definite,\n          matrices with real entries in terms of the entries of the matrices.\n        Let   for some real numbers  ,\n         ,\n        and  .\n       \n              Assume that   is positive definite.\n             \n                    Show that   must be positive.\n                   \n                    Use the fact that the eigenvalues of   must be positive to show that  .\n                    We conclude that if   is positive definite,\n                    then   and  .\n                   \n              Now show that if   and  ,\n              then   is positive definite.\n              This will complete our classification of positive definite   matrices.\n             \n        In this exercise we determine the form of\n         ,\n        where   is a symmetric   matrix.\n        Let   be a matrix that orthogonally diagonalizes   and let  .\n       \n              Substitute   for   in the equation  .\n              What form does the resulting equation have (write this form in terms of the eigenvalues of  )?\n             \n               .\n             \n              What kind of graph does the equation   have if   is positive definite?\n              Why?\n             \n              An ellipse.\n             \n              What kind of graph does the equation   have if   has both positive and negative eigenvalues?\n              Why?\n             \n              A hyperbola.\n             \n              What kind of graph does the equation   have if one eigenvalue of   is zero and the other non-zero?\n              Why?\n             \n              Two lines.\n             \n        Let   be a symmetric   matrix.\n       \n              Show that  ,\n              where   is the  th standard unit vector for  . (This result will be useful in  )\n             \n              Let   be a unit eigenvector of   with eigenvalue  .\n              Find   in terms of  .\n             \n        Suppose   and   are symmetric   matrices,\n        and let   and  .\n        If   for all   in  ,\n        show that  . (Hint: Use  \n        (a) to compare   and  ,\n        then compare   to\n          for  .) Thus,\n        quadratic forms are uniquely determined by their symmetric matrices.\n       \n        Use the previous exercise to compare\n          and  ,\n        then compare   to   for  .\n       \n        In this exercise we analyze all inner products on  .\n        Let   be an inner product on  .\n        Let   and\n          be arbitrary vectors in  .\n        Then\n         ,\n        where   is the  th standard vector in  .\n       \n              Explain why\n               .\n             \n              Calculate the matrix product\n               \n              and compare to  .\n              What do you notice?\n             \n              Explain why any inner product on   is of the form   for some symmetric,\n              positive definite matrix  .\n             \n         \n        shows that any inner product\n          on   has the form   for some symmetric,\n        positive definite matrix  .\n        Find the matrix   for which\n          for all   and   in  .\n       \n         \n       \n        Let   be an inner product on  ,\n        let  ,  ,\n        and   be vectors  ,\n        and let   be a scalar.\n        Verify the following properties.\n       \n               \n             \n               \n             \n               \n             \n               \n             Pythagorean Theorem in   with respect to an inner product \n        We extend the notions of length and orthogonality in   with respect to an inner product as follows.\n        If   is an inner product on  ,\n        we define the length of a vector   with respect to the inner product to be  .\n        We can also define two vectors   and   to be orthogonal if  .\n        In this exercise we verify the Pythagorean Theorem with respect to an inner product.\n       \n        The Pythagorean Theorem states that if   and   are the lengths of the legs of a right triangle whose hypotenuse has length  ,\n        then  .\n        If we think of the legs as defining vectors   and  ,\n        then the hypotenuse is the vector   and we can restate the Pythagorean Theorem as\n         .\n        In this exercise we show that this result holds in any dimension and for any inner product.\n        Use an arbitrary inner product  .\n       \n              Let   and   be orthogonal vectors in  .\n              Show that  .\n              \n             \n              Expand   using the dot product.\n             \n              Must it be true that if   and   are vectors in   with  ,\n              then   and   are orthogonal?\n              If not, provide a counterexample.\n              If true, verify the statement.\n             \n              Expand   using the dot product.\n             Cauchy-Schwarz inequality in   with respect to an inner product \n        The Cauchy-Schwarz inequality,\n         \n        for any vectors   and   in  ,\n        is considered one of the most important inequalities in mathematics.\n        We verify the Cauchy-Schwarz inequality for an arbitrary inner product   in this exercise.\n        Let   and   be vectors in  .\n       \n              Explain why the inequality   is true if either   or   is the zero vector.\n              As a consequence,\n              we assume that   and   are nonzero vectors for the remainder of this exercise.\n             \n              Let   and let  .\n              We know that  .\n              Use   of this section to show that\n               .\n             \n              Now show that  .\n             \n              Combine parts (b) and (c) to explain why equation   is true.\n             triangle inequality in   with respect to an inner product Triangle Inequality \n        Expand   using the dot product.\n       \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If   is a quadratic form,\n              then there is exactly one matrix   such that  .\n             \n              F\n             True\/False \n              The matrix of a quadratic form is unique.\n             True\/False \n              If the matrix of a quadratic form is a diagonal matrix,\n              then the quadratic form has no cross-product terms.\n             \n              T\n             True\/False \n              The eigenvectors of the symmetric matrix   form the principal axes of the quadratic form  .\n             True\/False \n              The principal axes of a quadratic form are orthogonal.\n             \n              T\n             True\/False \n              If   and   are positive,\n              then the quadratic equation   defines an ellipse.\n             True\/False \n              If the entries of a symmetric matrix   are all positive,\n              then the quadratic form   is positive definite.\n             \n              F\n             True\/False \n              If a quadratic form\n                defined by a symmetric matrix   is positive definite,\n              then the entries of   are all non-negative.\n             True\/False \n              If a quadratic form   on   is positive definite,\n              then the graph of   is a paraboloid opening upward.\n             \n              T\n             True\/False \n              If a quadratic form   on   is negative definite,\n              then the graph of   is a paraboloid opening downward.\n             True\/False \n              If a quadratic form   on   is indefinite,\n              then there is a nonzero vector   such that  .\n             \n              T\n             True\/False \n              If   is positive definite,\n              then so is the quadratic form   for  .\n             True\/False \n              If If   is indefinite,\n              then at least one of the eigenvalues of   is negative and at least one positive.\n             \n              T\n             True\/False \n              If   symmetric matrices   and   define positive definite quadratic forms,\n              then so does  .\n             True\/False \n              If an invertible symmetric matrix   defines a positive definite quadratic form,\n              then so does  .\n             \n              T\n             Project: The Tennis Racket Theorem moment of inertia moment of inertia primary intermediate \n    Assume that we have a rigid body moving through space.\n    Euler's (rotation) equation describes the rotation of a rigid body with respect to the body's principal axes of inertia.\n    Assume that  ,  ,\n    and   are the moments of inertia around the primary,\n    intermediate,\n    and third principal axes with  .\n    Also assume that  ,  ,\n    and   are the components of the angular velocity along each axis.\n    When there is no torque applied,\n    using a principal orthogonal coordinates, Euler's equation tells us that\n     .\n   \n    (The dots indicate a derivative with respect to time,\n    which is common notation in physics.) We will use Euler's equations to understand the Tennis Racket Theorem.\n   \n      To start, we consider rotation around the first principal axis.\n      Our goal is to show that rotation around this axis is stable.\n      That is, small perturbations in angular velocity will have only small effects on the rotation of the object.\n      So we assume that   and   are small.\n      In general, the product of two small quantities will be much smaller,\n      so   implies that   must be very small.\n      So we can disregard   in our calculations.\n     \n            Differentiate   with respect to time to explain why\n             .\n           \n            Substitute for   from   to show that\n              is an approximate solution to\n             \n            for some positive constant  .\n           \n            The equation   is a differential equation because it is an equation that involves derivatives of a function.\n            Show by differentiating twice that, if\n             \n            (where   and   are any scalars),\n            then   is a solution to  .\n            (In fact,   is the general solution to  ,\n            which is verified in just about any course in differential equations.)\n           \n    Equation  \n    shows that   is is bounded,\n    so that any slight perturbations in angular velocity have a limited effect on  .\n    A similar argument can be made for  .\n    This implies that the rotation around the principal axes is stable   slight changes in angular velocity have limited effects on the rotations around the other axes.\n   \n    We can make a similar argument for rotation around the third principal axes.\n   \n      In this activity,\n      repeat the process from Project Activity to show that rotation around the third principal axis is stable.\n      So assume that   and   are small,\n      which implies by   implies that\n        must be very small and can be disregarded in calculations.\n     \n    Now the issue is why is rotation around the second principal axis different.\n   \n      Now assume that   and   are small.\n      Thus,   is very small by  ,\n      and we consider   to be negligible.\n     \n            Differentiate    to show that\n             .\n           \n            Substitute for   from   to show that\n              is an approximate solution to\n             \n            for some positive scalar  .\n           \n            The fact that  the constant multiplier in   is positive instead of negative as in   completely changes the type of solution.\n            Show that\n             \n            (where   and   are any scalars) is a solution to  \n            (and, in fact, is the general solution).\n            Explain why this shows that rotation around the second principal axis is not stable.\n           "
},
{
  "id": "objectives-28",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#objectives-28",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is a quadratic form on  ?\n           \n         \n           \n            What does the Principal Axis Theorem tell us about quadratic forms?\n           \n         "
},
{
  "id": "F_tennis_racket",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#F_tennis_racket",
  "type": "Figure",
  "number": "28.1",
  "title": "",
  "body": "Two principal axes of a tennis racket. "
},
{
  "id": "p-4719",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#p-4719",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "tennis racket effect "
},
{
  "id": "definition-62",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#definition-62",
  "type": "Definition",
  "number": "28.2",
  "title": "",
  "body": "quadratic form quadratic form "
},
{
  "id": "p-4722",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#p-4722",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "matrix of the quadratic form "
},
{
  "id": "pa_7_b",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#pa_7_b",
  "type": "Preview Activity",
  "number": "28.1",
  "title": "",
  "body": "\n            To get a little more comfortable with quadratic forms,\n            write the quadratic forms in matrix form,\n            explicitly identifying the vector   and the symmetric matrix   of the quadratic form.\n           \n                   \n                 \n                   \n                 \n            Some quadratic forms form equations in   that are very familiar:\n              is an equation of a circle,\n              is an equation of an ellipse,\n            and   is an equation of a hyperbola.\n            Of course, these do not represent all of the quadratic forms in     some contain cross-product terms.\n            We can recognize the equations above because they contain no cross-product terms\n            (terms involving  ).\n            We can more easily recognize the quadratic forms that contain cross-product terms if  we can somehow rewrite the forms in a different format with no cross-product terms.\n            We illustrate how this can be done with the quadratic form   defined by  .\n           \n                  Write   in the form  ,\n                  where   is a   symmetric matrix.\n                 \n                  Since   is a symmetric matrix we can orthogonally diagonalize  .\n                  Given that the eigenvalues of   are   and\n                    with corresponding eigenvectors\n                    and  ,\n                  respectively,\n                  find a matrix   that orthogonally diagonalizes  .\n                 \n                  Define   to satisfy  .\n                  Substitute for   in the quadratic form   to write the quadratic form in terms of   and  .\n                  What kind of graph does the quadratic equation   have?\n                 "
},
{
  "id": "thm_7_b_Principal_Axis_Theorem",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#thm_7_b_Principal_Axis_Theorem",
  "type": "Theorem",
  "number": "28.3",
  "title": "Principal Axis Theorem.",
  "body": "Principal Axis Theorem \n        Let   be an   symmetric matrix.\n        There is an orthogonal change of variables\n          so that the quadratic form   defined by\n          is transformed into the quadratic form\n          where   is a diagonal matrix.\n       "
},
{
  "id": "activity-108",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#activity-108",
  "type": "Activity",
  "number": "28.2",
  "title": "",
  "body": "\n      Let   be the quadratic form defined by  ,\n      where   and  .\n     \n            The eigenvalues of   are   and\n              with corresponding eigenvectors\n              and  ,\n            respectively.\n            Find an orthogonal matrix   with determinant 1 that diagonalizes  .\n            Is   unique?\n            Explain.\n            Is there a matrix without determinant 1 that orthogonally diagonalizes  ?\n            Explain.\n           \n            Use the matrix   to write the quadratic form without the cross-product.\n           \n            We can view   as a change of basis matrix from the coordinate system defined by\n              to the standard coordinate system.\n            In other words, in the standard   coordinate system,\n            the quadratic form is written as  ,\n            but in the new coordinate system defined by   the quadratic form is written as  .\n            As a change of basis matrix,   performs a rotation.\n            See if you can recall what we learned about rotation matrices and determine the angle of rotation   defines.\n            Plot the graph of the quadratic equation\n              in the new coordinate system and identify this angle on the graph.\n            Interpret the result.\n           "
},
{
  "id": "F_7_b_Paraboloids",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#F_7_b_Paraboloids",
  "type": "Figure",
  "number": "28.4",
  "title": "",
  "body": "Left: Paraboloid  . Center: Hyperbolic Paraboloid  . Right: Paraboloid  . "
},
{
  "id": "def_7_4_definite",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#def_7_4_definite",
  "type": "Definition",
  "number": "28.5",
  "title": "",
  "body": "matrix positive definite matrix positive semidefinite matrix negative definite matrix negative semidefinite matrix indefinite positive definite positive semidefinite negative definite negative semidefinite indefinite "
},
{
  "id": "p-4757",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#p-4757",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "inner products "
},
{
  "id": "pa_7_b_inner_product",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#pa_7_b_inner_product",
  "type": "Preview Activity",
  "number": "28.3",
  "title": "",
  "body": "\n      Define a mapping from   to   by\n       \n      for   and   in  .\n      (The brackets   provide a shorthand way of representing the function.)\n     \n            Calculate  .\n           \n            If   and   are in  , is it true that\n             \n            Verify your answer.\n           \n            If  ,\n             , and   are in  , is it true that\n             \n            Verify your answer.\n           \n            If   and   are in   and   is a scalar,\n            is it true that\n             \n            Verify your answer.\n           \n            If   and   are in  ,\n            must it be the case that  ?\n            When is  ?\n           \n            There is a matrix   such that  .\n            Find this matrix  .\n           "
},
{
  "id": "p-4765",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#p-4765",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "inner products "
},
{
  "id": "def_7_b_inner_product",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#def_7_b_inner_product",
  "type": "Definition",
  "number": "28.6",
  "title": "",
  "body": "inner product inner product "
},
{
  "id": "activity-109",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#activity-109",
  "type": "Activity",
  "number": "28.4",
  "title": "",
  "body": "\n      Let   be a symmetric   matrix,\n      and define   by\n       .\n     \n            Explain why it is necessary for   to be positive definite in order for   to define an inner product on  .\n           \n            Show that   defines an inner product on   if   is positive definite.\n           \n            Let   be the mapping from   defined by\n             .\n            Find a matrix   so that   and explain why\n              defines an inner product.\n           "
},
{
  "id": "example-56",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#example-56",
  "type": "Example",
  "number": "28.7",
  "title": "",
  "body": "\n        Write the given quadratic equation in a system in which it has no cross-product terms.\n       \n               \n             \n              We write the quadratic form\n                as  ,\n              where   and  .\n              The eigenvalues for   are   and  ,\n              and bases for the corresponding eigenspaces   and   are\n                and  ,\n              respectively.\n              An orthogonal matrix   that orthogonally diagonalizes   is\n               .\n              If   and we let  ,\n              then we can rewrite the quadratic equation   as\n               .\n              So the quadratic equation   is an ellipse.\n             \n               \n             \n              We write the quadratic form\n                as  ,\n              where   and  .\n              The eigenvalues for   are   and  ,\n              and bases for the corresponding eigenspaces   and   are\n                and  ,\n              respectively.\n              An orthogonal matrix   that orthogonally diagonalizes   is\n               .\n              If   and we let  ,\n              then we can rewrite the quadratic equation   as\n               .\n              So the quadratic equation   is a hyperbola.\n             \n               \n             \n              We write the quadratic form\n                as  ,\n              where   and  .\n              The eigenvalues for   are   and  ,\n              and bases for the corresponding eigenspaces   and   are\n                and  ,\n              respectively.\n              Applying the Gram-Schmidt process to the basis for   gives us an orthogonal basis   of  ,\n              where   and\n               .\n              An orthogonal matrix   that orthogonally diagonalizes   is\n               .\n              If   and we let  ,\n              then we can rewrite the quadratic equation   as\n               .\n              So the quadratic equation   is a ellipsoid.\n             "
},
{
  "id": "example-57",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#example-57",
  "type": "Example",
  "number": "28.8",
  "title": "",
  "body": "\n        Let   and   be positive definite matrices,\n        and let  .\n       \n              Must   be invertible?\n              Justify your answer.\n             \n              Since   has all positive eigenvalues and   is the product of the eigenvalues of  ,\n              then  .\n              Thus,   is invertible.\n             \n              Must   be positive definite?\n              Justify your answer.\n             \n              The fact that   is positive definite means that   is also symmetric.\n              Recall that  .\n              Since   is symmetric,\n              it follows that   and   is symmetric.\n              The eigenvalues of   are the reciprocals of the eigenvalues of  .\n              Since the eigenvalues of   are all positive,\n              so are the eigenvalues of  .\n              Thus,   is positive definite.\n             \n              Must   be positive definite?\n              Justify your answer.\n             \n              Notice that\n               ,\n              so   is symmetric.\n              The eigenvalues of   are the squares of the eigenvalues of  .\n              Since no eigenvalue of   is  ,\n              the eigenvalues of   are all positive and   is positive definite.\n             \n              Must   be positive definite?\n              Justify your answer.\n             \n              We know that   is symmetric, and\n               ,\n              so   is symmetric.\n              Also, the fact that   and\n                for all   implies that\n               \n              for all  .\n              Thus,   is positive definite.\n             \n              Is   positive definite?\n              Justify your answer.\n             \n              The matrix   is symmetric and\n               .\n              So the eigenvalues of   are\n                and  .\n              Since the eigenvalues of   are both positive,\n                is positive definite.\n             "
},
{
  "id": "exercise-274",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#exercise-274",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Find the matrix for each quadratic form.\n       \n                if   is in  \n             \n               \n             \n                if   is in  \n             \n               \n             \n                if   is in  \n             \n               \n             "
},
{
  "id": "exercise-275",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#exercise-275",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        For each quadratic form, identify the matrix   of the form,\n        find a matrix   that orthogonally diagonalizes  ,\n        and make a change of variable that transforms the quadratic form into one with no cross-product terms.\n       \n               \n             \n               \n             \n               \n             "
},
{
  "id": "ex_7_b_constrained_optimization",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#ex_7_b_constrained_optimization",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        One topic in multivariable calculus is constrained optimization.\n        We can use the techniques of this section to solve certain types of constrained optimization problems involving quadratic forms.\n        As an example,\n        we will find the maximum and minimum values of the quadratic form defined by the matrix   on the unit circle.\n       \n              First we determine some bounds on the values of a quadratic form.\n              Let   be the quadratic form defined by the\n                real symmetric matrix  .\n              Let   be the eigenvalues of  ,\n              and let   be a matrix that orthogonally diagonalizes  ,\n              with   as the matrix with diagonal entries  ,\n               ,  ,\n                in order.\n              Let  .\n             \n                    Show that\n                     .\n                   \n                    Let   be a matrix that orthogonally diagonalizes  ,\n                    with  .\n                    Use this to calculate  .\n                   \n                    Use the fact that   for each   and the fact that  \n                    (and  )\n                    is an orthogonal matrix to show that\n                     .\n                   \n                    Substitute in part i.\n                   \n                    Now show that  .\n                   \n                    Make an argument similar to part ii.\n                   \n              Use the result of part (a) to find the maximum and minimum values of the quadratic form defined by the matrix   on the unit circle.\n             \n              The maximum value of   on the unit circle is 1 and it occurs at the input  .\n             "
},
{
  "id": "exercise-277",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#exercise-277",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        In this exercise we characterize the symmetric, positive definite,\n          matrices with real entries in terms of the entries of the matrices.\n        Let   for some real numbers  ,\n         ,\n        and  .\n       \n              Assume that   is positive definite.\n             \n                    Show that   must be positive.\n                   \n                    Use the fact that the eigenvalues of   must be positive to show that  .\n                    We conclude that if   is positive definite,\n                    then   and  .\n                   \n              Now show that if   and  ,\n              then   is positive definite.\n              This will complete our classification of positive definite   matrices.\n             "
},
{
  "id": "ex_7_b_QF_characterization",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#ex_7_b_QF_characterization",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        In this exercise we determine the form of\n         ,\n        where   is a symmetric   matrix.\n        Let   be a matrix that orthogonally diagonalizes   and let  .\n       \n              Substitute   for   in the equation  .\n              What form does the resulting equation have (write this form in terms of the eigenvalues of  )?\n             \n               .\n             \n              What kind of graph does the equation   have if   is positive definite?\n              Why?\n             \n              An ellipse.\n             \n              What kind of graph does the equation   have if   has both positive and negative eigenvalues?\n              Why?\n             \n              A hyperbola.\n             \n              What kind of graph does the equation   have if one eigenvalue of   is zero and the other non-zero?\n              Why?\n             \n              Two lines.\n             "
},
{
  "id": "ex_7_b_aij",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#ex_7_b_aij",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Let   be a symmetric   matrix.\n       \n              Show that  ,\n              where   is the  th standard unit vector for  . (This result will be useful in  )\n             \n              Let   be a unit eigenvector of   with eigenvalue  .\n              Find   in terms of  .\n             "
},
{
  "id": "ex_7_b_unique_A",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#ex_7_b_unique_A",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        Suppose   and   are symmetric   matrices,\n        and let   and  .\n        If   for all   in  ,\n        show that  . (Hint: Use  \n        (a) to compare   and  ,\n        then compare   to\n          for  .) Thus,\n        quadratic forms are uniquely determined by their symmetric matrices.\n       \n        Use the previous exercise to compare\n          and  ,\n        then compare   to   for  .\n       "
},
{
  "id": "ex_7_b_PD_inner_product",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#ex_7_b_PD_inner_product",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        In this exercise we analyze all inner products on  .\n        Let   be an inner product on  .\n        Let   and\n          be arbitrary vectors in  .\n        Then\n         ,\n        where   is the  th standard vector in  .\n       \n              Explain why\n               .\n             \n              Calculate the matrix product\n               \n              and compare to  .\n              What do you notice?\n             \n              Explain why any inner product on   is of the form   for some symmetric,\n              positive definite matrix  .\n             "
},
{
  "id": "ex_7_b_dot_product",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#ex_7_b_dot_product",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n         \n        shows that any inner product\n          on   has the form   for some symmetric,\n        positive definite matrix  .\n        Find the matrix   for which\n          for all   and   in  .\n       \n         \n       "
},
{
  "id": "exercise-283",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#exercise-283",
  "type": "Exercise",
  "number": "10",
  "title": "",
  "body": "\n        Let   be an inner product on  ,\n        let  ,  ,\n        and   be vectors  ,\n        and let   be a scalar.\n        Verify the following properties.\n       \n               \n             \n               \n             \n               \n             \n               \n             "
},
{
  "id": "ex_7_b_Pyth_Thm",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#ex_7_b_Pyth_Thm",
  "type": "Exercise",
  "number": "11",
  "title": "",
  "body": "Pythagorean Theorem in   with respect to an inner product \n        We extend the notions of length and orthogonality in   with respect to an inner product as follows.\n        If   is an inner product on  ,\n        we define the length of a vector   with respect to the inner product to be  .\n        We can also define two vectors   and   to be orthogonal if  .\n        In this exercise we verify the Pythagorean Theorem with respect to an inner product.\n       \n        The Pythagorean Theorem states that if   and   are the lengths of the legs of a right triangle whose hypotenuse has length  ,\n        then  .\n        If we think of the legs as defining vectors   and  ,\n        then the hypotenuse is the vector   and we can restate the Pythagorean Theorem as\n         .\n        In this exercise we show that this result holds in any dimension and for any inner product.\n        Use an arbitrary inner product  .\n       \n              Let   and   be orthogonal vectors in  .\n              Show that  .\n              \n             \n              Expand   using the dot product.\n             \n              Must it be true that if   and   are vectors in   with  ,\n              then   and   are orthogonal?\n              If not, provide a counterexample.\n              If true, verify the statement.\n             \n              Expand   using the dot product.\n             "
},
{
  "id": "ex_7_b_Cauchy_Schwarz",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#ex_7_b_Cauchy_Schwarz",
  "type": "Exercise",
  "number": "12",
  "title": "",
  "body": "Cauchy-Schwarz inequality in   with respect to an inner product \n        The Cauchy-Schwarz inequality,\n         \n        for any vectors   and   in  ,\n        is considered one of the most important inequalities in mathematics.\n        We verify the Cauchy-Schwarz inequality for an arbitrary inner product   in this exercise.\n        Let   and   be vectors in  .\n       \n              Explain why the inequality   is true if either   or   is the zero vector.\n              As a consequence,\n              we assume that   and   are nonzero vectors for the remainder of this exercise.\n             \n              Let   and let  .\n              We know that  .\n              Use   of this section to show that\n               .\n             \n              Now show that  .\n             \n              Combine parts (b) and (c) to explain why equation   is true.\n             "
},
{
  "id": "exercise-286",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#exercise-286",
  "type": "Exercise",
  "number": "13",
  "title": "",
  "body": "triangle inequality in   with respect to an inner product Triangle Inequality \n        Expand   using the dot product.\n       "
},
{
  "id": "exercise-287",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#exercise-287",
  "type": "Exercise",
  "number": "14",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If   is a quadratic form,\n              then there is exactly one matrix   such that  .\n             \n              F\n             True\/False \n              The matrix of a quadratic form is unique.\n             True\/False \n              If the matrix of a quadratic form is a diagonal matrix,\n              then the quadratic form has no cross-product terms.\n             \n              T\n             True\/False \n              The eigenvectors of the symmetric matrix   form the principal axes of the quadratic form  .\n             True\/False \n              The principal axes of a quadratic form are orthogonal.\n             \n              T\n             True\/False \n              If   and   are positive,\n              then the quadratic equation   defines an ellipse.\n             True\/False \n              If the entries of a symmetric matrix   are all positive,\n              then the quadratic form   is positive definite.\n             \n              F\n             True\/False \n              If a quadratic form\n                defined by a symmetric matrix   is positive definite,\n              then the entries of   are all non-negative.\n             True\/False \n              If a quadratic form   on   is positive definite,\n              then the graph of   is a paraboloid opening upward.\n             \n              T\n             True\/False \n              If a quadratic form   on   is negative definite,\n              then the graph of   is a paraboloid opening downward.\n             True\/False \n              If a quadratic form   on   is indefinite,\n              then there is a nonzero vector   such that  .\n             \n              T\n             True\/False \n              If   is positive definite,\n              then so is the quadratic form   for  .\n             True\/False \n              If If   is indefinite,\n              then at least one of the eigenvalues of   is negative and at least one positive.\n             \n              T\n             True\/False \n              If   symmetric matrices   and   define positive definite quadratic forms,\n              then so does  .\n             True\/False \n              If an invertible symmetric matrix   defines a positive definite quadratic form,\n              then so does  .\n             \n              T\n             "
},
{
  "id": "p-4888",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#p-4888",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "moment of inertia "
},
{
  "id": "p-4889",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#p-4889",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "primary intermediate "
},
{
  "id": "act_TRT_axes1",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#act_TRT_axes1",
  "type": "Project Activity",
  "number": "28.5",
  "title": "",
  "body": "\n      To start, we consider rotation around the first principal axis.\n      Our goal is to show that rotation around this axis is stable.\n      That is, small perturbations in angular velocity will have only small effects on the rotation of the object.\n      So we assume that   and   are small.\n      In general, the product of two small quantities will be much smaller,\n      so   implies that   must be very small.\n      So we can disregard   in our calculations.\n     \n            Differentiate   with respect to time to explain why\n             .\n           \n            Substitute for   from   to show that\n              is an approximate solution to\n             \n            for some positive constant  .\n           \n            The equation   is a differential equation because it is an equation that involves derivatives of a function.\n            Show by differentiating twice that, if\n             \n            (where   and   are any scalars),\n            then   is a solution to  .\n            (In fact,   is the general solution to  ,\n            which is verified in just about any course in differential equations.)\n           "
},
{
  "id": "act_TRT_axes3",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#act_TRT_axes3",
  "type": "Project Activity",
  "number": "28.6",
  "title": "",
  "body": "\n      In this activity,\n      repeat the process from Project Activity to show that rotation around the third principal axis is stable.\n      So assume that   and   are small,\n      which implies by   implies that\n        must be very small and can be disregarded in calculations.\n     "
},
{
  "id": "act_TRT_axes2",
  "level": "2",
  "url": "chap_principal_axis_theorem.html#act_TRT_axes2",
  "type": "Project Activity",
  "number": "28.7",
  "title": "",
  "body": "\n      Now assume that   and   are small.\n      Thus,   is very small by  ,\n      and we consider   to be negligible.\n     \n            Differentiate    to show that\n             .\n           \n            Substitute for   from   to show that\n              is an approximate solution to\n             \n            for some positive scalar  .\n           \n            The fact that  the constant multiplier in   is positive instead of negative as in   completely changes the type of solution.\n            Show that\n             \n            (where   and   are any scalars) is a solution to  \n            (and, in fact, is the general solution).\n            Explain why this shows that rotation around the second principal axis is not stable.\n           "
},
{
  "id": "chap_SVD",
  "level": "1",
  "url": "chap_SVD.html",
  "type": "Section",
  "number": "29",
  "title": "The Singular Value Decomposition",
  "body": "The Singular Value Decomposition \n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            What is the operator norm of a matrix and what does it tell us about the matrix?\n           \n         \n           \n            What is a singular value decomposition of a matrix?\n            Why is a singular value decomposition important?\n           \n         \n           \n            How does a singular value decomposition relate fundamental subspaces connected to a matrix?\n           \n         \n           \n            What is an outer product decomposition of a matrix and how is it useful?\n           \n         Application: Search Engines and Semantics Latent Semantic Indexing synonymy polysemy Latent Semantic Analysis Introduction \n    The singular value decomposition (SVD) of a matrix is an important and useful matrix decomposition.\n    Unlike other matrix decompositions,\n     every  matrix has a singular value decomposition.\n    The SVD is used in a variety of applications including scientific computing,\n    digital signal processing, image compression,\n    principal component analysis,\n    web searching through latent semantic indexing, and seismology.\n    Recall that the eigenvector decomposition of an\n      diagonalizable matrix   has the form  ,\n    where the columns of the matrix   are   linearly independent eigenvectors of   and the diagonal entries of the diagonal matrix\n      are the eigenvalues of  .\n    The singular value decomposition does something similar for any matrix of any size.\n    One of the keys to the SVD is that the matrix\n      is symmetric for any matrix  .\n   The Operator Norm of a Matrix \n    Before we introduce the Singular Value Decomposition,\n    let us work through some preliminaries to motivate the idea.\n    The first is to provide an answer to the question\n     How  big  is a matrix? \n    There are many ways to interpret and answer this question,\n    but a substantial\n    (and useful)\n    answer should involve more than just the dimensions of the matrix.\n    A good measure of the size of a matrix,\n    which we will refer to as the norm of the matrix,\n    should take into account the action of the linear transformation defined by the matrix on vectors.\n    This then will lead to questions about how difficult or easy is it to solve a matrix equation  .\n   relative operator norm of a matrix operator norm \n    Due to the linearity of matrix multiplication,\n    we can restrict ourselves to unit vectors for an equivalent definition of the operator norm of the matrix   as\n     .\n   \n            Determine   if   is the zero matrix.\n           \n            Determine  ,\n            where   is the   identity matrix.\n           \n            Let  .\n            Find  .\n            Justify your answer. (Hint:  .)\n           \n            If   is an orthogonal matrix, what is  ?\n            Why?\n           \n    The operator norm of a matrix tells us that how big the action of an\n      matrix is can be determined by its action on the unit sphere in  \n    (the unit sphere is the set of terminal point of unit vectors).\n    Let us consider two examples.\n   \n        Let  .\n        We can draw a graph to see the action of   on the unit circle.\n        A picture of the set   is shown in  .\n       The image of the unit circle under the action of  . \n        It appears that   transforms the unit circle into an ellipse.\n        To find   we want to maximize\n          for   on the unit circle.\n        This is the same as maximizing\n         .\n       \n        Now   is a symmetric matrix,\n        so we can orthogonally diagonalize  .\n        The eigenvalues of   are 32 and 2.\n        Let  ,\n        where   is a unit eigenvector of\n          with eigenvalue 32 and\n          is a unit eigenvector of   with eigenvalue 2.\n        Then   is an orthogonal matrix such that  .\n        It follows that\n         .\n       \n        Now   is orthogonal,\n        so   and   maps the unit circle to the unit circle.\n        Moreover, if   is on the unit circle,\n        then   is also on the unit circle and  .\n        So every point   on the unit circle corresponds to a point   on the unit circle.\n        Thus, the forms   and\n          take on exactly the same values over all points on the unit circle.\n        Now we just need to find the maximum value of  .\n        This turns out to be relatively easy since   is a diagonal matrix.\n       \n        Let's simplify the notation.\n        Let  .\n        Then our job is to maximize  .\n        If  , then\n         .\n       \n        We want to find the maximum value of this expression for   on the unit circle.\n        Note that   and so\n         .\n       \n        Since   is on the unit circle,\n        the expression   attains the value 32 at some point on the unit circle,\n        so 32 is the maximum value of\n          over all   on the unit circle.\n        While we are at it, we can similarly find the minimum value of\n          for   on the unit circle.\n        Since   we see that\n         .\n       \n        Since the expression   attains the value 2 at   on the unit circle,\n        we can see that   attains the minimum value of 2 on the unit circle.\n       \n        Now we can return to the expression  .\n        Since   assumes the same values as  ,\n        we can say that the maximum value of\n          for   on the unit circle is 32\n        (and the minimum value is 2).\n        Moreover, the quadratic form\n          assumes its maximum value when\n          or  .\n        Thus, the form   assumes its maximum value at the vector   or  .\n        Similarly, the quadratic form\n          attains its minimum value at   or  .\n        We conclude that  .\n       \n         \n        shows the image of the unit circle under the action of   and the images of   and   where\n          are the two unit eigenvectors of  .\n        The image also supports that   assumes its maximum and minimum values for points on the unit circle at   and  .\n       The image of the unit circle under the action of  , and the vectors   and  IMPORTANTE NOTE 1 \n    What we have just argued is that the maximum value of\n      for   on the unit sphere in   is the square root of the largest eigenvalue of\n      and occurs at a corresponding unit eigenvector.\n   \n        This same process works for matrices other than   ones.\n        For example,\n        consider  .\n        In this case   maps   to  .\n        The image of the unit sphere\n          under left multiplication by   is a filled ellipse as shown in  .\n       The image of the unit circle under the action of  , and the vectors   and  \n        As with the previous example,\n        the norm of   is the square root of the maximum value of\n          and this maximum value is the dominant eigenvalue of  .\n        The eigenvalues of   are\n         ,  ,\n        and   with corresponding unit eigenvectors  ,\n         ,\n        and  .\n        So in this case we have  .\n        The transformation defined by matrix multiplication by   from   to   has a one-dimensional kernel which is spanned by the eigenvector corresponding to  .\n        The image of the transformation is 2-dimensional and the image of the unit circle is an ellipse where   gives the major axis of the ellipse and   gives the minor axis.\n        Essentially, the square roots of the eigenvalues of\n          tell us how   stretches the image space in each direction.\n       IMPORTANT NOTE 2 \n    We have just argued that the image of the unit  -sphere under the action of an\n      matrix is an ellipsoid in   stretched the greatest amount,\n     ,\n    in the direction of an eigenvector for the largest eigenvalue ( ) of  ;\n    the next greatest amount,  ,\n    in the direction of a unit vector for the second largest eigenvalue ( ) of  ;\n    and so on.\n   \n      Let  .\n      Then  .\n      The eigenvalues of   are   and\n        with respective eigenvectors\n        and  .\n     \n            Find  .\n           \n            Find a unit vector   at which\n              assumes its maximum value.\n           The SVD \n    The Singular Value Decomposition (SVD) is essentially a concise statement of what we saw in the previous section that works for  any  matrix.\n    We will uncover the SVD in this section.\n   \n      Let  .\n      Since   is not square,\n      we cannot diagonalize  .\n      However, the matrix\n       \n      is a symmetric matrix and can be orthogonally diagonalized.\n      The eigenvalues of   are 3, 1, and 0 with corresponding eigenvectors\n       ,\n      respectively.\n      Use appropriate technology to do the following.\n     \n            Find an orthogonal matrix   that orthogonally diagonalizes  , where\n             .\n           \n            For  , let  .\n            Find each  .\n            Why don't we define   in this way?\n           \n            Let  .\n            What kind of matrix is  ?\n            Explain.\n           \n            Calculate the matrix product  .\n            What do you notice?\n            How is this similar to the eigenvector decomposition of a matrix?\n           \n     \n    contains the basic ideas behind the Singular Value Decomposition.\n    Let   be an   matrix with real entries.\n    Note that   is a symmetric   matrix and, hence,\n    it can be orthogonally diagonalized.\n    Let   be an\n      orthogonal matrix whose columns form an orthonormal set of eigenvectors for  .\n    For each  , let  .\n    We know\n     .\n   \n    Now notice that for each   we have\n     ,\n    so  .\n    Thus, the matrix   has no negative eigenvalues.\n    We can always arrange the eigenvectors and eigenvalues of   so that\n     .\n   \n    Also note that\n     \n    if  .\n    So the set   is an orthogonal set in  .\n    Each of the vectors   is in  ,\n    and so   is an orthogonal subset of  .\n    It is possible that   for some of the  \n    (if   has 0 as an eigenvalue).\n    Let  ,  ,  ,\n      be the eigenvectors corresponding to the nonzero eigenvalues.\n    Then the set\n     \n    is a linearly independent set of nonzero orthogonal vectors in  .\n    Now we will show that   is a basis for  .\n    Let   be a vector in  .\n    Then   for some vector   in  .\n    Recall that the vectors  ,\n     ,  ,\n      form an orthonormal basis of  , so\n     \n    for some scalars  ,  ,\n     ,  .\n    Since   for   we have\n     .\n   \n    So   and   is an orthogonal basis for  .\n   \n    Now we are ready to find the Singular Value Decomposition of  .\n    First we create an orthonormal basis\n      for   by normalizing the vectors  .\n    So we let\n     \n    for   from 1 to  .\n   \n    Remember from   that  ,\n    so if we let  , then we have\n     .\n   \n    We ordered the   so that  ,\n    so we also have\n     .\n   \n    The scalars  ,  ,  ,\n      are called the  singular values  of  .\n   singular values singular values \n    The vectors  ,  ,  ,\n      are   orthonormal vectors in  .\n    We can extend the set  ,\n     ,  ,\n      to an orthonormal basis   of  .\n    Recall that   for   and\n      for  , so\n     .\n   \n    We can write the matrix   in another way.\n    Let   be the   matrix defined as\n     .\n   \n    Now\n     .\n   \n    So if  , then\n     .\n   \n    Since   is an orthogonal matrix, we have that\n     .\n\n    This is the Singular Value Decomposition of  .\n   The Singular Value Decomposition \n        Let   be an   matrix of rank  .\n        There exist an   orthogonal matrix  ,\n        an   orthogonal matrix  ,\n        and an   matrix   whose first   diagonal entries are the singular values  ,\n         ,  ,\n          and whose other entries are 0, such that\n         .\n       SVD Summary singular vectors right singular vectors left right singular vectors left singular vectors \n      Let  .\n      Then  .\n      The eigenvalues of   are   and\n        with respective eigenvectors\n        and  .\n     \n            Find an orthonormal basis   of eigenvectors of  .\n            What is  ?\n            Find the matrix   in a SVD for  .\n           \n            Find the singular values of  .\n            What is the rank   of  ?\n            Why?\n           \n            What are the dimensions of the matrix   in a SVD of  ?\n            Find  .\n           \n            Find the vectors  ,\n             ,  ,  .\n            If necessary, extend this set to an orthonormal basis\n             \n            of  .\n           \n            Find the matrix   so that\n              is a SVD for  .\n           \n    There is another way we can write this SVD of  .\n    Let the   matrix   have a singular value decomposition  , where\n     .\n   \n    Since   we see that\n     .\n   outer product decomposition SVD and the Null, Column, and Row Spaces of a Matrix \n    We conclude this section with a short discussion of how a singular value decomposition relates fundamental subspaces of a matrix.\n    We have seen that the vectors  ,\n     ,  ,\n      in an SVD for an\n      matrix   form a basis for  .\n    Recall also that   for  .\n    Since  ,\n    it follows that the vectors  ,  ,\n     ,   form a basis for  .\n    As you will show in the exercises,\n    the set   is a basis for  .\n    Thus, an SVD for a matrix   tells us about three fundamental vector spaces related to  .\n   Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Let  .\n       \n              Find a singular value decomposition for  .\n              You may use technology to find eigenvalues and eigenvectors of matrices.\n             \n              With   as given,\n              we have  .\n              Technology shows that the eigenvalues of\n                are  ,\n               ,  ,\n              and   with corresponding orthonormal eigenvectors  ,\n               ,\n               ,\n              and  .\n              This makes  .\n              The singular values of   are  ,\n               ,\n               , and  ,\n              so   is the   matrix with the nonzero singular values along the diagonal and zeros everywhere else.\n              Finally, we define the vectors   as  .\n              Again, technology gives us  ,\n               ,\n              and  .\n              Thus, a singular value decomposition of   is  , where\n               .\n             \n              Use the singular value decomposition to find a basis for  ,\n               , and  .\n             \n              Recall that the right singular vectors of an\n                matrix   of rank   form an orthonormal basis\n                of eigenvectors of   such that\n                for   from 1 to   with  .\n              These vectors are the columns of the matrix\n                in a singular value decomposition of  .\n              For   from 1 to  ,\n              we let  .\n              Then the set   forms an orthonormal basis of  .\n              We extend this set   to an orthonormal basis   of  .\n              Recall also that   for  .\n              Since  ,\n              it follows that the vectors  ,  ,\n               ,   form a basis for  .\n              Finally, the set   is a basis for  .\n              So in our example, we have  ,  ,\n               ,\n               ,\n               ,\n              and  .\n              Since the singular values of   are  ,  ,\n               , and  , it follows that  .\n              We also have  ,\n               ,\n              and  .\n              So\n               \n              is a basis for   and\n               \n              is a basis for  .\n              Finally, the set\n               \n              is a basis for  .\n             \n        Let\n         .\n       \n        The eigenvalues of   are\n         ,  ,\n        and   with corresponding eigenvectors\n         .\n       \n        In addition,\n         .\n       \n              Find orthogonal matrices   and  ,\n              and the matrix  ,\n              so that   is a singular value decomposition of  .\n             \n              Normalizing the eigenvectors  ,  ,\n              and   to normal eigenvectors  ,  ,\n              and  , respectively,\n              gives us an orthogonal matrix\n               .\n              Now  ,\n              so normalizing the vectors   and   gives us vectors\n               \n              that are the first two columns of our matrix  .\n              Given that   is a   matrix,\n              we need to find two other vectors orthogonal to   and   that will combine with   and   to form an orthogonal basis for  .\n              Letting  ,\n               ,\n               ,\n              and  ,\n              a computer algebra system shows that the reduced row echelon form of the matrix   is  ,\n              so that vectors  ,\n               ,\n               ,   are linearly independent.\n              Letting   and  ,\n              the Gram-Schmidt process shows that the set\n                is an orthogonal basis for  , where\n               \n              and (using   for  )\n               .\n              The set   where  ,\n               ,\n                and\n                is an orthonormal basis for   and we can let\n               .\n              The singular values of   are\n                and  , and so\n               .\n              Therefore, a singular value decomposition of   is   of\n               .\n             \n              Determine the best rank 1 approximation to  .\n             \n              Determine the best rank 1 approximation to  .\n              The outer product decomposition of   is\n               .\n              So the rank one approximation to   is\n               .\n              Note that the rows in this rank one approximation are the averages of the two distinct rows in the matrix  ,\n              which makes sense considering that this is the closest rank one matrix to  .\n             Summary \n    We learned about the singular value decomposition of a matrix.\n   \n       \n        The operator norm of an   matrix   is\n         .\n        The operator norm of a matrix tells us that how big the action of an\n          matrix is can be determined by its action on the unit sphere in  .\n       \n     \n       \n        A singular value decomposition of an\n          matrix is of the form  , where\n       \n       \n           \n              where\n              is an orthonormal basis of eigenvectors of   such that\n              for   from 1 to   with  ,\n           \n         \n           \n              where\n              for   from 1 to  ,\n            and this orthonormal basis of   is extended to an orthonormal basis   of  ,\n           \n         \n           \n             ,\n            where   for   from 1 to  .\n           \n           \n        \n      A singular value decomposition is important in that every matrix has a singular value decomposition,\n      and a singular value decomposition has a variety of applications including scientific computing and digital signal processing,\n      image compression, principal component analysis,\n      web searching through latent semantic indexing, and seismology.\n       \n         \n     \n       \n        The vectors  ,  ,  ,\n          in an SVD for an\n          matrix   form a basis for   while the vectors  ,\n         ,\n         ,   form a basis for  .\n        Also, the set   is a basis for  .\n       \n     \n       \n        Let   have an SVD as in the second bullet.\n        Decomposing   as\n         \n        is an outer product decomposition of  .\n        An outer product decomposition allows us to approximate   with smaller rank matrices.\n        For example,\n        the matrix   is the best rank 1 approximation to  ,\n          is the best rank 2 approximation,\n        and so on.\n       \n     \n        Find a singular value decomposition of the following matrices.\n       \n                 \n             \n               ,\n               ,\n               .\n             \n                 \n             \n               ,\n               ,  .\n             \n                 \n             \n               ,\n               ,\n               .\n             \n                 \n             \n               ,\n               ,\n               .\n             \n                \n             \n               ,\n               ,\n               .\n             \n        Let   be an   matrix of rank   with singular value decomposition  ,\n        where   and  .\n        We have seen that the set   is a basis for  ,\n        and the vectors  ,\n         ,  ,\n          form a basis for  .\n        In this exercise we examine the set\n          and determine what this set tells us about  .\n       \n              Find a singular value decomposition for  .\n              \n             \n              Use the singular value decomposition   for  .\n             \n              Explain why the result of (a) shows that the set\n                is a basis for  .\n             \n        Let  .\n       \n              Find the singular values of  .\n             \n               \n             \n              Find a singular value decomposition of  .\n             \n               ,\n               ,\n               .\n             \n              Use a singular value decomposition to find orthonormal bases for the following:\n             \n                     \n                   \n                     .\n                   \n                     \n                   \n                     .\n                   \n                     \n                   \n                     .\n                   \n        Let   have the singular value decomposition as in  .\n       \n              Show, using  , that  .\n             \n              Explain why  .\n              \n             \n              The set   is an orthonormal basis of  .\n              Use this to show that   for any unit vector   in  .\n             \n        Show that   and   have the same nonzero singular values.\n        How are their singular value decompositions related?\n       \n        Find the transpose of an SVD for  .\n       \n        The vectors   that form the columns of the matrix   in a singular value decomposition of a matrix   are eigenvectors of  .\n        In this exercise we investigate the vectors   that make up the columns of the matrix   in a singular value decomposition of a matrix   for each   between 1 and the rank of  ,\n        and their connection to the matrix  .\n       >\n             \n              Let  .\n              A singular value decomposition of   is  , where\n               .\n             \n                    Determine the rank   of\n                      and identify the vectors  ,\n                     ,  ,  .\n                   \n                    Calculate   for each   between 1 and  .\n                    How is   related to  ?\n                   \n              Now we examine the result of part (a) in general.\n              Let   be an arbitrary matrix.\n              Calculate   for\n                and determine specifically how\n                is related to  .\n              What does this tell us about the vectors   and the matrix  ?\n             \n              Now show in general that the columns of   are orthonormal eigenvectors for  . (That is,\n              what can we say about the vectors   if  ?)\n             \n        If   is a symmetric matrix with eigenvalues\n         ,\n        what is  ?\n        Justify your answer.\n       \n         \n       \n        Let   be a   symmetric matrix.\n       \n              Show that if   is an eigenvector of   with eigenvalue  ,\n              then   is an eigenvector for  .\n              What is the corresponding eigenvalue?\n             \n              Show that if   is an eigenvector of\n                with non-negative eigenvalue  ,\n              then   is an eigenvector of  .\n              What is the corresponding eigenvalue?\n             \n              Suppose   is a singular value decomposition of  .\n              Explain why   is also a singular value decomposition of  .\n             \n        Let  ,  ,\n         ,   and  ,\n         ,  ,\n          be the vectors found in a singular value decomposition of a matrix  ,\n        where   is the rank of  .\n        Show that   is a rank 1 matrix for each  .\n        \n       \n        Mimic  \n        in  .\n       \n        Is it possible for a matrix   to have a singular value decomposition\n          in which  ?\n        If no, explain why.\n        If yes, determine for which matrices we can have  .\n       \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If   is a singular value of a matrix  ,\n              then   is an eigenvalue of  .\n             \n              F\n             True\/False \n              A set of right singular vectors of a matrix   is also a set of left singular vectors of  .\n             True\/False \n              The transpose of a singular value decomposition of a matrix   is a singular value decomposition for  .\n             \n              T\n             True\/False \n              Similar matrices have the same singular values.\n             True\/False \n              If   is an   matrix,\n              then the singular values of   are the squares of the singular values of  .\n             \n              F\n             True\/False \n              The   matrix in an SVD of   is unique.\n             True\/False \n              The matrices   in an SVD of   are unique.\n             \n              F\n             True\/False \n              If   is a positive definite matrix,\n              then an orthogonal diagonalization of   is an SVD of  .\n             Project: Latent Semantic Indexing \n    As an elementary example to illustrate the idea behind Latent Semantic Indexing (LSI), consider the problem of creating a program to search a collection of documents for words,\n    or words related to a given word.\n    Document collections are usually very large,\n    but we use a small example for illustrative purposes.\n    A standard example that is given in several publications \n    e.g., Deerwester, S., Dumais, S. T., Fumas, G. W., Landauer, T. K. and Harshman, R. Indexing by latent semantic analysis.\n     Journal of the American Society for Information Science , 1990, 41: 391 407,\n    and Landauer, T. and Dutnais, S. A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction,\n    and Representation of Knowledge.\n     Psychological Review , 1997.\n    Vol. 1M. No. 2, 211-240.\n      is the following.\n    Suppose we have nine documents   through  \n    (titles of documents about human-computer interaction)\n    and   through  \n    (titles of graph theory papers)\n    that make up our library:\n     \n         \n           :  Human  machine  interface \n          for ABC  computer  applications\n         \n       \n         \n           : A  survey  of  user \n          opinion of  computer system response time \n         \n       \n         \n           : The  EPS user interface \n          management  system \n         \n       \n         \n           :  System  and  human system \n          engineering testing of  EPS \n         \n       \n         \n           : Relation of  user \n          perceived  response time  to error measurement\n         \n       \n         \n           : The generation of random,\n          binary, ordered  trees \n         \n       \n         \n           : The intersection  graph \n          of paths in  trees \n         \n       \n         \n           :  Graph minors \n          IV: Widths of  trees  and well-quasi-ordering\n         \n       \n         \n           :  Graph minors : A  survey \n         \n       \n   stop words term-document \n    One of our goals is to rate the pages in our library for relevance if we search for a query.\n    For example, suppose we want to rate the pages for the query\n     survey, computer .\n    This query can be represented by the vector  .\n   \n      In a standard term-matching search with\n        term-document matrix  ,\n      a query vector   would be matched with the terms to determine the number of matches.\n      The matching counts the number of times each document agrees with the query.\n     \n            Explain why this matching is accomplished by the matrix-vector product  .\n           \n            Let  .\n            Explain why  ,\n            where   is the  th column of   and\n              is the angle between   and  .\n           \n            We can use the cosine calculation from part (b) to compare matches to our query   the closer the cosine is to  ,\n            the better the match\n            (dividing by the product of the norms is essentially converting all vectors to unit vectors for comparison purposes).\n            This is often referred to as the cosine distance.\n            Calculate the cosines of the\n              for our example of the query  .\n            Order the documents from best to worst match for this query.\n           \n    Though we were able to rate the documents in   using the cosine distance,\n    the result is less than satisfying.\n    Documents  ,  ,\n    and   are all related to computers but do not appear at all in or results.\n    This is a problem with language searches   we don't want to compare just words,\n    but we also need to compare the concepts the words represent.\n    The fact that words can represent different things implies that a random choice of word by different authors can introduce noise into the word-concept relationship.\n    To filter out this noise,\n    we can apply the singular value decomposition to find a smaller set of concepts to better represent the relationships.\n    Before we do so,\n    we examine some useful properties of the term-document matrix.\n   \n      Let  ,\n      where  ,  ,\n       ,   are the columns of  .\n     \n            In  \n            you should have seen that  .\n            Assume for the moment that all of the entries in   are either   or  .\n            Explain why in this case the dot product\n              tells us how many terms documents   and   have in common.\n            Also, the matrix   takes dot products of the columns of  ,\n            which refer to what's happening in each document and so is looking at document-document interactions.\n            For these reasons, we call\n              the document-document matrix.\n           \n            Use appropriate technology to calculate the entries of the matrix  .\n            This matrix is the term-term matrix.\n            Assume for the moment that all of the entries in   are either   or  .\n            Explain why if terms   and   occur together in   documents,\n            then  .\n           \n    The nature of the term-term and document-document matrices makes it realistic to think about a SVD.\n   \n      To see why a singular value decomposition might be useful,\n      suppose our term-document matrix   has singular value decomposition\n       . (Don't actually calculate the SVD yet).\n     \n            Show that the document-document matrix\n              satisfies  .\n            This means that we can compare document   and document   using the dot product of row   and column   of the matrix product  .\n           \n            Show that the term-term matrix\n              satisfies  .\n            Thus we can compare term   and term   using the dot product of row   and column   of the matrix product  .\n            ( \n            shows that the columns of   are orthogonal eigenvectors of  .)\n           \n    As we will see,\n    the connection of the matrices   and   to documents and terms that we saw in  \n    will be very useful when we use the SVD of the term-document matrix to reduce dimensions to a\n     concept \n    space.\n    We will be able to interpret the rows of the matrices   and   as providing coordinates for terms and documents in this space.\n   \n      The singular value decomposition (SVD) allows us to produce new,\n      improved term-document matrices.\n      For this activity,\n      use the term-document matrix   in  .\n     \n            Use appropriate technology to find a singular value decomposition of   so that  .\n            Print your entries to two decimal places\n            (but keep as many as possible for computational purposes).\n           \n            Each singular value tells us how important its semantic dimension is.\n            If we remove the smaller singular values\n            (the less important dimensions),\n            we retain the important information but eliminate minor details and noise.\n            We produce a new term-document matrix   by keeping the largest   of the singular values and discarding the rest.\n            This gives us an approximation\n             \n            using the outer product decomposition,\n            where  ,  ,  ,\n              are the   largest singular values of  .\n            Note that if   is an   matrix,\n            letting  \n            (an   matrix),\n              the   matrix with the first   singular values along the diagonal,\n            and   (a   matrix),\n            then we can also write  .\n            This is sometimes referred to as a reduced SVD.  Find  ,\n             ,\n            and  , and find the new term-document matrix  .\n           \n    Once we have our term-document matrix,\n    there are three basic comparisons to make:\n    comparing terms, comparing documents,\n    and comparing terms and documents.\n    Term-document matrices are usually very large,\n    with dimension being the number of terms.\n    By using a reduced SVD we can create a much smaller approximation.\n    In our example,\n    the matrix   in  \n    reduces our problem to a  -dimensional space.\n    Intuitively,\n    we can think of LSI as representing terms as averages of all of the documents in which they appear and documents as averages of all of the terms they contain.\n    Through this process, LSI attempts to combine the surface information in our library into a deeper abstraction (the\n     concept \n    space) that captures the mutual relationships between terms and documents.\n   \n    We now need to understand how we can represent documents and terms in this smaller space where  .\n    Informally, we can consider the rows of   as representing the coordinates of each term in the lower dimensional concept space and the columns of\n      as the coordinates of the documents,\n    while the entries of   tell us how important each semantic dimension is.\n    The dot product of two row vectors of   indicates how terms compare across documents.\n    This product is  .\n    Just as in  ,\n    we have  .\n    In other words, if we consider the rows of\n      as coordinates for terms,\n    then the dot products of these rows give us term to term comparisons.\n    (Note that multiplying   by   just stretches the rows of   by the singular values according to the importance of the concept represented by that singular value.)\n    Similarly, the dot product between columns of   provide a comparison of documents.\n    This comparison is given by  \n    (again by  ).\n    So we can consider the rows of\n      as providing coordinates for documents.\n   \n      We have seen how to compare terms to terms and documents to documents.\n      The matrix   itself compares terms to documents.\n      Show that  ,\n      where   is the diagonal matrix of the same size as\n        whose diagonal entries are the square roots of the corresponding diagonal entries in  .\n      Thus, all useful comparisons of terms and documents can be made using the rows of the matrices   and  ,\n      scaled in some way by the singular values in  .\n     pseudo-document \n      For an original query  ,\n      we start with its term vector\n        (a vector in the coordinate system determined by the columns of  ) and find a representation\n        that we can use as a column of   in the document-document comparison matrix.\n      If this representation was perfect,\n      then it would take a real document in the original system given by   and produce the corresponding column of   if we used the full SVD. In other words,\n      we would have  .\n     \n            Use the fact that  ,\n            to show that  .\n            It follows that   is transformed into the query  .\n           \n            In our example, using  ,\n            the terms can now be represented as  -dimensional vectors\n            (the rows of  , see  ),\n            or as points in the plane.\n            More specifically,  human \n            is represented by the vector\n            (to two decimal places)\n             ,\n             interface  by  , etc.\n            Similarly, the documents are represented by columns of  \n            (see  ),\n            so that the document   is represented by  ,\n              by  , etc.\n            From this perspective we can visualize these documents in the plane.\n            Plot the documents and the query in the 2-dimensional concept space.\n            Then calculate the cosine distances from the query to the documents in this space.\n            Which documents now give the best three matches to the query?\n            Compare the matches to your plot.\n           \n    As we can see from  ,\n    the original query had no match at all with any documents except  ,\n     , and  .\n    In the new concept space,\n    the query now has some connection to every document,. So LSI has made semantic connections between the terms and documents that were not present in the original term-document matrix,\n    which gives us better results for our search.\n   "
},
{
  "id": "objectives-29",
  "level": "2",
  "url": "chap_SVD.html#objectives-29",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            What is the operator norm of a matrix and what does it tell us about the matrix?\n           \n         \n           \n            What is a singular value decomposition of a matrix?\n            Why is a singular value decomposition important?\n           \n         \n           \n            How does a singular value decomposition relate fundamental subspaces connected to a matrix?\n           \n         \n           \n            What is an outer product decomposition of a matrix and how is it useful?\n           \n         "
},
{
  "id": "p-4909",
  "level": "2",
  "url": "chap_SVD.html#p-4909",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "synonymy polysemy Latent Semantic Analysis "
},
{
  "id": "p-4912",
  "level": "2",
  "url": "chap_SVD.html#p-4912",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "relative "
},
{
  "id": "definition-65",
  "level": "2",
  "url": "chap_SVD.html#definition-65",
  "type": "Definition",
  "number": "29.1",
  "title": "",
  "body": "operator norm of a matrix operator norm "
},
{
  "id": "pa_7_c_1",
  "level": "2",
  "url": "chap_SVD.html#pa_7_c_1",
  "type": "Preview Activity",
  "number": "29.1",
  "title": "",
  "body": "\n            Determine   if   is the zero matrix.\n           \n            Determine  ,\n            where   is the   identity matrix.\n           \n            Let  .\n            Find  .\n            Justify your answer. (Hint:  .)\n           \n            If   is an orthogonal matrix, what is  ?\n            Why?\n           "
},
{
  "id": "example-58",
  "level": "2",
  "url": "chap_SVD.html#example-58",
  "type": "Example",
  "number": "29.2",
  "title": "",
  "body": "\n        Let  .\n        We can draw a graph to see the action of   on the unit circle.\n        A picture of the set   is shown in  .\n       The image of the unit circle under the action of  . \n        It appears that   transforms the unit circle into an ellipse.\n        To find   we want to maximize\n          for   on the unit circle.\n        This is the same as maximizing\n         .\n       \n        Now   is a symmetric matrix,\n        so we can orthogonally diagonalize  .\n        The eigenvalues of   are 32 and 2.\n        Let  ,\n        where   is a unit eigenvector of\n          with eigenvalue 32 and\n          is a unit eigenvector of   with eigenvalue 2.\n        Then   is an orthogonal matrix such that  .\n        It follows that\n         .\n       \n        Now   is orthogonal,\n        so   and   maps the unit circle to the unit circle.\n        Moreover, if   is on the unit circle,\n        then   is also on the unit circle and  .\n        So every point   on the unit circle corresponds to a point   on the unit circle.\n        Thus, the forms   and\n          take on exactly the same values over all points on the unit circle.\n        Now we just need to find the maximum value of  .\n        This turns out to be relatively easy since   is a diagonal matrix.\n       \n        Let's simplify the notation.\n        Let  .\n        Then our job is to maximize  .\n        If  , then\n         .\n       \n        We want to find the maximum value of this expression for   on the unit circle.\n        Note that   and so\n         .\n       \n        Since   is on the unit circle,\n        the expression   attains the value 32 at some point on the unit circle,\n        so 32 is the maximum value of\n          over all   on the unit circle.\n        While we are at it, we can similarly find the minimum value of\n          for   on the unit circle.\n        Since   we see that\n         .\n       \n        Since the expression   attains the value 2 at   on the unit circle,\n        we can see that   attains the minimum value of 2 on the unit circle.\n       \n        Now we can return to the expression  .\n        Since   assumes the same values as  ,\n        we can say that the maximum value of\n          for   on the unit circle is 32\n        (and the minimum value is 2).\n        Moreover, the quadratic form\n          assumes its maximum value when\n          or  .\n        Thus, the form   assumes its maximum value at the vector   or  .\n        Similarly, the quadratic form\n          attains its minimum value at   or  .\n        We conclude that  .\n       \n         \n        shows the image of the unit circle under the action of   and the images of   and   where\n          are the two unit eigenvectors of  .\n        The image also supports that   assumes its maximum and minimum values for points on the unit circle at   and  .\n       The image of the unit circle under the action of  , and the vectors   and  "
},
{
  "id": "example-59",
  "level": "2",
  "url": "chap_SVD.html#example-59",
  "type": "Example",
  "number": "29.5",
  "title": "",
  "body": "\n        This same process works for matrices other than   ones.\n        For example,\n        consider  .\n        In this case   maps   to  .\n        The image of the unit sphere\n          under left multiplication by   is a filled ellipse as shown in  .\n       The image of the unit circle under the action of  , and the vectors   and  \n        As with the previous example,\n        the norm of   is the square root of the maximum value of\n          and this maximum value is the dominant eigenvalue of  .\n        The eigenvalues of   are\n         ,  ,\n        and   with corresponding unit eigenvectors  ,\n         ,\n        and  .\n        So in this case we have  .\n        The transformation defined by matrix multiplication by   from   to   has a one-dimensional kernel which is spanned by the eigenvector corresponding to  .\n        The image of the transformation is 2-dimensional and the image of the unit circle is an ellipse where   gives the major axis of the ellipse and   gives the minor axis.\n        Essentially, the square roots of the eigenvalues of\n          tell us how   stretches the image space in each direction.\n       "
},
{
  "id": "activity-110",
  "level": "2",
  "url": "chap_SVD.html#activity-110",
  "type": "Activity",
  "number": "29.2",
  "title": "",
  "body": "\n      Let  .\n      Then  .\n      The eigenvalues of   are   and\n        with respective eigenvectors\n        and  .\n     \n            Find  .\n           \n            Find a unit vector   at which\n              assumes its maximum value.\n           "
},
{
  "id": "pa_7_c_2",
  "level": "2",
  "url": "chap_SVD.html#pa_7_c_2",
  "type": "Preview Activity",
  "number": "29.3",
  "title": "",
  "body": "\n      Let  .\n      Since   is not square,\n      we cannot diagonalize  .\n      However, the matrix\n       \n      is a symmetric matrix and can be orthogonally diagonalized.\n      The eigenvalues of   are 3, 1, and 0 with corresponding eigenvectors\n       ,\n      respectively.\n      Use appropriate technology to do the following.\n     \n            Find an orthogonal matrix   that orthogonally diagonalizes  , where\n             .\n           \n            For  , let  .\n            Find each  .\n            Why don't we define   in this way?\n           \n            Let  .\n            What kind of matrix is  ?\n            Explain.\n           \n            Calculate the matrix product  .\n            What do you notice?\n            How is this similar to the eigenvector decomposition of a matrix?\n           "
},
{
  "id": "definition-66",
  "level": "2",
  "url": "chap_SVD.html#definition-66",
  "type": "Definition",
  "number": "29.7",
  "title": "",
  "body": "singular values singular values "
},
{
  "id": "theorem-72",
  "level": "2",
  "url": "chap_SVD.html#theorem-72",
  "type": "Theorem",
  "number": "29.8",
  "title": "The Singular Value Decomposition.",
  "body": "The Singular Value Decomposition \n        Let   be an   matrix of rank  .\n        There exist an   orthogonal matrix  ,\n        an   orthogonal matrix  ,\n        and an   matrix   whose first   diagonal entries are the singular values  ,\n         ,  ,\n          and whose other entries are 0, such that\n         .\n       "
},
{
  "id": "p-4958",
  "level": "2",
  "url": "chap_SVD.html#p-4958",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "right singular vectors left singular vectors "
},
{
  "id": "ex_7_c_SVD",
  "level": "2",
  "url": "chap_SVD.html#ex_7_c_SVD",
  "type": "Activity",
  "number": "29.4",
  "title": "",
  "body": "\n      Let  .\n      Then  .\n      The eigenvalues of   are   and\n        with respective eigenvectors\n        and  .\n     \n            Find an orthonormal basis   of eigenvectors of  .\n            What is  ?\n            Find the matrix   in a SVD for  .\n           \n            Find the singular values of  .\n            What is the rank   of  ?\n            Why?\n           \n            What are the dimensions of the matrix   in a SVD of  ?\n            Find  .\n           \n            Find the vectors  ,\n             ,  ,  .\n            If necessary, extend this set to an orthonormal basis\n             \n            of  .\n           \n            Find the matrix   so that\n              is a SVD for  .\n           "
},
{
  "id": "p-4973",
  "level": "2",
  "url": "chap_SVD.html#p-4973",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "outer product decomposition "
},
{
  "id": "example-60",
  "level": "2",
  "url": "chap_SVD.html#example-60",
  "type": "Example",
  "number": "29.9",
  "title": "",
  "body": "\n        Let  .\n       \n              Find a singular value decomposition for  .\n              You may use technology to find eigenvalues and eigenvectors of matrices.\n             \n              With   as given,\n              we have  .\n              Technology shows that the eigenvalues of\n                are  ,\n               ,  ,\n              and   with corresponding orthonormal eigenvectors  ,\n               ,\n               ,\n              and  .\n              This makes  .\n              The singular values of   are  ,\n               ,\n               , and  ,\n              so   is the   matrix with the nonzero singular values along the diagonal and zeros everywhere else.\n              Finally, we define the vectors   as  .\n              Again, technology gives us  ,\n               ,\n              and  .\n              Thus, a singular value decomposition of   is  , where\n               .\n             \n              Use the singular value decomposition to find a basis for  ,\n               , and  .\n             \n              Recall that the right singular vectors of an\n                matrix   of rank   form an orthonormal basis\n                of eigenvectors of   such that\n                for   from 1 to   with  .\n              These vectors are the columns of the matrix\n                in a singular value decomposition of  .\n              For   from 1 to  ,\n              we let  .\n              Then the set   forms an orthonormal basis of  .\n              We extend this set   to an orthonormal basis   of  .\n              Recall also that   for  .\n              Since  ,\n              it follows that the vectors  ,  ,\n               ,   form a basis for  .\n              Finally, the set   is a basis for  .\n              So in our example, we have  ,  ,\n               ,\n               ,\n               ,\n              and  .\n              Since the singular values of   are  ,  ,\n               , and  , it follows that  .\n              We also have  ,\n               ,\n              and  .\n              So\n               \n              is a basis for   and\n               \n              is a basis for  .\n              Finally, the set\n               \n              is a basis for  .\n             "
},
{
  "id": "example-61",
  "level": "2",
  "url": "chap_SVD.html#example-61",
  "type": "Example",
  "number": "29.10",
  "title": "",
  "body": "\n        Let\n         .\n       \n        The eigenvalues of   are\n         ,  ,\n        and   with corresponding eigenvectors\n         .\n       \n        In addition,\n         .\n       \n              Find orthogonal matrices   and  ,\n              and the matrix  ,\n              so that   is a singular value decomposition of  .\n             \n              Normalizing the eigenvectors  ,  ,\n              and   to normal eigenvectors  ,  ,\n              and  , respectively,\n              gives us an orthogonal matrix\n               .\n              Now  ,\n              so normalizing the vectors   and   gives us vectors\n               \n              that are the first two columns of our matrix  .\n              Given that   is a   matrix,\n              we need to find two other vectors orthogonal to   and   that will combine with   and   to form an orthogonal basis for  .\n              Letting  ,\n               ,\n               ,\n              and  ,\n              a computer algebra system shows that the reduced row echelon form of the matrix   is  ,\n              so that vectors  ,\n               ,\n               ,   are linearly independent.\n              Letting   and  ,\n              the Gram-Schmidt process shows that the set\n                is an orthogonal basis for  , where\n               \n              and (using   for  )\n               .\n              The set   where  ,\n               ,\n                and\n                is an orthonormal basis for   and we can let\n               .\n              The singular values of   are\n                and  , and so\n               .\n              Therefore, a singular value decomposition of   is   of\n               .\n             \n              Determine the best rank 1 approximation to  .\n             \n              Determine the best rank 1 approximation to  .\n              The outer product decomposition of   is\n               .\n              So the rank one approximation to   is\n               .\n              Note that the rows in this rank one approximation are the averages of the two distinct rows in the matrix  ,\n              which makes sense considering that this is the closest rank one matrix to  .\n             "
},
{
  "id": "exercise-288",
  "level": "2",
  "url": "chap_SVD.html#exercise-288",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Find a singular value decomposition of the following matrices.\n       \n                 \n             \n               ,\n               ,\n               .\n             \n                 \n             \n               ,\n               ,  .\n             \n                 \n             \n               ,\n               ,\n               .\n             \n                 \n             \n               ,\n               ,\n               .\n             \n                \n             \n               ,\n               ,\n               .\n             "
},
{
  "id": "exercise-289",
  "level": "2",
  "url": "chap_SVD.html#exercise-289",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Let   be an   matrix of rank   with singular value decomposition  ,\n        where   and  .\n        We have seen that the set   is a basis for  ,\n        and the vectors  ,\n         ,  ,\n          form a basis for  .\n        In this exercise we examine the set\n          and determine what this set tells us about  .\n       \n              Find a singular value decomposition for  .\n              \n             \n              Use the singular value decomposition   for  .\n             \n              Explain why the result of (a) shows that the set\n                is a basis for  .\n             "
},
{
  "id": "exercise-290",
  "level": "2",
  "url": "chap_SVD.html#exercise-290",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Let  .\n       \n              Find the singular values of  .\n             \n               \n             \n              Find a singular value decomposition of  .\n             \n               ,\n               ,\n               .\n             \n              Use a singular value decomposition to find orthonormal bases for the following:\n             \n                     \n                   \n                     .\n                   \n                     \n                   \n                     .\n                   \n                     \n                   \n                     .\n                   "
},
{
  "id": "exercise-291",
  "level": "2",
  "url": "chap_SVD.html#exercise-291",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Let   have the singular value decomposition as in  .\n       \n              Show, using  , that  .\n             \n              Explain why  .\n              \n             \n              The set   is an orthonormal basis of  .\n              Use this to show that   for any unit vector   in  .\n             "
},
{
  "id": "exercise-292",
  "level": "2",
  "url": "chap_SVD.html#exercise-292",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Show that   and   have the same nonzero singular values.\n        How are their singular value decompositions related?\n       \n        Find the transpose of an SVD for  .\n       "
},
{
  "id": "ex_7_c_AAT",
  "level": "2",
  "url": "chap_SVD.html#ex_7_c_AAT",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        The vectors   that form the columns of the matrix   in a singular value decomposition of a matrix   are eigenvectors of  .\n        In this exercise we investigate the vectors   that make up the columns of the matrix   in a singular value decomposition of a matrix   for each   between 1 and the rank of  ,\n        and their connection to the matrix  .\n       >\n             \n              Let  .\n              A singular value decomposition of   is  , where\n               .\n             \n                    Determine the rank   of\n                      and identify the vectors  ,\n                     ,  ,  .\n                   \n                    Calculate   for each   between 1 and  .\n                    How is   related to  ?\n                   \n              Now we examine the result of part (a) in general.\n              Let   be an arbitrary matrix.\n              Calculate   for\n                and determine specifically how\n                is related to  .\n              What does this tell us about the vectors   and the matrix  ?\n             \n              Now show in general that the columns of   are orthonormal eigenvectors for  . (That is,\n              what can we say about the vectors   if  ?)\n             "
},
{
  "id": "exercise-294",
  "level": "2",
  "url": "chap_SVD.html#exercise-294",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        If   is a symmetric matrix with eigenvalues\n         ,\n        what is  ?\n        Justify your answer.\n       \n         \n       "
},
{
  "id": "ex_7_c_Symmetric_SVD",
  "level": "2",
  "url": "chap_SVD.html#ex_7_c_Symmetric_SVD",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Let   be a   symmetric matrix.\n       \n              Show that if   is an eigenvector of   with eigenvalue  ,\n              then   is an eigenvector for  .\n              What is the corresponding eigenvalue?\n             \n              Show that if   is an eigenvector of\n                with non-negative eigenvalue  ,\n              then   is an eigenvector of  .\n              What is the corresponding eigenvalue?\n             \n              Suppose   is a singular value decomposition of  .\n              Explain why   is also a singular value decomposition of  .\n             "
},
{
  "id": "ex_7_c_rank1",
  "level": "2",
  "url": "chap_SVD.html#ex_7_c_rank1",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        Let  ,  ,\n         ,   and  ,\n         ,  ,\n          be the vectors found in a singular value decomposition of a matrix  ,\n        where   is the rank of  .\n        Show that   is a rank 1 matrix for each  .\n        \n       \n        Mimic  \n        in  .\n       "
},
{
  "id": "exercise-297",
  "level": "2",
  "url": "chap_SVD.html#exercise-297",
  "type": "Exercise",
  "number": "10",
  "title": "",
  "body": "\n        Is it possible for a matrix   to have a singular value decomposition\n          in which  ?\n        If no, explain why.\n        If yes, determine for which matrices we can have  .\n       "
},
{
  "id": "exercise-298",
  "level": "2",
  "url": "chap_SVD.html#exercise-298",
  "type": "Exercise",
  "number": "11",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If   is a singular value of a matrix  ,\n              then   is an eigenvalue of  .\n             \n              F\n             True\/False \n              A set of right singular vectors of a matrix   is also a set of left singular vectors of  .\n             True\/False \n              The transpose of a singular value decomposition of a matrix   is a singular value decomposition for  .\n             \n              T\n             True\/False \n              Similar matrices have the same singular values.\n             True\/False \n              If   is an   matrix,\n              then the singular values of   are the squares of the singular values of  .\n             \n              F\n             True\/False \n              The   matrix in an SVD of   is unique.\n             True\/False \n              The matrices   in an SVD of   are unique.\n             \n              F\n             True\/False \n              If   is a positive definite matrix,\n              then an orthogonal diagonalization of   is an SVD of  .\n             "
},
{
  "id": "p-5068",
  "level": "2",
  "url": "chap_SVD.html#p-5068",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "stop words term-document "
},
{
  "id": "act_LSI_standard",
  "level": "2",
  "url": "chap_SVD.html#act_LSI_standard",
  "type": "Project Activity",
  "number": "29.5",
  "title": "",
  "body": "\n      In a standard term-matching search with\n        term-document matrix  ,\n      a query vector   would be matched with the terms to determine the number of matches.\n      The matching counts the number of times each document agrees with the query.\n     \n            Explain why this matching is accomplished by the matrix-vector product  .\n           \n            Let  .\n            Explain why  ,\n            where   is the  th column of   and\n              is the angle between   and  .\n           \n            We can use the cosine calculation from part (b) to compare matches to our query   the closer the cosine is to  ,\n            the better the match\n            (dividing by the product of the norms is essentially converting all vectors to unit vectors for comparison purposes).\n            This is often referred to as the cosine distance.\n            Calculate the cosines of the\n              for our example of the query  .\n            Order the documents from best to worst match for this query.\n           "
},
{
  "id": "act_LSI_tt_dd",
  "level": "2",
  "url": "chap_SVD.html#act_LSI_tt_dd",
  "type": "Project Activity",
  "number": "29.6",
  "title": "",
  "body": "\n      Let  ,\n      where  ,  ,\n       ,   are the columns of  .\n     \n            In  \n            you should have seen that  .\n            Assume for the moment that all of the entries in   are either   or  .\n            Explain why in this case the dot product\n              tells us how many terms documents   and   have in common.\n            Also, the matrix   takes dot products of the columns of  ,\n            which refer to what's happening in each document and so is looking at document-document interactions.\n            For these reasons, we call\n              the document-document matrix.\n           \n            Use appropriate technology to calculate the entries of the matrix  .\n            This matrix is the term-term matrix.\n            Assume for the moment that all of the entries in   are either   or  .\n            Explain why if terms   and   occur together in   documents,\n            then  .\n           "
},
{
  "id": "act_LSI_ATA_AAT",
  "level": "2",
  "url": "chap_SVD.html#act_LSI_ATA_AAT",
  "type": "Project Activity",
  "number": "29.7",
  "title": "",
  "body": "\n      To see why a singular value decomposition might be useful,\n      suppose our term-document matrix   has singular value decomposition\n       . (Don't actually calculate the SVD yet).\n     \n            Show that the document-document matrix\n              satisfies  .\n            This means that we can compare document   and document   using the dot product of row   and column   of the matrix product  .\n           \n            Show that the term-term matrix\n              satisfies  .\n            Thus we can compare term   and term   using the dot product of row   and column   of the matrix product  .\n            ( \n            shows that the columns of   are orthogonal eigenvectors of  .)\n           "
},
{
  "id": "act_LSI_SVD",
  "level": "2",
  "url": "chap_SVD.html#act_LSI_SVD",
  "type": "Project Activity",
  "number": "29.8",
  "title": "",
  "body": "\n      The singular value decomposition (SVD) allows us to produce new,\n      improved term-document matrices.\n      For this activity,\n      use the term-document matrix   in  .\n     \n            Use appropriate technology to find a singular value decomposition of   so that  .\n            Print your entries to two decimal places\n            (but keep as many as possible for computational purposes).\n           \n            Each singular value tells us how important its semantic dimension is.\n            If we remove the smaller singular values\n            (the less important dimensions),\n            we retain the important information but eliminate minor details and noise.\n            We produce a new term-document matrix   by keeping the largest   of the singular values and discarding the rest.\n            This gives us an approximation\n             \n            using the outer product decomposition,\n            where  ,  ,  ,\n              are the   largest singular values of  .\n            Note that if   is an   matrix,\n            letting  \n            (an   matrix),\n              the   matrix with the first   singular values along the diagonal,\n            and   (a   matrix),\n            then we can also write  .\n            This is sometimes referred to as a reduced SVD.  Find  ,\n             ,\n            and  , and find the new term-document matrix  .\n           "
},
{
  "id": "act_LSI_term_document",
  "level": "2",
  "url": "chap_SVD.html#act_LSI_term_document",
  "type": "Project Activity",
  "number": "29.9",
  "title": "",
  "body": "\n      We have seen how to compare terms to terms and documents to documents.\n      The matrix   itself compares terms to documents.\n      Show that  ,\n      where   is the diagonal matrix of the same size as\n        whose diagonal entries are the square roots of the corresponding diagonal entries in  .\n      Thus, all useful comparisons of terms and documents can be made using the rows of the matrices   and  ,\n      scaled in some way by the singular values in  .\n     "
},
{
  "id": "p-5089",
  "level": "2",
  "url": "chap_SVD.html#p-5089",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "pseudo-document "
},
{
  "id": "act_LSI_concept_space",
  "level": "2",
  "url": "chap_SVD.html#act_LSI_concept_space",
  "type": "Project Activity",
  "number": "29.10",
  "title": "",
  "body": "\n      For an original query  ,\n      we start with its term vector\n        (a vector in the coordinate system determined by the columns of  ) and find a representation\n        that we can use as a column of   in the document-document comparison matrix.\n      If this representation was perfect,\n      then it would take a real document in the original system given by   and produce the corresponding column of   if we used the full SVD. In other words,\n      we would have  .\n     \n            Use the fact that  ,\n            to show that  .\n            It follows that   is transformed into the query  .\n           \n            In our example, using  ,\n            the terms can now be represented as  -dimensional vectors\n            (the rows of  , see  ),\n            or as points in the plane.\n            More specifically,  human \n            is represented by the vector\n            (to two decimal places)\n             ,\n             interface  by  , etc.\n            Similarly, the documents are represented by columns of  \n            (see  ),\n            so that the document   is represented by  ,\n              by  , etc.\n            From this perspective we can visualize these documents in the plane.\n            Plot the documents and the query in the 2-dimensional concept space.\n            Then calculate the cosine distances from the query to the documents in this space.\n            Which documents now give the best three matches to the query?\n            Compare the matches to your plot.\n           "
},
{
  "id": "chap_pseudoinverses",
  "level": "1",
  "url": "chap_pseudoinverses.html",
  "type": "Section",
  "number": "30",
  "title": "Using the Singular Value Decomposition",
  "body": "Using the Singular Value Decomposition \n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            What is the condition number of a matrix and what does it tell us about the matrix?\n           \n         \n           \n            What is the pseudoinverse of a matrix?\n           \n         \n           \n            Why are pseudoinverses useful?\n           \n         \n           \n            How does the pseudoinverse of a matrix allow us to find least squares solutions to linear systems?\n           \n         Application: Global Positioning System \n    You are probably familiar with the Global Positioning System (GPS).\n    The system allows anyone with the appropriate software to accurately determine their location at any time.\n    The applications are almost endless,\n    including getting real-time driving directions while in your car,\n    guiding missiles, and providing distances on golf courses.\n   \n    The GPS is a worldwide radio-navigation system owned by the US government and operated by the US Air Force.\n    GPS is one of four global navigation satellite systems.\n    At least twenty four GPS satellites orbit the Earth at an altitude of approximately 11,000 nautical miles.\n    The satellites are placed so that at any time at least four of them can be accessed by a GPS receiver.\n    Each satellite carries an atomic clock to relay a time stamp along with its position in space.\n    There are five ground stations to coordinate and ensure that the system is working properly.\n   \n    The system works by triangulation,\n    but there is also error involved in the measurements that go into determining position.\n    Later in this section we will see how the method of least squares can be used to determine the receiver's position.\n   Introduction \n    A singular value decomposition has many applications,\n    and in this section we discuss how a singular value decomposition can be used in image compression,\n    to determine how sensitive a matrix can be to rounding errors in the process of row reduction,\n    and to solve least squares problems.\n   Image Compression \n    The digital age has brought many new opportunities for the collection,\n    analysis,\n    and dissemination of information.\n    Along with these opportunities come new difficulties as well.\n    All of this digital information must be stored in some way and be retrievable in an efficient manner.\n    A singular value decomposition of digitally stored information can be used to compress the information or clean up corrupted information.\n    In this section we will see how a singular value decomposition can be used in image compression.\n    While a singular value decomposition is normally used with very large matrices,\n    we will restrict ourselves to small examples so that we can more clearly see how a singular value decomposition is applied.\n   \n      Let  .\n      A singular value decomposition for   is  , where\n       .\n       \n            Write the summands in the corresponding outer product decomposition of  .\n           \n            The outer product decomposition of   writes   as a sum of rank 1 matrices (the summands  .\n            Each summand contains some information about the matrix  .\n            Since   is the largest of the singular values,\n            it is reasonable to expect that the summand\n              contains the most information about   among all of the summands.\n            To get a measure of how much information   contains of  ,\n            we can think of   as simply a long vector in   where we have folded the data into a rectangular array\n            (we will see later why taking the norm as the norm of the vector in   makes sense,\n            but for now, just use this definition).\n            If we are interested in determining the error in approximating an image by a compressed image,\n            it makes sense to use the standard norm in   to determine length and distance,\n            which is really just the Frobenius norm that comes from the Frobenius inner product defined by\n             ,\n            where   and\n              are   matrices.\n            (That   defines an inner product on the set of all\n              matrices is left to discuss in a later section.)\n            So in this section all the norms for matrices will refer to the Frobenius norm.\n            Rather than computing the distance between   and   to measure the error,\n            we are more interested in the relative error\n             .\n           \n                  Calculate the relative error in approximating   by  .\n                  What does this tell us about how much information   contains about  ?\n                 \n                  Let  .\n                  Calculate the relative error in approximating   by  .\n                  What does this tell us about how much information   contains about  ?\n                 \n                  Let  .\n                  Calculate the relative error in approximating   by  .\n                  What does this tell us about how much information   contains about  ?\n                 \n                  Let  .\n                  Calculate the relative error in approximating   by  .\n                  What does this tell us about how much information   contains about  ?\n                  Why?\n                 \n    The first step in compressing an image is to digitize the image.\n    There are many ways to do this and we will consider one of the simplest ways and only work with gray-scale images,\n    with the scale from 0 (black) to 255\n    (white).\n    A digital image can be created by taking a small grid of squares\n    (called pixels)\n    and coloring each pixel with some shade of gray.\n    The resolution of this grid is a measure of how many pixels are used per square inch.\n    As an example,\n    consider the 16 by 16 pixel picture of a flower shown in  .\n   A 16 by 16 pixel image \n    To store this image pixel by pixel would require\n      units of storage space (1 for each pixel).\n    If we let   be the matrix whose  th entry is the scale of the  th pixel,\n    then   is the matrix\n     .\n   \n    Recall that if   is a singular value decomposition for  ,\n    then we can also write   in the form\n     .\n    given in  .\n    For this  , the singular values are approximately\n     .\n   \n    Notice that some of these singular values are very small compared to others.\n    As in  ,\n    the terms with the largest singular values contain most of the information about the matrix.\n    Thus, we shouldn't lose much information if we eliminate the small singular values.\n    In this particular example,\n    the last 4 singular values are significantly smaller than the rest.\n    If we let\n     ,\n    then we should expect the image determined by   to be close to the image made by  .\n    The two images are presented side by side in  .\n   A 16 by 16 pixel image and a compressed image using a singular value decomposition. \n    This small example illustrates the general idea.\n    Suppose we had a satellite image that was\n      pixels and we let   represent this image.\n    If we have a singular value decomposition of this image  , say\n     ,\n    if the rank of   is large,\n    it is likely that many of the singular values will be very small.\n    If we only keep   of the singular values,\n    we can approximate   by\n     \n    and store the image with only the vectors  ,\n     ,  ,  ,\n     ,  ,  ,  .\n    For example,\n    if we only need 10 of the singular values of a satellite image ( ),\n    then we can store the satellite image with only 20 vectors in   or with\n      numbers instead of   numbers.\n   \n    A similar process can be used to denoise data. \n    For example,\n    as stated in  ,\n     The SVD [singular value decomposition] has also applications in digital signal processing,\n    e.g., as a method for noise reduction.\n    The central idea is to let a matrix   represent the noisy signal,\n    compute the SVD, and then discard small singular values of  .\n    It can be shown that the small singular values mainly represent the noise,\n    and thus the rank-  matrix   represents a filtered signal with less noise. \n     \n   Calculating the Error in Approximating an Image Frobenius Frobenius product \n    If an   matrix   of rank   has a singular value decomposition  ,\n    we have seen that we can write   as an outer product\n     ,\n    where the   are the columns of   and the   the columns of  .\n    Each of the products   is an   matrix.\n    Since the columns of   are all scalar multiples of  ,\n    the matrix   is a rank 1 matrix.\n    So   expresses   as a sum of rank 1 matrices.\n    Moreover, if we let   and   be\n      vectors and let   and   be   vectors with\n      and  , then\n     .\n   \n    Using the vectors from the singular value decomposition of   as in   we see that\n     \n   \n    It follows that\n     .\n   \n      Verify   that  .\n     \n    When we used the singular value decomposition to approximate the image defined by  ,\n    we replaced   with a matrix of the form\n     .\n   \n    We call   the rank   approximation to  .\n    Notice that the outer product expansion in   is in fact a singular value decomposition for  .\n    The error   in approximating   with   is\n     .\n   \n    Once again, notice that   is a singular value decomposition for  .\n    We define the relative error in approximating   with   as\n     .\n   \n    Now   shows that\n     .\n   \n    In applications,\n    we often want to retain a certain degree of accuracy in our approximations and this error term can help us accomplish that.\n   \n    In our flower example,\n    the singular values of   are given in  .\n    The relative error in approximating   with   is\n     .\n   \n    Errors (rounded to 4 decimal places) for approximating   with some of the   are shown in  \n   Errors in approximating   by  10 9 8 7 6 0.0070 0.0146 0.0252 0.0413 0.06426 5 4 3 2 1 0.0918 0.1231 0.1590 0.201 0.2460 \n      Let   represent the flower image.\n     \n            Find the relative errors in approximating   by   and  .\n            You may use the fact that  .\n           \n            About how much of the information in the image is contained in the rank 1 approximation?\n            Explain.\n           The Condition Number of a Matrix \n    A singular value decomposition for a matrix   can tell us a lot about how difficult it is to accurately solve a system  .\n    Solutions to systems of linear equations can be very sensitive to rounding as the next exercise demonstrates.\n   \n      Find the solution to each of the systems.\n     \n             \n           \n             \n           \n    Notice that a simple rounding in the   entry of the coefficient matrix led to a significantly different solution.\n    If there are rounding errors at any stage of the Gaussian elimination process,\n    they can be compounded by further row operations.\n    This is an important problem since computers can only approximate irrational numbers with rational numbers and so rounding can be critical.\n    Finding ways of dealing with these kinds of errors is an area of on-going research in numerical linear algebra.\n    This problem is given a name.\n   ill-conditioned well-conditioned \n    Suppose we want to solve the system  ,\n    where   is an invertible matrix.\n     \n    illustrates that if   is really close to being singular,\n    then small changes in the entries of   can have significant effects on the solution to the system.\n    So the system can be very hard to solve accurately if   is close to singular.\n    It is important to have a sense of how\n     good \n    we can expect any calculated solution to be.\n    Suppose we think we solve the system   but,\n    through rounding error in our calculation of  ,\n    get a solution   so that  ,\n    where   is not exactly  .\n    Let   be the error in our calculated solution and\n      the difference between   and  .\n    We would like to know how large the error   can be.\n    But this isn't exactly the right question.\n    We could scale everything to make   as large as we want.\n    What we really need is a measure of the  relative error \n     ,\n    or how big the error is compared to   itself.\n    More specifically, we want to know how large the relative error in\n      is compared to the relative error in  .\n    In other words, we want to know how good the relative error in\n      is as a predictor of the relative error in  \n    (we may have some control over the relative error in  ,\n    perhaps by keeping more significant digits).\n    So we want know if there is a best constant   such that\n     .\n   \n    This best constant   is the condition number   a measure of how well the relative error in\n      predicts the relative error in  .\n    How can we find  ?\n   \n    Since   we have\n     .\n   \n    Distributing on the left and using the fact that   gives us\n     .\n   \n    We return for a moment to the operator norm of a matrix.\n    This is an appropriate norm to use here since we are considering   to be a transformation.\n    Recall that if   is an   matrix,\n    we defined the operator norm of   to be\n     .\n \n    One important property that the norm has is that if the product   is defined, then\n     .\n\n    To see why, notice that\n     .\n   \n    Now   and\n      by the definition of the norm,\n    so we conclude that\n     \n    for every  .\n    Thus,\n     .\n   \n    Now we can find the condition number.\n    From   we have\n     ,\n    so\n     .\n   \n    Similarly,   implies that   or\n     .\n   \n    Combining   and   gives\n     .\n   \n    This constant   is the best bound and so is called the condition number of  .\n   condition number \n    How does a singular value decomposition tell us about the condition number of a matrix?\n    Recall that the maximum value of\n      for   on the unit  -sphere is  .\n    So  .\n    If   is an invertible matrix and\n      is a singular value decomposition for  , then\n     ,\n    where\n     .\n   \n    Now   is a singular value decomposition for   with the diagonal entries in reverse order, so\n     .\n   \n    Therefore, the condition number of   is\n     .\n   \n      Let  .\n      A computer algebra system gives the singular values of   as 2.00025003124999934 and 0.000249968750000509660.\n      What is the condition number of  .\n      What does that tell us about  ?\n      Does this seem reasonable given the result of  ?\n     \n          What is the smallest the condition number of a matrix can be?\n          Find an entire class of matrices with this smallest condition number.\n         \n          What is the condition number of an orthogonal matrix?\n          Why does this make sense?\n          \n         \n          If   is an orthogonal matrix,\n          what is   for any vector  ?\n          What does this make  ?\n         \n          What is the condition number of an invertible symmetric matrix in terms of its eigenvalues?\n         \n          Why do we not define the condition number of a non-invertible matrix?\n          If we did, what would the condition number have to be?\n          Why?\n         Pseudoinverses \n    Not every matrix is invertible,\n    so we cannot always solve a matrix equation  .\n    However, every matrix has a pseudoinverse   that acts something like an inverse.\n    Even when we can't solve a matrix equation\n      because   isn't in  ,\n    we can use the pseudoinverse of   to\n     solve \n    the equation   with the\n     solution \n     .\n    While not an exact solution,\n      turns out to be the best approximation to a solution in the least squares sense.\n    We will use the singular value decomposition to find the pseudoinverse of a matrix.\n   \n      Let  .\n      The singular value decomposition of   is   where\n       .\n     \n            Explain why   is not an invertible matrix.\n           \n            Explain why the matrices   and   are invertible.\n            How are   and   related to   and  ?\n           \n            Recall that one property of invertible matrices is that the inverse of a product of invertible matrices is the product of the inverses in the reverse order.\n            If   were invertible,\n            then   would be  .\n            Even though   and   are invertible,\n            the matrix   is not.\n            But   does contain non-zero eigenvalues that have reciprocals,\n            so consider the matrix  .\n            Calculate the products   and  .\n            How are the results similar to that obtained with a matrix inverse?\n           \n            The only matrix in the singular value decomposition of   that is not invertible is  .\n            But the matrix   acts somewhat like an inverse of  ,\n            so let us define   as  .\n            Now we explore a few properties of the matrix  .\n           \n                  Calculate   and   for  .\n                  What do you notice?\n                 \n                  Calculate   and   for  .\n                  What do you notice?\n                 \n    Only some square matrices have inverses.\n    However, every matrix has a pseudoinverse.\n    A pseudoinverse   of a matrix   provides something like an inverse when a matrix doesn't have an inverse.\n    Pseudoinverses are useful to approximate solutions to linear systems.\n    If   is invertible,\n    then the equation   has the solution  ,\n    but when   is not invertible and   is not in  ,\n    then the equation   has no solution.\n    In the invertible case of an   matrix  ,\n    there is a matrix   so that  .\n    This also implies that   and  .\n    To mimic this situation when   is not invertible,\n    we search for a matrix   (a pseudoinverse of  ) so that\n      and  ,\n    as we saw in  .\n    Then it turns out that   acts something like an inverse for  .\n    In this case, we approximate the solution to\n      by  ,\n    and we will see that the vector\n      turns out to be the vector in   that is closest to   in the least squares sense.\n   \n    A reasonable question to ask is how we can find a pseudoinverse of a matrix  .\n    A singular value decomposition provides an answer to this question.\n    If   is an invertible   matrix,\n    then 0 is not an eigenvalue of  .\n    As a result,\n    in the singular value decomposition   of  ,\n    the matrix   is an invertible matrix\n    (note that  ,  ,\n    and   are all   matrices in this case).\n    So\n     ,\n    where\n     .\n   \n    In this case,\n      is a singular value decomposition for  .\n   \n    To understand in general how a pseudoinverse is found,\n    let   be an   matrix with  ,\n    or an   with rank less than  .\n    In these cases   does not have an inverse.\n    But as in  ,\n    a singular value decomposition provides a pseudoinverse   for  .\n    Let   be a singular value decomposition of an\n      matrix   of rank  , with\n     \n   \n   pseudoinverse \n    The matrices   and   are invertible,\n    but the matrix   is not if   is not invertible.\n    If we let   be the   matrix defined by\n     ,\n    then   will act much like an inverse of   might.\n    In fact, it is not difficult to see that\n     ,\n    where   is an   matrix and\n      is an   matrix.\n   pseudoinverse \n          Find the pseudoinverse   of  .\n          Use the singular value decomposition   of  , where\n           .\n         \n          The vector   is not in  .\n          The vector   is an approximation to a solution of  ,\n          and   is in  .\n          Find   and determine how far   is from  .\n         \n    Pseudoinverses satisfy several properties that are similar to those of inverses.\n    For example,\n    we had an example in  \n    where   and  .\n    That   always satisfies these properties is the subject of the next activity.\n   \n      Let   be an   matrix with singular value decomposition  .\n      Let   be defined as in  .\n     \n            Show that  .\n           \n            Show that  .\n           Penrose Moore-Penrose The Moore-Penrose Conditions \n        A pseudoinverse of a matrix   is a matrix   that satisfies the following properties.\n         \n             \n               \n             \n           \n             \n               \n             \n           \n             \n               \n             \n           \n             \n               \n             \n           \n       \n    Also, there is a unique matrix   that satisfies these properties.\n    The verification of this property is left to the exercises.\n   Least Squares Approximations \n    The pseudoinverse of a matrix is also connected to least squares solutions of linear systems as we encountered in  .\n    Recall from  ,\n    that if the columns of   are linearly independent,\n    then the least squares solution to\n      is  .\n    In this section we will see how to use a pseudoinverse to solve a least squares problem,\n    and verify that if the columns of   are linearly dependent,\n    then   is in fact the pseudoinverse of  .\n   \n    Let   be a singular value decomposition for an\n      matrix   of rank  .\n    Then the columns of\n     \n    form an orthonormal basis for   and\n      is a basis for  .\n    Remember from  \n    that if   is any vector in  , then\n     \n    is the least squares approximation of the vector   by a vector in  .\n    We can extend this sum to all of columns of   as\n     .\n   \n    It follows that\n     ,\n    where\n     .\n   \n    Now, if  , then\n     ,\n    and hence the vector   is the vector   in   that minimizes  .\n    Thus,   is in actuality the least squares approximation to  .\n    So a singular value decomposition allows us to construct the pseudoinverse of a matrix   and then directly solve the least squares problem.\n   \n    If the columns of   are linearly independent,\n    then we do not need to use an SVD to find the pseudoinverse,\n    as the next activity illustrates.\n   \n      Having to calculate eigenvalues and eigenvectors for a matrix to produce a singular value decomposition to find pseudoinverse can be computationally intense.\n      As we demonstrate in this activity,\n      the process is easier if the columns of   are linearly independent.\n      More specifically, we will prove the following theorem.\n     \n          If the columns of a matrix   are linearly independent,\n          then  .\n         \n      To see how, suppose that   is an\n        matrix with linearly independent columns.\n     \n            Given that the columns of   are linearly independent,\n            what must be the relationship between   and  ?\n           \n            Since the columns of   are linearly independent,\n            it follows that   is invertible\n            (see  ).\n            So the eigenvalues of   are all non-zero.\n            Let  ,  ,  ,\n              be the singular values of  .\n            How is   related to  ,\n            and what do   and   look like?\n           \n            Let us now investigate the form of the invertible matrix  \n            (note that neither   nor   is necessarily invertible).\n            If a singular value decomposition of   is  ,\n            show that\n             .\n           \n            Let   for   from 1 to  .\n            It is straightforward to see that   is an\n              diagonal matrix  , where\n             .\n            Then  .\n            Recall that  ,\n            so to relate   to   we need a product that is equal to  .\n            Explain why\n             .\n           \n            Complete the activity by showing that\n             .\n           \n    Therefore, to calculate   and solve a least squares problem,\n     \n    shows that as long as the columns of   are linearly independent,\n    we can avoid using a singular value decomposition of   in finding  .\n   Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Let\n         .\n       \n        The eigenvalues of   are\n         ,  ,\n        and   with corresponding eigenvectors\n         .\n       \n        In addition,\n         .\n       \n              Find orthogonal matrices   and  ,\n              and the matrix  ,\n              so that   is a singular value decomposition of  .\n             \n              Normalizing the eigenvectors  ,  ,\n              and   to normal eigenvectors  ,  ,\n              and  , respectively,\n              gives us an orthogonal matrix\n               .\n              Now  ,\n              so normalizing the vectors   and   gives us vectors\n               \n              that are the first two columns of our matrix  .\n              Given that   is a   matrix,\n              we need to find two other vectors orthogonal to   and   that will combine with   and   to form an orthogonal basis for  .\n              Letting  ,\n               ,\n               ,\n              and  ,\n              a computer algebra system shows that the reduced row echelon form of the matrix   is  ,\n              so that vectors  ,\n               ,\n               ,   are linearly independent.\n              Letting   and  ,\n              the Gram-Schmidt process shows that the set\n                is an orthogonal basis for  ,\n              where   and\n              (using   for  )\n               .\n              The set   where  ,\n               ,\n                and\n                is an orthonormal basis for   and we can let\n               .\n              The singular values of   are\n                and  , and so\n               .\n              Therefore, a singular value decomposition of   is   of\n               .\n             \n              Determine the best rank 1 approximation to  .\n              Give an appropriate numerical estimate as to how good this approximation is to  .\n             \n              The outer product decomposition of   is\n               .\n              So the rank one approximation to   is\n               .\n              The error in approximating   with this rank one approximation is\n               .\n             \n              Find the pseudoinverse   of  .\n             \n              Given that  ,\n              we use the pseudoinverse   of   to find the pseudoinverse   of   by\n               .\n              Now\n               ,\n              so\n               .\n             \n              Let  .\n              Does the matrix equation\n               \n              have a solution?\n              If so, find the solution.\n              If not, find the best approximation you can to a solution to this matrix equation.\n             \n              Augmenting   with   and row reducing shows that\n               ,\n              so   is not in   and the equation   has no solution.\n              However, the best approximation to a solution to\n                is found using the pseudoinverse   of  .\n              That best solution is\n               .\n             \n              Use the orthogonal basis   of   to find the projection of   onto  .\n              Compare to your solution in part (c).\n             \n              The rank of   is 2 and an orthonormal basis for   is  ,\n              where   and  .\n              So\n               \n              as expected from part (c).\n             \n         \n        shows the per capita debt in the U.S. in from 2014 to 2019\n        (source  statistica.com ).\n       U.S. per capita debt Year 2014 2015 2016 2017 2018 2019 Debt 55905 56513 60505 62174 65697 69064 \n            Set up a linear system of the form\n              whose least squares solution provides a linear fit to the data.\n           \n            A linear approximation   to the system would satisfy the equation  ,\n              where  ,\n               ,\n              and  .\n           \n            Use technology to approximate a singular value decomposition\n            (round to four decimal places).\n            Use this svd to approximate the pseudoinverse of  .\n            Then use this pseudoinverse to approximate the least squares linear approximation to the system.\n           \n            Technology shows that a singular value decomposition of   is approximately  , where\n               .\n              Thus, with  ,\n              we have that the pseudoinverse of   is approximately\n               .\n              So our least squares linear approximation is found by\n               .\n              This makes our least squares linear approximation to be (to four decimal places)\n               .\n           \n            Calculate   directly and compare to the pseudoinverse you found in part (b).\n           \n            Calculating   gives the same matrix as  ,\n              so we obtain the same linear approximation.\n           \n            Use your approximation to estimate the U.S. per capita debt in 2020.\n           \n            The approximate U.S. per capita debt in 2020 is\n               .\n           Summary \n       \n        The condition number of an\n          matrix   is the number  .\n        The condition number provides a measure of how well the relative error in a calculated value\n          predicts the relative error in\n          when we are trying to solve a system  .\n       \n     \n       \n        A pseudoinverse   of a matrix   can be found through a singular value decomposition.\n        Let   be a singular value decomposition of an\n          matrix   of rank  , with\n         \n        If   is the   matrix defined by\n         ,\n        then  .\n       \n     \n       \n        A pseudoinverse   of a matrix   acts like an inverse for  .\n        So if we can't solve a matrix equation\n          because   isn't in  ,\n        we can use the pseudoinverse of   to\n         solve \n        the equation   with the\n         solution \n         .\n        While not an exact solution,\n          turns out to be the best approximation to a solution in the least squares sense.\n       \n     \n        Let  .\n        Then   has singular value decomposition   , where\n         .\n       \n              What are the singular values of  ?\n             \n               ,  , and  \n             \n              Write the outer product decomposition of  .\n             \n               .\n             \n              Find the best rank 1 approximation to  .\n              What is the relative error in approximating   by this rank 1 matrix?\n             \n               ,\n               .\n             \n              Find the best rank 2 approximation to  .\n              What is the relative error in approximating   by this rank 2 matrix?\n             \n               ,\n               .\n             \n        Let  .\n       \n              Find a singular value decomposition for  .\n             \n              What are the singular values of  ?\n             \n              Write the outer product decomposition of  .\n             \n              Find the best rank 1, 2, and 3 approximations to  .\n              How much information about   does each of these approximations contain?\n             \n        Assume that the number of feet traveled by a batted baseball at various angles in degrees\n        (all hit at the same bat speed)\n        is given in  .\n       Distance traveled by batted ball Angle Distance 116 190 254 285 270 230 \n              Plot the data and explain why a quadratic function is likely a better fit to the data than a linear function.\n             \n              The ball goes up and then it comes down.\n             \n              Find the least squares quadratic approximation to this data.\n              Plot the quadratic function on same axes as your data.\n             \n               .\n             \n              At what angle\n              (or angles),\n              to the nearest degree,\n              must a player bat the ball in order for the ball to travel a distance of 220 feet?\n             \n              Approximately   and   degrees.\n             \n        How close can a matrix be to being non-invertible?\n        We explore that idea in this exercise.\n        Let   be the\n          upper triangular matrix with 1s along the diagonal and with every other entry being  .\n       \n              What is  ?\n              What are the eigenvalues of  ?\n              Is   invertible?\n             \n              Let   be the\n                matrix so that   and\n                for all other   and  .\n             \n                    For the matrix   with  ,\n                    show that the equation   has a non-trivial solution.\n                    Find one non-trivial solution.\n                   \n                    For the matrix   with  ,\n                    show that the equation   has a non-trivial solution.\n                    Find one non-trivial solution.\n                   \n                    Use the pattern established in parts (i.) and (ii.) to find a non-trivial solution to the equation\n                      for an arbitrary value of  .\n                    Be sure to verify that you have a solution.\n                    Is   invertible?\n                    \n                   \n                    For any positive integer  ,\n                    the sum   is the partial sum of a geometric series with ratio   and so  .\n                   \n                    Explain why   is not an invertible matrix.\n                    Notice that   and   differ by a single entry,\n                    and that   is invertible and   is not.\n                    Let us examine how close   is to  .\n                    Calculate  ?\n                    What happens to   as   goes to infinity?\n                    How close can an invertible matrix be to becoming non-invertible?\n                   \n        Let  .\n        In this exercise we find a matrix   so that  ,\n        that is, find a square root of the matrix  .\n       \n              Find the eigenvalues and corresponding eigenvectors for   and  .\n              Explain what you see.\n             \n              The eigenvalues for   are 2, 1, and 0, the eigenvalues for\n                are 4, 1, and 0 with the same corresponding eigenvectors.\n             \n              Find a matrix   that orthogonally diagonalizes  .\n             \n               \n             \n               \n              in  \n              shows if   is a singular value decomposition for a symmetric matrix  ,\n              then so is  .\n              Recall that   for any positive integer  .\n              We can exploit this idea to define   to be the matrix\n               ,\n              where   is the matrix whose diagonal entries are the square roots of the corresponding entries of  .\n              Let  .\n              Calculate   and show that  .\n             \n               \n             \n              Why was it important that   be a symmetric matrix for this process to work,\n              and what had to be true about the eigenvalues of   for this to work?\n             \n                was symmetric to write a singular value decomposition for   in the form  ,\n                is positive definite\n             \n              Can you extend the process in this exercise to find a cube root of  ?\n             \n               \n             \n        Let   be an   matrix with singular value decomposition  .\n        Let   be defined as in  .\n        In this exercise we prove the remaining parts of  .\n       \n              Show that  .\n              \n             \n                is a symmetric matrix.\n             \n              Show that  .\n             \n        In this exercise we show that the pseudoinverse of a matrix is the unique matrix that satisfies the Moore-Penrose conditions.\n        Let   be an   matrix with singular value decomposition\n          and pseudoinverse  .\n        To show that   is the unique matrix that satisfies the Moore-Penrose conditions,\n        suppose that there is another matrix   that also satisfies the Moore-Penrose conditions.\n       \n              Show that  .\n             \n              Use the fact that  .\n             \n              Show that  .\n             \n              Use the fact that  .\n             \n              How do the results of parts (a) and (b) show that   is the unique matrix satisfying the Moore-Penrose conditions?\n             \n              Compare the results of (a) and (b).\n             \n        Find the pseudoinverse of the\n          zero matrix  .\n        Explain the conclusion.\n       \n        In all of the examples that we have done finding a singular value decomposition of a matrix,\n        it has been the case\n        (though we haven't mentioned it),\n        that if   is an   matrix,\n        then  .\n        Prove this result.\n       \n        Use the Rank-Nullity Theorem.\n       \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              A matrix has a pseudo-inverse if and only if the matrix is singular.\n             \n              F\n             True\/False \n              The pseudoinverse of an invertible matrix   is the matrix  .\n             True\/False \n              If the columns of   are linearly dependent,\n              then there is no least squares solution to  .\n             \n              F\n             True\/False \n              If the columns of   are linearly independent,\n              then there is a unique least squares solution to  .\n             True\/False \n              If   is the matrix transformation defined by a matrix   and   is the matrix transformation defined by  ,\n              then   and   are inverse transformations.\n             \n              F\n             Project: GPS and Least Squares \n    In this project we discuss some of the details about how the GPS works.\n    The idea is based on intersections of spheres.\n    To build a basic understanding of the system,\n    we begin with a 2-dimensional example.\n   \n      Suppose that there are three base stations  ,  ,\n      and   in   that can send and receive signals from your mobile phone.\n      Assume that   is located at point  ,\n        at point  ,\n      and   at point  .\n      Also assume that your mobile phone location is point  .\n      Based on the time that it takes to receive the signals from the three base stations,\n      it can be determined that your distance to base station   is   km,\n      your distance to base station   is   km,\n      and your distance to base station   is   km using a coordinate system with measurements in kilometers based on a reference point chosen to be  .\n      Due to limitations on the measurement equipment,\n      these measurements all contain some unknown error which we will denote as  .\n      The goal is to determine your location in   based on this information.\n     \n      If the distance readings were accurate,\n      then the point   would lie on the circle centered at   of radius  .\n      The distance from   to base station   can be represented in two different ways:\n        km and  .\n      However, there is some error in the measurements\n      (due to the receiver clock and satellite clocks not being snychronized),\n      so we really have\n       ,\n      where   is the error.\n      Similarly,   must also satisfy\n       \n      and\n       .\n       \n            Explain how these three equations can be written in the equivalent form\n             .\n\n             Intersections of circles. \n           \n            If all measurements were accurate,\n            your position would be at the intersection of the circles centered at   with radius   km,\n            centered at   with radius   km,\n            and centered at   with radius   km as shown in  .\n            Even though the figure might seem to imply it,\n            because of the error in the measurements the three circles do not intersect in one point.\n            So instead, we want to find the best estimate of a point of intersection that we can.\n            The system of  equations ,\n             ,\n            and  \n            is non-linear and can be difficult to solve,\n            if it even has a solution.\n            To approximate a solution, we can linearize the system.\n            To do this, show that if we subtract corresponding sides of equation    from   and expand both sides,\n            we can obtain the linear equation\n             \n            in the unknowns  ,  , and  .\n           \n            Repeat the process in part (b),\n            subtracting   from   and show that we can obtain the linear equation\n             \n            in  ,  , and  .\n           \n            We have reduced our system of three non-linear equations to the system\n             \n            of two linear equations in the unknowns  ,  ,\n            and  .\n            Use technology to find a pseudoinverse of the coefficient matrix of this system.\n            Use the pseudoinverse to find the least squares solution to this system.\n            Does your solution correspond to an approximate point of intersection of the three circles?\n           \n     \n    provides the basic idea behind GPS. Suppose you receive a signal from a GPS satellite.\n    The transmission from satellite   provides four pieces of information   a location\n      and a time stamp   according to the satellite's atomic clock.\n    The time stamp allows the calculation of the distance between you and the  th satellite.\n    The transmission travel time is calculated by subtracting the current time on the GPS receiver from the satellite's time stamp.\n    Distance is then found by multiplying the transmission travel time by the rate,\n    which is the speed of light   km\/s. \n    The signals travel in radio waves,\n    which are electromagnetic waves,\n    and travel at the speed of light.\n    Also,   is the speed of light in a vacuum,\n    but atmosphere is not too dense so we assume this value of  \n      So distance is found as  ,\n    where   is the time at the receiver.\n    This signal places your location within in a sphere of that radius from the center of the satellite.\n    If you receive a signal at the same time from two satellites,\n    then your position is at the intersection of two spheres.\n    As can be seen at left in  ,\n    that intersection is a circle.\n    So your position has been narrowed quite a bit.\n    Now if you receive simultaneous signals from three spheres,\n    your position is narrowed to the intersection of three spheres,\n    or two points as shown at right in  .\n    So if we could receive perfect information from three satellites,\n    then your location would be exactly determined.\n   Intersections of spheres. pseudoranges \n    To ensure accuracy, the GPS uses signals from four satellites.\n    Assume a satellite is positioned at point\n      at a distance   from the GPS receiver located at point  .\n    The distance can also be measured in two ways: as\n     .\n    and as  .\n    So\n     .\n   \n    Again, we are treating   as an unknown,\n    so this equation has the four unknowns  ,\n     ,  , and  .\n    Using signals from four satellites produces the system of equations\n     .\n   \n      The system of equations  ,\n       ,  ,\n      and   is a non-linear system and is difficult to solve,\n      if it even has a solution.\n      We want a method that will provide at least an approximate solution as well as apply if we use more than four satellites.\n      We choose a reference node (say\n       ) and make calculations relative to that node as we did in  .\n     \n            First square both sides of the equations  ,\n             ,  ,\n            and   to remove the roots.\n            Then subtract corresponding sides of the new first equation (involving\n             ) from the new second equation  (involving\n             ) to show that we can obtain the linear equation\n             ,\n            where  .\n            (Note that the unknowns are  ,  ,  ,\n            and     all other quantities are known.)\n           \n            Use the result of part (a) to write a linear system that can be obtained by subtracting the first equation from the third and fourth equations as well.\n           \n            The linearizations from part (b) determine a system   of linear equations.\n            Identify  ,  , and  .\n            Then explain how we can approximate a best solution to this system in the least squares sense.\n           \n    We conclude this project with a final note.\n    At times a GPS receiver may only be able to receive signals from three satellites.\n    In these situations,\n    the receiver can substitute the surface of the Earth as a fourth sphere and continue the computation.\n   "
},
{
  "id": "objectives-30",
  "level": "2",
  "url": "chap_pseudoinverses.html#objectives-30",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            What is the condition number of a matrix and what does it tell us about the matrix?\n           \n         \n           \n            What is the pseudoinverse of a matrix?\n           \n         \n           \n            Why are pseudoinverses useful?\n           \n         \n           \n            How does the pseudoinverse of a matrix allow us to find least squares solutions to linear systems?\n           \n         "
},
{
  "id": "pa_7_d_1",
  "level": "2",
  "url": "chap_pseudoinverses.html#pa_7_d_1",
  "type": "Preview Activity",
  "number": "30.1",
  "title": "",
  "body": "\n      Let  .\n      A singular value decomposition for   is  , where\n       .\n       \n            Write the summands in the corresponding outer product decomposition of  .\n           \n            The outer product decomposition of   writes   as a sum of rank 1 matrices (the summands  .\n            Each summand contains some information about the matrix  .\n            Since   is the largest of the singular values,\n            it is reasonable to expect that the summand\n              contains the most information about   among all of the summands.\n            To get a measure of how much information   contains of  ,\n            we can think of   as simply a long vector in   where we have folded the data into a rectangular array\n            (we will see later why taking the norm as the norm of the vector in   makes sense,\n            but for now, just use this definition).\n            If we are interested in determining the error in approximating an image by a compressed image,\n            it makes sense to use the standard norm in   to determine length and distance,\n            which is really just the Frobenius norm that comes from the Frobenius inner product defined by\n             ,\n            where   and\n              are   matrices.\n            (That   defines an inner product on the set of all\n              matrices is left to discuss in a later section.)\n            So in this section all the norms for matrices will refer to the Frobenius norm.\n            Rather than computing the distance between   and   to measure the error,\n            we are more interested in the relative error\n             .\n           \n                  Calculate the relative error in approximating   by  .\n                  What does this tell us about how much information   contains about  ?\n                 \n                  Let  .\n                  Calculate the relative error in approximating   by  .\n                  What does this tell us about how much information   contains about  ?\n                 \n                  Let  .\n                  Calculate the relative error in approximating   by  .\n                  What does this tell us about how much information   contains about  ?\n                 \n                  Let  .\n                  Calculate the relative error in approximating   by  .\n                  What does this tell us about how much information   contains about  ?\n                  Why?\n                 "
},
{
  "id": "Fig_7_c_Flower",
  "level": "2",
  "url": "chap_pseudoinverses.html#Fig_7_c_Flower",
  "type": "Figure",
  "number": "30.1",
  "title": "",
  "body": "A 16 by 16 pixel image "
},
{
  "id": "F_7_c_Compress",
  "level": "2",
  "url": "chap_pseudoinverses.html#F_7_c_Compress",
  "type": "Figure",
  "number": "30.2",
  "title": "",
  "body": "A 16 by 16 pixel image and a compressed image using a singular value decomposition. "
},
{
  "id": "p-5117",
  "level": "2",
  "url": "chap_pseudoinverses.html#p-5117",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Frobenius "
},
{
  "id": "p-5118",
  "level": "2",
  "url": "chap_pseudoinverses.html#p-5118",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Frobenius product "
},
{
  "id": "activity-112",
  "level": "2",
  "url": "chap_pseudoinverses.html#activity-112",
  "type": "Activity",
  "number": "30.2",
  "title": "",
  "body": "\n      Verify   that  .\n     "
},
{
  "id": "T_7_c_Errors",
  "level": "2",
  "url": "chap_pseudoinverses.html#T_7_c_Errors",
  "type": "Table",
  "number": "30.3",
  "title": "Errors in approximating <span class=\"process-math\">\\(M\\)<\/span> by <span class=\"process-math\">\\(M_k\\)<\/span>",
  "body": "Errors in approximating   by  10 9 8 7 6 0.0070 0.0146 0.0252 0.0413 0.06426 5 4 3 2 1 0.0918 0.1231 0.1590 0.201 0.2460 "
},
{
  "id": "activity-113",
  "level": "2",
  "url": "chap_pseudoinverses.html#activity-113",
  "type": "Activity",
  "number": "30.3",
  "title": "",
  "body": "\n      Let   represent the flower image.\n     \n            Find the relative errors in approximating   by   and  .\n            You may use the fact that  .\n           \n            About how much of the information in the image is contained in the rank 1 approximation?\n            Explain.\n           "
},
{
  "id": "act_7_c_cond_num",
  "level": "2",
  "url": "chap_pseudoinverses.html#act_7_c_cond_num",
  "type": "Activity",
  "number": "30.4",
  "title": "",
  "body": "\n      Find the solution to each of the systems.\n     \n             \n           \n             \n           "
},
{
  "id": "definition-67",
  "level": "2",
  "url": "chap_pseudoinverses.html#definition-67",
  "type": "Definition",
  "number": "30.4",
  "title": "",
  "body": "ill-conditioned "
},
{
  "id": "p-5139",
  "level": "2",
  "url": "chap_pseudoinverses.html#p-5139",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "well-conditioned "
},
{
  "id": "definition-68",
  "level": "2",
  "url": "chap_pseudoinverses.html#definition-68",
  "type": "Definition",
  "number": "30.5",
  "title": "",
  "body": "condition number "
},
{
  "id": "activity-115",
  "level": "2",
  "url": "chap_pseudoinverses.html#activity-115",
  "type": "Activity",
  "number": "30.5",
  "title": "",
  "body": "\n      Let  .\n      A computer algebra system gives the singular values of   as 2.00025003124999934 and 0.000249968750000509660.\n      What is the condition number of  .\n      What does that tell us about  ?\n      Does this seem reasonable given the result of  ?\n     "
},
{
  "id": "activity-116",
  "level": "2",
  "url": "chap_pseudoinverses.html#activity-116",
  "type": "Activity",
  "number": "30.6",
  "title": "",
  "body": "\n          What is the smallest the condition number of a matrix can be?\n          Find an entire class of matrices with this smallest condition number.\n         \n          What is the condition number of an orthogonal matrix?\n          Why does this make sense?\n          \n         \n          If   is an orthogonal matrix,\n          what is   for any vector  ?\n          What does this make  ?\n         \n          What is the condition number of an invertible symmetric matrix in terms of its eigenvalues?\n         \n          Why do we not define the condition number of a non-invertible matrix?\n          If we did, what would the condition number have to be?\n          Why?\n         "
},
{
  "id": "pa_7_d_2",
  "level": "2",
  "url": "chap_pseudoinverses.html#pa_7_d_2",
  "type": "Preview Activity",
  "number": "30.7",
  "title": "",
  "body": "\n      Let  .\n      The singular value decomposition of   is   where\n       .\n     \n            Explain why   is not an invertible matrix.\n           \n            Explain why the matrices   and   are invertible.\n            How are   and   related to   and  ?\n           \n            Recall that one property of invertible matrices is that the inverse of a product of invertible matrices is the product of the inverses in the reverse order.\n            If   were invertible,\n            then   would be  .\n            Even though   and   are invertible,\n            the matrix   is not.\n            But   does contain non-zero eigenvalues that have reciprocals,\n            so consider the matrix  .\n            Calculate the products   and  .\n            How are the results similar to that obtained with a matrix inverse?\n           \n            The only matrix in the singular value decomposition of   that is not invertible is  .\n            But the matrix   acts somewhat like an inverse of  ,\n            so let us define   as  .\n            Now we explore a few properties of the matrix  .\n           \n                  Calculate   and   for  .\n                  What do you notice?\n                 \n                  Calculate   and   for  .\n                  What do you notice?\n                 "
},
{
  "id": "p-5173",
  "level": "2",
  "url": "chap_pseudoinverses.html#p-5173",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "pseudoinverse "
},
{
  "id": "activity-117",
  "level": "2",
  "url": "chap_pseudoinverses.html#activity-117",
  "type": "Activity",
  "number": "30.8",
  "title": "",
  "body": "\n          Find the pseudoinverse   of  .\n          Use the singular value decomposition   of  , where\n           .\n         \n          The vector   is not in  .\n          The vector   is an approximation to a solution of  ,\n          and   is in  .\n          Find   and determine how far   is from  .\n         "
},
{
  "id": "act_7_d_Moore-Penrose",
  "level": "2",
  "url": "chap_pseudoinverses.html#act_7_d_Moore-Penrose",
  "type": "Activity",
  "number": "30.9",
  "title": "",
  "body": "\n      Let   be an   matrix with singular value decomposition  .\n      Let   be defined as in  .\n     \n            Show that  .\n           \n            Show that  .\n           "
},
{
  "id": "p-5180",
  "level": "2",
  "url": "chap_pseudoinverses.html#p-5180",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Penrose Moore-Penrose "
},
{
  "id": "thm_7_d_pseudoinverse",
  "level": "2",
  "url": "chap_pseudoinverses.html#thm_7_d_pseudoinverse",
  "type": "Theorem",
  "number": "30.6",
  "title": "The Moore-Penrose Conditions.",
  "body": "The Moore-Penrose Conditions \n        A pseudoinverse of a matrix   is a matrix   that satisfies the following properties.\n         \n             \n               \n             \n           \n             \n               \n             \n           \n             \n               \n             \n           \n             \n               \n             \n           \n       "
},
{
  "id": "act_7_b_lin_indep_cols",
  "level": "2",
  "url": "chap_pseudoinverses.html#act_7_b_lin_indep_cols",
  "type": "Activity",
  "number": "30.10",
  "title": "",
  "body": "\n      Having to calculate eigenvalues and eigenvectors for a matrix to produce a singular value decomposition to find pseudoinverse can be computationally intense.\n      As we demonstrate in this activity,\n      the process is easier if the columns of   are linearly independent.\n      More specifically, we will prove the following theorem.\n     \n          If the columns of a matrix   are linearly independent,\n          then  .\n         \n      To see how, suppose that   is an\n        matrix with linearly independent columns.\n     \n            Given that the columns of   are linearly independent,\n            what must be the relationship between   and  ?\n           \n            Since the columns of   are linearly independent,\n            it follows that   is invertible\n            (see  ).\n            So the eigenvalues of   are all non-zero.\n            Let  ,  ,  ,\n              be the singular values of  .\n            How is   related to  ,\n            and what do   and   look like?\n           \n            Let us now investigate the form of the invertible matrix  \n            (note that neither   nor   is necessarily invertible).\n            If a singular value decomposition of   is  ,\n            show that\n             .\n           \n            Let   for   from 1 to  .\n            It is straightforward to see that   is an\n              diagonal matrix  , where\n             .\n            Then  .\n            Recall that  ,\n            so to relate   to   we need a product that is equal to  .\n            Explain why\n             .\n           \n            Complete the activity by showing that\n             .\n           "
},
{
  "id": "example-62",
  "level": "2",
  "url": "chap_pseudoinverses.html#example-62",
  "type": "Example",
  "number": "30.8",
  "title": "",
  "body": "\n        Let\n         .\n       \n        The eigenvalues of   are\n         ,  ,\n        and   with corresponding eigenvectors\n         .\n       \n        In addition,\n         .\n       \n              Find orthogonal matrices   and  ,\n              and the matrix  ,\n              so that   is a singular value decomposition of  .\n             \n              Normalizing the eigenvectors  ,  ,\n              and   to normal eigenvectors  ,  ,\n              and  , respectively,\n              gives us an orthogonal matrix\n               .\n              Now  ,\n              so normalizing the vectors   and   gives us vectors\n               \n              that are the first two columns of our matrix  .\n              Given that   is a   matrix,\n              we need to find two other vectors orthogonal to   and   that will combine with   and   to form an orthogonal basis for  .\n              Letting  ,\n               ,\n               ,\n              and  ,\n              a computer algebra system shows that the reduced row echelon form of the matrix   is  ,\n              so that vectors  ,\n               ,\n               ,   are linearly independent.\n              Letting   and  ,\n              the Gram-Schmidt process shows that the set\n                is an orthogonal basis for  ,\n              where   and\n              (using   for  )\n               .\n              The set   where  ,\n               ,\n                and\n                is an orthonormal basis for   and we can let\n               .\n              The singular values of   are\n                and  , and so\n               .\n              Therefore, a singular value decomposition of   is   of\n               .\n             \n              Determine the best rank 1 approximation to  .\n              Give an appropriate numerical estimate as to how good this approximation is to  .\n             \n              The outer product decomposition of   is\n               .\n              So the rank one approximation to   is\n               .\n              The error in approximating   with this rank one approximation is\n               .\n             \n              Find the pseudoinverse   of  .\n             \n              Given that  ,\n              we use the pseudoinverse   of   to find the pseudoinverse   of   by\n               .\n              Now\n               ,\n              so\n               .\n             \n              Let  .\n              Does the matrix equation\n               \n              have a solution?\n              If so, find the solution.\n              If not, find the best approximation you can to a solution to this matrix equation.\n             \n              Augmenting   with   and row reducing shows that\n               ,\n              so   is not in   and the equation   has no solution.\n              However, the best approximation to a solution to\n                is found using the pseudoinverse   of  .\n              That best solution is\n               .\n             \n              Use the orthogonal basis   of   to find the projection of   onto  .\n              Compare to your solution in part (c).\n             \n              The rank of   is 2 and an orthonormal basis for   is  ,\n              where   and  .\n              So\n               \n              as expected from part (c).\n             "
},
{
  "id": "example-63",
  "level": "2",
  "url": "chap_pseudoinverses.html#example-63",
  "type": "Example",
  "number": "30.9",
  "title": "",
  "body": "\n         \n        shows the per capita debt in the U.S. in from 2014 to 2019\n        (source  statistica.com ).\n       U.S. per capita debt Year 2014 2015 2016 2017 2018 2019 Debt 55905 56513 60505 62174 65697 69064 \n            Set up a linear system of the form\n              whose least squares solution provides a linear fit to the data.\n           \n            A linear approximation   to the system would satisfy the equation  ,\n              where  ,\n               ,\n              and  .\n           \n            Use technology to approximate a singular value decomposition\n            (round to four decimal places).\n            Use this svd to approximate the pseudoinverse of  .\n            Then use this pseudoinverse to approximate the least squares linear approximation to the system.\n           \n            Technology shows that a singular value decomposition of   is approximately  , where\n               .\n              Thus, with  ,\n              we have that the pseudoinverse of   is approximately\n               .\n              So our least squares linear approximation is found by\n               .\n              This makes our least squares linear approximation to be (to four decimal places)\n               .\n           \n            Calculate   directly and compare to the pseudoinverse you found in part (b).\n           \n            Calculating   gives the same matrix as  ,\n              so we obtain the same linear approximation.\n           \n            Use your approximation to estimate the U.S. per capita debt in 2020.\n           \n            The approximate U.S. per capita debt in 2020 is\n               .\n           "
},
{
  "id": "exercise-299",
  "level": "2",
  "url": "chap_pseudoinverses.html#exercise-299",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Let  .\n        Then   has singular value decomposition   , where\n         .\n       \n              What are the singular values of  ?\n             \n               ,  , and  \n             \n              Write the outer product decomposition of  .\n             \n               .\n             \n              Find the best rank 1 approximation to  .\n              What is the relative error in approximating   by this rank 1 matrix?\n             \n               ,\n               .\n             \n              Find the best rank 2 approximation to  .\n              What is the relative error in approximating   by this rank 2 matrix?\n             \n               ,\n               .\n             "
},
{
  "id": "exercise-300",
  "level": "2",
  "url": "chap_pseudoinverses.html#exercise-300",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Let  .\n       \n              Find a singular value decomposition for  .\n             \n              What are the singular values of  ?\n             \n              Write the outer product decomposition of  .\n             \n              Find the best rank 1, 2, and 3 approximations to  .\n              How much information about   does each of these approximations contain?\n             "
},
{
  "id": "exercise-301",
  "level": "2",
  "url": "chap_pseudoinverses.html#exercise-301",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Assume that the number of feet traveled by a batted baseball at various angles in degrees\n        (all hit at the same bat speed)\n        is given in  .\n       Distance traveled by batted ball Angle Distance 116 190 254 285 270 230 \n              Plot the data and explain why a quadratic function is likely a better fit to the data than a linear function.\n             \n              The ball goes up and then it comes down.\n             \n              Find the least squares quadratic approximation to this data.\n              Plot the quadratic function on same axes as your data.\n             \n               .\n             \n              At what angle\n              (or angles),\n              to the nearest degree,\n              must a player bat the ball in order for the ball to travel a distance of 220 feet?\n             \n              Approximately   and   degrees.\n             "
},
{
  "id": "exercise-302",
  "level": "2",
  "url": "chap_pseudoinverses.html#exercise-302",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        How close can a matrix be to being non-invertible?\n        We explore that idea in this exercise.\n        Let   be the\n          upper triangular matrix with 1s along the diagonal and with every other entry being  .\n       \n              What is  ?\n              What are the eigenvalues of  ?\n              Is   invertible?\n             \n              Let   be the\n                matrix so that   and\n                for all other   and  .\n             \n                    For the matrix   with  ,\n                    show that the equation   has a non-trivial solution.\n                    Find one non-trivial solution.\n                   \n                    For the matrix   with  ,\n                    show that the equation   has a non-trivial solution.\n                    Find one non-trivial solution.\n                   \n                    Use the pattern established in parts (i.) and (ii.) to find a non-trivial solution to the equation\n                      for an arbitrary value of  .\n                    Be sure to verify that you have a solution.\n                    Is   invertible?\n                    \n                   \n                    For any positive integer  ,\n                    the sum   is the partial sum of a geometric series with ratio   and so  .\n                   \n                    Explain why   is not an invertible matrix.\n                    Notice that   and   differ by a single entry,\n                    and that   is invertible and   is not.\n                    Let us examine how close   is to  .\n                    Calculate  ?\n                    What happens to   as   goes to infinity?\n                    How close can an invertible matrix be to becoming non-invertible?\n                   "
},
{
  "id": "exercise-303",
  "level": "2",
  "url": "chap_pseudoinverses.html#exercise-303",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Let  .\n        In this exercise we find a matrix   so that  ,\n        that is, find a square root of the matrix  .\n       \n              Find the eigenvalues and corresponding eigenvectors for   and  .\n              Explain what you see.\n             \n              The eigenvalues for   are 2, 1, and 0, the eigenvalues for\n                are 4, 1, and 0 with the same corresponding eigenvectors.\n             \n              Find a matrix   that orthogonally diagonalizes  .\n             \n               \n             \n               \n              in  \n              shows if   is a singular value decomposition for a symmetric matrix  ,\n              then so is  .\n              Recall that   for any positive integer  .\n              We can exploit this idea to define   to be the matrix\n               ,\n              where   is the matrix whose diagonal entries are the square roots of the corresponding entries of  .\n              Let  .\n              Calculate   and show that  .\n             \n               \n             \n              Why was it important that   be a symmetric matrix for this process to work,\n              and what had to be true about the eigenvalues of   for this to work?\n             \n                was symmetric to write a singular value decomposition for   in the form  ,\n                is positive definite\n             \n              Can you extend the process in this exercise to find a cube root of  ?\n             \n               \n             "
},
{
  "id": "exercise-304",
  "level": "2",
  "url": "chap_pseudoinverses.html#exercise-304",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Let   be an   matrix with singular value decomposition  .\n        Let   be defined as in  .\n        In this exercise we prove the remaining parts of  .\n       \n              Show that  .\n              \n             \n                is a symmetric matrix.\n             \n              Show that  .\n             "
},
{
  "id": "exercise-305",
  "level": "2",
  "url": "chap_pseudoinverses.html#exercise-305",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        In this exercise we show that the pseudoinverse of a matrix is the unique matrix that satisfies the Moore-Penrose conditions.\n        Let   be an   matrix with singular value decomposition\n          and pseudoinverse  .\n        To show that   is the unique matrix that satisfies the Moore-Penrose conditions,\n        suppose that there is another matrix   that also satisfies the Moore-Penrose conditions.\n       \n              Show that  .\n             \n              Use the fact that  .\n             \n              Show that  .\n             \n              Use the fact that  .\n             \n              How do the results of parts (a) and (b) show that   is the unique matrix satisfying the Moore-Penrose conditions?\n             \n              Compare the results of (a) and (b).\n             "
},
{
  "id": "exercise-306",
  "level": "2",
  "url": "chap_pseudoinverses.html#exercise-306",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Find the pseudoinverse of the\n          zero matrix  .\n        Explain the conclusion.\n       "
},
{
  "id": "exercise-307",
  "level": "2",
  "url": "chap_pseudoinverses.html#exercise-307",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        In all of the examples that we have done finding a singular value decomposition of a matrix,\n        it has been the case\n        (though we haven't mentioned it),\n        that if   is an   matrix,\n        then  .\n        Prove this result.\n       \n        Use the Rank-Nullity Theorem.\n       "
},
{
  "id": "exercise-308",
  "level": "2",
  "url": "chap_pseudoinverses.html#exercise-308",
  "type": "Exercise",
  "number": "10",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              A matrix has a pseudo-inverse if and only if the matrix is singular.\n             \n              F\n             True\/False \n              The pseudoinverse of an invertible matrix   is the matrix  .\n             True\/False \n              If the columns of   are linearly dependent,\n              then there is no least squares solution to  .\n             \n              F\n             True\/False \n              If the columns of   are linearly independent,\n              then there is a unique least squares solution to  .\n             True\/False \n              If   is the matrix transformation defined by a matrix   and   is the matrix transformation defined by  ,\n              then   and   are inverse transformations.\n             \n              F\n             "
},
{
  "id": "act_GPS_plane_ex",
  "level": "2",
  "url": "chap_pseudoinverses.html#act_GPS_plane_ex",
  "type": "Project Activity",
  "number": "30.11",
  "title": "",
  "body": "\n      Suppose that there are three base stations  ,  ,\n      and   in   that can send and receive signals from your mobile phone.\n      Assume that   is located at point  ,\n        at point  ,\n      and   at point  .\n      Also assume that your mobile phone location is point  .\n      Based on the time that it takes to receive the signals from the three base stations,\n      it can be determined that your distance to base station   is   km,\n      your distance to base station   is   km,\n      and your distance to base station   is   km using a coordinate system with measurements in kilometers based on a reference point chosen to be  .\n      Due to limitations on the measurement equipment,\n      these measurements all contain some unknown error which we will denote as  .\n      The goal is to determine your location in   based on this information.\n     \n      If the distance readings were accurate,\n      then the point   would lie on the circle centered at   of radius  .\n      The distance from   to base station   can be represented in two different ways:\n        km and  .\n      However, there is some error in the measurements\n      (due to the receiver clock and satellite clocks not being snychronized),\n      so we really have\n       ,\n      where   is the error.\n      Similarly,   must also satisfy\n       \n      and\n       .\n       \n            Explain how these three equations can be written in the equivalent form\n             .\n\n             Intersections of circles. \n           \n            If all measurements were accurate,\n            your position would be at the intersection of the circles centered at   with radius   km,\n            centered at   with radius   km,\n            and centered at   with radius   km as shown in  .\n            Even though the figure might seem to imply it,\n            because of the error in the measurements the three circles do not intersect in one point.\n            So instead, we want to find the best estimate of a point of intersection that we can.\n            The system of  equations ,\n             ,\n            and  \n            is non-linear and can be difficult to solve,\n            if it even has a solution.\n            To approximate a solution, we can linearize the system.\n            To do this, show that if we subtract corresponding sides of equation    from   and expand both sides,\n            we can obtain the linear equation\n             \n            in the unknowns  ,  , and  .\n           \n            Repeat the process in part (b),\n            subtracting   from   and show that we can obtain the linear equation\n             \n            in  ,  , and  .\n           \n            We have reduced our system of three non-linear equations to the system\n             \n            of two linear equations in the unknowns  ,  ,\n            and  .\n            Use technology to find a pseudoinverse of the coefficient matrix of this system.\n            Use the pseudoinverse to find the least squares solution to this system.\n            Does your solution correspond to an approximate point of intersection of the three circles?\n           "
},
{
  "id": "F_Spheres",
  "level": "2",
  "url": "chap_pseudoinverses.html#F_Spheres",
  "type": "Figure",
  "number": "30.13",
  "title": "",
  "body": "Intersections of spheres. "
},
{
  "id": "p-5298",
  "level": "2",
  "url": "chap_pseudoinverses.html#p-5298",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "pseudoranges "
},
{
  "id": "act_GPS_3D",
  "level": "2",
  "url": "chap_pseudoinverses.html#act_GPS_3D",
  "type": "Project Activity",
  "number": "30.12",
  "title": "",
  "body": "\n      The system of equations  ,\n       ,  ,\n      and   is a non-linear system and is difficult to solve,\n      if it even has a solution.\n      We want a method that will provide at least an approximate solution as well as apply if we use more than four satellites.\n      We choose a reference node (say\n       ) and make calculations relative to that node as we did in  .\n     \n            First square both sides of the equations  ,\n             ,  ,\n            and   to remove the roots.\n            Then subtract corresponding sides of the new first equation (involving\n             ) from the new second equation  (involving\n             ) to show that we can obtain the linear equation\n             ,\n            where  .\n            (Note that the unknowns are  ,  ,  ,\n            and     all other quantities are known.)\n           \n            Use the result of part (a) to write a linear system that can be obtained by subtracting the first equation from the third and fourth equations as well.\n           \n            The linearizations from part (b) determine a system   of linear equations.\n            Identify  ,  , and  .\n            Then explain how we can approximate a best solution to this system in the least squares sense.\n           "
},
{
  "id": "chap_vector_spaces",
  "level": "1",
  "url": "chap_vector_spaces.html",
  "type": "Section",
  "number": "31",
  "title": "Vector Spaces",
  "body": "Vector Spaces \n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is a vector space?\n           \n         \n           \n            What is a subspace of a vector space?\n           \n         \n           \n            What is a linear combination of vectors in a vector space  ?\n           \n         \n           \n            What is the span of a set of vectors in a vector space  ?\n           \n         \n           \n            What special structure does the span of a set of vectors in a vector space   have?\n           \n         \n           \n            Why is the vector space concept important?\n           \n         Application: The Hat Puzzle \n    In a New York Times article (April 10, 2001)\n     Why Mathematicians Now Care About Their Hat Color , the following puzzle is posed.\n   Three players enter a room and a red or blue hat is placed on each person's head. The color of each hat is determined by a coin toss, with the outcome of one coin toss having no effect on the others. Each person can see the other players' hats but not his own.\n    No communication of any sort is allowed, except for an initial strategy session before the game begins. Once they have had a chance to look at the other hats, the players must simultaneously guess the color of their own hats or pass. The group shares a hypothetical $3 million prize if at least one player guesses correctly and no players guess incorrectly. \n    The game can be played with more players,\n    and the problem is to find a strategy for the group that maximizes its chance of winning.\n    One strategy is for a designated player to make a random guess and for the others to pass.\n    This gives a 50% chance of winning.\n    However, there are much better strategies that provide a nearly 100% probability of winning as the number of players increases.\n    One such strategy is based on Hamming codes and subspaces of a particular vector space to implement the most effective approach.\n   Introduction vector space \n    An example of a set that has a structure similar to vectors is a collection of polynomials.\n    Let   be the collection of all polynomials of degree less than or equal to 1 with real coefficients.\n    That is,\n     .\n   \n    So, for example, the polynomials  ,  ,\n     ,\n    and   are in  ,\n    but   is not in  .\n    Two polynomials   and\n      in   are equal if   and  .\n   \n    We define addition of polynomials in   by adding the coefficients of the like degree terms.\n    So if   and  ,\n    then the polynomial sum of   and   is\n     .\n   \n    So, for example,\n     .\n   \n    We now consider the properties of the addition operation.\n    For example, we can ask if polynomial addition is commutative.\n    That is, if   and   are in  ,\n    must it be the case that\n     \n   \n    To show that addition is commutative in  ,\n    we choose arbitrary polynomials\n      and   in  .\n    Then we have\n     .\n   \n    Note that in the middle step,\n    we used the definition of equality of polynomials since   and\n      due to the fact that addition of real numbers is commutative.\n    So addition of elements in   is a commutative operation.\n   \n            Now we investigate other properties of addition in  .\n           \n                  To show addition is associative in  ,\n                  we need to verify that if  ,  ,\n                  and   are in  , it must be the case that\n                   .\n                  Either verify this property by using the definition of two polynomials being equal,\n                  or give a counterexample to show the equality fails in that case.\n                 \n                  Find a polynomial   such that\n                   \n                  for all  .\n                  This polynomial is called the  zero \n                  polynomial or the  additive identity \n                  polynomial in  .\n                 \n                  If   is an element of  ,\n                  is there an element   such that\n                   ,\n                  where   is the additive identity polynomial you found above?\n                  If not, why not?\n                  If so, what polynomial is  ?\n                  Explain.\n                 \n            We can also define a multiplication of polynomials by scalars\n            (real numbers).\n           \n                  What element in   could be the scalar multiple  ?\n                 \n                  In general, if   is a scalar and   is in  ,\n                  how do we define the scalar multiple   in  ?\n                 \n                  If   is a scalar and   and\n                    are elements in  ,\n                  is it true that\n                   \n                  If no, explain why.\n                  If yes, verify your answer using the definition of two polynomials being equal.\n                 \n                  If   and   are scalars and\n                    is an element in  ,\n                  is it true that\n                   \n                  If no, explain why.\n                  If yes, verify your answer.\n                 \n                  If   and   are scalars and\n                    is an element in  ,\n                  is it true that\n                   \n                  If no, explain why.\n                  If yes, verify your answer.\n                 \n                  If   is an element of  ,\n                  is it true that\n                   \n                  If no, explain why.\n                  If yes, verify your answer.\n                 Spaces with Similar Structure to  vector spaces \n    In  ,\n    we showed that the set   of polynomials of degree less than or equal to one with real coefficients,\n    with the operations of addition and scalar multiplication defined by\n     ,\n    has a structure similar to  .\n   \n    By structure we mean how the elements in the set relate to each other under addition and multiplication by scalars.\n    That is, if  ,  ,\n    and   are elements of   and   and   are scalars, then\n     \n         \n            is an element of  ,\n         \n       \n         \n           ,\n         \n       \n         \n           ,\n         \n       \n         \n          there is a zero polynomial   (namely,\n           ) in   so that  ,\n         \n       \n         \n          there is an element   in   (namely,\n           ) so that  ,\n         \n       \n         \n            is an element of  ,\n         \n       \n         \n           ,\n         \n       \n         \n           ,\n         \n       \n         \n           ,\n         \n       \n         \n           .\n         \n       \n   \n    The properties we saw for polynomials in   stated above are the same as the properties for vector addition and multiplication by scalars in  ,\n    as well as matrix addition and multiplication by scalars identified in  .\n    This indicates that polynomials in  , vectors in  ,\n    and the set of   matrices behave in much the same way as regards their addition and multiplication by scalars.\n    There is an even closer connection between linear polynomials and vectors in  .\n    An element   in   can be naturally associated with the vector   in  .\n    All the results of polynomial addition and multiplication by scalars then translate to corresponding results of addition and multiplication by scalars of vectors in  .\n    So for all intents and purposes,\n    as far as addition and multiplication by scalars is concerned,\n    there is no difference between elements in   and vectors in     the only difference is how we choose to present the elements\n    (as polynomials or as vectors).\n    This sameness of structure of our sets as it relates to addition and multiplication by scalars is the type of similarity mentioned in the introduction.\n    We can study all of the types of objects that exhibit this same structure at one time by studying vector spaces.\n   Vector Spaces \n    We defined vector spaces in the context of subspaces of   in  .\n    In general, any set that has the same kind of additive and multiplicative structure as our sets of vectors,\n    matrices, and linear polynomials is called a vector space.\n    As we will see,\n    the ideas that we introduced about subspaces of   apply to vector spaces in general,\n    so the material in this chapter should have a familiar feel.\n   vector space vector space closed commutative associative additive identity additive inverse closed multiplication by scalars distributes over scalar addition multiplication by scalars distributes over addition in  scalars Note \n      Unless otherwise stated, in this book the scalars\n          \n      will refer to real numbers.\n      However, we can define vector spaces where scalars are complex numbers,\n      or rational numbers,\n      or integers modulo   where   is a prime number, or,\n      more generally, elements of a field.\n      A field is an algebraic structure which generalizes the structure of real numbers and rational numbers under the addition and multiplication operations.\n      Since we will focus on the real numbers as scalars,\n      the reader is not required to be familiar with the concept of a field.\n     vectors \n              The space   of all vectors with   components is a vector space using the standard vector addition and multiplication by scalars.\n              The zero element is the zero vector   whose components are all 0.\n             \n              The set   of all polynomials of degree less than or equal to 1 with addition and scalar multiplication as defined earlier.\n              Recall that   is essentially the same as  .\n             \n              The properties listed in the introduction for   are equally true for the collection of all polynomials of degree less than or equal to some fixed number.\n              We label as   this set of all polynomials of degree less than or equal to  ,\n              with the standard addition and scalar multiplication.\n              Note that   is essentially the same as  .\n              More generally,\n              the space   of all polynomials is also a vector space with standard addition and scalar multiplication.\n             \n              As a subspace of  ,\n              the eigenspace of an   matrix corresponding to an eigenvalue   is a vector space.\n             \n              As a subspace of  ,\n              the null space of an   matrix is a vector space.\n             \n              As a subspace of  ,\n              the column space of an   matrix is a vector space.\n             \n              The span of a set of vectors in   is a subspace of  ,\n              and is therefore a vector space.\n             \n              Let   be a vector space and let   be the additive identity in  .\n              The set   is a vector space in which   and\n                for any scalar  .\n              This space is called the  trivial  vector space.\n             \n              The space   (or\n                when it is important to indicate that the entries of our matrices are real numbers) of all\n                matrices with real entries with the standard addition and multiplication by scalars we have already defined.\n              In this case,\n                is essentially the same vector space as  .\n             \n              The space   of all functions from   to  ,\n              where we define the sum of two functions   and   in   as the function   satisfying\n               \n              for all real numbers  ,\n              and the scalar multiple   of the function   by the scalar   to be the function satisfying\n               \n              for all real numbers  .\n              The verification of the vector space properties for this space is left to the reader.\n             \n              The space   of all infinite real sequences  .\n              We define addition and scalar multiplication termwise:\n               ,\n               ,\n              is a vector space.\n              In addition, the set of convergent sequences inside\n                forms a vector space using this addition and multiplication by scalars\n              (as we did in  ,\n              we will call this set of convergent sequences a subspace of  ).\n             \n              (For those readers who are familiar with differential equations).\n              The set of solutions to a second order homogeneous differential equation forms a vector space under addition and scalar multiplication defined as in the space   above.\n             \n              The set of polynomials of positive degree in   is not a vector space using the standard addition and multiplication by scalars in   is not a vector space.\n              Notice that   is not a polynomial of positive degree,\n              and so this set is not closed under addition.\n             \n              The color space where each color is assigned an RGB (red, green,\n              blue) coordinate between 0 and 255,\n              with addition and scalar multiplication defined component-wise, however,\n              does not define a vector space.\n              The color space is not closed under either operation due to the color coordinates being integers ranging from 0 to 255.\n             \n    It is important to note that the set of defining properties of a vector space is intended to be a minimum set.\n    Any other properties of a vector space must be verified or proved using the defining properties.\n    For example,\n    in   it is clear that the scalar multiple   is the zero vector for any vector   in  .\n    This might be true in any vector space,\n    but it is not a defining property.\n    Therefore, if this property is true,\n    then we must be able to prove it using just the defining properties.\n    To see how this might work,\n    let   be any vector in a vector space  .\n    We want to show that   (the existence of the zero vector is property (4)).\n    Using the fact that   and that scalar multiplication distributes over scalar addition,\n    we can see that\n     .\n   \n    Property (5) tells us that   contains an additive inverse for every vector in  ,\n    so let   be an additive inverse of the vector   in  .\n    Then  \n    It is very important to keep track of the different kinds of zeros here   the boldface zero   is the additive identity in the vector space and the non-bold 0 is the scalar zero.\n      and so\n     .\n   \n    Now   has the property that\n      for any vector   in   (by properties (4) and (2)),\n    and so we can conclude that\n     .\n   \n      Another property that will be useful is a cancellation property.\n      In the set of real numbers we know that if  ,\n      then  ,\n      and we verify this by subtracting   from both sides.\n      This is the same as adding the additive inverse of   to both sides,\n      so we ought to be able to make the same argument using additive inverses in a vector space.\n      To see how, let  ,  ,\n      and   be vectors in a vector space and suppose that\n       .\n     \n            Why does our space contain an additive inverse   of  ?\n           \n            Now add the vector   to both sides of equation   to obtain\n             .\n            Which property of a vector space allows us to state the following equality?\n             .\n           \n            Now use the properties of additive inverses and the additive identity to explain why  .\n            Conclude that we have a cancellation law for addition in any vector space.\n           \n    We should also note that the definition of a vector space only states the existence of a zero vector and an additive inverse for each vector in the space,\n    and does not say that there cannot be more than one zero vector or more than one additive inverse of a vector in the space.\n    The reason why is that the uniqueness of the zero vector and an additive inverse of a vector can be proved from the defining properties of a vector space,\n    and so we don't list this consequence as a defining property.\n    Similarly, the defining properties of a vector space do not state that the additive inverse of a vector   is the scalar multiple  .\n    Verification of these properties are left for the exercises.\n    We summarize the results of this section in the following theorem.\n   \n        Let   be any vector space with identity  .\n         \n             \n                for any vector   in  .\n             \n           \n             \n              The vector   is unique.\n             \n           \n             \n                for any scalar  .\n             \n           \n             \n              For any   in  ,\n              the additive inverse of   is unique.\n             \n           \n             \n              The additive inverse of a vector   in   is the vector  .\n             \n           \n             \n              If  ,  ,\n              and   are in   and\n               , then  .\n             \n           \n       Subspaces \n      In  \n      we saw that   contained subsets that we called subspaces that had the same algebraic structure as  .\n      The same idea applies to vector spaces in general.\n     \n        Let  .\n        Notice that   is a subset of  .\n       \n              Is   closed under the addition in  ?\n              Verify your answer.\n             \n              Does   contain the zero vector from  ?\n              Verify your answer.\n             \n              Is   closed under multiplication by scalars?\n              Verify your answer.\n             \n              Explain why   satisfies every other property of the definition of a vector space automatically just by being a subset of   and using the same operations as in  .\n              Conclude that   is a vector space.\n             subspace vector space subspace subspace of a vector space subspace closed closed \n        Is the given subset   a subspace of the indicated vector space  ?\n        Verify your answer.\n       \n                is any vector space and  \n             \n               ,\n              the vector space of   matrices and  .\n             \n               ,\n              the vector space of all polynomials of degree less than or equal to 2 and  .\n             \n                and  .\n             \n                and  .\n             \n      There is an interesting subspace relationship between the spaces   and  .\n      For every  ,   is a subspace of  .\n      Furthermore,   is a subspace of  ,\n        is a subspace of  , and so on.\n      Note however that a similar relationship does NOT hold for  ,\n      even though   looks like  .\n      For example,   is NOT a subspace of  .\n      Similarly,   is NOT a subspace of  .\n      Since the vectors in different  's are of different sizes,\n      none of the  's is a subset of another   with  ,\n      and hence,\n        is not a subspace of   when  .\n     The Subspace Spanned by a Set of Vectors \n      In   we showed that the span of any set of vectors forms a subspace of  .\n      The same is true in any vector space.\n      Recall that the span of a set of vectors in   is the set of all linear combinations of those vectors.\n      So before we can discuss the span of a set of vectors in a vector space,\n      we need to extend the definition of linear combinations to vector spaces\n      (compare to  \n      and  ).\n     linear combination span \n      The argument that the span of any finite set of vectors in a vector space forms a subspace is the same as we gave for the span of a set of vectors in  \n      (see  ).\n      The proof is left for the exercises.\n     \n          Given a vector space   and vectors  ,\n           ,  ,   in  ,\n            is a subspace of  .\n         subspace of   spanned by\n       \n              Let  .\n              Note that   is a subset of  .\n              Find two vectors   in   so that\n                and hence conclude that   is a subspace of  .\n              (Note that the vectors   are not unique.)\n             \n              Let   and  ,\n              and let   in  .\n              Is the polynomial   in\n               ? (Hint: Create a matrix equation of the form\n                by setting up an appropriate polynomial equation involving  ,\n                and  .\n              Under what conditions on   is the system   consistent?)\n             \n              With   as in part (b),\n              describe as best you can the subspace   of  .\n             spanning set Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Determine if each of the following sets is a vector space.\n       \n                with addition and multiplication by scalars defined by\n               ,\n              where   and   are in   and  \n             \n              We consider the vector space properties in  .\n              Let  ,  ,\n              and   be in   and let  .\n              By the definition of addition and multiplication by scalars,\n              both   and   are in  .\n              Note also that\n               ,\n              and so addition is commutative in  .\n              Since\n               \n              and\n               ,\n              we see that addition is not associative and conclude that   is not a vector space.\n              At this point we can stop since we have shown that   is not a vector space.\n             \n                with addition   and multiplication by scalars defined by\n               ,\n              where   and   are in  ,  ,\n              and   is the standard product of   and  \n             \n              We consider the vector space properties in  .\n              Let  ,  ,\n              and   be in   and let  .\n              Since   and   are both positive real numbers,\n              we know that   is a positive real number.\n              Thus,   and   is closed under its addition.\n              Also,   is a positive real number,\n              so   as well.\n              Now\n               \n              and addition is commutative in  .\n              Also,\n               \n              and addition is associative in  .\n              Since\n               ,\n                contains an additive identity, which is  .\n              The fact that   is a positive real number implies that\n                is a positive real number.\n              Thus,   and\n               \n              and   contains an additive inverse for each of its elements.\n              We have that\n               .\n              So   satisfies all of the properties of a vector space.\n             \n              The set   of all   matrices of the form\n                where   and   are real numbers using the standard addition and multiplication by scalars on matrices.\n             \n              Recall that   is a vector space using the standard addition and multiplication by scalars on matrices.\n              Any matrix of the form    can be written as\n               .\n              So   and   is a subspace of  .\n              Thus,   is a vector space.\n             \n              The set   of all functions   from   to   such that\n                using the standard addition and multiplication by scalars on functions.\n             \n              We will show that   is not a vector space.\n              Let   be defined by  .\n              Then   and  .\n              However, if  ,\n              then   and  .\n              It follows that   is not closed under multiplication by scalars and   is not a vector space.\n             \n        Let   be a vector space and   and   vectors in  .\n        Also, let   and   be scalars.\n        You may use the result of  \n        that   for any scalar   in any vector space.\n       \n              If   and  , must  ?\n              Use the properties of a vector space or provide a counterexample to justify your answer.\n             \n              We will show that this statement is true.\n              Suppose   and  .\n              Then  .\n              If  , then we are done.\n              So suppose  .\n              Then   and   is a real number.\n              Then\n               .\n              But we assumed that  ,\n              so we can conclude that   as desired.\n             \n              If   and\n               , must  ?\n              Use the properties of a vector space or provide a counterexample to justify your answer.\n             \n              We will show that this statement is true.\n              Suppose   and  .\n              Then  .\n              Since  , we know that   is a real number.\n              Thus,\n               .\n             \n              If  , must   and  ?\n              Use the properties of a vector space or provide a counterexample to justify your answer.\n             \n              We will demonstrate that this statement is false with a counterexample.\n              Let  ,  ,\n                and   in  .\n              Then\n               ,\n              but   and  .\n             Summary \n       \n        A set   on which an operation of addition and a multiplication by scalars is defined is a vector space if for all  ,\n         ,\n        and   in   and all scalars   and  :\n         \n             \n                is an element of  ,\n             \n           \n             \n               ,\n             \n           \n             \n               ,\n             \n           \n             \n              there is a zero vector   in   so that  ,\n             \n           \n             \n              for each   in   there is an element   in   so that  ,\n             \n           \n             \n                is an element of  ,\n             \n           \n             \n               ,\n             \n           \n             \n               ,\n             \n           \n             \n               ,\n             \n           \n             \n               .\n             \n           \n       \n     \n       \n        A subset   of a vector space   is a subspace of   if\n         \n             \n              whenever   and   are in   it is also true that   is in  ,\n             \n           \n             \n              whenever   is in   and   is a scalar it is also true that   is in  ,\n             \n           \n             \n                is in  .\n             \n           \n       \n     \n       \n        A linear combination of vectors  ,\n         ,  ,\n          in a vector space   is a vector of the form\n         ,\n        where  ,  ,\n         ,   are scalars.\n       \n     \n       \n        The span of the vectors  ,\n         ,  ,\n          in a vector space   is the collection of all linear combinations of  ,\n         ,  ,  .\n        That is,\n         .\n       \n     \n       \n        The span of any finite set of vectors in a vector space   is always a subspace of  .\n       \n     \n       \n        This concept of vector space is important because there are many different types of sets (e.g.,  ,\n         ,  ,\n         ) that have similar structure,\n        and we can relate them all as members of this larger collection of vector spaces.\n       \n     \n        The definition of a vector space only states the existence of a zero vector and does not say how many zero vectors the space might have.\n        In this exercise we show that the zero vector in a vector space is unique.\n        To show that the zero vector is unique,\n        we assume that two vectors   and\n          have the zero vector property.\n       \n              Using the fact that   is a zero vector,\n              what vector is  ?\n             \n              Use the fact that   for any vector   in our vector space.\n             \n              Using the fact that   is a zero vector,\n              what vector is  ?\n             \n              Same reasoning as in part (a).\n             \n              How do we conclude that the zero vector is unique?\n             \n              Use the transitive property of equality.\n             \n        The definition of a vector space only states the existence of an additive inverse for each vector in the space,\n        but does not say how many additive inverses a vector can have.\n        In this exercise we show that the additive inverse of a vector in a vector space is unique.\n        To show that a vector   has only one additive inverse,\n        we suppose that   has two additive inverses,\n          and  , and demonstrate that  .\n       \n              What equations must   and   satisfy if   and   are additive inverses of  ?\n             \n              Use the equations from part (a) to show that  .\n              Clearly identify all vector space properties you use in your argument.\n             \n        Let   be a vector space and   a vector in  .\n        In all of the vector spaces we have seen to date,\n        the additive inverse of the vector   is equal to the scalar multiple  .\n        This seems reasonable,\n        but it is important to note that this result is not stated in the definition of a vector space,\n        so this it is something that we need to verify.\n        To show that   is an additive inverse of the vector  ,\n        we need to demonstrate that\n         .\n        Verify this equation,\n        explicitly stating which properties you use at each step.\n       \n        Use the fact that  .\n       \n        It is reasonable to expect that if   is any scalar and   is the zero vector in a vector space  ,\n        then  .\n        Use the fact that   to prove this statement.\n       \n        Let   be two subspaces of a vector space  .\n        Determine whether   and\n          are subspaces of  .\n        Justify each answer clearly.\n       \n        The intersection   is a subspace of  ,\n        but the union   is not in general a subspace of  .\n       \n        Find three vectors   to express\n         \n        as  .\n        How does this justify why   is a subspace of  ?\n       \n        Find three vectors   to express\n         \n        as  .\n        How does this justify why   is a subspace of  ?\n       \n        The space   is the span of  ,\n         ,\n        and  .\n       \n        Let   be the set of all functions from   to  ,\n        where we define the sum of two functions   and   in   as the function   satisfying\n         \n        for all real numbers  ,\n        and the scalar multiple   of the function   by the scalar   to be the function satisfying\n         \n        for all real numbers  .\n        Show that   is a vector space using these operations.\n       \n        Prove  .\n        \n       \n        Mimic the proof of  .\n       \n        Determine if each of the following sets of elements is a vector space or not.\n        If appropriate,\n        you can identify a set as a subspace of another vector space,\n        or as a span of a collection of vectors to shorten your solution.\n       \n              A line through the origin in  .\n             \n              The first quadrant in  .\n             \n              The set of vectors  .\n             \n              The set of all differentiable functions from   to  .\n             \n              The set of all functions from   to   which are increasing for every  . (Assume that a function   is increasing if\n                whenever  .)\n             \n              The set of all functions   from   to   for which   for some fixed   in  .\n             \n              The set of polynomials of the form  , where  .\n             \n              The set of all upper triangular   real matrices.\n             \n              The set of complex numbers   where scalar multiplication is defined as multiplication by real numbers.\n             \n        A reasonable way to extend the idea of the vector space   to infinity is to let\n          be the set of all sequences of real numbers.\n        Define addition and multiplication by scalars on   by\n         \n        where   denotes the sequence  ,\n          denotes the sequence\n          and   is a scalar.\n       \n              Show that   is a vector space using these operations.\n             \n              Closure is by definition,\n              the sequence   is the additive identity,\n              the sequence   is the additive inverse of the sequence  .\n              The other properties follow from the definitions of addition and multiplication by    scalars.\n             \n              Is the set of sequences that have infinitely many zeros a subspace of  ?\n              Verify your answer.\n             \n              The answer is no.\n             eventually zero \n              The answer is yes.\n             decreasing \n              The answer is no.\n             \n              Is the set of sequences in\n                that have limits at infinity a subspace of  ?\n             \n              The answer is yes.\n             Hilbert space \n              To show that   is closed under addition, expand the square.\n             subspace sum \n        Given two subspaces   of a vector space  , define\n         .\n        Show that   is a subspace of   containing both   as subspaces.\n        The space   is the sum\n    \n        of the subspaces   and  .\n       \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              The intersection of any two subspaces of   is also a subspace.\n             \n              T\n             True\/False \n              The union of any two subspaces of   is also a subspace.\n             True\/False \n              If   is a subspace of a vector space  ,\n              then   is equal to  .\n             \n              T\n             True\/False \n              If   is a nonzero vector in  ,\n              a subspace of  ,\n              then   contains the line through the origin and   in  .\n             True\/False \n              If   are nonzero,\n              non-parallel vectors in  ,\n              a subspace of  ,\n              then   contains the plane through the origin,\n                and   in  .\n             \n              T\n             True\/False \n              The smallest subspace in   containing a vector   is a line through the origin.\n             True\/False \n              The largest subspace of   is  .\n             \n              T\n             True\/False \n              The space   is a subspace of   for  .\n             True\/False \n              The set of constant functions from   to   is a subspace of  .\n             \n              T\n             True\/False \n              The set of all polynomial functions with rational coefficients is a subspace of  .\n             Project: Hamming Codes and the Hat Puzzle \n    Recall the hat problem from the beginning of this section.\n    Three players are assigned either a red or blue hat and can only see the colors of the hats of the other players.\n    The goal is to devise a high probability strategy for one player to correctly guess the color of their hat.\n    The players have a 50% chance of winning if one player guesses randomly and all of the others pass.\n    However, the group can do better than 50% with a reasonably simple strategy.\n    There are 2 possibilities for each hat color for a total of   possible distributions of hat colors.\n    Of these, only red-red-red and blue-blue-blue contain only one hat color,\n    so   of   of the possible hat distributions have two hats of one color and one of the other color.\n    So if a player sees two hats of the same color,\n    that player guesses the other color and passes otherwise.\n    This gives a 75% chance of winning.\n    This strategy will only work for three players, though.\n    We want to develop an effective strategy that works for larger groups of players.\n   Hamming codes \n    Note that as  ,\n    this probability has a limit of 1.\n    Note also that if  \n    (so that there are 3 players),\n    then the probability is   or     the same strategy we came up with earlier.\n   \n    To understand this strategy,\n    we need to build a slightly different kind of vector space than we have seen until now,\n    one that is based on a binary choice of red or blue.\n    To do so, we identify the hat colors with numbers   0 for red and 1 for blue.\n    So let  .\n    Assume there are   players for some integer  .\n    We can then view a distribution of hats among the\n      players as a vector with   components from  .\n    That is,\n     .\n   \n    We can give some structure to both   and\n      by noting that we can define addition and multiplication in   by\n     .\n   \n      Show that   has the same structure as  .\n      That is, show that for all  ,  ,\n      and   in  ,\n      the following properties are satisfied.\n     \n              and  \n           \n              and  \n           \n              and  \n           \n            There is an element   in\n              such that  \n           \n            There is an element   in\n              such that  \n           \n            There is an element   in\n              such that  \n           \n            If  , there is an element   in\n              such that  \n           \n             \n           \n     \n    shows that   has the same properties as     that is that   is a field.\n    Until now, we have worked with vector spaces whose scalars come from the set of real numbers,\n    but that is not necessary.\n    None of the results we have discovered so far about vector spaces require our scalars to come from  .\n    In fact, we can replace   with any field and all of the same vector space properties hold.\n    It follows that   is a vector space over  .\n    As we did in  ,\n    we define the standard unit vectors  ,\n     ,  ,\n      in  .\n   \n    Now we return to the hat puzzle.\n    We have   players.\n    Label the players  ,  ,  ,  .\n    We can now represent a random placements of hats on heads as a vector\n      in  ,\n    where   in the  th entry represents a red hat and\n      a blue hat on player  .\n    Since player   can see all of the other hats,\n    from player  's perspective the distribution of hats has the form\n     ,\n    where   is the unknown color of hat on player  's head and\n     .\n   \n    In order to analyze the vectors   from player  's perspective and to devise an effective strategy,\n    we will partition the set   into an appropriate disjoint union of subsets.\n   \n    To provide a different way to look at players,\n    we will use a subspace of  .\n    Let   be a subspace of   that has a basis of   vectors.\n    The elements of   are the linear combinations of   basis vectors,\n    and each basis vector in a linear combination has 2 possibilities for its weight\n    (from  ).\n    Thus,   contains exactly   vectors.\n    We can then use the   nonzero vectors in   to represent our players.\n    Each distribution of hats can be seen as a linear combination of the vectors in  .\n    Let  ,  ,  ,\n      be the nonzero vectors in  .\n    We then define a function   as\n     \n    that identifies a distribution of hats with a vector in  .\n    The subspace that we need to devise our strategy is what is called a Hamming code.\n   Hamming code \n      Let\n       .\n     Hamming code \n    Now for each   between 0 and   we define   as\n     ,\n    where we let  .\n    The sets   are called  cosets  of  .\n   \n      To complete our strategy for the hat puzzle,\n      we need to know some additional information about the sets  .\n     \n            Show that the sets   are disjoint.\n            That is, show that   if  .\n            \n           \n            If   and  ,\n            what can we say about  ?\n           \n            Since   for each  ,\n            it follows that  .\n            Now we show that   by demonstrating that\n              has exactly the same number of elements as  .\n            We will need one fact for our argument.\n            We will see in a later section that   has a basis of   elements,\n            so the number of elements in   is  .\n           \n                  Since the sets   are disjoint,\n                  the number of elements in   is equal to the sum of the number of elements in each  .\n                  Show that each   has the same number of elements as  .\n                 \n                  Now use the fact that the number of elements in\n                    is equal to the sum of the number of elements in each   to argue that  .\n                 \n    The useful idea from  \n    is that any hat distribution in   is in exactly one of the sets  .\n    Recall that a hat distribution\n      in   can be written from player  's perspective as\n     ,\n    where  .\n    Our strategy for the hat game can now be revealed.\n     \n         \n          If   is not in   for either choice of  ,\n          then player   should pass.\n         \n       \n         \n          If   is in  ,\n          then player   guesses  .\n         \n       \n   \n      Let us analyze this strategy.\n     \n            Explain why every player guesses wrong if   is in  .\n           \n            Now we see determine that our strategy is a winning strategy for all hat distributions   that are not in  .\n            First we need to know that these two options are the only ones.\n            That is, show that it is not possible for\n              to be in   for both choices of  .\n           \n            Now we want to demonstrate that this is a winning strategy if  .\n            That is, at least one player guesses a correct hat color and no one else guesses incorrectly.\n            So assume  .\n           \n                  We know that   for some unique choice of  ,\n                  so let   for some  .\n                  Explain why player   can correctly choose color  .\n                 \n                  Finally, we need to argue that every player except player   must pass.\n                  So consider player  , with  .\n                  Recall that\n                   .\n                  Analyze our strategy and the conditions under which player   does not pass.\n                  Show that this leads to a contradiction.\n                 \n     \n    completes our analysis of this strategy and shows that our strategy results in a win with probability\n     .\n   "
},
{
  "id": "objectives-31",
  "level": "2",
  "url": "chap_vector_spaces.html#objectives-31",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is a vector space?\n           \n         \n           \n            What is a subspace of a vector space?\n           \n         \n           \n            What is a linear combination of vectors in a vector space  ?\n           \n         \n           \n            What is the span of a set of vectors in a vector space  ?\n           \n         \n           \n            What special structure does the span of a set of vectors in a vector space   have?\n           \n         \n           \n            Why is the vector space concept important?\n           \n         "
},
{
  "id": "p-5315",
  "level": "2",
  "url": "chap_vector_spaces.html#p-5315",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "vector space "
},
{
  "id": "pa_5_a",
  "level": "2",
  "url": "chap_vector_spaces.html#pa_5_a",
  "type": "Preview Activity",
  "number": "31.1",
  "title": "",
  "body": "\n            Now we investigate other properties of addition in  .\n           \n                  To show addition is associative in  ,\n                  we need to verify that if  ,  ,\n                  and   are in  , it must be the case that\n                   .\n                  Either verify this property by using the definition of two polynomials being equal,\n                  or give a counterexample to show the equality fails in that case.\n                 \n                  Find a polynomial   such that\n                   \n                  for all  .\n                  This polynomial is called the  zero \n                  polynomial or the  additive identity \n                  polynomial in  .\n                 \n                  If   is an element of  ,\n                  is there an element   such that\n                   ,\n                  where   is the additive identity polynomial you found above?\n                  If not, why not?\n                  If so, what polynomial is  ?\n                  Explain.\n                 \n            We can also define a multiplication of polynomials by scalars\n            (real numbers).\n           \n                  What element in   could be the scalar multiple  ?\n                 \n                  In general, if   is a scalar and   is in  ,\n                  how do we define the scalar multiple   in  ?\n                 \n                  If   is a scalar and   and\n                    are elements in  ,\n                  is it true that\n                   \n                  If no, explain why.\n                  If yes, verify your answer using the definition of two polynomials being equal.\n                 \n                  If   and   are scalars and\n                    is an element in  ,\n                  is it true that\n                   \n                  If no, explain why.\n                  If yes, verify your answer.\n                 \n                  If   and   are scalars and\n                    is an element in  ,\n                  is it true that\n                   \n                  If no, explain why.\n                  If yes, verify your answer.\n                 \n                  If   is an element of  ,\n                  is it true that\n                   \n                  If no, explain why.\n                  If yes, verify your answer.\n                 "
},
{
  "id": "p-5334",
  "level": "2",
  "url": "chap_vector_spaces.html#p-5334",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "vector spaces "
},
{
  "id": "def_vector_space",
  "level": "2",
  "url": "chap_vector_spaces.html#def_vector_space",
  "type": "Definition",
  "number": "31.1",
  "title": "",
  "body": "vector space vector space closed commutative associative additive identity additive inverse closed multiplication by scalars distributes over scalar addition multiplication by scalars distributes over addition in  "
},
{
  "id": "p-5361",
  "level": "2",
  "url": "chap_vector_spaces.html#p-5361",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "vectors "
},
{
  "id": "example-64",
  "level": "2",
  "url": "chap_vector_spaces.html#example-64",
  "type": "Example",
  "number": "31.2",
  "title": "",
  "body": "\n              The space   of all vectors with   components is a vector space using the standard vector addition and multiplication by scalars.\n              The zero element is the zero vector   whose components are all 0.\n             \n              The set   of all polynomials of degree less than or equal to 1 with addition and scalar multiplication as defined earlier.\n              Recall that   is essentially the same as  .\n             \n              The properties listed in the introduction for   are equally true for the collection of all polynomials of degree less than or equal to some fixed number.\n              We label as   this set of all polynomials of degree less than or equal to  ,\n              with the standard addition and scalar multiplication.\n              Note that   is essentially the same as  .\n              More generally,\n              the space   of all polynomials is also a vector space with standard addition and scalar multiplication.\n             \n              As a subspace of  ,\n              the eigenspace of an   matrix corresponding to an eigenvalue   is a vector space.\n             \n              As a subspace of  ,\n              the null space of an   matrix is a vector space.\n             \n              As a subspace of  ,\n              the column space of an   matrix is a vector space.\n             \n              The span of a set of vectors in   is a subspace of  ,\n              and is therefore a vector space.\n             \n              Let   be a vector space and let   be the additive identity in  .\n              The set   is a vector space in which   and\n                for any scalar  .\n              This space is called the  trivial  vector space.\n             \n              The space   (or\n                when it is important to indicate that the entries of our matrices are real numbers) of all\n                matrices with real entries with the standard addition and multiplication by scalars we have already defined.\n              In this case,\n                is essentially the same vector space as  .\n             \n              The space   of all functions from   to  ,\n              where we define the sum of two functions   and   in   as the function   satisfying\n               \n              for all real numbers  ,\n              and the scalar multiple   of the function   by the scalar   to be the function satisfying\n               \n              for all real numbers  .\n              The verification of the vector space properties for this space is left to the reader.\n             \n              The space   of all infinite real sequences  .\n              We define addition and scalar multiplication termwise:\n               ,\n               ,\n              is a vector space.\n              In addition, the set of convergent sequences inside\n                forms a vector space using this addition and multiplication by scalars\n              (as we did in  ,\n              we will call this set of convergent sequences a subspace of  ).\n             \n              (For those readers who are familiar with differential equations).\n              The set of solutions to a second order homogeneous differential equation forms a vector space under addition and scalar multiplication defined as in the space   above.\n             \n              The set of polynomials of positive degree in   is not a vector space using the standard addition and multiplication by scalars in   is not a vector space.\n              Notice that   is not a polynomial of positive degree,\n              and so this set is not closed under addition.\n             \n              The color space where each color is assigned an RGB (red, green,\n              blue) coordinate between 0 and 255,\n              with addition and scalar multiplication defined component-wise, however,\n              does not define a vector space.\n              The color space is not closed under either operation due to the color coordinates being integers ranging from 0 to 255.\n             "
},
{
  "id": "act_5_a_1",
  "level": "2",
  "url": "chap_vector_spaces.html#act_5_a_1",
  "type": "Activity",
  "number": "31.2",
  "title": "",
  "body": "\n      Another property that will be useful is a cancellation property.\n      In the set of real numbers we know that if  ,\n      then  ,\n      and we verify this by subtracting   from both sides.\n      This is the same as adding the additive inverse of   to both sides,\n      so we ought to be able to make the same argument using additive inverses in a vector space.\n      To see how, let  ,  ,\n      and   be vectors in a vector space and suppose that\n       .\n     \n            Why does our space contain an additive inverse   of  ?\n           \n            Now add the vector   to both sides of equation   to obtain\n             .\n            Which property of a vector space allows us to state the following equality?\n             .\n           \n            Now use the properties of additive inverses and the additive identity to explain why  .\n            Conclude that we have a cancellation law for addition in any vector space.\n           "
},
{
  "id": "theorem-75",
  "level": "2",
  "url": "chap_vector_spaces.html#theorem-75",
  "type": "Theorem",
  "number": "31.3",
  "title": "",
  "body": "\n        Let   be any vector space with identity  .\n         \n             \n                for any vector   in  .\n             \n           \n             \n              The vector   is unique.\n             \n           \n             \n                for any scalar  .\n             \n           \n             \n              For any   in  ,\n              the additive inverse of   is unique.\n             \n           \n             \n              The additive inverse of a vector   in   is the vector  .\n             \n           \n             \n              If  ,  ,\n              and   are in   and\n               , then  .\n             \n           \n       "
},
{
  "id": "act_5_a_2",
  "level": "2",
  "url": "chap_vector_spaces.html#act_5_a_2",
  "type": "Activity",
  "number": "31.3",
  "title": "",
  "body": "\n        Let  .\n        Notice that   is a subset of  .\n       \n              Is   closed under the addition in  ?\n              Verify your answer.\n             \n              Does   contain the zero vector from  ?\n              Verify your answer.\n             \n              Is   closed under multiplication by scalars?\n              Verify your answer.\n             \n              Explain why   satisfies every other property of the definition of a vector space automatically just by being a subset of   and using the same operations as in  .\n              Conclude that   is a vector space.\n             "
},
{
  "id": "p-5397",
  "level": "2",
  "url": "chap_vector_spaces.html#p-5397",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "subspace "
},
{
  "id": "def_5_a_subspace",
  "level": "2",
  "url": "chap_vector_spaces.html#def_5_a_subspace",
  "type": "Definition",
  "number": "31.4",
  "title": "",
  "body": "vector space subspace subspace of a vector space subspace closed closed "
},
{
  "id": "act_5_a_3",
  "level": "2",
  "url": "chap_vector_spaces.html#act_5_a_3",
  "type": "Activity",
  "number": "31.4",
  "title": "",
  "body": "\n        Is the given subset   a subspace of the indicated vector space  ?\n        Verify your answer.\n       \n                is any vector space and  \n             \n               ,\n              the vector space of   matrices and  .\n             \n               ,\n              the vector space of all polynomials of degree less than or equal to 2 and  .\n             \n                and  .\n             \n                and  .\n             "
},
{
  "id": "definition-71",
  "level": "2",
  "url": "chap_vector_spaces.html#definition-71",
  "type": "Definition",
  "number": "31.5",
  "title": "",
  "body": "linear combination span "
},
{
  "id": "thm_VS_span",
  "level": "2",
  "url": "chap_vector_spaces.html#thm_VS_span",
  "type": "Theorem",
  "number": "31.6",
  "title": "",
  "body": "\n          Given a vector space   and vectors  ,\n           ,  ,   in  ,\n            is a subspace of  .\n         "
},
{
  "id": "p-5413",
  "level": "2",
  "url": "chap_vector_spaces.html#p-5413",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "subspace of   spanned by\n       "
},
{
  "id": "act_5_a_4",
  "level": "2",
  "url": "chap_vector_spaces.html#act_5_a_4",
  "type": "Activity",
  "number": "31.5",
  "title": "",
  "body": "\n              Let  .\n              Note that   is a subset of  .\n              Find two vectors   in   so that\n                and hence conclude that   is a subspace of  .\n              (Note that the vectors   are not unique.)\n             \n              Let   and  ,\n              and let   in  .\n              Is the polynomial   in\n               ? (Hint: Create a matrix equation of the form\n                by setting up an appropriate polynomial equation involving  ,\n                and  .\n              Under what conditions on   is the system   consistent?)\n             \n              With   as in part (b),\n              describe as best you can the subspace   of  .\n             "
},
{
  "id": "p-5417",
  "level": "2",
  "url": "chap_vector_spaces.html#p-5417",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "spanning set "
},
{
  "id": "example-65",
  "level": "2",
  "url": "chap_vector_spaces.html#example-65",
  "type": "Example",
  "number": "31.7",
  "title": "",
  "body": "\n        Determine if each of the following sets is a vector space.\n       \n                with addition and multiplication by scalars defined by\n               ,\n              where   and   are in   and  \n             \n              We consider the vector space properties in  .\n              Let  ,  ,\n              and   be in   and let  .\n              By the definition of addition and multiplication by scalars,\n              both   and   are in  .\n              Note also that\n               ,\n              and so addition is commutative in  .\n              Since\n               \n              and\n               ,\n              we see that addition is not associative and conclude that   is not a vector space.\n              At this point we can stop since we have shown that   is not a vector space.\n             \n                with addition   and multiplication by scalars defined by\n               ,\n              where   and   are in  ,  ,\n              and   is the standard product of   and  \n             \n              We consider the vector space properties in  .\n              Let  ,  ,\n              and   be in   and let  .\n              Since   and   are both positive real numbers,\n              we know that   is a positive real number.\n              Thus,   and   is closed under its addition.\n              Also,   is a positive real number,\n              so   as well.\n              Now\n               \n              and addition is commutative in  .\n              Also,\n               \n              and addition is associative in  .\n              Since\n               ,\n                contains an additive identity, which is  .\n              The fact that   is a positive real number implies that\n                is a positive real number.\n              Thus,   and\n               \n              and   contains an additive inverse for each of its elements.\n              We have that\n               .\n              So   satisfies all of the properties of a vector space.\n             \n              The set   of all   matrices of the form\n                where   and   are real numbers using the standard addition and multiplication by scalars on matrices.\n             \n              Recall that   is a vector space using the standard addition and multiplication by scalars on matrices.\n              Any matrix of the form    can be written as\n               .\n              So   and   is a subspace of  .\n              Thus,   is a vector space.\n             \n              The set   of all functions   from   to   such that\n                using the standard addition and multiplication by scalars on functions.\n             \n              We will show that   is not a vector space.\n              Let   be defined by  .\n              Then   and  .\n              However, if  ,\n              then   and  .\n              It follows that   is not closed under multiplication by scalars and   is not a vector space.\n             "
},
{
  "id": "example-66",
  "level": "2",
  "url": "chap_vector_spaces.html#example-66",
  "type": "Example",
  "number": "31.8",
  "title": "",
  "body": "\n        Let   be a vector space and   and   vectors in  .\n        Also, let   and   be scalars.\n        You may use the result of  \n        that   for any scalar   in any vector space.\n       \n              If   and  , must  ?\n              Use the properties of a vector space or provide a counterexample to justify your answer.\n             \n              We will show that this statement is true.\n              Suppose   and  .\n              Then  .\n              If  , then we are done.\n              So suppose  .\n              Then   and   is a real number.\n              Then\n               .\n              But we assumed that  ,\n              so we can conclude that   as desired.\n             \n              If   and\n               , must  ?\n              Use the properties of a vector space or provide a counterexample to justify your answer.\n             \n              We will show that this statement is true.\n              Suppose   and  .\n              Then  .\n              Since  , we know that   is a real number.\n              Thus,\n               .\n             \n              If  , must   and  ?\n              Use the properties of a vector space or provide a counterexample to justify your answer.\n             \n              We will demonstrate that this statement is false with a counterexample.\n              Let  ,  ,\n                and   in  .\n              Then\n               ,\n              but   and  .\n             "
},
{
  "id": "exercise-309",
  "level": "2",
  "url": "chap_vector_spaces.html#exercise-309",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        The definition of a vector space only states the existence of a zero vector and does not say how many zero vectors the space might have.\n        In this exercise we show that the zero vector in a vector space is unique.\n        To show that the zero vector is unique,\n        we assume that two vectors   and\n          have the zero vector property.\n       \n              Using the fact that   is a zero vector,\n              what vector is  ?\n             \n              Use the fact that   for any vector   in our vector space.\n             \n              Using the fact that   is a zero vector,\n              what vector is  ?\n             \n              Same reasoning as in part (a).\n             \n              How do we conclude that the zero vector is unique?\n             \n              Use the transitive property of equality.\n             "
},
{
  "id": "exercise-310",
  "level": "2",
  "url": "chap_vector_spaces.html#exercise-310",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        The definition of a vector space only states the existence of an additive inverse for each vector in the space,\n        but does not say how many additive inverses a vector can have.\n        In this exercise we show that the additive inverse of a vector in a vector space is unique.\n        To show that a vector   has only one additive inverse,\n        we suppose that   has two additive inverses,\n          and  , and demonstrate that  .\n       \n              What equations must   and   satisfy if   and   are additive inverses of  ?\n             \n              Use the equations from part (a) to show that  .\n              Clearly identify all vector space properties you use in your argument.\n             "
},
{
  "id": "exercise-311",
  "level": "2",
  "url": "chap_vector_spaces.html#exercise-311",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": ""
},
{
  "id": "exercise-312",
  "level": "2",
  "url": "chap_vector_spaces.html#exercise-312",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Let   be a vector space and   a vector in  .\n        In all of the vector spaces we have seen to date,\n        the additive inverse of the vector   is equal to the scalar multiple  .\n        This seems reasonable,\n        but it is important to note that this result is not stated in the definition of a vector space,\n        so this it is something that we need to verify.\n        To show that   is an additive inverse of the vector  ,\n        we need to demonstrate that\n         .\n        Verify this equation,\n        explicitly stating which properties you use at each step.\n       \n        Use the fact that  .\n       "
},
{
  "id": "ex_5_a_scalar_times_0",
  "level": "2",
  "url": "chap_vector_spaces.html#ex_5_a_scalar_times_0",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        It is reasonable to expect that if   is any scalar and   is the zero vector in a vector space  ,\n        then  .\n        Use the fact that   to prove this statement.\n       "
},
{
  "id": "exercise-314",
  "level": "2",
  "url": "chap_vector_spaces.html#exercise-314",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Let   be two subspaces of a vector space  .\n        Determine whether   and\n          are subspaces of  .\n        Justify each answer clearly.\n       \n        The intersection   is a subspace of  ,\n        but the union   is not in general a subspace of  .\n       "
},
{
  "id": "exercise-315",
  "level": "2",
  "url": "chap_vector_spaces.html#exercise-315",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        Find three vectors   to express\n         \n        as  .\n        How does this justify why   is a subspace of  ?\n       "
},
{
  "id": "exercise-316",
  "level": "2",
  "url": "chap_vector_spaces.html#exercise-316",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Find three vectors   to express\n         \n        as  .\n        How does this justify why   is a subspace of  ?\n       \n        The space   is the span of  ,\n         ,\n        and  .\n       "
},
{
  "id": "exercise-317",
  "level": "2",
  "url": "chap_vector_spaces.html#exercise-317",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        Let   be the set of all functions from   to  ,\n        where we define the sum of two functions   and   in   as the function   satisfying\n         \n        for all real numbers  ,\n        and the scalar multiple   of the function   by the scalar   to be the function satisfying\n         \n        for all real numbers  .\n        Show that   is a vector space using these operations.\n       "
},
{
  "id": "exercise-318",
  "level": "2",
  "url": "chap_vector_spaces.html#exercise-318",
  "type": "Exercise",
  "number": "10",
  "title": "",
  "body": "\n        Prove  .\n        \n       \n        Mimic the proof of  .\n       "
},
{
  "id": "exercise-319",
  "level": "2",
  "url": "chap_vector_spaces.html#exercise-319",
  "type": "Exercise",
  "number": "11",
  "title": "",
  "body": "\n        Determine if each of the following sets of elements is a vector space or not.\n        If appropriate,\n        you can identify a set as a subspace of another vector space,\n        or as a span of a collection of vectors to shorten your solution.\n       \n              A line through the origin in  .\n             \n              The first quadrant in  .\n             \n              The set of vectors  .\n             \n              The set of all differentiable functions from   to  .\n             \n              The set of all functions from   to   which are increasing for every  . (Assume that a function   is increasing if\n                whenever  .)\n             \n              The set of all functions   from   to   for which   for some fixed   in  .\n             \n              The set of polynomials of the form  , where  .\n             \n              The set of all upper triangular   real matrices.\n             \n              The set of complex numbers   where scalar multiplication is defined as multiplication by real numbers.\n             "
},
{
  "id": "exercise-320",
  "level": "2",
  "url": "chap_vector_spaces.html#exercise-320",
  "type": "Exercise",
  "number": "12",
  "title": "",
  "body": "\n        A reasonable way to extend the idea of the vector space   to infinity is to let\n          be the set of all sequences of real numbers.\n        Define addition and multiplication by scalars on   by\n         \n        where   denotes the sequence  ,\n          denotes the sequence\n          and   is a scalar.\n       \n              Show that   is a vector space using these operations.\n             \n              Closure is by definition,\n              the sequence   is the additive identity,\n              the sequence   is the additive inverse of the sequence  .\n              The other properties follow from the definitions of addition and multiplication by    scalars.\n             \n              Is the set of sequences that have infinitely many zeros a subspace of  ?\n              Verify your answer.\n             \n              The answer is no.\n             eventually zero \n              The answer is yes.\n             decreasing \n              The answer is no.\n             \n              Is the set of sequences in\n                that have limits at infinity a subspace of  ?\n             \n              The answer is yes.\n             Hilbert space \n              To show that   is closed under addition, expand the square.\n             "
},
{
  "id": "ex_5_a_sum",
  "level": "2",
  "url": "chap_vector_spaces.html#ex_5_a_sum",
  "type": "Exercise",
  "number": "13",
  "title": "",
  "body": "subspace sum \n        Given two subspaces   of a vector space  , define\n         .\n        Show that   is a subspace of   containing both   as subspaces.\n        The space   is the sum\n    \n        of the subspaces   and  .\n       "
},
{
  "id": "exercise-322",
  "level": "2",
  "url": "chap_vector_spaces.html#exercise-322",
  "type": "Exercise",
  "number": "14",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              The intersection of any two subspaces of   is also a subspace.\n             \n              T\n             True\/False \n              The union of any two subspaces of   is also a subspace.\n             True\/False \n              If   is a subspace of a vector space  ,\n              then   is equal to  .\n             \n              T\n             True\/False \n              If   is a nonzero vector in  ,\n              a subspace of  ,\n              then   contains the line through the origin and   in  .\n             True\/False \n              If   are nonzero,\n              non-parallel vectors in  ,\n              a subspace of  ,\n              then   contains the plane through the origin,\n                and   in  .\n             \n              T\n             True\/False \n              The smallest subspace in   containing a vector   is a line through the origin.\n             True\/False \n              The largest subspace of   is  .\n             \n              T\n             True\/False \n              The space   is a subspace of   for  .\n             True\/False \n              The set of constant functions from   to   is a subspace of  .\n             \n              T\n             True\/False \n              The set of all polynomial functions with rational coefficients is a subspace of  .\n             "
},
{
  "id": "p-5516",
  "level": "2",
  "url": "chap_vector_spaces.html#p-5516",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Hamming codes "
},
{
  "id": "act_hat_1",
  "level": "2",
  "url": "chap_vector_spaces.html#act_hat_1",
  "type": "Project Activity",
  "number": "31.6",
  "title": "",
  "body": "\n      Show that   has the same structure as  .\n      That is, show that for all  ,  ,\n      and   in  ,\n      the following properties are satisfied.\n     \n              and  \n           \n              and  \n           \n              and  \n           \n            There is an element   in\n              such that  \n           \n            There is an element   in\n              such that  \n           \n            There is an element   in\n              such that  \n           \n            If  , there is an element   in\n              such that  \n           \n             \n           "
},
{
  "id": "act_hat_2",
  "level": "2",
  "url": "chap_vector_spaces.html#act_hat_2",
  "type": "Project Activity",
  "number": "31.7",
  "title": "",
  "body": "Hamming code \n      Let\n       .\n     Hamming code "
},
{
  "id": "act_hat_3",
  "level": "2",
  "url": "chap_vector_spaces.html#act_hat_3",
  "type": "Project Activity",
  "number": "31.8",
  "title": "",
  "body": "\n      To complete our strategy for the hat puzzle,\n      we need to know some additional information about the sets  .\n     \n            Show that the sets   are disjoint.\n            That is, show that   if  .\n            \n           \n            If   and  ,\n            what can we say about  ?\n           \n            Since   for each  ,\n            it follows that  .\n            Now we show that   by demonstrating that\n              has exactly the same number of elements as  .\n            We will need one fact for our argument.\n            We will see in a later section that   has a basis of   elements,\n            so the number of elements in   is  .\n           \n                  Since the sets   are disjoint,\n                  the number of elements in   is equal to the sum of the number of elements in each  .\n                  Show that each   has the same number of elements as  .\n                 \n                  Now use the fact that the number of elements in\n                    is equal to the sum of the number of elements in each   to argue that  .\n                 "
},
{
  "id": "act_hat_4",
  "level": "2",
  "url": "chap_vector_spaces.html#act_hat_4",
  "type": "Project Activity",
  "number": "31.9",
  "title": "",
  "body": "\n      Let us analyze this strategy.\n     \n            Explain why every player guesses wrong if   is in  .\n           \n            Now we see determine that our strategy is a winning strategy for all hat distributions   that are not in  .\n            First we need to know that these two options are the only ones.\n            That is, show that it is not possible for\n              to be in   for both choices of  .\n           \n            Now we want to demonstrate that this is a winning strategy if  .\n            That is, at least one player guesses a correct hat color and no one else guesses incorrectly.\n            So assume  .\n           \n                  We know that   for some unique choice of  ,\n                  so let   for some  .\n                  Explain why player   can correctly choose color  .\n                 \n                  Finally, we need to argue that every player except player   must pass.\n                  So consider player  , with  .\n                  Recall that\n                   .\n                  Analyze our strategy and the conditions under which player   does not pass.\n                  Show that this leads to a contradiction.\n                 "
},
{
  "id": "chap_bases",
  "level": "1",
  "url": "chap_bases.html",
  "type": "Section",
  "number": "32",
  "title": "Bases for Vector Spaces",
  "body": "Bases for Vector Spaces \n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            What does it mean for a set\n              of vectors in a vector space   to be linearly independent?\n           \n         \n           \n            What is another equivalent characterization of a linearly independent set?\n           \n         \n           \n            What does is mean for a set\n              of vectors in a vector space   to be linearly dependent?\n           \n         \n           \n            Describe another characterization of a linearly dependent set.\n           \n         \n           \n            What is a basis for a vector space  ?\n           \n         \n           \n            What makes a basis for a vector space useful?\n           \n         \n           \n            How can we find a basis for a vector space  ?\n           \n         Application: Image Compression \n    If you painted a picture with a sky, clouds, trees, and flowers, you would use a different size brush depending on the size of the features. Wavelets are like those brushes.\n     Ingrid Daubechies \n    The advent of the digital age has presented many new opportunities for the collection,\n    analysis,\n    and dissemination of information.\n    Along with these opportunities come new difficulties as well.\n    All of this digital information must be stored in some way and be retrievable in an efficient manner.\n    One collection of tools that is used to deal with these problems is wavelets.\n    For example,\n    the FBI fingerprint files contain millions of cards,\n    each of which contains 10 rolled fingerprint impressions.\n    Each card produces about 10 megabytes of data.\n    To store all of these cards would require an enormous amount of space,\n    and transmitting one full card over existing data lines is slow and inefficient.\n    Without some sort of image compression,\n    a sortable and searchable electronic fingerprint database would be next to impossible.\n    To deal with this problem,\n    the FBI adopted standards for fingerprint digitization using a wavelet compression standard.\n   \n    Another problem with electronics is noise.\n    Noise can be a big problem when collecting and transmitting data.\n    Wavelet decomposition filters data by averaging and detailing.\n    The detailing coefficients indicate where the details are in the original data set.\n    If some details are very small in relation to others,\n    eliminating them may not substantially alter the original data set.\n    Similar ideas may be used to restore damaged audio, \n    see   for a discussion of the denoising of a Brahms recording\n      video, photographs,\n    and medical information. \n    A review of wavelets in biomedical applications.\n    M. Unser, A. Aldroubi.\n     Proceedings of the IEEE , Volume: 84, Issue: 4 , Apr 1996\n     \n   \n    We will consider wavelets as a tool for image compression.\n    The basic idea behind using wavelets to compress images is that we start with a digital image,\n    made up of pixels.\n    Each pixel can be assigned a number or a vector\n    (depending on the makeup of the image).\n    The image can then be represented as a matrix\n    (or a set of matrices)\n     , where each entry in   represents a pixel in the image.\n    As a simple example, consider the\n      image of a flower as shown at left in  .\n    (We will work with small images like this to make the calculations more manageable,\n    but the ideas work for any size image.\n    We could also extend our methods to consider color images,\n    but for the sake of simplicity we focus on grayscale.)\n   Left: A 16 by 16 pixel image. Right: The image compressed. \n    This flower image is a gray-scale image,\n    so each pixel has a numeric representation between 0 and 255,\n    where 0 is black, 255 is white,\n    and numbers between 0 and 255 represent shades of gray.\n    The matrix for this flower image is\n     .\n   \n    Now we can apply wavelets to the image and compress it.\n    Essentially, wavelets act by averaging and differencing.\n    The averaging creates smaller versions of the image and the differencing keeps track of how far the smaller version is from a previous copy.\n    The differencing often produces many small\n    (close to 0)\n    entries, and so replacing these entries with 0 doesn't have much effect on the image\n    (this is called  thresholding ).\n    By introducing long strings of zeros into our data,\n    we are able to store a (compressed) copy of the image in a smaller amount of space.\n    For example,\n    using a threshold value of 10 produces the flower image shown at right in  .\n   \n    The averaging and differencing is done with special vectors (wavelets) that form a basis for a suitable function space.\n    More details of this process can be found at the end of this section.\n   Introduction \n    In   we defined a basis for a subspace   of   to be a minimal spanning set for  ,\n    or a linearly independent spanning set\n    (see  ).\n    So to consider the idea of a basis in a vector space,\n    we will need the notion of linear independence in that context.\n   \n    Since we can add vectors and multiply vectors by scalars in any vector space,\n    and because we have a zero vector in any vector space,\n    we can define linear independence of a finite set of vectors in any vector space as follows\n    (compare to  ).\n   linearly independent vectors in a vector space linear dependent vectors in a vector space linearly independent linearly dependent \n    Alternatively, we say that the vectors\n      are linearly independent\n    (or dependent)\n    if the set   is linearly independent\n    (or dependent).\n   \n            We can use the tools we developed to determine if a set of vectors in   is linearly independent to answer the same questions for sets of vectors in other vector spaces.\n            For example, consider the question of whether the set\n              in   is linearly independent or dependent.\n            To answer this question we need to determine if there is a non-trivial solution to the equation\n             .\n            Note that equation   can also be written in the form\n             .\n           \n                  Recall that two polynomials are equal if all coefficients of like powers are the same.\n                  By equating coefficients of like power terms,\n                  rewrite equation   as an equivalent system of two equations in the two unknowns   and  ,\n                  and solve for  .\n                 \n                  What does your answer to the previous part tell you about the linear independence or dependence of the set   in  ?\n                 \n                  Recall that in  ,\n                  a set of two vectors is linearly dependent if and only if one of the vectors in the set is a scalar multiple of the other and linearly independent if neither vector is a scalar multiple of the other.\n                  Verify your answer to part (c) from a similar perspective in  .\n                 \n            We can use the same type of method as in problem (1) to address the question of whether the set\n             \n            is linearly independent or dependent in  .\n            To answer this question we need to determine if there is a non-trivial solution to the equation\n             \n            for some scalars  ,  , and  .\n            Note that the linear combination on the left side of equation   has entries\n             .\n           \n                  Recall that two matrices are equal if all corresponding entries are the same.\n                  Equate corresponding entries of the matrices in equation   to rewrite the equation as an equivalent system of four equations in the three unknowns  ,\n                   , and  .\n                 \n                  Use appropriate matrix tools and techniques to find all solutions to the system from part (a).\n                 \n                  What does the set of solutions to the system from part (a) tell you about the linear independence or dependence of the set\n                   \n                 \n                  Recall that in  ,\n                  a set of vectors is linearly dependent if and only if one of the vectors in the set is a linear combination of the others and linearly independent if no vector in the set is a linear combination of the others.\n                  Verify your answer to part (c) from a similar perspective in  .\n                 \n            We will define a basis for a vector space to be a linearly independent spanning set.\n            Which, if any,\n            of the sets in parts (1) and (2) is a basis for its vector space?\n            Explain.\n           Linear Independence \n    The concept of linear independence,\n    which we formally defined in  ,\n    provides us with a process to determine if there is redundancy in a spanning set to \n    obtain an efficient spanning set.\n   \n    The definition tells us that a set\n      of vectors in a vector space   is linearly \n    dependent if there are scalars  ,\n     ,\n     ,  , not all of which are 0 so that\n     .\n   \n    As examples,\n    we saw in  \n    that the set   is linearly independent in  .\n    The set  , on the other hand,\n    is linearly dependent in   since  .\n   \n    In addition to the definition,\n    there are other ways to characterize linearly independent and dependent sets in vector \n    spaces as the next theorems illustrate.\n    These characterizations are the same as those we saw in  ,\n    and the proofs are essentially the same as well.\n    The proof of  \n    is similar to that of  \n    and is left for the exercises.\n   \n        A set   of vectors in a vector space   is linearly dependent if and only if at least one of the vectors in the set can be written as a linear combination of the remaining vectors in the set.\n       \n     \n    is equivalent to the following theorem that provides the corresponding result for linearly independent sets.\n   \n        A set   of vectors in a vector space   is linearly independent if and only if no vector in the set can be written as a linear combination of the remaining vectors in the set.\n       \n    One consequence of  \n    and  \n    is that if a spanning set is linearly dependent,\n    then one of the vectors in the set can be written as a linear combination of the others.\n    In other words, at least one of the vectors is redundant.\n    In that case,\n    we can find a smaller spanning set as the next theorem states.\n    The proof of this theorem is similar to that of  \n    and is left for the exercises.\n   \n        Let   be a set of vectors in a vector space  .\n        If for some   between 1 and  ,\n          is in  , then\n         .\n       Bases \n    A basis for a vector space is a spanning set that is as small as it can be.\n    We already saw how to define bases formally in  .\n    We will now formally define a basis for a vector space and understand why with this definition a basis is a minimal spanning set.\n    Bases are important because any vector in a vector space can be uniquely represented as a linear combination of basis vectors.\n    We will see in later sections that this representation will allow us to identify any vector space with a basis of   vectors with  .\n   minimal basis basis for a vector space vector space basis basis \n    In other words,\n    a basis for a vector space   is a linearly independent spanning set for  .\n    To put it another way,\n    a basis for a vector space is a minimal spanning set for the vector space.\n    Similar reasoning will show that a basis is also a maximal linearly independent set.\n   \n    The key ideas to take from the previous theorems are:\n     \n         \n          A basis for a vector space   is a minimal spanning set for  .\n         \n       \n         \n          A basis for   is a subset   of   so that\n           \n               \n                  spans   and\n               \n             \n               \n                  is linearly independent.\n               \n             \n         \n       \n         \n          No vector in a basis can be written as a linear combination of the other vectors in the basis.\n         \n       \n         \n          If a subset   of a vector space   has the property that one of the vectors in   is a linear combination of the other vectors in  ,\n          then   is not a basis for  .\n         \n       \n   \n    As an example of a basis of a vector space,\n    we saw in  \n    that the set   is both linearly independent and spans  ,\n    and so   is a basis for  .\n   \n            Is   a basis for  ?\n            Explain.\n           standard basis for  standard basis \n            Show that the set\n             \n            is a basis for  .\n           \n    It should be noted that not every vector space has a finite basis.\n    For example,\n    the space   of all polynomials with real coefficients\n    (of any degree)\n    is a vector space,\n    but no finite set of vectors will span  .\n    In fact, the infinite set   is both linearly independent and spans  ,\n    so   has an infinite basis.\n   Finding a Basis for a Vector Space \n    We already know how to find bases for certain vector spaces,\n    namely   and  ,\n    where   is any matrix.\n    Finding a basis for a different kind of vector space will require other methods.\n    Since a basis for a vector space is a minimal spanning set,\n    to find a basis for a given vector space we might begin from scratch,\n    starting with a given vector in the space and adding one vector at a time until we have a spanning set.\n   \n      Let  .\n      We will find a basis of   that contains the polynomial  .\n     \n            Let  .\n            Find a polynomial   in   that is not in  .\n            Explain why this means that the set   does not span  .\n           \n            Let  .\n            Find a polynomial   that is not in  .\n            What does this mean about   being a possible spanning set of  ?\n           \n            Let  .\n            Explain why the set   is a basis for  .\n           \n    Alternatively,\n    we might construct a basis from a known spanning set.\n   \n      Let  .\n      Assume that   is a subspace of  .\n     \n            Find a set   of five\n              matrices that spans  \n            (since   is a span of a set of vectors in  ,\n              is a subspace of  ).\n            Without doing any computation,\n            can this set   be a basis for  ?\n            Why or why not?\n           \n            Find a subset   of   that is a basis for  .\n           \n     \n    and  \n    give us two ways of finding a basis for a subspace   of a vector space  ,\n    assuming   has a basis with finitely many vectors.\n    One way (illustrated in  ) is to start by choosing any non-zero vector   in  .\n    Let  .\n    If   spans  ,\n    then   is a basis for  .\n    If not, there is a vector   in   that is not in  .\n    Then   is a linearly independent set.\n    If  ,\n    then   is a basis for   and we are done.\n    If not, repeat the process.\n    We will show later that this process must stop as long as we know that   has a basis with fini\n    tely many vectors.\n   \n    Another way\n    (illustrated in  )\n    to find a basis for   is to start with a spanning set   of  .\n    If   is linearly independent,\n    then   is a basis for  .\n    If   is linearly dependent,\n    then one vector in   is a linear combination of the others and we can remove that vector to obtain a new set   that also spans  .\n    If   is linearly independent,\n    then   is a basis for  .\n    If not, we repeat the process as many times as needed until we arrive until at a subset   of   that is linearly independent and spans  .\n    We summarize these results in the following theorem.\n   \n        Let   be a subspace of a finite-dimensional vector space  .\n        Then\n         \n             \n              any linearly independent subset of   can be extended to a basis of  ,\n             \n           \n             \n              any subset of   that spans   can be reduced to a basis of  .\n             \n           \n       \n    We conclude this section with the result mentioned in the introduction   that every vector in a vector space with basis   can be written in one and only one way as a linear combination of basis vectors.\n    The proof is similar to that of   and is left to the exercises.\n   \n        Let  ,  ,  ,\n          be vectors in a vector space   that make up a basis   for  .\n        If   is a vector in  ,\n        then   can be written in one and only one way as a linear combination of vectors  ,\n         ,  ,   in  .\n       Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Let  .\n       \n              Does   span  ?\n              Explain.\n             \n              Let   be an arbitrary vector in  .\n              If   is in  ,\n              then there are weights  ,\n               ,  ,  , and   such that\n               .\n              Equating coefficients of like powers gives us the system\n               \n              The reduced row echelon form of the coefficient matrix   is\n               .\n              Since there is a pivot in every row of  ,\n              the system   is always consistent.\n              We conclude that   does span  .\n             \n              Explain why   is not a basis for  .\n             \n              The fact that the coefficient matrix   of our system has non-pivot columns means that each vector in   can be written in more than one way as a linear combination of vectors in  .\n              This means that   is not linearly independent and so cannot be a basis for  .\n             \n              Find a subset of   that is a basis for  .\n              Explain your reasoning.\n             \n              That the first three columns of   are pivot columns implies that the polynomials  ,\n               , and   are linearly independent.\n              Since there is a pivot in every row of  ,\n              the three polynomials  ,\n               , and   also span  .\n              So   is a subset of   that is a basis for  .\n             \n        Let   be the set of all matrices of real numbers of the form\n          and   be the set of all real matrices of the form  .\n       \n              Find a basis for   and a basis for  .\n             \n              Every matrix in   has the form\n               .\n              Let  .\n              Then   and   is a subspace of  .\n              If\n               ,\n              then   and   is also linearly independent.\n              This makes   a basis for  .\n              Similarly, every matrix in   has the form\n               .\n              Let  .\n              Then   and   is a subspace of  .\n              If\n               ,\n              then   and   is also linearly independent.\n              This makes   a basis for  .\n             \n              Let  .\n              Show that   is a subspace of\n                and find a basis for  .\n             \n              Every matrix in   has the form\n               .\n              Let  .\n              Then   and   is a subspace of  .\n              If\n               ,\n              then\n               \n              The reduced row echelon form of\n                is  .\n              The vectors that correspond to the pivot columns are linearly independent and span  ,\n              so a basis for   is\n               .\n             Summary \n    The important idea in this section is that of a basis for a vector space.\n    A basis is a minimal spanning set and another equivalent characterization of the\n     minimal \n    property is linear independence.\n     \n         \n          A set   of vectors in a vector space   is linearly independent if the vector equation\n           \n          for scalars   has only the trivial solution\n           .\n          If a set of vectors is not linearly independent,\n          then the set is linearly dependent.\n         \n       \n         \n          A set   of vectors in a vector space   is linearly independent if and only if none of the vectors in the set can be written as a linear combination of the remaining vectors in the set.\n         \n       \n         \n          A set   of vectors in a vector space   is linearly dependent if and only if at least one of the vectors in the set can be written as a linear combination of the remaining vectors in the set.\n         \n       \n         \n          A basis for a vector space   is a subset   of   if\n           \n               \n                  and\n               \n             \n               \n                  is a linearly independent set.\n               \n             \n         \n       \n         \n          A basis is important in that it provides us with an efficient way to represent any vector in the vector space   any vector can be written in one and only one way as a linear combination of vectors in a basis.\n         \n       \n         \n          To find a basis of a vector space,\n          we can start with a spanning set   and toss out any vector in   that can be written as a linear combination of the remaining vectors in  .\n          We repeat the process with the remaining subset of   until we arrive at a linearly independent spanning set.\n          Alternatively,\n          we can find a spanning set for the space and remove any vector that is a linear combination of the others in the spanning set.\n          We can repeat this process until we wind up with a linearly independent spanning set.\n         \n       \n   \n        Determine if the given sets are linearly independent or dependent in the indicated vector space.\n        If dependent,\n        write one of the vectors as a linear combination of the others.\n        If independent,\n        determine if the set is a basis for the vector space.\n       \n                in  \n             \n              This set is a basis for  .\n             \n                in  \n             \n              The set is linearly independent in   but does not span  .\n             \n                in  \n             \n              The set is a basis for  .\n             \n                in  .\n             \n              The set is linearly independent in\n                but does not span  .\n             \n        Let   in  .\n       \n              Show that the set   spans  .\n             \n              Show that the set   is linearly dependent.\n             \n              Find a subset of   that is a basis for  .\n              Be sure to verify that you have a basis.\n             \n        Find two different bases for  .\n        Explain how you know that each set is a basis.\n       \n        The set   where   is the matrix with a   in the  th position and zeros everywhere else is a basis for  ,\n        as is the set  ,\n        where   is the matrix with a   in the  th position and zeros everywhere else.\n       \n        The set   is a subspace of  .\n       \n              Find a set of vectors in   that spans  .\n             \n              Find a basis for  .\n              Be sure to verify that you have a basis.\n             \n        Suppose that the set   is a basis for a vector space  .\n        Is the set   a basis for  ?\n        Verify your result.\n       \n        The set is a basis for  .\n       \n        Determine all scalars   so that the set\n          is a basis for  .\n       \n        A symmetric matrix is a matrix   so that  .\n        Is it possible to find a basis for\n          consisting entirely of symmetric matrices?\n        If so, exhibit one such basis.\n        If not, explain why not.\n       \n        It is not possible.\n       \n        Find a basis of the subspace of\n          consisting of all matrices of the form\n          where   and  .\n       \n        Prove  .\n        \n       \n        Mimic the proof of  .\n       \n        Prove  .\n        \n       \n        Mimic the proof of  .\n       \n        Prove  .\n        \n       \n        Compare to  .\n       \n        Show that if   are subspaces of   such that  ,\n        then for any linearly independent vectors\n          in   and   in  ,\n        the set  ,  ,\n         ,  ,  ,\n         ,  ,\n          is linearly independent in  .\n       \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n        Throughout, let   be a vector space.\n       True\/False \n              If   is in  ,\n              then the set   is linearly independent.\n             \n              F\n             True\/False \n              If a set of vectors span a subspace,\n              then the set forms a basis of this subspace.\n             True\/False \n              If a linearly independent set of vectors spans a subspace,\n              then the set forms a basis of this subspace.\n             \n              T\n             True\/False \n              If the set   spans   and removing any vector from   makes it not a spanning set anymore,\n              then   is a basis.\n             True\/False \n              If   is a linearly independent set in   and for every   in  ,\n              adding   to   makes it not linearly independent anymore,\n              then   is a basis.\n             \n              T\n             True\/False \n              If a subset   of   spans  ,\n              then   must be linearly independent.\n             True\/False \n              If a subset   of   is linearly independent,\n              then   must span  .\n             \n              F\n             True\/False \n              If   is a linearly dependent set in  ,\n              then every vector in   is a linear combination of the other vectors in  .\n             True\/False \n              A vector space cannot have more than one basis.\n             \n              F\n             True\/False \n              If   is a non-zero vector in  ,\n              then there is a basis of   containing  .\n             True\/False \n              If   are two linearly independent vectors in  ,\n              then there is a basis of   containing  .\n             \n              T\n             True\/False \n              If   is in a basis of  ,\n              then   cannot be in a basis of  .\n             Project: Image Compression with Wavelets \n    We return to the problem of image compression introduced at the beginning of this section.\n    The first step in the wavelet compression process is to digitize an image.\n    There are two important ideas about digitalization to understand here:\n    intensity levels and resolution.\n    In grayscale image processing,\n    it is common to think of 256 different intensity levels,\n    or scales,\n    of gray ranging from 0 (black) to 255\n    (white).\n    A digital image can be created by taking a small grid of squares\n    (called pixels)\n    and coloring each pixel with some shade of gray.\n    The resolution of this grid is a measure of how many pixels are used per square inch.\n    An example of a 16 by 16 pixel picture of a flower was shown in  .\n   signal \n    To process a signal we select a family of wavelets.\n    There are many different families of wavelets   which family to use depends on the problem to be addressed.\n    The simplest family of wavelets is the Haar family.\n    More complicated families of wavelets are usually used in applications,\n    but the basic ideas in wavelets can be seen through working with the Haar wavelets,\n    and their relative simplicity will make the details easier to follow.\n    Each family of wavelets has a father wavelet\n    (usually denoted  )\n    and a mother wavelet\n    ( ).\n   \n    Wavelets are generated from the mother wavelet by scalings and translations.\n    To further simplify our work we will restrict ourselves to wavelets on [0,1], although this is not necessary.\n    The advantage the wavelets have over other methods of data analysis (Fourier analysis for example) is that with the scalings and translations we are able to analyze both frequency on large intervals and isolate signal discontinuities on very small intervals.\n    The way this is done is by using a large collection (infinite,\n    in fact) of basis functions with which to transform the data.\n    We'll begin by looking at how these basis functions arise.\n   \n    If we sample data at various points,\n    we can consider our data to represent a piecewise constant function obtained by partitioning [0,1] into   equal sized subintervals,\n    where   represents the number of sample points.\n    For the purposes of this project we will always choose   to be a power of 2.\n    So we can consider all of our data to represent functions.\n    For us, then,\n    it is natural to look at these functions in the vector space of all functions from   to  .\n    Since our data is piecewise constant,\n    we can really restrict ourselves to a subspace of this larger vector space   subspaces of piecewise constant functions.\n    The most basic piecewise constant function on the interval   is the one whose value is 1 on the entire interval.\n    We define   to be this constant function\n    (called the characteristic function of the unit interval).\n    That is\n     \n   \n    This function   is the Father Haar wavelet.\n   Graphs of  ,  , and   from left to right. \n    This function   may seem to be a very simple function but it has properties that will be important to us.\n    One property is that   satisfies a scaling equation.\n    For example,\n      shows that\n     \n    while   shows that\n     .\n   Graphs of  ,  ,  , and  , from left to right. \n    So   is a sum of scalings and translations of itself.\n    In general, for each positive integer   and integers   between 0 and   we define\n     .\n   \n    Then   for each  .\n   \n    These functions   are useful in that they form a basis for the vector space   of all piecewise constant functions on   that have possible breaks at the points\n     ,  ,\n     ,  ,  .\n    This is exactly the kind of space in which digital signals live,\n    especially if we sample signals at   evenly spaced points on  .\n    Let  .\n    You may assume without proof that   is a basis of  .\n   \n            Draw the linear combination  .\n            What does this linear combination look like?\n            Explain the statement made previously ``Notice that these   functions\n              form a basis for the vector space of all piecewise constant functions on   that have possible breaks at the points\n             ,  ,\n             ,  ,\n             \".\n           \n            Remember that we can consider our data to represent a piecewise constant function obtained by partitioning   into   subintervals,\n            where   represents the number of sample points.\n            Suppose we collect the following data:\n             ,  ,  ,\n             ,  ,  ,  ,  .\n            Explain how we can use this data to define a piecewise constant function   on  .\n            Express   as a linear combination of suitable functions  .\n            Plot this linear combination of   to verify.\n           \n    Working with functions can be more cumbersome than working with vectors in  ,\n    but the digital nature of our data makes it possible to view our piecewise constant functions as vectors in   for suitable  .\n    More specifically, if   is an element in  ,\n    then   is a piecewise constant function on   with possible breaks at the points\n     ,  ,\n     ,  ,  .\n    If   has the value of   on the interval between\n      and  ,\n    then we can identify   with the vector  .\n   \n            Determine the vector in   that is identified with  .\n           \n            Determine the value of   and the vectors in   that are identified with  ,\n             ,  , and  .\n           wavelet basis \n      The space   consists of all functions that are piecewise constant on   with a possible break at  .\n      The functions   are used to records the values of a signal,\n      and by summing these values we can calculate their average.\n      Wavelets act by averaging and differencing,\n      and so   does the averaging.\n      We need functions that will perform the differencing.\n     \n            Define   as\n             .\n            A picture of   is shown in  .\n            Since   assumes values of   and  ,\n            we can use   to perform differencing.\n            The function   is the Mother Haar wavelet. \n            The first mention of wavelets appeared in an appendix to the thesis of A. Haar in 1909.\n              Show that\n              is a basis for  .\n\n            \n           The graphs of  ,   and   from left to right. \n            We continue in a manner similar to the one in which we constructed bases for  .\n            For   and  ,\n            let  .\n            Graphs of   and\n              are shown in  .\n            The functions   assume the values of   and   on smaller intervals,\n            and so can be used to perform differencing on smaller scale than  .\n            Show that   is a basis for  .\n           \n    As   suggests,\n    we can make a basis for   from\n      and functions of the form   defined by\n      for   from 0 to  .\n    More specifically,\n    if we let  ,\n    then the set\n     \n    is a basis for  \n    (we state this without proof).\n    The functions   are the  wavelets .\n   \n      We can now write any function in   using the basis  .\n      As an example,\n      the string 50, 16, 14, 28 represents a piecewise constant function which can be written as\n       ,\n      an element in  .\n     \n            Specifically identify the functions in  ,  ,\n            and  , and  .\n           \n            As mentioned earlier, we can identify a signal,\n            and each wavelet function,\n            with a vector in   for an appropriate value of  .\n            We can then use this identification to decompose any signal as a linear combination of wavelets.\n            We illustrate this idea with the signal   in  .\n            Recall that we can represent this signal as the function  .\n           \n                  Find the the vectors  ,\n                   ,  ,\n                  and   in   that are identified with  ,\n                   ,\n                   , and  , respectively.\n                 \n                  Any linear combination   is then identified with the linear combination  .\n                  Use this idea to find the weights to write the function   as a linear combination of  ,\n                   ,  , and  .\n                 wavelet coefficients \n    Once we have recognized the pattern in expressing our original function as an overall average and wavelet coefficients we can perform these operations more quickly with matrices.\n   \n      The process of averaging and differencing discussed in and following  \n      can be viewed as a matrix-vector problem.\n      As we saw in  ,\n      we can translate the problem of finding wavelet coefficients to the matrix world.\n     \n            Consider again the problem of finding the wavelet coefficients contained in the vector\n              for the signal  .\n            Find the matrix   that has the property that  .\n            (You have already done part of this problem in  .)\n            Explain how   performs the averaging and differencing discussed earlier.\n           \n            Repeat the process in part (a) to find the matrix   that converts a signal to its wavelet coefficients.\n           forward wavelet transformation matrix inverse wavelet transform matrix \n    Now we have all of the necessary background to discuss image compression.\n    Suppose we want to store an image.\n    We partition the image vertically and horizontally and record the color or shade at each grid entry.\n    The grid entries will be our pixels.\n    This gives a matrix,  , of colors,\n    indexed by pixels or horizontal and vertical position.\n    To simplify our examples we will work in gray-scale,\n    where our grid entries are integers between 0 (black) and 255\n    (white).\n    We can treat each column of our grid as a piecewise constant function.\n    As an example,\n    the image matrix   that produced the picture at left in  \n    is given in  .\n   \n    We can then apply a 16 by 16 forward wavelet transformation matrix   to   to convert the columns to averages and wavelet coefficients that will appear in the matrix  .\n    These wavelet coefficients allow us to compress the image   that is,\n    create a smaller set of data that contains the essence of the original image.\n   \n    Recall that the forward wavelet transformation matrix computes weighted differences of consecutive entries in the columns of the image matrix  .\n    If two entries in   are close in values,\n    the weighted difference in   will be close to 0.\n    For our example, the matrix   is approximately\n     .\n   thresholding hard thresholding keep or kill tolerance \n    We now have introduced many zeros in our matrix.\n    This is where we compress the image.\n    To store the original image, we need to store every pixel.\n    Once we introduce strings of zeros we can identify a new code (say 256) that indicates we have a string of zeros.\n    We can then follow that code with the number of zeros in the string.\n    So if we had a string of 15 zeros in a signal,\n    we could store that information in 2 bytes rather than 15 and obtain significant savings in storage.\n    This process removes some detail from our picture,\n    but only the small detail.\n    To convert back to an image,\n    we just undo the forward processing by multiplying our thresholded matrix   by  .\n    The ultimate goal is to obtain significant compression but still have\n      retain all of the essence of the original image.\n   \n    In our example using  ,\n    the reconstructed image matrix is  \n    (rounded to the nearest whole number)\n    is\n     .\n   \n    We convert this into a gray-scale image and obtain the image at right in  .\n    Compare this image to the original at right in  .\n    It is difficult to tell the difference.\n   \n    There is a Sage file you can use at   that allows you to create your own 16 by 16 image and process,\n    process your image with the Haar wavelets in  ,\n    apply thresholding, and reconstruct the compressed image. matrix.\n    You can create your own image,\n    experiment with several different threshold levels,\n    and choose the one that you feel gives the best combination of strings of 0s while reproducing a reasonable copy of the original image.\n   "
},
{
  "id": "objectives-32",
  "level": "2",
  "url": "chap_bases.html#objectives-32",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            What does it mean for a set\n              of vectors in a vector space   to be linearly independent?\n           \n         \n           \n            What is another equivalent characterization of a linearly independent set?\n           \n         \n           \n            What does is mean for a set\n              of vectors in a vector space   to be linearly dependent?\n           \n         \n           \n            Describe another characterization of a linearly dependent set.\n           \n         \n           \n            What is a basis for a vector space  ?\n           \n         \n           \n            What makes a basis for a vector space useful?\n           \n         \n           \n            How can we find a basis for a vector space  ?\n           \n         "
},
{
  "id": "F_Flower_1",
  "level": "2",
  "url": "chap_bases.html#F_Flower_1",
  "type": "Figure",
  "number": "32.1",
  "title": "",
  "body": "Left: A 16 by 16 pixel image. Right: The image compressed. "
},
{
  "id": "def_vs_linear_independence",
  "level": "2",
  "url": "chap_bases.html#def_vs_linear_independence",
  "type": "Definition",
  "number": "32.2",
  "title": "",
  "body": "linearly independent vectors in a vector space linear dependent vectors in a vector space linearly independent linearly dependent "
},
{
  "id": "pa_5_b",
  "level": "2",
  "url": "chap_bases.html#pa_5_b",
  "type": "Preview Activity",
  "number": "32.1",
  "title": "",
  "body": "\n            We can use the tools we developed to determine if a set of vectors in   is linearly independent to answer the same questions for sets of vectors in other vector spaces.\n            For example, consider the question of whether the set\n              in   is linearly independent or dependent.\n            To answer this question we need to determine if there is a non-trivial solution to the equation\n             .\n            Note that equation   can also be written in the form\n             .\n           \n                  Recall that two polynomials are equal if all coefficients of like powers are the same.\n                  By equating coefficients of like power terms,\n                  rewrite equation   as an equivalent system of two equations in the two unknowns   and  ,\n                  and solve for  .\n                 \n                  What does your answer to the previous part tell you about the linear independence or dependence of the set   in  ?\n                 \n                  Recall that in  ,\n                  a set of two vectors is linearly dependent if and only if one of the vectors in the set is a scalar multiple of the other and linearly independent if neither vector is a scalar multiple of the other.\n                  Verify your answer to part (c) from a similar perspective in  .\n                 \n            We can use the same type of method as in problem (1) to address the question of whether the set\n             \n            is linearly independent or dependent in  .\n            To answer this question we need to determine if there is a non-trivial solution to the equation\n             \n            for some scalars  ,  , and  .\n            Note that the linear combination on the left side of equation   has entries\n             .\n           \n                  Recall that two matrices are equal if all corresponding entries are the same.\n                  Equate corresponding entries of the matrices in equation   to rewrite the equation as an equivalent system of four equations in the three unknowns  ,\n                   , and  .\n                 \n                  Use appropriate matrix tools and techniques to find all solutions to the system from part (a).\n                 \n                  What does the set of solutions to the system from part (a) tell you about the linear independence or dependence of the set\n                   \n                 \n                  Recall that in  ,\n                  a set of vectors is linearly dependent if and only if one of the vectors in the set is a linear combination of the others and linearly independent if no vector in the set is a linear combination of the others.\n                  Verify your answer to part (c) from a similar perspective in  .\n                 \n            We will define a basis for a vector space to be a linearly independent spanning set.\n            Which, if any,\n            of the sets in parts (1) and (2) is a basis for its vector space?\n            Explain.\n           "
},
{
  "id": "thm_5_b_1",
  "level": "2",
  "url": "chap_bases.html#thm_5_b_1",
  "type": "Theorem",
  "number": "32.3",
  "title": "",
  "body": "\n        A set   of vectors in a vector space   is linearly dependent if and only if at least one of the vectors in the set can be written as a linear combination of the remaining vectors in the set.\n       "
},
{
  "id": "thm_5_b_2",
  "level": "2",
  "url": "chap_bases.html#thm_5_b_2",
  "type": "Theorem",
  "number": "32.4",
  "title": "",
  "body": "\n        A set   of vectors in a vector space   is linearly independent if and only if no vector in the set can be written as a linear combination of the remaining vectors in the set.\n       "
},
{
  "id": "thm_5_b_3",
  "level": "2",
  "url": "chap_bases.html#thm_5_b_3",
  "type": "Theorem",
  "number": "32.5",
  "title": "",
  "body": "\n        Let   be a set of vectors in a vector space  .\n        If for some   between 1 and  ,\n          is in  , then\n         .\n       "
},
{
  "id": "p-5592",
  "level": "2",
  "url": "chap_bases.html#p-5592",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "minimal basis "
},
{
  "id": "definition-73",
  "level": "2",
  "url": "chap_bases.html#definition-73",
  "type": "Definition",
  "number": "32.6",
  "title": "",
  "body": "basis for a vector space vector space basis basis "
},
{
  "id": "act_5_b_1",
  "level": "2",
  "url": "chap_bases.html#act_5_b_1",
  "type": "Activity",
  "number": "32.2",
  "title": "",
  "body": "\n            Is   a basis for  ?\n            Explain.\n           standard basis for  standard basis \n            Show that the set\n             \n            is a basis for  .\n           "
},
{
  "id": "act_5_b_2",
  "level": "2",
  "url": "chap_bases.html#act_5_b_2",
  "type": "Activity",
  "number": "32.3",
  "title": "",
  "body": "\n      Let  .\n      We will find a basis of   that contains the polynomial  .\n     \n            Let  .\n            Find a polynomial   in   that is not in  .\n            Explain why this means that the set   does not span  .\n           \n            Let  .\n            Find a polynomial   that is not in  .\n            What does this mean about   being a possible spanning set of  ?\n           \n            Let  .\n            Explain why the set   is a basis for  .\n           "
},
{
  "id": "act_5_b_2_b",
  "level": "2",
  "url": "chap_bases.html#act_5_b_2_b",
  "type": "Activity",
  "number": "32.4",
  "title": "",
  "body": "\n      Let  .\n      Assume that   is a subspace of  .\n     \n            Find a set   of five\n              matrices that spans  \n            (since   is a span of a set of vectors in  ,\n              is a subspace of  ).\n            Without doing any computation,\n            can this set   be a basis for  ?\n            Why or why not?\n           \n            Find a subset   of   that is a basis for  .\n           "
},
{
  "id": "theorem-80",
  "level": "2",
  "url": "chap_bases.html#theorem-80",
  "type": "Theorem",
  "number": "32.7",
  "title": "",
  "body": "\n        Let   be a subspace of a finite-dimensional vector space  .\n        Then\n         \n             \n              any linearly independent subset of   can be extended to a basis of  ,\n             \n           \n             \n              any subset of   that spans   can be reduced to a basis of  .\n             \n           \n       "
},
{
  "id": "thm_5_b_4",
  "level": "2",
  "url": "chap_bases.html#thm_5_b_4",
  "type": "Theorem",
  "number": "32.8",
  "title": "",
  "body": "\n        Let  ,  ,  ,\n          be vectors in a vector space   that make up a basis   for  .\n        If   is a vector in  ,\n        then   can be written in one and only one way as a linear combination of vectors  ,\n         ,  ,   in  .\n       "
},
{
  "id": "example-67",
  "level": "2",
  "url": "chap_bases.html#example-67",
  "type": "Example",
  "number": "32.9",
  "title": "",
  "body": "\n        Let  .\n       \n              Does   span  ?\n              Explain.\n             \n              Let   be an arbitrary vector in  .\n              If   is in  ,\n              then there are weights  ,\n               ,  ,  , and   such that\n               .\n              Equating coefficients of like powers gives us the system\n               \n              The reduced row echelon form of the coefficient matrix   is\n               .\n              Since there is a pivot in every row of  ,\n              the system   is always consistent.\n              We conclude that   does span  .\n             \n              Explain why   is not a basis for  .\n             \n              The fact that the coefficient matrix   of our system has non-pivot columns means that each vector in   can be written in more than one way as a linear combination of vectors in  .\n              This means that   is not linearly independent and so cannot be a basis for  .\n             \n              Find a subset of   that is a basis for  .\n              Explain your reasoning.\n             \n              That the first three columns of   are pivot columns implies that the polynomials  ,\n               , and   are linearly independent.\n              Since there is a pivot in every row of  ,\n              the three polynomials  ,\n               , and   also span  .\n              So   is a subset of   that is a basis for  .\n             "
},
{
  "id": "example-68",
  "level": "2",
  "url": "chap_bases.html#example-68",
  "type": "Example",
  "number": "32.10",
  "title": "",
  "body": "\n        Let   be the set of all matrices of real numbers of the form\n          and   be the set of all real matrices of the form  .\n       \n              Find a basis for   and a basis for  .\n             \n              Every matrix in   has the form\n               .\n              Let  .\n              Then   and   is a subspace of  .\n              If\n               ,\n              then   and   is also linearly independent.\n              This makes   a basis for  .\n              Similarly, every matrix in   has the form\n               .\n              Let  .\n              Then   and   is a subspace of  .\n              If\n               ,\n              then   and   is also linearly independent.\n              This makes   a basis for  .\n             \n              Let  .\n              Show that   is a subspace of\n                and find a basis for  .\n             \n              Every matrix in   has the form\n               .\n              Let  .\n              Then   and   is a subspace of  .\n              If\n               ,\n              then\n               \n              The reduced row echelon form of\n                is  .\n              The vectors that correspond to the pivot columns are linearly independent and span  ,\n              so a basis for   is\n               .\n             "
},
{
  "id": "exercise-323",
  "level": "2",
  "url": "chap_bases.html#exercise-323",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Determine if the given sets are linearly independent or dependent in the indicated vector space.\n        If dependent,\n        write one of the vectors as a linear combination of the others.\n        If independent,\n        determine if the set is a basis for the vector space.\n       \n                in  \n             \n              This set is a basis for  .\n             \n                in  \n             \n              The set is linearly independent in   but does not span  .\n             \n                in  \n             \n              The set is a basis for  .\n             \n                in  .\n             \n              The set is linearly independent in\n                but does not span  .\n             "
},
{
  "id": "exercise-324",
  "level": "2",
  "url": "chap_bases.html#exercise-324",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Let   in  .\n       \n              Show that the set   spans  .\n             \n              Show that the set   is linearly dependent.\n             \n              Find a subset of   that is a basis for  .\n              Be sure to verify that you have a basis.\n             "
},
{
  "id": "exercise-325",
  "level": "2",
  "url": "chap_bases.html#exercise-325",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Find two different bases for  .\n        Explain how you know that each set is a basis.\n       \n        The set   where   is the matrix with a   in the  th position and zeros everywhere else is a basis for  ,\n        as is the set  ,\n        where   is the matrix with a   in the  th position and zeros everywhere else.\n       "
},
{
  "id": "exercise-326",
  "level": "2",
  "url": "chap_bases.html#exercise-326",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        The set   is a subspace of  .\n       \n              Find a set of vectors in   that spans  .\n             \n              Find a basis for  .\n              Be sure to verify that you have a basis.\n             "
},
{
  "id": "exercise-327",
  "level": "2",
  "url": "chap_bases.html#exercise-327",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Suppose that the set   is a basis for a vector space  .\n        Is the set   a basis for  ?\n        Verify your result.\n       \n        The set is a basis for  .\n       "
},
{
  "id": "exercise-328",
  "level": "2",
  "url": "chap_bases.html#exercise-328",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Determine all scalars   so that the set\n          is a basis for  .\n       "
},
{
  "id": "exercise-329",
  "level": "2",
  "url": "chap_bases.html#exercise-329",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        A symmetric matrix is a matrix   so that  .\n        Is it possible to find a basis for\n          consisting entirely of symmetric matrices?\n        If so, exhibit one such basis.\n        If not, explain why not.\n       \n        It is not possible.\n       "
},
{
  "id": "exercise-330",
  "level": "2",
  "url": "chap_bases.html#exercise-330",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Find a basis of the subspace of\n          consisting of all matrices of the form\n          where   and  .\n       "
},
{
  "id": "exercise-331",
  "level": "2",
  "url": "chap_bases.html#exercise-331",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        Prove  .\n        \n       \n        Mimic the proof of  .\n       "
},
{
  "id": "exercise-332",
  "level": "2",
  "url": "chap_bases.html#exercise-332",
  "type": "Exercise",
  "number": "10",
  "title": "",
  "body": "\n        Prove  .\n        \n       \n        Mimic the proof of  .\n       "
},
{
  "id": "exercise-333",
  "level": "2",
  "url": "chap_bases.html#exercise-333",
  "type": "Exercise",
  "number": "11",
  "title": "",
  "body": "\n        Prove  .\n        \n       \n        Compare to  .\n       "
},
{
  "id": "problem_disjoint_subspaces",
  "level": "2",
  "url": "chap_bases.html#problem_disjoint_subspaces",
  "type": "Exercise",
  "number": "12",
  "title": "",
  "body": "\n        Show that if   are subspaces of   such that  ,\n        then for any linearly independent vectors\n          in   and   in  ,\n        the set  ,  ,\n         ,  ,  ,\n         ,  ,\n          is linearly independent in  .\n       "
},
{
  "id": "exercise-335",
  "level": "2",
  "url": "chap_bases.html#exercise-335",
  "type": "Exercise",
  "number": "13",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n        Throughout, let   be a vector space.\n       True\/False \n              If   is in  ,\n              then the set   is linearly independent.\n             \n              F\n             True\/False \n              If a set of vectors span a subspace,\n              then the set forms a basis of this subspace.\n             True\/False \n              If a linearly independent set of vectors spans a subspace,\n              then the set forms a basis of this subspace.\n             \n              T\n             True\/False \n              If the set   spans   and removing any vector from   makes it not a spanning set anymore,\n              then   is a basis.\n             True\/False \n              If   is a linearly independent set in   and for every   in  ,\n              adding   to   makes it not linearly independent anymore,\n              then   is a basis.\n             \n              T\n             True\/False \n              If a subset   of   spans  ,\n              then   must be linearly independent.\n             True\/False \n              If a subset   of   is linearly independent,\n              then   must span  .\n             \n              F\n             True\/False \n              If   is a linearly dependent set in  ,\n              then every vector in   is a linear combination of the other vectors in  .\n             True\/False \n              A vector space cannot have more than one basis.\n             \n              F\n             True\/False \n              If   is a non-zero vector in  ,\n              then there is a basis of   containing  .\n             True\/False \n              If   are two linearly independent vectors in  ,\n              then there is a basis of   containing  .\n             \n              T\n             True\/False \n              If   is in a basis of  ,\n              then   cannot be in a basis of  .\n             "
},
{
  "id": "p-5698",
  "level": "2",
  "url": "chap_bases.html#p-5698",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "signal "
},
{
  "id": "F_phi_graphs_1",
  "level": "2",
  "url": "chap_bases.html#F_phi_graphs_1",
  "type": "Figure",
  "number": "32.11",
  "title": "",
  "body": "Graphs of  ,  , and   from left to right. "
},
{
  "id": "F_phi_graphs_2",
  "level": "2",
  "url": "chap_bases.html#F_phi_graphs_2",
  "type": "Figure",
  "number": "32.12",
  "title": "",
  "body": "Graphs of  ,  ,  , and  , from left to right. "
},
{
  "id": "project-105",
  "level": "2",
  "url": "chap_bases.html#project-105",
  "type": "Project Activity",
  "number": "32.5",
  "title": "",
  "body": "\n            Draw the linear combination  .\n            What does this linear combination look like?\n            Explain the statement made previously ``Notice that these   functions\n              form a basis for the vector space of all piecewise constant functions on   that have possible breaks at the points\n             ,  ,\n             ,  ,\n             \".\n           \n            Remember that we can consider our data to represent a piecewise constant function obtained by partitioning   into   subintervals,\n            where   represents the number of sample points.\n            Suppose we collect the following data:\n             ,  ,  ,\n             ,  ,  ,  ,  .\n            Explain how we can use this data to define a piecewise constant function   on  .\n            Express   as a linear combination of suitable functions  .\n            Plot this linear combination of   to verify.\n           "
},
{
  "id": "act_wavelets_vectors",
  "level": "2",
  "url": "chap_bases.html#act_wavelets_vectors",
  "type": "Project Activity",
  "number": "32.6",
  "title": "",
  "body": "\n            Determine the vector in   that is identified with  .\n           \n            Determine the value of   and the vectors in   that are identified with  ,\n             ,  , and  .\n           "
},
{
  "id": "p-5712",
  "level": "2",
  "url": "chap_bases.html#p-5712",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "wavelet basis "
},
{
  "id": "act_psi",
  "level": "2",
  "url": "chap_bases.html#act_psi",
  "type": "Project Activity",
  "number": "32.7",
  "title": "",
  "body": "\n      The space   consists of all functions that are piecewise constant on   with a possible break at  .\n      The functions   are used to records the values of a signal,\n      and by summing these values we can calculate their average.\n      Wavelets act by averaging and differencing,\n      and so   does the averaging.\n      We need functions that will perform the differencing.\n     \n            Define   as\n             .\n            A picture of   is shown in  .\n            Since   assumes values of   and  ,\n            we can use   to perform differencing.\n            The function   is the Mother Haar wavelet. \n            The first mention of wavelets appeared in an appendix to the thesis of A. Haar in 1909.\n              Show that\n              is a basis for  .\n\n            \n           The graphs of  ,   and   from left to right. \n            We continue in a manner similar to the one in which we constructed bases for  .\n            For   and  ,\n            let  .\n            Graphs of   and\n              are shown in  .\n            The functions   assume the values of   and   on smaller intervals,\n            and so can be used to perform differencing on smaller scale than  .\n            Show that   is a basis for  .\n           "
},
{
  "id": "act_wavelets_differencing",
  "level": "2",
  "url": "chap_bases.html#act_wavelets_differencing",
  "type": "Project Activity",
  "number": "32.8",
  "title": "",
  "body": "\n      We can now write any function in   using the basis  .\n      As an example,\n      the string 50, 16, 14, 28 represents a piecewise constant function which can be written as\n       ,\n      an element in  .\n     \n            Specifically identify the functions in  ,  ,\n            and  , and  .\n           \n            As mentioned earlier, we can identify a signal,\n            and each wavelet function,\n            with a vector in   for an appropriate value of  .\n            We can then use this identification to decompose any signal as a linear combination of wavelets.\n            We illustrate this idea with the signal   in  .\n            Recall that we can represent this signal as the function  .\n           \n                  Find the the vectors  ,\n                   ,  ,\n                  and   in   that are identified with  ,\n                   ,\n                   , and  , respectively.\n                 \n                  Any linear combination   is then identified with the linear combination  .\n                  Use this idea to find the weights to write the function   as a linear combination of  ,\n                   ,  , and  .\n                 "
},
{
  "id": "p-5722",
  "level": "2",
  "url": "chap_bases.html#p-5722",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "wavelet coefficients "
},
{
  "id": "act_wavelets_matrices",
  "level": "2",
  "url": "chap_bases.html#act_wavelets_matrices",
  "type": "Project Activity",
  "number": "32.9",
  "title": "",
  "body": "\n      The process of averaging and differencing discussed in and following  \n      can be viewed as a matrix-vector problem.\n      As we saw in  ,\n      we can translate the problem of finding wavelet coefficients to the matrix world.\n     \n            Consider again the problem of finding the wavelet coefficients contained in the vector\n              for the signal  .\n            Find the matrix   that has the property that  .\n            (You have already done part of this problem in  .)\n            Explain how   performs the averaging and differencing discussed earlier.\n           \n            Repeat the process in part (a) to find the matrix   that converts a signal to its wavelet coefficients.\n           forward wavelet transformation matrix inverse wavelet transform matrix "
},
{
  "id": "p-5731",
  "level": "2",
  "url": "chap_bases.html#p-5731",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "thresholding "
},
{
  "id": "p-5732",
  "level": "2",
  "url": "chap_bases.html#p-5732",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "hard thresholding keep or kill tolerance "
},
{
  "id": "chap_dimension",
  "level": "1",
  "url": "chap_dimension.html",
  "type": "Section",
  "number": "33",
  "title": "The Dimension of a Vector Space",
  "body": "The Dimension of a Vector Space \n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            What is a finite dimensional vector space?\n           \n         \n           \n            What is the dimension of a finite dimensional vector space?\n            What important result about bases of finite dimensional vector spaces makes dimension well-defined?\n           \n         \n           \n            What must be true about any linearly independent subset of   vectors in a vector space with dimension  ?\n            Why?\n           \n         \n           \n            What must be true about any subset of   vectors in a vector space with dimension   that spans the vector space?\n            Why?\n           \n         Application: Principal Component Analysis \n    The discipline of statistics is based on the idea of analyzing data.\n    In large data sets it is usually the case that one wants to understand the relationships between the different data in the set.\n    This can be difficult to do when the data set is large and it is impossible to visually examine the data for patterns.\n    Principal Component Analysis (PCA) is a tool for identifying and representing underlying patterns in large data sets,\n    and PCA has been called one of the most important and valuable linear algebra tools for statistical analysis.\n    PCA is used to transform a collection of variables into a\n    (usually smaller)\n    number of uncorrelated variables called principal components.\n    The principal components form the most meaningful basis from which to view data by removing extraneous information and revealing underlying relationships in the data.\n    This presents a framework for how to reduce a complex data set to a lower dimension while retaining the important attributes of the data set.\n    The output helps the experimenter determine which dynamics in the data are important and which can be ignored.\n   Introduction \n    In  \n    we learned that any two bases for a subspace of   contain the same number of vectors.\n    This allowed us to define the dimension of a subspace of  .\n    In this section we extend the arguments we made in  \n    to arbitrary vector spaces and define the dimension of a vector space.\n   \n      The main tool we used to prove that any two bases for a subspace of   must contain the same number of elements was  .\n      In this preview activity we will show that the same argument can be used for vector spaces.\n      More specifically,\n      we will prove a special case of the following theorem generalizing  .\n     \n          If   is a vector space with a basis   of   vectors,\n          then any subset of   containing more than   vectors is linearly dependent.\n         \n      Suppose   is a vector space with basis  .\n      Consider the set   of vectors in  .\n      We will show that   is linearly dependent using a similar approach to the  .\n     \n            What vector equation involving\n              do we need to solve to determine linear independence\/dependence of these vectors?\n            Use   for coefficients.\n           \n            Since   is a basis of  , it spans  .\n            Using this information,\n            rewrite the vectors   in terms of   and substitute into the above equation to obtain another equation in terms of  .\n           \n            Since   is a basis of  ,\n            the vectors   are linearly independent.\n            Using the equation in the previous part,\n            determine what this means about the coefficients  .\n           \n            Express the conditions on   in the form of a matrix-vector equation.\n            Explain why there are infinitely many solutions for  's and why this means the vectors\n              are linearly dependent.\n           Finite Dimensional Vector Spaces \n     \n    shows that if sets   and   are finite bases for a vector space  ,\n    which are linearly independent by definition,\n    then each cannot contain more elements than the other,\n    so the number of elements in each basis must be equal.\n   \n        If a non-trivial vector space   has a basis of   vectors,\n        then every basis of   contains exactly   vectors.\n       invariant vector space finite dimensional vector space dimension finite-dimensional dimension \n    We denote the dimension of a finite dimensional vector space   by  .\n   infinite dimensional \n      Since columns of the   identity matrix span   and are linearly independent,\n      the columns of   form a basis for  \n      (the standard basis).\n      Consequently, we have that  .\n      In this activity we determine the dimensions of other familiar vector spaces.\n      Find the dimensions of each of the indicated vector spaces.\n      Verify your answers.\n     \n               \n           \n               \n           \n               \n           \n               \n           \n               \n           \n              \n           \n    Finding the dimension of a finite-dimensional vector space amounts to finding a basis for the space.\n   \n      Let  .\n     \n            Find a finite set of polynomials in   that span  .\n           \n            Determine if the spanning set from part (a) is linearly independent or dependent.\n            Clearly explain your process.\n           \n            What is  ?\n            Explain.\n           The Dimension of a Subspace \n    Every subspace of a finite-dimensional vector space is a vector space,\n    and since a subspace is contained in a vector space it is natural to think that the dimension of a subspace should be less than or equal to the dimension of the larger vector space.\n    We verify that fact in this section.\n   \n      Let   be a finite dimensional vector space of dimension   and let   be a subspace of  .\n      Explain why   cannot have dimension larger than  ,\n      and if   then  .\n     \n        Use  .\n       Conditions for a Basis of a Vector Space \n    There are two items we need to confirm before we can state that a subset   of a subspace   of a vector space is a basis for  :\n    the set   must be linearly independent and span  .\n    We can reduce the amount of work it takes to show that a set is a basis if we know the dimension of the vector space in advance.\n   \n      Let   be a subspace of a vector space   with  .\n      We know that every basis of   contains exactly   vectors.\n     \n            Suppose that   is a subset of   that contains   vectors and is linearly independent.\n            In this part of the activity we will show that   must span  .\n           \n                  Suppose that   does not span  .\n                  Explain why this implies that   contains a set of   linearly independent vectors.\n                 \n                  Explain why the result of i tells us that   is a basis for  .\n                 \n            Now suppose that   is a subset of   with   vectors that spans  .\n            In this part of the activity we will show that   must be linearly independent.\n           \n                  Suppose that   is not linearly independent.\n                  Explain why we can then find a proper subset of   that is linearly independent but has the same span as  .\n                 \n                  Explain why the result of i tells us that   is a basis for  .\n                 \n    The result of  \n    is summarized in the following theorem\n    (compare to  ).\n   \n        Let   be a subspace of dimension   of a vector space   and let   be a subset of   containing exactly   vectors.\n         \n             \n              If   is linearly independent,\n              then   is a basis for  .\n             \n           \n             \n              If   spans  , then   is a basis for  .\n             \n           \n       Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Find a basis and dimension for each of the indicated subspaces of the given vector spaces.\n       \n                in  \n             \n              Let  .\n              Every element in   has the form\n               .\n              So  .\n              Since neither   nor   is a scalar multiple of the other,\n              the set   is linearly independent.\n              Thus,   is a basis for   and  .\n             \n                in  \n             \n              Let  .\n              To find a basis for  ,\n              we find a linearly independent subset of  .\n              Consider the equation\n               .\n              To find the weights   for which this equality of functions holds,\n              we use the fact that the we must have equality for every  .\n              So we pick four different values for   to obtain a linear system that we can solve for the weights.\n              Evaluating both sides of the equation at  ,\n               ,  , and   yields the equations\n               \n              The reduced row echelon form of the coefficient matrix\n               \n              is  .\n              The general solution to this linear system is   and  .\n              Notice that\n               \n              or\n               ,\n              so   is a linear combination of the other vectors.\n              The vectors corresponding to the pivot columns are linearly independent,\n              so it follows that  ,\n               , and\n                are linearly independent.\n              We conclude that   is a basis for   and  .\n             \n                in   (The polynomials with the property that   are called\n               even  polynomials.)\n             \n              Let   in  .\n              Let   and suppose that  .\n              Since  ,\n              we must have   and  .\n              Since  , it follows that\n               \n              or\n               .\n              Similarly, the fact that   yields the equation\n               \n              or\n               .\n              The reduced row echelon form of the coefficient matrix of system\n                and   is  ,\n              so it follows that  .\n              Thus,   and so  .\n              Equating like terms in the equation\n               \n              yields  .\n              We conclude that   is linearly independent and is therefore a basis for  .\n              Thus,  .\n              As an alternative solution, notice that   is not in  .\n              So   and we know that  .\n              Since  ,  ,\n              and   are in  ,\n              we can show as above that  ,\n               , and   are linearly independent.\n              We can conclude that   since it cannot be  .\n             \n        Let   be the set of all\n          upper triangular matrices.\n        Recall that a matrix   is upper triangular if\n          whenever  .\n        That is, a matrix is upper triangular if all entries below the diagonal are 0.\n       \n              Show that   is a subspace of  .\n             \n              Since the   zero matrix\n                has all entries equal to 0, it follows that\n                is in  .\n              Let   and\n                be in  ,\n              and let  .\n              Then   when  .\n              So   is an upper triangular matrix and\n                is closed under addition.\n              Let   be a scalar.\n              The  th entry of   is\n                whenever  .\n              So   is an upper triangular matrix and\n                is closed under multiplication by scalars.\n              We conclude that   is a subspace of  .\n             \n              Find the dimensions of   and  .\n              Explain.\n              Make a conjecture as to what\n                is in terms of  .\n             \n              Let  ,\n               ,\n              and  .\n              We will show that   is a basis for  .\n              Consider the equation\n               .\n              Equating like entries shows that  ,\n              and so   is linearly independent.\n              If   is in  , then\n               \n              and so   spans  .\n              Thus,   is a basis for\n                and so  .\n              Similarly, for the   case let   for   be the\n                matrix with a 1 in the   position and 0 in every other position.\n              Let\n               .\n              Equating corresponding entries shows that if\n               ,\n              then  .\n              So   is a linearly independent set.\n              If   is in  ,\n              then   and   spans  .\n              We conclude that   is a basis for\n                and  .\n              In general, for an   matrix,\n              the set of matrices  ,\n              one for each entry on and above the diagonal,\n              is a basis for  .\n              There are   such matrices for the entries on the diagonal.\n              The number of entries above the diagonal is equal to half the total number of entries ( ) minus half the number of entries on the diagonal\n              ( ).\n              So there is a total of   such matrices for the entries above the diagonal.\n              Therefore,\n               .\n             Summary \n       \n        A finite dimensional vector space is a vector space that can be spanned by a finite set of vectors.\n       \n     \n       dimension \n     \n       \n        If   is a vector space with dimension   and   is any linearly independent subset of   with   vectors,\n        then   is a basis for  .\n        Otherwise, we could add vectors to   to make a basis for   and then   would have a basis of more than   vectors.\n       \n     \n       \n        If   is a vector space with dimension   and   is any subset of   with   vectors that spans  ,\n        then   is a basis for  .\n        Otherwise, we could remove vectors from   to obtain a basis for   and then   would have a basis of fewer than   vectors.\n       \n     \n       \n        For any finite dimensional space   and a subspace   of  ,\n         .\n       \n     \n        Let   in  .\n        Find a basis for  .\n        What is the dimension of  ?\n       \n        The set\n          is a basis for   and  .\n       \n        Let   and  .\n       \n              Are   and   linearly independent or dependent?\n              Verify your result.\n             \n              Extend the set   to a basis for  .\n              That is, find a basis for   that contains both   and  .\n             \n        Let  ,\n         ,\n         ,\n         ,\n        and   in\n          and let  .\n       \n              Is   a basis for  ?\n              Explain.\n             \n              No.\n             \n              Determine if   is a linearly independent or dependent set.\n              Verify your result.\n             \n              The set   is linearly dependent.\n             \n              Find a basis   for\n                that is a subset of   and write all of the vectors in   as linear combinations of the vectors in  .\n             \n              The set   forms a basis for\n                and  ,  .\n             \n              Extend your basis   from part (c) to a basis  .\n              Explain your method.\n             \n              Let  ,\n               ,\n              and  .\n              The set   is a basis for  .\n             \n        Determine the dimension of each of the following vector spaces.\n       \n                in  \n             \n              The space of all polynomials in   whose constant terms is 0.\n             \n               \n             \n                in  \n             \n        Let   be the set of matrices in\n          whose diagonal entries sum to 0.\n        Show that   is a subspace of  ,\n        find a basis for  , and then find  .\n       \n        The set\n         \n        is a basis for  .\n        It follows that  .\n       \n        Show that if   is a subspace of a finite dimensional vector space  ,\n        then any basis of   can be extended to a basis of  .\n       \n        Let   be the set of all polynomials\n          in   such that  .\n        Show that   is a subspace of  ,\n        find a basis for  , and then find  .\n       \n        The set   is a basis for   and  .\n       \n        Suppose   are subspaces in a finite-dimensional space  .\n       \n              Show that it is not true in general that  .\n             \n              Are there any conditions on   and   that will ensure that  ?\n              \n             \n              See  problem  in the previous section.\n             \n        Suppose   are two subspaces of a finite-dimensional space.\n        Show that if  , then  .\n       \n        Consider dimensions.\n       \n        Suppose   are both three-dimensional subspaces inside  .\n        In this exercise we will show that   contains a plane.\n        Let   be a basis for   and let\n          be a basis for  .\n       \n              If  ,  ,\n              and   are all in  ,\n              explain why   must contain a plane.\n             \n              Now we consider the case where not all of  ,\n               ,\n              and   are in  .\n              Since the arguments will be the same,\n              let us assume that   is not in  .\n             \n                    Explain why the set   is a basis for  .\n                   \n                    Explain why   and   can be written as linear combinations of the vectors in  .\n                    Use these linear combinations to find two vectors that are in  .\n                    Then show that these vectors span a plane in  .\n                   \n        A magic matrix is an   matrix in which the sum of the entries along any row,\n        column, or diagonal is the same.\n        For example, the   matrix\n         \n        is a magic matrix.\n        Note that the entries of a magic matrix can be any real numbers.\n       \n              Show that the set of   magic matrices is a subspace of  .\n             \n              Consider the row, column, and diagonal sums.\n             \n              Let   be the space of   magic matrices.\n              Find a basis for   and determine the dimension of  .\n             \n              Set up a system of equations.\n              The dimension is 3.\n             \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              The dimension of a finite dimensional vector space is the minimum number of vectors needed to span that space.\n             \n              T\n             True\/False \n              The dimension of a finite dimensional vector space is the maximum number of linearly independent vectors that can exist in that space.\n             True\/False \n              If   vectors span an  -dimensional vector space  ,\n              then these vectors form a basis of  .\n             \n              T\n             True\/False \n              Any set of   vectors form a basis in an  -dimensional vector space.\n             True\/False \n              Every vector in a vector space   spans a one-dimensional subspace of  .\n             \n              F\n             True\/False \n              Any set of   linearly independent vectors in a vector space   of dimensional   is a basis for  .\n             True\/False \n              If   is linearly independent in  ,\n              then  .\n             \n              T\n             True\/False \n              If a set of   vectors span  ,\n              then any set of more than   vectors in   is linearly dependent.\n             True\/False \n              If an infinite set of vectors span  ,\n              then   is infinite-dimensional.\n             \n              F\n             True\/False \n              If   are both two-dimensional subspaces of a three dimensional vector space  ,\n              then  .\n             True\/False \n              If   and   is a subspace of   with dimension  ,\n              then  .\n             \n              T\n             Project: Understanding Principal Component Analysis \n    Suppose we were to film an experiment involving a ball that is bouncing up and down.\n    Naively, we set up several cameras to follow the process of the experiment from different perspectives and collect the data.\n    All of this data tells us something about the bouncing ball,\n    but there may be no perspective that tells us an important piece of information   the axis along which the ball bounces.\n    The question, then,\n    is how we can extract from the data this important piece of information.\n    Principal Component Analysis (PCA) is a tool for just this type of analysis.\n   SAT data State EBRW Math \n    We will use an example to illustrate important concepts we will need.\n    To realistically apply PCA we will have much more data than this,\n    but for now we will restrict ourselves to only two variables so that we can visualize our results.\n     \n    presents information from ten states on two attributes related to the SAT   Evidence-Based Reading and Writing (EBRW) score and Math score.\n    The SAT is made up of three sections: Reading, Writing and Language\n    (also just called Writing),\n    and Math.\n    The The EBRW score is calculated by combining the Reading and Writing section scores   both the Math and EBRW are scored on a scale of 200-800.\n   \n    Each attribute (Math, EBRW score) creates a vector whose entries are the student responses for that attribute.\n    The data provides the average scores from participating students in each state.\n    In this example we have two attribute vectors:\n     .\n   \n    These vectors form the rows of a   matrix\n     \n    that makes up our data set.\n    A plot of the data is shown at left in  ,\n    where the EBRW score is along the horizontal axis and the math score is along the vertical axis.\n   Two views of the data set (EBRW horizontal, math vertical). \n    The question we want to answer is,\n    how do we represent our data set so that the most important features in the data set are revealed?\n   \n      Before analyzing a data set there is often some initial preparation that needs to be made.\n      Issues that have to be dealt with include the problem that attributes might have different units and scales.\n      For example, in a data set with attributes about people,\n      height could be measured in inches while weight is measured in pounds.\n      It is difficult to compare variables when they are on different scales.\n      Another issue to consider is that some attributes are independent of the others (height,\n      for example does not depend on hair color),\n      while some are interrelated\n      (body mass index depends on height and weight).\n      To simplify our work, we will not address these type of problems.\n      The only preparation we will do with our data is to center it.\n     \n            An important piece of information about a one-dimensional data set\n               is the sample average or mean\n             .\n            Calculate the means   and\n              for our SAT data from the matrix  .\n           \n            We can use these sample means to center our data at the origin by translating the data so that each column of our data matrix has mean  .\n            We do this by subtracting the mean for that row vector from each component of the vector.\n            Determine the matrix   that contains the centered data for our SAT data set from matrix  .\n           \n    A plot of the centered data for our SAT data is shown at right in  .\n    Later we will see why centering the data is useful   it will more easily allow us to project onto subspaces.\n    The goal of PCA is to find a matrix   so that  ,\n    and   is suitably transformed to identify the important aspects of the data.\n    We will discuss what the important aspects are shortly.\n    Before we do so,\n    we need to discuss a way to compare the one dimensional data vectors   and  .\n   \n      To compare the two one dimensional data vectors,\n      we need to consider variance and covariance.\n     \n            It is often useful to know how spread out a data set is,\n            something the average doesn't tell us.\n            For example, the data sets   and\n              both have averages of  ,\n            but the data in   is more spread out.\n            Variance provides one measure of how spread out a one-dimensional data set   is.\n            Variance is defined as\n             .\n            The variance provides a measure of how far from the average the data is spread. \n            It might seem that we should divide by   instead of   in the variance,\n            but it is generally accepted to do this for reasons we won't get into.\n            Suffice it to say that if we are using a sample of the entire population,\n            then dividing by   provides a variance whose square root is closer to the standard deviation than we would get if we divide by  .\n            If we are calculating the variance of an entire population,\n            then we would divide by  .\n               Determine the variances of the two data vectors   and  .\n            Which is more spread out?\n           \n            In general, we will have more than one-dimensional data,\n            as in our SAT data set.\n            It will be helpful to have a way to compare one-dimensional data sets to try to capture the idea of variance for different data sets   how much the data in two different data sets varies from the mean with respect to each other.\n            One such measure is covariance   essentially the average of all corresponding products of deviations from the means.\n            We define the covariance of two data vectors\n              and   as\n             .\n            Determine all covariances\n             .\n            How are   and   related?\n            How are    and\n              related to variances?\n           \n            What is most important about covariance is its sign.\n            Suppose  ,\n              and  .\n            Then if   is larger than   it is likely that   is also larger than  .\n            For example,\n            if   is a vector that records a persons height from age   to   and   records that same person's weight in the same years,\n            we might expect that when   increases so does  .\n            Similarly, if  ,\n            then as one data set increases, the other decreases.\n            As an example,\n            if   records the number of hours a student spends playing video games each semester ad   gives the student's GPA for each semester,\n            then we might expect that   decreases as   increases.\n            When  ,\n            then   and   are said to be uncorrelated or independent of each other.\n            For our example   and  ,\n            what does   tell us about the relationship between   and  ?\n            Why should we expect this from the context?\n           covariance matrix covariance matrix \n    Recall that the goal of PCA is to find a matrix   such that   where   transforms the data set to a coordinate system in which the important aspects of the data are revealed.\n    We are now in a position to discuss what that means.\n   \n    An ideal view of our data would be one in which we can see the direction of greatest variance and one that minimizes redundancy.\n    With redundant data the variables are not independent   that is,\n    covariance is nonzero.\n    So we would like the covariances to all be zero\n    (or as close to zero as possible)\n    to remove redundancy in our data.\n    That means that we would like a covariance matrix in which the non-diagonal entries are all zero.\n    This will be possible if the covariance matrix is diagonalizable.\n   \n      Consider the covariance matrix  .\n      Explain why we can find a matrix   with determinant 1 whose columns are unit vectors that diagonalizes  .\n      Then find such a matrix.\n      Use technology as appropriate.\n     \n    For our purposes, we want to diagonalize\n      with  ,\n    so the matrix   that serve our purposes is the one whose  rows \n    are the eigenvectors of  .\n    To understand why this matrix is the one we want,\n    recall that we want to have  ,\n    and we want to diagonalize\n      to a diagonal covariance matrix  .\n    In this situation we will have (recalling that  )\n     .\n   \n    So the matrix   that we want is exactly the one that diagonalizes  .\n   first principal component \n            There is another way we can interpret this result.\n            If we drop a perpendicular from one of our data points to the space\n              it creates a right triangle with sides of length  ,\n             ,\n            and   as illustrated in the middle of  .\n            Use this idea to explain why maximizing the variation also minimizes the sum of the squares of the distances from the data points to this line.\n            As a result,\n            we have projected our two-dimensional data onto the one-dimensional space that maximizes the variance of the data.\n            \n           The principal component. \n            Recall that the matrix\n            (to two decimal places)\n              transforms the data set   to a new data set   whose covariance matrix is diagonal.\n            Explain how the  -axis is related to the transformed data set  .\n            \n           Applying  . \n    The result of  \n    is that we have reduced the problem from considering the data in a two-dimensional space to a one-dimensional space\n      where the most important aspect of the data is revealed.\n    Of course, we eliminate some of the characteristics of the data,\n    but the most important aspect is still included and highlighted.\n    This is one of the important uses of PCA, data dimension reduction,\n    which allows us to reveal key relationships between the variables that might not be evident when looking at a large dataset.\n   \n    The second eigenvector of   also has meaning.\n    A picture of the eigenspace\n      corresponding to the smaller eigenvector   of\n      is shown in  .\n    The second eigenvector of   is orthogonal to the first,\n    and the direction of the second eigenvector tells us the direction of the second most amount of variance as can be seen in  .\n   The second principal component. principal components \n      We can use the eigenvalues of\n        to quantify the amount of variance that is accounted for by our projections.\n      Notice that the points along the  -axis at right in  \n      are exactly the numbers in the first row of  .\n      These numbers provide the projections of the data in   onto the  -axis   the axis along which the data has its greatest variance.\n     \n            Calculate the variance of the data given by the first row of  .\n            This is the variance of the data i the direction of the eigenspace  .\n            How does the result compare to entries of the covariance matrix for  .\n           \n            Repeat part (a) for the data along the second row of  .\n           \n            The total variance of the data set is the sum of the variances.\n            Explain why the amount of variance in the data that is accounted for in the direction of   is\n             .\n            Then calculate this amount for the SAT data.\n           \n    In general, PCA is most useful for larger data sets.\n    The process is the same.\n     \n         \n          Start with a set of data that forms the rows of an   matrix.\n          We center the data by subtracting the mean of each row from the entries of that row to create a centered data set in a matrix  .\n         \n       \n         \n          The principal components of   are the eigenvectors of  ,\n          ordered so that they correspond to the eigenvalues of   in decreasing order.\n         \n       \n         \n          Let   be the matrix whose rows are the principal components of  ,\n          ordered from highest to lowest.\n          Then   is suitably transformed to identify the important aspects of the data.\n         \n       \n         \n          If  ,  ,  ,\n            are the eigenvalues of   in decreasing order,\n          then the amount of variance in the data accounted for by the first   principal components is given by\n           .\n         \n       \n         \n          The first   rows of   provide the projection of the data set   onto an  -dimensional space spanned by the first   principal components of  .\n         \n       \n   \n      Let us now consider a problem with more than two variables.\n      We continue to keep the data set small so that we can conveniently operate with it.\n       \n      presents additional information from ten states on four attributes related to the SAT   Participation rates, Evidence-Based Reading and Writing (EBRW) score, Math score,\n      and average SAT score.\n      Use technology as appropriate for this activity.\n     SAT data State Rate EBRW Math SAT \n          Determine the centered data matrix   for this data set.\n         \n          Find the covariance matrix for this data set.\n          Round to four decimal places.\n         \n          Find the principal components of  .\n          Include at least four decimal places accuracy.\n         \n          How much variation is accounted for in the data by the first principal component?\n          In other words, if we reduce this data to one dimension,\n          how much of the variation do we retain?\n          Explain.\n         \n          How much variation is accounted for in the data by the first two principal components?\n          In other words, if we reduce this data to two dimensions,\n          how much of the variation do we retain?\n          Explain.\n         \n    We conclude with a comment.\n    A reasonable question to ask is how we interpret the principal components.\n    Let   be an orthogonal matrix such that\n      is the diagonal matrix with the eigenvalues of   along the diagonal,\n    in decreasing order.\n    We then have the new perspective   from which to view the data.\n    The first principal component  \n    (the first row of  )\n    determines the new variable  \n    (the first row of  )\n    in the following manner.\n    Let   and let   represent the columns of   so that  .\n    Recognizing that\n     ,\n    we have that\n     .\n   \n    That is,\n     .\n   \n    So each   is a linear combination of the original variables\n    (contained in the  )\n    with weights from the first principal component.\n    The other new variables are obtained in the same way from the remaining principal components.\n    So even though the principal components may not have an easy interpretation in context,\n    they are connected to the original data in this way.\n    By reducing the data to a few important principal components   that is,\n    visualizing the data in a subspace of small dimension   we can account for almost all of the variation in the data and relate that information back to the original data.\n   "
},
{
  "id": "objectives-33",
  "level": "2",
  "url": "chap_dimension.html#objectives-33",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            What is a finite dimensional vector space?\n           \n         \n           \n            What is the dimension of a finite dimensional vector space?\n            What important result about bases of finite dimensional vector spaces makes dimension well-defined?\n           \n         \n           \n            What must be true about any linearly independent subset of   vectors in a vector space with dimension  ?\n            Why?\n           \n         \n           \n            What must be true about any subset of   vectors in a vector space with dimension   that spans the vector space?\n            Why?\n           \n         "
},
{
  "id": "pa_5_c",
  "level": "2",
  "url": "chap_dimension.html#pa_5_c",
  "type": "Preview Activity",
  "number": "33.1",
  "title": "",
  "body": "\n      The main tool we used to prove that any two bases for a subspace of   must contain the same number of elements was  .\n      In this preview activity we will show that the same argument can be used for vector spaces.\n      More specifically,\n      we will prove a special case of the following theorem generalizing  .\n     \n          If   is a vector space with a basis   of   vectors,\n          then any subset of   containing more than   vectors is linearly dependent.\n         \n      Suppose   is a vector space with basis  .\n      Consider the set   of vectors in  .\n      We will show that   is linearly dependent using a similar approach to the  .\n     \n            What vector equation involving\n              do we need to solve to determine linear independence\/dependence of these vectors?\n            Use   for coefficients.\n           \n            Since   is a basis of  , it spans  .\n            Using this information,\n            rewrite the vectors   in terms of   and substitute into the above equation to obtain another equation in terms of  .\n           \n            Since   is a basis of  ,\n            the vectors   are linearly independent.\n            Using the equation in the previous part,\n            determine what this means about the coefficients  .\n           \n            Express the conditions on   in the form of a matrix-vector equation.\n            Explain why there are infinitely many solutions for  's and why this means the vectors\n              are linearly dependent.\n           "
},
{
  "id": "thm_5_c_2",
  "level": "2",
  "url": "chap_dimension.html#thm_5_c_2",
  "type": "Theorem",
  "number": "33.2",
  "title": "",
  "body": "\n        If a non-trivial vector space   has a basis of   vectors,\n        then every basis of   contains exactly   vectors.\n       "
},
{
  "id": "p-5753",
  "level": "2",
  "url": "chap_dimension.html#p-5753",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "invariant "
},
{
  "id": "definition-74",
  "level": "2",
  "url": "chap_dimension.html#definition-74",
  "type": "Definition",
  "number": "33.3",
  "title": "",
  "body": "vector space finite dimensional vector space dimension finite-dimensional dimension "
},
{
  "id": "p-5756",
  "level": "2",
  "url": "chap_dimension.html#p-5756",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "infinite dimensional "
},
{
  "id": "act_5_c_1",
  "level": "2",
  "url": "chap_dimension.html#act_5_c_1",
  "type": "Activity",
  "number": "33.2",
  "title": "",
  "body": "\n      Since columns of the   identity matrix span   and are linearly independent,\n      the columns of   form a basis for  \n      (the standard basis).\n      Consequently, we have that  .\n      In this activity we determine the dimensions of other familiar vector spaces.\n      Find the dimensions of each of the indicated vector spaces.\n      Verify your answers.\n     \n               \n           \n               \n           \n               \n           \n               \n           \n               \n           \n              \n           "
},
{
  "id": "act_5_c_2",
  "level": "2",
  "url": "chap_dimension.html#act_5_c_2",
  "type": "Activity",
  "number": "33.3",
  "title": "",
  "body": "\n      Let  .\n     \n            Find a finite set of polynomials in   that span  .\n           \n            Determine if the spanning set from part (a) is linearly independent or dependent.\n            Clearly explain your process.\n           \n            What is  ?\n            Explain.\n           "
},
{
  "id": "act_5_c_3",
  "level": "2",
  "url": "chap_dimension.html#act_5_c_3",
  "type": "Activity",
  "number": "33.4",
  "title": "",
  "body": "\n      Let   be a finite dimensional vector space of dimension   and let   be a subspace of  .\n      Explain why   cannot have dimension larger than  ,\n      and if   then  .\n     \n        Use  .\n       "
},
{
  "id": "act_5_c_5",
  "level": "2",
  "url": "chap_dimension.html#act_5_c_5",
  "type": "Activity",
  "number": "33.5",
  "title": "",
  "body": "\n      Let   be a subspace of a vector space   with  .\n      We know that every basis of   contains exactly   vectors.\n     \n            Suppose that   is a subset of   that contains   vectors and is linearly independent.\n            In this part of the activity we will show that   must span  .\n           \n                  Suppose that   does not span  .\n                  Explain why this implies that   contains a set of   linearly independent vectors.\n                 \n                  Explain why the result of i tells us that   is a basis for  .\n                 \n            Now suppose that   is a subset of   with   vectors that spans  .\n            In this part of the activity we will show that   must be linearly independent.\n           \n                  Suppose that   is not linearly independent.\n                  Explain why we can then find a proper subset of   that is linearly independent but has the same span as  .\n                 \n                  Explain why the result of i tells us that   is a basis for  .\n                 "
},
{
  "id": "theorem-84",
  "level": "2",
  "url": "chap_dimension.html#theorem-84",
  "type": "Theorem",
  "number": "33.4",
  "title": "",
  "body": "\n        Let   be a subspace of dimension   of a vector space   and let   be a subset of   containing exactly   vectors.\n         \n             \n              If   is linearly independent,\n              then   is a basis for  .\n             \n           \n             \n              If   spans  , then   is a basis for  .\n             \n           \n       "
},
{
  "id": "example-69",
  "level": "2",
  "url": "chap_dimension.html#example-69",
  "type": "Example",
  "number": "33.5",
  "title": "",
  "body": "\n        Find a basis and dimension for each of the indicated subspaces of the given vector spaces.\n       \n                in  \n             \n              Let  .\n              Every element in   has the form\n               .\n              So  .\n              Since neither   nor   is a scalar multiple of the other,\n              the set   is linearly independent.\n              Thus,   is a basis for   and  .\n             \n                in  \n             \n              Let  .\n              To find a basis for  ,\n              we find a linearly independent subset of  .\n              Consider the equation\n               .\n              To find the weights   for which this equality of functions holds,\n              we use the fact that the we must have equality for every  .\n              So we pick four different values for   to obtain a linear system that we can solve for the weights.\n              Evaluating both sides of the equation at  ,\n               ,  , and   yields the equations\n               \n              The reduced row echelon form of the coefficient matrix\n               \n              is  .\n              The general solution to this linear system is   and  .\n              Notice that\n               \n              or\n               ,\n              so   is a linear combination of the other vectors.\n              The vectors corresponding to the pivot columns are linearly independent,\n              so it follows that  ,\n               , and\n                are linearly independent.\n              We conclude that   is a basis for   and  .\n             \n                in   (The polynomials with the property that   are called\n               even  polynomials.)\n             \n              Let   in  .\n              Let   and suppose that  .\n              Since  ,\n              we must have   and  .\n              Since  , it follows that\n               \n              or\n               .\n              Similarly, the fact that   yields the equation\n               \n              or\n               .\n              The reduced row echelon form of the coefficient matrix of system\n                and   is  ,\n              so it follows that  .\n              Thus,   and so  .\n              Equating like terms in the equation\n               \n              yields  .\n              We conclude that   is linearly independent and is therefore a basis for  .\n              Thus,  .\n              As an alternative solution, notice that   is not in  .\n              So   and we know that  .\n              Since  ,  ,\n              and   are in  ,\n              we can show as above that  ,\n               , and   are linearly independent.\n              We can conclude that   since it cannot be  .\n             "
},
{
  "id": "example-70",
  "level": "2",
  "url": "chap_dimension.html#example-70",
  "type": "Example",
  "number": "33.6",
  "title": "",
  "body": "\n        Let   be the set of all\n          upper triangular matrices.\n        Recall that a matrix   is upper triangular if\n          whenever  .\n        That is, a matrix is upper triangular if all entries below the diagonal are 0.\n       \n              Show that   is a subspace of  .\n             \n              Since the   zero matrix\n                has all entries equal to 0, it follows that\n                is in  .\n              Let   and\n                be in  ,\n              and let  .\n              Then   when  .\n              So   is an upper triangular matrix and\n                is closed under addition.\n              Let   be a scalar.\n              The  th entry of   is\n                whenever  .\n              So   is an upper triangular matrix and\n                is closed under multiplication by scalars.\n              We conclude that   is a subspace of  .\n             \n              Find the dimensions of   and  .\n              Explain.\n              Make a conjecture as to what\n                is in terms of  .\n             \n              Let  ,\n               ,\n              and  .\n              We will show that   is a basis for  .\n              Consider the equation\n               .\n              Equating like entries shows that  ,\n              and so   is linearly independent.\n              If   is in  , then\n               \n              and so   spans  .\n              Thus,   is a basis for\n                and so  .\n              Similarly, for the   case let   for   be the\n                matrix with a 1 in the   position and 0 in every other position.\n              Let\n               .\n              Equating corresponding entries shows that if\n               ,\n              then  .\n              So   is a linearly independent set.\n              If   is in  ,\n              then   and   spans  .\n              We conclude that   is a basis for\n                and  .\n              In general, for an   matrix,\n              the set of matrices  ,\n              one for each entry on and above the diagonal,\n              is a basis for  .\n              There are   such matrices for the entries on the diagonal.\n              The number of entries above the diagonal is equal to half the total number of entries ( ) minus half the number of entries on the diagonal\n              ( ).\n              So there is a total of   such matrices for the entries above the diagonal.\n              Therefore,\n               .\n             "
},
{
  "id": "p-5798",
  "level": "2",
  "url": "chap_dimension.html#p-5798",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "dimension "
},
{
  "id": "exercise-336",
  "level": "2",
  "url": "chap_dimension.html#exercise-336",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Let   in  .\n        Find a basis for  .\n        What is the dimension of  ?\n       \n        The set\n          is a basis for   and  .\n       "
},
{
  "id": "exercise-337",
  "level": "2",
  "url": "chap_dimension.html#exercise-337",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Let   and  .\n       \n              Are   and   linearly independent or dependent?\n              Verify your result.\n             \n              Extend the set   to a basis for  .\n              That is, find a basis for   that contains both   and  .\n             "
},
{
  "id": "exercise-338",
  "level": "2",
  "url": "chap_dimension.html#exercise-338",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Let  ,\n         ,\n         ,\n         ,\n        and   in\n          and let  .\n       \n              Is   a basis for  ?\n              Explain.\n             \n              No.\n             \n              Determine if   is a linearly independent or dependent set.\n              Verify your result.\n             \n              The set   is linearly dependent.\n             \n              Find a basis   for\n                that is a subset of   and write all of the vectors in   as linear combinations of the vectors in  .\n             \n              The set   forms a basis for\n                and  ,  .\n             \n              Extend your basis   from part (c) to a basis  .\n              Explain your method.\n             \n              Let  ,\n               ,\n              and  .\n              The set   is a basis for  .\n             "
},
{
  "id": "exercise-339",
  "level": "2",
  "url": "chap_dimension.html#exercise-339",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Determine the dimension of each of the following vector spaces.\n       \n                in  \n             \n              The space of all polynomials in   whose constant terms is 0.\n             \n               \n             \n                in  \n             "
},
{
  "id": "exercise-340",
  "level": "2",
  "url": "chap_dimension.html#exercise-340",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Let   be the set of matrices in\n          whose diagonal entries sum to 0.\n        Show that   is a subspace of  ,\n        find a basis for  , and then find  .\n       \n        The set\n         \n        is a basis for  .\n        It follows that  .\n       "
},
{
  "id": "exercise-341",
  "level": "2",
  "url": "chap_dimension.html#exercise-341",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Show that if   is a subspace of a finite dimensional vector space  ,\n        then any basis of   can be extended to a basis of  .\n       "
},
{
  "id": "exercise-342",
  "level": "2",
  "url": "chap_dimension.html#exercise-342",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        Let   be the set of all polynomials\n          in   such that  .\n        Show that   is a subspace of  ,\n        find a basis for  , and then find  .\n       \n        The set   is a basis for   and  .\n       "
},
{
  "id": "exercise-343",
  "level": "2",
  "url": "chap_dimension.html#exercise-343",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Suppose   are subspaces in a finite-dimensional space  .\n       \n              Show that it is not true in general that  .\n             \n              Are there any conditions on   and   that will ensure that  ?\n              \n             \n              See  problem  in the previous section.\n             "
},
{
  "id": "exercise-344",
  "level": "2",
  "url": "chap_dimension.html#exercise-344",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        Suppose   are two subspaces of a finite-dimensional space.\n        Show that if  , then  .\n       \n        Consider dimensions.\n       "
},
{
  "id": "exercise-345",
  "level": "2",
  "url": "chap_dimension.html#exercise-345",
  "type": "Exercise",
  "number": "10",
  "title": "",
  "body": "\n        Suppose   are both three-dimensional subspaces inside  .\n        In this exercise we will show that   contains a plane.\n        Let   be a basis for   and let\n          be a basis for  .\n       \n              If  ,  ,\n              and   are all in  ,\n              explain why   must contain a plane.\n             \n              Now we consider the case where not all of  ,\n               ,\n              and   are in  .\n              Since the arguments will be the same,\n              let us assume that   is not in  .\n             \n                    Explain why the set   is a basis for  .\n                   \n                    Explain why   and   can be written as linear combinations of the vectors in  .\n                    Use these linear combinations to find two vectors that are in  .\n                    Then show that these vectors span a plane in  .\n                   "
},
{
  "id": "exercise-346",
  "level": "2",
  "url": "chap_dimension.html#exercise-346",
  "type": "Exercise",
  "number": "11",
  "title": "",
  "body": "\n        A magic matrix is an   matrix in which the sum of the entries along any row,\n        column, or diagonal is the same.\n        For example, the   matrix\n         \n        is a magic matrix.\n        Note that the entries of a magic matrix can be any real numbers.\n       \n              Show that the set of   magic matrices is a subspace of  .\n             \n              Consider the row, column, and diagonal sums.\n             \n              Let   be the space of   magic matrices.\n              Find a basis for   and determine the dimension of  .\n             \n              Set up a system of equations.\n              The dimension is 3.\n             "
},
{
  "id": "exercise-347",
  "level": "2",
  "url": "chap_dimension.html#exercise-347",
  "type": "Exercise",
  "number": "12",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              The dimension of a finite dimensional vector space is the minimum number of vectors needed to span that space.\n             \n              T\n             True\/False \n              The dimension of a finite dimensional vector space is the maximum number of linearly independent vectors that can exist in that space.\n             True\/False \n              If   vectors span an  -dimensional vector space  ,\n              then these vectors form a basis of  .\n             \n              T\n             True\/False \n              Any set of   vectors form a basis in an  -dimensional vector space.\n             True\/False \n              Every vector in a vector space   spans a one-dimensional subspace of  .\n             \n              F\n             True\/False \n              Any set of   linearly independent vectors in a vector space   of dimensional   is a basis for  .\n             True\/False \n              If   is linearly independent in  ,\n              then  .\n             \n              T\n             True\/False \n              If a set of   vectors span  ,\n              then any set of more than   vectors in   is linearly dependent.\n             True\/False \n              If an infinite set of vectors span  ,\n              then   is infinite-dimensional.\n             \n              F\n             True\/False \n              If   are both two-dimensional subspaces of a three dimensional vector space  ,\n              then  .\n             True\/False \n              If   and   is a subspace of   with dimension  ,\n              then  .\n             \n              T\n             "
},
{
  "id": "T_PCA_SAT_2",
  "level": "2",
  "url": "chap_dimension.html#T_PCA_SAT_2",
  "type": "Table",
  "number": "33.7",
  "title": "SAT data",
  "body": "SAT data State EBRW Math "
},
{
  "id": "F_PCA_data_plot",
  "level": "2",
  "url": "chap_dimension.html#F_PCA_data_plot",
  "type": "Figure",
  "number": "33.8",
  "title": "",
  "body": "Two views of the data set (EBRW horizontal, math vertical). "
},
{
  "id": "act_PCA_centering",
  "level": "2",
  "url": "chap_dimension.html#act_PCA_centering",
  "type": "Project Activity",
  "number": "33.6",
  "title": "",
  "body": "\n      Before analyzing a data set there is often some initial preparation that needs to be made.\n      Issues that have to be dealt with include the problem that attributes might have different units and scales.\n      For example, in a data set with attributes about people,\n      height could be measured in inches while weight is measured in pounds.\n      It is difficult to compare variables when they are on different scales.\n      Another issue to consider is that some attributes are independent of the others (height,\n      for example does not depend on hair color),\n      while some are interrelated\n      (body mass index depends on height and weight).\n      To simplify our work, we will not address these type of problems.\n      The only preparation we will do with our data is to center it.\n     \n            An important piece of information about a one-dimensional data set\n               is the sample average or mean\n             .\n            Calculate the means   and\n              for our SAT data from the matrix  .\n           \n            We can use these sample means to center our data at the origin by translating the data so that each column of our data matrix has mean  .\n            We do this by subtracting the mean for that row vector from each component of the vector.\n            Determine the matrix   that contains the centered data for our SAT data set from matrix  .\n           "
},
{
  "id": "act_PCA_covariance",
  "level": "2",
  "url": "chap_dimension.html#act_PCA_covariance",
  "type": "Project Activity",
  "number": "33.7",
  "title": "",
  "body": "\n      To compare the two one dimensional data vectors,\n      we need to consider variance and covariance.\n     \n            It is often useful to know how spread out a data set is,\n            something the average doesn't tell us.\n            For example, the data sets   and\n              both have averages of  ,\n            but the data in   is more spread out.\n            Variance provides one measure of how spread out a one-dimensional data set   is.\n            Variance is defined as\n             .\n            The variance provides a measure of how far from the average the data is spread. \n            It might seem that we should divide by   instead of   in the variance,\n            but it is generally accepted to do this for reasons we won't get into.\n            Suffice it to say that if we are using a sample of the entire population,\n            then dividing by   provides a variance whose square root is closer to the standard deviation than we would get if we divide by  .\n            If we are calculating the variance of an entire population,\n            then we would divide by  .\n               Determine the variances of the two data vectors   and  .\n            Which is more spread out?\n           \n            In general, we will have more than one-dimensional data,\n            as in our SAT data set.\n            It will be helpful to have a way to compare one-dimensional data sets to try to capture the idea of variance for different data sets   how much the data in two different data sets varies from the mean with respect to each other.\n            One such measure is covariance   essentially the average of all corresponding products of deviations from the means.\n            We define the covariance of two data vectors\n              and   as\n             .\n            Determine all covariances\n             .\n            How are   and   related?\n            How are    and\n              related to variances?\n           \n            What is most important about covariance is its sign.\n            Suppose  ,\n              and  .\n            Then if   is larger than   it is likely that   is also larger than  .\n            For example,\n            if   is a vector that records a persons height from age   to   and   records that same person's weight in the same years,\n            we might expect that when   increases so does  .\n            Similarly, if  ,\n            then as one data set increases, the other decreases.\n            As an example,\n            if   records the number of hours a student spends playing video games each semester ad   gives the student's GPA for each semester,\n            then we might expect that   decreases as   increases.\n            When  ,\n            then   and   are said to be uncorrelated or independent of each other.\n            For our example   and  ,\n            what does   tell us about the relationship between   and  ?\n            Why should we expect this from the context?\n           covariance matrix covariance matrix "
},
{
  "id": "act_PCA_diagonalize",
  "level": "2",
  "url": "chap_dimension.html#act_PCA_diagonalize",
  "type": "Project Activity",
  "number": "33.8",
  "title": "",
  "body": "\n      Consider the covariance matrix  .\n      Explain why we can find a matrix   with determinant 1 whose columns are unit vectors that diagonalizes  .\n      Then find such a matrix.\n      Use technology as appropriate.\n     "
},
{
  "id": "act_PCA_max_min",
  "level": "2",
  "url": "chap_dimension.html#act_PCA_max_min",
  "type": "Project Activity",
  "number": "33.9",
  "title": "",
  "body": "first principal component \n            There is another way we can interpret this result.\n            If we drop a perpendicular from one of our data points to the space\n              it creates a right triangle with sides of length  ,\n             ,\n            and   as illustrated in the middle of  .\n            Use this idea to explain why maximizing the variation also minimizes the sum of the squares of the distances from the data points to this line.\n            As a result,\n            we have projected our two-dimensional data onto the one-dimensional space that maximizes the variance of the data.\n            \n           The principal component. \n            Recall that the matrix\n            (to two decimal places)\n              transforms the data set   to a new data set   whose covariance matrix is diagonal.\n            Explain how the  -axis is related to the transformed data set  .\n            \n           Applying  . "
},
{
  "id": "F_PCA_second_pc",
  "level": "2",
  "url": "chap_dimension.html#F_PCA_second_pc",
  "type": "Figure",
  "number": "33.11",
  "title": "",
  "body": "The second principal component. "
},
{
  "id": "p-5884",
  "level": "2",
  "url": "chap_dimension.html#p-5884",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "principal components "
},
{
  "id": "act_PCA_variances",
  "level": "2",
  "url": "chap_dimension.html#act_PCA_variances",
  "type": "Project Activity",
  "number": "33.10",
  "title": "",
  "body": "\n      We can use the eigenvalues of\n        to quantify the amount of variance that is accounted for by our projections.\n      Notice that the points along the  -axis at right in  \n      are exactly the numbers in the first row of  .\n      These numbers provide the projections of the data in   onto the  -axis   the axis along which the data has its greatest variance.\n     \n            Calculate the variance of the data given by the first row of  .\n            This is the variance of the data i the direction of the eigenspace  .\n            How does the result compare to entries of the covariance matrix for  .\n           \n            Repeat part (a) for the data along the second row of  .\n           \n            The total variance of the data set is the sum of the variances.\n            Explain why the amount of variance in the data that is accounted for in the direction of   is\n             .\n            Then calculate this amount for the SAT data.\n           "
},
{
  "id": "act_PCA_4D_SAT",
  "level": "2",
  "url": "chap_dimension.html#act_PCA_4D_SAT",
  "type": "Project Activity",
  "number": "33.11",
  "title": "",
  "body": "\n      Let us now consider a problem with more than two variables.\n      We continue to keep the data set small so that we can conveniently operate with it.\n       \n      presents additional information from ten states on four attributes related to the SAT   Participation rates, Evidence-Based Reading and Writing (EBRW) score, Math score,\n      and average SAT score.\n      Use technology as appropriate for this activity.\n     SAT data State Rate EBRW Math SAT \n          Determine the centered data matrix   for this data set.\n         \n          Find the covariance matrix for this data set.\n          Round to four decimal places.\n         \n          Find the principal components of  .\n          Include at least four decimal places accuracy.\n         \n          How much variation is accounted for in the data by the first principal component?\n          In other words, if we reduce this data to one dimension,\n          how much of the variation do we retain?\n          Explain.\n         \n          How much variation is accounted for in the data by the first two principal components?\n          In other words, if we reduce this data to two dimensions,\n          how much of the variation do we retain?\n          Explain.\n         "
},
{
  "id": "chap_coordinate_vectors_vector_spaces",
  "level": "1",
  "url": "chap_coordinate_vectors_vector_spaces.html",
  "type": "Section",
  "number": "34",
  "title": "Coordinate Vectors and Coordinate Transformations",
  "body": "Coordinate Vectors and Coordinate Transformations \n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            How do we find the coordinate vector of a vector   with respect to a basis  ?\n           \n         \n           \n            How can we visualize coordinate systems?\n           \n         \n           \n            How do we identify a vector space of dimension   with  ?\n           \n         \n           \n            What properties does the coordinate transformation   have?\n           \n         \n           \n            How are the coordinates of a vector with respect to a basis related to its coordinates with respect to the standard basis?\n           \n         Application: Calculating Sums \n    Consider the problem of calculating sums of powers of integers.\n    For example,\n     \n    and so on.\n    It is possible to prove these formulas if we have an idea of what the formula is,\n    but how can one come up with these formulas in the first place?\n    As we will see later in this section,\n    we can make use of coordinate vectors to find and verify such formulas.\n   Introduction \n    In  \n    we defined the coordinate vector of a vector   in   with respect to a basis\n      of   to be the vector   if\n     .\n   \n    We can define coordinate vectors in vector spaces in the same way.\n    As we will see,\n    this is a powerful idea as it allows us to translate problems in finite dimensional vector spaces to problems in   where we have many tools at our disposal.\n    First we define a coordinate vector.\n   coordinate vector with respect to a basis coordinates with respect to a basis coordinate vector coordinates of the vector with respect to the basis \n      Let  ,\n       ,  ,  ,\n      and let   and  .\n     \n            Show that   and   are bases for  .\n           \n            Let  .\n            What is  ?\n           \n            Find the coordinate vector of\n              with respect to both   and  .\n           \n            Find a polynomial   such that  .\n            Determine exactly how many such polynomials   have the property that  .\n            Explain your reasoning.\n           \n            Find a polynomial   such that  .\n            Determine exactly how many such polynomials   have the property that  .\n            Explain your reasoning.\n           \n            What specific property does a basis have that ensures the responses to parts (c) and (d) will always be the same?\n           \n    Recall that there is exactly one way to write a vector as a linear combination of basis vectors,\n    so there is only one coordinate vector of a given vector with respect to a basis.\n    Therefore, the coordinate vector of any vector is well-defined.\n   The Coordinate Transformation coordinate transformation \n    We have seen that both   and   are vector spaces of dimension 2.\n    Moreover, we can pair the polynomial\n      in   with the vector\n      in   as the coordinate vector of\n      with respect to the standard basis for  .\n    In this way we can see that the vector space  \n     looks \n    like the vector space  .\n    In fact, using this same idea,\n    we can show that any vector space of dimension  \n     looks \n    like  .\n    The key idea to make this work is the coordinate transformation.\n   coordinate transformation coordinate transformation coordinate transformation \n    Coordinate transformations have several useful properties that allow us to use them to compare the structure of vector spaces.\n   \n      Let   and   in  .\n      Suppose we know   is a basis of  .\n      Let   for   in  .\n     \n            What are   and  ?\n           \n            Find  .\n           \n            What is  ?\n           \n            What is the relationship between\n              and  ?\n           \n            Show that if   is any scalar,\n            then  .\n           \n            Where have we seen functions with these properties before?\n           \n     \n    shows that coordinate transformations behave in ways similar to matrix transformations.\n    Recall that if   is an   matrix, then\n     \n    for any vectors   and   in   and any scalar  .\n    Because of these properties,\n    the transformation   preserves linear combinations.\n    That is, if  ,  ,  ,\n    and   are any vectors in   and  ,\n     ,  ,   are any scalars, then\n     .\n   \n     \n    contains the essential ideas to show that if   is a coordinate transformation from a vector space   with basis   to  , then\n     \n    for any vectors   and   in   and any scalar  .\n    The fact that any coordinate transformation satisfies these properties is contained in the following theorem.\n   \n        If a vector space   has a basis   of   vectors,\n        then the coordinate mapping\n          defined by   satisfies\n         \n             \n                and\n             \n           \n             \n               \n             \n           \n       \n        for any vectors   and   in   and any scalar  .\n       \n      Let  ,  ,  ,\n      and   be vectors that form a basis   for a vector space  .\n      To verify the first property,\n      let   and   be two arbitrary vectors from  .\n      We will show that   for these two vectors.\n      Consider  .\n      If  ,\n      then the fact that   implies that\n       .\n     \n      Similarly, if  ,\n      then   implies that\n       .\n     \n      We then obtain\n       .\n     \n      Thus, by definition of   again,\n       .\n     \n      To show that  , note that\n       .\n     \n      The proof of the second property is left for the exercises.\n     \n     \n    can be extended to any linear combination of vectors by mathematical induction.\n    That is, if   is the coordinate mapping defined by\n      for some basis  ,\n    and if  ,  ,  ,\n      are vectors in an  -dimensional vector space   with basis  ,\n    and  ,  ,  ,   are scalars,\n    then  .\n    In other words,\n     .\n   \n    In other words,\n    coordinate transformations take linear combinations of vectors in   to linear combinations of vectors in  .\n   \n    In essence, a coordinate transformation makes an identification of any vector space of dimension   with  .\n    One of the driving forces behind this identification is the fact that,\n    just as in  ,\n    if a vector space   has a basis of   elements,\n    then any basis for   will have exactly   elements.\n    The fact that any coordinate transformation from a vector space   to   is linear means that elements in   behave the same way with respect to addition and multiplication by scalars in   as do their images in   under the coordinate transformation.\n    To make this identification complete,\n    there are still two questions to address.\n    First, given a coordinate transformation   from a vector space   to  ,\n    is is possible for two vectors in   to have the same image in  \n    (in other words, is   one-to-one),\n    and is every vector in   the image of some vector in   under   (in other words,\n    does   map   onto  )?\n    If the coordinate transformations are one-to-one and onto, then,\n    in essence,\n    any vector space of dimension   is just a copy of  .\n    As a result,\n    to understand any vector space with a basis of   vectors,\n    it is enough to understand  .\n   \n      Let   be a vector space with an ordered basis  .\n      Then   maps   into  .\n      We want to show that   maps   onto  .\n      Recall that a function   from a set   to a set   is onto if for any element   in  ,\n      there is an element   in   such that  .\n      Let   be a vector in  .\n      Must there be a vector   in   so that  ?\n      If so, find such a vector.\n      If not, explain why not.\n     \n     \n    shows that a coordinate transformation maps an  -dimensional vector space   onto  .\n    What's more, any coordinate transformation is also one-to-one\n    (the proof that   is one-to-one is left for the exercises).\n    We summarize these results in the following theorem.\n   \n        If a vector space   has a basis   of   vectors,\n        then the coordinate mapping   defined by\n          is both one-to-one and onto.\n       \n     \n    and  \n    show that a coordinate transformation from an  -dimensional vector space   with basis   to   provides an identification of   with  ,\n    where a vector   in   is identified with the vector  .\n    This identification allows us to transfer questions\n    (linear dependence, independence, span)\n    in   to   where we can apply our knowledge of matrices to answer the questions.\n   \n      Let   and let  .\n      Let  .\n     \n            Find each of  ,  ,\n             , and  .\n           \n            Are the vectors  ,\n             ,  ,\n            and   linearly independent or dependent?\n            Explain.\n            If the vectors are linearly dependent,\n            write one of the vectors as a linear combination of the others.\n           \n            The coordinate transformation identifies the vectors in\n              with their coordinate vectors in  .\n            Use that information to determine if   is a linearly independent or dependent set.\n            If dependent,\n            write one of the vectors in   as a linear combination of the others.\n           Examples \n    What follows are worked examples that use the concepts from this section.\n   \n              Find the coordinate vector of   with respect to the ordered basis   in the indicated vector space.\n             \n                      in   with  \n                   \n                    Find the coordinate vector of   with respect to the ordered basis   in the indicated vector space.\n                   \n                    We need to write   as a linear combination of   and  .\n                    If  ,\n                    then equating coefficients of like power terms yields the equations\n                      and  .\n                    The solution to this system is   and  ,\n                    so  .\n                   \n                      in\n                      with  \n                   \n                    Find the coordinate vector of   with respect to the ordered basis   in the indicated vector space.\n                   \n                    We need to write   as a linear combination of the vectors in  .\n                    If\n                     ,\n                    equating corresponding components produces the system\n                     \n                    The solution to this system is  ,\n                     ,  ,\n                    and  , so  .\n                   \n              Find the vector   in the indicated vector space   given the basis   of   and  .\n             \n                     ,  ,\n                     \n                   \n                    Find the vector   in the indicated vector space   given the basis   of   and  .\n                   \n                    Since  , it follows that\n                     .\n                   \n                     ,\n                      as a subspace of  ,\n                     \n                   \n                    Find the vector   in the indicated vector space   given the basis   of   and  .\n                   \n                    Since  , it follows that\n                     .\n                   \n        Let  ,  ,\n         ,  ,\n        and   be in  .\n        Also, let   be the standard basis for  .\n       \n              Find  ,  ,\n               ,  ,\n              and  .\n             \n              The coordinate vectors of a polynomial with respect to the standard basis in   is found by just reading off the coefficients of the polynomial.\n              So\n               .\n              \n             \n              Use the result of part (a) to explain why\n                is a basis for  .\n             \n              Let\n               .\n              Since the coordinate transformation is one-to-one and onto,\n              the two sets   and   are either both linearly dependent or linearly independent.\n              Technology shows that the reduced row echelon form of\n               \n              is  ,\n              so the sets   and   are both linearly linearly independent.\n              Since  ,\n              it follows that any linearly independent set of five vectors in   is a basis for  .\n              Therefore, the set   is a basis for  .\n             \n              Let  .\n              Find  .\n             \n              Similar to part (a),\n              we have  .\n             \n              Use the coordinate vectors in parts (a) and (c) to write   as a linear combination of  ,\n               ,  ,\n               , and  \n             \n              Technology shows that the reduced row echelon form of the augmented matrix\n               \n              is\n               .\n              So\n               \n              and\n               .\n             Summary \n    The key idea in this handout is the coordinate vector with respect to a basis.\n     \n         \n          If   is a basis for a vector space  ,\n          then the coordinate vector of   in   with respect to   is the vector\n           ,\n          where\n           .\n         \n       \n         \n          The coordinate transformation\n            is a one-to-one and onto transformation from an  -dimensional vector space   to   which preserves linear combinations.\n         \n       \n         \n          The coordinate transformation\n            allows us to translate problems in arbitrary vector spaces to   where we have already developed tools to solve the problems.\n         \n       \n   \n        Let   be a basis of the subspace defined by the equation  .\n        Find the coordinates of the vector\n          with respect to the basis  .\n       \n         .\n       \n        Given basis   of  ,\n       \n              For which   in   is  ?\n             \n              Determine coordinates of   in   with respect to basis  .\n             \n        Find two different bases   and   of   so that  ,\n        where  .\n       \n        Two such bases are   and  .\n       \n        If   and\n          with respect to some basis  ,\n        where   and  ,\n        what are the coordinates of  ?\n       \n        If   and\n          with respect to some basis  ,\n        where   and  ,\n        what are the vectors in  ?\n       \n         .\n       \n        Let   be a basis for a vector space  .\n        Describe how the coordinates of a vector with respect to   will change if   is replaced with  .\n       \n        Let   in  .\n       \n              Show that   is a basis for  .\n             \n              Show that   is linearly independent.\n             \n              Let  ,\n               , and   in  .\n             \n                    Find  ,  ,\n                    and  .\n                   \n                     ,\n                     ,\n                    and  .\n                   \n                    Use the coordinate vectors in part i. to determine if the set\n                      is linearly independent or dependent.\n                   \n                    Row reduce the matrix  .\n                   \n        Let   in  .\n        Let   be the standard basis for  .\n       \n              Calculate  ,\n                and  .\n             \n              Use the coordinate vectors from part (a) to determine if the polynomials  ,\n               ,\n              and   are linearly independent or dependent.\n             \n              Let  .\n              Find  .\n             \n              Use the calculations from parts (a) and (c) to determine if   is in  .\n              If so, write   as a linear combination of the polynomials  ,\n               , and  .\n              If not, explain why not.\n             \n        Let   in  .\n       \n              Show that   is a basis of  .\n             \n              Show that   is linearly independent.\n             \n              Let\n               .\n              Find  .\n             \n               ,\n               ,\n               ,\n              and  .\n             \n              Determine if the set   is linearly dependent or independent.\n             \n              Row reduce  .\n             \n        Let   be a vector space of dimension   and let   be a basis for  .\n        Show that the coordinate transformation   from   to   defined by\n          satisfies  ,\n        where   is the additive identity in  .\n       \n        Prove the second property of  .\n        That is, if a vector space   has a basis of   vectors,\n        then the coordinate mapping\n          defined by   satisfies\n         \n        for any vector   in   and any scalar  .\n       \n        Use the fact that  .\n       \n        Prove  \n        by demonstrating that if   is a vector space with ordered basis  ,\n        then the coordinate mapping\n          defined by   is one-to-one.\n       \n        The coordinate transformation   is one-to-one,\n        so it has an inverse  .\n        Let   be an  -dimensional vector space that has a basis  ,\n        and let   be the coordinate transformation defined by  .\n        Let   be a subset of   and let   in  .\n       \n              Suppose   is in   with  .\n              Write the vector   as a linear combination of the vectors in  .\n              Explain your reasoning and explain how your result shows that   preserves linear combinations.\n             \n              Write   as a linear combination of basis vectors.\n             \n              Now suppose   is in   so that  .\n              Write the vector   as a linear combination of the vectors in  .\n              Explain your reasoning and explain how your result shows that   preserves linear combinations.\n             \n              Apply   to an appropriate vector and use the fact that   is one-to-one.\n             \n        Let   be a subset of an  -dimensional vector space   with basis  .\n        Let   in  .\n       \n              Show that if   is linearly independent in  ,\n              then   is linearly independent in  .\n             \n              Is the converse of part (a) true?\n              That is, if   is linearly independent in  ,\n              must   be linearly independent in  ?\n              Justify your answer.\n             \n              Repeat parts (a) and (b), replacing\n               linearly independent \n              with\n               linearly dependent .\n             \n        Let   be an  -dimensional vector space with basis  ,\n        and let   be a subset of   that spans  .\n        Prove that   spans  .\n       \n        Find a vector   in   such that  .\n       \n        Suppose   is a basis of a vector space   with   elements and   is a basis of   with   elements.\n        Show that the map   which sends every   in   to the vector   in   such that\n          is one-to-one and onto.\n       \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              The coordinates of a non-zero vector cannot be the same in the coordinate systems defined by two different bases.\n             \n              F\n             True\/False \n              The coordinate vector of the zero vector with respect to any basis is always the zero vector.\n             True\/False \n              If   is a   dimensional subset of an   dimensional vector space  ,\n              and   is a basis of  ,\n              then   is a vector in   for any   in  .\n             \n              F\n             True\/False \n              The order of vectors in a basis do not affect the coordinates of vectors with respect to this basis.\n             True\/False \n              If   is a coordinate transformation from a vector space   with basis   to  ,\n              then the vector   is unique to the vector   in  .\n             \n              T\n             True\/False \n              If   is a coordinate transformation from a vector space   with basis   to  ,\n              then there is always a vector   in   that maps to any vector   in  .\n             True\/False \n              A coordinate transformation from a vector space   with basis   to   always maps the additive inverse of a vector   in   to the additive inverse of the vector   in  .\n             \n              T\n             True\/False \n              A coordinate transformation provides a unique identification of vectors in an  -dimensional vector space with vectors in   in a way that preserves the algebraic structure of the spaces.\n             True\/False \n              If the coordinate vector of   in a vector space   is  ,\n              then the coordinate vector of   is  .\n             \n              T\n             Project: Finding Formulas for Sums of Powers \n    One way to derive formulas for sums of powers of whole numbers is to use different bases and coordinate vectors.\n    One basis that will be useful is a basis of polynomials created by the binomial coefficients.\n    Recall that the binomial coefficient   is equal to\n     ,\n    with   if  .\n   \n    The binomial coefficient can be rewritten in a way to make it applicable to polynomials as\n     .\n   \n    With this representation of  ,\n    we can define a new polynomial in   of degree   as\n     \n    with  .\n    For example,\n     .\n   \n    These\n     generalized binomial coefficients \n    appear in Newton's generalized binomial theorem.\n   \n    Two facts will make these polynomials useful for our sums.\n   \n      Our polynomials   are defined in terms of binomial coefficients.\n      A useful identity will relate sums of binomial coefficients to other binomial coefficients.\n      This identity, called the  hockey-stick identity \n      after the way it can be visualized on Pascal's triangle,\n      is as follows:\n       \n      for positive integers  .\n      Use the definition of the binomial coefficients and some algebra to verify the hockey-stick identity.\n     \n    The second useful fact about our polynomials   is that they form a basis for  .\n   \n            Let   be a positive integer.\n            Explain why   and  .\n           \n            Let  .\n            Show that   is a basis for  . (Hint: Let  ,\n             ,\n             ,   be scalars and consider the equation\n             .\n            Evaluate this equation at  ,  ,\n             ,   and use the result of part (a).)\n           \n    Now we have the tools we need to derive our formulas.\n    To simplify computations,\n    we will change coordinates to the standard basis   for  .\n   \n    We will illustrate the process of deriving formulas for our sums with the sum  .\n    We want to write   as a linear combination of the vectors in   so that we can utilize the hockey-stick identity.\n    In this case, by definition we have  .\n    It follows by the hockey-stick identity and the fact that   that\n     .\n   \n    This is exactly the formula we saw at the beginning of this section.\n    The cases for sums of higher powers work the same way,\n    but we will need to do a little more work to write   in terms of the basis vectors in  .\n   \n      Consider the sum  .\n      We want to write   as a linear combination of  ,\n        and  .\n      To do so, we will use the coordinate vectors with respect to   and do our work in  .\n     \n            Find  ,\n             ,  , and  .\n           \n            Use the coordinate vectors from part (a) to write   as a linear combination of the vectors in  .\n           \n            Use the result of part (b) and,\n            the hockey-stick identity,\n            and the fact that   to find a formula for  .\n           \n    Deriving formulas for higher powers involves the same process,\n    just with more algebra.\n   \n      Use the process outlined in  \n      to derive formulas for the following sums.\n     \n             \n           \n             \n           "
},
{
  "id": "objectives-34",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#objectives-34",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            How do we find the coordinate vector of a vector   with respect to a basis  ?\n           \n         \n           \n            How can we visualize coordinate systems?\n           \n         \n           \n            How do we identify a vector space of dimension   with  ?\n           \n         \n           \n            What properties does the coordinate transformation   have?\n           \n         \n           \n            How are the coordinates of a vector with respect to a basis related to its coordinates with respect to the standard basis?\n           \n         "
},
{
  "id": "definition-75",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#definition-75",
  "type": "Definition",
  "number": "34.1",
  "title": "",
  "body": "coordinate vector with respect to a basis coordinates with respect to a basis coordinate vector coordinates of the vector with respect to the basis "
},
{
  "id": "pa_5_d",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#pa_5_d",
  "type": "Preview Activity",
  "number": "34.1",
  "title": "",
  "body": "\n      Let  ,\n       ,  ,  ,\n      and let   and  .\n     \n            Show that   and   are bases for  .\n           \n            Let  .\n            What is  ?\n           \n            Find the coordinate vector of\n              with respect to both   and  .\n           \n            Find a polynomial   such that  .\n            Determine exactly how many such polynomials   have the property that  .\n            Explain your reasoning.\n           \n            Find a polynomial   such that  .\n            Determine exactly how many such polynomials   have the property that  .\n            Explain your reasoning.\n           \n            What specific property does a basis have that ensures the responses to parts (c) and (d) will always be the same?\n           "
},
{
  "id": "p-5924",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#p-5924",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "coordinate transformation "
},
{
  "id": "definition-76",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#definition-76",
  "type": "Definition",
  "number": "34.2",
  "title": "",
  "body": "coordinate transformation coordinate transformation "
},
{
  "id": "act_5_d_2",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#act_5_d_2",
  "type": "Activity",
  "number": "34.2",
  "title": "",
  "body": "\n      Let   and   in  .\n      Suppose we know   is a basis of  .\n      Let   for   in  .\n     \n            What are   and  ?\n           \n            Find  .\n           \n            What is  ?\n           \n            What is the relationship between\n              and  ?\n           \n            Show that if   is any scalar,\n            then  .\n           \n            Where have we seen functions with these properties before?\n           "
},
{
  "id": "thm_5_d_5",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#thm_5_d_5",
  "type": "Theorem",
  "number": "34.3",
  "title": "",
  "body": "\n        If a vector space   has a basis   of   vectors,\n        then the coordinate mapping\n          defined by   satisfies\n         \n             \n                and\n             \n           \n             \n               \n             \n           \n       \n        for any vectors   and   in   and any scalar  .\n       \n      Let  ,  ,  ,\n      and   be vectors that form a basis   for a vector space  .\n      To verify the first property,\n      let   and   be two arbitrary vectors from  .\n      We will show that   for these two vectors.\n      Consider  .\n      If  ,\n      then the fact that   implies that\n       .\n     \n      Similarly, if  ,\n      then   implies that\n       .\n     \n      We then obtain\n       .\n     \n      Thus, by definition of   again,\n       .\n     \n      To show that  , note that\n       .\n     \n      The proof of the second property is left for the exercises.\n     "
},
{
  "id": "act_5_d_6",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#act_5_d_6",
  "type": "Activity",
  "number": "34.3",
  "title": "",
  "body": "\n      Let   be a vector space with an ordered basis  .\n      Then   maps   into  .\n      We want to show that   maps   onto  .\n      Recall that a function   from a set   to a set   is onto if for any element   in  ,\n      there is an element   in   such that  .\n      Let   be a vector in  .\n      Must there be a vector   in   so that  ?\n      If so, find such a vector.\n      If not, explain why not.\n     "
},
{
  "id": "thm_5_d_6",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#thm_5_d_6",
  "type": "Theorem",
  "number": "34.4",
  "title": "",
  "body": "\n        If a vector space   has a basis   of   vectors,\n        then the coordinate mapping   defined by\n          is both one-to-one and onto.\n       "
},
{
  "id": "act_5_d_7",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#act_5_d_7",
  "type": "Activity",
  "number": "34.4",
  "title": "",
  "body": "\n      Let   and let  .\n      Let  .\n     \n            Find each of  ,  ,\n             , and  .\n           \n            Are the vectors  ,\n             ,  ,\n            and   linearly independent or dependent?\n            Explain.\n            If the vectors are linearly dependent,\n            write one of the vectors as a linear combination of the others.\n           \n            The coordinate transformation identifies the vectors in\n              with their coordinate vectors in  .\n            Use that information to determine if   is a linearly independent or dependent set.\n            If dependent,\n            write one of the vectors in   as a linear combination of the others.\n           "
},
{
  "id": "example-71",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#example-71",
  "type": "Example",
  "number": "34.5",
  "title": "",
  "body": "\n              Find the coordinate vector of   with respect to the ordered basis   in the indicated vector space.\n             \n                      in   with  \n                   \n                    Find the coordinate vector of   with respect to the ordered basis   in the indicated vector space.\n                   \n                    We need to write   as a linear combination of   and  .\n                    If  ,\n                    then equating coefficients of like power terms yields the equations\n                      and  .\n                    The solution to this system is   and  ,\n                    so  .\n                   \n                      in\n                      with  \n                   \n                    Find the coordinate vector of   with respect to the ordered basis   in the indicated vector space.\n                   \n                    We need to write   as a linear combination of the vectors in  .\n                    If\n                     ,\n                    equating corresponding components produces the system\n                     \n                    The solution to this system is  ,\n                     ,  ,\n                    and  , so  .\n                   \n              Find the vector   in the indicated vector space   given the basis   of   and  .\n             \n                     ,  ,\n                     \n                   \n                    Find the vector   in the indicated vector space   given the basis   of   and  .\n                   \n                    Since  , it follows that\n                     .\n                   \n                     ,\n                      as a subspace of  ,\n                     \n                   \n                    Find the vector   in the indicated vector space   given the basis   of   and  .\n                   \n                    Since  , it follows that\n                     .\n                   "
},
{
  "id": "example-72",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#example-72",
  "type": "Example",
  "number": "34.6",
  "title": "",
  "body": "\n        Let  ,  ,\n         ,  ,\n        and   be in  .\n        Also, let   be the standard basis for  .\n       \n              Find  ,  ,\n               ,  ,\n              and  .\n             \n              The coordinate vectors of a polynomial with respect to the standard basis in   is found by just reading off the coefficients of the polynomial.\n              So\n               .\n              \n             \n              Use the result of part (a) to explain why\n                is a basis for  .\n             \n              Let\n               .\n              Since the coordinate transformation is one-to-one and onto,\n              the two sets   and   are either both linearly dependent or linearly independent.\n              Technology shows that the reduced row echelon form of\n               \n              is  ,\n              so the sets   and   are both linearly linearly independent.\n              Since  ,\n              it follows that any linearly independent set of five vectors in   is a basis for  .\n              Therefore, the set   is a basis for  .\n             \n              Let  .\n              Find  .\n             \n              Similar to part (a),\n              we have  .\n             \n              Use the coordinate vectors in parts (a) and (c) to write   as a linear combination of  ,\n               ,  ,\n               , and  \n             \n              Technology shows that the reduced row echelon form of the augmented matrix\n               \n              is\n               .\n              So\n               \n              and\n               .\n             "
},
{
  "id": "exercise-348",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#exercise-348",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Let   be a basis of the subspace defined by the equation  .\n        Find the coordinates of the vector\n          with respect to the basis  .\n       \n         .\n       "
},
{
  "id": "exercise-349",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#exercise-349",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Given basis   of  ,\n       \n              For which   in   is  ?\n             \n              Determine coordinates of   in   with respect to basis  .\n             "
},
{
  "id": "exercise-350",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#exercise-350",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Find two different bases   and   of   so that  ,\n        where  .\n       \n        Two such bases are   and  .\n       "
},
{
  "id": "exercise-351",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#exercise-351",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        If   and\n          with respect to some basis  ,\n        where   and  ,\n        what are the coordinates of  ?\n       "
},
{
  "id": "exercise-352",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#exercise-352",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        If   and\n          with respect to some basis  ,\n        where   and  ,\n        what are the vectors in  ?\n       \n         .\n       "
},
{
  "id": "exercise-353",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#exercise-353",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Let   be a basis for a vector space  .\n        Describe how the coordinates of a vector with respect to   will change if   is replaced with  .\n       "
},
{
  "id": "exercise-354",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#exercise-354",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        Let   in  .\n       \n              Show that   is a basis for  .\n             \n              Show that   is linearly independent.\n             \n              Let  ,\n               , and   in  .\n             \n                    Find  ,  ,\n                    and  .\n                   \n                     ,\n                     ,\n                    and  .\n                   \n                    Use the coordinate vectors in part i. to determine if the set\n                      is linearly independent or dependent.\n                   \n                    Row reduce the matrix  .\n                   "
},
{
  "id": "exercise-355",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#exercise-355",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Let   in  .\n        Let   be the standard basis for  .\n       \n              Calculate  ,\n                and  .\n             \n              Use the coordinate vectors from part (a) to determine if the polynomials  ,\n               ,\n              and   are linearly independent or dependent.\n             \n              Let  .\n              Find  .\n             \n              Use the calculations from parts (a) and (c) to determine if   is in  .\n              If so, write   as a linear combination of the polynomials  ,\n               , and  .\n              If not, explain why not.\n             "
},
{
  "id": "exercise-356",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#exercise-356",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        Let   in  .\n       \n              Show that   is a basis of  .\n             \n              Show that   is linearly independent.\n             \n              Let\n               .\n              Find  .\n             \n               ,\n               ,\n               ,\n              and  .\n             \n              Determine if the set   is linearly dependent or independent.\n             \n              Row reduce  .\n             "
},
{
  "id": "exercise-357",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#exercise-357",
  "type": "Exercise",
  "number": "10",
  "title": "",
  "body": "\n        Let   be a vector space of dimension   and let   be a basis for  .\n        Show that the coordinate transformation   from   to   defined by\n          satisfies  ,\n        where   is the additive identity in  .\n       "
},
{
  "id": "exercise-358",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#exercise-358",
  "type": "Exercise",
  "number": "11",
  "title": "",
  "body": "\n        Prove the second property of  .\n        That is, if a vector space   has a basis of   vectors,\n        then the coordinate mapping\n          defined by   satisfies\n         \n        for any vector   in   and any scalar  .\n       \n        Use the fact that  .\n       "
},
{
  "id": "exercise-359",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#exercise-359",
  "type": "Exercise",
  "number": "12",
  "title": "",
  "body": "\n        Prove  \n        by demonstrating that if   is a vector space with ordered basis  ,\n        then the coordinate mapping\n          defined by   is one-to-one.\n       "
},
{
  "id": "exercise-360",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#exercise-360",
  "type": "Exercise",
  "number": "13",
  "title": "",
  "body": "\n        The coordinate transformation   is one-to-one,\n        so it has an inverse  .\n        Let   be an  -dimensional vector space that has a basis  ,\n        and let   be the coordinate transformation defined by  .\n        Let   be a subset of   and let   in  .\n       \n              Suppose   is in   with  .\n              Write the vector   as a linear combination of the vectors in  .\n              Explain your reasoning and explain how your result shows that   preserves linear combinations.\n             \n              Write   as a linear combination of basis vectors.\n             \n              Now suppose   is in   so that  .\n              Write the vector   as a linear combination of the vectors in  .\n              Explain your reasoning and explain how your result shows that   preserves linear combinations.\n             \n              Apply   to an appropriate vector and use the fact that   is one-to-one.\n             "
},
{
  "id": "exercise-361",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#exercise-361",
  "type": "Exercise",
  "number": "14",
  "title": "",
  "body": "\n        Let   be a subset of an  -dimensional vector space   with basis  .\n        Let   in  .\n       \n              Show that if   is linearly independent in  ,\n              then   is linearly independent in  .\n             \n              Is the converse of part (a) true?\n              That is, if   is linearly independent in  ,\n              must   be linearly independent in  ?\n              Justify your answer.\n             \n              Repeat parts (a) and (b), replacing\n               linearly independent \n              with\n               linearly dependent .\n             "
},
{
  "id": "exercise-362",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#exercise-362",
  "type": "Exercise",
  "number": "15",
  "title": "",
  "body": "\n        Let   be an  -dimensional vector space with basis  ,\n        and let   be a subset of   that spans  .\n        Prove that   spans  .\n       \n        Find a vector   in   such that  .\n       "
},
{
  "id": "ex_5_d_T_VW",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#ex_5_d_T_VW",
  "type": "Exercise",
  "number": "16",
  "title": "",
  "body": "\n        Suppose   is a basis of a vector space   with   elements and   is a basis of   with   elements.\n        Show that the map   which sends every   in   to the vector   in   such that\n          is one-to-one and onto.\n       "
},
{
  "id": "exercise-364",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#exercise-364",
  "type": "Exercise",
  "number": "17",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              The coordinates of a non-zero vector cannot be the same in the coordinate systems defined by two different bases.\n             \n              F\n             True\/False \n              The coordinate vector of the zero vector with respect to any basis is always the zero vector.\n             True\/False \n              If   is a   dimensional subset of an   dimensional vector space  ,\n              and   is a basis of  ,\n              then   is a vector in   for any   in  .\n             \n              F\n             True\/False \n              The order of vectors in a basis do not affect the coordinates of vectors with respect to this basis.\n             True\/False \n              If   is a coordinate transformation from a vector space   with basis   to  ,\n              then the vector   is unique to the vector   in  .\n             \n              T\n             True\/False \n              If   is a coordinate transformation from a vector space   with basis   to  ,\n              then there is always a vector   in   that maps to any vector   in  .\n             True\/False \n              A coordinate transformation from a vector space   with basis   to   always maps the additive inverse of a vector   in   to the additive inverse of the vector   in  .\n             \n              T\n             True\/False \n              A coordinate transformation provides a unique identification of vectors in an  -dimensional vector space with vectors in   in a way that preserves the algebraic structure of the spaces.\n             True\/False \n              If the coordinate vector of   in a vector space   is  ,\n              then the coordinate vector of   is  .\n             \n              T\n             "
},
{
  "id": "act_hockey_stick",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#act_hockey_stick",
  "type": "Project Activity",
  "number": "34.5",
  "title": "",
  "body": "\n      Our polynomials   are defined in terms of binomial coefficients.\n      A useful identity will relate sums of binomial coefficients to other binomial coefficients.\n      This identity, called the  hockey-stick identity \n      after the way it can be visualized on Pascal's triangle,\n      is as follows:\n       \n      for positive integers  .\n      Use the definition of the binomial coefficients and some algebra to verify the hockey-stick identity.\n     "
},
{
  "id": "act_binomial_basis",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#act_binomial_basis",
  "type": "Project Activity",
  "number": "34.6",
  "title": "",
  "body": "\n            Let   be a positive integer.\n            Explain why   and  .\n           \n            Let  .\n            Show that   is a basis for  . (Hint: Let  ,\n             ,\n             ,   be scalars and consider the equation\n             .\n            Evaluate this equation at  ,  ,\n             ,   and use the result of part (a).)\n           "
},
{
  "id": "act_sum_2",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#act_sum_2",
  "type": "Project Activity",
  "number": "34.7",
  "title": "",
  "body": "\n      Consider the sum  .\n      We want to write   as a linear combination of  ,\n        and  .\n      To do so, we will use the coordinate vectors with respect to   and do our work in  .\n     \n            Find  ,\n             ,  , and  .\n           \n            Use the coordinate vectors from part (a) to write   as a linear combination of the vectors in  .\n           \n            Use the result of part (b) and,\n            the hockey-stick identity,\n            and the fact that   to find a formula for  .\n           "
},
{
  "id": "act_sums_higher",
  "level": "2",
  "url": "chap_coordinate_vectors_vector_spaces.html#act_sums_higher",
  "type": "Project Activity",
  "number": "34.8",
  "title": "",
  "body": "\n      Use the process outlined in  \n      to derive formulas for the following sums.\n     \n             \n           \n             \n           "
},
{
  "id": "chap_inner_products",
  "level": "1",
  "url": "chap_inner_products.html",
  "type": "Section",
  "number": "35",
  "title": "Inner Product Spaces",
  "body": "Inner Product Spaces \n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            What is an inner product space?\n           \n         \n           \n            What is an orthogonal set in an inner product space?\n           \n         \n           \n            What is an orthogonal basis for an inner product space?\n           \n         \n           \n            How do we find the coordinate vector for a vector in an inner product space relative to an orthogonal basis for the space?\n           \n         \n           \n            What is the projection of a vector orthogonal to a subspace and why are such orthogonal projections important?\n           \n         Application: Fourier Series local Introduction \n    In  \n    we were introduced to inner products in  .\n    The concept of an inner product can be extended to vector spaces,\n    as we will see in this section.\n    This will allow us to measure lengths and angles between vectors and define orthogonality in certain vector spaces.\n   \n    Recall that an inner product on   assigns to each pair of vectors   and   the scalar  .\n    Thus, an inner product on   defines a mapping from   to  .\n    Recall also that an inner product on   is commutative,\n    distributes over vector addition,\n    and respects scalar multiplication,\n    and the inner product of a vector in   by itself is always non-negative and is equal to 0 only when the vector is the zero vector.\n    We will investigate this ideas in vector spaces in  .\n   \n      Consider the vector space   of polynomials of degree less than or equal to   with real coefficients.\n      Define a mapping from   to   by\n       .\n     \n            Calculate  .\n           \n            If   and   are in  , is it true that\n             \n            Verify your answer.\n           \n            If  ,  ,\n            and   are in  , is it true that\n             \n            Verify your answer.\n           \n            If   and   are in   and   is a scalar,\n            is it true that\n             \n            Verify your answer.\n           \n            If   is in  ,\n            must it be the case that  ?\n            When is  ?\n           Inner Product Spaces \n    As we saw in  ,\n    we can define a mapping from\n      to   that has the same properties of inner products on  .\n    So we can extend the definition of inner product to arbitrary vector spaces.\n   inner product inner product space inner product inner product space \n      Consider the mapping   from\n        to   defined by\n       .\n     \n            Show that this mapping satisfies the second property of an inner product.\n           \n            Although this mapping satisfies the first three properties of an inner product,\n            show that this mapping does not satisfy the fourth property and so is not an inner product.\n           \n    Since inner products in vector spaces are defined in the same way as inner products in  ,\n    they will satisfy the same properties.\n    Some of these properties are summarized in the following theorem.\n   \n        Let   be an inner product on   and let  ,\n        and   be vectors in   and   a scalar.\n        Then\n         \n             \n               \n             \n           \n             \n               \n             \n           \n             \n               \n             \n           \n             \n               \n             \n           \n       \n    One special inner product is indicated in  .\n    Recall that   is the vector space of continuous functions on the closed interval  .\n    Let   map from\n      to   be defined by\n     .\n   \n    The verification that this mapping is an inner product is left to the exercises.\n   The Length of a Vector \n    We can use inner products to define the length of any vector in an inner product space and the distance between two vectors in an inner product space.\n    The idea comes right from the relationship between lengths of vectors in   and inner products on  \n    (compare to  ).\n   length of a vector in an inner product space length magnitude norm unit vector in an inner product space unit vector \n    We can find a unit vector in the direction of a nonzero vector   in an inner product space by dividing by the norm of the vector.\n    That is, the vector   is a unit vector in the direction of the vector  ,\n    provided that   is not zero.\n   \n    We define the distance between vectors   and   in an inner product space in the same way we defined distance using the dot product\n    (compare to  ).\n   distance between vectors in an inner product space distance between trace \n      If   and   are in the space   of\n        matrices with real entries,\n      we define the mapping   from   to   by\n       .\n     Frobenius \n            Find the length of the vectors   and   using the Frobenius inner product.\n           \n            Find the distance between   and   using the Frobenius inner product.\n           Orthogonality in Inner Product Spaces \n    We defined orthogonality in   using inner products in  \n    (see  )\n    and the angle between vectors.\n    We can extend those ideas to any inner product space.\n   angle   between\n      and  \n    With the angle between vectors in mind,\n    we can define orthogonal vectors in an inner product space.\n   orthogonal vectors in inner product spaces orthogonal \n    Note that this defines the zero vector to be orthogonal to every vector.\n   \n      In this activity we use the Frobenius inner product\n      (see  ).\n      Let   and\n        in  .\n     \n            Find a nonzero vector in   orthogonal to  .\n           \n            Find the angle between   and  .\n           \n    Using orthogonality we can generalize the notions of orthogonal sets and bases,\n    orthonormal bases and orthogonal complements we defined in   to all inner product spaces in a natural way.\n   Orthogonal and Orthonormal Bases in Inner Product Spaces \n    As we did with inner products in  ,\n    we define an orthogonal set to be one in which all of the vectors in the set are orthogonal to each other\n    (compare to  ).\n   orthogonal set in an inner product space orthogonal set \n    As in  ,\n    an orthogonal set of nonzero vectors is always linearly independent.\n    The proof is similar to that of   and is left to the exercises.\n   \n        Let   be a set of nonzero orthogonal vectors in an inner product space.\n        Then the vectors  ,  ,\n         ,   are linearly independent.\n       \n    A basis that is also an orthogonal set is given a special name\n    (compare to  ).\n   orthogonal basis in an inner product space orthogonal basis \n    Using inner products in  ,\n    we saw that the representation of a vector as a linear combination of vectors in an orthogonal basis was quite elegant.\n    The same is true in any inner product space.\n    To see this,\n    let   be an orthogonal basis for a subspace   of an inner product space and let   be any vector in  .\n    We know that\n     \n    for some scalars  ,  ,\n     ,  .\n    If  , then,\n    using inner product properties and the orthogonality of the vectors  , we have\n     .\n   \n    So\n     .\n   \n    Thus, we can calculate each weight individually with two simple inner product calculations.\n   \n    We summarize this discussion in the next theorem\n    (compare to  ).\n   \n        Let   be an orthogonal basis for a subspace of an inner product space.\n        Let   be a vector in  .\n        Then\n         .\n       \n      Let  ,  ,\n      and   be vectors in the inner product space   with inner product defined by  .\n      Let  .\n      You may assume that   is an orthogonal basis for  .\n      Let  .\n      Find the weight   so that  .\n      Use technology as appropriate to evaluate any integrals.\n     \n    The decomposition   is even simpler if   for each  .\n    Recall that\n     ,\n    so the condition   implies that the vector   has norm 1.\n    As with inner products in  ,\n    an orthogonal basis with this additional condition is given a special name\n    (compare to  ).\n   orthonormal basis in an inner product space orthonormal basis \n    If   is an orthonormal basis for a subspace   of an inner product space and   is a vector in  ,\n    then   becomes\n     .\n   \n    Recall that we can construct an orthonormal basis from an orthogonal basis by dividing each basis vector by its magnitude.\n   Orthogonal Projections onto Subspaces \n    In  \n    we saw how to project a vector   in   onto a subspace   of  .\n    The same process works for vectors in any inner product space.\n   orthogonal projection onto a subspace projection orthogonal to a subspace orthogonal projection of   onto  projection of   orthogonal to  \n    The notation   indicates that we expect this vector to be orthogonal to every vector in  .\n   \n      In  \n      we showed that in the inner product space   using the dot product as inner product,\n      if   is a subspace of   and   is in  ,\n      then   is orthogonal to every vector in  .\n      In this activity we verify that same fact in an inner product space.\n      That is, assume that   is an orthogonal basis for a subspace   of an inner product space   and   is a vector in  .\n      Follow the indicated steps to show that\n        is orthogonal to every vector in  .\n     \n            Let   be the projection of   onto  .\n            Write   in terms of the basis vectors in  .\n           \n            The vector   is the projection of   orthogonal to  .\n            Let   be between   and  .\n            Use the result of part (a) to show that\n              is orthogonal to  .\n             \n            then shows that   is orthogonal to every vector in  .\n           \n     \n    shows that the vector   is orthogonal to vector for  .\n    So, in fact,\n      is the projection of   onto the orthogonal complement of  ,\n    which will be defined shortly.\n   Best Approximations in Inner Product Spaces \n    We have seen,\n    e.g., linear regression to fit a line to a set of data,\n    that we often want to find a vector in a subspace that\n     best \n    approximates a given vector in a vector space.\n    As in  ,\n    the projection of a vector onto a subspace has this important property.\n    That is,   is the vector in   closest to   and therefore the best approximation of   by a vector in  .\n    To see that this is true in any inner product space,\n    we first need a generalization of the Pythagorean Theorem that holds in inner product spaces.\n   Generalized Pythagorean Theorem \n        Let   and   be orthogonal vectors in an inner product space  .\n        Then\n         .\n       \n      Let   and   be orthogonal vectors in an inner product space  .\n      Then\n       .\n     \n    Note that replacing   with   in the theorem also shows that\n      if   and   are orthogonal.\n   \n    Now we will prove that the projection of a vector   onto a subspace   of an inner product space   is the best approximation in   to the vector  .\n   \n        Let   be a subspace of an inner product space   and let   be a vector in  .\n        Then\n         \n        for every vector   in   different from  .\n       \n      Let   be a subspace of an inner product space   and let   be a vector in  .\n      Let   be a vector in  .\n      Now\n       .\n     \n      Since both   and   are in  ,\n      we know that   is in  .\n      Since   is orthogonal to every vector in  ,\n      we have that   is orthogonal to  .\n      We can now use the Generalized Pythagorean Theorem to conclude that\n       .\n     \n      Since  ,\n      it follows that   and\n       .\n     \n      Since norms are nonnegative,\n      we can conclude that   as desired.\n     \n     \n    shows that the distance from\n      to   is less than the distance from any other vector in   to  .\n    So   is the best approximation to   of all the vectors in  .\n   \n    In   using the dot product as inner product,\n    if   and  ,\n    then the square of the error in approximating   by   is given by\n     .\n   least squares approximation \n      The set   is an orthogonal basis for a subspace   of the inner product space   using the inner product  .\n      Find the polynomial in   that is closest to the polynomial\n        and give a numeric estimate of how good this approximation is.\n     Orthogonal Complements \n    If we have a set of vectors   in an inner product space  , we can define the orthogonal complement of   as we did in   (see  ).\n   orthogonal complement in inner product spaces orthogonal complement \n    As we saw in  , to show that a vector is in the orthogonal complement of a subspace, it is enough to show that the vector is orthogonal to every vector in a basis for that subspace. The same is true in any inner product space. The proof is left to the exercises. \n   \n        Let   be a basis for a subspace   of an inner product space  . A vector   in   is orthogonal to every vector in   if and only if   is orthogonal to every vector in  .\n       \n        Consider   with the inner product  .\n       \n          Find   where   is in  .\n         \n          Describe as best you can the orthogonal complement of   in  . Is   in this orthogonal complement? Is  ?\n         \n    As was the case in  , give a subspace   of an inner product space  , any vector in   can be written uniquely as a sum of a vector in   and a vector in  . \n   \n        Let   be an inner product space of dimension  , and let   be a subspace of  . Let   be any vector in  . We will demonstrate that   can be written uniquely as a sum of a vector in   and a vector in  . \n       \n          Explain why   is in  .\n         \n          Explain why   is in  .\n         \n          Explain why   can be written as a sum of vectors, one in   and one in  .\n         \n          Now we demonstrate the uniqueness of this decomposition. Suppose   and  , where   and   are in   and   and   are in  . Show that   and  , so that the representation of   as a sum of a vector in   and a vector in   is unique. (Hint: What is  ?)\n         \n    We summarize the result of  .\n   \n        Let   be a finite dimensional inner product space, and let   be a subspace of  . Any vector in   can be written in a unique way as a sum of a vector in   and a vector in  . \n       \n      is useful in many applications. For example, to compress an image using wavelets, we store the image as a collection of data, then rewrite the data using a succession of subspaces and their orthogonal complements.  This new representation allows us to visualize the data in a way that compression is possible. \n   Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Let   be the inner product space with inner product\n         .\n       \n        Let  ,  ,\n         , and  .\n       \n              Show that the set   is an orthogonal basis for  .\n             \n              All calculations are done by hand or with a computer algebra system,\n              so we leave those details to the reader.\n             \n              If we show that the set   is an orthogonal set,\n              then  \n              shows that   is linearly independent.\n              Since  ,\n              the linearly independent set   that contains four vectors must be a basis for  .\n              To determine if the set   is an orthogonal set,\n              we must calculate the inner products of pairs of distinct vectors in  .\n              Since  ,\n               ,\n               ,\n               ,\n               ,\n              and  ,\n              we conclude that   is an orthogonal basis for  .\n             \n               Use \n              to write the polynomial   as a linear combination of the basis vectors in  .\n             \n              All calculations are done by hand or with a computer algebra system,\n              so we leave those details to the reader.\n             \n              We can write the polynomial\n                as a linear combination of the basis vectors in   as follows:\n               .\n              Now\n               \n              so\n               .\n             \n        Let   be the inner product space   with inner product defined by\n         .\n       \n              Let   be the plane spanned by\n                and   in  .\n              Find the vector in   that is closest to the vector  .\n              Exactly how close is your best approximation to the vector  ?\n             \n              The vector we're looking for is the projection of   onto the plane.\n              A spanning set for the plane is  .\n              Neither vector in   is a scalar multiple of the other,\n              so   is a basis for the plane.\n              Since\n               ,\n              the set   is an orthogonal basis for the plane.\n              The projection of the vector\n                onto the plane spanned by\n                and   is given by\n               .\n              To measure how close close\n                is to  ,\n              we calculate\n               .\n             \n              Express the vector   as the sum of a vector in   and a vector orthogonal to  .\n             \n              If  ,\n              then   is in   and\n               \n              is in  ,\n              and  .\n             Summary \n       \n        An inner product   on a vector space   is a mapping from   satisfying\n         \n             \n                for all   and   in  ,\n             \n           \n             \n                for all  ,\n               , and   in  ,\n             \n           \n             \n                for all  ,\n                in   and  ,\n             \n           \n             \n                for all   in   and\n                if and only if  .\n             \n           \n       \n     \n       \n        An inner product space is a pair  ,\n          where   is a vector space and\n          is an inner product on  .\n       \n     \n       \n        The length of a vector   in an inner product space   is defined to be the real number  .\n       \n     \n       \n        The distance between two vectors   and   in an inner product space   is the scalar  .\n       \n     \n       \n        The angle   between two vectors   and   is the angle which satisfies   and\n         .\n       \n     \n       \n        Two vectors   and   in an inner product space   are orthogonal if  .\n       \n     \n       \n        A subset   of an inner product space is an orthogonal set if\n          for all   in  .\n       \n     \n       \n        A basis for a subspace of an inner product space is an orthogonal basis if the basis is also an orthogonal set.\n       \n     \n       \n        Let   be an orthogonal basis for a subspace of an inner product space  .\n        Let   be a vector in  .\n        Then\n         ,\n        where\n         \n        for each  .\n       \n     \n       \n        An orthogonal basis   for a subspace   of an inner product space   is an orthonormal basis if\n          for each   from 1 to  .\n       \n     \n       \n        If   is an orthogonal basis for   and  , then\n         .\n       \n     \n       \n        The projection of the vector   in an inner product space   onto a subspace   of   is the vector\n         ,\n        where   is an orthogonal basis of  .\n        Projections are important in that\n          is the best approximation of the vector   by a vector in   in the least squares sense.\n       \n     \n       \n        With   as in (a),\n        the projection of   orthogonal to   is the vector\n         .\n        The norm of   provides a measure of how well\n          approximates the vector  .\n       \n     \n       \n        The orthogonal complement of a subset   of an inner product space   is the set\n         .\n       \n     \n        Let   be the set of all continuous real valued functions on the interval  .\n        If   is in  ,\n        we can extend   to a continuous function from   to   by letting   be the function defined by\n         .\n        In this way we can view   as a subset of  ,\n        the vector space of all functions from   to  .\n        Verify that   is a vector space.\n       \n        Use properties of continuous functions.\n       \n        Use the definition of an inner product to determine which of the following defines an inner product on the indicated space.\n        Verify your answers.\n       \n                for\n                and   in  \n             \n                for\n                (where   is the vector space of all continuous functions on the interval  )\n             \n                for\n                (where   is the vector space of all differentiable functions on the interval  )\n             \n                for\n                and   an invertible   matrix\n             \n        We can sometimes visualize an inner product in   or  \n        (or other spaces)\n        by describing the unit circle  , where\n         \n        in that inner product space.\n        For example,\n        in the inner product space   with the dot product as inner product,\n        the unit circle is just our standard unit circle.\n        Inner products, however,\n        can distort this familiar picture of the unit circle.\n        Describe the points on the unit circle   in the inner product space   with inner product\n          using the following steps.\n       \n              Let  .\n              Set up an equation in   and   that is equivalent to the vector equation  .\n             \n               \n             \n              Describe the graph of the equation you found in  .\n              It should have a familiar form.\n              Draw a picture to illustrate.\n              What do you think of calling this graph a\n               circle ?\n             \n              An ellipse centered at the origin with major axis the segment from   to\n                and minor axis the segment from\n                to  .\n             \n        Define   on   by  .\n       \n              Show that   is an inner product.\n             \n              The inner product   can be represented as a matrix transformation  ,\n              where   and   are written as column vectors.\n              Find a matrix   that represents this inner product.\n             \n        This exercise is a generalization of  .\n        Define   on   by\n         \n        for some positive scalars  ,\n         ,  ,  .\n       \n              Show that   is an inner product.\n             \n              Verify the inner product properties.\n              Why is the assumption that the   are positive necessary?\n             \n              The inner product   can be represented as a matrix transformation  ,\n              where   and   are written as column vectors.\n              Find a matrix   that represents this inner product.\n             \n               .\n             \n        Is the sum of two inner products on an inner product space   an inner product on  ?\n        If yes, prove it.\n        If no, provide a counterexample.\n        (By the sum of inner products we mean a function   satisfying\n         \n        for all   and   in  ,\n        where   and\n          are inner products on  .)\n       \n            Does   define an inner product on   for every   matrix  ?\n            Verify your answer.\n           \n            No.\n           \n            If your answer to part (a) is no,\n            are there any types of matrices for which\n              defines an inner product?\n            \n           \n            See  \n            and  .\n           \n            If   is a diagonal matrix with positive diagonal entries.\n           \n        The trace of an   matrix\n          has some useful properties.\n       \n              Show that   for any\n                matrices   and  .\n             \n              Show that   for any\n                matrix   and any scalar  .\n             \n              Show that   for any   matrix.\n             \n        Let   be an inner product space and\n          be two vectors in  .\n       \n              Check that if  ,\n              the Cauchy-Schwarz inequality\n               \n              holds.\n             \n              Evaluate each side of the inequality.\n             \n              Assume  .\n              Let   and  .\n              Use the fact that   to conclude the Cauchy-Schwarz inequality in this case.\n             \n              Write   as an inner product and expand.\n             Frobenius inner product \n        Let   and\n          be two   matrices.\n       \n              Show that if  , then the Frobenius inner product\n              (see  )\n              of   and   is\n               .\n             \n              Expand the inner product.\n             \n              Extend part (a) to the general case.\n              That is, show that for an arbitrary  ,\n               .\n             \n              Expand the inner product.\n             \n              Compare the Frobenius inner product to the scalar product of two vectors.\n             \n              Convert   and   to vectors in\n            whose entries are the entries in the first row followed by the entries in the second row and so on.\n             \n        Let   and let   in  .\n       \n              Show that   is an orthogonal basis for  ,\n              using the dot product as inner product.\n             \n              Explain why the vector   is not in  .\n             \n              Find the vector in   that is closest to  .\n              How close is this vector to  ?\n             \n        Let   be the inner product space with inner product\n         .\n        Let   and let   in  .\n       \n              Show that   is an orthogonal basis for  ,\n              using the given inner product.\n             \n              Compute the inner product of the vectors.\n             \n              Explain why the vector   is not in  .\n             \n              Try to write   in terms of the basis vectors for  .\n             \n              Find the vector in   that is closest to  .\n              How close is this vector to  ?\n             \n               .\n              Approximately 1.41.\n             \n        Let   be the inner product space with inner product\n         .\n        Let   and let   in  .\n       \n              Show that   is an orthogonal basis for  ,\n              using the given inner product.\n             \n              Explain why the polynomial   is not in  .\n             \n              Find the vector in   that is closest to  .\n              How close is this vector to  ?\n             \n        Prove the remaining properties of  .\n        That is, if   is an inner product on a vector space   and  ,\n        and   are vectors in   and   is any scalar, then\n       \n               \n             \n              Use the fact that  .\n             \n               \n             \n              Use the fact that  .\n             \n               \n             \n              Same hint as part (b).\n             \n               \n             \n              Use the fact that  .\n             \n        Prove the following theorem referenced in  .\n        \n         \n              Let   be a basis for a subspace   of an inner product space  .\n              A vector   in   is orthogonal to every vector in   if and only if   is orthogonal to every vector in  .\n             \n       \n        Mimic  .\n       \n        Prove  .\n        \n       \n        Mimic  .\n       \n        Let   be a vector space with basis  .\n        Define   as follows:\n         \n        if   and   in  .\n        (Since the representation of a vector as a linear combination of basis elements is unique,\n        this mapping is well-defined.)\n        Show that   is an inner product on   and conclude that any finite dimensional vector space can be made into an inner product space.\n       \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              An inner product on a vector space   is a function from   to the real numbers.\n             \n              F\n             True\/False \n              If   is an inner product on a vector space  ,\n              and if   is a vector in  ,\n              then the set   is a subspace of  .\n             True\/False \n              There is exactly one inner product on each inner product space.\n             \n              F\n             True\/False \n              If  ,  ,\n              and   are vectors in an inner product space with\n               ,\n              then  .\n             True\/False \n              If   for all vectors   in an inner product space  ,\n              then  .\n             \n              T\n             True\/False \n              If   and   are vectors in an inner product space and the distance from   to   is the same as the distance from   to  ,\n              then   and   are orthogonal.\n             True\/False \n              If   is a subspace of an inner product space and a vector   is orthogonal to every vector in a basis of  ,\n              then   is in  .\n             \n              T\n             True\/False \n              If   is an orthogonal basis for an inner product space  ,\n              then so is   for any nonzero scalar  .\n             True\/False \n              An inner product\n                in an inner product space   results in another vector in  .\n             \n              F\n             True\/False \n              An inner product in an inner product space   is a function that maps pairs of vectors in   to the set of non-negative real numbers.\n             True\/False \n              The vector space of all\n                matrices can be made into an inner product space.\n             \n              T\n             True\/False \n              Any non-zero multiple of an inner product on space   is also an inner product on  .\n             True\/False \n              Every set of   non-zero orthogonal vectors in a vector space   of dimension   is a basis for  .\n             \n              T\n             True\/False \n              For any finite-dimensional inner product space   and a subspace   of  ,\n                is a subspace of  .\n             True\/False \n              If   is a subspace of an inner product space,\n              then  .\n             \n              T\n             Project: Fourier Series and Musical Tones \n    Joseph Fourier first studied trigonometric polynomials to understand the flow of heat in metallic plates and rods.\n    The resulting series, called Fourier series,\n    now have applications in a variety of areas including electrical engineering,\n    vibration analysis, acoustics,\n    optics, signal processing,\n    image processing, geology, quantum mechanics, and many more.\n    For our purposes, we will focus on synthesized music.\n   \n    Pure musical tones are periodic sine waves.\n    Simple electronic circuits can be designed to generate alternating current.\n    Alternating current is current that is periodic,\n    and hence is described by a combination of   and\n      for integer values of  .\n    To synthesize an instrument like a violin,\n    we can project the instrument's tones onto trigonometric polynomials   and then we can produce them electronically.\n    As we will see,\n    these projections are least squares approximations onto certain vector spaces.\n    The website   provides a tool for hearing sounds digitally created by certain functions.\n    For example,\n    you can listen to the sound generated by a sawtooth function   of the form\n     \n   \n    Try out some of the tones on this website\n    (click on the Sound button to hear the tones).\n    You can also alter the tones by clicking on any one of the white dots and moving it up or down. and play with the buttons.\n    We will learn much about what this website does in this project.\n   trigonometric polynomial \n    Our first order of business is to verify that   is,\n    in fact, an inner product.\n   \n      Let   be the set of continuous real-valued functions on the interval  .\n      In  \n      in  \n      we are asked to show that   is a vector space,\n      while  \n      in   asks us to show that\n        defines an inner product on  .\n      However,  \n      is slightly different than this inner product.\n      Show that any positive scalar multiple of an inner product is an inner product,\n      and conclude that   defines an inner product on  .\n      (We will see why we introduce the factor of   later.)\n     \n    Now we return to our inner product space\n      with inner product  .\n    Given a function   in  ,\n    we approximate   using only a finite number of the terms in a trigonometric polynomial.\n    Let   be the subspace of\n      spanned by the functions\n     .\n   \n    One thing we need to know is the dimension of  .\n   \n      We start with the initial case of  .\n     \n            Show directly that the functions  ,  ,\n            and   are orthogonal.\n           \n            What is the dimension of  ?\n            Explain.\n           \n    Now we need to see if what happened in   happens in general.\n    A few tables of integrals and some basic facts from trigonometry can help.\n   \n      A table of integrals shows the following for   (up to a constant):\n       \n     \n            Use   to show that   and\n              are orthogonal in   if  .\n           \n            Use   to show that   and\n              are orthogonal in   if  .\n           \n            Use   to show that   and\n              are orthogonal in   if  .\n           \n            Use   to show that   and\n              are orthogonal in  .\n           \n            What is  ?\n            Explain.\n           \n    Once we have an orthogonal basis for  ,\n    we might want to create an orthonormal basis for  .\n    Throughout the remainder of this project,\n    unless otherwise specified,\n    you should use a table of integrals or any appropriate technological tool to find integrals for any functions you need.\n   \n      Show that the set\n       \n      is an orthonormal basis for  .\n      Use the fact that the norm of a vector   in an inner product space with inner product\n        is defined to be  .\n      (This is where the factor of   will be helpful.)\n     \n    Now we need to recall how to find the best approximation to a vector by a vector in a subspace,\n    and apply that idea to approximate an arbitrary function   with a trigonometric polynomial in  .\n    Recall that the best approximation of a function   in\n      is the projection of   onto  .\n    If we have an orthonormal basis\n      of  , then the projection of   onto   is\n     .\n   \n    With this idea,\n    we can find formulas for the coefficients when we project an arbitrary function onto  .\n   Fourier coefficients \n      If   is an arbitrary function in  ,\n      we will write the projection of   onto   as\n       .\n     Fourier coefficients harmonic fundamental frequency \n            Show that\n             .\n            Explain why   gives the average value of   on  .\n            You may want to go back and review average value from calculus.\n            This is saying that the best constant approximation of   on\n              is its average value, which makes sense.\n           \n            Show that for  ,\n             .\n           \n            Show that for  ,\n             .\n           \n    Let us return to the sawtooth function defined earlier and find its Fourier coefficients.\n   \n      Let   be defined by   on\n        and repeated periodically afterwards with period  .\n      Let   be the projection of   onto  .\n     \n            Evaluate the integrals to find the projection  .\n           \n            Use appropriate technology to find the projections  ,\n             ,\n            and   for the sawtooth function  .\n            Draw pictures of these approximations against   and explain what you see.\n           \n            Now we find formulas for all the Fourier coefficients.\n            Use the fact that   is an odd function to explain why   for each  .\n            Then show that   for each  .\n           \n            Go back to the website   and replay the sawtooth tone.\n            Explain what the white buttons represent.\n           \n      This activity is not connected to the idea of musical tones,\n      so can be safely ignored if so desired.\n      We conclude with a derivation of a very fascinating formula that you may have seen for  .\n      To do so, we need to analyze the error in approximating a function   with a function in  .\n     \n      Let   be the projection of   onto  .\n      Notice that   is also in  .\n      It is beyond the scope of this project, but in\n       nice \n      situations we have   as  .\n      Now   is orthogonal to  ,\n      so the Pythagorean theorem shows that\n       .\n     \n      Since   as\n       , we can conclude that\n       .\n     \n      We use these ideas to derive a formula for  .\n     \n            Use the fact that   is an orthonormal basis to show that\n             .\n            Conclude that\n             .\n           \n            For the remainder of this activity,\n            let   be the sawtooth function defined by   on\n              and repeated periodically afterwards.\n            We determined the Fourier coefficients   and   of this function in  .\n           \n                  Show that\n                   .\n                 \n                  Calculate   using the inner product and compare to   to find a surprising formula for  .\n                 "
},
{
  "id": "objectives-35",
  "level": "2",
  "url": "chap_inner_products.html#objectives-35",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            What is an inner product space?\n           \n         \n           \n            What is an orthogonal set in an inner product space?\n           \n         \n           \n            What is an orthogonal basis for an inner product space?\n           \n         \n           \n            How do we find the coordinate vector for a vector in an inner product space relative to an orthogonal basis for the space?\n           \n         \n           \n            What is the projection of a vector orthogonal to a subspace and why are such orthogonal projections important?\n           \n         "
},
{
  "id": "p-6073",
  "level": "2",
  "url": "chap_inner_products.html#p-6073",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "local "
},
{
  "id": "pa_6_c",
  "level": "2",
  "url": "chap_inner_products.html#pa_6_c",
  "type": "Preview Activity",
  "number": "35.1",
  "title": "",
  "body": "\n      Consider the vector space   of polynomials of degree less than or equal to   with real coefficients.\n      Define a mapping from   to   by\n       .\n     \n            Calculate  .\n           \n            If   and   are in  , is it true that\n             \n            Verify your answer.\n           \n            If  ,  ,\n            and   are in  , is it true that\n             \n            Verify your answer.\n           \n            If   and   are in   and   is a scalar,\n            is it true that\n             \n            Verify your answer.\n           \n            If   is in  ,\n            must it be the case that  ?\n            When is  ?\n           "
},
{
  "id": "def_6_c_inner_product",
  "level": "2",
  "url": "chap_inner_products.html#def_6_c_inner_product",
  "type": "Definition",
  "number": "35.1",
  "title": "",
  "body": "inner product inner product space inner product inner product space "
},
{
  "id": "activity-134",
  "level": "2",
  "url": "chap_inner_products.html#activity-134",
  "type": "Activity",
  "number": "35.2",
  "title": "",
  "body": "\n      Consider the mapping   from\n        to   defined by\n       .\n     \n            Show that this mapping satisfies the second property of an inner product.\n           \n            Although this mapping satisfies the first three properties of an inner product,\n            show that this mapping does not satisfy the fourth property and so is not an inner product.\n           "
},
{
  "id": "thm_6_c_inner_product_properties",
  "level": "2",
  "url": "chap_inner_products.html#thm_6_c_inner_product_properties",
  "type": "Theorem",
  "number": "35.2",
  "title": "",
  "body": "\n        Let   be an inner product on   and let  ,\n        and   be vectors in   and   a scalar.\n        Then\n         \n             \n               \n             \n           \n             \n               \n             \n           \n             \n               \n             \n           \n             \n               \n             \n           \n       "
},
{
  "id": "definition-78",
  "level": "2",
  "url": "chap_inner_products.html#definition-78",
  "type": "Definition",
  "number": "35.3",
  "title": "",
  "body": "length of a vector in an inner product space length "
},
{
  "id": "p-6102",
  "level": "2",
  "url": "chap_inner_products.html#p-6102",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "magnitude norm "
},
{
  "id": "definition-79",
  "level": "2",
  "url": "chap_inner_products.html#definition-79",
  "type": "Definition",
  "number": "35.4",
  "title": "",
  "body": "unit vector in an inner product space unit vector "
},
{
  "id": "definition-80",
  "level": "2",
  "url": "chap_inner_products.html#definition-80",
  "type": "Definition",
  "number": "35.5",
  "title": "",
  "body": "distance between vectors in an inner product space distance between "
},
{
  "id": "act_6_c_Frobenius",
  "level": "2",
  "url": "chap_inner_products.html#act_6_c_Frobenius",
  "type": "Activity",
  "number": "35.3",
  "title": "",
  "body": "trace \n      If   and   are in the space   of\n        matrices with real entries,\n      we define the mapping   from   to   by\n       .\n     Frobenius \n            Find the length of the vectors   and   using the Frobenius inner product.\n           \n            Find the distance between   and   using the Frobenius inner product.\n           "
},
{
  "id": "p-6113",
  "level": "2",
  "url": "chap_inner_products.html#p-6113",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "angle   between\n      and  "
},
{
  "id": "definition-81",
  "level": "2",
  "url": "chap_inner_products.html#definition-81",
  "type": "Definition",
  "number": "35.6",
  "title": "",
  "body": "orthogonal vectors in inner product spaces orthogonal "
},
{
  "id": "activity-136",
  "level": "2",
  "url": "chap_inner_products.html#activity-136",
  "type": "Activity",
  "number": "35.4",
  "title": "",
  "body": "\n      In this activity we use the Frobenius inner product\n      (see  ).\n      Let   and\n        in  .\n     \n            Find a nonzero vector in   orthogonal to  .\n           \n            Find the angle between   and  .\n           "
},
{
  "id": "definition-82",
  "level": "2",
  "url": "chap_inner_products.html#definition-82",
  "type": "Definition",
  "number": "35.7",
  "title": "",
  "body": "orthogonal set in an inner product space orthogonal set "
},
{
  "id": "thm_6_c_Orth_li_ips",
  "level": "2",
  "url": "chap_inner_products.html#thm_6_c_Orth_li_ips",
  "type": "Theorem",
  "number": "35.8",
  "title": "",
  "body": "\n        Let   be a set of nonzero orthogonal vectors in an inner product space.\n        Then the vectors  ,  ,\n         ,   are linearly independent.\n       "
},
{
  "id": "definition-83",
  "level": "2",
  "url": "chap_inner_products.html#definition-83",
  "type": "Definition",
  "number": "35.9",
  "title": "",
  "body": "orthogonal basis in an inner product space orthogonal basis "
},
{
  "id": "thm_6_c_orth_dcomp",
  "level": "2",
  "url": "chap_inner_products.html#thm_6_c_orth_dcomp",
  "type": "Theorem",
  "number": "35.10",
  "title": "",
  "body": "\n        Let   be an orthogonal basis for a subspace of an inner product space.\n        Let   be a vector in  .\n        Then\n         .\n       "
},
{
  "id": "activity-137",
  "level": "2",
  "url": "chap_inner_products.html#activity-137",
  "type": "Activity",
  "number": "35.5",
  "title": "",
  "body": "\n      Let  ,  ,\n      and   be vectors in the inner product space   with inner product defined by  .\n      Let  .\n      You may assume that   is an orthogonal basis for  .\n      Let  .\n      Find the weight   so that  .\n      Use technology as appropriate to evaluate any integrals.\n     "
},
{
  "id": "definition-84",
  "level": "2",
  "url": "chap_inner_products.html#definition-84",
  "type": "Definition",
  "number": "35.11",
  "title": "",
  "body": "orthonormal basis in an inner product space orthonormal basis "
},
{
  "id": "definition-85",
  "level": "2",
  "url": "chap_inner_products.html#definition-85",
  "type": "Definition",
  "number": "35.12",
  "title": "",
  "body": "orthogonal projection onto a subspace projection orthogonal to a subspace orthogonal projection of   onto  projection of   orthogonal to  "
},
{
  "id": "act_6_c_orth_projection",
  "level": "2",
  "url": "chap_inner_products.html#act_6_c_orth_projection",
  "type": "Activity",
  "number": "35.6",
  "title": "",
  "body": "\n      In  \n      we showed that in the inner product space   using the dot product as inner product,\n      if   is a subspace of   and   is in  ,\n      then   is orthogonal to every vector in  .\n      In this activity we verify that same fact in an inner product space.\n      That is, assume that   is an orthogonal basis for a subspace   of an inner product space   and   is a vector in  .\n      Follow the indicated steps to show that\n        is orthogonal to every vector in  .\n     \n            Let   be the projection of   onto  .\n            Write   in terms of the basis vectors in  .\n           \n            The vector   is the projection of   orthogonal to  .\n            Let   be between   and  .\n            Use the result of part (a) to show that\n              is orthogonal to  .\n             \n            then shows that   is orthogonal to every vector in  .\n           "
},
{
  "id": "thm_6_c_PT",
  "level": "2",
  "url": "chap_inner_products.html#thm_6_c_PT",
  "type": "Theorem",
  "number": "35.13",
  "title": "Generalized Pythagorean Theorem.",
  "body": "Generalized Pythagorean Theorem \n        Let   and   be orthogonal vectors in an inner product space  .\n        Then\n         .\n       \n      Let   and   be orthogonal vectors in an inner product space  .\n      Then\n       .\n     "
},
{
  "id": "thm_6_c_best_approx",
  "level": "2",
  "url": "chap_inner_products.html#thm_6_c_best_approx",
  "type": "Theorem",
  "number": "35.14",
  "title": "",
  "body": "\n        Let   be a subspace of an inner product space   and let   be a vector in  .\n        Then\n         \n        for every vector   in   different from  .\n       \n      Let   be a subspace of an inner product space   and let   be a vector in  .\n      Let   be a vector in  .\n      Now\n       .\n     \n      Since both   and   are in  ,\n      we know that   is in  .\n      Since   is orthogonal to every vector in  ,\n      we have that   is orthogonal to  .\n      We can now use the Generalized Pythagorean Theorem to conclude that\n       .\n     \n      Since  ,\n      it follows that   and\n       .\n     \n      Since norms are nonnegative,\n      we can conclude that   as desired.\n     "
},
{
  "id": "p-6157",
  "level": "2",
  "url": "chap_inner_products.html#p-6157",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "least squares approximation "
},
{
  "id": "activity-139",
  "level": "2",
  "url": "chap_inner_products.html#activity-139",
  "type": "Activity",
  "number": "35.7",
  "title": "",
  "body": "\n      The set   is an orthogonal basis for a subspace   of the inner product space   using the inner product  .\n      Find the polynomial in   that is closest to the polynomial\n        and give a numeric estimate of how good this approximation is.\n     "
},
{
  "id": "definition-86",
  "level": "2",
  "url": "chap_inner_products.html#definition-86",
  "type": "Definition",
  "number": "35.15",
  "title": "",
  "body": "orthogonal complement in inner product spaces orthogonal complement "
},
{
  "id": "thm_6_e_ip_orth_complement_basis",
  "level": "2",
  "url": "chap_inner_products.html#thm_6_e_ip_orth_complement_basis",
  "type": "Theorem",
  "number": "35.16",
  "title": "",
  "body": "\n        Let   be a basis for a subspace   of an inner product space  . A vector   in   is orthogonal to every vector in   if and only if   is orthogonal to every vector in  .\n       "
},
{
  "id": "activity-140",
  "level": "2",
  "url": "chap_inner_products.html#activity-140",
  "type": "Activity",
  "number": "35.8",
  "title": "",
  "body": "\n        Consider   with the inner product  .\n       \n          Find   where   is in  .\n         \n          Describe as best you can the orthogonal complement of   in  . Is   in this orthogonal complement? Is  ?\n         "
},
{
  "id": "act_6_c_decompose_ips",
  "level": "2",
  "url": "chap_inner_products.html#act_6_c_decompose_ips",
  "type": "Activity",
  "number": "35.9",
  "title": "",
  "body": "\n        Let   be an inner product space of dimension  , and let   be a subspace of  . Let   be any vector in  . We will demonstrate that   can be written uniquely as a sum of a vector in   and a vector in  . \n       \n          Explain why   is in  .\n         \n          Explain why   is in  .\n         \n          Explain why   can be written as a sum of vectors, one in   and one in  .\n         \n          Now we demonstrate the uniqueness of this decomposition. Suppose   and  , where   and   are in   and   and   are in  . Show that   and  , so that the representation of   as a sum of a vector in   and a vector in   is unique. (Hint: What is  ?)\n         "
},
{
  "id": "thm_6_c_decompose_ips",
  "level": "2",
  "url": "chap_inner_products.html#thm_6_c_decompose_ips",
  "type": "Theorem",
  "number": "35.17",
  "title": "",
  "body": "\n        Let   be a finite dimensional inner product space, and let   be a subspace of  . Any vector in   can be written in a unique way as a sum of a vector in   and a vector in  . \n       "
},
{
  "id": "example-73",
  "level": "2",
  "url": "chap_inner_products.html#example-73",
  "type": "Example",
  "number": "35.18",
  "title": "",
  "body": "\n        Let   be the inner product space with inner product\n         .\n       \n        Let  ,  ,\n         , and  .\n       \n              Show that the set   is an orthogonal basis for  .\n             \n              All calculations are done by hand or with a computer algebra system,\n              so we leave those details to the reader.\n             \n              If we show that the set   is an orthogonal set,\n              then  \n              shows that   is linearly independent.\n              Since  ,\n              the linearly independent set   that contains four vectors must be a basis for  .\n              To determine if the set   is an orthogonal set,\n              we must calculate the inner products of pairs of distinct vectors in  .\n              Since  ,\n               ,\n               ,\n               ,\n               ,\n              and  ,\n              we conclude that   is an orthogonal basis for  .\n             \n               Use \n              to write the polynomial   as a linear combination of the basis vectors in  .\n             \n              All calculations are done by hand or with a computer algebra system,\n              so we leave those details to the reader.\n             \n              We can write the polynomial\n                as a linear combination of the basis vectors in   as follows:\n               .\n              Now\n               \n              so\n               .\n             "
},
{
  "id": "example-74",
  "level": "2",
  "url": "chap_inner_products.html#example-74",
  "type": "Example",
  "number": "35.19",
  "title": "",
  "body": "\n        Let   be the inner product space   with inner product defined by\n         .\n       \n              Let   be the plane spanned by\n                and   in  .\n              Find the vector in   that is closest to the vector  .\n              Exactly how close is your best approximation to the vector  ?\n             \n              The vector we're looking for is the projection of   onto the plane.\n              A spanning set for the plane is  .\n              Neither vector in   is a scalar multiple of the other,\n              so   is a basis for the plane.\n              Since\n               ,\n              the set   is an orthogonal basis for the plane.\n              The projection of the vector\n                onto the plane spanned by\n                and   is given by\n               .\n              To measure how close close\n                is to  ,\n              we calculate\n               .\n             \n              Express the vector   as the sum of a vector in   and a vector orthogonal to  .\n             \n              If  ,\n              then   is in   and\n               \n              is in  ,\n              and  .\n             "
},
{
  "id": "ex_6_c_Cab",
  "level": "2",
  "url": "chap_inner_products.html#ex_6_c_Cab",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Let   be the set of all continuous real valued functions on the interval  .\n        If   is in  ,\n        we can extend   to a continuous function from   to   by letting   be the function defined by\n         .\n        In this way we can view   as a subset of  ,\n        the vector space of all functions from   to  .\n        Verify that   is a vector space.\n       \n        Use properties of continuous functions.\n       "
},
{
  "id": "ex_6_c_inner_products",
  "level": "2",
  "url": "chap_inner_products.html#ex_6_c_inner_products",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Use the definition of an inner product to determine which of the following defines an inner product on the indicated space.\n        Verify your answers.\n       \n                for\n                and   in  \n             \n                for\n                (where   is the vector space of all continuous functions on the interval  )\n             \n                for\n                (where   is the vector space of all differentiable functions on the interval  )\n             \n                for\n                and   an invertible   matrix\n             "
},
{
  "id": "exercise-367",
  "level": "2",
  "url": "chap_inner_products.html#exercise-367",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        We can sometimes visualize an inner product in   or  \n        (or other spaces)\n        by describing the unit circle  , where\n         \n        in that inner product space.\n        For example,\n        in the inner product space   with the dot product as inner product,\n        the unit circle is just our standard unit circle.\n        Inner products, however,\n        can distort this familiar picture of the unit circle.\n        Describe the points on the unit circle   in the inner product space   with inner product\n          using the following steps.\n       \n              Let  .\n              Set up an equation in   and   that is equivalent to the vector equation  .\n             \n               \n             \n              Describe the graph of the equation you found in  .\n              It should have a familiar form.\n              Draw a picture to illustrate.\n              What do you think of calling this graph a\n               circle ?\n             \n              An ellipse centered at the origin with major axis the segment from   to\n                and minor axis the segment from\n                to  .\n             "
},
{
  "id": "ex_6_c_ip_1",
  "level": "2",
  "url": "chap_inner_products.html#ex_6_c_ip_1",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Define   on   by  .\n       \n              Show that   is an inner product.\n             \n              The inner product   can be represented as a matrix transformation  ,\n              where   and   are written as column vectors.\n              Find a matrix   that represents this inner product.\n             "
},
{
  "id": "ex_6_c_ip_2",
  "level": "2",
  "url": "chap_inner_products.html#ex_6_c_ip_2",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        This exercise is a generalization of  .\n        Define   on   by\n         \n        for some positive scalars  ,\n         ,  ,  .\n       \n              Show that   is an inner product.\n             \n              Verify the inner product properties.\n              Why is the assumption that the   are positive necessary?\n             \n              The inner product   can be represented as a matrix transformation  ,\n              where   and   are written as column vectors.\n              Find a matrix   that represents this inner product.\n             \n               .\n             "
},
{
  "id": "exercise-370",
  "level": "2",
  "url": "chap_inner_products.html#exercise-370",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Is the sum of two inner products on an inner product space   an inner product on  ?\n        If yes, prove it.\n        If no, provide a counterexample.\n        (By the sum of inner products we mean a function   satisfying\n         \n        for all   and   in  ,\n        where   and\n          are inner products on  .)\n       "
},
{
  "id": "exercise-371",
  "level": "2",
  "url": "chap_inner_products.html#exercise-371",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n            Does   define an inner product on   for every   matrix  ?\n            Verify your answer.\n           \n            No.\n           \n            If your answer to part (a) is no,\n            are there any types of matrices for which\n              defines an inner product?\n            \n           \n            See  \n            and  .\n           \n            If   is a diagonal matrix with positive diagonal entries.\n           "
},
{
  "id": "ex_6_c_trace",
  "level": "2",
  "url": "chap_inner_products.html#ex_6_c_trace",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        The trace of an   matrix\n          has some useful properties.\n       \n              Show that   for any\n                matrices   and  .\n             \n              Show that   for any\n                matrix   and any scalar  .\n             \n              Show that   for any   matrix.\n             "
},
{
  "id": "exercise-373",
  "level": "2",
  "url": "chap_inner_products.html#exercise-373",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        Let   be an inner product space and\n          be two vectors in  .\n       \n              Check that if  ,\n              the Cauchy-Schwarz inequality\n               \n              holds.\n             \n              Evaluate each side of the inequality.\n             \n              Assume  .\n              Let   and  .\n              Use the fact that   to conclude the Cauchy-Schwarz inequality in this case.\n             \n              Write   as an inner product and expand.\n             "
},
{
  "id": "ex_6_c_Frobenius",
  "level": "2",
  "url": "chap_inner_products.html#ex_6_c_Frobenius",
  "type": "Exercise",
  "number": "10",
  "title": "",
  "body": "Frobenius inner product "
},
{
  "id": "exercise-375",
  "level": "2",
  "url": "chap_inner_products.html#exercise-375",
  "type": "Exercise",
  "number": "11",
  "title": "",
  "body": "\n        Let   and\n          be two   matrices.\n       \n              Show that if  , then the Frobenius inner product\n              (see  )\n              of   and   is\n               .\n             \n              Expand the inner product.\n             \n              Extend part (a) to the general case.\n              That is, show that for an arbitrary  ,\n               .\n             \n              Expand the inner product.\n             \n              Compare the Frobenius inner product to the scalar product of two vectors.\n             \n              Convert   and   to vectors in\n            whose entries are the entries in the first row followed by the entries in the second row and so on.\n             "
},
{
  "id": "exercise-376",
  "level": "2",
  "url": "chap_inner_products.html#exercise-376",
  "type": "Exercise",
  "number": "12",
  "title": "",
  "body": "\n        Let   and let   in  .\n       \n              Show that   is an orthogonal basis for  ,\n              using the dot product as inner product.\n             \n              Explain why the vector   is not in  .\n             \n              Find the vector in   that is closest to  .\n              How close is this vector to  ?\n             "
},
{
  "id": "exercise-377",
  "level": "2",
  "url": "chap_inner_products.html#exercise-377",
  "type": "Exercise",
  "number": "13",
  "title": "",
  "body": "\n        Let   be the inner product space with inner product\n         .\n        Let   and let   in  .\n       \n              Show that   is an orthogonal basis for  ,\n              using the given inner product.\n             \n              Compute the inner product of the vectors.\n             \n              Explain why the vector   is not in  .\n             \n              Try to write   in terms of the basis vectors for  .\n             \n              Find the vector in   that is closest to  .\n              How close is this vector to  ?\n             \n               .\n              Approximately 1.41.\n             "
},
{
  "id": "exercise-378",
  "level": "2",
  "url": "chap_inner_products.html#exercise-378",
  "type": "Exercise",
  "number": "14",
  "title": "",
  "body": "\n        Let   be the inner product space with inner product\n         .\n        Let   and let   in  .\n       \n              Show that   is an orthogonal basis for  ,\n              using the given inner product.\n             \n              Explain why the polynomial   is not in  .\n             \n              Find the vector in   that is closest to  .\n              How close is this vector to  ?\n             "
},
{
  "id": "exercise-379",
  "level": "2",
  "url": "chap_inner_products.html#exercise-379",
  "type": "Exercise",
  "number": "15",
  "title": "",
  "body": "\n        Prove the remaining properties of  .\n        That is, if   is an inner product on a vector space   and  ,\n        and   are vectors in   and   is any scalar, then\n       \n               \n             \n              Use the fact that  .\n             \n               \n             \n              Use the fact that  .\n             \n               \n             \n              Same hint as part (b).\n             \n               \n             \n              Use the fact that  .\n             "
},
{
  "id": "ex_6_c_orth_complement_basis",
  "level": "2",
  "url": "chap_inner_products.html#ex_6_c_orth_complement_basis",
  "type": "Exercise",
  "number": "16",
  "title": "",
  "body": "\n        Prove the following theorem referenced in  .\n        \n         \n              Let   be a basis for a subspace   of an inner product space  .\n              A vector   in   is orthogonal to every vector in   if and only if   is orthogonal to every vector in  .\n             \n       \n        Mimic  .\n       "
},
{
  "id": "exercise-381",
  "level": "2",
  "url": "chap_inner_products.html#exercise-381",
  "type": "Exercise",
  "number": "17",
  "title": "",
  "body": "\n        Prove  .\n        \n       \n        Mimic  .\n       "
},
{
  "id": "ex_6_c_all_ips",
  "level": "2",
  "url": "chap_inner_products.html#ex_6_c_all_ips",
  "type": "Exercise",
  "number": "18",
  "title": "",
  "body": "\n        Let   be a vector space with basis  .\n        Define   as follows:\n         \n        if   and   in  .\n        (Since the representation of a vector as a linear combination of basis elements is unique,\n        this mapping is well-defined.)\n        Show that   is an inner product on   and conclude that any finite dimensional vector space can be made into an inner product space.\n       "
},
{
  "id": "exercise-383",
  "level": "2",
  "url": "chap_inner_products.html#exercise-383",
  "type": "Exercise",
  "number": "19",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              An inner product on a vector space   is a function from   to the real numbers.\n             \n              F\n             True\/False \n              If   is an inner product on a vector space  ,\n              and if   is a vector in  ,\n              then the set   is a subspace of  .\n             True\/False \n              There is exactly one inner product on each inner product space.\n             \n              F\n             True\/False \n              If  ,  ,\n              and   are vectors in an inner product space with\n               ,\n              then  .\n             True\/False \n              If   for all vectors   in an inner product space  ,\n              then  .\n             \n              T\n             True\/False \n              If   and   are vectors in an inner product space and the distance from   to   is the same as the distance from   to  ,\n              then   and   are orthogonal.\n             True\/False \n              If   is a subspace of an inner product space and a vector   is orthogonal to every vector in a basis of  ,\n              then   is in  .\n             \n              T\n             True\/False \n              If   is an orthogonal basis for an inner product space  ,\n              then so is   for any nonzero scalar  .\n             True\/False \n              An inner product\n                in an inner product space   results in another vector in  .\n             \n              F\n             True\/False \n              An inner product in an inner product space   is a function that maps pairs of vectors in   to the set of non-negative real numbers.\n             True\/False \n              The vector space of all\n                matrices can be made into an inner product space.\n             \n              T\n             True\/False \n              Any non-zero multiple of an inner product on space   is also an inner product on  .\n             True\/False \n              Every set of   non-zero orthogonal vectors in a vector space   of dimension   is a basis for  .\n             \n              T\n             True\/False \n              For any finite-dimensional inner product space   and a subspace   of  ,\n                is a subspace of  .\n             True\/False \n              If   is a subspace of an inner product space,\n              then  .\n             \n              T\n             "
},
{
  "id": "p-6307",
  "level": "2",
  "url": "chap_inner_products.html#p-6307",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "trigonometric polynomial "
},
{
  "id": "act_Fourier_inner_product",
  "level": "2",
  "url": "chap_inner_products.html#act_Fourier_inner_product",
  "type": "Project Activity",
  "number": "35.10",
  "title": "",
  "body": "\n      Let   be the set of continuous real-valued functions on the interval  .\n      In  \n      in  \n      we are asked to show that   is a vector space,\n      while  \n      in   asks us to show that\n        defines an inner product on  .\n      However,  \n      is slightly different than this inner product.\n      Show that any positive scalar multiple of an inner product is an inner product,\n      and conclude that   defines an inner product on  .\n      (We will see why we introduce the factor of   later.)\n     "
},
{
  "id": "act_Fourier_W1",
  "level": "2",
  "url": "chap_inner_products.html#act_Fourier_W1",
  "type": "Project Activity",
  "number": "35.11",
  "title": "",
  "body": "\n      We start with the initial case of  .\n     \n            Show directly that the functions  ,  ,\n            and   are orthogonal.\n           \n            What is the dimension of  ?\n            Explain.\n           "
},
{
  "id": "act_Fourier_Wn",
  "level": "2",
  "url": "chap_inner_products.html#act_Fourier_Wn",
  "type": "Project Activity",
  "number": "35.12",
  "title": "",
  "body": "\n      A table of integrals shows the following for   (up to a constant):\n       \n     \n            Use   to show that   and\n              are orthogonal in   if  .\n           \n            Use   to show that   and\n              are orthogonal in   if  .\n           \n            Use   to show that   and\n              are orthogonal in   if  .\n           \n            Use   to show that   and\n              are orthogonal in  .\n           \n            What is  ?\n            Explain.\n           "
},
{
  "id": "act_Fourier_basis",
  "level": "2",
  "url": "chap_inner_products.html#act_Fourier_basis",
  "type": "Project Activity",
  "number": "35.13",
  "title": "",
  "body": "\n      Show that the set\n       \n      is an orthonormal basis for  .\n      Use the fact that the norm of a vector   in an inner product space with inner product\n        is defined to be  .\n      (This is where the factor of   will be helpful.)\n     "
},
{
  "id": "act_Fourier_projection",
  "level": "2",
  "url": "chap_inner_products.html#act_Fourier_projection",
  "type": "Project Activity",
  "number": "35.14",
  "title": "",
  "body": "Fourier coefficients \n      If   is an arbitrary function in  ,\n      we will write the projection of   onto   as\n       .\n     Fourier coefficients harmonic fundamental frequency \n            Show that\n             .\n            Explain why   gives the average value of   on  .\n            You may want to go back and review average value from calculus.\n            This is saying that the best constant approximation of   on\n              is its average value, which makes sense.\n           \n            Show that for  ,\n             .\n           \n            Show that for  ,\n             .\n           "
},
{
  "id": "act_Fourier_sawtooth",
  "level": "2",
  "url": "chap_inner_products.html#act_Fourier_sawtooth",
  "type": "Project Activity",
  "number": "35.15",
  "title": "",
  "body": "\n      Let   be defined by   on\n        and repeated periodically afterwards with period  .\n      Let   be the projection of   onto  .\n     \n            Evaluate the integrals to find the projection  .\n           \n            Use appropriate technology to find the projections  ,\n             ,\n            and   for the sawtooth function  .\n            Draw pictures of these approximations against   and explain what you see.\n           \n            Now we find formulas for all the Fourier coefficients.\n            Use the fact that   is an odd function to explain why   for each  .\n            Then show that   for each  .\n           \n            Go back to the website   and replay the sawtooth tone.\n            Explain what the white buttons represent.\n           "
},
{
  "id": "act_Fourier_square_sums",
  "level": "2",
  "url": "chap_inner_products.html#act_Fourier_square_sums",
  "type": "Project Activity",
  "number": "35.16",
  "title": "",
  "body": "\n      This activity is not connected to the idea of musical tones,\n      so can be safely ignored if so desired.\n      We conclude with a derivation of a very fascinating formula that you may have seen for  .\n      To do so, we need to analyze the error in approximating a function   with a function in  .\n     \n      Let   be the projection of   onto  .\n      Notice that   is also in  .\n      It is beyond the scope of this project, but in\n       nice \n      situations we have   as  .\n      Now   is orthogonal to  ,\n      so the Pythagorean theorem shows that\n       .\n     \n      Since   as\n       , we can conclude that\n       .\n     \n      We use these ideas to derive a formula for  .\n     \n            Use the fact that   is an orthonormal basis to show that\n             .\n            Conclude that\n             .\n           \n            For the remainder of this activity,\n            let   be the sawtooth function defined by   on\n              and repeated periodically afterwards.\n            We determined the Fourier coefficients   and   of this function in  .\n           \n                  Show that\n                   .\n                 \n                  Calculate   using the inner product and compare to   to find a surprising formula for  .\n                 "
},
{
  "id": "chap_gram_schmidt_ips",
  "level": "1",
  "url": "chap_gram_schmidt_ips.html",
  "type": "Section",
  "number": "36",
  "title": "The Gram-Schmidt Process in Inner Product Spaces",
  "body": "The Gram-Schmidt Process in Inner Product Spaces \n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            What is the Gram-Schmidt process and why is it useful?\n           \n         Application: Gaussian Quadrature \n    Since integration of functions is difficult,\n    approximation techniques for definite integrals are very important.\n    In calculus we are introduced to various methods, e.g., the midpoint,\n    trapezoid, and Simpson's rule,\n    for approximating definite integrals.\n    These methods divide the interval of integration into subintervals and then use values of the integrand at the endpoints to approximate the integral.\n    These are useful methods when approximating integrals from tabulated data,\n    but there are better methods for other types of integrands.\n    If we make judicious choices in the points we use to evaluate the integrand,\n    we can obtain more accurate results with less work.\n    One such method is Gaussian quadrature (which, for example,\n    is widely used in solving problems of radiation heat transfer in direct integration of the equation of transfer of radiation over space),\n    which we explore later in this section.\n    This method utilizes the Gram-Schmidt process to produce orthogonal polynomials.\n   Introduction \n    We have seen that orthogonal bases make computations very convenient,\n    and the Gram-Schmidt process allowed us to create orthogonal bases in   using the dot product as inner product.\n    In this section we will see how the Gram-Schmidt process works in any inner product space.\n   The Gram-Schmidt Process using Inner Products \n    Our goal is to understand how the Gram-Schmidt process works in any inner product space.\n   \n      Let  ,  ,\n      and   in  .\n      Let   and let  .\n      Throughout this activity, use the inner product on   defined by\n       .\n     \n      We want to construct an orthogonal basis\n        from   so that  .\n      To begin, we start by letting  .\n     \n            Now we want to find a polynomial in   that is orthogonal to  .\n            Let  .\n            Explain why   is in   and is orthogonal to  .\n            Then calculate the polynomial  .\n           \n            Next we need to find a third polynomial   that is in   and is orthogonal to both   and  .\n            Let  .\n            Explain why   is in   and is orthogonal to both   and  .\n            Then calculate the polynomial  .\n           \n            Explain why the set   is an orthogonal basis for  .\n           \n     \n    shows the first steps of the Gram-Schmidt process to construct an orthogonal basis from any basis of an inner product space.\n    To understand how the process works in general,\n    let   be a basis for a subspace   of an inner product space  .\n    Let   and let  .\n    Since   we have that  .\n    Now consider the subspace\n     \n    of  .\n    The vectors   and   are possibly not orthogonal,\n    but we know the orthogonal projection of   onto\n      is orthogonal to  .\n    Let\n     .\n   \n    Then   is an orthogonal set.\n    Note that  ,\n    and the fact that   implies that  .\n    So the set   is linearly independent,\n    being a set of non-zero orthogonal vectors.\n    Now the question is whether  .\n    Note that   is a linear combination of   and  ,\n    so   is in  .\n    Since   is a 2-dimensional subspace of the 2-dimensional space  ,\n    it must be true that  .\n   \n    Now we take the next step, adding   into the mix.\n    Let\n     .\n   \n    The vector\n     \n    is orthogonal to both   and   and,\n    by construction,\n      is a linear combination of  ,\n     , and  .\n    So   is in  .\n    The fact that   implies that   and\n      is a linearly independent set.\n    Since   is a 3-dimensional subspace of the 3-dimensional space  ,\n    we conclude that   equals  .\n   \n    We continue inductively in this same manner.\n    If we have constructed a set  ,\n     ,  ,  ,\n      of   orthogonal vectors such that\n     ,\n    then we let\n     ,\n    where\n     .\n   \n    We know that   is orthogonal to  ,\n     ,\n     ,  .\n    Since  ,  ,\n     ,  ,\n    and   are all in\n      we see that   is also in  .\n    Since   implies that   and\n      is a linearly independent set.\n    Then   is a  -dimensional subspace of the  -dimensional space  ,\n    so it follows that\n     .\n   \n    This process will end when we have an orthogonal set  ,\n     ,  ,\n     ,\n      with  ,  ,\n     ,  ,   =  .\n   \n    We summarize the process in the following theorem.\n   The Gram-Schmidt Process Gram-Schmidt Process in an inner product space \n        Let   be a basis for a subspace   of an inner product space  .\n        The set   defined by\n         \n             \n               ,\n             \n           \n             \n               ,\n             \n           \n             \n               ,\n               \n             \n           \n             \n               .\n             \n           \n       \n        is an orthogonal basis for  .\n        Moreover,\n         \n        for  .\n       \n    The Gram-Schmidt process builds an orthogonal basis\n      for us from a given basis.\n    To make an orthonormal basis  ,\n    all we need do is normalize each basis vector:\n    that is, for each  , we let\n     .\n   \n      Use the Gram-Schmidt process to find an orthogonal basis for\n       \n      using the inner product\n       .\n     Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Let  , where\n         .\n       \n        Find an orthonormal basis for   using the Frobenius inner product  .\n       \n        Recall that the Frobenius inner product is just like a dot product for matrices.\n        First note that  ,  ,\n         , and   are linearly independent.\n        We let   and the Gram-Schmidt process gives us\n         \n         \n        and\n         .\n       \n        Then   is an orthogonal basis for  .\n        An orthonormal basis is found by dividing each vector by its magnitude, so\n         \n        is an orthonormal basis for  .\n       \n        Consider the space   with inner product  .\n       \n              Find the polynomial in  \n              (considered as a subspace of  )\n              that is closest to  .\n              Use technology to calculate any required integrals.\n              Draw a graph of your approximation against the graph of  .\n             \n              Our job is to find  .\n              To do this, we need an orthogonal basis of  .\n              We apply the Gram-Schmidt process to the standard basis\n                of   to obtain an orthogonal basis   of  .\n              We start with  , then\n               \n              and\n               .\n              Then\n               .\n              A graph of the approximation and   are shown in  \n               The graphs of   and  . \n            \n             \n              Provide a numeric measure of the error in approximating\n                by the polynomial you found in part (a).\n             \n              The norm of   tells us how well our projection\n                approximates  .\n              Technology shows that\n               .\n             Summary \n       \n        The Gram-Schmidt process produces an orthogonal basis from any basis.\n        It works as follows.\n        Let   be a basis for a subspace   of  .\n        The set  ,  ,\n         ,  ,   defined by\n         \n             \n               ,\n             \n           \n             \n               ,\n             \n           \n             \n               ,\n               \n             \n           \n             \n               .\n             \n           \n      \n      is an orthogonal basis for  .\n      Moreover,\n       \n      for each   between 1 and  .\n       \n     \n        Let   with the inner product  .\n        Let  .\n        Note that   is a subspace of  .\n       \n              Find the polynomial   in   that is closest to the function   defined by\n                in the least squares sense.\n              That is, find the projection of   onto  .\n              \n             \n              Recall the work done in  .\n             \n                \\item \n             \n              Find the second order Taylor polynomial   for   centered at 0.\n             \n               \n             \n              Plot  ,  ,\n              and   on the same set of axes.\n              Which approximation provides a better fit to   on this interval.\n              Why?\n             \n              The least squares polynomial is an overall better fit.\n             \n        Each set   is linearly independent.\n        Use the Gram-Schmidt process to create an orthogonal set of vectors with the same span as  .\n        Then find an orthonormal basis for the same span.\n       \n                in   using the inner product\n               \n             \n                in   using the inner product  ,\n              where  \n             \n                in   with the weighted inner product\n               \n             \n        Let  .\n       \n              Show that   is a linearly independent set in  .\n             \n              Evaluate an appropriate linear combination at several well chosen values to set up a linear system.\n             \n              Use the Gram-Schmidt process to find an orthogonal basis from   using the inner product\n               \n             \n               \n             \n        Find an orthonormal basis for the subspace   of   that consists of all polynomials of the form\n          using the inner product  .\n       \n        The Legendre polynomials form an orthonormal basis for the infinite dimensional inner product space   of all polynomials using the inner product\n         .\n        The Legendre polynomials have applications to differential equations,\n        statistics, numerical analysis,\n        and physics (e.g., they appear when solving Schrdinger equation in three dimensions for a central force).\n        The Legendre polynomials are found by using the Gram-Schmidt process to find an orthogonal basis from the standard basis   for  .\n        Find the first four Legendre polynomials by creating a orthonormal basis from the set  .\n       \n         \n       \n        The sine integral function   has applications in physics and engineering.\n        We define   as\n         .\n        Since we cannot find an elementary formula for  ,\n        we use approximations.\n        Find the best approximation to   in   with inner product  .\n        Use appropriate technology for computations and round output six places to the right of the decimal.\n       \n        Recall from  \n        in  \n        that any finite dimensional vector space   can be made into an inner product space by setting a basis   for   and defining\n         \n        if   and   in  .\n        Let    and  .\n        (You may assume that   is a basis for  .)\n       \n              Find an orthogonal basis for   containing the polynomial\n                using the inner product  .\n             \n               \n             \n              Find the best approximation possible as measured by the inner product   to the polynomial\n                by a polynomial in  .\n             \n               \n             \n        Let  ,  ,\n        and   be three distinct fixed real numbers and define   by\n         .\n       \n              Show that   as defined in   is an inner product on  .\n             \n              Is the mapping\n               \n              an inner product on  ?\n              Justify your answer.\n             \n              Let  ,  , and  .\n              Find an orthogonal basis for   using the inner product  .\n             \n              Continue with  ,  , and  .\n              Find the best approximation possible as measured by the inner product   to the polynomial\n                by a polynomial in  .\n             \n              If  ,\n              find   in   with respect to the inner product  .\n             \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n        Throughout, let   be a vector space.\n       True\/False \n              If   is a basis for a subspace   of an inner product space  ,\n              then the vector   is the vector in   closest to  .\n             \n              F\n             True\/False \n              If   is a subspace of an inner product space  ,\n              then the vector   is orthogonal to every vector in  .\n             True\/False \n              If  ,\n               ,\n                are vectors in an inner product space  ,\n              then the Gram-Schmidt process constructs an orthogonal set of vectors\n                with the same span as  .\n             \n              T\n             True\/False \n              Any set\n                of orthogonal vectors in an inner product space   can be extended to an orthogonal basis of  .\n             True\/False \n              Every nontrivial finite dimensional subspace of an inner product space has an orthogonal basis.\n             \n              T\n             True\/False \n              In any inner product space  ,\n              if   is a subspace satisfying  ,\n              then  .\n             Project: Gaussian Quadrature and Legendre Polynomials Gaussian quadrature \n    Simpson's rule is a reasonably accurate method for approximating definite integrals since it models the integrand on subintervals with quadratics.\n    For that reason, Simpson's rule provides exact values for integrals of all polynomials of degree less than or equal to 2.\n    In Gaussian quadrature,\n    we will use a family of polynomials to determine points at which to evaluate an integral of the form  .\n    By allowing ourselves to select evaluation points that are not uniformly distributed across the interval of integration,\n    we will be able to approximate our integrals much more efficiently.\n    The method is constructed so as to obtain exact values for as large of degree polynomial integrands as possible.\n    As a result,\n    if we can approximate our integrand well with polynomials,\n    we can obtain very good approximations with Gaussian quadrature with minimal effort.\n   \n    The Gaussian quadrature\n    approximation has the form\n     ,\n    where the   (weights) and the   (nodes) are points in the interval  \n    As we will see later,\n    integrations over   can be converted to an integral over   with a change of variable.\n     .\n    Gaussian quadrature describes how to find the weights and the points in   to obtain suitable approximations.\n    We begin to explore Gaussian quadrature with the simplest cases.\n   \n      In this activity we find through direct calculation the node and weight with   so that\n       .\n     \n      There are two unknowns in this situation (  and  ) and so we will need 2 equations to find these unknowns.\n      Keep in mind that we want to have the approximation   be exact for as large of degree polynomials as possible.\n     \n            Assume equality in   if we choose  .\n            Use the resulting equation to find  .\n           \n            Assume equality in   if we choose  .\n            Use the resulting equation to find  .\n           \n            Verify that   is in fact an equality for any linear polynomial of the form  ,\n            using the values of   and   you found\n           \n    We do one more specific case before considering the general case.\n   \n      In this problem we find through direct calculation the nodes and weights with   so that\n       .\n     \n      There are four unknowns in this situation (  and\n       ) and so we will need 4 equations to find these unknowns.\n      Keep in mind that we want to have the approximation   be exact for as large of degree polynomials as possible.\n      In this case we will use  ,\n       ,  , and  .\n     \n            Assume equality in   if we choose  .\n            This gives us an equation in   and  .\n            Find this equation.\n           \n            Assume equality in   if we choose  .\n            This gives us an equation in   and  .\n            Find this equation.\n           \n            Assume equality in   if we choose  .\n            This gives us an equation in   and  .\n            Find this equation.\n           \n            Assume equality in   if we choose  .\n            This gives us an equation in   and  .\n            Find this equation.\n           \n            Solve this system of 4 equations in 4 unknowns.\n            You can do this by hand or with any other appropriate tool.\n            Show that   and   are the roots of the polynomial  .\n           \n            Verify that   is in fact an equality with the values of   and\n              you found for any polynomial of the form  .\n           \n    Other than solving a system of linear equations as in  ,\n    it might be reasonable to ask what the connection is between Gaussian quadrature and linear algebra.\n    We explore that connection now.\n   \n    In the general case,\n    we want to find the weights and nodes to make the approximation exact for as large degree polynomials as possible.\n    We have   unknowns  ,\n     ,  ,\n      and  ,  ,\n     ,  ,\n    so we need to impose   conditions to determine the unknowns.\n    We will require equality for the   functions   for   from 0 to  .\n    This yields the equations\n     .\n   \n    In the  th equation the right hand side is\n     \n   \n      It is inefficient to always solve these systems of equations to find the nodes and weights,\n      especially since there is a more elegant way to find the nodes.\n     \n            Use appropriate technology to find the equations satisfied by the   for  ,\n             , and  .\n           Legendre polynomials \n    Although it would take us beyond the scope of this project to verify this fact,\n    the nodes in the  th Gaussian quadrature approximation   are in fact the roots of the  th order Legendre polynomial.\n    In other words,\n    if   is the  th order Legendre polynomial,\n    then  ,  ,\n     ,\n      are the roots of   in  .\n    Gaussian quadrature as described in   using the polynomial   is exact if the integrand   is a polynomial of degree less than  .\n   \n    We can find the corresponding weights,\n     , using the formula \n    Abramowitz, Milton; Stegun, Irene A., eds. (1972),\n    sec. 25.4, Integration, Handbook of Mathematical Functions\n    (with Formulas, Graphs, and Mathematical Tables),\n    Dover,\n     \n     ,\n    where   is the  th order Legendre polynomial scaled so that  .\n   \n      Let us see now how good the integral estimates are with Gaussian quadrature method using an example.\n      Use Gaussian quadrature with the indicated value of   to approximate  .\n      Be sure to explain how you found your nodes and weights\n      (approximate the nodes and weights to 8 decimal places).\n      Compare the approximations with the actual value of the integral.\n      Use technology as appropriate to help with calculations.\n     \n             \n           \n             \n           \n             \n           \n    Our Gaussian quadrature formula was derived for integrals on the interval  .\n    We conclude by seeing how a definite integral on an interval can be converted to one on the interval  .\n   \n      Consider the problem of approximating an integral of the form  .\n      Show that the change of variables  ,\n        reduces the integral   to the form  .\n      (This change of variables can be derived by finding a linear function that maps the interval   to the interval  .)\n     "
},
{
  "id": "objectives-36",
  "level": "2",
  "url": "chap_gram_schmidt_ips.html#objectives-36",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            What is the Gram-Schmidt process and why is it useful?\n           \n         "
},
{
  "id": "pa_6_d_2",
  "level": "2",
  "url": "chap_gram_schmidt_ips.html#pa_6_d_2",
  "type": "Preview Activity",
  "number": "36.1",
  "title": "",
  "body": "\n      Let  ,  ,\n      and   in  .\n      Let   and let  .\n      Throughout this activity, use the inner product on   defined by\n       .\n     \n      We want to construct an orthogonal basis\n        from   so that  .\n      To begin, we start by letting  .\n     \n            Now we want to find a polynomial in   that is orthogonal to  .\n            Let  .\n            Explain why   is in   and is orthogonal to  .\n            Then calculate the polynomial  .\n           \n            Next we need to find a third polynomial   that is in   and is orthogonal to both   and  .\n            Let  .\n            Explain why   is in   and is orthogonal to both   and  .\n            Then calculate the polynomial  .\n           \n            Explain why the set   is an orthogonal basis for  .\n           "
},
{
  "id": "thm_6_d_Gram_Schmidt_ips",
  "level": "2",
  "url": "chap_gram_schmidt_ips.html#thm_6_d_Gram_Schmidt_ips",
  "type": "Theorem",
  "number": "36.1",
  "title": "The Gram-Schmidt Process.",
  "body": "The Gram-Schmidt Process Gram-Schmidt Process in an inner product space \n        Let   be a basis for a subspace   of an inner product space  .\n        The set   defined by\n         \n             \n               ,\n             \n           \n             \n               ,\n             \n           \n             \n               ,\n               \n             \n           \n             \n               .\n             \n           \n       \n        is an orthogonal basis for  .\n        Moreover,\n         \n        for  .\n       "
},
{
  "id": "act_6_d_gs_example",
  "level": "2",
  "url": "chap_gram_schmidt_ips.html#act_6_d_gs_example",
  "type": "Activity",
  "number": "36.2",
  "title": "",
  "body": "\n      Use the Gram-Schmidt process to find an orthogonal basis for\n       \n      using the inner product\n       .\n     "
},
{
  "id": "example-75",
  "level": "2",
  "url": "chap_gram_schmidt_ips.html#example-75",
  "type": "Example",
  "number": "36.2",
  "title": "",
  "body": "\n        Let  , where\n         .\n       \n        Find an orthonormal basis for   using the Frobenius inner product  .\n       \n        Recall that the Frobenius inner product is just like a dot product for matrices.\n        First note that  ,  ,\n         , and   are linearly independent.\n        We let   and the Gram-Schmidt process gives us\n         \n         \n        and\n         .\n       \n        Then   is an orthogonal basis for  .\n        An orthonormal basis is found by dividing each vector by its magnitude, so\n         \n        is an orthonormal basis for  .\n       "
},
{
  "id": "example-76",
  "level": "2",
  "url": "chap_gram_schmidt_ips.html#example-76",
  "type": "Example",
  "number": "36.3",
  "title": "",
  "body": "\n        Consider the space   with inner product  .\n       \n              Find the polynomial in  \n              (considered as a subspace of  )\n              that is closest to  .\n              Use technology to calculate any required integrals.\n              Draw a graph of your approximation against the graph of  .\n             \n              Our job is to find  .\n              To do this, we need an orthogonal basis of  .\n              We apply the Gram-Schmidt process to the standard basis\n                of   to obtain an orthogonal basis   of  .\n              We start with  , then\n               \n              and\n               .\n              Then\n               .\n              A graph of the approximation and   are shown in  \n               The graphs of   and  . \n            \n             \n              Provide a numeric measure of the error in approximating\n                by the polynomial you found in part (a).\n             \n              The norm of   tells us how well our projection\n                approximates  .\n              Technology shows that\n               .\n             "
},
{
  "id": "exercise-384",
  "level": "2",
  "url": "chap_gram_schmidt_ips.html#exercise-384",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Let   with the inner product  .\n        Let  .\n        Note that   is a subspace of  .\n       \n              Find the polynomial   in   that is closest to the function   defined by\n                in the least squares sense.\n              That is, find the projection of   onto  .\n              \n             \n              Recall the work done in  .\n             \n                \\item \n             \n              Find the second order Taylor polynomial   for   centered at 0.\n             \n               \n             \n              Plot  ,  ,\n              and   on the same set of axes.\n              Which approximation provides a better fit to   on this interval.\n              Why?\n             \n              The least squares polynomial is an overall better fit.\n             "
},
{
  "id": "exercise-385",
  "level": "2",
  "url": "chap_gram_schmidt_ips.html#exercise-385",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Each set   is linearly independent.\n        Use the Gram-Schmidt process to create an orthogonal set of vectors with the same span as  .\n        Then find an orthonormal basis for the same span.\n       \n                in   using the inner product\n               \n             \n                in   using the inner product  ,\n              where  \n             \n                in   with the weighted inner product\n               \n             "
},
{
  "id": "exercise-386",
  "level": "2",
  "url": "chap_gram_schmidt_ips.html#exercise-386",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Let  .\n       \n              Show that   is a linearly independent set in  .\n             \n              Evaluate an appropriate linear combination at several well chosen values to set up a linear system.\n             \n              Use the Gram-Schmidt process to find an orthogonal basis from   using the inner product\n               \n             \n               \n             "
},
{
  "id": "exercise-387",
  "level": "2",
  "url": "chap_gram_schmidt_ips.html#exercise-387",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Find an orthonormal basis for the subspace   of   that consists of all polynomials of the form\n          using the inner product  .\n       "
},
{
  "id": "exercise-388",
  "level": "2",
  "url": "chap_gram_schmidt_ips.html#exercise-388",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        The Legendre polynomials form an orthonormal basis for the infinite dimensional inner product space   of all polynomials using the inner product\n         .\n        The Legendre polynomials have applications to differential equations,\n        statistics, numerical analysis,\n        and physics (e.g., they appear when solving Schrdinger equation in three dimensions for a central force).\n        The Legendre polynomials are found by using the Gram-Schmidt process to find an orthogonal basis from the standard basis   for  .\n        Find the first four Legendre polynomials by creating a orthonormal basis from the set  .\n       \n         \n       "
},
{
  "id": "exercise-389",
  "level": "2",
  "url": "chap_gram_schmidt_ips.html#exercise-389",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        The sine integral function   has applications in physics and engineering.\n        We define   as\n         .\n        Since we cannot find an elementary formula for  ,\n        we use approximations.\n        Find the best approximation to   in   with inner product  .\n        Use appropriate technology for computations and round output six places to the right of the decimal.\n       "
},
{
  "id": "exercise-390",
  "level": "2",
  "url": "chap_gram_schmidt_ips.html#exercise-390",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        Recall from  \n        in  \n        that any finite dimensional vector space   can be made into an inner product space by setting a basis   for   and defining\n         \n        if   and   in  .\n        Let    and  .\n        (You may assume that   is a basis for  .)\n       \n              Find an orthogonal basis for   containing the polynomial\n                using the inner product  .\n             \n               \n             \n              Find the best approximation possible as measured by the inner product   to the polynomial\n                by a polynomial in  .\n             \n               \n             "
},
{
  "id": "exercise-391",
  "level": "2",
  "url": "chap_gram_schmidt_ips.html#exercise-391",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Let  ,  ,\n        and   be three distinct fixed real numbers and define   by\n         .\n       \n              Show that   as defined in   is an inner product on  .\n             \n              Is the mapping\n               \n              an inner product on  ?\n              Justify your answer.\n             \n              Let  ,  , and  .\n              Find an orthogonal basis for   using the inner product  .\n             \n              Continue with  ,  , and  .\n              Find the best approximation possible as measured by the inner product   to the polynomial\n                by a polynomial in  .\n             \n              If  ,\n              find   in   with respect to the inner product  .\n             "
},
{
  "id": "exercise-392",
  "level": "2",
  "url": "chap_gram_schmidt_ips.html#exercise-392",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n        Throughout, let   be a vector space.\n       True\/False \n              If   is a basis for a subspace   of an inner product space  ,\n              then the vector   is the vector in   closest to  .\n             \n              F\n             True\/False \n              If   is a subspace of an inner product space  ,\n              then the vector   is orthogonal to every vector in  .\n             True\/False \n              If  ,\n               ,\n                are vectors in an inner product space  ,\n              then the Gram-Schmidt process constructs an orthogonal set of vectors\n                with the same span as  .\n             \n              T\n             True\/False \n              Any set\n                of orthogonal vectors in an inner product space   can be extended to an orthogonal basis of  .\n             True\/False \n              Every nontrivial finite dimensional subspace of an inner product space has an orthogonal basis.\n             \n              T\n             True\/False \n              In any inner product space  ,\n              if   is a subspace satisfying  ,\n              then  .\n             "
},
{
  "id": "act_GQ_1",
  "level": "2",
  "url": "chap_gram_schmidt_ips.html#act_GQ_1",
  "type": "Project Activity",
  "number": "36.3",
  "title": "",
  "body": "\n      In this activity we find through direct calculation the node and weight with   so that\n       .\n     \n      There are two unknowns in this situation (  and  ) and so we will need 2 equations to find these unknowns.\n      Keep in mind that we want to have the approximation   be exact for as large of degree polynomials as possible.\n     \n            Assume equality in   if we choose  .\n            Use the resulting equation to find  .\n           \n            Assume equality in   if we choose  .\n            Use the resulting equation to find  .\n           \n            Verify that   is in fact an equality for any linear polynomial of the form  ,\n            using the values of   and   you found\n           "
},
{
  "id": "act_GQ_2",
  "level": "2",
  "url": "chap_gram_schmidt_ips.html#act_GQ_2",
  "type": "Project Activity",
  "number": "36.4",
  "title": "",
  "body": "\n      In this problem we find through direct calculation the nodes and weights with   so that\n       .\n     \n      There are four unknowns in this situation (  and\n       ) and so we will need 4 equations to find these unknowns.\n      Keep in mind that we want to have the approximation   be exact for as large of degree polynomials as possible.\n      In this case we will use  ,\n       ,  , and  .\n     \n            Assume equality in   if we choose  .\n            This gives us an equation in   and  .\n            Find this equation.\n           \n            Assume equality in   if we choose  .\n            This gives us an equation in   and  .\n            Find this equation.\n           \n            Assume equality in   if we choose  .\n            This gives us an equation in   and  .\n            Find this equation.\n           \n            Assume equality in   if we choose  .\n            This gives us an equation in   and  .\n            Find this equation.\n           \n            Solve this system of 4 equations in 4 unknowns.\n            You can do this by hand or with any other appropriate tool.\n            Show that   and   are the roots of the polynomial  .\n           \n            Verify that   is in fact an equality with the values of   and\n              you found for any polynomial of the form  .\n           "
},
{
  "id": "act_GQ_n",
  "level": "2",
  "url": "chap_gram_schmidt_ips.html#act_GQ_n",
  "type": "Project Activity",
  "number": "36.5",
  "title": "",
  "body": "\n      It is inefficient to always solve these systems of equations to find the nodes and weights,\n      especially since there is a more elegant way to find the nodes.\n     \n            Use appropriate technology to find the equations satisfied by the   for  ,\n             , and  .\n           Legendre polynomials "
},
{
  "id": "act_GQ_example",
  "level": "2",
  "url": "chap_gram_schmidt_ips.html#act_GQ_example",
  "type": "Project Activity",
  "number": "36.6",
  "title": "",
  "body": "\n      Let us see now how good the integral estimates are with Gaussian quadrature method using an example.\n      Use Gaussian quadrature with the indicated value of   to approximate  .\n      Be sure to explain how you found your nodes and weights\n      (approximate the nodes and weights to 8 decimal places).\n      Compare the approximations with the actual value of the integral.\n      Use technology as appropriate to help with calculations.\n     \n             \n           \n             \n           \n             \n           "
},
{
  "id": "project-131",
  "level": "2",
  "url": "chap_gram_schmidt_ips.html#project-131",
  "type": "Project Activity",
  "number": "36.7",
  "title": "",
  "body": "\n      Consider the problem of approximating an integral of the form  .\n      Show that the change of variables  ,\n        reduces the integral   to the form  .\n      (This change of variables can be derived by finding a linear function that maps the interval   to the interval  .)\n     "
},
{
  "id": "chap_linear_transformation",
  "level": "1",
  "url": "chap_linear_transformation.html",
  "type": "Section",
  "number": "37",
  "title": "Linear Transformations",
  "body": "Linear Transformations \n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            What is a linear transformation?\n           \n         \n           \n            What is the kernel of a linear transformation?\n            What algebraic structure does a kernel of a linear transformation have?\n           \n         \n           \n            What is a one-to-one linear transformation?\n            How does its kernel tell us if a linear transformation is one-to-one?\n           \n         \n           \n            What is the range of a linear transformation?\n            What algebraic property does the range of a linear transformation possess?\n           \n         \n           \n            What is an onto linear transformation?\n            What relationship is there between the codomain and range if a linear transformation is onto?\n           \n         \n           \n            What is an isomorphism of vector spaces?\n           \n         Application: Fractals fractals Iterated Function System An approximation of the Black Spleenwort fern. Introduction linear transformation linear transformation linear transformation \n        for all   in   and all scalars  .\n       linear \n    This is the property that   respects linear combinations.\n   \n            Consider the transformation   defined by\n             \n            Check that   is not linear by finding two vectors\n              which violate the additive property of linear transformations.\n           \n            Consider the transformation   defined by\n             \n            Check that   is a linear transformation.\n           \n            Let   be the set of all differentiable functions from   to  .\n            Since  ,\n              and\n              for any differentiable functions   and   and any scalar  ,\n            it follows that   is a subspace of  ,\n            the vector space of all functions from   to  .\n            Let   be defined by  .\n            Check that   is a linear transformation.\n           \n            Every matrix transformation is a linear transformation,\n            so we might expect that general linear transformations share some of the properties of matrix transformations.\n            Let   be a linear transformation from a vector space   to a vector space  .\n            Use the linearity properties to show that  ,\n            where   is the additive identity in   and\n              is the additive identity in  .\n            \n           \n             .\n           Onto and One-to-One Transformations \n    Recall that in  \n    we expressed existence and uniqueness questions for matrix equations in terms of one-to-one and onto properties of matrix transformations.\n    The question about the existence of a solution to the matrix equation\n      for any vector  ,\n    where   is an   matrix,\n    is also a question about the existence of a vector   so that  ,\n    where  .\n    If, for each   in   there is at least one   with  ,\n    then   is an onto transformation.\n    We can make a similar definition for any linear transformation.\n   linear transformation onto onto \n    Similarly, the uniqueness of a solution to\n      for any   in   is the same as saying that for any   in  ,\n    there is at most one   in   such that  .\n    A matrix transformation with this property is one-to-one,\n    and we can make a similar definition for any linear transformation.\n   linear transformation onto one-to-one \n    With matrix transformations we saw that there are easy pivot criteria for determining whether a matrix transformation is one-to-one (a pivot in each column) or onto (a pivot in each row).\n    If a linear transformation can be represented as a matrix transformation,\n    then we can use these ideas.\n    However, not every general linear transformation can be easily viewed as a matrix transformation,\n    and in those cases we might have to resort to applying the definitions directly.\n   \n      For each of the following transformations,\n      determine if   is one-to-one and\/or onto.\n     \n              defined by  .\n           \n              defined by  .\n           The Kernel and Range of Linear Transformation \n    As we saw in  ,\n    any linear transformation sends the additive identity to the additive identity.\n    If   is a matrix transformation defined by\n      for some   matrix  ,\n    then we have seen that the set of vectors that   maps to the zero vector is  ,\n    which is also  .\n    We can extend this idea of the kernel of a matrix transformation to any linear transformation  .\n   linear transformation kernel kernel \n    Just as the null space of an\n      matrix   is a subspace of  ,\n    the kernel of a linear transformation from a vector space   to a vector space   is a subspace of  .\n    The proof is left to the exercises.\n   \n        Let   be a linear transformation from a vector space   to vector space  .\n        Then   is a subspace of  .\n       \n    The kernel of a linear transformation provides a convenient way to determine if the linear transformation is one-to-one.\n    If   is one-to-one,\n    then the only solution to   is\n      and   contains only the zero vector.\n    If   is not one-to-one\n    (and the domain of   is not just  ),\n    then the number of solutions to\n      is infinite and   contains more than just the zero vector.\n    We formalize this idea in the next theorem.\n    (Compare to  .)\n    The formal proof is left for the exercises.\n   \n        A linear transformation   from a vector space   to a vector space   is one-to-one if and only if  ,\n        where   is the additive identity in  .\n       \n            Let   be defined by  .\n            Find  .\n            Is   one-to-one?\n            Explain.\n           \n            Let   be defined by  .\n            Find  .\n           range range of a linear transformation range image \n    If   for some   matrix  ,\n    we know that  ,\n    the span of the columns of  ,\n    is a subspace of  .\n    Consequently, the range of  ,\n    which is also the column space of  ,\n    is a subspace of  .\n    In general, the range of a linear transformation   from   to   is a subspace of  .\n    The fact that the range of a transformation is a subspace of the codomain is a consequence of the more general result in the following theorem.\n   \n        Let   be a linear transformation from a vector space   to vector space  ,\n        and let   be a subspace of  .\n        Then the set\n         \n        is a subspace of  .\n       \n      Let   be a linear transformation from a vector space   to vector space  ,\n      and let   be a subspace of  .\n      Let   and   be the additive identities in   and  ,\n      respectively.\n      We have already shown that  ,\n      so   is in  .\n      To show that   is a subspace of   we must also demonstrate that   is in   whenever   and   are in   and that   is in   whenever   is a scalar and   is in  .\n      Let   and   be in  .\n      Then   and\n        for some vectors   and   in  .\n      Since   is a subspace of  ,\n      we know that   is in  .\n      The fact that   is a linear transformation means that\n       .\n     \n      So   is in  .\n     \n      Finally, let   be a scalar.\n      Since   is a subspace of  ,\n      we know that   is in  .\n      The linearity of   gives us\n       ,\n      so   is in  .\n      We conclude that   is a subspace of  .\n     \n    Letting   shows that\n      is a subspace of  .\n   \n    The subspace   provides us with a convenient criterion for the transformation   being onto.\n    The transformation   is onto if for each   in  ,\n    there is at least one   for which  .\n    This means that every   in   belongs to\n      for   to be onto.\n   \n        A linear transformation   from a vector space   to a vector space   is onto if and only if  .\n       \n      Let   be defined by\n        as in  .\n      Describe the vectors in  .\n      Is   onto?\n      Explain.\n     Isomorphisms isomorphic isomorphism isomorphism \n    We this terminology,\n    we summarize the discussion in the following theorem.\n   \n        If   is a vector space of dimension  ,\n        then any coordinate transformation from   to   is an isomorphism.\n       isomorphic \n      Assume that each of the following maps is a linear transformation.\n      Which, if any, is an isomorphism?\n      Justify your reasoning.\n     \n              defined by  \n           \n              defined by  .\n           \n              defined by  .\n           Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Let   be defined by  .\n       \n              Show that   is a linear transformation.\n             \n              To show that   is a linear transformation we must show that   and\n                for every  ,\n                in   and any scalar  .\n              Let   and   be in  .\n              Then\n               \n              and\n               .\n              Therefore,   is a linear transformation.\n             \n              Is   one-to-one?\n              Justify your answer.\n             \n              To determine if   is one-to-one, we find  .\n              Suppose  .\n              Then\n               .\n              Equating coefficients on like powers of   shows that  .\n              Thus,   and  .\n              Thus,   is one-to-one.\n             \n                  Find three different polynomials in  .\n                 \n                  We can find three polynomials in\n                    by applying   to three different polynomials in  .\n                  So three polynomials in   are\n                   .\n                 \n                  Find, if possible, a polynomial that is not in  .\n                 \n                  A polynomial   is in   if\n                    for some polynomial   in  .\n                  This would require that\n                   .\n                  But this would mean that  .\n                  So the polynomial   is not in  .\n                 \n                  Describe  .\n                  What is  ?\n                  Is   an onto transformation?\n                  Explain.\n                 \n                  Let   be in  .\n                  Then   for some polynomial  .\n                  Thus,\n                   .\n                  Therefore,  .\n                  Since the reduced row echelon form of\n                    is  ,\n                  we conclude that the set   is linearly independent.\n                  Thus,  .\n                  Since   is a three-dimensional subspace of the four-dimensional space  ,\n                  it follows that   is not onto.\n                 \n        Let   be defined by\n         .\n       \n        Show that   is an isomorphism from   to  .\n       \n        We need to show that   is a linear transformation,\n        and that   is both one-to-one and onto.\n        We start by demonstrating that   is a linear transformation.\n        Let   and\n          be in  .\n        Then\n         .\n       \n        Now let   be a scalar.\n        Then\n         .\n       \n        Therefore,   is a linear transformation.\n       \n        Next we determine  .\n        Suppose  .\n        Then\n         .\n       \n        Equating coefficients of like powers of   shows that\n          for each   and  .\n        Thus,   and  .\n        It follows that   is one-to-one.\n       \n        Finally, we show that  .\n        If  ,\n        then  .\n        So every polynomial in   is the image of some matrix in  .\n        It follows that   is onto and also that   is an isomorphism.\n       Summary \n    Let   be a linear transformation from a vector space   to a vector space  .\n     \n         \n          A function   from a vector space   to a vector space   is a linear transformation if\n           \n               \n                  for all   and   in   and\n               \n             \n               \n                  for all   in   and all scalars  .\n               \n             \n         \n       \n         \n          Let   be a linear transformation from a vector space   to a vector space  .\n          The kernel of   is the set\n           .\n          The kernel of   is a subspace of  .\n         \n       \n         \n          Let   be a linear transformation from a vector space   to a vector space  .\n          The transformation   is one-to-one if every vector in   is the image under   of at most one vector in  .\n          A linear transformation   is one-to-one if and only if  .\n         \n       \n         \n          Let   be a linear transformation from a vector space   to a vector space  .\n          The range of   is the set\n           .\n          The range of   is a subspace of  .\n         \n       \n         \n          Let   be a linear transformation from a vector space   to a vector space  .\n          The linear transformation   is onto if every vector in   is the image under   of at least one vector in  .\n          The transformation   is onto if and only if  .\n         \n       \n         \n          An isomorphism from a vector space   to a vector space   is a linear transformation\n            that is one-to-one and onto.\n         \n       \n   \n        We have seen that  ,\n        the set of all continuous functions on the interval   is a vector space.\n        Let   be defined by  .\n        Show that   is a linear transformation.\n       \n        Use properties of the definite integral from calculus.\n       \n        If   is a vector space,\n        prove that the identity map   defined by\n          for all   is an isomorphism.\n       \n        Let   and   be vector spaces and let   be an isomorphism.\n        Since   is a bijection,\n          has an inverse function. (Recall that two functions   and   are inverses if\n          for all   in the domain of   and\n          for all   in the domain of  .)\n       \n              Let   be defined by  .\n              Assume that   is an isomorphism.\n              Show that   defined by\n                is the inverse of  .\n             \n              Show that   for all   in   and that\n                for all   in  .\n             \n              Let   be defined by  .\n              Assume that   is an isomorphism.\n              Find the inverse of  .\n              Make sure to verify that your conjectured function really is the inverse of  .\n             \n              Define   by\n               .\n             \n              Now assume that   is an arbitrary isomorphism from a vector space   to a vector space  .\n              We know that the inverse   of   satisfies the property that\n                whenever  .\n              From previous courses you have probably shown that the inverse of a bijection is a bijection.\n              You may assume this as fact.\n              Show that   is a linear transformation and,\n              consequently, an isomorphism.\n             \n              The property that   whenever\n                is the key to this problem.\n             \n        Let  ,  , and   be vector spaces,\n        and let   and\n          be linear transformations.\n        Determine which of the following is true.\n        If a statement is true, verify it.\n        If a statement is false, give a counterexample.\n        (The result of this exercise,\n        along with  \n        and  ,\n        shows that the isomorphism relation is reflexive,\n        symmetric, and transitive.\n        Thus, isomorphism is an equivalence relation.)\n       \n                is a linear transformation\n             \n                is a linear transformation\n             \n              If   and   are one-to-one,\n              then   is one-to-one.\n             \n              If   and   are onto,\n              then   is onto.\n             \n              If   and   are isomorphisms,\n              then   is an isomorphism.\n             \n        For each of the following maps,\n        determine which is a linear transformation.\n        If a mapping is not a linear transformation,\n        provide a specific example to show that a property of a linear transformation is not satisfied.\n        If a mapping is a linear transformation,\n        verify that fact and then determine if the mapping is one-to-one and\/or onto.\n        Throughout, let   and   be vector spaces.\n       \n                defined by  \n             \n              linear transformation, one-to-one, onto\n             \n                defined by  \n             \n              linear transformation, one-to-one, onto\n             \n        Let   be a vector space.\n        In this exercise, we will investigate mappings\n          of the form  ,\n        where   is a scalar.\n       \n              For which values,\n              if any, of   is   a linear transformation?\n             \n              For which values of  ,\n              if any, is   an isomorphism?\n             \n        Let   be a finite-dimensional vector space and   a vector space.\n        Show that if   is an isomorphism,\n        and   is a basis for  ,\n        then  ,  ,\n         ,   is a basis for  .\n        Hence, if   is a vector space of dimension   and   is isomorphic to  ,\n        then   as well.\n       \n        Use the fact that   is one-to-one to show that   is linearly independent,\n        and that   is onto to show that   spans  .\n       \n        Let  .\n       \n              Show that   is a subspace of  .\n             \n              The space   is isomorphic to   for some  .\n              Determine the value of   and explain why   is isomorphic to  .\n             \n        Is it possible for a vector space to be isomorphic to one of its proper subspaces?\n        Justify your answer.\n       \n        Yes, but the vector space needs to be infinite dimensional.\n       \n        Prove  \n        \n       \n        See  .\n       \n        Prove  .\n       \n        Show that   has at most one solution for each   in  .\n       \n        Let   and   be finite dimensional vector spaces,\n        and let   be a linear transformation.\n        Show that   is uniquely determined by its action on a basis for  .\n        That is, if   is a basis for  ,\n        and we know  ,  ,\n         ,  ,\n        then we know exactly which vector   is for any   in  .\n       \n        Let   and   be vectors spaces and let\n          be a linear transformation.\n        If   is a subspace of  , let\n         .\n       \n              Show that   is a subspace of  .\n             \n              Think of   as the range of a suitable restriction of  .\n             \n              Suppose  .\n              Show that  .\n              Can   even if   is not an isomorphism?\n              Justify your answer.\n             \n              Use Exercise 12.\n              It is possible.\n             \n        Let   and   be vector spaces and define   to be the set of all linear transformations from   to  .\n        If   and   are in   and   is a scalar,\n        then define   and   as follows:\n         \n        for all   in  .\n        Prove that   is a vector space.\n       \n        The Rank-Nullity Theorem\n        ( )\n        states that the rank plus the nullity of a matrix equals the number of columns of the matrix.\n        There is a similar result for linear transformations that we prove in this exercise.\n        Let   be a linear transformation from an   dimensional vector space   to an   dimensional vector space  .\n        Show that  . (Hint: Let\n          be a basis for  .\n        Extend this basis to a basis  ,\n         ,  ,  ,\n         ,  ,\n          of  .\n        Use this basis to find a basis for  .)\n       \n        Let   and   be vector spaces with  ,\n        and let   be a linear transformation.\n       \n              Prove or disprove: If   is one-to-one,\n              then   is also onto.\n              \n             \n              Use  .\n             \n              Prove or disprove: If   is onto,\n              then   is also one-to-one.\n              \n             \n              Use  .\n             \n        Suppose   is a basis of an  -dimensional vector space  ,\n        and   is a basis of an  -dimensional vector space  .\n        Show that the map   which sends every   in   to the vector   in   such that\n          is a linear transformation.\n        (Combined with  \n        in  ,\n        we can conclude that   is an isomorphism.\n        Thus, any two vectors space of dimension   are isomorphic.)\n       \n        There is an important characterization of linear functionals that we examine in this problem.\n        A linear functional is a linear transformation from a vector space   to  .\n        Let   be a finite-dimensional inner product space,\n        and let   be a linear functional.\n        We will show that there is a vector   in   such that\n         \n        for every vector   in  .\n        That is, every linear functional on an inner product space can be represented by an inner product with a fixed vector.\n        Let  .\n       \n              Suppose  .\n              What vector could we use for  ?\n             \n              Now assume that  .\n              Note that   is a subspace of  .\n              Explain why the only subspaces of   are   and  .\n              Conclude that  .\n             \n              Explain why  .\n             \n              Let   be an orthonormal basis for  .\n              Explain why there is a vector   in   so that\n                is an orthonormal basis for  .\n             \n              Let   be in  .\n              Explain why\n               .\n             \n              Now explain why\n               .\n             \n              Finally, explain why   is the vector   we are looking for.\n             \n              As an example,\n              let   be defined by  .\n              Consider   as an inner product space with the inner product\n               .\n              Find a polynomial   so that\n               \n              for every   in  .\n             \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              The mapping   defined by\n                is a linear transformation.\n             \n              F\n             True\/False \n              The mapping   defined by\n                if   is invertible and  \n              (the zero matrix)\n              otherwise is a linear transformation.\n             True\/False \n              Let   and   be vector spaces.\n              The mapping   defined by\n                is a linear transformation.\n             \n              T\n             True\/False \n              If   is a linear transformation,\n              and if   is a linearly independent set in  ,\n              then the set   is a linearly independent set in  .\n             True\/False \n              A one-to-one transformation is a transformation where each input has a unique output.\n             \n              F\n             True\/False \n              A one-to-one linear transformation is a transformation where each output can only come from a unique input.\n             True\/False \n              Let   and   be vector spaces with\n                and  .\n              A linear transformation from   to   cannot be onto.\n             \n              T\n             True\/False \n              Let   and   be vector spaces with\n                and  .\n              A linear transformation from   to   cannot be onto.\n             True\/False \n              Let   and   be vector spaces with\n                and  .\n              A linear transformation from   to   cannot be one-to-one.\n             \n              F\n             True\/False \n              If   is in the range of a linear transformation  ,\n              then there is an   in the domain of   such that  .\n             True\/False \n              If   is a one-to-one linear transformation,\n              then   has a non-trivial solution.\n             Project: Fractals via Iterated Function Systems \n    In this project we will see how to use linear transformations to create iterated functions systems to generate fractals.\n    We illustrate the idea with the Sierpinski triangle,\n    an approximate picture of which is shown at left in  .\n   Left: An approximation of the Sierpinski triangle. Right: A triangle. \n    To make this figure,\n    we need to identify linear transformations that can be put together to produce the Sierpinski triangle.\n   \n      Let  ,\n       ,\n      and   be three vectors in the plane whose endpoints form the vertices of a triangle   as shown at right in  .\n      Let   be the linear transformation defined by  ,\n      where  .\n     \n            What are  ,  , and  ?\n            Draw a picture of the figure   whose vertices are the endpoint of these three vectors.\n            How is   related to  ?\n           contraction mapping \n     \n    contains the information we need to create an iterated function system to produce the Sierpinski triangle.\n    One more step should help us understand the general process.\n   \n      Using the results of  ,\n      define   to be   for each  .\n      That is,  ,\n       , and  .\n      So   is a triangle half the size of the original translated to the  th vertex of the original.\n      Let  .\n      That is,   is the union of the shaded triangles in  .\n     ,  , and  . \n          Apply   from   to  .\n          What is the resulting figure?\n          Draw a picture to illustrate.\n         \n          Apply   from   to  .\n          What is the resulting figure?\n          Draw a picture to illustrate.\n         \n          Apply   from   to  .\n          What is the resulting figure?\n          Draw a picture to illustrate.\n         \n    The procedures from  \n    and   can be continued,\n    replacing   with  ,\n    then  , and so on.\n    In other words,\n    for   = 1, 2, and 3, let  .\n    Then let  .\n    A picture of   is shown at left in  .\n    We can continue this procedure,\n    each time replacing   with  .\n    A picture of   is shown at right in  .\n   Left:  . Right:  . deterministic algorithm attractor self-similar \n    If we have an IFS, then we can determine the attractor by drawing the sequence of sets that the IFS generates.\n    A more interesting problem is,\n    given a self-similar figure,\n    whether we can construct an IFS that has that figure as its attractor.\n   \n      A picture of an emerging Sierpinski carpet is shown at left in  .\n      A Sage cell to illustrate this algorithm for producing approximations to the Sierpinski carpet can be found at  . In this activity we will see how to find an iterated function system that will generate this fractal.\n     A Sierpinski carpet. \n          To create an IFS to generate this fractal,\n          we need to understand how many self-similar pieces make up this figure.\n          Use the image at right in  \n          to determine how many pieces we need.\n         \n          For each of the self-similar pieces identified in part (a),\n          find a linear transformation and a translation that maps the entire figure to the self-similar piece.\n          \n         \n          You could assume that the carpet is embedded in the unit square.\n         \n          Test your IFS to make sure that it actually generates the Sierpinski carpet.\n          There are many websites that allow you to do this,\n          one of which is  . In this program,\n          the mapping   defined by\n           \n          is represented as the string\n           .\n          Most programs generally use a different algorithm to create the attractor,\n          plotting points instead of sets.\n          In this algorithm,\n          each contraction mapping is assigned a probability as well\n          (the larger parts of the figure are usually given higher probabilities),\n          so you will enter each contraction mapping in the form\n           \n          where   is the probability attached to that contraction mapping.\n          As an example, the IFS code for the Sierpinski triangle is\n           \n         \n    The contraction mappings we have used so far only involve contractions and translations.\n    But we do not have to restrict ourselves to just these types of contraction mappings.\n   \n      Consider the fractal represented in  .\n      Find an IFS that generates this fractal.\n      Explain your reasoning.\n     \n      Check with a fractal generator to ensure that you have an appropriate IFS.\n     A fractal. \n        Two reflections are involved.\n       \n    We conclude our construction of fractals with one more example.\n    The contraction mappings in iterated function can also involve rotations.\n   \n      Consider the Lvy Dragon fractal shown at left in  .\n      Find an IFS that generates this fractal.\n      Explain your reasoning.\n     \n      Check with a fractal generator to ensure that you have an appropriate IFS.\n     The Lvy Dragon. \n        Two rotations are involved   think of the fractal as contained in a blue triangle as shown at right in  .\n       \n    We end this project with a short discussion of fractal dimension.\n    The fractals we have seen are very strange in many ways,\n    one of which is dimension.\n    We have studied the dimensions of subspaces of     each subspace has an integer dimension that is determined by the number of elements in a basis.\n    Fractals also have a dimension,\n    but the dimension of a fractal is generally not an integer.\n    To try to understand fractal dimension,\n    notice that a line segment is self-similar.\n    We can break up a line segment into 2 non-overlapping line segments of the same length,\n    or 3, or 4, or any number of non-overlapping segments of the same length.\n    A square is slightly different.\n    We can partition a square into 4 non-overlapping squares,\n    or 9 non-overlapping squares,\n    or   non-overlapping squares for any positive integer   as shown in  .\n   Self-similar squares. \n    Similarly, we can break up a cube into   non-overlapping congruent cubes.\n    A line segment lies in a one-dimensional space,\n    a square in a two-dimensional space,\n    and a cube in the three-dimensional space.\n    Notice that these dimensions correspond to the exponent of the number of self-similar pieces with scaling   into which we can partition the object.\n    We can use this idea of dimension in a way that we can apply to fractals.\n    Let   be the dimension of the object.\n    We can partition a square into   non-overlapping squares, so\n     .\n   \n    Similarly, for the cube we have\n     .\n   \n    We can then take this as our definition of the dimension of a self-similar object,\n    when the scaling factors are all the same\n    (the fern fractal in  \n    is an example of a fractal generated by an iterated function system in which the scaling factors are not all the same).\n   Hausdorff dimension fractal Hausdorff \n      Find the fractal dimensions of the Sierpinski triangle and the Sierpinski carpet.\n      These are well-known and you can look them up to check your result.\n      Then find the fractal dimension of the fractal with IFS\n       .\n     \n      You might want to draw this fractal using an online generator.\n     "
},
{
  "id": "objectives-37",
  "level": "2",
  "url": "chap_linear_transformation.html#objectives-37",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            What is a linear transformation?\n           \n         \n           \n            What is the kernel of a linear transformation?\n            What algebraic structure does a kernel of a linear transformation have?\n           \n         \n           \n            What is a one-to-one linear transformation?\n            How does its kernel tell us if a linear transformation is one-to-one?\n           \n         \n           \n            What is the range of a linear transformation?\n            What algebraic property does the range of a linear transformation possess?\n           \n         \n           \n            What is an onto linear transformation?\n            What relationship is there between the codomain and range if a linear transformation is onto?\n           \n         \n           \n            What is an isomorphism of vector spaces?\n           \n         "
},
{
  "id": "p-6465",
  "level": "2",
  "url": "chap_linear_transformation.html#p-6465",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "fractals Iterated Function System "
},
{
  "id": "F_Fern",
  "level": "2",
  "url": "chap_linear_transformation.html#F_Fern",
  "type": "Figure",
  "number": "37.1",
  "title": "",
  "body": "An approximation of the Black Spleenwort fern. "
},
{
  "id": "p-6466",
  "level": "2",
  "url": "chap_linear_transformation.html#p-6466",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "linear transformation "
},
{
  "id": "definition-87",
  "level": "2",
  "url": "chap_linear_transformation.html#definition-87",
  "type": "Definition",
  "number": "37.2",
  "title": "",
  "body": "linear transformation linear transformation \n        for all   in   and all scalars  .\n       "
},
{
  "id": "p-6471",
  "level": "2",
  "url": "chap_linear_transformation.html#p-6471",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "linear "
},
{
  "id": "pa_8_a",
  "level": "2",
  "url": "chap_linear_transformation.html#pa_8_a",
  "type": "Preview Activity",
  "number": "37.1",
  "title": "",
  "body": "\n            Consider the transformation   defined by\n             \n            Check that   is not linear by finding two vectors\n              which violate the additive property of linear transformations.\n           \n            Consider the transformation   defined by\n             \n            Check that   is a linear transformation.\n           \n            Let   be the set of all differentiable functions from   to  .\n            Since  ,\n              and\n              for any differentiable functions   and   and any scalar  ,\n            it follows that   is a subspace of  ,\n            the vector space of all functions from   to  .\n            Let   be defined by  .\n            Check that   is a linear transformation.\n           \n            Every matrix transformation is a linear transformation,\n            so we might expect that general linear transformations share some of the properties of matrix transformations.\n            Let   be a linear transformation from a vector space   to a vector space  .\n            Use the linearity properties to show that  ,\n            where   is the additive identity in   and\n              is the additive identity in  .\n            \n           \n             .\n           "
},
{
  "id": "definition-88",
  "level": "2",
  "url": "chap_linear_transformation.html#definition-88",
  "type": "Definition",
  "number": "37.3",
  "title": "",
  "body": "linear transformation onto onto "
},
{
  "id": "definition-89",
  "level": "2",
  "url": "chap_linear_transformation.html#definition-89",
  "type": "Definition",
  "number": "37.4",
  "title": "",
  "body": "linear transformation onto one-to-one "
},
{
  "id": "activity-143",
  "level": "2",
  "url": "chap_linear_transformation.html#activity-143",
  "type": "Activity",
  "number": "37.2",
  "title": "",
  "body": "\n      For each of the following transformations,\n      determine if   is one-to-one and\/or onto.\n     \n              defined by  .\n           \n              defined by  .\n           "
},
{
  "id": "definition-90",
  "level": "2",
  "url": "chap_linear_transformation.html#definition-90",
  "type": "Definition",
  "number": "37.5",
  "title": "",
  "body": "linear transformation kernel kernel "
},
{
  "id": "thm_8_a_ker",
  "level": "2",
  "url": "chap_linear_transformation.html#thm_8_a_ker",
  "type": "Theorem",
  "number": "37.6",
  "title": "",
  "body": "\n        Let   be a linear transformation from a vector space   to vector space  .\n        Then   is a subspace of  .\n       "
},
{
  "id": "thm_8_a_one_to_one_kernel",
  "level": "2",
  "url": "chap_linear_transformation.html#thm_8_a_one_to_one_kernel",
  "type": "Theorem",
  "number": "37.7",
  "title": "",
  "body": "\n        A linear transformation   from a vector space   to a vector space   is one-to-one if and only if  ,\n        where   is the additive identity in  .\n       "
},
{
  "id": "act_8_a_Lin_trans_1",
  "level": "2",
  "url": "chap_linear_transformation.html#act_8_a_Lin_trans_1",
  "type": "Activity",
  "number": "37.3",
  "title": "",
  "body": "\n            Let   be defined by  .\n            Find  .\n            Is   one-to-one?\n            Explain.\n           \n            Let   be defined by  .\n            Find  .\n           "
},
{
  "id": "p-6494",
  "level": "2",
  "url": "chap_linear_transformation.html#p-6494",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "range "
},
{
  "id": "definition-91",
  "level": "2",
  "url": "chap_linear_transformation.html#definition-91",
  "type": "Definition",
  "number": "37.8",
  "title": "",
  "body": "range of a linear transformation range "
},
{
  "id": "p-6496",
  "level": "2",
  "url": "chap_linear_transformation.html#p-6496",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "image "
},
{
  "id": "theorem-98",
  "level": "2",
  "url": "chap_linear_transformation.html#theorem-98",
  "type": "Theorem",
  "number": "37.9",
  "title": "",
  "body": "\n        Let   be a linear transformation from a vector space   to vector space  ,\n        and let   be a subspace of  .\n        Then the set\n         \n        is a subspace of  .\n       \n      Let   be a linear transformation from a vector space   to vector space  ,\n      and let   be a subspace of  .\n      Let   and   be the additive identities in   and  ,\n      respectively.\n      We have already shown that  ,\n      so   is in  .\n      To show that   is a subspace of   we must also demonstrate that   is in   whenever   and   are in   and that   is in   whenever   is a scalar and   is in  .\n      Let   and   be in  .\n      Then   and\n        for some vectors   and   in  .\n      Since   is a subspace of  ,\n      we know that   is in  .\n      The fact that   is a linear transformation means that\n       .\n     \n      So   is in  .\n     \n      Finally, let   be a scalar.\n      Since   is a subspace of  ,\n      we know that   is in  .\n      The linearity of   gives us\n       ,\n      so   is in  .\n      We conclude that   is a subspace of  .\n     "
},
{
  "id": "thm_8_a_onto_range",
  "level": "2",
  "url": "chap_linear_transformation.html#thm_8_a_onto_range",
  "type": "Theorem",
  "number": "37.10",
  "title": "",
  "body": "\n        A linear transformation   from a vector space   to a vector space   is onto if and only if  .\n       "
},
{
  "id": "activity-145",
  "level": "2",
  "url": "chap_linear_transformation.html#activity-145",
  "type": "Activity",
  "number": "37.4",
  "title": "",
  "body": "\n      Let   be defined by\n        as in  .\n      Describe the vectors in  .\n      Is   onto?\n      Explain.\n     "
},
{
  "id": "p-6506",
  "level": "2",
  "url": "chap_linear_transformation.html#p-6506",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "isomorphic isomorphism "
},
{
  "id": "definition-92",
  "level": "2",
  "url": "chap_linear_transformation.html#definition-92",
  "type": "Definition",
  "number": "37.11",
  "title": "",
  "body": "isomorphism "
},
{
  "id": "theorem-100",
  "level": "2",
  "url": "chap_linear_transformation.html#theorem-100",
  "type": "Theorem",
  "number": "37.12",
  "title": "",
  "body": "\n        If   is a vector space of dimension  ,\n        then any coordinate transformation from   to   is an isomorphism.\n       "
},
{
  "id": "p-6510",
  "level": "2",
  "url": "chap_linear_transformation.html#p-6510",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "isomorphic "
},
{
  "id": "activity-146",
  "level": "2",
  "url": "chap_linear_transformation.html#activity-146",
  "type": "Activity",
  "number": "37.5",
  "title": "",
  "body": "\n      Assume that each of the following maps is a linear transformation.\n      Which, if any, is an isomorphism?\n      Justify your reasoning.\n     \n              defined by  \n           \n              defined by  .\n           \n              defined by  .\n           "
},
{
  "id": "example-77",
  "level": "2",
  "url": "chap_linear_transformation.html#example-77",
  "type": "Example",
  "number": "37.13",
  "title": "",
  "body": "\n        Let   be defined by  .\n       \n              Show that   is a linear transformation.\n             \n              To show that   is a linear transformation we must show that   and\n                for every  ,\n                in   and any scalar  .\n              Let   and   be in  .\n              Then\n               \n              and\n               .\n              Therefore,   is a linear transformation.\n             \n              Is   one-to-one?\n              Justify your answer.\n             \n              To determine if   is one-to-one, we find  .\n              Suppose  .\n              Then\n               .\n              Equating coefficients on like powers of   shows that  .\n              Thus,   and  .\n              Thus,   is one-to-one.\n             \n                  Find three different polynomials in  .\n                 \n                  We can find three polynomials in\n                    by applying   to three different polynomials in  .\n                  So three polynomials in   are\n                   .\n                 \n                  Find, if possible, a polynomial that is not in  .\n                 \n                  A polynomial   is in   if\n                    for some polynomial   in  .\n                  This would require that\n                   .\n                  But this would mean that  .\n                  So the polynomial   is not in  .\n                 \n                  Describe  .\n                  What is  ?\n                  Is   an onto transformation?\n                  Explain.\n                 \n                  Let   be in  .\n                  Then   for some polynomial  .\n                  Thus,\n                   .\n                  Therefore,  .\n                  Since the reduced row echelon form of\n                    is  ,\n                  we conclude that the set   is linearly independent.\n                  Thus,  .\n                  Since   is a three-dimensional subspace of the four-dimensional space  ,\n                  it follows that   is not onto.\n                 "
},
{
  "id": "example-78",
  "level": "2",
  "url": "chap_linear_transformation.html#example-78",
  "type": "Example",
  "number": "37.14",
  "title": "",
  "body": "\n        Let   be defined by\n         .\n       \n        Show that   is an isomorphism from   to  .\n       \n        We need to show that   is a linear transformation,\n        and that   is both one-to-one and onto.\n        We start by demonstrating that   is a linear transformation.\n        Let   and\n          be in  .\n        Then\n         .\n       \n        Now let   be a scalar.\n        Then\n         .\n       \n        Therefore,   is a linear transformation.\n       \n        Next we determine  .\n        Suppose  .\n        Then\n         .\n       \n        Equating coefficients of like powers of   shows that\n          for each   and  .\n        Thus,   and  .\n        It follows that   is one-to-one.\n       \n        Finally, we show that  .\n        If  ,\n        then  .\n        So every polynomial in   is the image of some matrix in  .\n        It follows that   is onto and also that   is an isomorphism.\n       "
},
{
  "id": "exercise-393",
  "level": "2",
  "url": "chap_linear_transformation.html#exercise-393",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        We have seen that  ,\n        the set of all continuous functions on the interval   is a vector space.\n        Let   be defined by  .\n        Show that   is a linear transformation.\n       \n        Use properties of the definite integral from calculus.\n       "
},
{
  "id": "ex_8_a_reflexive_isomorphism",
  "level": "2",
  "url": "chap_linear_transformation.html#ex_8_a_reflexive_isomorphism",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        If   is a vector space,\n        prove that the identity map   defined by\n          for all   is an isomorphism.\n       "
},
{
  "id": "ex_8_a_inverse_isomorphism",
  "level": "2",
  "url": "chap_linear_transformation.html#ex_8_a_inverse_isomorphism",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Let   and   be vector spaces and let   be an isomorphism.\n        Since   is a bijection,\n          has an inverse function. (Recall that two functions   and   are inverses if\n          for all   in the domain of   and\n          for all   in the domain of  .)\n       \n              Let   be defined by  .\n              Assume that   is an isomorphism.\n              Show that   defined by\n                is the inverse of  .\n             \n              Show that   for all   in   and that\n                for all   in  .\n             \n              Let   be defined by  .\n              Assume that   is an isomorphism.\n              Find the inverse of  .\n              Make sure to verify that your conjectured function really is the inverse of  .\n             \n              Define   by\n               .\n             \n              Now assume that   is an arbitrary isomorphism from a vector space   to a vector space  .\n              We know that the inverse   of   satisfies the property that\n                whenever  .\n              From previous courses you have probably shown that the inverse of a bijection is a bijection.\n              You may assume this as fact.\n              Show that   is a linear transformation and,\n              consequently, an isomorphism.\n             \n              The property that   whenever\n                is the key to this problem.\n             "
},
{
  "id": "exercise-396",
  "level": "2",
  "url": "chap_linear_transformation.html#exercise-396",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Let  ,  , and   be vector spaces,\n        and let   and\n          be linear transformations.\n        Determine which of the following is true.\n        If a statement is true, verify it.\n        If a statement is false, give a counterexample.\n        (The result of this exercise,\n        along with  \n        and  ,\n        shows that the isomorphism relation is reflexive,\n        symmetric, and transitive.\n        Thus, isomorphism is an equivalence relation.)\n       \n                is a linear transformation\n             \n                is a linear transformation\n             \n              If   and   are one-to-one,\n              then   is one-to-one.\n             \n              If   and   are onto,\n              then   is onto.\n             \n              If   and   are isomorphisms,\n              then   is an isomorphism.\n             "
},
{
  "id": "exercise-397",
  "level": "2",
  "url": "chap_linear_transformation.html#exercise-397",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        For each of the following maps,\n        determine which is a linear transformation.\n        If a mapping is not a linear transformation,\n        provide a specific example to show that a property of a linear transformation is not satisfied.\n        If a mapping is a linear transformation,\n        verify that fact and then determine if the mapping is one-to-one and\/or onto.\n        Throughout, let   and   be vector spaces.\n       \n                defined by  \n             \n              linear transformation, one-to-one, onto\n             \n                defined by  \n             \n              linear transformation, one-to-one, onto\n             "
},
{
  "id": "exercise-398",
  "level": "2",
  "url": "chap_linear_transformation.html#exercise-398",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Let   be a vector space.\n        In this exercise, we will investigate mappings\n          of the form  ,\n        where   is a scalar.\n       \n              For which values,\n              if any, of   is   a linear transformation?\n             \n              For which values of  ,\n              if any, is   an isomorphism?\n             "
},
{
  "id": "ex_8_a_isomorphic_dimension",
  "level": "2",
  "url": "chap_linear_transformation.html#ex_8_a_isomorphic_dimension",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        Let   be a finite-dimensional vector space and   a vector space.\n        Show that if   is an isomorphism,\n        and   is a basis for  ,\n        then  ,  ,\n         ,   is a basis for  .\n        Hence, if   is a vector space of dimension   and   is isomorphic to  ,\n        then   as well.\n       \n        Use the fact that   is one-to-one to show that   is linearly independent,\n        and that   is onto to show that   spans  .\n       "
},
{
  "id": "exercise-400",
  "level": "2",
  "url": "chap_linear_transformation.html#exercise-400",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        Let  .\n       \n              Show that   is a subspace of  .\n             \n              The space   is isomorphic to   for some  .\n              Determine the value of   and explain why   is isomorphic to  .\n             "
},
{
  "id": "exercise-401",
  "level": "2",
  "url": "chap_linear_transformation.html#exercise-401",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        Is it possible for a vector space to be isomorphic to one of its proper subspaces?\n        Justify your answer.\n       \n        Yes, but the vector space needs to be infinite dimensional.\n       "
},
{
  "id": "exercise-402",
  "level": "2",
  "url": "chap_linear_transformation.html#exercise-402",
  "type": "Exercise",
  "number": "10",
  "title": "",
  "body": "\n        Prove  \n        \n       \n        See  .\n       "
},
{
  "id": "exercise-403",
  "level": "2",
  "url": "chap_linear_transformation.html#exercise-403",
  "type": "Exercise",
  "number": "11",
  "title": "",
  "body": "\n        Prove  .\n       \n        Show that   has at most one solution for each   in  .\n       "
},
{
  "id": "exercise-404",
  "level": "2",
  "url": "chap_linear_transformation.html#exercise-404",
  "type": "Exercise",
  "number": "12",
  "title": "",
  "body": "\n        Let   and   be finite dimensional vector spaces,\n        and let   be a linear transformation.\n        Show that   is uniquely determined by its action on a basis for  .\n        That is, if   is a basis for  ,\n        and we know  ,  ,\n         ,  ,\n        then we know exactly which vector   is for any   in  .\n       "
},
{
  "id": "exercise-405",
  "level": "2",
  "url": "chap_linear_transformation.html#exercise-405",
  "type": "Exercise",
  "number": "13",
  "title": "",
  "body": "\n        Let   and   be vectors spaces and let\n          be a linear transformation.\n        If   is a subspace of  , let\n         .\n       \n              Show that   is a subspace of  .\n             \n              Think of   as the range of a suitable restriction of  .\n             \n              Suppose  .\n              Show that  .\n              Can   even if   is not an isomorphism?\n              Justify your answer.\n             \n              Use Exercise 12.\n              It is possible.\n             "
},
{
  "id": "ex_8_a_transformation_space",
  "level": "2",
  "url": "chap_linear_transformation.html#ex_8_a_transformation_space",
  "type": "Exercise",
  "number": "14",
  "title": "",
  "body": "\n        Let   and   be vector spaces and define   to be the set of all linear transformations from   to  .\n        If   and   are in   and   is a scalar,\n        then define   and   as follows:\n         \n        for all   in  .\n        Prove that   is a vector space.\n       "
},
{
  "id": "ex_8_a_rank_nullity",
  "level": "2",
  "url": "chap_linear_transformation.html#ex_8_a_rank_nullity",
  "type": "Exercise",
  "number": "15",
  "title": "",
  "body": "\n        The Rank-Nullity Theorem\n        ( )\n        states that the rank plus the nullity of a matrix equals the number of columns of the matrix.\n        There is a similar result for linear transformations that we prove in this exercise.\n        Let   be a linear transformation from an   dimensional vector space   to an   dimensional vector space  .\n        Show that  . (Hint: Let\n          be a basis for  .\n        Extend this basis to a basis  ,\n         ,  ,  ,\n         ,  ,\n          of  .\n        Use this basis to find a basis for  .)\n       "
},
{
  "id": "exercise-408",
  "level": "2",
  "url": "chap_linear_transformation.html#exercise-408",
  "type": "Exercise",
  "number": "16",
  "title": "",
  "body": "\n        Let   and   be vector spaces with  ,\n        and let   be a linear transformation.\n       \n              Prove or disprove: If   is one-to-one,\n              then   is also onto.\n              \n             \n              Use  .\n             \n              Prove or disprove: If   is onto,\n              then   is also one-to-one.\n              \n             \n              Use  .\n             "
},
{
  "id": "exercise-409",
  "level": "2",
  "url": "chap_linear_transformation.html#exercise-409",
  "type": "Exercise",
  "number": "17",
  "title": "",
  "body": "\n        Suppose   is a basis of an  -dimensional vector space  ,\n        and   is a basis of an  -dimensional vector space  .\n        Show that the map   which sends every   in   to the vector   in   such that\n          is a linear transformation.\n        (Combined with  \n        in  ,\n        we can conclude that   is an isomorphism.\n        Thus, any two vectors space of dimension   are isomorphic.)\n       "
},
{
  "id": "exercise-410",
  "level": "2",
  "url": "chap_linear_transformation.html#exercise-410",
  "type": "Exercise",
  "number": "18",
  "title": "",
  "body": "\n        There is an important characterization of linear functionals that we examine in this problem.\n        A linear functional is a linear transformation from a vector space   to  .\n        Let   be a finite-dimensional inner product space,\n        and let   be a linear functional.\n        We will show that there is a vector   in   such that\n         \n        for every vector   in  .\n        That is, every linear functional on an inner product space can be represented by an inner product with a fixed vector.\n        Let  .\n       \n              Suppose  .\n              What vector could we use for  ?\n             \n              Now assume that  .\n              Note that   is a subspace of  .\n              Explain why the only subspaces of   are   and  .\n              Conclude that  .\n             \n              Explain why  .\n             \n              Let   be an orthonormal basis for  .\n              Explain why there is a vector   in   so that\n                is an orthonormal basis for  .\n             \n              Let   be in  .\n              Explain why\n               .\n             \n              Now explain why\n               .\n             \n              Finally, explain why   is the vector   we are looking for.\n             \n              As an example,\n              let   be defined by  .\n              Consider   as an inner product space with the inner product\n               .\n              Find a polynomial   so that\n               \n              for every   in  .\n             "
},
{
  "id": "exercise-411",
  "level": "2",
  "url": "chap_linear_transformation.html#exercise-411",
  "type": "Exercise",
  "number": "19",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              The mapping   defined by\n                is a linear transformation.\n             \n              F\n             True\/False \n              The mapping   defined by\n                if   is invertible and  \n              (the zero matrix)\n              otherwise is a linear transformation.\n             True\/False \n              Let   and   be vector spaces.\n              The mapping   defined by\n                is a linear transformation.\n             \n              T\n             True\/False \n              If   is a linear transformation,\n              and if   is a linearly independent set in  ,\n              then the set   is a linearly independent set in  .\n             True\/False \n              A one-to-one transformation is a transformation where each input has a unique output.\n             \n              F\n             True\/False \n              A one-to-one linear transformation is a transformation where each output can only come from a unique input.\n             True\/False \n              Let   and   be vector spaces with\n                and  .\n              A linear transformation from   to   cannot be onto.\n             \n              T\n             True\/False \n              Let   and   be vector spaces with\n                and  .\n              A linear transformation from   to   cannot be onto.\n             True\/False \n              Let   and   be vector spaces with\n                and  .\n              A linear transformation from   to   cannot be one-to-one.\n             \n              F\n             True\/False \n              If   is in the range of a linear transformation  ,\n              then there is an   in the domain of   such that  .\n             True\/False \n              If   is a one-to-one linear transformation,\n              then   has a non-trivial solution.\n             "
},
{
  "id": "F_Striangle",
  "level": "2",
  "url": "chap_linear_transformation.html#F_Striangle",
  "type": "Figure",
  "number": "37.15",
  "title": "",
  "body": "Left: An approximation of the Sierpinski triangle. Right: A triangle. "
},
{
  "id": "act_IFS_1",
  "level": "2",
  "url": "chap_linear_transformation.html#act_IFS_1",
  "type": "Project Activity",
  "number": "37.6",
  "title": "",
  "body": "\n      Let  ,\n       ,\n      and   be three vectors in the plane whose endpoints form the vertices of a triangle   as shown at right in  .\n      Let   be the linear transformation defined by  ,\n      where  .\n     \n            What are  ,  , and  ?\n            Draw a picture of the figure   whose vertices are the endpoint of these three vectors.\n            How is   related to  ?\n           contraction mapping "
},
{
  "id": "act_IFS_2",
  "level": "2",
  "url": "chap_linear_transformation.html#act_IFS_2",
  "type": "Project Activity",
  "number": "37.7",
  "title": "",
  "body": "\n      Using the results of  ,\n      define   to be   for each  .\n      That is,  ,\n       , and  .\n      So   is a triangle half the size of the original translated to the  th vertex of the original.\n      Let  .\n      That is,   is the union of the shaded triangles in  .\n     ,  , and  . \n          Apply   from   to  .\n          What is the resulting figure?\n          Draw a picture to illustrate.\n         \n          Apply   from   to  .\n          What is the resulting figure?\n          Draw a picture to illustrate.\n         \n          Apply   from   to  .\n          What is the resulting figure?\n          Draw a picture to illustrate.\n         "
},
{
  "id": "F_Striangle_2",
  "level": "2",
  "url": "chap_linear_transformation.html#F_Striangle_2",
  "type": "Figure",
  "number": "37.17",
  "title": "",
  "body": "Left:  . Right:  . "
},
{
  "id": "p-6630",
  "level": "2",
  "url": "chap_linear_transformation.html#p-6630",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "deterministic algorithm "
},
{
  "id": "p-6631",
  "level": "2",
  "url": "chap_linear_transformation.html#p-6631",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "attractor "
},
{
  "id": "p-6632",
  "level": "2",
  "url": "chap_linear_transformation.html#p-6632",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "self-similar "
},
{
  "id": "IFS_carpet",
  "level": "2",
  "url": "chap_linear_transformation.html#IFS_carpet",
  "type": "Project Activity",
  "number": "37.8",
  "title": "",
  "body": "\n      A picture of an emerging Sierpinski carpet is shown at left in  .\n      A Sage cell to illustrate this algorithm for producing approximations to the Sierpinski carpet can be found at  . In this activity we will see how to find an iterated function system that will generate this fractal.\n     A Sierpinski carpet. \n          To create an IFS to generate this fractal,\n          we need to understand how many self-similar pieces make up this figure.\n          Use the image at right in  \n          to determine how many pieces we need.\n         \n          For each of the self-similar pieces identified in part (a),\n          find a linear transformation and a translation that maps the entire figure to the self-similar piece.\n          \n         \n          You could assume that the carpet is embedded in the unit square.\n         \n          Test your IFS to make sure that it actually generates the Sierpinski carpet.\n          There are many websites that allow you to do this,\n          one of which is  . In this program,\n          the mapping   defined by\n           \n          is represented as the string\n           .\n          Most programs generally use a different algorithm to create the attractor,\n          plotting points instead of sets.\n          In this algorithm,\n          each contraction mapping is assigned a probability as well\n          (the larger parts of the figure are usually given higher probabilities),\n          so you will enter each contraction mapping in the form\n           \n          where   is the probability attached to that contraction mapping.\n          As an example, the IFS code for the Sierpinski triangle is\n           \n         "
},
{
  "id": "act_Fractal_reflection",
  "level": "2",
  "url": "chap_linear_transformation.html#act_Fractal_reflection",
  "type": "Project Activity",
  "number": "37.9",
  "title": "",
  "body": "\n      Consider the fractal represented in  .\n      Find an IFS that generates this fractal.\n      Explain your reasoning.\n     \n      Check with a fractal generator to ensure that you have an appropriate IFS.\n     A fractal. \n        Two reflections are involved.\n       "
},
{
  "id": "act_Fractal_dragon",
  "level": "2",
  "url": "chap_linear_transformation.html#act_Fractal_dragon",
  "type": "Project Activity",
  "number": "37.10",
  "title": "",
  "body": "\n      Consider the Lvy Dragon fractal shown at left in  .\n      Find an IFS that generates this fractal.\n      Explain your reasoning.\n     \n      Check with a fractal generator to ensure that you have an appropriate IFS.\n     The Lvy Dragon. \n        Two rotations are involved   think of the fractal as contained in a blue triangle as shown at right in  .\n       "
},
{
  "id": "F_Squares",
  "level": "2",
  "url": "chap_linear_transformation.html#F_Squares",
  "type": "Figure",
  "number": "37.21",
  "title": "",
  "body": "Self-similar squares. "
},
{
  "id": "definition-93",
  "level": "2",
  "url": "chap_linear_transformation.html#definition-93",
  "type": "Definition",
  "number": "37.22",
  "title": "",
  "body": "Hausdorff dimension fractal Hausdorff "
},
{
  "id": "act_IFS_dimension",
  "level": "2",
  "url": "chap_linear_transformation.html#act_IFS_dimension",
  "type": "Project Activity",
  "number": "37.11",
  "title": "",
  "body": "\n      Find the fractal dimensions of the Sierpinski triangle and the Sierpinski carpet.\n      These are well-known and you can look them up to check your result.\n      Then find the fractal dimension of the fractal with IFS\n       .\n     \n      You might want to draw this fractal using an online generator.\n     "
},
{
  "id": "chap_transformation_matrix",
  "level": "1",
  "url": "chap_transformation_matrix.html",
  "type": "Section",
  "number": "38",
  "title": "The Matrix of a Linear Transformation",
  "body": "The Matrix of a Linear Transformation \n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            How do we represent a linear transformation from   to   as a matrix transformation?\n           \n         \n           \n            How can we represent any linear transformation from a finite dimensional vector space   to a finite dimensional vector space   as a matrix transformation?\n           \n         \n           \n            In what ways is representing a linear transformation as a matrix transformation useful?\n           \n         Application: Secret Sharing Algorithms \n    Suppose we have a secret that we want or need to share with several individuals.\n    For example,\n    a bank manager might want to share pieces of the code for the bank vault with several employees so that if the manager is not available,\n    some subset of these employees could open the vault if they work together.\n    This allows for a significant amount of security while still making the code available if needed.\n    As another example, in order to keep passwords secure\n    (as they can be hard to remember),\n    a person could implement a secret sharing scheme.\n    The person would generate a set of shares for a given password and store them in several different places.\n    If the person forgets the password,\n    it can be reconstructed by collecting some set of these shares.\n    Since the shares can be stored in many different places,\n    the password is relatively secure.\n   \n    The idea behind secret sharing algorithms is to break a secret into a number ( ) of pieces and give each person one piece.\n    A code is then created in such a way that any   individuals could combine their information and learn the secret,\n    but no group of fewer than   individuals could.\n    This is called a   threshold scheme.\n    One secret sharing algorithm is Shamir's Secret Sharing, which,\n    as we will see later in this section,\n    involves Lagrange polynomials.\n    In order to implement this algorithm,\n    we will use matrices of linear transformations from polynomials spaces to  .\n   Introduction \n    A matrix transformation   from   to   is a linear transformation defined by\n      for some   matrix  .\n    We can use the matrix to quickly determine if   is one-to-one or onto   if every column of   contains a pivot,\n    then   is one-to-one,\n    and if every row of   contains a pivot, then   is onto.\n    As we will see,\n    we can represent any linear transformation between finite dimensional vector spaces as a matrix transformation,\n    which will allow us to use all of the tools we have developed for matrices to study linear transformations.\n    We will begin with linear transformations from   to  .\n   \n      Let   be a linear transformation from   into  .\n      Let's also say that we know the following information about  :\n       .\n     \n            Find  .\n            \n           \n            Use the fact that   is a linear transformation.\n           \n            Find  .\n           \n            Find  .\n           \n            Find   for any real numbers   and  .\n           \n            Is it possible to find a matrix   so that\n              for any real numbers   and  ?\n            If so, what is this matrix?\n            If not, why not?\n           Linear Transformations from   to  \n     \n    illustrates the method for representing a linear transformation from   to   as a matrix transformation.\n    We now consider the general context.\n   \n      Let   be a linear transformation from   to  .\n      Let   be the  th column of the   identity matrix.\n      In other words,\n       \n      in  .\n      The set   is the standard basis for  .\n      Let   in  .\n      Explain why   for every   in  , where\n       \n      is the matrix with columns  ,  ,\n       ,  .\n     standard matrix linear transformation standard matrix standard matrix The Matrix of a Linear Transformation \n    We saw in the previous section that any linear transformation   from   to   is in fact a matrix transformation.\n    In this section,\n    we turn to the general question   can any linear transformation\n      between an  -dimensional vector space   and an  -dimensional vector space   be represented in some way as a matrix transformation?\n    This will allow us to extend ideas like eigenvalues and eigenvectors to arbitrary linear transformations.\n    We begin by investigating an example.\n   \n    The general process for representing a linear transformation as a matrix transformation is as described in  .\n   \n      Let   be an   dimensional vector space and   an   dimensional vector space and suppose\n        is a linear transformation.\n      Let   be a basis for   and\n        a basis for  .\n      Let   be in   so that\n       \n      for some scalars  ,  ,\n       ,  .\n     \n            What is  ?\n           \n            Note that for   in  ,\n              is an element in  ,\n            so we can consider the coordinate vector for   with respect to  .\n            Explain why\n             .\n           matrix for   relative to the bases   and  \n     \n    shows us how to represent a linear transformation as a matrix transformation.\n   linear transformation matrix of matrix for   with respect to the bases   and  \n    If   and  ,\n    then we use the notation   as a shorthand for  .\n    The matrix   has the property that\n     \n    for any   in  .\n    Recall that we can find the unique vector in   whose coordinate vector is  ,\n    so we have completely realized the transformation   as a matrix transformation.\n    In essence, we are viewing   as a composite of linear transformations,\n    first from   to  ,\n    then from   to  ,\n    then from   to   as illustrated in  .\n   Visualizing the matrix of a linear transformation. \n      Let   be defined by  .\n      Let   be a basis for   and\n        be a basis for  .\n     \n            Find the matrix  .\n           \n            Find  .\n            Then use the matrix   to calculate\n             . (Hint: Use the fact that  .)\n           \n            Calculate   directly from the definition of   and compare to your result from part (b).\n           \n            Find the matrix   where  .\n            Use the facts that\n             .\n           A Connection between   and a Matrix Representation of  \n    Recall that we defined the kernel of a matrix transformation\n      to be the set of vectors that   maps to the zero vector.\n    If   is the matrix transformation defined by  ,\n    we also saw that the kernel of   is the same as the null space of  .\n    We can make a similar connection between a linear transformation and a matrix that defines the transformation.\n   \n      Let   be a linear transformation from an  -dimensional vector space   to an  -dimensional vector space   with additive identity  .\n      Let   be a basis for   and   a basis for  .\n      Let  .\n      Let   be the matrix transformation from   to   defined by  .\n     \n            Show that if   is in  ,\n            then   is in  .\n            \n           \n            Apply an appropriate coordinate transformation.\n           \n            Let   and  .\n            Show that if the vector   is in  ,\n            then the vector   is in  .\n            \n           \n            Note that  .\n           \n     \n    shows that if   is a linear transformation from an  -dimensional vector space   with basis   to an  -dimensional vector space   with basis  ,\n    then there is a one-to-one correspondence between vectors in   and vectors in  .\n    Recall that   is one-to-one if and only if  ,\n    where   is the additive identity of  .\n    So   will be one-to-one if and only if  .\n    In other words,\n      is one-to-one if and only if every column of   is a pivot column.\n    Note that this does not depend on the basis   and  .\n   \n    A similar argument shows that   is onto if and only if every row of   contains a pivot.\n    This is left to the exercises\n    (see  ).\n   Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Let   be defined by  .\n       \n              Show that   is a linear transformation.\n             \n              Let   and   be in  .\n              Then\n               .\n              Now let   be a scalar.\n              Then\n               .\n              We conclude that   is a linear transformation.\n             \n              Let   and\n                be the standard bases for   and  ,\n              respectively.\n              Find the matrix   for   with respect to   and  .\n              Use the matrix   to determine if   is one-to-one and\/or onto.\n              Explain your reasoning.\n              Use technology as appropriate.\n             \n              Recall that\n               .\n              The linear transformation   is one-to-one and\/or onto if and only if the matrix transformation defined by\n                is one-to-one and\/or onto.\n              The reduced row echelon form of\n                is  .\n              Since   has a pivot in every column,\n                is one-to-one.\n              However,   does not have a pivot in every row,\n              so   is not onto.\n             \n              Use the matrix   to find   and  .\n              If possible, find a basis for each.\n             \n              From part (b) we know that   is one-to-one,\n              so  .\n              Let   be a polynomial in  .\n              Then there is a polynomial   in   such that  .\n              It follows that\n               .\n              If  , then\n               .\n              Thus,   is in the span of   and  .\n              So  .\n              Since neither   nor   is a scalar multiple of the other,\n              the vectors   and   are linearly independent.\n              Thus,   is a basis for  .\n             \n              Let   be a basis for   and\n                a basis for  .\n              Find the matrix   for   with respect to   and  .\n             \n              Recall that\n               .\n             \n              Find   using the matrix  .\n             \n              To find   using the matrix  ,\n              recall that  .\n              First note that  .\n              So\n               .\n              This makes\n               .\n              To check, using the definition of   we have\n               .\n             \n        Let   be a linear transformation.\n        Let   be a basis for   and\n          a basis for  .\n        Let   be the matrix transformation defined by  .\n       \n              Let   be a vector in  .\n              Show that   is in  .\n             \n              Let   be a vector in  .\n              Then there exists a vector   in   such that  .\n              It follows that\n               .\n              Recall that the vectors of the form   all linear combinations of the columns of   with weights from  ,\n              so   \n              (or  )\n              is in  .\n             \n              If  ,\n              part (a) shows that   is in  .\n              So the coordinate transformation   maps\n                into  .\n              Define   to be this coordinate transformation.\n              That is,  .\n              (As a coordinate transformation,\n                is a linear transformation.)\n              We know that coordinate transformations are one-to-one.\n              Show that   is also an onto transformation.\n             \n              Let   be the coordinate transformation\n                for each  .\n              To show that   is an onto transformation,\n              let   be in  .\n              Then there exists   in   such that  .\n              Let  .\n              Then   is in   and  .\n              Also,\n               .\n              So if we let   in  ,\n              then   and   and   is onto.\n             \n              What can we conclude about the relationship between the vector spaces\n                and  ?\n              What must then be true about\n                and  ?\n             \n              Since   is a one-to-one and onto linear transformation from\n                to  ,\n              it follows that   and\n                are isomorphic vector spaces,\n              and therefore we also have  .\n             \n        When we connect the results of this example with the result of   in this section,\n        we obtain a linear transformation analog of the Rank-Nullity Theorem.\n        That is, if   is a linear transformation from a vector space   of dimension   with basis   to a vector space   of dimension   with basis  , then\n         .\n       Summary \n       \n        The standard matrix for a linear transformation   from   to   is the matrix\n         ,\n        where   is the standard basis for  .\n        Then   is the matrix transformation defined by\n          for all   in  .\n       \n     \n       \n        Let   be an   dimensional vector space and   an   dimensional vector space and suppose\n          is a linear transformation.\n        Let   be a basis for   and\n          a basis for  .\n        The matrix for   with respect to the bases   and   is the matrix\n         .\n        If   and  ,\n        then we use the notation   as a shorthand for  .\n        The matrix   has the property that\n         \n        for any   in  .\n       \n     \n       \n        The matrix for a linear transformation allows us to represent any linear transformation between finite dimensional vectors spaces as a matrix transformation.\n        When we view a linear transformation   as a matrix transformation,\n        we are able to use the matrix tools we have developed to understand  .\n       \n     \n        Let   be defined by  .\n        You may assume that   is a linear transformation.\n        Let  ,\n        where  ,  , and  ,\n        be the standard basis for   and  ,\n        where  ,  ,  ,\n        and  , the standard basis for  .\n       \n              Write the polynomial   as a linear combination\n                of the basis vectors in  .\n              Identify the weights  ,\n               , and  .\n              What is  ?\n             \n                and  .\n             \n              Without doing any calculations, explain why\n               .\n             \n                is a linear transformation\n             \n              Without doing any calculations, explain why\n               .\n             \n              The coordinate mapping is a linear transformation.\n             \n              Explicitly determine  ,\n               ,  .\n             \n               ,\n               ,\n               \n             \n              Combine the results of parts (c) and (d) to find a matrix   so that\n               .\n             \n               .\n             \n              Use the matrix   to find  .\n              Then use this vector to calculate  .\n             \n               ,\n               .\n             \n              Calculate   directly from the rule for   and compare to the previous result.\n             \n               .\n             \n        Let   and   be finite dimensional vector spaces,\n        and let   and   be linear transformations from   to  .\n       \n              The sum   is defined as\n               \n              for all   in  .\n              Let   be a basis for   and   a basis for  .\n              Is the statement\n               \n              true or false?\n              If true, prove the statement.\n              If false, provide a counterexample.\n             \n              The scalar multiple  ,\n              for a scalar  , is defined as\n               \n              for all   in  .\n              Let   be a basis for   and   a basis for  .\n              Is the statement\n               \n              true or false?\n              If true, prove the statement.\n              If false, provide a counterexample.\n             \n        If  ,  ,\n        and   are finite dimensional vector spaces and   and\n          are linear transformations,\n        the composite   is defined as\n         \n        for all   in  .\n       \n              Prove that   is a linear transformation.\n             \n              Use the linearity of both   and  .\n             \n              Let   be a basis for  ,\n                a basis for  ,\n              and   a basis for  .\n              Is the statement\n               \n              true or false?\n              If true, prove the statement.\n              If false, provide a counterexample.\n             \n              True\n             \n        Let   be a finite dimensional vector space with basis  ,\n        and let   be a one-to-one linear transformation from   to  .\n       \n              Use the matrix   to explain why   is also onto.\n              (Recall that we use the shorthand notation\n                for  .)\n             \n              Since   is one-to-one and onto,\n              as a function   has an inverse defined by\n               \n              whenever  .\n              Show that   is a linear transformation from   to  .\n             \n              Is the statement\n               \n              true or false?\n              If true, prove the statement.\n              If false, provide a counterexample.\n             \n        Let   and   be vector spaces with\n          and  ,\n        and let  .\n        Let   be a basis for   and   a basis for  .\n        There is a connection between   and  .\n        Find the connection and verify it.\n       \n          if and only if   T\n       \n        Let   and   be vector spaces and define   to be the set of all linear transformations from   to   as in  \n        of  .\n        The set   is a vector space with the operations as as follows for   and   are in   and   a scalar:\n         \n        for all   in  .\n        If   and  ,\n        find the dimension of  . (Hint: Let   be a basis for   and   a basis for  .\n        What can be said about the mapping that sends   in   to  ?\n        Then use  \n        in  .)\n       \n        Let   be a linear transformation from an  -dimensional vector space   to an  -dimensional vector space  .\n        Let   be a basis for   and   a basis for  .\n        Let  .\n        Let   be the matrix transformation from   to   defined by  .\n       \n              Show that if   is in  ,\n              then   is in the range of  .\n             \n              Since   is in the range of  ,\n              there is a vector   so that  .\n             \n              Let   and  .\n              Show that if the vector   is in the range of  ,\n              then the vector   is in the range of  .\n             \n              Since   is in the range of  ,\n              there exists a vector   in   so that  .\n              What is  ?\n             \n              Explain why the results of (a) and (b) show that   is onto if and only if every row of   contains a pivot.\n             \n              How do we tell if   is onto?\n             resonance \n              Show that   is a linear transformation.\n             \n              Show that   is a basis for  .\n              \n             \n              You might consider using the Wronskian.\n             \n              Find the matrix of   with respect to the basis  .\n              That is, find  .\n              (Recall that we use the shorthand notation\n                for  .)\n             \n              Use the matrix to find  all \n              solutions to the equation   in  .\n             \n              If  ,\n              what does that say about the relationship between  ,\n               , and  ?\n             \n              Sketch a few graphs of solutions to\n                and explain what they look like and how they are related to resonance.\n             \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If   is a linear transformation from a vector space   to a vector space  ,\n              and   is a\n                matrix for some bases   of   and   of  ,\n              then   cannot be one-to-one.\n             \n              F\n             True\/False \n              If   is a linear transformation from a vector space   to a vector space  ,\n              and   is a\n                matrix for some bases   of   and   of  ,\n              then   cannot be onto.\n             True\/False \n              If   is a linear transformation from a vector space   to a vector space  ,\n              and   is a\n                matrix for some bases   of   and   of  ,\n              then   cannot be one-to-one.\n             \n              T\n             True\/False \n              If   is a linear transformation from a vector space   to a vector space  ,\n              and   is a\n                matrix for some bases   of   and   of  ,\n              then   cannot be onto.\n             True\/False \n              Let   be a linear transformation from a vector space   to a vector space   and let   be a linear transformation from a vector space   to a vector space  .\n              Suppose that   is a\n                matrix and   is a\n                matrix for some bases   of  ,\n                of   and   of  .\n              Then   is a   matrix.\n             \n              T\n             True\/False \n              Let   be a finite dimensional vector space of dimension   with basis   and   a finite dimensional vector space of dimension   with basis  .\n              Let   be a linear transformation from   to  .\n              If the columns of the matrix\n                are linearly independent,\n              then the transformation   is onto.\n             True\/False \n              Let   be a finite dimensional vector space of dimension   with basis   and   a finite dimensional vector space of dimension   with basis  .\n              Let   be a linear transformation from   to  .\n              If the columns of the matrix\n                are linearly independent,\n              then the transformation   is one-to-one.\n             \n              T\n             True\/False \n              Let   be a finite dimensional vector space of dimension   with basis   and   a finite dimensional vector space of dimension   with basis  .\n              Let   be a linear transformation from   to  .\n              If the matrix   has   pivots,\n              then the transformation   is onto.\n             True\/False \n              Let   be a finite dimensional vector space of dimension   with basis   and   a finite dimensional vector space of dimension   with basis  .\n              Let   be a linear transformation from   to  .\n              If the matrix   has   pivots,\n              then the transformation   is one-to-one.\n             \n              T\n             Project: Shamir's Secret Sharing and Lagrange Polynomials \n    Shamir's Secret Sharing is a secret sharing algorithm developed by the Israeli cryptographer Adi Shamir,\n    who also contributed to the invention of RSA algorithm.\n    The idea is to develop shares related to a secret code that can be distributed in such a way that the secret code can only be discovered if a fixed number of shareholders combine their information.\n   \n    The general algorithm for Shamir's Secret Sharing works by creating a polynomial with the secret code as one coefficient and random remaining coefficients.\n    More specifically, to create a   threshold scheme,\n    let   be the secret code.\n    Then choose at random   positive integers  ,\n     ,\n     ,   and create the polynomial\n     ,\n    where   is the secret.\n    Evaluate   at   different inputs  ,\n     ,  ,\n      to create   points  ,\n     ,  ,\n     .\n    Each participant is given one of these points.\n    Since any collection of   distinct points on the graph of   uniquely determines  ,\n    any combination of   of the participants could reconstruct  .\n    The secret is then  .\n    (Note that, except under very restrictive circumstances,\n    there is no polynomial of degree less than   that passes through the given   points,\n    so it would be impossible for fewer than   participants to reconstruct the message.)\n   \n    As an example, let our secret code be  .\n    Let   and  .\n    Choose two positive integers at random,\n    say   and  .\n    Then\n     .\n   \n    Now we construct   points by selecting   positive integers,\n    say  ,  ,  ,\n     ,  , and  .\n    Evaluating   at   through   gives us the points  ,\n     ,  ,\n     ,  ,\n    and  .\n    Our goal is to find the polynomial that passes through any three of these points.\n    The next activity will show us how to find this polynomial.\n   \n      It will be instructive to work in the most general setting.\n      We restrict ourselves to the degree   case for now.\n      Given three scalars  ,  ,\n      and  , define   by\n       .\n     \n            Show that   is a linear transformation.\n           \n            Find the matrix for   with respect to the standard bases\n              for   and   for  .\n           \n            Recall that the matrix   has the property that\n              for any   in  .\n            So   is one-to-one if and only if the matrix transformation defined by   is one-to-one.\n            Use this idea to show that   is one-to-one if and only if  ,\n             ,\n            and   are all different.\n            Use appropriate technology where needed.\n           \n            Given three points  ,   ,\n            and   with distinct  ,\n             , and  ,\n            our objective is to find a polynomial   such that\n              for   from   to  .\n            We proceed by finding quadratic polynomials  ,\n             ,\n            and   so that  ,\n             , and  .\n            Explain why, if  ,  ,\n            and   are quadratic polynomials so that  ,\n             ,\n            and  ,\n            and  ,  ,\n            and   are all different,\n            then the polynomial   will satisfy\n             .\n           Lagrange polynomials \n      In this activity we see how to find the quadratic polynomials  ,\n       ,\n      and   so that  ,\n       , and  .\n      Let   and\n        be the standard bases for   and  ,\n      respectively.\n     \n            Explain why the problem of solving the equations  ,\n             ,\n            and   is equivalent to solving the equations\n             .\n           \n            Explain why we can solve the equations\n             \n            all at once by solving the matrix equation\n             .\n            What does this tell us about the relationship between the matrix\n              and  ?\n           \n            Technology shows that\n             .\n            Use   to determine the coefficients of  .\n            Then apply some algebra to show that\n             .\n           \n            Find similar expressions for\n              and  .\n            Explain why these three polynomials satisfy the required conditions that  ,\n             , and  .\n           \n    Now we return to our secret code with  .\n   \n      Pick any three of the points  ,\n       ,  ,\n       ,  ,\n      and  .\n      Use the Lagrange polynomials  ,  ,\n      and   and   to find the polynomial whose graph contains the three chosen points.\n      How does your polynomial compare to the polynomial   in  ?\n     \n    The Shamir Secret Sharing algorithm depends on being able to find a unique polynomial   that passes through the created points.\n    We can understand this result with Lagrange polynomials.\n   \n      The process described in  \n      for finding the Lagrange polynomials can be applied to any number of points.\n      Let  ,  ,  ,\n       ,  ,\n      and   be   points with distinct  .\n      Generalizing the results of  ,\n      define   for   from   to   as follows:\n       .\n     \n            Explain why   is in   for each  .\n           \n            Explain why the polynomial   satisfies\n              and   if  .\n           \n            Let  .\n            That is,   is the polynomial\n             .\n            Explain why   is a polynomial in   whose graph contains the points\n              for   from   to  .\n           \n            The previous part demonstrates that there is always a polynomial in   whose graph contains the points  ,\n             ,  ,\n             ,  ,\n            and   with distinct  .\n            The last piece we need is to know that this polynomial is unique.\n            Use the fact that a polynomial of degree   can have at most   roots to show that if   and   are two polynomials in   whose graphs contain the points  ,\n             ,\n             ,  ,\n              with distinct values of  ,\n            then  .\n            This completes Shamir's Secret Sharing algorithm.\n           "
},
{
  "id": "objectives-38",
  "level": "2",
  "url": "chap_transformation_matrix.html#objectives-38",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            How do we represent a linear transformation from   to   as a matrix transformation?\n           \n         \n           \n            How can we represent any linear transformation from a finite dimensional vector space   to a finite dimensional vector space   as a matrix transformation?\n           \n         \n           \n            In what ways is representing a linear transformation as a matrix transformation useful?\n           \n         "
},
{
  "id": "pa_8_b",
  "level": "2",
  "url": "chap_transformation_matrix.html#pa_8_b",
  "type": "Preview Activity",
  "number": "38.1",
  "title": "",
  "body": "\n      Let   be a linear transformation from   into  .\n      Let's also say that we know the following information about  :\n       .\n     \n            Find  .\n            \n           \n            Use the fact that   is a linear transformation.\n           \n            Find  .\n           \n            Find  .\n           \n            Find   for any real numbers   and  .\n           \n            Is it possible to find a matrix   so that\n              for any real numbers   and  ?\n            If so, what is this matrix?\n            If not, why not?\n           "
},
{
  "id": "act_8_b_matrix_of_a_linear_transformation",
  "level": "2",
  "url": "chap_transformation_matrix.html#act_8_b_matrix_of_a_linear_transformation",
  "type": "Activity",
  "number": "38.2",
  "title": "",
  "body": "\n      Let   be a linear transformation from   to  .\n      Let   be the  th column of the   identity matrix.\n      In other words,\n       \n      in  .\n      The set   is the standard basis for  .\n      Let   in  .\n      Explain why   for every   in  , where\n       \n      is the matrix with columns  ,  ,\n       ,  .\n     "
},
{
  "id": "p-6670",
  "level": "2",
  "url": "chap_transformation_matrix.html#p-6670",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "standard matrix "
},
{
  "id": "definition-94",
  "level": "2",
  "url": "chap_transformation_matrix.html#definition-94",
  "type": "Definition",
  "number": "38.1",
  "title": "",
  "body": "linear transformation standard matrix standard matrix "
},
{
  "id": "act_8_b_general_matrix_transformation",
  "level": "2",
  "url": "chap_transformation_matrix.html#act_8_b_general_matrix_transformation",
  "type": "Activity",
  "number": "38.3",
  "title": "",
  "body": "\n      Let   be an   dimensional vector space and   an   dimensional vector space and suppose\n        is a linear transformation.\n      Let   be a basis for   and\n        a basis for  .\n      Let   be in   so that\n       \n      for some scalars  ,  ,\n       ,  .\n     \n            What is  ?\n           \n            Note that for   in  ,\n              is an element in  ,\n            so we can consider the coordinate vector for   with respect to  .\n            Explain why\n             .\n           matrix for   relative to the bases   and  "
},
{
  "id": "definition-95",
  "level": "2",
  "url": "chap_transformation_matrix.html#definition-95",
  "type": "Definition",
  "number": "38.2",
  "title": "",
  "body": "linear transformation matrix of matrix for   with respect to the bases   and  "
},
{
  "id": "F_matrix_transformation",
  "level": "2",
  "url": "chap_transformation_matrix.html#F_matrix_transformation",
  "type": "Figure",
  "number": "38.3",
  "title": "",
  "body": "Visualizing the matrix of a linear transformation. "
},
{
  "id": "activity-149",
  "level": "2",
  "url": "chap_transformation_matrix.html#activity-149",
  "type": "Activity",
  "number": "38.4",
  "title": "",
  "body": "\n      Let   be defined by  .\n      Let   be a basis for   and\n        be a basis for  .\n     \n            Find the matrix  .\n           \n            Find  .\n            Then use the matrix   to calculate\n             . (Hint: Use the fact that  .)\n           \n            Calculate   directly from the definition of   and compare to your result from part (b).\n           \n            Find the matrix   where  .\n            Use the facts that\n             .\n           "
},
{
  "id": "act_8_b_kernel",
  "level": "2",
  "url": "chap_transformation_matrix.html#act_8_b_kernel",
  "type": "Activity",
  "number": "38.5",
  "title": "",
  "body": "\n      Let   be a linear transformation from an  -dimensional vector space   to an  -dimensional vector space   with additive identity  .\n      Let   be a basis for   and   a basis for  .\n      Let  .\n      Let   be the matrix transformation from   to   defined by  .\n     \n            Show that if   is in  ,\n            then   is in  .\n            \n           \n            Apply an appropriate coordinate transformation.\n           \n            Let   and  .\n            Show that if the vector   is in  ,\n            then the vector   is in  .\n            \n           \n            Note that  .\n           "
},
{
  "id": "example-79",
  "level": "2",
  "url": "chap_transformation_matrix.html#example-79",
  "type": "Example",
  "number": "38.4",
  "title": "",
  "body": "\n        Let   be defined by  .\n       \n              Show that   is a linear transformation.\n             \n              Let   and   be in  .\n              Then\n               .\n              Now let   be a scalar.\n              Then\n               .\n              We conclude that   is a linear transformation.\n             \n              Let   and\n                be the standard bases for   and  ,\n              respectively.\n              Find the matrix   for   with respect to   and  .\n              Use the matrix   to determine if   is one-to-one and\/or onto.\n              Explain your reasoning.\n              Use technology as appropriate.\n             \n              Recall that\n               .\n              The linear transformation   is one-to-one and\/or onto if and only if the matrix transformation defined by\n                is one-to-one and\/or onto.\n              The reduced row echelon form of\n                is  .\n              Since   has a pivot in every column,\n                is one-to-one.\n              However,   does not have a pivot in every row,\n              so   is not onto.\n             \n              Use the matrix   to find   and  .\n              If possible, find a basis for each.\n             \n              From part (b) we know that   is one-to-one,\n              so  .\n              Let   be a polynomial in  .\n              Then there is a polynomial   in   such that  .\n              It follows that\n               .\n              If  , then\n               .\n              Thus,   is in the span of   and  .\n              So  .\n              Since neither   nor   is a scalar multiple of the other,\n              the vectors   and   are linearly independent.\n              Thus,   is a basis for  .\n             \n              Let   be a basis for   and\n                a basis for  .\n              Find the matrix   for   with respect to   and  .\n             \n              Recall that\n               .\n             \n              Find   using the matrix  .\n             \n              To find   using the matrix  ,\n              recall that  .\n              First note that  .\n              So\n               .\n              This makes\n               .\n              To check, using the definition of   we have\n               .\n             "
},
{
  "id": "example-80",
  "level": "2",
  "url": "chap_transformation_matrix.html#example-80",
  "type": "Example",
  "number": "38.5",
  "title": "",
  "body": "\n        Let   be a linear transformation.\n        Let   be a basis for   and\n          a basis for  .\n        Let   be the matrix transformation defined by  .\n       \n              Let   be a vector in  .\n              Show that   is in  .\n             \n              Let   be a vector in  .\n              Then there exists a vector   in   such that  .\n              It follows that\n               .\n              Recall that the vectors of the form   all linear combinations of the columns of   with weights from  ,\n              so   \n              (or  )\n              is in  .\n             \n              If  ,\n              part (a) shows that   is in  .\n              So the coordinate transformation   maps\n                into  .\n              Define   to be this coordinate transformation.\n              That is,  .\n              (As a coordinate transformation,\n                is a linear transformation.)\n              We know that coordinate transformations are one-to-one.\n              Show that   is also an onto transformation.\n             \n              Let   be the coordinate transformation\n                for each  .\n              To show that   is an onto transformation,\n              let   be in  .\n              Then there exists   in   such that  .\n              Let  .\n              Then   is in   and  .\n              Also,\n               .\n              So if we let   in  ,\n              then   and   and   is onto.\n             \n              What can we conclude about the relationship between the vector spaces\n                and  ?\n              What must then be true about\n                and  ?\n             \n              Since   is a one-to-one and onto linear transformation from\n                to  ,\n              it follows that   and\n                are isomorphic vector spaces,\n              and therefore we also have  .\n             \n        When we connect the results of this example with the result of   in this section,\n        we obtain a linear transformation analog of the Rank-Nullity Theorem.\n        That is, if   is a linear transformation from a vector space   of dimension   with basis   to a vector space   of dimension   with basis  , then\n         .\n       "
},
{
  "id": "ex_8_b_matrix_transformation",
  "level": "2",
  "url": "chap_transformation_matrix.html#ex_8_b_matrix_transformation",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Let   be defined by  .\n        You may assume that   is a linear transformation.\n        Let  ,\n        where  ,  , and  ,\n        be the standard basis for   and  ,\n        where  ,  ,  ,\n        and  , the standard basis for  .\n       \n              Write the polynomial   as a linear combination\n                of the basis vectors in  .\n              Identify the weights  ,\n               , and  .\n              What is  ?\n             \n                and  .\n             \n              Without doing any calculations, explain why\n               .\n             \n                is a linear transformation\n             \n              Without doing any calculations, explain why\n               .\n             \n              The coordinate mapping is a linear transformation.\n             \n              Explicitly determine  ,\n               ,  .\n             \n               ,\n               ,\n               \n             \n              Combine the results of parts (c) and (d) to find a matrix   so that\n               .\n             \n               .\n             \n              Use the matrix   to find  .\n              Then use this vector to calculate  .\n             \n               ,\n               .\n             \n              Calculate   directly from the rule for   and compare to the previous result.\n             \n               .\n             "
},
{
  "id": "exercise-413",
  "level": "2",
  "url": "chap_transformation_matrix.html#exercise-413",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Let   and   be finite dimensional vector spaces,\n        and let   and   be linear transformations from   to  .\n       \n              The sum   is defined as\n               \n              for all   in  .\n              Let   be a basis for   and   a basis for  .\n              Is the statement\n               \n              true or false?\n              If true, prove the statement.\n              If false, provide a counterexample.\n             \n              The scalar multiple  ,\n              for a scalar  , is defined as\n               \n              for all   in  .\n              Let   be a basis for   and   a basis for  .\n              Is the statement\n               \n              true or false?\n              If true, prove the statement.\n              If false, provide a counterexample.\n             "
},
{
  "id": "exercise-414",
  "level": "2",
  "url": "chap_transformation_matrix.html#exercise-414",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        If  ,  ,\n        and   are finite dimensional vector spaces and   and\n          are linear transformations,\n        the composite   is defined as\n         \n        for all   in  .\n       \n              Prove that   is a linear transformation.\n             \n              Use the linearity of both   and  .\n             \n              Let   be a basis for  ,\n                a basis for  ,\n              and   a basis for  .\n              Is the statement\n               \n              true or false?\n              If true, prove the statement.\n              If false, provide a counterexample.\n             \n              True\n             "
},
{
  "id": "exercise-415",
  "level": "2",
  "url": "chap_transformation_matrix.html#exercise-415",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Let   be a finite dimensional vector space with basis  ,\n        and let   be a one-to-one linear transformation from   to  .\n       \n              Use the matrix   to explain why   is also onto.\n              (Recall that we use the shorthand notation\n                for  .)\n             \n              Since   is one-to-one and onto,\n              as a function   has an inverse defined by\n               \n              whenever  .\n              Show that   is a linear transformation from   to  .\n             \n              Is the statement\n               \n              true or false?\n              If true, prove the statement.\n              If false, provide a counterexample.\n             "
},
{
  "id": "ex_8_b_Ker_Nul",
  "level": "2",
  "url": "chap_transformation_matrix.html#ex_8_b_Ker_Nul",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Let   and   be vector spaces with\n          and  ,\n        and let  .\n        Let   be a basis for   and   a basis for  .\n        There is a connection between   and  .\n        Find the connection and verify it.\n       \n          if and only if   T\n       "
},
{
  "id": "ex_8_b_transformation_space",
  "level": "2",
  "url": "chap_transformation_matrix.html#ex_8_b_transformation_space",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Let   and   be vector spaces and define   to be the set of all linear transformations from   to   as in  \n        of  .\n        The set   is a vector space with the operations as as follows for   and   are in   and   a scalar:\n         \n        for all   in  .\n        If   and  ,\n        find the dimension of  . (Hint: Let   be a basis for   and   a basis for  .\n        What can be said about the mapping that sends   in   to  ?\n        Then use  \n        in  .)\n       "
},
{
  "id": "ex_8_b_range_matrix",
  "level": "2",
  "url": "chap_transformation_matrix.html#ex_8_b_range_matrix",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        Let   be a linear transformation from an  -dimensional vector space   to an  -dimensional vector space  .\n        Let   be a basis for   and   a basis for  .\n        Let  .\n        Let   be the matrix transformation from   to   defined by  .\n       \n              Show that if   is in  ,\n              then   is in the range of  .\n             \n              Since   is in the range of  ,\n              there is a vector   so that  .\n             \n              Let   and  .\n              Show that if the vector   is in the range of  ,\n              then the vector   is in the range of  .\n             \n              Since   is in the range of  ,\n              there exists a vector   in   so that  .\n              What is  ?\n             \n              Explain why the results of (a) and (b) show that   is onto if and only if every row of   contains a pivot.\n             \n              How do we tell if   is onto?\n             "
},
{
  "id": "exercise-419",
  "level": "2",
  "url": "chap_transformation_matrix.html#exercise-419",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "resonance \n              Show that   is a linear transformation.\n             \n              Show that   is a basis for  .\n              \n             \n              You might consider using the Wronskian.\n             \n              Find the matrix of   with respect to the basis  .\n              That is, find  .\n              (Recall that we use the shorthand notation\n                for  .)\n             \n              Use the matrix to find  all \n              solutions to the equation   in  .\n             \n              If  ,\n              what does that say about the relationship between  ,\n               , and  ?\n             \n              Sketch a few graphs of solutions to\n                and explain what they look like and how they are related to resonance.\n             "
},
{
  "id": "exercise-420",
  "level": "2",
  "url": "chap_transformation_matrix.html#exercise-420",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If   is a linear transformation from a vector space   to a vector space  ,\n              and   is a\n                matrix for some bases   of   and   of  ,\n              then   cannot be one-to-one.\n             \n              F\n             True\/False \n              If   is a linear transformation from a vector space   to a vector space  ,\n              and   is a\n                matrix for some bases   of   and   of  ,\n              then   cannot be onto.\n             True\/False \n              If   is a linear transformation from a vector space   to a vector space  ,\n              and   is a\n                matrix for some bases   of   and   of  ,\n              then   cannot be one-to-one.\n             \n              T\n             True\/False \n              If   is a linear transformation from a vector space   to a vector space  ,\n              and   is a\n                matrix for some bases   of   and   of  ,\n              then   cannot be onto.\n             True\/False \n              Let   be a linear transformation from a vector space   to a vector space   and let   be a linear transformation from a vector space   to a vector space  .\n              Suppose that   is a\n                matrix and   is a\n                matrix for some bases   of  ,\n                of   and   of  .\n              Then   is a   matrix.\n             \n              T\n             True\/False \n              Let   be a finite dimensional vector space of dimension   with basis   and   a finite dimensional vector space of dimension   with basis  .\n              Let   be a linear transformation from   to  .\n              If the columns of the matrix\n                are linearly independent,\n              then the transformation   is onto.\n             True\/False \n              Let   be a finite dimensional vector space of dimension   with basis   and   a finite dimensional vector space of dimension   with basis  .\n              Let   be a linear transformation from   to  .\n              If the columns of the matrix\n                are linearly independent,\n              then the transformation   is one-to-one.\n             \n              T\n             True\/False \n              Let   be a finite dimensional vector space of dimension   with basis   and   a finite dimensional vector space of dimension   with basis  .\n              Let   be a linear transformation from   to  .\n              If the matrix   has   pivots,\n              then the transformation   is onto.\n             True\/False \n              Let   be a finite dimensional vector space of dimension   with basis   and   a finite dimensional vector space of dimension   with basis  .\n              Let   be a linear transformation from   to  .\n              If the matrix   has   pivots,\n              then the transformation   is one-to-one.\n             \n              T\n             "
},
{
  "id": "act_SSS_1",
  "level": "2",
  "url": "chap_transformation_matrix.html#act_SSS_1",
  "type": "Project Activity",
  "number": "38.6",
  "title": "",
  "body": "\n      It will be instructive to work in the most general setting.\n      We restrict ourselves to the degree   case for now.\n      Given three scalars  ,  ,\n      and  , define   by\n       .\n     \n            Show that   is a linear transformation.\n           \n            Find the matrix for   with respect to the standard bases\n              for   and   for  .\n           \n            Recall that the matrix   has the property that\n              for any   in  .\n            So   is one-to-one if and only if the matrix transformation defined by   is one-to-one.\n            Use this idea to show that   is one-to-one if and only if  ,\n             ,\n            and   are all different.\n            Use appropriate technology where needed.\n           \n            Given three points  ,   ,\n            and   with distinct  ,\n             , and  ,\n            our objective is to find a polynomial   such that\n              for   from   to  .\n            We proceed by finding quadratic polynomials  ,\n             ,\n            and   so that  ,\n             , and  .\n            Explain why, if  ,  ,\n            and   are quadratic polynomials so that  ,\n             ,\n            and  ,\n            and  ,  ,\n            and   are all different,\n            then the polynomial   will satisfy\n             .\n           "
},
{
  "id": "p-6786",
  "level": "2",
  "url": "chap_transformation_matrix.html#p-6786",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Lagrange polynomials "
},
{
  "id": "act_Lagrange_polynomials",
  "level": "2",
  "url": "chap_transformation_matrix.html#act_Lagrange_polynomials",
  "type": "Project Activity",
  "number": "38.7",
  "title": "",
  "body": "\n      In this activity we see how to find the quadratic polynomials  ,\n       ,\n      and   so that  ,\n       , and  .\n      Let   and\n        be the standard bases for   and  ,\n      respectively.\n     \n            Explain why the problem of solving the equations  ,\n             ,\n            and   is equivalent to solving the equations\n             .\n           \n            Explain why we can solve the equations\n             \n            all at once by solving the matrix equation\n             .\n            What does this tell us about the relationship between the matrix\n              and  ?\n           \n            Technology shows that\n             .\n            Use   to determine the coefficients of  .\n            Then apply some algebra to show that\n             .\n           \n            Find similar expressions for\n              and  .\n            Explain why these three polynomials satisfy the required conditions that  ,\n             , and  .\n           "
},
{
  "id": "project-140",
  "level": "2",
  "url": "chap_transformation_matrix.html#project-140",
  "type": "Project Activity",
  "number": "38.8",
  "title": "",
  "body": "\n      Pick any three of the points  ,\n       ,  ,\n       ,  ,\n      and  .\n      Use the Lagrange polynomials  ,  ,\n      and   and   to find the polynomial whose graph contains the three chosen points.\n      How does your polynomial compare to the polynomial   in  ?\n     "
},
{
  "id": "act_SSS_Lagrange_general",
  "level": "2",
  "url": "chap_transformation_matrix.html#act_SSS_Lagrange_general",
  "type": "Project Activity",
  "number": "38.9",
  "title": "",
  "body": "\n      The process described in  \n      for finding the Lagrange polynomials can be applied to any number of points.\n      Let  ,  ,  ,\n       ,  ,\n      and   be   points with distinct  .\n      Generalizing the results of  ,\n      define   for   from   to   as follows:\n       .\n     \n            Explain why   is in   for each  .\n           \n            Explain why the polynomial   satisfies\n              and   if  .\n           \n            Let  .\n            That is,   is the polynomial\n             .\n            Explain why   is a polynomial in   whose graph contains the points\n              for   from   to  .\n           \n            The previous part demonstrates that there is always a polynomial in   whose graph contains the points  ,\n             ,  ,\n             ,  ,\n            and   with distinct  .\n            The last piece we need is to know that this polynomial is unique.\n            Use the fact that a polynomial of degree   can have at most   roots to show that if   and   are two polynomials in   whose graphs contain the points  ,\n             ,\n             ,  ,\n              with distinct values of  ,\n            then  .\n            This completes Shamir's Secret Sharing algorithm.\n           "
},
{
  "id": "chap_transformations_eigenvalues",
  "level": "1",
  "url": "chap_transformations_eigenvalues.html",
  "type": "Section",
  "number": "39",
  "title": "Eigenvalues of Linear Transformations",
  "body": "Eigenvalues of Linear Transformations \n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            How, and under what conditions,\n            do we define the eigenvalues and eigenvectors of a linear transformation?\n           \n         \n           \n            How do we find eigenvalues and eigenvectors of a linear transformation?\n            What important result provides that our method for doing so works?\n           \n         \n           \n            What does it mean to diagonalize a linear transformation?\n           \n         \n           \n            Under what conditions is a linear transformation diagonalizable?\n           \n         Application: Linear Differential Equations \n    A body in motion obeys Newton's second law that force equals mass times acceleration,\n    or  .\n    Here   is the force acting on the object,\n      the mass of the object,\n    and   the acceleration of the object.\n    For example, if a mass is hanging from a spring,\n    gravity acts to pull the mass downward and the spring acts to pull the mass up.\n    Hooke?s law says that the force of the spring acting on mass is proportional to the displacement   of the spring from equilibrium.\n    There is also a damping force that weakens the action of the spring that might be due to air resistance or the medium in which the system is enclosed.\n    If this mass is not too large,\n    then the resistance can be taken to be proportional to the velocity   of the mass.\n    This produces forces   acting to pull the mass down and forces   and   acting in the opposite direction,\n    where   and   are constant.\n    A similar type of analysis would apply to an object falling through the air,\n    where the resistant to falling could be proportional to the velocity of the object.\n    Since   and  ,\n    equating the forces produces the equation'\n     .\n   differential equation Introduction \n    Recall that a scalar   is an eigenvalue of an   matrix   if\n      for some nonzero vector   in  .\n    Now that we have seen how to represent a linear transformation   from a finite dimensional vector space   to itself with a matrix transformation,\n    we can exploit this idea to define and find eigenvalues and eigenvectors for   just as we did for matrices.\n   linear transformation eigenvalue linear transformation eigenvector eigenvalue eigenvector \n    We can exploit the fact that we can represent linear transformations as matrix transformations to find eigenvalues and eigenvectors of a linear transformation.\n   \n      Let   be defined by  .\n      Assume   is a linear transformation.\n     \n            Let   be the standard basis for  .\n            Find the matrix  .\n            (Recall that we use the shorthand notation\n              for  .)\n           \n            Check that   and\n              are the eigenvalues of  .\n            Find an eigenvector   for\n              corresponding to the eigenvalue   and an eigenvector   of\n              corresponding to the eigenvalue  .\n           \n            Find the vector in   corresponding to the eigenvector of\n              for the eigenvalue  .\n            Check that this is an eigenvector of  .\n           \n            Explain why in general,\n            if   is a vector space with basis   and\n              is a linear transformation,\n            and if   in   satisfies  ,\n            where   is an eigenvector of\n              with eigenvalue  ,\n            then   is an eigenvector of   with eigenvalue  .\n           Finding Eigenvalues and Eigenvectors of Linear Transformations \n     \n    presents a method for finding eigenvalues and eigenvectors of linear transformations.\n    That is, if   is a linear transformation and   is a basis for  ,\n    and if   is an eigenvector of\n      with eigenvalue  ,\n    then the vector   in   satisfying\n      has the property that\n     .\n   \n    Since the coordinate transformation is one-to-one,\n    it follows that   and   is an eigenvector of   with eigenvalue  .\n    So every eigenvector of   corresponds to an eigenvector of  .\n    The fact that every eigenvector and eigenvalue of   can be obtained from the eigenvectors and eigenvalues of\n      is left as  .\n    Now we address the question of how eigenvalues of matrices of   with respect to different bases are related.\n   \n      Let   be defined by  .\n      Assume   is a linear transformation.\n     \n            Let   be a basis for  .\n            Find the matrix  .\n           \n            Check that   and\n              are the eigenvalues of  .\n            Find an eigenvector   for\n              corresponding to the eigenvalue  .\n           \n            Use the matrix   to find an eigenvector for   corresponding to the eigenvalue  .\n           \n    You should notice that the matrices   and\n      in  \n    and   are similar matrices.\n    In fact, if   is the change of basis matrix from   to  ,\n    then  .\n    So it must be the case that   and\n      have the same eigenvalues.\n    What  ,\n     ,\n    and  \n    demonstrate is that the eigenvalues of a linear transformation\n      for any   dimensional vector space   are the eigenvalues of the matrix for   relative to any basis for  .\n    We can then find corresponding eigenvectors for   using the methods demonstrated in  \n    and  .\n    That is, if   is an eigenvector of   with eigenvalue  ,\n    and   is any basis for  , then\n     .\n   \n    Thus, once we find an eigenvector\n      for  ,\n    the vector   is an eigenvector for  .\n    This is summarized in the following theorem.\n   \n        Let   be a finite dimensional vector space with basis   and let\n          be a linear transformation.\n        A vector   in   is an eigenvector of   with eigenvalue   if and only if the vector   is an eigenvector of\n          with eigenvalue  .\n       Diagonalization \n    If   is a linear transformation,\n    one important question is that of finding a basis for   in which the matrix of   has as simple a form as possible.\n    In other words,\n    can we find a basis   for   for which the matrix\n      is a diagonal matrix?\n    Since any matrices for   with respect to different bases are similar,\n    this will happen if we can diagonalize any matrix for  .\n   linear transformation diagonalizable diagonalizable \n    Now the question is, if   is diagonalizable,\n    how do we find a basis   to that\n      is a diagonal matrix?\n    Recall that to diagonalize a matrix   means to find a matrix   so that\n      is a diagonal matrix.\n   \n      Let  ,\n        defined by  .\n      Let   be the standard basis for  .\n     \n            Find the matrix   for   relative to the basis  .\n           \n            Use the fact that the eigenvalues of\n              are   and 3 with corresponding eigenvectors\n              and  ,\n            respectively,\n            to find a matrix   so that\n              is a diagonal matrix  .\n           \n            To find a basis   for which   is diagonal,\n            note that if   of the previous part is the change of basis matrix from   to  ,\n            then the matrices of   with respect to   and   are related by  ,\n            which makes   a diagonal matrix.\n            Use the fact that   to find the vectors in the basis  .\n           \n            Now show directly that  ,\n            where  ,\n            and verify that we have found a basis for   for which the matrix for   is diagonal.\n           \n    The general idea for diagonalizing a linear transformation is contained in  .\n    Let   be an   dimensional vector space and assume the linear transformation   is diagonalizable.\n    So there exists a basis   for which\n      is a diagonal matrix.\n    To find this basis,\n    note that for any other basis   we know that\n      and   are similar.\n    That means that there is an invertible matrix   so that  ,\n    where   is the change of basis matrix from   to  .\n    So to find  ,\n    we choose   so that   is the matrix that diagonalizes  .\n    Using the definition of the change of basis matrix,\n    we then know that each basis vector   in   satisfies  .\n    From this we can find  .\n    Note that a standard basis   is often a convenient choice to make for the basis  .\n    This is the process we used in  .\n   \n    Recall that an   matrix   is diagonalizable if and only if   has   linearly independent eigenvectors.\n    We apply this idea to a linear transformation as well,\n    summarized by the following theorem.\n   \n        Let   be a vector space of dimension   and let   be a linear transformation from   to  .\n        Then the following are equivalent.\n         \n             \n                is diagonalizable.\n             \n           \n             \n              There exists a basis   for   and an invertible matrix   so that\n                is a diagonal matrix.\n             \n           \n             \n              There exists a basis   of   for which the matrix\n                has   linearly independent eigenvectors.\n             \n           \n             \n              There exists a basis of   consisting of eigenvectors of  .\n             \n           \n       Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Let   be defined by  .\n        Let  ,\n         ,\n         ,\n        and  ,\n        and let   be the ordered basis  .\n       \n              Show that   is a linear transformation.\n             \n              Let   and   be\n                matrices and let   be a scalar.\n              The properties of the transpose show that\n               \n              and\n               .\n              Thus,   is a linear transformation.\n             \n              Find  .\n             \n              Notice that  ,  ,\n         , and  .\n        So\n         .\n             \n              Is the matrix   diagonalizable?\n              If so, find a matrix   such that\n                is a diagonal matrix.\n              Use technology as appropriate.\n             \n              Technology shows that the characteristic polynomial of\n          is  .\n        It follows that the eigenvalues of\n          are   and  \n        (with multiplicity  ).\n        Technology also shows that\n          is a basis for the eigenspace corresponding to the eigenvalue   and the vectors  ,\n         ,\n          form a basis for the eigenspace corresponding to the eigenvalue  .\n        If we let  ,\n        then  .\n             \n              Use part (c) to find a basis   for   for which\n                is a diagonal matrix.\n             \n              Suppose   is a basis for\n          satisfying  .\n        This makes   a diagonal matrix with the eigenvalues of   along the diagonal.\n        In this case we have\n         \n        for any matrix  .\n        So   is the change of basis matrix from   to  .\n        That is,  .\n        It follows that   for   from   to  .\n        Since  ,\n        we can see that   is the  th column of  .\n        So the  columns of   provide the weights for basis vectors in   in terms of the basis  .\n        Letting  ,\n         ,  , and  ,\n        we conclude that   is a diagonal matrix.\n             \n        We have shown that every linear transformation from a finite dimensional vector space   to   can be represented as a matrix transformation.\n        As a consequence, all such linear transformations have eigenvalues.\n        In this example we consider the problem of determining eigenvalues and eigenvectors of the linear transformation\n          defined by  ,\n        where   is the vector space of all infinitely differentiable functions from   to  .\n        Suppose that   is an eigenvector of   with a nonzero eigenvalue.\n       \n              Use the Fundamental Theorem of Calculus to show that   must satisfy the equation\n                for some scalar nonzero scalar  .\n             \n              Assuming that   is an eigenvector of   with nonzero eigenvalue  , then\n         .\n        Recall that   by the Fundamental Theorem of Calculus.\n        Differentiating both sides of   with respect to   leaves us with  ,\n        or  .\n             \n              From calculus,\n              we know that the functions   that satisfy the equation   all have the form\n                for some scalar  .\n              So if   is an eigenvector of  ,\n              then   for some scalar  .\n              Show, however,\n              that   cannot be an eigenvector of  .\n              Thus,   has no eigenvectors with nonzero eigenvalues.\n             \n              We can directly check from the definition that   is not a multiple of\n          unless  , which is not allowed.\n        Another method is to note that   by definition.\n        But if  , then\n         .\n        This means that   .\n        But   can never be an eigenvector by definition.\n        So   has no eigenvectors with nonzero eigenvalues.\n             \n              Now show that   is not an eigenvalue of  .\n              Conclude that   has no eigenvalues or eigenvectors.\n             \n              Suppose that   is an eigenvalue of  .\n        Then there is a nonzero function   such that  .\n        In other words,  .\n        Again, differentiating with respect to   yields the equation  .\n        So   has no eigenvectors with eigenvalue  .\n        Since   does not have a nonzero eigenvalue or zero as an eigenvalue,\n          has no eigenvalues\n        (and eigenvectors).\n             \n              Explain why this example does not contradict the statement that every linear transformation from a finite dimensional vector space   to   has eigenvalues.\n             \n              The reason this example does not contradict the statement is that   is an infinite dimensional vector space.\n        In fact, the linearly independent monomials   are all in   for any positive integer  .\n             Summary \n       \n        We can define eigenvalues and eigenvectors of a linear transformation  ,\n        where   is a finite dimensional vector space.\n        In this case,\n        a scalar   is an eigenvalue for   if there exists a non-zero vector   in   so that  .\n       \n     \n       \n        To find eigenvalues and eigenvectors of a linear transformation  ,\n        where   is a finite dimensional vector space,\n        we find the eigenvalues and eigenvectors for  ,\n        where   is an basis for  .\n        If   is any other basis for  ,\n        then   and   are similar matrices and have the same eigenvalues.\n        Once we find an eigenvector\n          for  ,\n        then   is an eigenvector for  .\n       \n     \n       \n        A linear transformation  ,\n        where   is a finite dimensional vector space,\n        is diagonalizable if there is a basis   for   for which\n          is a diagonalizable matrix.\n       \n     \n       \n        To determine if a linear transformation   is diagonalizable,\n        we pick a basis   for  .\n        If the matrix   has   linearly independent eigenvectors,\n        then   is diagonalizable.\n       \n     \n        Let   and define\n          by  .\n       \n              Show that   is a linear transformation.\n             \n              Use properties of the derivative.\n             \n              Let   be the standard basis for  .\n              Find  .\n             \n               .\n             \n              Find the eigenvalues and a basis for each eigenspace of  .\n             \n               ,  , and   with bases  ,\n               ,  \n             \n              Is   diagonalizable?\n              If so, find a basis   for   so that\n                is a diagonal matrix.\n              If not, explain why not.\n             \n               \n             \n        Let   be a linear transformation,\n        and let   be a basis for  .\n        Show that every eigenvector   of   with eigenvalue   corresponds to an eigenvector of\n          with eigenvalue  .\n       \n        Let   be the set of all functions from   to   that have derivatives of all orders.\n       \n              Explain why   is a subspace of  .\n             \n              Use properties of differentiable functions.\n             \n              Let   be defined by  .\n              Explain why   is a linear transformation.\n             \n              Use properties of the derivative.\n             \n              Let   be any real number and let\n                be the exponential function defined by  .\n              Show that   is an eigenvector of  .\n              What is the corresponding eigenvalue?\n              How many eigenvalues does   have?\n             \n              What is  ?\n             \n        Consider  ,\n        where   is the derivative operator defined as in  .\n        Find all of the eigenvalues of   and a basis for each eigenspace of  .\n       \n        Let   be a positive integer and define\n          by  .\n       \n              Show that   is a linear transformation.\n             \n              Use properties of the matrix transpose.\n             \n              Is   an eigenvalue of  ?\n              Explain.\n              If so, describe in detail the vectors in the corresponding eigenspace.\n             \n              For which matrices is  ?\n             \n              Does   have any other eigenvalues?\n              If so, what are they and what are the vectors in the corresponding eigenspaces?\n              If not, why not?\n             \n              When is it possible to have  ?\n             \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              The number 0 cannot be an eigenvalue of a linear transformation.\n             \n              F\n             True\/False \n              The zero vector cannot be an eigenvector of a linear transformation.\n             True\/False \n              If   is an eigenvector of a linear transformation  ,\n              then so is  .\n             \n              T\n             True\/False \n              If   is an eigenvector of a linear transformation  ,\n              then   is also an eigenvector of the transformation  .\n             True\/False \n              If   and   are eigenvectors of a linear transformation   with the same eigenvalue,\n              then   is also an eigenvector of   with the same eigenvalue.\n             \n              T\n             True\/False \n              If   is an eigenvalue of a linear transformation  ,\n              then   is an eigenvalue of  .\n             True\/False \n              Let   and   be two linear transformations with the same domain and codomain.\n              If   is an eigenvector of both   and  ,\n              then   is an eigenvector of  .\n             \n              T\n             True\/False \n              Let   be a vector space and let\n                be a linear transformation.\n              Then   has 0 as an eigenvalue if and only if\n                has linearly dependent columns for any basis   of  .\n             Project: Linear Transformations and Differential Equations \n    There are many different types of differential equations,\n    but we will focus on differential equations of the form\n      presented in the introduction, a second order\n    (the highest derivative in the equation is a second order derivative)\n    linear\n    (the coefficients are constants)\n    differential equation\n    (also called damped harmonic oscillators).\n   \n    To solve a differential equation means to find all solutions to the differential equation.\n    That is, find all functions   that satisfy the differential equation.\n    For example, since  ,\n    we see that   satisfies the differential equation  .\n    But   is not the only solution to this differential equation.\n    In fact,   is a solution to   for any scalar  .\n    We will see how to represent solutions to the differential equation   in this project.\n   \n    The next activity shows that the set of solutions to the linear differential equation\n      is a subspace of the vector space   of all functions from   to  .\n    So we should expect close connections between differential equations and linear algebra,\n    and we will make some of these connections as we proceed.\n   \n      We can represent differential equations using linear transformations.\n      To see how, let   be the function from the space   of all differentiable real-valued functions to   given by  .\n     \n            Show that   is a linear transformation.\n           \n            In order for a function   to be a solution to a differential equation of the form  ,\n            it is necessary for   to be twice differentiable.\n            We will assume that   acts only on such functions from this point on.\n            Use the fact that   is a linear transformation to show that the differential equation\n              can be written in the form\n             .\n           \n     \n    shows that any solution to the differential equation\n      is an eigenvector for the linear transformation  .\n    That is, the solutions to the differential equation\n      form the eigenspace   of\n      with eigenvalue  .\n    The eigenvectors for a linear transformation acting on a function space are also called\n     eigenfunctions .\n    To build up to solutions to the second order differential equation we have been considering,\n    we start with solutions to the first order equation.\n   \n      Let   be a scalar.\n      Show that the solutions of the differential equation\n        form a one-dimensional subspace of  .\n      Find a basis for this subspace.\n      Note that this is the eigenspace of the transformation   corresponding to the eigenvalue  .\n     \n        To find the solutions to  ,\n        write   as   and express the equation\n          in the form  .\n        Divide by   and integrate with respect to  .\n       \n    Before considering the general second order differential equation,\n    we start with a simpler example.\n   \n      As a specific example of a second order linear equation,\n      as discussed at the beginning of this section, Hooke's law states that if a mass is hanging from a spring,\n      the force acting on the spring is proportional to the displacement of the spring from equilibrium.\n      If we let   be the displacement of the object from its equilibrium,\n      and ignore any resistance,\n      the position of the mass-spring system can be represented by the differential equation\n       ,\n      where   is the mass of the object and   is a positive constant that depends on the spring.\n      Assuming that the mass is positive,\n      we can divide both sides by   and rewrite this differential equation in the form\n       \n      where  .\n      So the solutions to the differential equation   make up the eigenspace   for   with eigenvalue  .\n     \n            Since the derivatives of   and   are scalar multiples of   and  ,\n            it may be reasonable that these make up solutions to  .\n            Show that   and\n              are both in  .\n           \n            As functions,\n            the cosine and sine are related in many ways (e.g., the Pythagorean Identity).\n            An important property for this application is the linear independence of the cosine and sine.\n            Show, using the  definition  of linear independence,\n            that the cosine and sine functions are linearly independent in  .\n           \n            Part (a) shows that there are at least two different functions in  .\n            To solve a differential equation is to find all of the solutions to the differential equation.\n            In other words,\n            we want to completely determine the eigenspace  .\n            We have already seen that any function   of the form\n              is a solution to the differential equation  .\n            The theory of linear differential equations tells us that there is a unique solution to\n              if we specify two initial conditions.\n            What this means is that to show that any solution   to the differential equation\n              with two initial values   and   for some scalar   is of the form  ,\n            we need to verify that there are values of   and   such that\n              and  .\n            Here we will use this idea to show that any function in   is a linear combination of   and  .\n            That is, that the set   spans  .\n            Let  .\n            Show that there are values for   and   such that\n             .\n            This result, along with part(b),\n            shows that   is a basis for  .\n            (Note: That the solutions to differential equation   involve sines and cosines models the situation that a mass hanging from a spring will oscillate up and down.)\n           fundamental set of solutions \n      Suppose we have   functions  ,  ,\n       ,  , each with   derivatives.\n      To determine the independence of the functions we must understand the solutions to the equation\n       .\n     \n      We can differentiate both sides of Equation   to obtain the new equation\n       .\n     \n      We can continue to differentiate as long as the functions are differentiable to obtain the system\n       \n       \n            Write this system in matrix form, with coefficient matrix\n             .\n           Wronskian \n            The matrix in part (a) is called the Wronskian matrix of the system.\n            The scalar\n             \n            is called the Wronskian\n            of  ,  ,  ,  .\n            What must be true about the Wronskian for our system to have a unique solution?\n            If the system has a unique solution, what is the solution?\n            What does this result tell us about the functions  ,\n             ,  ,  ?\n           \n            Use the Wronskian to show that the cosine and sine functions are linearly independent.\n           \n    We can apply the Wronskian to help find bases for the eigenspace of the linear transformation\n      with eigenvalue  .\n   \n      The solution to the Hooke's Law differential equation in  \n      indicates that the spring will continue to oscillate forever.\n      In reality, we know that this does not happen.\n      In the non-ideal case,\n      there is always some force (e.g., friction, air resistance,\n      a physical damper as in a piston) that acts to dampen the motion of the spring causing the oscillations to die off.\n      Damping acts to oppose the motion,\n      and we generally assume that the faster an object moves,\n      the higher the damping.\n      For this reason we assume the damping force is proportional to the velocity.\n      That is, the damping force has the form   for some positive constant  .\n      This produces the differential equation\n       \n      or  .\n      We will find bases for the eigenspace of the linear transformation   with eigenvalue   in this activity.\n       \n            Since derivatives of exponential functions are still exponential functions,\n            it seems reasonable to try an exponential function as a solution to  .\n            Show that if   for some constant   is a solution to  , then  .\n            The equation   is the characteristic or auxiliary equation for the differential equation.\n           \n            Part (a) shows that our solutions to the differential equation   are exponential of the form  ,\n            where   is a solution to the auxiliary equation.\n            Recall that if we can find two linearly independent solutions to  ,\n            then we have found a basis for the eigenspace   of   with eigenvalue  .\n            The quadratic equation shows that the roots of the auxiliary equation are\n             .\n            As we will see,\n            our basis depends on the types of roots the auxiliary equation has.\n           overdamped critically damped underdamped \n     \n    tells us that if   is not zero, then  ,  ,\n     ,   are linearly independent.\n    You might wonder what conclusion we can draw if   is zero.\n   \n      In this activity we consider the Wronskian of two different pairs of functions.\n     \n            Calculate  .\n            Are   and   linearly independent or dependent?\n            Explain.\n           \n            Now let   and  .\n           \n                  Calculate   and  .\n                  \n                 \n                  Recall that  \n                 \n                  Calculate  .\n                  Are   and   linearly independent or dependent in  ?\n                  Explain.\n                  \n                 \n                  Consider the cases when   and  .\n                 \n                  What conclusion can we draw about the functions  ,\n                   ,  ,\n                    if   is zero?\n                  Explain.\n                 "
},
{
  "id": "objectives-39",
  "level": "2",
  "url": "chap_transformations_eigenvalues.html#objectives-39",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            How, and under what conditions,\n            do we define the eigenvalues and eigenvectors of a linear transformation?\n           \n         \n           \n            How do we find eigenvalues and eigenvectors of a linear transformation?\n            What important result provides that our method for doing so works?\n           \n         \n           \n            What does it mean to diagonalize a linear transformation?\n           \n         \n           \n            Under what conditions is a linear transformation diagonalizable?\n           \n         "
},
{
  "id": "p-6806",
  "level": "2",
  "url": "chap_transformations_eigenvalues.html#p-6806",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "differential equation "
},
{
  "id": "definition-96",
  "level": "2",
  "url": "chap_transformations_eigenvalues.html#definition-96",
  "type": "Definition",
  "number": "39.1",
  "title": "",
  "body": "linear transformation eigenvalue linear transformation eigenvector eigenvalue eigenvector "
},
{
  "id": "pa_8_c",
  "level": "2",
  "url": "chap_transformations_eigenvalues.html#pa_8_c",
  "type": "Preview Activity",
  "number": "39.1",
  "title": "",
  "body": "\n      Let   be defined by  .\n      Assume   is a linear transformation.\n     \n            Let   be the standard basis for  .\n            Find the matrix  .\n            (Recall that we use the shorthand notation\n              for  .)\n           \n            Check that   and\n              are the eigenvalues of  .\n            Find an eigenvector   for\n              corresponding to the eigenvalue   and an eigenvector   of\n              corresponding to the eigenvalue  .\n           \n            Find the vector in   corresponding to the eigenvector of\n              for the eigenvalue  .\n            Check that this is an eigenvector of  .\n           \n            Explain why in general,\n            if   is a vector space with basis   and\n              is a linear transformation,\n            and if   in   satisfies  ,\n            where   is an eigenvector of\n              with eigenvalue  ,\n            then   is an eigenvector of   with eigenvalue  .\n           "
},
{
  "id": "act_8_c_ltev2",
  "level": "2",
  "url": "chap_transformations_eigenvalues.html#act_8_c_ltev2",
  "type": "Activity",
  "number": "39.2",
  "title": "",
  "body": "\n      Let   be defined by  .\n      Assume   is a linear transformation.\n     \n            Let   be a basis for  .\n            Find the matrix  .\n           \n            Check that   and\n              are the eigenvalues of  .\n            Find an eigenvector   for\n              corresponding to the eigenvalue  .\n           \n            Use the matrix   to find an eigenvector for   corresponding to the eigenvalue  .\n           "
},
{
  "id": "theorem-101",
  "level": "2",
  "url": "chap_transformations_eigenvalues.html#theorem-101",
  "type": "Theorem",
  "number": "39.2",
  "title": "",
  "body": "\n        Let   be a finite dimensional vector space with basis   and let\n          be a linear transformation.\n        A vector   in   is an eigenvector of   with eigenvalue   if and only if the vector   is an eigenvector of\n          with eigenvalue  .\n       "
},
{
  "id": "definition-97",
  "level": "2",
  "url": "chap_transformations_eigenvalues.html#definition-97",
  "type": "Definition",
  "number": "39.3",
  "title": "",
  "body": "linear transformation diagonalizable diagonalizable "
},
{
  "id": "act_8_c_lt_diag",
  "level": "2",
  "url": "chap_transformations_eigenvalues.html#act_8_c_lt_diag",
  "type": "Activity",
  "number": "39.3",
  "title": "",
  "body": "\n      Let  ,\n        defined by  .\n      Let   be the standard basis for  .\n     \n            Find the matrix   for   relative to the basis  .\n           \n            Use the fact that the eigenvalues of\n              are   and 3 with corresponding eigenvectors\n              and  ,\n            respectively,\n            to find a matrix   so that\n              is a diagonal matrix  .\n           \n            To find a basis   for which   is diagonal,\n            note that if   of the previous part is the change of basis matrix from   to  ,\n            then the matrices of   with respect to   and   are related by  ,\n            which makes   a diagonal matrix.\n            Use the fact that   to find the vectors in the basis  .\n           \n            Now show directly that  ,\n            where  ,\n            and verify that we have found a basis for   for which the matrix for   is diagonal.\n           "
},
{
  "id": "theorem-102",
  "level": "2",
  "url": "chap_transformations_eigenvalues.html#theorem-102",
  "type": "Theorem",
  "number": "39.4",
  "title": "",
  "body": "\n        Let   be a vector space of dimension   and let   be a linear transformation from   to  .\n        Then the following are equivalent.\n         \n             \n                is diagonalizable.\n             \n           \n             \n              There exists a basis   for   and an invertible matrix   so that\n                is a diagonal matrix.\n             \n           \n             \n              There exists a basis   of   for which the matrix\n                has   linearly independent eigenvectors.\n             \n           \n             \n              There exists a basis of   consisting of eigenvectors of  .\n             \n           \n       "
},
{
  "id": "example-81",
  "level": "2",
  "url": "chap_transformations_eigenvalues.html#example-81",
  "type": "Example",
  "number": "39.5",
  "title": "",
  "body": "\n        Let   be defined by  .\n        Let  ,\n         ,\n         ,\n        and  ,\n        and let   be the ordered basis  .\n       \n              Show that   is a linear transformation.\n             \n              Let   and   be\n                matrices and let   be a scalar.\n              The properties of the transpose show that\n               \n              and\n               .\n              Thus,   is a linear transformation.\n             \n              Find  .\n             \n              Notice that  ,  ,\n         , and  .\n        So\n         .\n             \n              Is the matrix   diagonalizable?\n              If so, find a matrix   such that\n                is a diagonal matrix.\n              Use technology as appropriate.\n             \n              Technology shows that the characteristic polynomial of\n          is  .\n        It follows that the eigenvalues of\n          are   and  \n        (with multiplicity  ).\n        Technology also shows that\n          is a basis for the eigenspace corresponding to the eigenvalue   and the vectors  ,\n         ,\n          form a basis for the eigenspace corresponding to the eigenvalue  .\n        If we let  ,\n        then  .\n             \n              Use part (c) to find a basis   for   for which\n                is a diagonal matrix.\n             \n              Suppose   is a basis for\n          satisfying  .\n        This makes   a diagonal matrix with the eigenvalues of   along the diagonal.\n        In this case we have\n         \n        for any matrix  .\n        So   is the change of basis matrix from   to  .\n        That is,  .\n        It follows that   for   from   to  .\n        Since  ,\n        we can see that   is the  th column of  .\n        So the  columns of   provide the weights for basis vectors in   in terms of the basis  .\n        Letting  ,\n         ,  , and  ,\n        we conclude that   is a diagonal matrix.\n             "
},
{
  "id": "example-82",
  "level": "2",
  "url": "chap_transformations_eigenvalues.html#example-82",
  "type": "Example",
  "number": "39.6",
  "title": "",
  "body": "\n        We have shown that every linear transformation from a finite dimensional vector space   to   can be represented as a matrix transformation.\n        As a consequence, all such linear transformations have eigenvalues.\n        In this example we consider the problem of determining eigenvalues and eigenvectors of the linear transformation\n          defined by  ,\n        where   is the vector space of all infinitely differentiable functions from   to  .\n        Suppose that   is an eigenvector of   with a nonzero eigenvalue.\n       \n              Use the Fundamental Theorem of Calculus to show that   must satisfy the equation\n                for some scalar nonzero scalar  .\n             \n              Assuming that   is an eigenvector of   with nonzero eigenvalue  , then\n         .\n        Recall that   by the Fundamental Theorem of Calculus.\n        Differentiating both sides of   with respect to   leaves us with  ,\n        or  .\n             \n              From calculus,\n              we know that the functions   that satisfy the equation   all have the form\n                for some scalar  .\n              So if   is an eigenvector of  ,\n              then   for some scalar  .\n              Show, however,\n              that   cannot be an eigenvector of  .\n              Thus,   has no eigenvectors with nonzero eigenvalues.\n             \n              We can directly check from the definition that   is not a multiple of\n          unless  , which is not allowed.\n        Another method is to note that   by definition.\n        But if  , then\n         .\n        This means that   .\n        But   can never be an eigenvector by definition.\n        So   has no eigenvectors with nonzero eigenvalues.\n             \n              Now show that   is not an eigenvalue of  .\n              Conclude that   has no eigenvalues or eigenvectors.\n             \n              Suppose that   is an eigenvalue of  .\n        Then there is a nonzero function   such that  .\n        In other words,  .\n        Again, differentiating with respect to   yields the equation  .\n        So   has no eigenvectors with eigenvalue  .\n        Since   does not have a nonzero eigenvalue or zero as an eigenvalue,\n          has no eigenvalues\n        (and eigenvectors).\n             \n              Explain why this example does not contradict the statement that every linear transformation from a finite dimensional vector space   to   has eigenvalues.\n             \n              The reason this example does not contradict the statement is that   is an infinite dimensional vector space.\n        In fact, the linearly independent monomials   are all in   for any positive integer  .\n             "
},
{
  "id": "exercise-421",
  "level": "2",
  "url": "chap_transformations_eigenvalues.html#exercise-421",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Let   and define\n          by  .\n       \n              Show that   is a linear transformation.\n             \n              Use properties of the derivative.\n             \n              Let   be the standard basis for  .\n              Find  .\n             \n               .\n             \n              Find the eigenvalues and a basis for each eigenspace of  .\n             \n               ,  , and   with bases  ,\n               ,  \n             \n              Is   diagonalizable?\n              If so, find a basis   for   so that\n                is a diagonal matrix.\n              If not, explain why not.\n             \n               \n             "
},
{
  "id": "ex_8_c_evals_linear_Transformations",
  "level": "2",
  "url": "chap_transformations_eigenvalues.html#ex_8_c_evals_linear_Transformations",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Let   be a linear transformation,\n        and let   be a basis for  .\n        Show that every eigenvector   of   with eigenvalue   corresponds to an eigenvector of\n          with eigenvalue  .\n       "
},
{
  "id": "ex_8_c_derivative_operator",
  "level": "2",
  "url": "chap_transformations_eigenvalues.html#ex_8_c_derivative_operator",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Let   be the set of all functions from   to   that have derivatives of all orders.\n       \n              Explain why   is a subspace of  .\n             \n              Use properties of differentiable functions.\n             \n              Let   be defined by  .\n              Explain why   is a linear transformation.\n             \n              Use properties of the derivative.\n             \n              Let   be any real number and let\n                be the exponential function defined by  .\n              Show that   is an eigenvector of  .\n              What is the corresponding eigenvalue?\n              How many eigenvalues does   have?\n             \n              What is  ?\n             "
},
{
  "id": "exercise-424",
  "level": "2",
  "url": "chap_transformations_eigenvalues.html#exercise-424",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Consider  ,\n        where   is the derivative operator defined as in  .\n        Find all of the eigenvalues of   and a basis for each eigenspace of  .\n       "
},
{
  "id": "exercise-425",
  "level": "2",
  "url": "chap_transformations_eigenvalues.html#exercise-425",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Let   be a positive integer and define\n          by  .\n       \n              Show that   is a linear transformation.\n             \n              Use properties of the matrix transpose.\n             \n              Is   an eigenvalue of  ?\n              Explain.\n              If so, describe in detail the vectors in the corresponding eigenspace.\n             \n              For which matrices is  ?\n             \n              Does   have any other eigenvalues?\n              If so, what are they and what are the vectors in the corresponding eigenspaces?\n              If not, why not?\n             \n              When is it possible to have  ?\n             "
},
{
  "id": "exercise-426",
  "level": "2",
  "url": "chap_transformations_eigenvalues.html#exercise-426",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              The number 0 cannot be an eigenvalue of a linear transformation.\n             \n              F\n             True\/False \n              The zero vector cannot be an eigenvector of a linear transformation.\n             True\/False \n              If   is an eigenvector of a linear transformation  ,\n              then so is  .\n             \n              T\n             True\/False \n              If   is an eigenvector of a linear transformation  ,\n              then   is also an eigenvector of the transformation  .\n             True\/False \n              If   and   are eigenvectors of a linear transformation   with the same eigenvalue,\n              then   is also an eigenvector of   with the same eigenvalue.\n             \n              T\n             True\/False \n              If   is an eigenvalue of a linear transformation  ,\n              then   is an eigenvalue of  .\n             True\/False \n              Let   and   be two linear transformations with the same domain and codomain.\n              If   is an eigenvector of both   and  ,\n              then   is an eigenvector of  .\n             \n              T\n             True\/False \n              Let   be a vector space and let\n                be a linear transformation.\n              Then   has 0 as an eigenvalue if and only if\n                has linearly dependent columns for any basis   of  .\n             "
},
{
  "id": "act_de_operator",
  "level": "2",
  "url": "chap_transformations_eigenvalues.html#act_de_operator",
  "type": "Project Activity",
  "number": "39.4",
  "title": "",
  "body": "\n      We can represent differential equations using linear transformations.\n      To see how, let   be the function from the space   of all differentiable real-valued functions to   given by  .\n     \n            Show that   is a linear transformation.\n           \n            In order for a function   to be a solution to a differential equation of the form  ,\n            it is necessary for   to be twice differentiable.\n            We will assume that   acts only on such functions from this point on.\n            Use the fact that   is a linear transformation to show that the differential equation\n              can be written in the form\n             .\n           "
},
{
  "id": "act_de_first_order",
  "level": "2",
  "url": "chap_transformations_eigenvalues.html#act_de_first_order",
  "type": "Project Activity",
  "number": "39.5",
  "title": "",
  "body": "\n      Let   be a scalar.\n      Show that the solutions of the differential equation\n        form a one-dimensional subspace of  .\n      Find a basis for this subspace.\n      Note that this is the eigenspace of the transformation   corresponding to the eigenvalue  .\n     \n        To find the solutions to  ,\n        write   as   and express the equation\n          in the form  .\n        Divide by   and integrate with respect to  .\n       "
},
{
  "id": "act_de_Hooke",
  "level": "2",
  "url": "chap_transformations_eigenvalues.html#act_de_Hooke",
  "type": "Project Activity",
  "number": "39.6",
  "title": "",
  "body": "\n      As a specific example of a second order linear equation,\n      as discussed at the beginning of this section, Hooke's law states that if a mass is hanging from a spring,\n      the force acting on the spring is proportional to the displacement of the spring from equilibrium.\n      If we let   be the displacement of the object from its equilibrium,\n      and ignore any resistance,\n      the position of the mass-spring system can be represented by the differential equation\n       ,\n      where   is the mass of the object and   is a positive constant that depends on the spring.\n      Assuming that the mass is positive,\n      we can divide both sides by   and rewrite this differential equation in the form\n       \n      where  .\n      So the solutions to the differential equation   make up the eigenspace   for   with eigenvalue  .\n     \n            Since the derivatives of   and   are scalar multiples of   and  ,\n            it may be reasonable that these make up solutions to  .\n            Show that   and\n              are both in  .\n           \n            As functions,\n            the cosine and sine are related in many ways (e.g., the Pythagorean Identity).\n            An important property for this application is the linear independence of the cosine and sine.\n            Show, using the  definition  of linear independence,\n            that the cosine and sine functions are linearly independent in  .\n           \n            Part (a) shows that there are at least two different functions in  .\n            To solve a differential equation is to find all of the solutions to the differential equation.\n            In other words,\n            we want to completely determine the eigenspace  .\n            We have already seen that any function   of the form\n              is a solution to the differential equation  .\n            The theory of linear differential equations tells us that there is a unique solution to\n              if we specify two initial conditions.\n            What this means is that to show that any solution   to the differential equation\n              with two initial values   and   for some scalar   is of the form  ,\n            we need to verify that there are values of   and   such that\n              and  .\n            Here we will use this idea to show that any function in   is a linear combination of   and  .\n            That is, that the set   spans  .\n            Let  .\n            Show that there are values for   and   such that\n             .\n            This result, along with part(b),\n            shows that   is a basis for  .\n            (Note: That the solutions to differential equation   involve sines and cosines models the situation that a mass hanging from a spring will oscillate up and down.)\n           "
},
{
  "id": "p-6914",
  "level": "2",
  "url": "chap_transformations_eigenvalues.html#p-6914",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "fundamental set of solutions "
},
{
  "id": "act_de_Wronskian",
  "level": "2",
  "url": "chap_transformations_eigenvalues.html#act_de_Wronskian",
  "type": "Project Activity",
  "number": "39.7",
  "title": "",
  "body": "\n      Suppose we have   functions  ,  ,\n       ,  , each with   derivatives.\n      To determine the independence of the functions we must understand the solutions to the equation\n       .\n     \n      We can differentiate both sides of Equation   to obtain the new equation\n       .\n     \n      We can continue to differentiate as long as the functions are differentiable to obtain the system\n       \n       \n            Write this system in matrix form, with coefficient matrix\n             .\n           Wronskian \n            The matrix in part (a) is called the Wronskian matrix of the system.\n            The scalar\n             \n            is called the Wronskian\n            of  ,  ,  ,  .\n            What must be true about the Wronskian for our system to have a unique solution?\n            If the system has a unique solution, what is the solution?\n            What does this result tell us about the functions  ,\n             ,  ,  ?\n           \n            Use the Wronskian to show that the cosine and sine functions are linearly independent.\n           "
},
{
  "id": "project-146",
  "level": "2",
  "url": "chap_transformations_eigenvalues.html#project-146",
  "type": "Project Activity",
  "number": "39.8",
  "title": "",
  "body": "\n      The solution to the Hooke's Law differential equation in  \n      indicates that the spring will continue to oscillate forever.\n      In reality, we know that this does not happen.\n      In the non-ideal case,\n      there is always some force (e.g., friction, air resistance,\n      a physical damper as in a piston) that acts to dampen the motion of the spring causing the oscillations to die off.\n      Damping acts to oppose the motion,\n      and we generally assume that the faster an object moves,\n      the higher the damping.\n      For this reason we assume the damping force is proportional to the velocity.\n      That is, the damping force has the form   for some positive constant  .\n      This produces the differential equation\n       \n      or  .\n      We will find bases for the eigenspace of the linear transformation   with eigenvalue   in this activity.\n       \n            Since derivatives of exponential functions are still exponential functions,\n            it seems reasonable to try an exponential function as a solution to  .\n            Show that if   for some constant   is a solution to  , then  .\n            The equation   is the characteristic or auxiliary equation for the differential equation.\n           \n            Part (a) shows that our solutions to the differential equation   are exponential of the form  ,\n            where   is a solution to the auxiliary equation.\n            Recall that if we can find two linearly independent solutions to  ,\n            then we have found a basis for the eigenspace   of   with eigenvalue  .\n            The quadratic equation shows that the roots of the auxiliary equation are\n             .\n            As we will see,\n            our basis depends on the types of roots the auxiliary equation has.\n           overdamped critically damped underdamped "
},
{
  "id": "project-147",
  "level": "2",
  "url": "chap_transformations_eigenvalues.html#project-147",
  "type": "Project Activity",
  "number": "39.9",
  "title": "",
  "body": "\n      In this activity we consider the Wronskian of two different pairs of functions.\n     \n            Calculate  .\n            Are   and   linearly independent or dependent?\n            Explain.\n           \n            Now let   and  .\n           \n                  Calculate   and  .\n                  \n                 \n                  Recall that  \n                 \n                  Calculate  .\n                  Are   and   linearly independent or dependent in  ?\n                  Explain.\n                  \n                 \n                  Consider the cases when   and  .\n                 \n                  What conclusion can we draw about the functions  ,\n                   ,  ,\n                    if   is zero?\n                  Explain.\n                 "
},
{
  "id": "chap_JCF",
  "level": "1",
  "url": "chap_JCF.html",
  "type": "Section",
  "number": "40",
  "title": "The Jordan Canonical Form",
  "body": "The Jordan Canonical Form \n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            What is the Jordan canonical form of a square matrix?\n           \n         \n           \n            What is a generalized eigenvector of a matrix and how are generalized eigenvectors related to the Jordan canonical form of a matrix?\n           \n         \n           \n            What does it mean for a vector space   to be a direct sum of subspaces  ,\n             ,  ,  ?\n           \n         \n           \n            What is a nilpotent matrix?\n            How do nilpotent matrices play a role in the Jordan canonical form?\n           \n         \n           \n            What does it mean for a subspace   of a vector space   to be invariant under a linear transformation  ?\n           \n         Application: The Bailey Model of an Epidemic \n    The COVID-19 epidemic has generated many mathematical and statistical models to try to understand the spread of the virus.\n    In 1950 Norman Bailey proposed a simple stochastic model of the spread of an epidemic.\n    The solution to the model involves matrix exponentials and the Jordan canonical form is a useful tool for calculating matrix exponentials.\n   Introduction \n    We have seen several different matrix factorizations so far,\n    eigenvalue decomposition\n    ( ),\n    singular value decomposition\n    ( ),\n    QR factorization\n    ( ),\n    and LU factorization\n    ( ).\n    In this section, we investigate the Jordan canonical form,\n    which in a way generalizes the eigenvalue decomposition.\n    For matrices with an eigenvalue decomposition,\n    the geometric multiplicity of each eigenvalue\n    (the dimension of the corresponding eigenspace)\n    must equal its algebraic multiplicity\n    (the number of times the eigenvalue occurs as a root of the characteristic polynomial of the matrix).\n    We know that not every matrix has an eigenvalue decomposition.\n    However, every square matrix has a Jordan canonical form,\n    in which we use generalized eigenvectors and block diagonal form to approximate the eigenvalue decomposition behavior.\n    At the end of the section we provide a complete proof of the existence of the Jordan canonical form.\n   When an Eigenvalue Decomposition Does Not Exist \n    Recall that an eigenvalue decomposition of a matrix exists if and only if the algebraic multiplicity equals the geometric multiplicity for each eigenvalue.\n    In this case,\n    the whole space   decomposes into eigenspaces corresponding to the eigenvalues and the matrix acts as scalar multiplication in each eigenspace.\n    We consider some cases where such a decomposition exists and some where it does not to notice some differences between the two cases and think of ways to improvise the situation.\n   \n          All of the matrices below have only one eigenvalue,\n            and characteristic polynomial\n            where   is the matrix size.\n          Therefore, for each case,\n          the algebraic multiplicity of this eigenvalue is the size of the matrix.\n          Find the geometric multiplicity of this eigenvalue (i.e. the dimension of\n           ) in each case below to see how the behavior is different in each case.\n         \n                 \n               \n                 \n               \n                 \n               \n                 \n               \n                 \n               \n                 \n               \n                 \n               \n                 \n               \n                 \n               \n          In the examples above,\n          the only matrices where the algebraic and geometric multiplicities of the eigenvalue 2 are equal are the diagonal matrices,\n          which obviously have eigenvalue decompositions.\n          The existence of ones above the diagonal destroys this property.\n          However, the positioning of the ones is also strategical.\n          By letting ones above the diagonal determine how to split the matrix into diagonal blocks,\n          we can categorize the matrices.\n          For example, the matrix in part (c) has one big\n            block with twos on the diagonal and ones above,\n          while the matrix in part (d) as a\n            block at the top left and another\n            block at the bottom right.\n          Determine the blocks for all matrices above,\n          and identify the relationship between the number of blocks and the geometric multiplicity.\n         \n          For this last problem, we will focus on the matrix   above.\n          We know that   is an eigenvector.\n          We do not have a second linearly independent eigenvector,\n          i.e. a solution to  .\n          However, since  ,\n          we know that   for any  ,\n          which implies that   always lies inside the eigenspace corresponding to  .\n          Find   to verify this works in this particular case.\n         Generalized Eigenvectors and the Jordan Canonical Form \n    If an   matrix   has   linearly independent eigenvectors,\n    then   is similar to a diagonal matrix with the eigenvalues along the diagonal.\n    You discovered in  \n    a matrix without enough linearly independent eigenvectors can look close enough to a diagonal matrix.\n    We now investigate whether this generalized representation can be achieved for all matrices.\n   \n      Let  .\n      The characteristic polynomial of   is  ,\n      so the eigenvalues of   are   and  .\n      For  ,\n      the eigenspace is also one dimensional and the vector   is an eigenvector.\n      For  ,\n      although the algebraic multiplicity is 2, the corresponding eigenspace is only one dimensional and   is an eigenvector.\n      We cannot diagonalize matrix   because we do not have a second linearly independent eigenvector for the eigenvalue 5, which would give us the three linearly independent eigenvectors we need.\n      Using the idea as in the last problem of  ,\n      we can find another linearly independent vector to give us a matrix close to a diagonal matrix.\n     \n            Find a vector   that is in\n              but not in  .\n            Then calculate  .\n            How is   related to  ?\n           \n            Let   and calculate the product\n              for our example matrix  .\n           \n            Describe the effect of   and\n              on each of the column vectors of   and explain why this justifies that   is the 0 matrix.\n           defective \n    The lack of two linearly independent eigenvectors for this eigenvalue of algebraic multiplicity two will ensure that   is not similar to a diagonal matrix,\n    but   is similar to a matrix with a diagonal block of the form  .\n    So even though the matrix   is not diagonalizable,\n    we can find an invertible matrix   so that\n      is almost diagonalizable.\n   generalized eigenvectors eigenvector generalized generalized eigenvector \n    In other words, a generalized eigenvector of an\n      matrix   corresponding to the eigenvalue   is a nonzero vector in   for some  .\n    Note that every eigenvector of   is a generalized eigenvector\n    (with  ).\n    In  ,\n      is a   matrix with eigenvalue\n      having algebraic multiplicity 2 and geometric multiplicity 1.\n    We were able to see that   is similar to a matrix of the form\n      because of the existence of a generalized eigenvector for the eigenvalue  .\n   \n    The example in  \n    presents the basic idea behind how we can find a\n     simple \n    matrix that is similar to any square matrix,\n    even if that matrix is not diagonalizable.\n    The key is to find generalized eigenvectors for eigenvalues whose algebraic multiplicities exceed their geometric multiplicities.\n    One way to do this is indicated in   and in the next activity.\n   \n      Let\n       .\n     \n      The matrix   has   as its only eigenvalue,\n      and the geometric multiplicity of   as an eigenvalue is 1.\n      For this activity you may use the fact that the reduced row echelon forms of  ,\n       , and   are, respectively,\n       .\n       \n            To begin, we look for a vector   that is in\n              that is not in  .\n            Find such a vector.\n           \n            Let  .\n            Show that   is in\n              but is not in  .\n           \n            Let  .\n            Show that   is an eigenvector of   with eigenvalue  .\n            That is,   is in  .\n           \n            Let  .\n            Calculate the matrix product  .\n            What do you notice?\n           \n    It is the equations   from  \n    that give us this simple form  .\n    To better understand why,\n    notice that the equations imply that  .\n    So if  , then\n     .\n   \n    This method will provide us with the Jordan canonical form.\n    The major reason that this method always works is contained in the following theorem whose proof follows from the proof of the existence of the Jordan canonical form\n    (presented later).\n   \n        Let   be an   matrix with eigenvalue   of algebraic multiplicity  .\n        Then there is a positive integer\n          such that  .\n       generalized eigenvector chain \n      Let\n       .\n     \n      The only eigenvalue of   is\n        and   has geometric multiplicity 2.\n      The vectors   and\n        are eigenvectors for  .\n      The reduced row echelon forms for  ,\n       ,   are,\n      respectively,\n       .\n       \n            Identify the smallest value of   as in  .\n           \n            Find a vector   in\n              that is not in  .\n           \n            Now let   and  .\n            What special property does   have?\n           \n            Find a fourth vector   so that\n              is a basis of   consisting of generalized eigenvectors of  .\n            Let  .\n            Calculate the product  .\n            What do you see?\n           Jordan block Jordan canonical form Jordan normal form \n        Every square matrix is similar to a matrix in Jordan canonical form.\n       \n    Another example may help illustrate the process.\n   \n      Let  .\n      The eigenvalues of   are   and  ,\n      both with algebraic multiplicity 3.\n      A basis for the eigenspace   corresponding to the eigenvalue   is\n        and a basis for the eigenspace   corresponding to the eigenvalue   is  .\n      In this activity we find a Jordan canonical form of  . \n     \n            Assume that the reduced row echelon forms of  ,\n             , and   are, respectively,\n             .\n            Find a vector   that is in\n              but not in  .\n            Then let   and  .\n            Notice that we obtain a string of three generalized eigenvectors.\n           \n            Assume that the reduced row echelon forms of\n              and   are, respectively,\n             .\n            Find a vector   that is in\n              but not in  .\n            Then let  .\n            Notice that we obtain a string of two generalized eigenvectors.\n           \n            Find a generalized eigenvector   for   such that\n              is a basis for  .\n            Let  .\n            Calculate  .\n            Make sure that   is a matrix in Jordan canonical form.\n           \n            How does the matrix   tell us about the eigenvalues of   and their algebraic multiplicities?\n           \n            How many Jordan blocks are there in   for the eigenvalue  ?\n            How many Jordan blocks are there in   for the eigenvalue  ?\n            How do these numbers compare to the geometric multiplicities of   and   as eigenvalues of  ?\n           \n    The previous activities highlight some of the information that a Jordan canonical tells us about a matrix.\n    Assuming that  ,\n    where   is in Jordan canonical form, we can say the following.\n     \n         \n          Since similar matrices have the same eigenvalues,\n          the eigenvalues of  ,\n          and therefore of  , are the diagonal entries of  .\n          Moreover, the number of times a diagonal entry appears in   is the algebraic multiplicity of the eigenvalue.\n          This is also the sum of the sizes of all Jordan blocks corresponding to  .\n         \n       \n         \n          Given an eigenvalue  ,\n          its geometric multiplicity is the number of Jordan blocks corresponding to  .\n         \n       \n         \n          Each generalized eigenvector leads to a Jordan block for that eigenvector.\n          The number of Jordan blocks corresponding to   of size at least   is  .\n          Thus, the number of Jordan blocks of size exactly   is\n           .\n         \n       \n   \n    One interesting consequence of the existence of the Jordan canonical form is the famous Cayley-Hamilton Theorem.\n   The Cayley-Hamilton Theorem Cayley-Hamilton Theorem    \n        Let   be a square matrix with characteristic polynomial  .\n        Then  .\n       \n    The proof of the Cayley-Hamilton Theorem follows from  \n    that shows that every upper triangular matrix satisfies its characteristic polynomial.\n    If   is a square matrix,\n    then there exists a matrix   such that  ,\n    where   is in Jordan canonical form\n    (that is,   is upper triangular).\n    So   is similar to  .\n    If   is the characteristic polynomial of  ,\n     \n    in  \n    tells us that   is the characteristic polynomial of  .\n    Therefore,  .\n    Then  \n    in  \n    shows that   and   satisfies its characteristic polynomial.\n   Geometry of Matrix Transformations using the Jordan Canonical Form The image of the matrix transformation  . \n    Recall that we can visualize the action of a matrix transformation defined by a diagonalizable matrix by using a change of basis.\n    For example, let  ,\n    where  .\n    The eigenvalues of   are   and\n      with corresponding eigenvectors\n      and  .\n    So   is diagonalizable by the matrix  ,\n    with  .\n    Note that  .\n    Now   is a change of basis matrix from the standard basis to the basis  ,\n    and   stretches space in the direction of   by a factor of 3 and stretches space in the direction of   by a factor of 2, and then   changes basis back to the standard basis.\n    This is illustrated in  .\n   \n    In general, if an   matrix   is diagonalizable,\n    then there is a basis   of   consisting of eigenvectors of  .\n    Assume that   for each  .\n    Letting   we know that\n     ,\n    where   is the diagonal matrix with  ,\n     ,  ,\n      down the diagonal.\n    If   is the matrix transformation defined by  , then\n     .\n   \n    Now   is a change of basis matrix from the standard basis to the basis  ,\n    and   stretches or contracts space in the direction of   by the factor  ,\n    and then   changes basis back to the standard basis.\n    In this way we can visualize the action of the matrix transformation using the basis  .\n    If   is not a diagonalizable matrix,\n    we can use the Jordan canonical form to understand the action of the transformation defined by  .\n    We start by analyzing shears.\n   Left: A shear in the  -direction. Right: A shear in the direction of the line  . \n      Recall from  \n      that a matrix transformation   defined by  ,\n      where   is of the form\n        performs a shear in the   direction,\n      as illustrated at left in  .\n      That is, while  ,\n      it is the case that  .\n      In other words,\n        is in  .\n      But we can say something more.\n      Show that if   is not in  , then\n       .\n     \n      The result is that if   is not in  ,\n      then   is in  .\n      This leads us to a general definition of a shear.\n     shear shear \n          Let  ,\n          where  .\n          Also let   and  .\n         \n                Let   for some scalar  .\n                Calculate  .\n                How is this related to the eigenvalues of  ?\n               \n                Let   be any vector not in  .\n                Show that   is in  .\n               \n                Explain why   is a shear and how   is related to the image at right in  .\n               \n    As we did with diagonalizable matrices,\n    we can understand a general matrix transformation of the form\n      by using a Jordan canonical form of  .\n    In this context, we will encounter matrices of the form\n      for some positive constant  .\n    If  ,\n    then   performs a shear in the direction of   and then an expansion or contraction in all directions by a factor of  .\n    We illustrate with an example.\n   \n      Let  ,\n      where  .\n      The only eigenvalue of   is  ,\n      and this eigenvalue has algebraic multiplicity 2 and geometric multiplicity 1.\n      The vector   is an eigenvector for   with eigenvalue  ,\n      and   satisfies  .\n      Let  .\n     \n            Explain why  ,\n            where  .\n           \n            The matrix   is a change of basis matrix\n              from some basis   to another basis  .\n            Specifically identify   and  .\n           \n            If we begin with an arbitrary vector  ,\n            then  .\n            How is   related to  ?\n           \n            Describe in detail what   does to a vector in the   coordinate system.\n            \n           \n             .\n           \n            Put this all together to describe the action of   as illustrated in  .\n            The word shear should appear in your explanation.\n           Change of basis, a shear, and scaling. \n     \n    provides the essential ideas to understand the geometry of a general linear transformation using the Jordan canonical form.\n    Let   with\n      and  .\n    Then   maps   to\n      by a factor of  ,\n    and   maps   to the line containing the terminal point of   in the direction of  .\n    The matrix   performs a change of basis from the standard basis to the basis  ,\n    then the matrix   performs an expansion by a factor of 2 in all directions and a shear in the direction of  .\n   Proof of the Existence of the Jordan Canonical Form \n    While we have constructed an algorithm to find a Jordan canonical form of a square matrix,\n    we haven't yet addressed the question of whether every square matrix has a Jordan canonical form.\n    We do that in this section.\n   \n    Consider that any vector   in   can be written as the sum  .\n    The vector   is a vector in the subspace   and the vector\n      is in the subspace  .\n    Also notice that  .\n    We can extend this idea to  ,\n    where any vector   can be written as  ,\n    with   in  ,\n      in  ,\n    and   in  .\n    In this situation we write   and\n      and say that   is the direct sum of   and  ,\n    while   is the direct sum of  ,\n     , and  .\n   direct sum of subspaces direct sum \n    If   is a direct sum of subspaces  ,\n     ,  ,  , then we write\n     .\n   \n    Some useful facts about direct sums are given in the following theorem.\n    The proofs are left for the exercises.\n   \n        Let   be a vector space that is a direct sum\n          for some positive integer  .\n         \n             \n                whenever  .\n             \n           \n             \n              If   is finite dimensional,\n              and if   is a basis for  ,\n              then the set   is a basis for  .\n             \n           \n             \n               .\n             \n           \n       Nilpotent Matrices and Invariant Subspaces \n    We will prove the existence of the Jordan canonical form in two steps.\n    In the next subsection  \n    will show that every linear transformation can be diagonalized in some form,\n    and  \n    will provide the specific Jordan canonical form.\n    Before we proceed to the lemmas,\n    there are two concepts we need to introduce   nilpotent matrices and invariant subspaces.\n    We don't need these concepts beyond our proof,\n    so we won't spend a lot of time on them.\n   \n      Let   and  .\n     \n            Calculate the positive integer powers of   and  .\n            What do you notice?\n           \n            Compare the eigenvalues of   to the eigenvalues of  .\n            What do you notice?\n           \n     \n    shows that there are some matrices whose powers eventually become the zero matrix,\n    and that there might be some connection to the eigenvalues of these matrices.\n    Such matrices are given a special name.\n   nilpotent matrix nilpotent transfomation nilpotent nilpotent index \n    A characterization of nilpotent matrices is given in the following theorem.\n   \n        A square matrix   is nilpotent if and only if   is the only eigenvalue of  .\n       \n    The proof is left to the exercises.\n   \n    We have seen that if   is a linear transformation from a vector space to itself,\n    and if   is an eigenvalue of   with eigenvector  ,\n    then  .\n    In other words,   maps every vector in\n      to a vector in  .\n    When this happens we say that   is invariant under the transformation  .\n   invariant subspace invariant \n    So, for example,\n    every eigenspace of a transformation is invariant under the transformation.\n    Other spaces that are always invariant are   and  .\n   \n      Let   be a vector space and let\n        be a linear transformation.\n     \n            Let   and   the linear transformation defined by  .\n            Find two invariant subspaces besides   or   for  .\n           \n            Recall that  .\n            Is   invariant under  ?\n            Explain.\n           \n            Recall that  .\n            Is   invariant under  ?\n            Explain.\n           The Jordan Canonical Form \n    We are now ready to prove the existence of the Jordan canonical form.\n   \n        Let   be an  -dimensional vector space and let\n          be a linear transformation.\n        Let  ,  ,  ,\n          be the distinct eigenvalues for  .\n        Then there are integers  ,  ,\n         ,   such that\n         .\n       \n    An example might help illustrate the lemma.\n   \n      Let   be defined by\n       .\n     \n      The matrix of   with respect to the standard basis   is\n       .\n     \n      The eigenvalues of  \n      (and  )\n      are   and  ,\n      and the algebraic multiplicity of the eigenvalue   is 2 while its geometric multiplicity is 1, and the algebraic multiplicity of the eigenvalue   is 3 while its geometric multiplicity is also 1.\n     \n      For every  , we can find\n        using  .\n     \n            Technology shows that  ,\n             , and  .\n            A basis for   is  .\n            Find a basis   for  .\n           \n            Technology also shows that\n              and  .\n            A basis for   is  .\n            Find a basis   for  .\n           \n            Identify the   and   in  .\n            Let  .\n            Find the matrix  .\n           \n    Since each   in   is   invariant,\n      maps vectors in   back into  .\n    So   applied to each\n      provides a matrix  .\n    Applying   to each   produces the matrix\n     ,\n    where the   are square matrices corresponding to the eigenvalues of  .\n    These blocks are determined by the restriction of   to the spaces\n      with respect to the found basis.\n    To obtain the Jordan canonical form,\n    we need to know that we can always choose these basis to create the correct block matrices.\n      will provide those details.\n   Proof of  \n      Choose a   and,\n      for convenience, label it  .\n      For each positive integer  ,\n      let  .\n      If  , then\n       ,\n      so  .\n      Thus we have the containments\n       .\n     \n      Now   is finite dimensional,\n      so this sequence must reach equality at some integer  .\n      That is,  .\n      Let   be the smallest positive integer for which this happens.\n     \n      We plan to show that  .\n      We begin by demonstrating that  .\n      Let  .\n      Then   and there exists\n        such that  .\n      It follows that\n       .\n     \n      But  , so\n       .\n     \n      We conclude that  .\n     \n      Now we will show that  .\n      Let   with\n        and  .\n      First we will show that   is uniquely represented in this way.\n      Suppose   with\n        and  .\n      Then\n       \n      and\n       .\n     \n      But  ,\n      so   and   which means\n        and  .\n      Now let  .\n      We then know that\n       .\n     \n      Also, the Rank-Nullity Theorem shows that\n       .\n     \n      So   is a subspace of   with  .\n      We conclude that   and that\n       .\n     \n      Next we demonstrate that   and\n        are invariant under  .\n      Note that\n       ,\n      and   commutes with  .\n      By induction,   commutes with  .\n      Suppose that  .\n      Then\n       .\n     \n      So  .\n      Similarly, suppose that  .\n      Then there is a   such that  .\n      Then\n       ,\n      and  .\n     \n      We conclude our proof by induction on the number   of eigenvalues of  .\n      Suppose that   and so   has exactly one eigenvalue  .\n      Then   has only zero as an eigenvalue (otherwise,\n      there is   such that\n        has a nontrivial kernel.\n      This makes   an eigenvalue of  .) In this situation,\n        is nilpotent and so\n        for some positive integer  .\n      If   is the smallest such power,\n      then   and  .\n      So every vector in   is in   and\n       .\n     \n      Thus, the statement is true when  .\n      Assume that the statement is true for linear transformations with fewer than   eigenvalues.\n      Now assume that   has distinct eigenvalues  ,\n       ,\n       ,  .\n      By our previous work, we know that\n       \n      for some positive integer  .\n      Let  .\n      Since   is   invariant,\n      we know that   maps   to  .\n      The eigenvalues of   on   are  ,\n       ,\n       ,  .\n      By our induction hypothesis, we have\n       ,\n      for some positive integers  ,  ,\n       ,  , which makes\n       .\n     \n     \n    tells us that   has a diagonal form with block matrices down the diagonal.\n    To obtain a Jordan canonical form,\n    we need to identify the correct bases for the summands of  .\n    (Lemma is due to Mark Wildon from\n     A SHORT PROOF OF THE EXISTENCE OF JORDAN NORMAL FORM .)\n   \n        Let   be an  -dimensional vector space and let\n          be a linear transformation such that   for some positive integer  .\n        Then there exist vectors  ,\n         ,  ,\n          and natural numbers  ,\n         ,  ,\n          such that   for   from   to   and the vectors\n         \n        are non-zero vectors that form a basis of  .\n       \n    Notice the similarity of  \n    to chains of generalized eigenvectors.\n    An example might help illustrate  .\n   \n      Let   be defined by\n       .\n     \n      Let   be the standard basis for  .\n      Then\n       .\n     \n      Technology shows that the only eigenvalue of   is   and that the geometric multiplicity of   is  .\n      Since   is the only eigenvalue of  , we know that  \n      (and  )\n      is nilpotent.\n      Using technology we find that the reduced row echelon forms of   and   and respectively,\n       ,\n      while  .\n      We see that   while  .\n     \n            Notice that the vector   is in\n              but not in  .\n            Use this vector to construct one chain  ,  ,\n            and   of generalized eigenvectors starting with a vector   that is in\n              but not in  .\n            What can we say about the vector\n              in relation to eigenvectors of  ?\n           \n            We know two other eigenvectors of  ,\n            so we need another chain of generalized eigenvectors to provide a basis of   of generalized eigenvectors.\n            Use the fact that   is in\n              but not in   to find another generalized eigenvector   in\n              that is not in  .\n            Then create a chain   and   of generalized eigenvectors.\n            What is true about   in relation to eigenvectors of  ?\n           \n            Let   be a third eigenvector of  .\n            Explain why   is a basis of  .\n            Identify the values of   and the   in Lemma 40.12.\n           \n    Notice that if we let   using the vectors from   we should find that\n     ,\n    and this basis provides a matrix that produces a Jordan canonical form of  .\n   \n     \n    provides the sequences of generalized eigenvectors that we need to make the block matrices in Jordan canonical form.\n    This works as follows.\n    Start, for example, with  ,\n     ,  , .\n    Since  ,\n    we know that   is nilpotent and so has only   as an eigenvalue.\n    If   has a nonzero eigenvalue  ,\n    we replace   with  .\n   \n    Let  .\n    Then\n     .\n   \n    This makes\n     ,\n    which gives one Jordan block.\n   Proof of  \n      If   for all  ,\n      then we can choose  ,\n       ,  ,\n        to form any basis of   and all  .\n      So we can assume that   is a nonzero transformation.\n      Also, we claim that   is a proper subset of   (recall that   is the same as  ).\n      If not,   for any positive integer  .\n      But this contradicts the fact that  .\n     \n      We proceed by induction on  .\n      If  ,\n      since  is a proper subset of   the only possibility is that  .\n      We have already discussed this case.\n      For the inductive step assume that the lemma is true for any vector space of positive integer dimension less than  .\n      Our assumption that   is a nonzero transformation allows us to conclude that  .\n      Thus,  .\n      We apply the inductive hypothesis to the transformation\n        to find integers  ,\n       ,  ,\n        and vectors  ,\n       ,  ,\n        in   such that the vectors\n       \n      form a basis for   and\n        for  .\n     \n      Now  ,  ,  ,\n        are in  ,\n      so there exist  ,  ,  ,\n        in   such that\n        for each  .\n      This implies that   for all   and all positive integers  .\n      The vectors  ,\n       ,  ,\n        are linearly independent and\n        for  ,\n      so the vectors  ,\n       ,  ,\n        are all in  .\n      Extend the set   to a basis of   with the vectors  ,\n       ,\n       ,   for  .\n      That is, the set\n       \n      is a basis for  .\n      We will now show that the vectors\n       \n      form a basis for  .\n      To demonstrate linear independence, suppose that\n       \n      for some scalars   and  .\n      Apply   to this linear combination to obtain the vector equation\n       .\n     \n      Using the relationship   gives us the equation\n       .\n     \n      Recall that   and that  ,\n       ,  ,\n        are in   to obtain the equation\n       .\n     \n      But this final equation is a linear combination of the basis elements in   of  ,\n      and so the scalars are all  .\n      Replacing these scalars with   in   results in\n       .\n     \n      But this is a linear combination of vectors in a basis for   and so all of the scalars are also  .\n      Hence, the vectors  ,\n       ,  ,  ,\n       ,  ,  ,\n       ,  ,  ,\n       ,   are linearly independent.\n     \n      The Rank-Nullity Theorem shows tells us that  .\n      The vectors in   form a basis for  ,\n      and so  .\n      The vectors in   form a basis for  ,\n      so  .\n      Thus,\n       .\n     \n      But this is exactly the number of vectors in our claimed basis  .\n      This verifies   with  ,\n        for  ,\n        and   for  .\n     \n    We return to  \n    to illustrate the use of  .\n   \n        We work with the transformation   defined by\n         .\n       \n        Recall from   that\n         .\n        and a basis for   is\n          with  ,\n         , and  .\n        Since\n         \n        we see that   maps\n          to  .\n        The matrix of   with respect to   is\n         .\n       \n        The reduced row echelon form of\n          and   are\n         ,\n        while  .\n        This makes  .\n        We apply  \n        to  .\n        We choose   to be a vector in\n          that is not in  .\n        Once such vector has  ,\n        or  .\n        We then let   and  .\n        This gives us the basis   for  .\n       \n        We can also apply  \n        to  .\n        Since\n         ,\n        we have that\n         .\n       \n        It follows that  .\n        Selecting   and letting  ,\n        we obtain the basis   for  .\n       \n        Let  ,  ,\n         ,  , and  ,\n        and let  .\n        Since\n         \n        it follows that\n         ,\n        and we have found a basis for   for which the matrix   has a Jordan canonical form.\n       Examples \n    What follows are worked examples that use the concepts from this section.\n   \n        Find a Jordan form   for each of the following matrices.\n        Find a matrix   such that  .\n       \n                 \n             \n              The eigenvalues of   are   and   with corresponding eigenvectors\n          and  .\n        Since we have a basis for   consisting of eigenvectors for  ,\n        we know that   is diagonalizable.\n        Moreover, Jordan canonical form of   is\n          and  ,\n        where  .\n             \n                 \n             \n              Since   is upper triangular,\n        its eigenvalues are the diagonal entries.\n        So the only eigenvalue of   is 0, and technology shows that this eigenvalue has geometric multiplicity 2.\n        An eigenvector for   is  .\n        A vector   that satisfies\n          is  .\n        Letting   gives us\n         .\n             \n                \n             \n              Again,   is upper triangular,\n        so the eigenvalues of   are   and  ,\n        both of algebraic multiplicity 2 and geometric multiplicity 1.\n        Technology shows that the reduced row echelon forms of\n          and   are\n         .\n        Now   is in  , and\n         .\n        Notice that  Let   is an eigenvector of   with eigenvalue  .\n        Technology also shows that the reduced row echelon forms of   and   are\n         .\n        Now   is in  , and\n         .\n        Notice that  Let   is an eigenvector of   with eigenvalue  .\n        Letting   gives us\n         .\n             The effect of the transformation  . \n        Let  ,\n        where  .\n        Assume that  ,\n        where  .\n        Find a specific coordinate system in which it is possible to succinctly describe the action of  ,\n        then describe the action of   on   in as much detail as possible.\n       \n    First note that   is the only eigenvalue of  .\n    Since   does not have   as an eigenvalue,\n    it follows that   is invertible and so   is both one-to-one and onto.\n    Let  ,\n     ,\n    and  .\n    We have that\n     .\n    and so\n     .\n   \n    If we consider the coordinate system in   defined by the basis\n      as shown in blue in  ,\n    the fact that   shows that   fixes all vectors in  .\n    That   tells us that   maps\n      onto  ,\n    and   shows that   maps\n      onto  .\n    So   sends the box defined by  ,  ,\n    and   onto the box defined by  ,\n     ,\n    and   (in red in  .\n    So the action of   is conveniently viewed in the coordinate system determined by the columns of a matrix   that converts   into its Jordan canonical form.\n   Summary \n       \n        Any square matrix   is similar to a Jordan canonical form\n         ,\n        where each matrix   is a Jordan block of the form\n         .\n        with   as a eigenvalue of  .\n       \n     \n       \n        A generalized eigenvector of an\n          matrix   corresponding to an eigenvalue   of   is a non-zero vector   satisfying\n         \n        for some positive integer  .\n        If   is an   matrix,\n        then we can find a basis of   consisting of generalized eigenvectors  ,\n         ,  ,\n          of   so that that matrix\n          has the property that\n          is a Jordan canonical form.\n       \n     \n       \n        A vector space   is a direct sum of subspaces  ,\n         ,  ,\n          if every vector   in   can be written uniquely as a sum\n         ,\n        with   for each  .\n       \n     \n       \n        A square matrix   is nilpotent if and only if   is the only eigenvalue of  .\n        The Jordan form of a matrix   can always be written in the form  ,\n        where   is a diagonal matrix and   is a nilpotent matrix.\n       \n     \n       \n        A subspace   of a vector space   is invariant under a linear transformation   if\n          whenever   is in  .\n       \n     \n        Find a Jordan canonical form for each of the following matrices.\n       \n               \n             \n               \n             \n               \n             \n               \n             \n               \n             \n               \n             \n               \n             \n               \n             \n        Let  .\n        Show that the Jordan canonical form of\n          is independent of the value of  .\n       \n        Show that the Jordan canonical form of\n          is independent of the value of  .\n       \n        The Jordan normal form is  .\n       \n        Let   and   be similar matrices with  .\n        Let   be the Jordan canonical form of  .\n        Show that   is also the Jordan canonical form of   with  .\n       \n        Find all of the Jordan canonical forms for\n          matrices and   matrices.\n       \n         ,\n         ,\n         ,\n         ,\n         ,\n         .\n       \n        Find the Jordan canonical form of  .\n       \n        For the matrix  ,\n        find a matrix   so that\n          is the Jordan canonical form of  ,\n        where first:\n        the diagonal entries do not increase as we move down the matrix   and, second:\n        the Jordan blocks do not increase in size as we move down the matrix  .\n         .\n       \n         \n       \n        A polynomial in two variables is an object of the form\n         \n        where   and   are integers greater than or equal to 0.\n        For example,\n         \n        is a polynomial in two variables.\n        The degree of a monomial   is  .\n        The degree of a polynomial   is the largest degree of any monomial summand of  .\n        So the degree of the example polynomial   is 4.\n        Two polynomials in   are equal if they have the same coefficients on like terms.\n        We add two polynomials in the variables   and   by adding coefficients of like terms.\n        Scalar multiplication is done by multiplying each coefficient by the given scalar.\n        Let   be the set of all polynomials of two variables of degree less than or equal to 2.\n        You may assume that   is a vector space under this addition and multiplication by scalars and has the standard additive identity and additive inverses.\n        Define   and   by\n         \n        and\n         .\n        That is,\n         \n        and\n         .\n        You may assume that   and   are linear transformations.\n       \n              Explain why   is a basis for  .\n             \n              Explain why   and   are not diagonalizable.\n             \n              Show that   and\n                have the same Jordan canonical form.\n              (So different transformations can have the same Jordan canonical form.)\n             \n              Find ordered bases   and\n                of   for which   and\n                are the Jordan canonical form from (c).\n             \n              Recall that a linear transformation can be defined by its action on a basis.\n              So define   satisfying\n                for   from 1 to 6.\n              Show that   is invertible and that  .\n              This is the essential argument to show that any two linear transformations with the same matrix with respect to some bases are similar.\n              \n             \n              What matrix is  ?\n             \n        Let\n         \n        be a chain of generalized eigenvectors for a matrix   corresponding to the eigenvalue  .\n        In this problem we show that the set\n          is linearly independent.\n       \n              Explain why   is in\n                for any   between   and  .\n             \n              Show that   for each   from   to  .\n             \n              Consider the equation\n               \n              for scalars  ,  ,\n               ,  .\n             \n                    Multiply both sides of   on the left by  .\n                    Explain why we can then conclude that  .\n                   \n                      is not in  ,\n                    so  \n                   \n                    Rewrite   using the result from part i.\n                    Explain how we can then demonstrate that  .\n                   \n                    The same process as in i. shows that  .\n                   \n                    Describe how we can use the process in parts i. and ii. to show that  .\n                    What does this tell us about the set  ?\n                   \n                    Keep repeating the process.\n                   \n        Find, if possible, a matrix transformation\n          for which there is a one-dimensional invariant subspace but no two-dimensional invariant subspace.\n        If not possible, explain why.\n       \n        Let  .\n        It is the case that   has a single eigenvalue of algebraic multiplicity 2 and geometric multiplicity 1.\n       \n              Find a matrix   whose columns are generalized eigenvectors for   so that\n                is in Jordan canonical form.\n             \n                with\n                and  \n             \n              Determine the entries of   and then find the entries of   for any positive integer  .\n             \n               ,\n               \n             \n        Let  ,  ,  ,\n          be eigenvectors of an   matrix  ,\n        and let  .\n        Show that   is invariant under the matrix transformation   defined by  .\n       \n        Let   be an   matrix with eigenvalue  .\n        Let   be defined by\n          for some matrix  .\n        Show that if   commutes with  ,\n        then the eigenspace   of   corresponding to the eigenvalue   is invariant under  .\n       \n        Show that  \n       \n        Determine which of the following matrices is nilpotent.\n        Justify your answers.\n        For each nilpotent matrix, find its index.\n       \n               \n             \n               \n             \n               \n             \n                for any real numbers   and  \n             \n        Find two different nonzero\n          nilpotent matrices whose index is  .\n        If no such matrices exist, explain why.\n       \n          and  \n       \n        Find, if possible,\n          matrices whose indices are  ,\n         ,  , and  .\n        If not possible, explain why.\n       \n        Let   be an  -dimensional vector space and let   be a subspace of  .\n        Show that  .\n       \n        Use a projection onto a subspace.\n       \n        Let   be a subspace of an  -dimensional vector space   that is invariant under a transformation  .\n       \n              Show by example that   need not be invariant under  .\n             \n              Show that if   is an isometry,\n              then   is invariant under  .\n             \n        Let   be defined by  ,\n        where   is the derivative of  .\n        That is, if  ,\n        then  .\n        Find a basis   for   in which the matrix\n          is in Jordan canonical form.\n       \n         \n       \n        Let   be an   nilpotent matrix with index  .\n        Since   is not zero,\n        there is a vector   such that  .\n        Prove that the vectors  ,  ,\n         ,  ,\n          are linearly independent.\n       \n            Let  .\n           \n                  Show that   is nilpotent.\n                 \n                  Calculate powers of  .\n                 \n                  Calculate the matrix product  .\n                  What do you notice and what does this tell us about  ?\n                 \n                   .\n                 \n            If   is an   nilpotent matrix,\n            show that   is nonsingular,\n            where   is the   identity matrix.\n           \n            Calculate\n             .\n           \n        In this exercise we show that every upper triangular matrix satisfies its characteristic polynomial.\n       \n              To illustrate how this will work,\n              consider a   example.\n              Let  .\n             \n                    What is the characteristic polynomial   of  ?\n                   \n                    Consider the matrices  ,\n                     ,\n                    and  .\n                    Show that the first column of   is  ,\n                    the first two columns of   are  ,\n                    and that every column of   is  .\n                    Conclude that   satisfies its characteristic polynomial.\n                   \n              Now we consider the general case.\n              Suppose   is an   upper triangular matrix with diagonal entries  ,\n               ,  ,\n                in order.\n              For   from   to   let  .\n              Show that for each   from 1 to  ,\n              the first   columns of\n                are equal to  .\n              Then explain how this demonstrates that   satisfies its characteristic polynomial.\n             \n        Prove  \n        that a square matrix   is nilpotent if and only if 0 is the only eigenvalue of  .\n        \n       \n        For one direction, use the Cayley-Hamilton Theorem.\n       \n        If   is nilpotent and   is an eigenvector of  ,\n        what is   for every positive integer  ?\n        If   is the only eigenvalue of  ,\n        what is the characteristic polynomial of  ?\n       \n        Let   be a vector space that is a direct sum\n          for some positive integer  .\n        Prove the following.\n       \n                whenever  .\n             \n              If   is finite dimensional,\n              and if   is a basis for  ,\n              then the set   is a basis for  .\n             \n               .\n             \n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If   is an   nilpotent matrix,\n              then the index of   is  .\n             \n              F\n             True\/False \n              The Jordan canonical form of a matrix is unique.\n             True\/False \n              Every nilpotent matrix is singular.\n             \n              T\n             True\/False \n              Eigenvectors of a linear transformation   are also generalized eigenvectors of  .\n             True\/False \n              It is possible for a generalized eigenvector of a matrix   to correspond to a scalar that is not an eigenvalue for  .\n             \n              F\n             True\/False \n              The vectors in a cycle of generalized eigenvectors of a matrix are linearly independent.\n             True\/False \n              A Jordan canonical form of a diagonal matrix   is  .\n             \n              T\n             True\/False \n              Let   be a finite-dimensional vector space and let   be a linear transformation from   to  .\n              Let   be a Jordan canonical form for  .\n              If   is a basis of  ,\n              then a Jordan canonical form of   is  .\n             True\/False \n              Matrices with the same Jordan canonical form are similar.\n             \n              T\n             True\/False \n              Let   be a finite-dimensional vector space and let   be a linear transformation from   to  .\n              If   is an ordered basis for  ,\n              then   is nilpotent if and only if   is nilpotent.\n             True\/False \n              If   is a linear transformation whose only eigenvalue is 0, then   is the zero transformation.\n             \n              F\n             True\/False \n              Let   be a finite-dimensional vector space and let   be a linear transformation from   to  .\n              Then  .\n             True\/False \n              Let   be a finite-dimensional vector space and let   be a linear transformation from   to  .\n              If   is an ordered basis for   of generalized eigenvectors of  ,\n              then   is a Jordan canonical form for  .\n             \n              F\n             Project: Modeling an Epidemic stochastic \n    Bailey begins by considering a community of   persons who are susceptible to a disease,\n    and supposes introducing a single infected individual into the community.\n    Bailey makes the following assumptions:\n     We shall assume that the infection spreads by contact between the members of the community,\n    and that it is not sufficiently serious for cases to be withdrawn from circulation by isolation or death;\n    also that no case becomes clear of infection during the course of the main part of the epidemic. \n   \n    To see how the Bailey's model is constructed, we make some assumption.\n    We split the population at any time into two groups,\n    those infected with the disease,\n    and those susceptible, i.e., not currently infected.\n    We assume that once an individual is infected,\n    that individual is always infected.\n    A person catches the disease by interacting with an infected individual.\n    That is, if a susceptible individual meets an infected individual,\n    the chance that the susceptible person contracts the disease is  \n    (the  infection rate ).\n    For example,\n    there many be a   chance that an encounter between a susceptible individual and an infected individual results in the susceptible individual contracting the disease.\n    (Of course, there are many variables involved in such an interaction   if no one is wearing masks and the interaction involves close contact,\n    the infection rate would be higher than if everyone practices social distancing.\n    For the sake of simplicity,\n    we assume one infection rate for the entire population.)\n    With this simple model we assume assume homogeneous mixing in the population.\n    That is, it is equally likely that any one individual will interact with any other in a given time frame.\n    Let   be the number of susceptible individuals in the population at time  .\n    Then the number of infected individuals is the total population minus  ,\n    or  \n    (recall that we introduced an infected individual into the population).\n    So the change in the number of susceptible individuals in the population at time   is  \n    (since the   susceptible individuals interact with the   infected individuals).\n    That is,  .\n    Bailey makes the substitution of   for   to simplify the equation.\n    This substitution makes  ,\n    which produces the differential equation  .\n    The  s cancel, producing the differential equation\n     \n    with  .\n   \n    The above analysis is just to provide some background.\n    Bailey's stochastic model deals with probabilities instead of actual numbers,\n    so we now take that approach.\n    For   from   to  ,\n    let   be the probability that there are   susceptible individuals still uninfected at time  .\n    Similar to the case describe above,\n    if there are   susceptible individuals,\n    any one of them can be come infected by interacting with the   infected individuals.\n    With that in mind, Bailey's model of the spread of the disease is the system\n     \n   \n    That is,\n     .\n   \n    Since we start with   susceptible individuals,\n    at time   we have   if   and  .\n   \n    This system can be written in matrix form.\n    Let  .\n    Assuming that we can differentiate a vector function component-wise,\n    our system becomes\n     \n    with initial condition  , where\n     .\n   \n    The solution to this system involves a matrix exponential  .\n    The matrix exponential acts much like our familiar exponential function in that\n     .\n   \n    With this in mind it is not difficult to see that Bailey's system has solution  .\n    To truly understand this solution,\n    we need to make sense of the matrix exponential  .\n   \n    We can make sense of the matrix exponential by utilizing the Taylor series of the exponential function centered at the origin.\n    From calculus we know that\n     .\n   \n    Since powers of square matrices are defined,\n    the matrix exponential   of a square matrix   is then\n     .\n   \n    Just as in calculus,\n      converges for any square matrix  .\n   \n      If   is diagonalizable,\n      then   can be found fairly easily.\n      Assume that there is an invertible matrix   such that  ,\n      where   is a diagonal matrix.\n     \n            Use   to explain why\n             .\n           \n            Now show that\n             .\n            \n           \n            What is   for any positive integer  ?\n            Then add corresponding components and compare to  .\n           \n    As we will see,\n    the matrix   in Bailey's model is not diagonalizable,\n    so we need to learn how to consider the matrix exponential in this new situation.\n    In these cases we can utilize the Jordan canonical form.\n    Of course, the computations are more complicated.\n    We first illustrate with the   case.\n   \n      Assume   is a   matrix with Jordan canonical form ,\n      where  .\n      The same argument as above shows that\n       \n      and so we only have to be able to find  .\n     \n            We can write   in the form  ,\n            where   is a diagonal matrix and   is a nilpotent matrix.\n            Find   and   in this case and explain why   is a nilpotent matrix.\n           \n            Find the index of  .\n            That is, find the smallest positive power   of   such that  .\n            Then use   to find  .\n           \n            Assume that the matrix exponential satisfies the standard property of exponential functions that   for any\n              matrices   and  .\n            Use this property to explain why\n             .\n           \n            Use the previous information to calculate   where  .\n           \n    Now we turn to the general case of finding the matrix exponential   when   is not diagonalizable.\n    Suppose   is an invertible matrix and  ,\n    where   is a Jordan canonical form of  .\n    Then we have\n     .\n   \n    We know that   can be written in the form  ,\n    where   is a diagonal matrix and   is an upper triangular matrix with zeros on the diagonal and some ones along the superdiagonal.\n    It follows that\n     .\n   \n    We know that if\n     ,\n    then\n     .\n   \n    So finding   boils down to determining  .\n   \n    Now   has only zero as an eigenvalue, so   is nilpotent.\n    If   is the index of  , then\n     .\n   \n      Let us apply this analysis to a specific case of Bailey's model,\n      with  .\n     \n            Find the entries of   when  .\n           \n            Let   be a Jordan canonical form for  .\n            Explain why the solution to Bailey's model has the form\n             ,\n            where   is an invertible matrix,\n              is a diagonal matrix,\n            and   is a nilpotent matrix.\n           \n            Find a Jordan canonical form   for  .\n           \n            Find a diagonal matrix   and a nilpotent matrix   so that  .\n           \n            Find   and  .\n            Then show that\n             .\n           \n    One use for mathematical models like Bailey's is to make predictions that can help set policies.\n    Recall that we made the substitution of   for   in our original equation in order to make the equations dimensionless,\n    where   is the infection rate   the rate at which people catch the disease.\n    Let us replace   with   in our solution and analyze the effect of changing the value of  .\n   \n      If  ,\n      then the disease is easily transmitted from person to person.\n      If   can be made smaller,\n      then the disease is not so easily transmitted.\n      We continue to work with the case where  .\n      Plot the curves   through   on the same set of axes for the following values of  :\n       .\n     \n      Explain what you see and how this might be related to the phrase\n       flattening the curve \n      used during the COVID-19 pandemic of 2020.\n     \n    In general, the matrix   has eigenvalues of algebraic multiplicity 1 and 2.\n    When   is even,\n      is odd and the eigenvalues of   will occur in pairs except for the single eigenvalue   of multiplicity 1.\n    When   is odd,\n      is even and we have two eigenvalues of multiplicity 1:\n      and the eigenvalue\n      when  ,\n    at which  .\n    It can be shown, although we won't do it here,\n    that every eigenvalue of algebraic multiplicity 2 has geometric multiplicity 1.\n    This information completely determines the Jordan canonical formof  .\n   "
},
{
  "id": "objectives-40",
  "level": "2",
  "url": "chap_JCF.html#objectives-40",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough answers to the\n          questions listed below. You may want to keep these questions in mind to focus your thoughts as\n          you complete the section.\n         \n           \n            What is the Jordan canonical form of a square matrix?\n           \n         \n           \n            What is a generalized eigenvector of a matrix and how are generalized eigenvectors related to the Jordan canonical form of a matrix?\n           \n         \n           \n            What does it mean for a vector space   to be a direct sum of subspaces  ,\n             ,  ,  ?\n           \n         \n           \n            What is a nilpotent matrix?\n            How do nilpotent matrices play a role in the Jordan canonical form?\n           \n         \n           \n            What does it mean for a subspace   of a vector space   to be invariant under a linear transformation  ?\n           \n         "
},
{
  "id": "pa_JCF",
  "level": "2",
  "url": "chap_JCF.html#pa_JCF",
  "type": "Preview Activity",
  "number": "40.1",
  "title": "",
  "body": "\n          All of the matrices below have only one eigenvalue,\n            and characteristic polynomial\n            where   is the matrix size.\n          Therefore, for each case,\n          the algebraic multiplicity of this eigenvalue is the size of the matrix.\n          Find the geometric multiplicity of this eigenvalue (i.e. the dimension of\n           ) in each case below to see how the behavior is different in each case.\n         \n                 \n               \n                 \n               \n                 \n               \n                 \n               \n                 \n               \n                 \n               \n                 \n               \n                 \n               \n                 \n               \n          In the examples above,\n          the only matrices where the algebraic and geometric multiplicities of the eigenvalue 2 are equal are the diagonal matrices,\n          which obviously have eigenvalue decompositions.\n          The existence of ones above the diagonal destroys this property.\n          However, the positioning of the ones is also strategical.\n          By letting ones above the diagonal determine how to split the matrix into diagonal blocks,\n          we can categorize the matrices.\n          For example, the matrix in part (c) has one big\n            block with twos on the diagonal and ones above,\n          while the matrix in part (d) as a\n            block at the top left and another\n            block at the bottom right.\n          Determine the blocks for all matrices above,\n          and identify the relationship between the number of blocks and the geometric multiplicity.\n         \n          For this last problem, we will focus on the matrix   above.\n          We know that   is an eigenvector.\n          We do not have a second linearly independent eigenvector,\n          i.e. a solution to  .\n          However, since  ,\n          we know that   for any  ,\n          which implies that   always lies inside the eigenspace corresponding to  .\n          Find   to verify this works in this particular case.\n         "
},
{
  "id": "act_JC_intro",
  "level": "2",
  "url": "chap_JCF.html#act_JC_intro",
  "type": "Activity",
  "number": "40.2",
  "title": "",
  "body": "\n      Let  .\n      The characteristic polynomial of   is  ,\n      so the eigenvalues of   are   and  .\n      For  ,\n      the eigenspace is also one dimensional and the vector   is an eigenvector.\n      For  ,\n      although the algebraic multiplicity is 2, the corresponding eigenspace is only one dimensional and   is an eigenvector.\n      We cannot diagonalize matrix   because we do not have a second linearly independent eigenvector for the eigenvalue 5, which would give us the three linearly independent eigenvectors we need.\n      Using the idea as in the last problem of  ,\n      we can find another linearly independent vector to give us a matrix close to a diagonal matrix.\n     \n            Find a vector   that is in\n              but not in  .\n            Then calculate  .\n            How is   related to  ?\n           \n            Let   and calculate the product\n              for our example matrix  .\n           \n            Describe the effect of   and\n              on each of the column vectors of   and explain why this justifies that   is the 0 matrix.\n           "
},
{
  "id": "p-6963",
  "level": "2",
  "url": "chap_JCF.html#p-6963",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "defective "
},
{
  "id": "p-6965",
  "level": "2",
  "url": "chap_JCF.html#p-6965",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "generalized eigenvectors "
},
{
  "id": "definition-98",
  "level": "2",
  "url": "chap_JCF.html#definition-98",
  "type": "Definition",
  "number": "40.1",
  "title": "",
  "body": "eigenvector generalized generalized eigenvector "
},
{
  "id": "act_JCF_gev_2",
  "level": "2",
  "url": "chap_JCF.html#act_JCF_gev_2",
  "type": "Activity",
  "number": "40.3",
  "title": "",
  "body": "\n      Let\n       .\n     \n      The matrix   has   as its only eigenvalue,\n      and the geometric multiplicity of   as an eigenvalue is 1.\n      For this activity you may use the fact that the reduced row echelon forms of  ,\n       , and   are, respectively,\n       .\n       \n            To begin, we look for a vector   that is in\n              that is not in  .\n            Find such a vector.\n           \n            Let  .\n            Show that   is in\n              but is not in  .\n           \n            Let  .\n            Show that   is an eigenvector of   with eigenvalue  .\n            That is,   is in  .\n           \n            Let  .\n            Calculate the matrix product  .\n            What do you notice?\n           "
},
{
  "id": "thm_JCF_1",
  "level": "2",
  "url": "chap_JCF.html#thm_JCF_1",
  "type": "Theorem",
  "number": "40.2",
  "title": "",
  "body": "\n        Let   be an   matrix with eigenvalue   of algebraic multiplicity  .\n        Then there is a positive integer\n          such that  .\n       "
},
{
  "id": "p-6978",
  "level": "2",
  "url": "chap_JCF.html#p-6978",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "generalized eigenvector chain "
},
{
  "id": "act_JCF_3",
  "level": "2",
  "url": "chap_JCF.html#act_JCF_3",
  "type": "Activity",
  "number": "40.4",
  "title": "",
  "body": "\n      Let\n       .\n     \n      The only eigenvalue of   is\n        and   has geometric multiplicity 2.\n      The vectors   and\n        are eigenvectors for  .\n      The reduced row echelon forms for  ,\n       ,   are,\n      respectively,\n       .\n       \n            Identify the smallest value of   as in  .\n           \n            Find a vector   in\n              that is not in  .\n           \n            Now let   and  .\n            What special property does   have?\n           \n            Find a fourth vector   so that\n              is a basis of   consisting of generalized eigenvectors of  .\n            Let  .\n            Calculate the product  .\n            What do you see?\n           "
},
{
  "id": "p-6985",
  "level": "2",
  "url": "chap_JCF.html#p-6985",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Jordan block "
},
{
  "id": "p-6986",
  "level": "2",
  "url": "chap_JCF.html#p-6986",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Jordan canonical form Jordan normal form "
},
{
  "id": "thm_JCF",
  "level": "2",
  "url": "chap_JCF.html#thm_JCF",
  "type": "Theorem",
  "number": "40.3",
  "title": "",
  "body": "\n        Every square matrix is similar to a matrix in Jordan canonical form.\n       "
},
{
  "id": "activity-156",
  "level": "2",
  "url": "chap_JCF.html#activity-156",
  "type": "Activity",
  "number": "40.5",
  "title": "",
  "body": "\n      Let  .\n      The eigenvalues of   are   and  ,\n      both with algebraic multiplicity 3.\n      A basis for the eigenspace   corresponding to the eigenvalue   is\n        and a basis for the eigenspace   corresponding to the eigenvalue   is  .\n      In this activity we find a Jordan canonical form of  . \n     \n            Assume that the reduced row echelon forms of  ,\n             , and   are, respectively,\n             .\n            Find a vector   that is in\n              but not in  .\n            Then let   and  .\n            Notice that we obtain a string of three generalized eigenvectors.\n           \n            Assume that the reduced row echelon forms of\n              and   are, respectively,\n             .\n            Find a vector   that is in\n              but not in  .\n            Then let  .\n            Notice that we obtain a string of two generalized eigenvectors.\n           \n            Find a generalized eigenvector   for   such that\n              is a basis for  .\n            Let  .\n            Calculate  .\n            Make sure that   is a matrix in Jordan canonical form.\n           \n            How does the matrix   tell us about the eigenvalues of   and their algebraic multiplicities?\n           \n            How many Jordan blocks are there in   for the eigenvalue  ?\n            How many Jordan blocks are there in   for the eigenvalue  ?\n            How do these numbers compare to the geometric multiplicities of   and   as eigenvalues of  ?\n           "
},
{
  "id": "corollary-1",
  "level": "2",
  "url": "chap_JCF.html#corollary-1",
  "type": "Corollary",
  "number": "40.4",
  "title": "The Cayley-Hamilton Theorem.",
  "body": "The Cayley-Hamilton Theorem Cayley-Hamilton Theorem    \n        Let   be a square matrix with characteristic polynomial  .\n        Then  .\n       "
},
{
  "id": "F_JCF_geometry_1",
  "level": "2",
  "url": "chap_JCF.html#F_JCF_geometry_1",
  "type": "Figure",
  "number": "40.5",
  "title": "",
  "body": "The image of the matrix transformation  . "
},
{
  "id": "F_JCF_shear_1",
  "level": "2",
  "url": "chap_JCF.html#F_JCF_shear_1",
  "type": "Figure",
  "number": "40.6",
  "title": "",
  "body": "Left: A shear in the  -direction. Right: A shear in the direction of the line  . "
},
{
  "id": "act_JCF_shears",
  "level": "2",
  "url": "chap_JCF.html#act_JCF_shears",
  "type": "Activity",
  "number": "40.6",
  "title": "",
  "body": "\n      Recall from  \n      that a matrix transformation   defined by  ,\n      where   is of the form\n        performs a shear in the   direction,\n      as illustrated at left in  .\n      That is, while  ,\n      it is the case that  .\n      In other words,\n        is in  .\n      But we can say something more.\n      Show that if   is not in  , then\n       .\n     \n      The result is that if   is not in  ,\n      then   is in  .\n      This leads us to a general definition of a shear.\n     shear shear \n          Let  ,\n          where  .\n          Also let   and  .\n         \n                Let   for some scalar  .\n                Calculate  .\n                How is this related to the eigenvalues of  ?\n               \n                Let   be any vector not in  .\n                Show that   is in  .\n               \n                Explain why   is a shear and how   is related to the image at right in  .\n               "
},
{
  "id": "act_JCF_geometry",
  "level": "2",
  "url": "chap_JCF.html#act_JCF_geometry",
  "type": "Activity",
  "number": "40.7",
  "title": "",
  "body": "\n      Let  ,\n      where  .\n      The only eigenvalue of   is  ,\n      and this eigenvalue has algebraic multiplicity 2 and geometric multiplicity 1.\n      The vector   is an eigenvector for   with eigenvalue  ,\n      and   satisfies  .\n      Let  .\n     \n            Explain why  ,\n            where  .\n           \n            The matrix   is a change of basis matrix\n              from some basis   to another basis  .\n            Specifically identify   and  .\n           \n            If we begin with an arbitrary vector  ,\n            then  .\n            How is   related to  ?\n           \n            Describe in detail what   does to a vector in the   coordinate system.\n            \n           \n             .\n           \n            Put this all together to describe the action of   as illustrated in  .\n            The word shear should appear in your explanation.\n           "
},
{
  "id": "F_JCF_shear_3",
  "level": "2",
  "url": "chap_JCF.html#F_JCF_shear_3",
  "type": "Figure",
  "number": "40.8",
  "title": "",
  "body": "Change of basis, a shear, and scaling. "
},
{
  "id": "definition-100",
  "level": "2",
  "url": "chap_JCF.html#definition-100",
  "type": "Definition",
  "number": "40.9",
  "title": "",
  "body": "direct sum of subspaces direct sum "
},
{
  "id": "thm_Direct_sum_properties",
  "level": "2",
  "url": "chap_JCF.html#thm_Direct_sum_properties",
  "type": "Theorem",
  "number": "40.10",
  "title": "",
  "body": "\n        Let   be a vector space that is a direct sum\n          for some positive integer  .\n         \n             \n                whenever  .\n             \n           \n             \n              If   is finite dimensional,\n              and if   is a basis for  ,\n              then the set   is a basis for  .\n             \n           \n             \n               .\n             \n           \n       "
},
{
  "id": "act_nilpotent_intro",
  "level": "2",
  "url": "chap_JCF.html#act_nilpotent_intro",
  "type": "Activity",
  "number": "40.8",
  "title": "",
  "body": "\n      Let   and  .\n     \n            Calculate the positive integer powers of   and  .\n            What do you notice?\n           \n            Compare the eigenvalues of   to the eigenvalues of  .\n            What do you notice?\n           "
},
{
  "id": "definition-101",
  "level": "2",
  "url": "chap_JCF.html#definition-101",
  "type": "Definition",
  "number": "40.11",
  "title": "",
  "body": "nilpotent matrix nilpotent transfomation nilpotent nilpotent "
},
{
  "id": "p-7038",
  "level": "2",
  "url": "chap_JCF.html#p-7038",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "index "
},
{
  "id": "thm_nilpotent_evals",
  "level": "2",
  "url": "chap_JCF.html#thm_nilpotent_evals",
  "type": "Theorem",
  "number": "40.12",
  "title": "",
  "body": "\n        A square matrix   is nilpotent if and only if   is the only eigenvalue of  .\n       "
},
{
  "id": "def_JCF_invariant",
  "level": "2",
  "url": "chap_JCF.html#def_JCF_invariant",
  "type": "Definition",
  "number": "40.13",
  "title": "",
  "body": "invariant subspace invariant "
},
{
  "id": "activity-160",
  "level": "2",
  "url": "chap_JCF.html#activity-160",
  "type": "Activity",
  "number": "40.9",
  "title": "",
  "body": "\n      Let   be a vector space and let\n        be a linear transformation.\n     \n            Let   and   the linear transformation defined by  .\n            Find two invariant subspaces besides   or   for  .\n           \n            Recall that  .\n            Is   invariant under  ?\n            Explain.\n           \n            Recall that  .\n            Is   invariant under  ?\n            Explain.\n           "
},
{
  "id": "lem_JCF_1",
  "level": "2",
  "url": "chap_JCF.html#lem_JCF_1",
  "type": "Lemma",
  "number": "40.14",
  "title": "",
  "body": "\n        Let   be an  -dimensional vector space and let\n          be a linear transformation.\n        Let  ,  ,  ,\n          be the distinct eigenvalues for  .\n        Then there are integers  ,  ,\n         ,   such that\n         .\n       "
},
{
  "id": "act_JCF_Lemma_1",
  "level": "2",
  "url": "chap_JCF.html#act_JCF_Lemma_1",
  "type": "Activity",
  "number": "40.10",
  "title": "",
  "body": "\n      Let   be defined by\n       .\n     \n      The matrix of   with respect to the standard basis   is\n       .\n     \n      The eigenvalues of  \n      (and  )\n      are   and  ,\n      and the algebraic multiplicity of the eigenvalue   is 2 while its geometric multiplicity is 1, and the algebraic multiplicity of the eigenvalue   is 3 while its geometric multiplicity is also 1.\n     \n      For every  , we can find\n        using  .\n     \n            Technology shows that  ,\n             , and  .\n            A basis for   is  .\n            Find a basis   for  .\n           \n            Technology also shows that\n              and  .\n            A basis for   is  .\n            Find a basis   for  .\n           \n            Identify the   and   in  .\n            Let  .\n            Find the matrix  .\n           "
},
{
  "id": "proof-20",
  "level": "2",
  "url": "chap_JCF.html#proof-20",
  "type": "Proof",
  "number": "1",
  "title": "Proof of Lemma40.14.",
  "body": "Proof of  \n      Choose a   and,\n      for convenience, label it  .\n      For each positive integer  ,\n      let  .\n      If  , then\n       ,\n      so  .\n      Thus we have the containments\n       .\n     \n      Now   is finite dimensional,\n      so this sequence must reach equality at some integer  .\n      That is,  .\n      Let   be the smallest positive integer for which this happens.\n     \n      We plan to show that  .\n      We begin by demonstrating that  .\n      Let  .\n      Then   and there exists\n        such that  .\n      It follows that\n       .\n     \n      But  , so\n       .\n     \n      We conclude that  .\n     \n      Now we will show that  .\n      Let   with\n        and  .\n      First we will show that   is uniquely represented in this way.\n      Suppose   with\n        and  .\n      Then\n       \n      and\n       .\n     \n      But  ,\n      so   and   which means\n        and  .\n      Now let  .\n      We then know that\n       .\n     \n      Also, the Rank-Nullity Theorem shows that\n       .\n     \n      So   is a subspace of   with  .\n      We conclude that   and that\n       .\n     \n      Next we demonstrate that   and\n        are invariant under  .\n      Note that\n       ,\n      and   commutes with  .\n      By induction,   commutes with  .\n      Suppose that  .\n      Then\n       .\n     \n      So  .\n      Similarly, suppose that  .\n      Then there is a   such that  .\n      Then\n       ,\n      and  .\n     \n      We conclude our proof by induction on the number   of eigenvalues of  .\n      Suppose that   and so   has exactly one eigenvalue  .\n      Then   has only zero as an eigenvalue (otherwise,\n      there is   such that\n        has a nontrivial kernel.\n      This makes   an eigenvalue of  .) In this situation,\n        is nilpotent and so\n        for some positive integer  .\n      If   is the smallest such power,\n      then   and  .\n      So every vector in   is in   and\n       .\n     \n      Thus, the statement is true when  .\n      Assume that the statement is true for linear transformations with fewer than   eigenvalues.\n      Now assume that   has distinct eigenvalues  ,\n       ,\n       ,  .\n      By our previous work, we know that\n       \n      for some positive integer  .\n      Let  .\n      Since   is   invariant,\n      we know that   maps   to  .\n      The eigenvalues of   on   are  ,\n       ,\n       ,  .\n      By our induction hypothesis, we have\n       ,\n      for some positive integers  ,  ,\n       ,  , which makes\n       .\n     "
},
{
  "id": "lem_JCF_2",
  "level": "2",
  "url": "chap_JCF.html#lem_JCF_2",
  "type": "Lemma",
  "number": "40.15",
  "title": "",
  "body": "\n        Let   be an  -dimensional vector space and let\n          be a linear transformation such that   for some positive integer  .\n        Then there exist vectors  ,\n         ,  ,\n          and natural numbers  ,\n         ,  ,\n          such that   for   from   to   and the vectors\n         \n        are non-zero vectors that form a basis of  .\n       "
},
{
  "id": "act_JCF_Lem_2",
  "level": "2",
  "url": "chap_JCF.html#act_JCF_Lem_2",
  "type": "Activity",
  "number": "40.11",
  "title": "",
  "body": "\n      Let   be defined by\n       .\n     \n      Let   be the standard basis for  .\n      Then\n       .\n     \n      Technology shows that the only eigenvalue of   is   and that the geometric multiplicity of   is  .\n      Since   is the only eigenvalue of  , we know that  \n      (and  )\n      is nilpotent.\n      Using technology we find that the reduced row echelon forms of   and   and respectively,\n       ,\n      while  .\n      We see that   while  .\n     \n            Notice that the vector   is in\n              but not in  .\n            Use this vector to construct one chain  ,  ,\n            and   of generalized eigenvectors starting with a vector   that is in\n              but not in  .\n            What can we say about the vector\n              in relation to eigenvectors of  ?\n           \n            We know two other eigenvectors of  ,\n            so we need another chain of generalized eigenvectors to provide a basis of   of generalized eigenvectors.\n            Use the fact that   is in\n              but not in   to find another generalized eigenvector   in\n              that is not in  .\n            Then create a chain   and   of generalized eigenvectors.\n            What is true about   in relation to eigenvectors of  ?\n           \n            Let   be a third eigenvector of  .\n            Explain why   is a basis of  .\n            Identify the values of   and the   in Lemma 40.12.\n           "
},
{
  "id": "proof-21",
  "level": "2",
  "url": "chap_JCF.html#proof-21",
  "type": "Proof",
  "number": "2",
  "title": "Proof of Lemma40.15.",
  "body": "Proof of  \n      If   for all  ,\n      then we can choose  ,\n       ,  ,\n        to form any basis of   and all  .\n      So we can assume that   is a nonzero transformation.\n      Also, we claim that   is a proper subset of   (recall that   is the same as  ).\n      If not,   for any positive integer  .\n      But this contradicts the fact that  .\n     \n      We proceed by induction on  .\n      If  ,\n      since  is a proper subset of   the only possibility is that  .\n      We have already discussed this case.\n      For the inductive step assume that the lemma is true for any vector space of positive integer dimension less than  .\n      Our assumption that   is a nonzero transformation allows us to conclude that  .\n      Thus,  .\n      We apply the inductive hypothesis to the transformation\n        to find integers  ,\n       ,  ,\n        and vectors  ,\n       ,  ,\n        in   such that the vectors\n       \n      form a basis for   and\n        for  .\n     \n      Now  ,  ,  ,\n        are in  ,\n      so there exist  ,  ,  ,\n        in   such that\n        for each  .\n      This implies that   for all   and all positive integers  .\n      The vectors  ,\n       ,  ,\n        are linearly independent and\n        for  ,\n      so the vectors  ,\n       ,  ,\n        are all in  .\n      Extend the set   to a basis of   with the vectors  ,\n       ,\n       ,   for  .\n      That is, the set\n       \n      is a basis for  .\n      We will now show that the vectors\n       \n      form a basis for  .\n      To demonstrate linear independence, suppose that\n       \n      for some scalars   and  .\n      Apply   to this linear combination to obtain the vector equation\n       .\n     \n      Using the relationship   gives us the equation\n       .\n     \n      Recall that   and that  ,\n       ,  ,\n        are in   to obtain the equation\n       .\n     \n      But this final equation is a linear combination of the basis elements in   of  ,\n      and so the scalars are all  .\n      Replacing these scalars with   in   results in\n       .\n     \n      But this is a linear combination of vectors in a basis for   and so all of the scalars are also  .\n      Hence, the vectors  ,\n       ,  ,  ,\n       ,  ,  ,\n       ,  ,  ,\n       ,   are linearly independent.\n     \n      The Rank-Nullity Theorem shows tells us that  .\n      The vectors in   form a basis for  ,\n      and so  .\n      The vectors in   form a basis for  ,\n      so  .\n      Thus,\n       .\n     \n      But this is exactly the number of vectors in our claimed basis  .\n      This verifies   with  ,\n        for  ,\n        and   for  .\n     "
},
{
  "id": "ex_JCF_Lemma_2_2",
  "level": "2",
  "url": "chap_JCF.html#ex_JCF_Lemma_2_2",
  "type": "Example",
  "number": "40.16",
  "title": "",
  "body": "\n        We work with the transformation   defined by\n         .\n       \n        Recall from   that\n         .\n        and a basis for   is\n          with  ,\n         , and  .\n        Since\n         \n        we see that   maps\n          to  .\n        The matrix of   with respect to   is\n         .\n       \n        The reduced row echelon form of\n          and   are\n         ,\n        while  .\n        This makes  .\n        We apply  \n        to  .\n        We choose   to be a vector in\n          that is not in  .\n        Once such vector has  ,\n        or  .\n        We then let   and  .\n        This gives us the basis   for  .\n       \n        We can also apply  \n        to  .\n        Since\n         ,\n        we have that\n         .\n       \n        It follows that  .\n        Selecting   and letting  ,\n        we obtain the basis   for  .\n       \n        Let  ,  ,\n         ,  , and  ,\n        and let  .\n        Since\n         \n        it follows that\n         ,\n        and we have found a basis for   for which the matrix   has a Jordan canonical form.\n       "
},
{
  "id": "example-84",
  "level": "2",
  "url": "chap_JCF.html#example-84",
  "type": "Example",
  "number": "40.17",
  "title": "",
  "body": "\n        Find a Jordan form   for each of the following matrices.\n        Find a matrix   such that  .\n       \n                 \n             \n              The eigenvalues of   are   and   with corresponding eigenvectors\n          and  .\n        Since we have a basis for   consisting of eigenvectors for  ,\n        we know that   is diagonalizable.\n        Moreover, Jordan canonical form of   is\n          and  ,\n        where  .\n             \n                 \n             \n              Since   is upper triangular,\n        its eigenvalues are the diagonal entries.\n        So the only eigenvalue of   is 0, and technology shows that this eigenvalue has geometric multiplicity 2.\n        An eigenvector for   is  .\n        A vector   that satisfies\n          is  .\n        Letting   gives us\n         .\n             \n                \n             \n              Again,   is upper triangular,\n        so the eigenvalues of   are   and  ,\n        both of algebraic multiplicity 2 and geometric multiplicity 1.\n        Technology shows that the reduced row echelon forms of\n          and   are\n         .\n        Now   is in  , and\n         .\n        Notice that  Let   is an eigenvector of   with eigenvalue  .\n        Technology also shows that the reduced row echelon forms of   and   are\n         .\n        Now   is in  , and\n         .\n        Notice that  Let   is an eigenvector of   with eigenvalue  .\n        Letting   gives us\n         .\n             "
},
{
  "id": "F_JCF_example_3_d",
  "level": "2",
  "url": "chap_JCF.html#F_JCF_example_3_d",
  "type": "Figure",
  "number": "40.18",
  "title": "",
  "body": "The effect of the transformation  . "
},
{
  "id": "example-85",
  "level": "2",
  "url": "chap_JCF.html#example-85",
  "type": "Example",
  "number": "40.19",
  "title": "",
  "body": "\n        Let  ,\n        where  .\n        Assume that  ,\n        where  .\n        Find a specific coordinate system in which it is possible to succinctly describe the action of  ,\n        then describe the action of   on   in as much detail as possible.\n       \n    First note that   is the only eigenvalue of  .\n    Since   does not have   as an eigenvalue,\n    it follows that   is invertible and so   is both one-to-one and onto.\n    Let  ,\n     ,\n    and  .\n    We have that\n     .\n    and so\n     .\n   \n    If we consider the coordinate system in   defined by the basis\n      as shown in blue in  ,\n    the fact that   shows that   fixes all vectors in  .\n    That   tells us that   maps\n      onto  ,\n    and   shows that   maps\n      onto  .\n    So   sends the box defined by  ,  ,\n    and   onto the box defined by  ,\n     ,\n    and   (in red in  .\n    So the action of   is conveniently viewed in the coordinate system determined by the columns of a matrix   that converts   into its Jordan canonical form.\n   "
},
{
  "id": "exercise-427",
  "level": "2",
  "url": "chap_JCF.html#exercise-427",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": "\n        Find a Jordan canonical form for each of the following matrices.\n       \n               \n             \n               \n             \n               \n             \n               \n             \n               \n             \n               \n             \n               \n             \n               \n             "
},
{
  "id": "exercise-428",
  "level": "2",
  "url": "chap_JCF.html#exercise-428",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": "\n        Let  .\n        Show that the Jordan canonical form of\n          is independent of the value of  .\n       "
},
{
  "id": "exercise-429",
  "level": "2",
  "url": "chap_JCF.html#exercise-429",
  "type": "Exercise",
  "number": "3",
  "title": "",
  "body": "\n        Show that the Jordan canonical form of\n          is independent of the value of  .\n       \n        The Jordan normal form is  .\n       "
},
{
  "id": "exercise-430",
  "level": "2",
  "url": "chap_JCF.html#exercise-430",
  "type": "Exercise",
  "number": "4",
  "title": "",
  "body": "\n        Let   and   be similar matrices with  .\n        Let   be the Jordan canonical form of  .\n        Show that   is also the Jordan canonical form of   with  .\n       "
},
{
  "id": "exercise-431",
  "level": "2",
  "url": "chap_JCF.html#exercise-431",
  "type": "Exercise",
  "number": "5",
  "title": "",
  "body": "\n        Find all of the Jordan canonical forms for\n          matrices and   matrices.\n       \n         ,\n         ,\n         ,\n         ,\n         ,\n         .\n       "
},
{
  "id": "exercise-432",
  "level": "2",
  "url": "chap_JCF.html#exercise-432",
  "type": "Exercise",
  "number": "6",
  "title": "",
  "body": "\n        Find the Jordan canonical form of  .\n       "
},
{
  "id": "exercise-433",
  "level": "2",
  "url": "chap_JCF.html#exercise-433",
  "type": "Exercise",
  "number": "7",
  "title": "",
  "body": "\n        For the matrix  ,\n        find a matrix   so that\n          is the Jordan canonical form of  ,\n        where first:\n        the diagonal entries do not increase as we move down the matrix   and, second:\n        the Jordan blocks do not increase in size as we move down the matrix  .\n         .\n       \n         \n       "
},
{
  "id": "exercise-434",
  "level": "2",
  "url": "chap_JCF.html#exercise-434",
  "type": "Exercise",
  "number": "8",
  "title": "",
  "body": "\n        A polynomial in two variables is an object of the form\n         \n        where   and   are integers greater than or equal to 0.\n        For example,\n         \n        is a polynomial in two variables.\n        The degree of a monomial   is  .\n        The degree of a polynomial   is the largest degree of any monomial summand of  .\n        So the degree of the example polynomial   is 4.\n        Two polynomials in   are equal if they have the same coefficients on like terms.\n        We add two polynomials in the variables   and   by adding coefficients of like terms.\n        Scalar multiplication is done by multiplying each coefficient by the given scalar.\n        Let   be the set of all polynomials of two variables of degree less than or equal to 2.\n        You may assume that   is a vector space under this addition and multiplication by scalars and has the standard additive identity and additive inverses.\n        Define   and   by\n         \n        and\n         .\n        That is,\n         \n        and\n         .\n        You may assume that   and   are linear transformations.\n       \n              Explain why   is a basis for  .\n             \n              Explain why   and   are not diagonalizable.\n             \n              Show that   and\n                have the same Jordan canonical form.\n              (So different transformations can have the same Jordan canonical form.)\n             \n              Find ordered bases   and\n                of   for which   and\n                are the Jordan canonical form from (c).\n             \n              Recall that a linear transformation can be defined by its action on a basis.\n              So define   satisfying\n                for   from 1 to 6.\n              Show that   is invertible and that  .\n              This is the essential argument to show that any two linear transformations with the same matrix with respect to some bases are similar.\n              \n             \n              What matrix is  ?\n             "
},
{
  "id": "exercise-435",
  "level": "2",
  "url": "chap_JCF.html#exercise-435",
  "type": "Exercise",
  "number": "9",
  "title": "",
  "body": "\n        Let\n         \n        be a chain of generalized eigenvectors for a matrix   corresponding to the eigenvalue  .\n        In this problem we show that the set\n          is linearly independent.\n       \n              Explain why   is in\n                for any   between   and  .\n             \n              Show that   for each   from   to  .\n             \n              Consider the equation\n               \n              for scalars  ,  ,\n               ,  .\n             \n                    Multiply both sides of   on the left by  .\n                    Explain why we can then conclude that  .\n                   \n                      is not in  ,\n                    so  \n                   \n                    Rewrite   using the result from part i.\n                    Explain how we can then demonstrate that  .\n                   \n                    The same process as in i. shows that  .\n                   \n                    Describe how we can use the process in parts i. and ii. to show that  .\n                    What does this tell us about the set  ?\n                   \n                    Keep repeating the process.\n                   "
},
{
  "id": "exercise-436",
  "level": "2",
  "url": "chap_JCF.html#exercise-436",
  "type": "Exercise",
  "number": "10",
  "title": "",
  "body": "\n        Find, if possible, a matrix transformation\n          for which there is a one-dimensional invariant subspace but no two-dimensional invariant subspace.\n        If not possible, explain why.\n       "
},
{
  "id": "exercise-437",
  "level": "2",
  "url": "chap_JCF.html#exercise-437",
  "type": "Exercise",
  "number": "11",
  "title": "",
  "body": "\n        Let  .\n        It is the case that   has a single eigenvalue of algebraic multiplicity 2 and geometric multiplicity 1.\n       \n              Find a matrix   whose columns are generalized eigenvectors for   so that\n                is in Jordan canonical form.\n             \n                with\n                and  \n             \n              Determine the entries of   and then find the entries of   for any positive integer  .\n             \n               ,\n               \n             "
},
{
  "id": "exercise-438",
  "level": "2",
  "url": "chap_JCF.html#exercise-438",
  "type": "Exercise",
  "number": "12",
  "title": "",
  "body": "\n        Let  ,  ,  ,\n          be eigenvectors of an   matrix  ,\n        and let  .\n        Show that   is invariant under the matrix transformation   defined by  .\n       "
},
{
  "id": "exercise-439",
  "level": "2",
  "url": "chap_JCF.html#exercise-439",
  "type": "Exercise",
  "number": "13",
  "title": "",
  "body": "\n        Let   be an   matrix with eigenvalue  .\n        Let   be defined by\n          for some matrix  .\n        Show that if   commutes with  ,\n        then the eigenspace   of   corresponding to the eigenvalue   is invariant under  .\n       \n        Show that  \n       "
},
{
  "id": "exercise-440",
  "level": "2",
  "url": "chap_JCF.html#exercise-440",
  "type": "Exercise",
  "number": "14",
  "title": "",
  "body": "\n        Determine which of the following matrices is nilpotent.\n        Justify your answers.\n        For each nilpotent matrix, find its index.\n       \n               \n             \n               \n             \n               \n             \n                for any real numbers   and  \n             "
},
{
  "id": "exercise-441",
  "level": "2",
  "url": "chap_JCF.html#exercise-441",
  "type": "Exercise",
  "number": "15",
  "title": "",
  "body": "\n        Find two different nonzero\n          nilpotent matrices whose index is  .\n        If no such matrices exist, explain why.\n       \n          and  \n       "
},
{
  "id": "exercise-442",
  "level": "2",
  "url": "chap_JCF.html#exercise-442",
  "type": "Exercise",
  "number": "16",
  "title": "",
  "body": "\n        Find, if possible,\n          matrices whose indices are  ,\n         ,  , and  .\n        If not possible, explain why.\n       "
},
{
  "id": "exercise-443",
  "level": "2",
  "url": "chap_JCF.html#exercise-443",
  "type": "Exercise",
  "number": "17",
  "title": "",
  "body": "\n        Let   be an  -dimensional vector space and let   be a subspace of  .\n        Show that  .\n       \n        Use a projection onto a subspace.\n       "
},
{
  "id": "exercise-444",
  "level": "2",
  "url": "chap_JCF.html#exercise-444",
  "type": "Exercise",
  "number": "18",
  "title": "",
  "body": "\n        Let   be a subspace of an  -dimensional vector space   that is invariant under a transformation  .\n       \n              Show by example that   need not be invariant under  .\n             \n              Show that if   is an isometry,\n              then   is invariant under  .\n             "
},
{
  "id": "exercise-445",
  "level": "2",
  "url": "chap_JCF.html#exercise-445",
  "type": "Exercise",
  "number": "19",
  "title": "",
  "body": "\n        Let   be defined by  ,\n        where   is the derivative of  .\n        That is, if  ,\n        then  .\n        Find a basis   for   in which the matrix\n          is in Jordan canonical form.\n       \n         \n       "
},
{
  "id": "exercise-446",
  "level": "2",
  "url": "chap_JCF.html#exercise-446",
  "type": "Exercise",
  "number": "20",
  "title": "",
  "body": "\n        Let   be an   nilpotent matrix with index  .\n        Since   is not zero,\n        there is a vector   such that  .\n        Prove that the vectors  ,  ,\n         ,  ,\n          are linearly independent.\n       "
},
{
  "id": "exercise-447",
  "level": "2",
  "url": "chap_JCF.html#exercise-447",
  "type": "Exercise",
  "number": "21",
  "title": "",
  "body": "\n            Let  .\n           \n                  Show that   is nilpotent.\n                 \n                  Calculate powers of  .\n                 \n                  Calculate the matrix product  .\n                  What do you notice and what does this tell us about  ?\n                 \n                   .\n                 \n            If   is an   nilpotent matrix,\n            show that   is nonsingular,\n            where   is the   identity matrix.\n           \n            Calculate\n             .\n           "
},
{
  "id": "ex_8_d_upper_triangular",
  "level": "2",
  "url": "chap_JCF.html#ex_8_d_upper_triangular",
  "type": "Exercise",
  "number": "22",
  "title": "",
  "body": "\n        In this exercise we show that every upper triangular matrix satisfies its characteristic polynomial.\n       \n              To illustrate how this will work,\n              consider a   example.\n              Let  .\n             \n                    What is the characteristic polynomial   of  ?\n                   \n                    Consider the matrices  ,\n                     ,\n                    and  .\n                    Show that the first column of   is  ,\n                    the first two columns of   are  ,\n                    and that every column of   is  .\n                    Conclude that   satisfies its characteristic polynomial.\n                   \n              Now we consider the general case.\n              Suppose   is an   upper triangular matrix with diagonal entries  ,\n               ,  ,\n                in order.\n              For   from   to   let  .\n              Show that for each   from 1 to  ,\n              the first   columns of\n                are equal to  .\n              Then explain how this demonstrates that   satisfies its characteristic polynomial.\n             "
},
{
  "id": "exercise-449",
  "level": "2",
  "url": "chap_JCF.html#exercise-449",
  "type": "Exercise",
  "number": "23",
  "title": "",
  "body": "\n        Prove  \n        that a square matrix   is nilpotent if and only if 0 is the only eigenvalue of  .\n        \n       \n        For one direction, use the Cayley-Hamilton Theorem.\n       \n        If   is nilpotent and   is an eigenvector of  ,\n        what is   for every positive integer  ?\n        If   is the only eigenvalue of  ,\n        what is the characteristic polynomial of  ?\n       "
},
{
  "id": "ex_Direct_sum_properties",
  "level": "2",
  "url": "chap_JCF.html#ex_Direct_sum_properties",
  "type": "Exercise",
  "number": "24",
  "title": "",
  "body": "\n        Let   be a vector space that is a direct sum\n          for some positive integer  .\n        Prove the following.\n       \n                whenever  .\n             \n              If   is finite dimensional,\n              and if   is a basis for  ,\n              then the set   is a basis for  .\n             \n               .\n             "
},
{
  "id": "exercise-451",
  "level": "2",
  "url": "chap_JCF.html#exercise-451",
  "type": "Exercise",
  "number": "25",
  "title": "",
  "body": "\n        Label each of the following statements as True or False.\n        Provide justification for your response.\n       True\/False \n              If   is an   nilpotent matrix,\n              then the index of   is  .\n             \n              F\n             True\/False \n              The Jordan canonical form of a matrix is unique.\n             True\/False \n              Every nilpotent matrix is singular.\n             \n              T\n             True\/False \n              Eigenvectors of a linear transformation   are also generalized eigenvectors of  .\n             True\/False \n              It is possible for a generalized eigenvector of a matrix   to correspond to a scalar that is not an eigenvalue for  .\n             \n              F\n             True\/False \n              The vectors in a cycle of generalized eigenvectors of a matrix are linearly independent.\n             True\/False \n              A Jordan canonical form of a diagonal matrix   is  .\n             \n              T\n             True\/False \n              Let   be a finite-dimensional vector space and let   be a linear transformation from   to  .\n              Let   be a Jordan canonical form for  .\n              If   is a basis of  ,\n              then a Jordan canonical form of   is  .\n             True\/False \n              Matrices with the same Jordan canonical form are similar.\n             \n              T\n             True\/False \n              Let   be a finite-dimensional vector space and let   be a linear transformation from   to  .\n              If   is an ordered basis for  ,\n              then   is nilpotent if and only if   is nilpotent.\n             True\/False \n              If   is a linear transformation whose only eigenvalue is 0, then   is the zero transformation.\n             \n              F\n             True\/False \n              Let   be a finite-dimensional vector space and let   be a linear transformation from   to  .\n              Then  .\n             True\/False \n              Let   be a finite-dimensional vector space and let   be a linear transformation from   to  .\n              If   is an ordered basis for   of generalized eigenvectors of  ,\n              then   is a Jordan canonical form for  .\n             \n              F\n             "
},
{
  "id": "p-7218",
  "level": "2",
  "url": "chap_JCF.html#p-7218",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "stochastic "
},
{
  "id": "project-148",
  "level": "2",
  "url": "chap_JCF.html#project-148",
  "type": "Project Activity",
  "number": "40.12",
  "title": "",
  "body": "\n      If   is diagonalizable,\n      then   can be found fairly easily.\n      Assume that there is an invertible matrix   such that  ,\n      where   is a diagonal matrix.\n     \n            Use   to explain why\n             .\n           \n            Now show that\n             .\n            \n           \n            What is   for any positive integer  ?\n            Then add corresponding components and compare to  .\n           "
},
{
  "id": "project-149",
  "level": "2",
  "url": "chap_JCF.html#project-149",
  "type": "Project Activity",
  "number": "40.13",
  "title": "",
  "body": "\n      Assume   is a   matrix with Jordan canonical form ,\n      where  .\n      The same argument as above shows that\n       \n      and so we only have to be able to find  .\n     \n            We can write   in the form  ,\n            where   is a diagonal matrix and   is a nilpotent matrix.\n            Find   and   in this case and explain why   is a nilpotent matrix.\n           \n            Find the index of  .\n            That is, find the smallest positive power   of   such that  .\n            Then use   to find  .\n           \n            Assume that the matrix exponential satisfies the standard property of exponential functions that   for any\n              matrices   and  .\n            Use this property to explain why\n             .\n           \n            Use the previous information to calculate   where  .\n           "
},
{
  "id": "project-150",
  "level": "2",
  "url": "chap_JCF.html#project-150",
  "type": "Project Activity",
  "number": "40.14",
  "title": "",
  "body": "\n      Let us apply this analysis to a specific case of Bailey's model,\n      with  .\n     \n            Find the entries of   when  .\n           \n            Let   be a Jordan canonical form for  .\n            Explain why the solution to Bailey's model has the form\n             ,\n            where   is an invertible matrix,\n              is a diagonal matrix,\n            and   is a nilpotent matrix.\n           \n            Find a Jordan canonical form   for  .\n           \n            Find a diagonal matrix   and a nilpotent matrix   so that  .\n           \n            Find   and  .\n            Then show that\n             .\n           "
},
{
  "id": "project-151",
  "level": "2",
  "url": "chap_JCF.html#project-151",
  "type": "Project Activity",
  "number": "40.15",
  "title": "",
  "body": "\n      If  ,\n      then the disease is easily transmitted from person to person.\n      If   can be made smaller,\n      then the disease is not so easily transmitted.\n      We continue to work with the case where  .\n      Plot the curves   through   on the same set of axes for the following values of  :\n       .\n     \n      Explain what you see and how this might be related to the phrase\n       flattening the curve \n      used during the COVID-19 pandemic of 2020.\n     "
},
{
  "id": "app_complex_numbers",
  "level": "1",
  "url": "app_complex_numbers.html",
  "type": "Appendix",
  "number": "A",
  "title": "Complex Numbers",
  "body": "Complex Numbers \n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is a complex number?\n           \n         \n           \n            How is the sum and product of two complex numbers defined?\n           \n         \n           \n            How do we find the multiplicative inverse of a nonzero complex number?\n           \n         \n           \n            What general structure does the set of complex numbers have?\n           \n         Complex Numbers \n    Complex numbers are usually introduced as a tool to solve the quadratic equation  .\n    However, that is not how complex numbers first came to light.\n    The story actually involves solutions to the general cubic equation.\n    The interested reader could consult Chapter 6 of William Dunham's excellent book\n     Journey Through Genius .\n    In this appendix we touch on the basics of complex numbers to provide enough context for the section on complex eigenvalues.\n   real part imaginary part complex number complex number \n   complex number real part \n   complex number imaginary part \n    The number   is the real part of the complex number and the number   is the imaginary part.\n    We often write\n     \n    for a complex number  ,\n      for the real part of   and\n      for the imaginary part of  .\n    That is, if   with   and   real numbers,\n    then   and  .\n    We say that two complex numbers   and   are equal if   and  .\n   \n    There is an arithmetic of complex numbers that is determined by an addition and multiplication of complex numbers.\n    Adding complex numbers is natural:\n     .\n   \n    That is, to add two complex numbers we add their real parts together and their imaginary parts together.\n   \n      Multiplication of complex numbers is is also done in a natural way.\n     \n            By expanding the product as usual,\n            treating   as we would any real number,\n            and exploiting the fact that  ,\n            explain why we define the product of complex numbers   and   as\n             .\n           \n            Use the definitions of addition and multiplication to write each of the sums or products as a complex number in the form  .\n           \n                   \n                 \n                   \n                 \n                   \n                 \n    It isn't difficult to show that the set of complex numbers,\n    which we denote by  ,\n    satisfies many useful and familiar properties.\n   \n      Show that   has the same structure as  .\n      That is, show that for all  ,  ,\n      and   in  , the following properties are satisfied.\n     \n              and  \n           \n              and  \n           \n              and  \n           \n            There is an element   in   such that  \n           \n            There is an element   in   such that  \n           \n            There is an element   in   such that  \n           \n            If  , there is an element\n              in   such that  \n           \n             \n           \n    The result of   is that,\n    just like  , the set   is a field.\n    If we wanted to,\n    we could define vector spaces over   just like we did over  .\n    The same results hold.\n   Conjugates and Modulus \n    We can draw pictures of complex numbers in the plane.\n    We let the  -axis be the real axis for a complex number and the  -axis the imaginary axis.\n    That is, if   we can think of   as a directed line segment from the origin to the point  ,\n    where the terminal point of the segment is   units from the imaginary axis and   units from the real axis.\n    For example,\n    the complex numbers   and   are shown in  .\n   Two complex numbers. norm modulus complex conjugate \n      Let   and  .\n     \n            Find   and  .\n           \n            Compute   and  .\n           \n            Compute   and  .\n           \n            Let   be an arbitrary complex number.\n            There is a relationship between  ,\n             , and  .\n            Find and verify this relationship.\n           \n            What is   if  ?\n           Complex Vectors \n    A vector can have real and imaginary parts, too.\n    For example,\n    the vector   can be written as\n     .\n   \n    The vector   is the real part of   and the vector\n      is the imaginary part of  .\n    In this way any vector   with complex entries can be written in the form\n     ,\n    where   and\n      are vectors with real entries.\n    The conjugate of the vector\n      is the vector  .\n    We can do the same with matrices with complex entries.\n   \n    There are several properties of complex conjugates that can be useful.\n    Suppose   is a complex number,\n      is a vector with possibly complex entries,\n    and   and   are matrices with possibly complex entries.\n    Assume all products that are listed are defined.\n    Then\n     \n         \n           \n         \n       \n         \n           \n         \n       \n         \n           \n         \n       \n         \n           \n         \n       \n   \n    These properties can be verified using complex arithmetic.\n    For example, to verify the first property,\n    let   be a complex number and let   be a complex vector.\n    The operations on complex numbers give us\n     .\n   \n    Verification of the remaining properties is left to the reader.\n   "
},
{
  "id": "objectives-41",
  "level": "2",
  "url": "app_complex_numbers.html#objectives-41",
  "type": "Focus Questions",
  "number": "",
  "title": "",
  "body": "\n          By the end of this section, you should be able to give precise and thorough\n          answers to the questions listed below. You may want to keep these questions\n          in mind to focus your thoughts as you complete the section.\n         \n           \n            What is a complex number?\n           \n         \n           \n            How is the sum and product of two complex numbers defined?\n           \n         \n           \n            How do we find the multiplicative inverse of a nonzero complex number?\n           \n         \n           \n            What general structure does the set of complex numbers have?\n           \n         "
},
{
  "id": "p-7261",
  "level": "2",
  "url": "app_complex_numbers.html#p-7261",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "real part imaginary part "
},
{
  "id": "definition-103",
  "level": "2",
  "url": "app_complex_numbers.html#definition-103",
  "type": "Definition",
  "number": "A.1",
  "title": "",
  "body": "complex number complex number "
},
{
  "id": "activity-163",
  "level": "2",
  "url": "app_complex_numbers.html#activity-163",
  "type": "Activity",
  "number": "A.1",
  "title": "",
  "body": "\n      Multiplication of complex numbers is is also done in a natural way.\n     \n            By expanding the product as usual,\n            treating   as we would any real number,\n            and exploiting the fact that  ,\n            explain why we define the product of complex numbers   and   as\n             .\n           \n            Use the definitions of addition and multiplication to write each of the sums or products as a complex number in the form  .\n           \n                   \n                 \n                   \n                 \n                   \n                 "
},
{
  "id": "act_complex_field",
  "level": "2",
  "url": "app_complex_numbers.html#act_complex_field",
  "type": "Activity",
  "number": "A.2",
  "title": "",
  "body": "\n      Show that   has the same structure as  .\n      That is, show that for all  ,  ,\n      and   in  , the following properties are satisfied.\n     \n              and  \n           \n              and  \n           \n              and  \n           \n            There is an element   in   such that  \n           \n            There is an element   in   such that  \n           \n            There is an element   in   such that  \n           \n            If  , there is an element\n              in   such that  \n           \n             \n           "
},
{
  "id": "F_complex_numbers",
  "level": "2",
  "url": "app_complex_numbers.html#F_complex_numbers",
  "type": "Figure",
  "number": "A.2",
  "title": "",
  "body": "Two complex numbers. "
},
{
  "id": "p-7284",
  "level": "2",
  "url": "app_complex_numbers.html#p-7284",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "norm modulus "
},
{
  "id": "p-7285",
  "level": "2",
  "url": "app_complex_numbers.html#p-7285",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "complex conjugate "
},
{
  "id": "activity-165",
  "level": "2",
  "url": "app_complex_numbers.html#activity-165",
  "type": "Activity",
  "number": "A.3",
  "title": "",
  "body": "\n      Let   and  .\n     \n            Find   and  .\n           \n            Compute   and  .\n           \n            Compute   and  .\n           \n            Let   be an arbitrary complex number.\n            There is a relationship between  ,\n             , and  .\n            Find and verify this relationship.\n           \n            What is   if  ?\n           "
},
{
  "id": "app_answers",
  "level": "1",
  "url": "app_answers.html",
  "type": "Appendix",
  "number": "B",
  "title": "Answers and Hints for Selected Exercises",
  "body": "Answers and Hints for Selected Exercises "
},
{
  "id": "index-1",
  "level": "1",
  "url": "index-1.html",
  "type": "Index",
  "number": "",
  "title": "Index",
  "body": "Index "
}
]

var ptx_lunr_idx = lunr(function () {
  this.ref('id')
  this.field('title')
  this.field('body')

  ptx_lunr_docs.forEach(function (doc) {
    this.add(doc)
  }, this)
})
