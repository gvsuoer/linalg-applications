<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US">
<head xmlns:og="http://ogp.me/ns#" xmlns:book="https://ogp.me/ns/book#">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Least Squares Approximations</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:type" content="book">
<meta property="book:title" content="An Inquiry-Based Introduction to Linear Algebra and Applications">
<meta property="book:author" content="Feryal Alayont">
<meta property="book:author" content="Steven Schlicker">
<script>var runestoneMathReady = new Promise((resolve) => window.rsMathReady = resolve);
window.MathJax = {
  tex: {
    inlineMath: [['\\(','\\)']],
    tags: "none",
    tagSide: "right",
    tagIndent: ".8em",
    packages: {'[+]': ['base', 'extpfeil', 'ams', 'amscd', 'newcommand', 'knowl']}
  },
  options: {
    ignoreHtmlClass: "tex2jax_ignore|ignore-math",
    processHtmlClass: "process-math",
  },
  chtml: {
    scale: 0.88,
    mtextInheritFont: true
  },
  loader: {
    load: ['input/asciimath', '[tex]/extpfeil', '[tex]/amscd', '[tex]/newcommand', '[pretext]/mathjaxknowl3.js'],
    paths: {pretext: "https://pretextbook.org/js/lib"},
  },
  startup: {
    pageReady() {
      return MathJax.startup.defaultPageReady().then(function () {
      console.log("in ready function");
      rsMathReady();
      }
    )}
},
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/themes/prism.css" rel="stylesheet">
<script async="" src="https://cse.google.com/cse.js?cx=a16e70a6cb1434676"></script><script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.13/pretext.js"></script><script>miniversion=0.674</script><script src="https://pretextbook.org/js/0.13/pretext_add_on.js?x=1"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script>sagecellEvalName='Evaluate (Sage)';
</script><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/banner_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/toc_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/knowls_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/style_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/colors_blue_grey.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/setcolors.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="external/custom_style.css">
</head>
<body class="pretext-book ignore-math has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div id="latex-macros" class="hidden-content process-math" style="display:none"><span class="process-math">\(\require{colortbl}\usepackage{amsmath}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\E}{\mathbb{E}}
\DeclareMathOperator{\ch}{char}
\newcommand{\N}{\mathbb{N}}
\newcommand{\W}{\mathbb{W}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\J}{\mathbb{J}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathbb{D}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\NE}{\mathcal{E}}
\newcommand{\Mn}[1]{\mathcal{M}_{#1 \times #1}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\Rn}{\R^n}
\newcommand{\Mat}{\mathbf}
\newcommand{\Seq}{\boldsymbol}
\newcommand{\seq}[1]{\boldsymbol{#1}}
\newcommand{\set}[1]{\mathcal{#1}}
\newcommand{\abs}[1]{\left\lvert{}#1\right\rvert}
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\cq}{\scalebox{.34}{\pscirclebox{ \textbf{?}}}}
\newcommand{\cqup}{\,$^{\text{\scalebox{.2}{\pscirclebox{\textbf{?}}}}}$}
\newcommand{\cqupmath}{\,^{\text{\scalebox{.2}{\pscirclebox{\textbf{?}}}}}}
\newcommand{\cqupmathnospace}{^{\text{\scalebox{.2}{\pscirclebox{\textbf{?}}}}}}
\newcommand{\uspace}[1]{\underline{}}
\newcommand{\muspace}[1]{\underline{\mspace{#1 mu}}}
\newcommand{\bspace}[1]{}
\newcommand{\ie}{\emph{i}.\emph{e}.}
\newcommand{\nq}[1]{\scalebox{.34}{\pscirclebox{\textbf{#1}}}}
\newcommand{\equalwhy}{\stackrel{\cqupmath}{=}}
\newcommand{\notequalwhy}{\stackrel{\cqupmath}{\neq}}
\newcommand{\Ker}{\text{Ker}}
\newcommand{\Image}{\text{Im}}
\newcommand{\polyp}{p(x) = a_nx^n + a_{n-1}x^{n-1} + \cdots a_2x^2 + a_1x + a_0}
\newcommand{\GL}{\text{GL}}
\newcommand{\SL}{\text{SL}}
\newcommand{\lcm}{\text{lcm}}
\newcommand{\Aut}{\text{Aut}}
\newcommand{\Inn}{\text{Inn}}
\newcommand{\Hol}{\text{Hol}}
\newcommand{\cl}{\text{cl}}
\newcommand{\bsq}{\hfill $\blacksquare$}
\newcommand{\NIMdot}{{ $\cdot$ }}
\newcommand{\eG}{e_{\scriptscriptstyle{G}}}
\newcommand{\eGroup}[1]{e_{\scriptscriptstyle{#1}}}
\newcommand{\Gdot}[1]{\cdot_{\scriptscriptstyle{#1}}}
\newcommand{\rbar}{\overline{r}}
\newcommand{\nuG}[1]{\nu_{\scriptscriptstyle{#1}}}
\newcommand{\rightarray}[2]{\left[ \begin{array}{#1} #2 \end{array} \right]}
\newcommand{\polymod}[1]{\mspace{5 mu}(\text{mod} #1)}
\newcommand{\ts}{\mspace{2 mu}}
\newcommand{\ds}{\displaystyle}
\newcommand{\adj}{\text{adj}}
\newcommand{\pol}{\mathbb{P}}
\newcommand{\CS}{\mathcal{S}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\CB}{\mathcal{B}}
\renewcommand{\CD}{\mathcal{D}}
\newcommand{\CP}{\mathcal{P}}
\newcommand{\CQ}{\mathcal{Q}}
\newcommand{\CW}{\mathcal{W}}
\newcommand{\rank}{\text{rank}}
\newcommand{\nullity}{\text{nullity}}
\newcommand{\trace}{\text{trace}}
\newcommand{\Area}{\text{Area}}
\newcommand{\Vol}{\text{Vol}}
\newcommand{\cov}{\text{cov}}
\newcommand{\var}{\text{var}}
\newcommand{\va}{\mathbf{a}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\vc}{\mathbf{c}}
\newcommand{\vd}{\mathbf{d}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\vf}{\mathbf{f}}
\newcommand{\vg}{\mathbf{g}}
\newcommand{\vh}{\mathbf{h}}
\newcommand{\vm}{\mathbf{m}}
\newcommand{\vn}{\mathbf{n}}
\newcommand{\vp}{\mathbf{p}}
\newcommand{\vq}{\mathbf{q}}
\newcommand{\vr}{\mathbf{r}}
\newcommand{\vs}{\mathbf{s}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\vzero}{\mathbf{0}}
\newcommand{\vi}{\mathbf{i}}
\newcommand{\vj}{\mathbf{j}}
\newcommand{\vk}{\mathbf{k}}
\newcommand{\vF}{\mathbf{F}}
\newcommand{\vP}{\mathbf{P}}
\newcommand{\nin}{}
\newcommand{\Dom}{\text{Dom}}
\newcommand{\tr}{\mathsf{T}}
\newcommand{\proj}{\text{proj}}
\newcommand{\comp}{\text{comp}}
\newcommand{\Row}{\text{Row }}
\newcommand{\Col}{\text{Col }}
\newcommand{\Nul}{\text{Nul }}
\newcommand{\Span}{\text{Span}}
\newcommand{\Range}{\text{Range}}
\newcommand{\Domain}{\text{Domain}}
\newcommand{\hthin}{\hlinewd{.1pt}}
\newcommand{\hthick}{\hlinewd{.7pt}}
\newcommand{\pbreaks}{1}
\newcommand{\pbreak}{
}
\newcommand{\lint}{\underline{}
\int}
\newcommand{\uint}{ \underline{}
\int}


\newcommand{\Si}{\text{Si}}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\definecolor{fillinmathshade}{gray}{0.9}
\newcommand{\fillinmath}[1]{\mathchoice{\colorbox{fillinmathshade}{$\displaystyle     \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\textstyle        \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptstyle      \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptscriptstyle\phantom{\,#1\,}$}}}
\)</span></div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="book-1.html"><span class="title">An Inquiry-Based Introduction to Linear Algebra and Applications</span></a></h1>
<p class="byline">Feryal Alayont, Steven Schlicker</p>
</div>
<div class="searchwrapper" role="search"><div class="gcse-search"></div></div>
</div></div>
<nav id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3">
<a class="index-button toolbar-item button" href="index-1.html" title="Index">Index</a><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="chap_gram_schmidt.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="part-orthog.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="part-app-orthog.html" title="Next">Next</a></span>
</div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="chap_gram_schmidt.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="part-orthog.html" title="Up">Up</a><a class="next-button button toolbar-item" href="part-app-orthog.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link frontmatter">
<a href="frontmatter.html" data-scroll="frontmatter" class="internal"><span class="title">Front Matter</span></a><ul><li><a href="fm_preface.html" data-scroll="fm_preface" class="internal">Preface</a></li></ul>
</li>
<li class="link part"><a href="part-systems.html" data-scroll="part-systems" class="internal"><span class="codenumber">I</span> <span class="title">Systems of Linear Equations</span></a></li>
<li class="link">
<a href="chap_intro_linear_systems.html" data-scroll="chap_intro_linear_systems" class="internal"><span class="codenumber">1</span> <span class="title">Introduction to Systems of Linear Equations</span></a><ul>
<li><a href="chap_intro_linear_systems.html#sec_appl_elec_circuits" data-scroll="sec_appl_elec_circuits" class="internal">Application: Electrical Circuits</a></li>
<li><a href="chap_intro_linear_systems.html#sec_intro_le_intro" data-scroll="sec_intro_le_intro" class="internal">Introduction</a></li>
<li><a href="chap_intro_linear_systems.html#sec_notation" data-scroll="sec_notation" class="internal">Notation and Terminology</a></li>
<li><a href="chap_intro_linear_systems.html#sec_solve_systems" data-scroll="sec_solve_systems" class="internal">Solving Systems of Linear Equations</a></li>
<li><a href="chap_intro_linear_systems.html#sec_geom_solu_sets" data-scroll="sec_geom_solu_sets" class="internal">The Geometry of Solution Sets of Linear Systems</a></li>
<li><a href="chap_intro_linear_systems.html#sec_intro_le_exam" data-scroll="sec_intro_le_exam" class="internal">Examples</a></li>
<li><a href="chap_intro_linear_systems.html#sec_intro_le_summ" data-scroll="sec_intro_le_summ" class="internal">Summary</a></li>
<li><a href="chap_intro_linear_systems.html#sec_intro_le_exer" data-scroll="sec_intro_le_exer" class="internal">Exercises</a></li>
<li><a href="chap_intro_linear_systems.html#sec_1_a_circuits" data-scroll="sec_1_a_circuits" class="internal">Project: Modeling an Electrical Circuit and the Wheatstone Bridge Circuit</a></li>
</ul>
</li>
<li class="link">
<a href="chap_matrix_representation.html" data-scroll="chap_matrix_representation" class="internal"><span class="codenumber">2</span> <span class="title">The Matrix Representation of a Linear System</span></a><ul>
<li><a href="chap_matrix_representation.html#sec_appl_area_curve" data-scroll="sec_appl_area_curve" class="internal">Application: Approximating Area Under a Curve</a></li>
<li><a href="chap_matrix_representation.html#sec_mtx_lin_intro" data-scroll="sec_mtx_lin_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_representation.html#sec_simp_mtx_sys" data-scroll="sec_simp_mtx_sys" class="internal">Simplifying Linear Systems Represented in Matrix Form</a></li>
<li><a href="chap_matrix_representation.html#sec_sys_inf_sols" data-scroll="sec_sys_inf_sols" class="internal">Linear Systems with Infinitely Many Solutions</a></li>
<li><a href="chap_matrix_representation.html#sec_sys_no_sols" data-scroll="sec_sys_no_sols" class="internal">Linear Systems with No Solutions</a></li>
<li><a href="chap_matrix_representation.html#sec_mtx_sys_exam" data-scroll="sec_mtx_sys_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_representation.html#sec_mtx_sys_summ" data-scroll="sec_mtx_sys_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_representation.html#sec_mtx_sys_exer" data-scroll="sec_mtx_sys_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_representation.html#sec_1_b_polynomial" data-scroll="sec_1_b_polynomial" class="internal">Project: Polynomial Interpolation to Approximate the Area Under a Curve</a></li>
</ul>
</li>
<li class="link">
<a href="chap_row_echelon_forms.html" data-scroll="chap_row_echelon_forms" class="internal"><span class="codenumber">3</span> <span class="title">Row Echelon Forms</span></a><ul>
<li><a href="chap_row_echelon_forms.html#sec_appl_chem_react" data-scroll="sec_appl_chem_react" class="internal">Application: Balancing Chemical Reactions</a></li>
<li><a href="chap_row_echelon_forms.html#sec_row_ech_intro" data-scroll="sec_row_ech_intro" class="internal">Introduction</a></li>
<li><a href="chap_row_echelon_forms.html#sec_mtx_ech_forms" data-scroll="sec_mtx_ech_forms" class="internal">The Echelon Forms of a Matrix</a></li>
<li><a href="chap_row_echelon_forms.html#sec_num_sols_ls" data-scroll="sec_num_sols_ls" class="internal">Determining the Number of Solutions of a Linear System</a></li>
<li><a href="chap_row_echelon_forms.html#sec_prod_ech_forms" data-scroll="sec_prod_ech_forms" class="internal">Producing the Echelon Forms</a></li>
<li><a href="chap_row_echelon_forms.html#sec_row_ech_exam" data-scroll="sec_row_ech_exam" class="internal">Examples</a></li>
<li><a href="chap_row_echelon_forms.html#sec_row_ech_summ" data-scroll="sec_row_ech_summ" class="internal">Summary</a></li>
<li><a href="chap_row_echelon_forms.html#sec_row_ech_exer" data-scroll="sec_row_ech_exer" class="internal">Exercises</a></li>
<li><a href="chap_row_echelon_forms.html#sec_prof_chem_react" data-scroll="sec_prof_chem_react" class="internal">Project: Modeling a Chemical Reaction</a></li>
</ul>
</li>
<li class="link">
<a href="chap_vector_representation.html" data-scroll="chap_vector_representation" class="internal"><span class="codenumber">4</span> <span class="title">Vector Representation</span></a><ul>
<li><a href="chap_vector_representation.html#sec_appl_knight" data-scroll="sec_appl_knight" class="internal">Application: The Knight's Tour</a></li>
<li><a href="chap_vector_representation.html#sec_vec_rep_intro" data-scroll="sec_vec_rep_intro" class="internal">Introduction</a></li>
<li><a href="chap_vector_representation.html#sec_vec_ops" data-scroll="sec_vec_ops" class="internal">Vectors and Vector Operations</a></li>
<li><a href="chap_vector_representation.html#sec_geom_vec_ops" data-scroll="sec_geom_vec_ops" class="internal">Geometric Representation of Vectors and Vector Operations</a></li>
<li><a href="chap_vector_representation.html#sec_lin_comb_vec" data-scroll="sec_lin_comb_vec" class="internal">Linear Combinations of Vectors</a></li>
<li><a href="chap_vector_representation.html#sec_vec_span" data-scroll="sec_vec_span" class="internal">The Span of a Set of Vectors</a></li>
<li><a href="chap_vector_representation.html#sec_vec_rep_exam" data-scroll="sec_vec_rep_exam" class="internal">Examples</a></li>
<li><a href="chap_vector_representation.html#sec_vec_rep_summ" data-scroll="sec_vec_rep_summ" class="internal">Summary</a></li>
<li><a href="chap_vector_representation.html#exercises-4" data-scroll="exercises-4" class="internal">Exercises</a></li>
<li><a href="chap_vector_representation.html#sec_proj_knight" data-scroll="sec_proj_knight" class="internal">Project: Analyzing Knight Moves</a></li>
</ul>
</li>
<li class="link">
<a href="chap_matrix_vector.html" data-scroll="chap_matrix_vector" class="internal"><span class="codenumber">5</span> <span class="title">The Matrix-Vector Form of a Linear System</span></a><ul>
<li><a href="chap_matrix_vector.html#sec_appl_model_econ" data-scroll="sec_appl_model_econ" class="internal">Application: Modeling an Economy</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form_intro" data-scroll="sec_mv_form_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_prod" data-scroll="sec_mv_prod" class="internal">The Matrix-Vector Product</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form" data-scroll="sec_mv_form" class="internal">The Matrix-Vector Form of a Linear System</a></li>
<li><a href="chap_matrix_vector.html#sec_homog_sys" data-scroll="sec_homog_sys" class="internal">Homogeneous and Nonhomogeneous Systems</a></li>
<li><a href="chap_matrix_vector.html#sec_geom_homog_sys" data-scroll="sec_geom_homog_sys" class="internal">The Geometry of Solutions to the Homogeneous System</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form_exam" data-scroll="sec_mv_form_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form_summ" data-scroll="sec_mv_form_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form_exer" data-scroll="sec_mv_form_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_vector.html#sec_proj_io_models" data-scroll="sec_proj_io_models" class="internal">Project: Input-Output Models</a></li>
</ul>
</li>
<li class="link">
<a href="chap_independence.html" data-scroll="chap_independence" class="internal"><span class="codenumber">6</span> <span class="title">Linear Dependence and Independence</span></a><ul>
<li><a href="chap_independence.html#sec_appl_bezier" data-scroll="sec_appl_bezier" class="internal">Application: Bézier Curves</a></li>
<li><a href="chap_independence.html#sec_indep_intro" data-scroll="sec_indep_intro" class="internal">Introduction</a></li>
<li><a href="chap_independence.html#lin_indep_intro_new" data-scroll="lin_indep_intro_new" class="internal">Linear Independence</a></li>
<li><a href="chap_independence.html#sec_determ_lin_ind" data-scroll="sec_determ_lin_ind" class="internal">Determining Linear Independence</a></li>
<li><a href="chap_independence.html#sec_min_span_set" data-scroll="sec_min_span_set" class="internal">Minimal Spanning Sets</a></li>
<li><a href="chap_independence.html#sec_indep_exam" data-scroll="sec_indep_exam" class="internal">Examples</a></li>
<li><a href="chap_independence.html#sec_indep_summ" data-scroll="sec_indep_summ" class="internal">Summary</a></li>
<li><a href="chap_independence.html#sec_indep_exer" data-scroll="sec_indep_exer" class="internal">Exercises</a></li>
<li><a href="chap_independence.html#sec_proj_bezier" data-scroll="sec_proj_bezier" class="internal">Project: Generating Bézier Curves</a></li>
</ul>
</li>
<li class="link">
<a href="chap_matrix_transformations.html" data-scroll="chap_matrix_transformations" class="internal"><span class="codenumber">7</span> <span class="title">Matrix Transformations</span></a><ul>
<li><a href="chap_matrix_transformations.html#sec_appl_graphics" data-scroll="sec_appl_graphics" class="internal">Application: Computer Graphics</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_intro" data-scroll="sec_mtx_trans_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_prop" data-scroll="sec_mtx_trans_prop" class="internal">Properties of Matrix Transformations</a></li>
<li><a href="chap_matrix_transformations.html#sec_trans_onto_oto" data-scroll="sec_trans_onto_oto" class="internal">Onto and One-to-One Transformations</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_exam" data-scroll="sec_mtx_trans_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_summ" data-scroll="sec_mtx_trans_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_exer" data-scroll="sec_mtx_trans_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_transformations.html#sec_proj_geom_mtx" data-scroll="sec_proj_geom_mtx" class="internal">Project: The Geometry of Matrix Transformations</a></li>
</ul>
</li>
<li class="link part"><a href="part-matrices.html" data-scroll="part-matrices" class="internal"><span class="codenumber">II</span> <span class="title">Matrices</span></a></li>
<li class="link">
<a href="chap_matrix_operations.html" data-scroll="chap_matrix_operations" class="internal"><span class="codenumber">8</span> <span class="title">Matrix Operations</span></a><ul>
<li><a href="chap_matrix_operations.html#sec_appl_mtx_mult" data-scroll="sec_appl_mtx_mult" class="internal">Application: Algorithms for Matrix Multiplication</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_ops_intro" data-scroll="sec_mtx_ops_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_add_smult" data-scroll="sec_mtx_add_smult" class="internal">Properties of Matrix Addition and Multiplication by Scalars</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_prod" data-scroll="sec_mtx_prod" class="internal">A Matrix Product</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_transpose" data-scroll="sec_mtx_transpose" class="internal">The Transpose of a Matrix</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_transpose_prop" data-scroll="sec_mtx_transpose_prop" class="internal">Properties of the Matrix Transpose</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_ops_exam" data-scroll="sec_mtx_ops_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_ops_summ" data-scroll="sec_mtx_ops_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_ops_exer" data-scroll="sec_mtx_ops_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_operations.html#sec_proj_starassen" data-scroll="sec_proj_starassen" class="internal">Project: Strassen's Algorithm and Partitioned Matrices</a></li>
</ul>
</li>
<li class="link">
<a href="chap_intro_eigenvals_eigenvects.html" data-scroll="chap_intro_eigenvals_eigenvects" class="internal"><span class="codenumber">9</span> <span class="title">Introduction to Eigenvalues and Eigenvectors</span></a><ul>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_appl_pagerank" data-scroll="sec_appl_pagerank" class="internal">Application: The Google PageRank Algorithm</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigen_intro" data-scroll="sec_eigen_intro" class="internal">Introduction</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigval_eigvec" data-scroll="sec_eigval_eigvec" class="internal">Eigenvalues and Eigenvectors</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_dynam_sys" data-scroll="sec_dynam_sys" class="internal">Dynamical Systems</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigen_exam" data-scroll="sec_eigen_exam" class="internal">Examples</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigen_summ" data-scroll="sec_eigen_summ" class="internal">Summary</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigen_exer" data-scroll="sec_eigen_exer" class="internal">Exercises</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_proj_pagerank" data-scroll="sec_proj_pagerank" class="internal">Project: Understanding the PageRank Algorithm</a></li>
</ul>
</li>
<li class="link">
<a href="chap_matrix_inverse.html" data-scroll="chap_matrix_inverse" class="internal"><span class="codenumber">10</span> <span class="title">The Inverse of a Matrix</span></a><ul>
<li><a href="chap_matrix_inverse.html#sec_appl_arms_race" data-scroll="sec_appl_arms_race" class="internal">Application: Modeling an Arms Race</a></li>
<li><a href="chap_matrix_inverse.html#sec_inverse_intro" data-scroll="sec_inverse_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_inverse.html#sec_mtx_invertible" data-scroll="sec_mtx_invertible" class="internal">Invertible Matrices</a></li>
<li><a href="chap_matrix_inverse.html#sec_mtx_inverse" data-scroll="sec_mtx_inverse" class="internal">Finding the Inverse of a Matrix</a></li>
<li><a href="chap_matrix_inverse.html#sec_mtx_inverse_props" data-scroll="sec_mtx_inverse_props" class="internal">Properties of the Matrix Inverse</a></li>
<li><a href="chap_matrix_inverse.html#sec_inverse_exam" data-scroll="sec_inverse_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_inverse.html#sec_inverse_summ" data-scroll="sec_inverse_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_inverse.html#sec_inverse_exer" data-scroll="sec_inverse_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_inverse.html#sec_proj_arms_race" data-scroll="sec_proj_arms_race" class="internal">Project: The Richardson Arms Race Model</a></li>
</ul>
</li>
<li class="link">
<a href="chap_IMT.html" data-scroll="chap_IMT" class="internal"><span class="codenumber">11</span> <span class="title">The Invertible Matrix Theorem</span></a><ul>
<li><a href="chap_IMT.html#sec_imt_intro" data-scroll="sec_imt_intro" class="internal">Introduction</a></li>
<li><a href="chap_IMT.html#sec_imt" data-scroll="sec_imt" class="internal">The Invertible Matrix Theorem</a></li>
<li><a href="chap_IMT.html#sec_imt_exam" data-scroll="sec_imt_exam" class="internal">Examples</a></li>
<li><a href="chap_IMT.html#sec_imt_summ" data-scroll="sec_imt_summ" class="internal">Summary</a></li>
<li><a href="chap_IMT.html#sec_imt_exer" data-scroll="sec_imt_exer" class="internal">Exercises</a></li>
</ul>
</li>
<li class="link part"><a href="part-vector-rn.html" data-scroll="part-vector-rn" class="internal"><span class="codenumber">III</span> <span class="title">The Vector Space <span class="process-math">\(\R^n\)</span></span></a></li>
<li class="link">
<a href="chap_R_n.html" data-scroll="chap_R_n" class="internal"><span class="codenumber">12</span> <span class="title">The Structure of <span class="process-math">\(\R^n\)</span></span></a><ul>
<li><a href="chap_R_n.html#sec_appl_romania" data-scroll="sec_appl_romania" class="internal">Application: Connecting GDP and Consumption in Romania</a></li>
<li><a href="chap_R_n.html#sec_rn_intro" data-scroll="sec_rn_intro" class="internal">Introduction</a></li>
<li><a href="chap_R_n.html#sec_vec_spaces" data-scroll="sec_vec_spaces" class="internal">Vector Spaces</a></li>
<li><a href="chap_R_n.html#sec_sub_space_span" data-scroll="sec_sub_space_span" class="internal">The Subspace Spanned by a Set of Vectors</a></li>
<li><a href="chap_R_n.html#sec_rn_exam" data-scroll="sec_rn_exam" class="internal">Examples</a></li>
<li><a href="chap_R_n.html#sec_rn_summ" data-scroll="sec_rn_summ" class="internal">Summary</a></li>
<li><a href="chap_R_n.html#sec_rn_exer" data-scroll="sec_rn_exer" class="internal">Exercises</a></li>
<li><a href="chap_R_n.html#sec_proj_ls_approx" data-scroll="sec_proj_ls_approx" class="internal">Project: Least Squares Linear Approximation</a></li>
</ul>
</li>
<li class="link">
<a href="chap_null_space.html" data-scroll="chap_null_space" class="internal"><span class="codenumber">13</span> <span class="title">The Null Space and Column Space of a Matrix</span></a><ul>
<li><a href="chap_null_space.html#sec_appl_lights_out" data-scroll="sec_appl_lights_out" class="internal">Application: The Lights Out Game</a></li>
<li><a href="chap_null_space.html#sec_null_intro" data-scroll="sec_null_intro" class="internal">Introduction</a></li>
<li><a href="chap_null_space.html#sec_null_kernel" data-scroll="sec_null_kernel" class="internal">The Null Space of a Matrix and the Kernel of a Matrix Transformation</a></li>
<li><a href="chap_null_space.html#sec_column_range" data-scroll="sec_column_range" class="internal">The Column Space of a Matrix and the Range of a Matrix Transformation</a></li>
<li><a href="chap_null_space.html#sec_row_space" data-scroll="sec_row_space" class="internal">The Row Space of a Matrix</a></li>
<li><a href="chap_null_space.html#sec_null_col_base" data-scroll="sec_null_col_base" class="internal">Bases for <span class="process-math">\(\Nul A\)</span> and <span class="process-math">\(\Col A\)</span></a></li>
<li><a href="chap_null_space.html#sec_null_exam" data-scroll="sec_null_exam" class="internal">Examples</a></li>
<li><a href="chap_null_space.html#sec_null_summ" data-scroll="sec_null_summ" class="internal">Summary</a></li>
<li><a href="chap_null_space.html#sec_null_exer" data-scroll="sec_null_exer" class="internal">Exercises</a></li>
<li><a href="chap_null_space.html#sec_proj_lights_out" data-scroll="sec_proj_lights_out" class="internal">Project: Solving the Lights Out Game</a></li>
</ul>
</li>
<li class="link">
<a href="chap_eigenspaces.html" data-scroll="chap_eigenspaces" class="internal"><span class="codenumber">14</span> <span class="title">Eigenspaces of a Matrix</span></a><ul>
<li><a href="chap_eigenspaces.html#sec_appl_pop_dynam" data-scroll="sec_appl_pop_dynam" class="internal">Application: Population Dynamics</a></li>
<li><a href="chap_eigenspaces.html#sec_egspace_intro" data-scroll="sec_egspace_intro" class="internal">Introduction</a></li>
<li><a href="chap_eigenspaces.html#sec_mtx_egspace" data-scroll="sec_mtx_egspace" class="internal">Eigenspaces of Matrix</a></li>
<li><a href="chap_eigenspaces.html#sec_lin_ind_egvec" data-scroll="sec_lin_ind_egvec" class="internal">Linearly Independent Eigenvectors</a></li>
<li><a href="chap_eigenspaces.html#sec_egspace_exam" data-scroll="sec_egspace_exam" class="internal">Examples</a></li>
<li><a href="chap_eigenspaces.html#sec_egspace_summ" data-scroll="sec_egspace_summ" class="internal">Summary</a></li>
<li><a href="chap_eigenspaces.html#sec_egspace_exer" data-scroll="sec_egspace_exer" class="internal">Exercises</a></li>
<li><a href="chap_eigenspaces.html#sec_proj_migration" data-scroll="sec_proj_migration" class="internal">Project: Modeling Population Migration</a></li>
</ul>
</li>
<li class="link">
<a href="chap_bases_dimension.html" data-scroll="chap_bases_dimension" class="internal"><span class="codenumber">15</span> <span class="title">Bases and Dimension</span></a><ul>
<li><a href="chap_bases_dimension.html#sec_appl_latt_crypt" data-scroll="sec_appl_latt_crypt" class="internal">Application: Lattice Based Cryptography</a></li>
<li><a href="chap_bases_dimension.html#sec_base_dim_intro" data-scroll="sec_base_dim_intro" class="internal">Introduction</a></li>
<li><a href="chap_bases_dimension.html#sec_dim_sub_rn" data-scroll="sec_dim_sub_rn" class="internal">The Dimension of a Subspace of <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_bases_dimension.html#sec_cond_basis_subspace" data-scroll="sec_cond_basis_subspace" class="internal">Conditions for a Basis of a Subspace of <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_bases_dimension.html#sec_find_basis_subspace" data-scroll="sec_find_basis_subspace" class="internal">Finding a Basis for a Subspace</a></li>
<li><a href="chap_bases_dimension.html#sec_mtx_rank" data-scroll="sec_mtx_rank" class="internal">Rank of a Matrix</a></li>
<li><a href="chap_bases_dimension.html#sec_base_dim_exam" data-scroll="sec_base_dim_exam" class="internal">Examples</a></li>
<li><a href="chap_bases_dimension.html#sec_base_dim_summ" data-scroll="sec_base_dim_summ" class="internal">Summary</a></li>
<li><a href="chap_bases_dimension.html#sec_base_dim_exer" data-scroll="sec_base_dim_exer" class="internal">Exercises</a></li>
<li><a href="chap_bases_dimension.html#sec_proj_ggh_crypto" data-scroll="sec_proj_ggh_crypto" class="internal">Project: The GGH Cryptosystem</a></li>
</ul>
</li>
<li class="link">
<a href="chap_coordinate_vectors.html" data-scroll="chap_coordinate_vectors" class="internal"><span class="codenumber">16</span> <span class="title">Coordinate Vectors and Change of Basis</span></a><ul>
<li><a href="chap_coordinate_vectors.html#sec_appl_orbits" data-scroll="sec_appl_orbits" class="internal">Application: Describing Orbits of Planets</a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_intro" data-scroll="sec_cob_intro" class="internal">Introduction</a></li>
<li><a href="chap_coordinate_vectors.html#sec_coor_base" data-scroll="sec_coor_base" class="internal">Bases as Coordinate Systems in <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_rn" data-scroll="sec_cob_rn" class="internal">Change of Basis in <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_coordinate_vectors.html#sec_mtx_cob" data-scroll="sec_mtx_cob" class="internal">The Change of Basis Matrix in <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_coordinate_vectors.html#sec_prop_mtx_cob" data-scroll="sec_prop_mtx_cob" class="internal">Properties of the Change of Basis Matrix</a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_exam" data-scroll="sec_cob_exam" class="internal">Examples</a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_summ" data-scroll="sec_cob_summ" class="internal">Summary</a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_exer" data-scroll="sec_cob_exer" class="internal">Exercises</a></li>
<li><a href="chap_coordinate_vectors.html#sec_proj_orbits_cob" data-scroll="sec_proj_orbits_cob" class="internal">Project: Planetary Orbits and Change of Basis</a></li>
</ul>
</li>
<li class="link part"><a href="part-eigen.html" data-scroll="part-eigen" class="internal"><span class="codenumber">IV</span> <span class="title">Eigenvalues and Eigenvectors</span></a></li>
<li class="link">
<a href="chap_determinants.html" data-scroll="chap_determinants" class="internal"><span class="codenumber">17</span> <span class="title">The Determinant</span></a><ul>
<li><a href="chap_determinants.html#sec_appl_area_vol" data-scroll="sec_appl_area_vol" class="internal">Application: Area and Volume</a></li>
<li><a href="chap_determinants.html#sec_det_intro" data-scroll="sec_det_intro" class="internal">Introduction</a></li>
<li><a href="chap_determinants.html#sec_det_square" data-scroll="sec_det_square" class="internal">The Determinant of a Square Matrix</a></li>
<li><a href="chap_determinants.html#sec_cofactors" data-scroll="sec_cofactors" class="internal">Cofactors</a></li>
<li><a href="chap_determinants.html#sec_det_3by3" data-scroll="sec_det_3by3" class="internal">The Determinant of a <span class="process-math">\(3 \times 3\)</span> Matrix</a></li>
<li><a href="chap_determinants.html#sec_det_remember" data-scroll="sec_det_remember" class="internal">Two Devices for Remembering Determinants</a></li>
<li><a href="chap_determinants.html#sec_det_exam" data-scroll="sec_det_exam" class="internal">Examples</a></li>
<li><a href="chap_determinants.html#sec_det_summ" data-scroll="sec_det_summ" class="internal">Summary</a></li>
<li><a href="chap_determinants.html#sec_det_exer" data-scroll="sec_det_exer" class="internal">Exercises</a></li>
<li><a href="chap_determinants.html#sec_proj_det_area_vol" data-scroll="sec_proj_det_area_vol" class="internal">Project: Area and Volume Using Determinants</a></li>
</ul>
</li>
<li class="link">
<a href="chap_characteristic_equation.html" data-scroll="chap_characteristic_equation" class="internal"><span class="codenumber">18</span> <span class="title">The Characteristic Equation</span></a><ul>
<li><a href="chap_characteristic_equation.html#sec_appl_thermo" data-scroll="sec_appl_thermo" class="internal">Application: Modeling the Second Law of Thermodynamics</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq_intro" data-scroll="sec_chareq_intro" class="internal">Introduction</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq" data-scroll="sec_chareq" class="internal">The Characteristic Equation</a></li>
<li><a href="chap_characteristic_equation.html#sec_egspace_geom" data-scroll="sec_egspace_geom" class="internal">Eigenspaces, A Geometric Example</a></li>
<li><a href="chap_characteristic_equation.html#sec_egspace_dims" data-scroll="sec_egspace_dims" class="internal">Dimensions of Eigenspaces</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq_exam" data-scroll="sec_chareq_exam" class="internal">Examples</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq_summ" data-scroll="sec_chareq_summ" class="internal">Summary</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq_exer" data-scroll="sec_chareq_exer" class="internal">Exercises</a></li>
<li><a href="chap_characteristic_equation.html#sec_proj_ehrenfest" data-scroll="sec_proj_ehrenfest" class="internal">Project: The Ehrenfest Model</a></li>
</ul>
</li>
<li class="link">
<a href="chap_diagonalization.html" data-scroll="chap_diagonalization" class="internal"><span class="codenumber">19</span> <span class="title">Diagonalization</span></a><ul>
<li><a href="chap_diagonalization.html#sec_appl_fib_num" data-scroll="sec_appl_fib_num" class="internal">Application: The Fibonacci Numbers</a></li>
<li><a href="chap_diagonalization.html#sec_diag_intro" data-scroll="sec_diag_intro" class="internal">Introduction</a></li>
<li><a href="chap_diagonalization.html#sec_diag" data-scroll="sec_diag" class="internal">Diagonalization</a></li>
<li><a href="chap_diagonalization.html#sec_mtx_similar" data-scroll="sec_mtx_similar" class="internal">Similar Matrices</a></li>
<li><a href="chap_diagonalization.html#sec_sim_mtx_trans" data-scroll="sec_sim_mtx_trans" class="internal">Similarity and Matrix Transformations</a></li>
<li><a href="chap_diagonalization.html#sec_diag_general" data-scroll="sec_diag_general" class="internal">Diagonalization in General</a></li>
<li><a href="chap_diagonalization.html#sec_diag_exam" data-scroll="sec_diag_exam" class="internal">Examples</a></li>
<li><a href="chap_diagonalization.html#sec_diag_summ" data-scroll="sec_diag_summ" class="internal">Summary</a></li>
<li><a href="chap_diagonalization.html#sec_diag_exer" data-scroll="sec_diag_exer" class="internal">Exercises</a></li>
<li><a href="chap_diagonalization.html#sec_proj_binet_fibo" data-scroll="sec_proj_binet_fibo" class="internal">Project: Binet's Formula for the Fibonacci Numbers</a></li>
</ul>
</li>
<li class="link">
<a href="chap_approx_eigenvalues.html" data-scroll="chap_approx_eigenvalues" class="internal"><span class="codenumber">20</span> <span class="title">Approximating Eigenvalues and Eigenvectors</span></a><ul>
<li><a href="chap_approx_eigenvalues.html#sec_appl_leslie_mtx" data-scroll="sec_appl_leslie_mtx" class="internal">Application: Leslie Matrices and Population Modeling</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_app_eigen_intro" data-scroll="sec_app_eigen_intro" class="internal">Introduction</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_power_method" data-scroll="sec_power_method" class="internal">The Power Method</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_power_method_inv" data-scroll="sec_power_method_inv" class="internal">The Inverse Power Method</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_app_eigen_exam" data-scroll="sec_app_eigen_exam" class="internal">Examples</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_app_eigen_summ" data-scroll="sec_app_eigen_summ" class="internal">Summary</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_app_eigen_exer" data-scroll="sec_app_eigen_exer" class="internal">Exercises</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_proj_sheep_herd" data-scroll="sec_proj_sheep_herd" class="internal">Project: Managing a Sheep Herd</a></li>
</ul>
</li>
<li class="link">
<a href="chap_complex_eigenvalues.html" data-scroll="chap_complex_eigenvalues" class="internal"><span class="codenumber">21</span> <span class="title">Complex Eigenvalues</span></a><ul>
<li><a href="chap_complex_eigenvalues.html#sec_appl_gershgorin" data-scroll="sec_appl_gershgorin" class="internal">Application: The Gershgorin Disk Theorem</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen_intro" data-scroll="sec_comp_eigen_intro" class="internal">Introduction</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen" data-scroll="sec_comp_eigen" class="internal">Complex Eigenvalues</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_mtx_rotate_scale" data-scroll="sec_mtx_rotate_scale" class="internal">Rotation and Scaling Matrices</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_mtx_comp_eigen" data-scroll="sec_mtx_comp_eigen" class="internal">Matrices with Complex Eigenvalues</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen_exam" data-scroll="sec_comp_eigen_exam" class="internal">Examples</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen_summ" data-scroll="sec_comp_eigen_summ" class="internal">Summary</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen_exer" data-scroll="sec_comp_eigen_exer" class="internal">Exercises</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_proj_gershgorin" data-scroll="sec_proj_gershgorin" class="internal">Project: Understanding the Gershgorin Disk Theorem</a></li>
</ul>
</li>
<li class="link">
<a href="chap_det_properties.html" data-scroll="chap_det_properties" class="internal"><span class="codenumber">22</span> <span class="title">Properties of Determinants</span></a><ul>
<li><a href="chap_det_properties.html#sec_det_prop_intro" data-scroll="sec_det_prop_intro" class="internal">Introduction</a></li>
<li><a href="chap_det_properties.html#sec_det_row_ops" data-scroll="sec_det_row_ops" class="internal">Elementary Row Operations and Their Effects on the Determinant</a></li>
<li><a href="chap_det_properties.html#sec_mtx_elem" data-scroll="sec_mtx_elem" class="internal">Elementary Matrices</a></li>
<li><a href="chap_det_properties.html#sec_det_geom" data-scroll="sec_det_geom" class="internal">Geometric Interpretation of the Determinant</a></li>
<li><a href="chap_det_properties.html#sec_inv_cramers" data-scroll="sec_inv_cramers" class="internal">An Explicit Formula for the Inverse and Cramer's Rule</a></li>
<li><a href="chap_det_properties.html#sec_det_transpose" data-scroll="sec_det_transpose" class="internal">The Determinant of the Transpose</a></li>
<li><a href="chap_det_properties.html#sec_det_row_swap" data-scroll="sec_det_row_swap" class="internal">Row Swaps and Determinants</a></li>
<li><a href="chap_det_properties.html#sec_cofactor_expand" data-scroll="sec_cofactor_expand" class="internal">Cofactor Expansions</a></li>
<li><a href="chap_det_properties.html#sec_mtx_lu_factor" data-scroll="sec_mtx_lu_factor" class="internal">The LU Factorization of a Matrix</a></li>
<li><a href="chap_det_properties.html#sec_det_prop_exam" data-scroll="sec_det_prop_exam" class="internal">Examples</a></li>
<li><a href="chap_det_properties.html#sec_det_prop_summ" data-scroll="sec_det_prop_summ" class="internal">Summary</a></li>
<li><a href="chap_det_properties.html#sec_det_prop_exer" data-scroll="sec_det_prop_exer" class="internal">Exercises</a></li>
</ul>
</li>
<li class="link part"><a href="part-orthog.html" data-scroll="part-orthog" class="internal"><span class="codenumber">V</span> <span class="title">Orthogonality</span></a></li>
<li class="link">
<a href="chap_dot_product.html" data-scroll="chap_dot_product" class="internal"><span class="codenumber">23</span> <span class="title">The Dot Product in <span class="process-math">\(\R^n\)</span></span></a><ul>
<li><a href="chap_dot_product.html#sec_appl_figs_computer" data-scroll="sec_appl_figs_computer" class="internal">Application: Hidden Figures in Computer Graphics</a></li>
<li><a href="chap_dot_product.html#sec_dot_prod_intro" data-scroll="sec_dot_prod_intro" class="internal">Introduction</a></li>
<li><a href="chap_dot_product.html#sec_dist_vec" data-scroll="sec_dist_vec" class="internal">The Distance Between Vectors</a></li>
<li><a href="chap_dot_product.html#sec_angle_vec" data-scroll="sec_angle_vec" class="internal">The Angle Between Two Vectors</a></li>
<li><a href="chap_dot_product.html#sec_orthog_proj" data-scroll="sec_orthog_proj" class="internal">Orthogonal Projections</a></li>
<li><a href="chap_dot_product.html#sec_orthog_comp" data-scroll="sec_orthog_comp" class="internal">Orthogonal Complements</a></li>
<li><a href="chap_dot_product.html#sec_dot_prod_exam" data-scroll="sec_dot_prod_exam" class="internal">Examples</a></li>
<li><a href="chap_dot_product.html#sec_dot_prod_summ" data-scroll="sec_dot_prod_summ" class="internal">Summary</a></li>
<li><a href="chap_dot_product.html#sec_dot_prod_exer" data-scroll="sec_dot_prod_exer" class="internal">Exercises</a></li>
<li><a href="chap_dot_product.html#sec_proj_back_face" data-scroll="sec_proj_back_face" class="internal">Project: Back-Face Culling</a></li>
</ul>
</li>
<li class="link">
<a href="chap_orthogonal_basis.html" data-scroll="chap_orthogonal_basis" class="internal"><span class="codenumber">24</span> <span class="title">Orthogonal and Orthonormal Bases in <span class="process-math">\(\R^n\)</span></span></a><ul>
<li><a href="chap_orthogonal_basis.html#sec_appl_3d_rotate" data-scroll="sec_appl_3d_rotate" class="internal">Application: Rotations in 3D</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_set_intro" data-scroll="sec_orthog_set_intro" class="internal">Introduction</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_sets" data-scroll="sec_orthog_sets" class="internal">Orthogonal Sets</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_bases_prop" data-scroll="sec_orthog_bases_prop" class="internal">Properties of Orthogonal Bases</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthon_bases" data-scroll="sec_orthon_bases" class="internal">Orthonormal Bases</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_mtx" data-scroll="sec_orthog_mtx" class="internal">Orthogonal Matrices</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_set_exam" data-scroll="sec_orthog_set_exam" class="internal">Examples</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_set_summ" data-scroll="sec_orthog_set_summ" class="internal">Summary</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_set_exer" data-scroll="sec_orthog_set_exer" class="internal">Exercises</a></li>
<li><a href="chap_orthogonal_basis.html#sec_proj_3d_rotate" data-scroll="sec_proj_3d_rotate" class="internal">Project: Understanding Rotations in 3-Space</a></li>
</ul>
</li>
<li class="link">
<a href="chap_gram_schmidt.html" data-scroll="chap_gram_schmidt" class="internal"><span class="codenumber">25</span> <span class="title">Projections onto Subspaces and the Gram-Schmidt Process in <span class="process-math">\(\R^n\)</span></span></a><ul>
<li><a href="chap_gram_schmidt.html#sec_mimo" data-scroll="sec_mimo" class="internal">Application: MIMO Systems</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_intro_noip" data-scroll="sec_gram_schmidt_intro_noip" class="internal">Introduction</a></li>
<li><a href="chap_gram_schmidt.html#sec_proj_subsp_orth" data-scroll="sec_proj_subsp_orth" class="internal">Projections onto Subspaces and Orthogonal Projections</a></li>
<li><a href="chap_gram_schmidt.html#sec_best_approx" data-scroll="sec_best_approx" class="internal">Best Approximations</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_process" data-scroll="sec_gram_schmidt_process" class="internal">The Gram-Schmidt Process</a></li>
<li><a href="chap_gram_schmidt.html#sec_qr_fact" data-scroll="sec_qr_fact" class="internal">The QR Factorization of a Matrix</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_examples" data-scroll="sec_gram_schmidt_examples" class="internal">Examples</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_summ_noips" data-scroll="sec_gram_schmidt_summ_noips" class="internal">Summary</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_exercises" data-scroll="sec_gram_schmidt_exercises" class="internal">Exercises</a></li>
<li><a href="chap_gram_schmidt.html#sec_project_mimo" data-scroll="sec_project_mimo" class="internal">Project: MIMO Systems and Householder Transformations</a></li>
</ul>
</li>
<li class="link active">
<a href="chap_least_squares.html" data-scroll="chap_least_squares" class="internal"><span class="codenumber">26</span> <span class="title">Least Squares Approximations</span></a><ul>
<li><a href="chap_least_squares.html#sec_appl_fit_func" data-scroll="sec_appl_fit_func" class="internal">Application: Fitting Functions to Data</a></li>
<li><a href="chap_least_squares.html#sec_ls_intro" data-scroll="sec_ls_intro" class="internal">Introduction</a></li>
<li><a href="chap_least_squares.html#sec_ls_approx" data-scroll="sec_ls_approx" class="internal">Least Squares Approximations</a></li>
<li><a href="chap_least_squares.html#sec_ls_exam" data-scroll="sec_ls_exam" class="internal">Examples</a></li>
<li><a href="chap_least_squares.html#sec_ls_summ" data-scroll="sec_ls_summ" class="internal">Summary</a></li>
<li><a href="chap_least_squares.html#sec_ls_exer" data-scroll="sec_ls_exer" class="internal">Exercises</a></li>
<li><a href="chap_least_squares.html#sec_proj_ls_approx_other" data-scroll="sec_proj_ls_approx_other" class="internal">Project: Other Least Squares Approximations</a></li>
</ul>
</li>
<li class="link part"><a href="part-app-orthog.html" data-scroll="part-app-orthog" class="internal"><span class="codenumber">VI</span> <span class="title">Applications of Orthogonality</span></a></li>
<li class="link">
<a href="chap_orthogonal_diagonalization.html" data-scroll="chap_orthogonal_diagonalization" class="internal"><span class="codenumber">27</span> <span class="title">Orthogonal Diagonalization</span></a><ul>
<li><a href="chap_orthogonal_diagonalization.html#sec_appl_mulit_2nd_deriv" data-scroll="sec_appl_mulit_2nd_deriv" class="internal">Application: The Multivariable Second Derivative Test</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_orthog_diag_intro" data-scroll="sec_orthog_diag_intro" class="internal">Introduction</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_mtx_symm" data-scroll="sec_mtx_symm" class="internal">Symmetric Matrices</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_spec_decomp_symm_mtx" data-scroll="sec_spec_decomp_symm_mtx" class="internal">The Spectral Decomposition of a Symmetric Matrix <span class="process-math">\(A\)</span></a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_orthog_diag_exam" data-scroll="sec_orthog_diag_exam" class="internal">Examples</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_orthog_diag_summ" data-scroll="sec_orthog_diag_summ" class="internal">Summary</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_orthog_diag_exer" data-scroll="sec_orthog_diag_exer" class="internal">Exercises</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_proj_two_var_deriv" data-scroll="sec_proj_two_var_deriv" class="internal">Project: The Second Derivative Test for Functions of Two Variables</a></li>
</ul>
</li>
<li class="link">
<a href="chap_principal_axis_theorem.html" data-scroll="chap_principal_axis_theorem" class="internal"><span class="codenumber">28</span> <span class="title">Quadratic Forms and the Principal Axis Theorem</span></a><ul>
<li><a href="chap_principal_axis_theorem.html#sec_appl_tennis" data-scroll="sec_appl_tennis" class="internal">Application: The Tennis Racket Effect</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_intro" data-scroll="sec_pat_intro" class="internal">Introduction</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_eqs_quad_r2" data-scroll="sec_eqs_quad_r2" class="internal">Equations Involving Quadratic Forms in <span class="process-math">\(\R^2\)</span></a></li>
<li><a href="chap_principal_axis_theorem.html#sec_class_quad_forms" data-scroll="sec_class_quad_forms" class="internal">Classifying Quadratic Forms</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_inner_prod" data-scroll="sec_pat_inner_prod" class="internal">Inner Products</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_exam" data-scroll="sec_pat_exam" class="internal">Examples</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_summ" data-scroll="sec_pat_summ" class="internal">Summary</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_exer" data-scroll="sec_pat_exer" class="internal">Exercises</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_proj_tennis" data-scroll="sec_proj_tennis" class="internal">Project: The Tennis Racket Theorem</a></li>
</ul>
</li>
<li class="link">
<a href="chap_SVD.html" data-scroll="chap_SVD" class="internal"><span class="codenumber">29</span> <span class="title">The Singular Value Decomposition</span></a><ul>
<li><a href="chap_SVD.html#sec_appl_search_engn" data-scroll="sec_appl_search_engn" class="internal">Application: Search Engines and Semantics</a></li>
<li><a href="chap_SVD.html#sec_svd_intro" data-scroll="sec_svd_intro" class="internal">Introduction</a></li>
<li><a href="chap_SVD.html#sec_mtx_op_norm" data-scroll="sec_mtx_op_norm" class="internal">The Operator Norm of a Matrix</a></li>
<li><a href="chap_SVD.html#sec_svd" data-scroll="sec_svd" class="internal">The SVD</a></li>
<li><a href="chap_SVD.html#sec_svd_mtx_spaces" data-scroll="sec_svd_mtx_spaces" class="internal">SVD and the Null, Column, and Row Spaces of a Matrix</a></li>
<li><a href="chap_SVD.html#sec_svd_exam" data-scroll="sec_svd_exam" class="internal">Examples</a></li>
<li><a href="chap_SVD.html#sec_svd_summ" data-scroll="sec_svd_summ" class="internal">Summary</a></li>
<li><a href="chap_SVD.html#sec_svd_exer" data-scroll="sec_svd_exer" class="internal">Exercises</a></li>
<li><a href="chap_SVD.html#sec_proj_indexing" data-scroll="sec_proj_indexing" class="internal">Project: Latent Semantic Indexing</a></li>
</ul>
</li>
<li class="link">
<a href="chap_pseudoinverses.html" data-scroll="chap_pseudoinverses" class="internal"><span class="codenumber">30</span> <span class="title">Using the Singular Value Decomposition</span></a><ul>
<li><a href="chap_pseudoinverses.html#sec_appl_gps" data-scroll="sec_appl_gps" class="internal">Application: Global Positioning System</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudo_intro" data-scroll="sec_pseudo_intro" class="internal">Introduction</a></li>
<li><a href="chap_pseudoinverses.html#sec_img_conpress" data-scroll="sec_img_conpress" class="internal">Image Compression</a></li>
<li><a href="chap_pseudoinverses.html#sec_err_approx_img" data-scroll="sec_err_approx_img" class="internal">Calculating the Error in Approximating an Image</a></li>
<li><a href="chap_pseudoinverses.html#sec_mtx_cond_num" data-scroll="sec_mtx_cond_num" class="internal">The Condition Number of a Matrix</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudoinverses" data-scroll="sec_pseudoinverses" class="internal">Pseudoinverses</a></li>
<li><a href="chap_pseudoinverses.html#sec_ls_approx_SVD" data-scroll="sec_ls_approx_SVD" class="internal">Least Squares Approximations</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudo_exam" data-scroll="sec_pseudo_exam" class="internal">Examples</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudo_summ" data-scroll="sec_pseudo_summ" class="internal">Summary</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudo_exer" data-scroll="sec_pseudo_exer" class="internal">Exercises</a></li>
<li><a href="chap_pseudoinverses.html#sec_proj_gps" data-scroll="sec_proj_gps" class="internal">Project: GPS and Least Squares</a></li>
</ul>
</li>
<li class="link part"><a href="part-vec-spaces.html" data-scroll="part-vec-spaces" class="internal"><span class="codenumber">VII</span> <span class="title">Vector Spaces</span></a></li>
<li class="link">
<a href="chap_vector_spaces.html" data-scroll="chap_vector_spaces" class="internal"><span class="codenumber">31</span> <span class="title">Vector Spaces</span></a><ul>
<li><a href="chap_vector_spaces.html#sec_appl_hat_puzzle" data-scroll="sec_appl_hat_puzzle" class="internal">Application: The Hat Puzzle</a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space_intro" data-scroll="sec_vec_space_intro" class="internal">Introduction</a></li>
<li><a href="chap_vector_spaces.html#sec_space_like_rn" data-scroll="sec_space_like_rn" class="internal">Spaces with Similar Structure to <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space" data-scroll="sec_vec_space" class="internal">Vector Spaces</a></li>
<li><a href="chap_vector_spaces.html#sec_subspaces" data-scroll="sec_subspaces" class="internal">Subspaces</a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space_exam" data-scroll="sec_vec_space_exam" class="internal">Examples</a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space_summ" data-scroll="sec_vec_space_summ" class="internal">Summary</a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space_exer" data-scroll="sec_vec_space_exer" class="internal">Exercises</a></li>
<li><a href="chap_vector_spaces.html#sec_proj_hamming_hat_puzzle" data-scroll="sec_proj_hamming_hat_puzzle" class="internal">Project: Hamming Codes and the Hat Puzzle</a></li>
</ul>
</li>
<li class="link">
<a href="chap_bases.html" data-scroll="chap_bases" class="internal"><span class="codenumber">32</span> <span class="title">Bases for Vector Spaces</span></a><ul>
<li><a href="chap_bases.html#sec_img_compress" data-scroll="sec_img_compress" class="internal">Application: Image Compression</a></li>
<li><a href="chap_bases.html#sec_bases_intro" data-scroll="sec_bases_intro" class="internal">Introduction</a></li>
<li><a href="chap_bases.html#sec_lin_indep" data-scroll="sec_lin_indep" class="internal">Linear Independence</a></li>
<li><a href="chap_bases.html#sec_bases" data-scroll="sec_bases" class="internal">Bases</a></li>
<li><a href="chap_bases.html#sec_basis_vec_space" data-scroll="sec_basis_vec_space" class="internal">Finding a Basis for a Vector Space</a></li>
<li><a href="chap_bases.html#sec_bases_exam" data-scroll="sec_bases_exam" class="internal">Examples</a></li>
<li><a href="chap_bases.html#sec_bases_summ" data-scroll="sec_bases_summ" class="internal">Summary</a></li>
<li><a href="chap_bases.html#sec_bases_exer" data-scroll="sec_bases_exer" class="internal">Exercises</a></li>
<li><a href="chap_bases.html#sec_proj_img_compress" data-scroll="sec_proj_img_compress" class="internal">Project: Image Compression with Wavelets</a></li>
</ul>
</li>
<li class="link">
<a href="chap_dimension.html" data-scroll="chap_dimension" class="internal"><span class="codenumber">33</span> <span class="title">The Dimension of a Vector Space</span></a><ul>
<li><a href="chap_dimension.html#sec_appl_pca" data-scroll="sec_appl_pca" class="internal">Application: Principal Component Analysis</a></li>
<li><a href="chap_dimension.html#sec_dims_intro" data-scroll="sec_dims_intro" class="internal">Introduction</a></li>
<li><a href="chap_dimension.html#sec_finite_dim_space" data-scroll="sec_finite_dim_space" class="internal">Finite Dimensional Vector Spaces</a></li>
<li><a href="chap_dimension.html#sec_dim_subspace" data-scroll="sec_dim_subspace" class="internal">The Dimension of a Subspace</a></li>
<li><a href="chap_dimension.html#sec_cond_basis_vec_space" data-scroll="sec_cond_basis_vec_space" class="internal">Conditions for a Basis of a Vector Space</a></li>
<li><a href="chap_dimension.html#sec_dims_exam" data-scroll="sec_dims_exam" class="internal">Examples</a></li>
<li><a href="chap_dimension.html#sec_dims_summ" data-scroll="sec_dims_summ" class="internal">Summary</a></li>
<li><a href="chap_dimension.html#sec_dims_exer" data-scroll="sec_dims_exer" class="internal">Exercises</a></li>
<li><a href="chap_dimension.html#sec_proj_pca" data-scroll="sec_proj_pca" class="internal">Project: Understanding Principal Component Analysis</a></li>
</ul>
</li>
<li class="link">
<a href="chap_coordinate_vectors_vector_spaces.html" data-scroll="chap_coordinate_vectors_vector_spaces" class="internal"><span class="codenumber">34</span> <span class="title">Coordinate Vectors and Coordinate Transformations</span></a><ul>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_appl_sums" data-scroll="sec_appl_sums" class="internal">Application: Calculating Sums</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coor_vec_intro" data-scroll="sec_coor_vec_intro" class="internal">Introduction</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coord_trans" data-scroll="sec_coord_trans" class="internal">The Coordinate Transformation</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coord_vec_exam" data-scroll="sec_coord_vec_exam" class="internal">Examples</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coord_vec_summ" data-scroll="sec_coord_vec_summ" class="internal">Summary</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coord_vec_exer" data-scroll="sec_coord_vec_exer" class="internal">Exercises</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_proj_sum_powers" data-scroll="sec_proj_sum_powers" class="internal">Project: Finding Formulas for Sums of Powers</a></li>
</ul>
</li>
<li class="link">
<a href="chap_inner_products.html" data-scroll="chap_inner_products" class="internal"><span class="codenumber">35</span> <span class="title">Inner Product Spaces</span></a><ul>
<li><a href="chap_inner_products.html#sec_appl_fourier" data-scroll="sec_appl_fourier" class="internal">Application: Fourier Series</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_intro" data-scroll="sec_inner_prod_intro" class="internal">Introduction</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_spaces" data-scroll="sec_inner_prod_spaces" class="internal">Inner Product Spaces</a></li>
<li><a href="chap_inner_products.html#sec_vec_length" data-scroll="sec_vec_length" class="internal">The Length of a Vector</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_orthog" data-scroll="sec_inner_prod_orthog" class="internal">Orthogonality in Inner Product Spaces</a></li>
<li><a href="chap_inner_products.html#sec_inner_prog_orthog_bases" data-scroll="sec_inner_prog_orthog_bases" class="internal">Orthogonal and Orthonormal Bases in Inner Product Spaces</a></li>
<li><a href="chap_inner_products.html#sec_orthog_proj_subspace" data-scroll="sec_orthog_proj_subspace" class="internal">Orthogonal Projections onto Subspaces</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_approx" data-scroll="sec_inner_prod_approx" class="internal">Best Approximations in Inner Product Spaces</a></li>
<li><a href="chap_inner_products.html#sec_orthog_comp_ip" data-scroll="sec_orthog_comp_ip" class="internal">Orthogonal Complements</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_exam" data-scroll="sec_inner_prod_exam" class="internal">Examples</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_summ" data-scroll="sec_inner_prod_summ" class="internal">Summary</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_exer" data-scroll="sec_inner_prod_exer" class="internal">Exercises</a></li>
<li><a href="chap_inner_products.html#sec_proj_fourier" data-scroll="sec_proj_fourier" class="internal">Project: Fourier Series and Musical Tones</a></li>
</ul>
</li>
<li class="link">
<a href="chap_gram_schmidt_ips.html" data-scroll="chap_gram_schmidt_ips" class="internal"><span class="codenumber">36</span> <span class="title">The Gram-Schmidt Process in Inner Product Spaces</span></a><ul>
<li><a href="chap_gram_schmidt_ips.html#sec_appl_gaussian_quad" data-scroll="sec_appl_gaussian_quad" class="internal">Application: Gaussian Quadrature</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_intro" data-scroll="sec_gram_schmidt_intro" class="internal">Introduction</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_inner_prod" data-scroll="sec_gram_schmidt_inner_prod" class="internal">The Gram-Schmidt Process using Inner Products</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_exam" data-scroll="sec_gram_schmidt_exam" class="internal">Examples</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_summ_ips" data-scroll="sec_gram_schmidt_summ_ips" class="internal">Summary</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_exer" data-scroll="sec_gram_schmidt_exer" class="internal">Exercises</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_proj_gaussian_quad" data-scroll="sec_proj_gaussian_quad" class="internal">Project: Gaussian Quadrature and Legendre Polynomials</a></li>
</ul>
</li>
<li class="link part"><a href="part-lin-trans.html" data-scroll="part-lin-trans" class="internal"><span class="codenumber">VIII</span> <span class="title">Linear Transformations</span></a></li>
<li class="link">
<a href="chap_linear_transformation.html" data-scroll="chap_linear_transformation" class="internal"><span class="codenumber">37</span> <span class="title">Linear Transformations</span></a><ul>
<li><a href="chap_linear_transformation.html#sec_appl_fractals" data-scroll="sec_appl_fractals" class="internal">Application: Fractals</a></li>
<li><a href="chap_linear_transformation.html#sec_lin_trans_intro" data-scroll="sec_lin_trans_intro" class="internal">Introduction</a></li>
<li><a href="chap_linear_transformation.html#sec_onto_oneone" data-scroll="sec_onto_oneone" class="internal">Onto and One-to-One Transformations</a></li>
<li><a href="chap_linear_transformation.html#sec_kernel_range" data-scroll="sec_kernel_range" class="internal">The Kernel and Range of Linear Transformation</a></li>
<li><a href="chap_linear_transformation.html#sec_isomorph" data-scroll="sec_isomorph" class="internal">Isomorphisms</a></li>
<li><a href="chap_linear_transformation.html#sec_lin_trans_exam" data-scroll="sec_lin_trans_exam" class="internal">Examples</a></li>
<li><a href="chap_linear_transformation.html#sec_lin_trans_summ" data-scroll="sec_lin_trans_summ" class="internal">Summary</a></li>
<li><a href="chap_linear_transformation.html#sec_lin_trans_exer" data-scroll="sec_lin_trans_exer" class="internal">Exercises</a></li>
<li><a href="chap_linear_transformation.html#sec_proj_fractals" data-scroll="sec_proj_fractals" class="internal">Project: Fractals via Iterated Function Systems</a></li>
</ul>
</li>
<li class="link">
<a href="chap_transformation_matrix.html" data-scroll="chap_transformation_matrix" class="internal"><span class="codenumber">38</span> <span class="title">The Matrix of a Linear Transformation</span></a><ul>
<li><a href="chap_transformation_matrix.html#sec_appl_secret" data-scroll="sec_appl_secret" class="internal">Application: Secret Sharing Algorithms</a></li>
<li><a href="chap_transformation_matrix.html#sec_mtxof_trans_intro" data-scroll="sec_mtxof_trans_intro" class="internal">Introduction</a></li>
<li><a href="chap_transformation_matrix.html#sec_trans_rn_rm" data-scroll="sec_trans_rn_rm" class="internal">Linear Transformations from <span class="process-math">\(\R^n\)</span> to <span class="process-math">\(\R^m\)</span></a></li>
<li><a href="chap_transformation_matrix.html#sec_mtx_lin_trans" data-scroll="sec_mtx_lin_trans" class="internal">The Matrix of a Linear Transformation</a></li>
<li><a href="chap_transformation_matrix.html#sec_ker_mtx" data-scroll="sec_ker_mtx" class="internal">A Connection between <span class="process-math">\(\Ker(T)\)</span> and a Matrix Representation of <span class="process-math">\(T\)</span></a></li>
<li><a href="chap_transformation_matrix.html#sec_mtxof_trans_exam" data-scroll="sec_mtxof_trans_exam" class="internal">Examples</a></li>
<li><a href="chap_transformation_matrix.html#sec_mtxof_trans_summ" data-scroll="sec_mtxof_trans_summ" class="internal">Summary</a></li>
<li><a href="chap_transformation_matrix.html#sec_mtxof_trans_exer" data-scroll="sec_mtxof_trans_exer" class="internal">Exercises</a></li>
<li><a href="chap_transformation_matrix.html#sec_proj_secret" data-scroll="sec_proj_secret" class="internal">Project: Shamir's Secret Sharing and Lagrange Polynomials</a></li>
</ul>
</li>
<li class="link">
<a href="chap_transformations_eigenvalues.html" data-scroll="chap_transformations_eigenvalues" class="internal"><span class="codenumber">39</span> <span class="title">Eigenvalues of Linear Transformations</span></a><ul>
<li><a href="chap_transformations_eigenvalues.html#sec_appl_diff_eq" data-scroll="sec_appl_diff_eq" class="internal">Application: Linear Differential Equations</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_eigen_trans_intro" data-scroll="sec_eigen_trans_intro" class="internal">Introduction</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_find_eigen_trans" data-scroll="sec_find_eigen_trans" class="internal">Finding Eigenvalues and Eigenvectors of Linear Transformations</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_diagonal" data-scroll="sec_diagonal" class="internal">Diagonalization</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_eigen_trans_exam" data-scroll="sec_eigen_trans_exam" class="internal">Examples</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_eigen_trans_summ" data-scroll="sec_eigen_trans_summ" class="internal">Summary</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_eigen_trans_exer" data-scroll="sec_eigen_trans_exer" class="internal">Exercises</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_proj_diff_eq" data-scroll="sec_proj_diff_eq" class="internal">Project: Linear Transformations and Differential Equations</a></li>
</ul>
</li>
<li class="link">
<a href="chap_JCF.html" data-scroll="chap_JCF" class="internal"><span class="codenumber">40</span> <span class="title">The Jordan Canonical Form</span></a><ul>
<li><a href="chap_JCF.html#sec_appl_epidemic" data-scroll="sec_appl_epidemic" class="internal">Application: The Bailey Model of an Epidemic</a></li>
<li><a href="chap_JCF.html#sec_jordan_intro" data-scroll="sec_jordan_intro" class="internal">Introduction</a></li>
<li><a href="chap_JCF.html#sec_eigen_dne" data-scroll="sec_eigen_dne" class="internal">When an Eigenvalue Decomposition Does Not Exist</a></li>
<li><a href="chap_JCF.html#sec_gen_eigen_jordan" data-scroll="sec_gen_eigen_jordan" class="internal">Generalized Eigenvectors and the Jordan Canonical Form</a></li>
<li><a href="chap_JCF.html#sec_mtx_jordan_geom" data-scroll="sec_mtx_jordan_geom" class="internal">Geometry of Matrix Transformations using the Jordan Canonical Form</a></li>
<li><a href="chap_JCF.html#sec_jordan_proof" data-scroll="sec_jordan_proof" class="internal">Proof of the Existence of the Jordan Canonical Form</a></li>
<li><a href="chap_JCF.html#sec_mtx_nilpotent" data-scroll="sec_mtx_nilpotent" class="internal">Nilpotent Matrices and Invariant Subspaces</a></li>
<li><a href="chap_JCF.html#sec_jordan" data-scroll="sec_jordan" class="internal">The Jordan Canonical Form</a></li>
<li><a href="chap_JCF.html#sec_jordan_exam" data-scroll="sec_jordan_exam" class="internal">Examples</a></li>
<li><a href="chap_JCF.html#sec_jordan_summ" data-scroll="sec_jordan_summ" class="internal">Summary</a></li>
<li><a href="chap_JCF.html#sec_jordan_exer" data-scroll="sec_jordan_exer" class="internal">Exercises</a></li>
<li><a href="chap_JCF.html#sec_proj_epidemic" data-scroll="sec_proj_epidemic" class="internal">Project: Modeling an Epidemic</a></li>
</ul>
</li>
<li class="link backmatter"><a href="backmatter-1.html" data-scroll="backmatter-1" class="internal"><span class="title">Back Matter</span></a></li>
<li class="link">
<a href="app_complex_numbers.html" data-scroll="app_complex_numbers" class="internal"><span class="codenumber">A</span> <span class="title">Complex Numbers</span></a><ul>
<li><a href="app_complex_numbers.html#sec_complex_numbers" data-scroll="sec_complex_numbers" class="internal">Complex Numbers</a></li>
<li><a href="app_complex_numbers.html#sec_conj_modulus" data-scroll="sec_conj_modulus" class="internal">Conjugates and Modulus</a></li>
<li><a href="app_complex_numbers.html#sec_complex_vect" data-scroll="sec_complex_vect" class="internal">Complex Vectors</a></li>
</ul>
</li>
<li class="link"><a href="app_answers.html" data-scroll="app_answers" class="internal"><span class="codenumber">B</span> <span class="title">Answers and Hints for Selected Exercises</span></a></li>
<li class="link"><a href="index-1.html" data-scroll="index-1" class="internal"><span class="title">Index</span></a></li>
</ul></nav><div class="extras"><nav><a class="pretext-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content">
<section class="chapter" id="chap_least_squares"><h2 class="heading">
<span class="type">Section</span> <span class="codenumber">26</span> <span class="title">Least Squares Approximations</span>
</h2>
<section class="introduction" id="introduction-423"><article class="objectives goal-like" id="objectives-26"><h3 class="heading"><span class="type">Focus Questions</span></h3>
<div class="introduction" id="introduction-424"><p id="p-4397">By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.</p></div>
<ul class="disc">
<li id="li-703"><p id="p-4398">How, in general, can we find a least squares approximation to a system <span class="process-math">\(A \vx = \vb\text{?}\)</span></p></li>
<li id="li-704"><p id="p-4399">If the columns of <span class="process-math">\(A\)</span> are linearly independent, how can we find a least squares approximation to <span class="process-math">\(A \vx = \vb\)</span> using just matrix operations?</p></li>
<li id="li-705"><p id="p-4400">Why are these approximations called “least squares” approximations?</p></li>
</ul></article></section><section class="section" id="sec_appl_fit_func"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber"></span> <span class="title">Application: Fitting Functions to Data</span>
</h3>
<p id="p-4401">Data is all around us. Data is collected on almost everything and it is important to be able to use data to make predictions. However, data is rarely well-behaved and so we generally need to use approximation techniques to estimate from the data. One technique for this is least squares approximations. As we will see, we can use linear algebra to fit a variety of different types of curves to data.</p></section><section class="section" id="sec_ls_intro"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber"></span> <span class="title">Introduction</span>
</h3>
<p id="p-4402">In this section our focus is on fitting linear and polynomial functions to data sets.</p>
<article class="exploration project-like" id="pa_LS_1"><h4 class="heading">
<span class="type">Preview Activity</span><span class="space"> </span><span class="codenumber">26.1</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-425">
<p id="p-4403">NBC was awarded the U.S. television broadcast rights to the 2016 and 2020 summer Olympic games. <a href="" class="xref" data-knowl="./knowl/T_LS_Olympics.html" title="Table 26.1: Olympics television broadcast rights.">Table 26.1</a> lists the amounts paid (in millions of dollars) by NBC sports for the 2008 through 2012 summer Olympics plus the recently concluded bidding for the 2016 and 2020 Olympics, where year 0 is the year 2008. (We will assume a simple model here, ignoring variations such as value of money due to inflation, viewership data which might affect NBC's expected revenue, etc.) <a href="" class="xref" data-knowl="./knowl/F_LS_Olympics.html" title="Figure 26.2">Figure 26.2</a> shows a plot of the data. Our goal in this activity is to find a linear function <span class="process-math">\(f\)</span> defined by <span class="process-math">\(f(x) = a_0 + a_1x\)</span> that fits the data well.</p>
<div class="sidebyside"><div class="sbsrow" style="margin-left:0%;margin-right:0%;">
<div class="sbspanel fixed-width top" style="width:50%;"><figure class="table table-like" id="T_LS_Olympics"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">26.1<span class="period">.</span></span><span class="space"> </span>Olympics television broadcast rights.</figcaption><table class="tabular">
<tr>
<td class="c m b1 r1 l1 t1 lines">Year</td>
<td class="c m b1 r1 l0 t1 lines">Amount</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">0</td>
<td class="c m b1 r1 l0 t0 lines">894</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">4</td>
<td class="c m b1 r1 l0 t0 lines">1180</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">8</td>
<td class="c m b1 r1 l0 t0 lines">1226</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">12</td>
<td class="c m b1 r1 l0 t0 lines">1418</td>
</tr>
</table></figure></div>
<div class="sbspanel top" style="width:50%;"><figure class="figure figure-like" id="F_LS_Olympics"><img src="external/7_d_pa_3.svg" role="img" class="contained"><figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">26.2<span class="period">.</span></span><span class="space"> </span>A plot of the data.</figcaption></figure></div>
</div></div>
<p id="p-4404">If the data were actually linear, then the data would satisfy the system</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-178">
\begin{alignat*}{3}
{}a_0 \amp {}+{}  \amp {0}a_1 \amp = \amp 894\amp {}\\
{}a_0 \amp {}+{}  \amp {4}a_1 \amp = \amp 1180\amp {}\\
{}a_0 \amp {}+{}  \amp {8}a_1 \amp = \amp 1226\amp {}\\
{}a_0 \amp {}+{}  \amp {12}a_1 \amp = \amp 1418\amp {.}
\end{alignat*}
</div>
<p id="p-4405">The vector form of this equation is</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
a_0 [1 \ 1 \ 1 \ 1]^{\tr} + a_1[0 \ 4 \ 8 \ 12]^{\tr} = [894 \ 1180 \ 1226 \ 1418]^{\tr}\text{.}
\end{equation*}
</div>
<p id="p-4406">This equation does not have a solution, so we seek the best approximation to a solution we can find. That is, we want to find <span class="process-math">\(a_0\)</span> and <span class="process-math">\(a_1\)</span> so that the line <span class="process-math">\(f(x) = a_0+a_1x\)</span> provides a best fit to the data.</p>
<p id="p-4407">Letting <span class="process-math">\(\vv_1 = [1 \ 1 \ 1 \ 1]^{\tr}\)</span> and <span class="process-math">\(\vv_2 = [0 \ 4 \ 8 \ 12]^{\tr}\text{,}\)</span> and <span class="process-math">\(\vb = [894 \ 1180 \ 1226 \ 1418]^{\tr}\text{,}\)</span> our vector equation becomes</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
a_0 \vv_1 + a_1\vv_2 = \vb\text{.}
\end{equation*}
</div>
<p id="p-4408">To make a best fit, we will minimize the square of the distance between <span class="process-math">\(\vb\)</span> and a vector of the form <span class="process-math">\(a_0\vv_1 + a_1 \vv_2\text{.}\)</span> That is, minimize</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="eq_LS_line_1">
\begin{equation}
||\vb - (a_0 \vv_1 + a_1\vv_2)||^2\text{.}\tag{26.1}
\end{equation}
</div>
<p id="p-4409">Rephrasing this in terms of projections, we are looking for the vector in <span class="process-math">\(W = \Span\{\vv_1, \vv_2\}\)</span> that is closest to <span class="process-math">\(\vb\text{.}\)</span> In other words, the values of <span class="process-math">\(a_0\)</span> and <span class="process-math">\(a_1\)</span> will occur as the weights when we write <span class="process-math">\(\proj_{W} \vb\)</span> as a linear combination of <span class="process-math">\(\vv_1\)</span> and <span class="process-math">\(\vv_2\text{.}\)</span> The one wrinkle in this problem is that we need an orthogonal basis for <span class="process-math">\(W\)</span> to find this projection. Use appropriate technology throughout this activity.</p>
</div>
<article class="task exercise-like" id="task-1448"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4410">Find an orthogonal basis <span class="process-math">\(\CB = \{\vw_1, \vw_2\}\)</span> for <span class="process-math">\(W\text{.}\)</span></p></article><article class="task exercise-like" id="task-1449"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-4411">Use the basis <span class="process-math">\(\CB\)</span> to find <span class="process-math">\(\vy = \proj_{W} \vb\)</span> as illustrated in <a href="" class="xref" data-knowl="./knowl/F_6_f_proj.html" title="Figure 26.3">Figure 26.3</a>.</p>
<figure class="figure figure-like" id="F_6_f_proj"><div class="image-box" style="width: 30%; margin-left: 35%; margin-right: 35%;"><img src="external/6_f_O_proj.svg" role="img" class="contained"></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">26.3<span class="period">.</span></span><span class="space"> </span>Projecting <span class="process-math">\(\vb\)</span> onto <span class="process-math">\(W\text{.}\)</span></figcaption></figure></article><article class="task exercise-like" id="task-1450"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-4412">Find the values of <span class="process-math">\(a_0\)</span> and <span class="process-math">\(a_1\)</span> that give our best fit line by writing <span class="process-math">\(\vy\)</span> as a linear combination of <span class="process-math">\(\vv_1\)</span> and <span class="process-math">\(\vv_2\text{.}\)</span></p></article><article class="task exercise-like" id="task-1451"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<p id="p-4413">Draw a picture of your line from the previous part on the axes with the data set. How well do you think your line approximates the data? Explain.</p></article></article></section><section class="section" id="sec_ls_approx"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber"></span> <span class="title">Least Squares Approximations</span>
</h3>
<p id="p-4414">In <a href="chap_gram_schmidt.html" class="internal" title="Section 25: Projections onto Subspaces and the Gram-Schmidt Process in \R^n">Section 25</a> we saw that the projection of a vector <span class="process-math">\(\vv\)</span> in <span class="process-math">\(\R^n\)</span> onto a subspace <span class="process-math">\(W\)</span> of <span class="process-math">\(\R^n\)</span> is the best approximation to <span class="process-math">\(\vv\)</span> of all the vectors in <span class="process-math">\(W\text{.}\)</span> In fact, if <span class="process-math">\(\vv = [v_1 \ v_2 \ \ldots \ v_n]^{\tr}\)</span> and <span class="process-math">\(\proj_W \vv =  [w_1 \ w_2 \ w_3 \ \ldots \ w_m]^{\tr}\text{,}\)</span> then the error in approximating <span class="process-math">\(\vv\)</span> by <span class="process-math">\(\vw\)</span> is given by</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/chap_gram_schmidt.html ./knowl/pa_LS_1.html ./knowl/F_6_f_proj.html ./knowl/F_LS_Olympics2.html">
\begin{equation*}
|| \vv - \proj_W \vv ||^2 = \sum_{i=1}^m (v_i - w_i)^2\text{.}
\end{equation*}
</div>
<p class="continuation">In the context of <a href="" class="xref" data-knowl="./knowl/pa_LS_1.html" title="Preview Activity 26.1">Preview Activity 26.1</a>, we projected the vector <span class="process-math">\(\vb\)</span> onto the span of  the vectors <span class="process-math">\(\vv_1 = [1 \ 1 \ 1 \ 1]^{\tr}\)</span> and <span class="process-math">\(\vv_2 = [0 \ 4 \ 8 \ 12]^{\tr}\text{.}\)</span> The projection   minimizes the distance between the vectors in <span class="process-math">\(W\)</span> and the vector <span class="process-math">\(\vb\)</span> (as shown in <a href="" class="xref" data-knowl="./knowl/F_6_f_proj.html" title="Figure 26.3">Figure 26.3</a>), and also produces a line which minimizes the sums of the squares of the vertical  distances from the line to the data set as illustrated in <a href="" class="xref" data-knowl="./knowl/F_LS_Olympics2.html" title="Figure 26.4">Figure 26.4</a> with the olympics data. This is why these approximations are called least squares approximations.</p>
<figure class="figure figure-like" id="F_LS_Olympics2"><div class="image-box" style="width: 30%; margin-left: 35%; margin-right: 35%;"><img src="external/Regression1.svg" role="img" class="contained"></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">26.4<span class="period">.</span></span><span class="space"> </span>Least squares linear approximation</figcaption></figure><p id="p-4415">While we can always solve least squares problems using projections, we can often avoid having to create an orthogonal basis when fitting functions to data. We work in a more general setting, showing how to fit a polynomial of degree <span class="process-math">\(n\)</span> to a set of data points. Our goal is to fit a polynomial <span class="process-math">\(p(x) = a_0+a_1x+a_2x^2+ \cdots + 
a_nx^n\)</span> of degree <span class="process-math">\(n\)</span> to <span class="process-math">\(m\)</span> data points <span class="process-math">\((x_1,y_1)\text{,}\)</span> <span class="process-math">\((x_2,y_2)\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\((x_m,y_m)\text{,}\)</span> no two of which have the same <span class="process-math">\(x\)</span> coordinate. In the unlikely event that the polynomial <span class="process-math">\(p(x)\)</span> actually passes through the <span class="process-math">\(m\)</span> points, then we would have the <span class="process-math">\(m\)</span> equations</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="mdn-19">
\begin{alignat}{7}
y_1 \amp{}={} \amp{a_0} \amp{}+{} \amp{a_1}x_1 \amp{}+{} \amp{a_2}x_1^2 \amp{}+{} \amp\cdots \amp{}+{} \amp{a_{n-1}}x_1^{n-1} \amp{}+{} \amp{a_n}x_1^n\tag{26.2}\\
y_2 \amp{}={} \amp{a_0} \amp{}+{} \amp{a_1}x_2 \amp{}+{} \amp{a_2}x_2^2 \amp{}+{} \amp\cdots \amp{}+{} \amp{a_{n-1}}x_2^{n-1} \amp{}+{} \amp{a_n}x_2^n\tag{26.3}\\
y_3 \amp{}={} \amp{a_0} \amp{}+{} \amp{a_1}x_3 \amp{}+{} \amp{a_2}x_3^2 \amp{}+{} \amp\cdots \amp{}+{} \amp{a_{n-1}}x_3^{n-1} \amp{}+{} \amp{a_n}x_3^n\tag{26.4}\\
{}   \amp{}     \amp{}       \amp{}      \amp{}          \amp{}      \amp{}             \amp{}       \amp\vdots \amp{}     \amp{}                            \amp{}       \amp{}\tag{26.5}\\
y_m \amp{}={} \amp{a_0} \amp{}+{} \amp{a_1}x_m \amp{}+{} \amp{a_2}x_m^2 \amp{}+{} \amp\cdots \amp{}+{} \amp{a_{n-1}}x_m^{n-1} \amp{}+{} \amp{a_n}x_m^n\tag{26.6}
\end{alignat}
</div>
<p class="continuation">in the <span class="process-math">\(n+1\)</span> unknowns <span class="process-math">\(a_0\text{,}\)</span> <span class="process-math">\(a_1\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\(a_{n-1}\text{,}\)</span> and <span class="process-math">\(a_n\text{.}\)</span></p>
<p id="p-4416">The <span class="process-math">\(m\)</span> data points are known in this situation and the coefficients <span class="process-math">\(a_0\text{,}\)</span> <span class="process-math">\(a_1\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\(a_n\)</span> are the unknowns. To write the system in matrix-vector form, the coefficient matrix <span class="process-math">\(M\)</span> is</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
M = \left[ \begin{array}{cccccc} 1 \amp x_1 \amp x_1^2\amp \cdots  \amp x_1^{n-1} \amp x_1^{n} \\ 1 \amp x_2 \amp x_2^2\amp \cdots  \amp x_2^{n-1} \amp x_2^{n} \\ 1 \amp x_3 \amp x_3^2\amp \cdots  \amp x_3^{n-1} \amp x_3^{n} \\ \vdots \amp \vdots \amp \vdots \amp \cdots \amp\vdots \amp\vdots \\ 1 \amp x_m \amp x_m^2\amp \cdots  \amp x_m^{n-1} \amp x_m^{n} \end{array} \right]\text{,}
\end{equation*}
</div>
<p class="continuation">while the vectors <span class="process-math">\(\va\)</span> and <span class="process-math">\(\vy\)</span> are</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\va = \left[ \begin{array}{c} a_{0} \\ a_{1} \\ a_{2} \\ \vdots \\ a_{n-1} \\a_{n} \end{array} \right] \text{ and } \vy = \left[ \begin{array}{c} y_{1} \\ y_{2} \\ y_{3} \\ \vdots \\ y_{m-1} \\ y_{m} \end{array} \right]\text{.}
\end{equation*}
</div>
<p class="continuation">Letting <span class="process-math">\(\vy = [y_1 \ y_2 \ \ldots \ y_m]^{\tr}\)</span> and <span class="process-math">\(\vv_i = [x_1^{i-1} \ x_2^{i-1} \ \ldots \ x_m^{i-1}]^{\tr}\)</span> for <span class="process-math">\(1 \leq i \leq n+1\text{,}\)</span> the vector form of the system is</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\vy = a_0\vv_1 + a_1 \vv_2 +  \cdots + a_n \vv_{n+1}\text{.}
\end{equation*}
</div>
<p id="p-4417">Of course, it is unlikely that the <span class="process-math">\(m\)</span> data points already lie on a polynomial of degree <span class="process-math">\(n\text{,}\)</span> so the system will usually have no solution. So instead of attempting to find coefficients <span class="process-math">\(a_0\text{,}\)</span> <span class="process-math">\(a_1\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\(a_n\)</span> that give a solution to this system, which may be impossible, we instead look for a vector that is ``close" to a solution. As we have seen, the vector <span class="process-math">\(\proj_{W} \vy\text{,}\)</span> where <span class="process-math">\(W\)</span> is the span of the columns of <span class="process-math">\(M\text{,}\)</span> minimizes the sum of the squares of the differences of the components. That is, our desired approximation to a solution to <span class="process-math">\(M \vx = \vy\)</span> is the projection of <span class="process-math">\(\vy\)</span> onto <span class="process-math">\(\Col M\text{.}\)</span> Now <span class="process-math">\(\proj_W \vy\)</span> is a linear combination of the columns of <span class="process-math">\(M\text{,}\)</span> so <span class="process-math">\(\proj_W \vy = M \va^*\)</span> for some vector <span class="process-math">\(\va^*\text{.}\)</span> This vector <span class="process-math">\(\va^*\)</span> then minimizes <span class="process-math">\(|| \proj_{\perp W} \vy|| = ||\vy - M \va ||\text{.}\)</span> That is, if we let <span class="process-math">\((M\va)^{\tr} = [b_1 \ b_2 \ b_3 \ \cdots \ b_m]\text{,}\)</span> we are minimizing</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="eq_ls_approx">
\begin{equation}
||\vy - M\va||^2 = (y_1-b_1)^2 + (t_2-b_2)^2 + \cdots + (y_m-b_m)^2\text{.}\tag{26.7}
\end{equation}
</div>
<p class="continuation">The expression <span class="process-math">\(||\vy - M\va||^2\)</span> measures the error in our approximation.</p>
<p id="p-4418">The question we want to answer is how we can find the vector <span class="process-math">\(\va^*\)</span> that minimizes <span class="process-math">\(||\vy -M\va||\)</span> in a way that is more convenient than computing a projection. We answer this question in a general setting in the next activity.</p>
<article class="activity project-like" id="act_LS_matrices"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">26.2</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-426"><p id="p-4419">Let <span class="process-math">\(A\)</span> be an <span class="process-math">\(m \times n\)</span> matrix and let <span class="process-math">\(\vb\)</span> be in <span class="process-math">\(\R^m\text{.}\)</span> Let <span class="process-math">\(W = \Col A\text{.}\)</span> Then <span class="process-math">\(\proj_W \vb\)</span> is in <span class="process-math">\(\Col A\text{,}\)</span> so let <span class="process-math">\(\vx^*\)</span> be in <span class="process-math">\(\R^{n}\)</span> such that <span class="process-math">\(A\vx^* = \proj_W \vb\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1452"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4420">Explain why <span class="process-math">\(\vb - A\vx^*\)</span> is orthogonal to every vector of the form <span class="process-math">\(A\vx\text{,}\)</span> for any <span class="process-math">\(\vx\)</span> in <span class="process-math">\(\R^{n}\text{.}\)</span> That is, <span class="process-math">\(\vb - A\vx^*\)</span> is orthogonal to <span class="process-math">\(\Col A\text{.}\)</span></p></article><article class="task exercise-like" id="task-1453"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-4421">Let <span class="process-math">\(\va_i\)</span> be the <span class="process-math">\(i\)</span>th column of <span class="process-math">\(A\text{.}\)</span> Explain why <span class="process-math">\(\va_i \cdot \left(\vb - A\vx^*\right) = 0\text{.}\)</span> From this, explain why <span class="process-math">\(A^{\tr}\left(\vb - A\vx^*\right) = 0\text{.}\)</span></p></article><article class="task exercise-like" id="task-1454"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-4422">From the previous part, show that <span class="process-math">\(\vx^*\)</span> satisfies the equation</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A^{\tr}A\vx^* = A^{\tr}\vb\text{.}
\end{equation*}
</div></article></article><p id="p-4423">The result of <a href="" class="xref" data-knowl="./knowl/act_LS_matrices.html" title="Activity 26.2">Activity 26.2</a> is that we can now do least squares polynomial approximations with just matrix operations. We summarize this in the following theorem.</p>
<article class="theorem theorem-like" id="thm_6_f_least_squares_1"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">26.5</span><span class="period">.</span>
</h4>
<p id="p-4424">The least squares solutions to the system <span class="process-math">\(A \vx = \vb\)</span> are the solutions to the corresponding system</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="eq_6_f_normal_equations">
\begin{equation}
A^{\tr}A\vx = A^{\tr}\vb\text{.}\tag{26.8}
\end{equation}
</div></article><p id="p-4425">The equations in the system <a href="" class="xref" data-knowl="./knowl/eq_6_f_normal_equations.html" title="Equation 26.8">(26.8)</a> are called the <dfn class="terminology">normal equations</dfn> for <span class="process-math">\(A \vx = \vb\text{.}\)</span> To illustrate, with the Olympics data, our data points are <span class="process-math">\((0, 894)\text{,}\)</span> <span class="process-math">\((4, 1180)\text{,}\)</span> <span class="process-math">\((8, 1226)\text{,}\)</span> <span class="process-math">\((12,1418)\)</span> with <span class="process-math">\(\vy = [894 \ 1180 \ 1226 \ 1418]^{\tr}\text{.}\)</span> So <span class="process-math">\(M = \left[ \begin{array}{cc} 1\amp0 \\ 1\amp4 \\ 1\amp8 \\ 1\amp12 \end{array} \right]\text{.}\)</span> Notice that <span class="process-math">\(M^{\tr}M\)</span> is invertible, to find the degree 1 approximation to the data, technology shows that</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_6_f_normal_equations.html ./knowl/pa_LS_1.html">
\begin{equation*}
\va^* = \left(M^{\tr}M\right)^{-1}M^{\tr}\vy = \left[ \frac{4684}{5} \ \frac{809}{20}\right]
\end{equation*}
</div>
<p class="continuation">just as in <a href="" class="xref" data-knowl="./knowl/pa_LS_1.html" title="Preview Activity 26.1">Preview Activity 26.1</a>.</p>
<article class="activity project-like" id="activity-102"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">26.3</span><span class="period">.</span>
</h4>
<p id="p-4426">Now use the least squares method to find the best polynomial approximations (in the least squares sense) of degrees 2 and 3 for the Olympics data set in <a href="" class="xref" data-knowl="./knowl/T_LS_Olympics.html" title="Table 26.1: Olympics television broadcast rights.">Table 26.1</a>. Which polynomial seems to give the “best” fit? Explain why. Include a discussion of the errors in your approximations. Use your “best” least squares approximation to estimate how much NBC might pay for the television rights to the 2024 Olympic games. Use technology as appropriate.</p></article><p id="p-4427">The solution with our Olympics data gave us the situation where <span class="process-math">\(M^{\tr}M\)</span> was invertible. This corresponded to a unique least squares solution <span class="process-math">\(\left(M^{\tr}M\right)^{-1}M^{\tr}\vy\text{.}\)</span> It is reasonable to ask when this happens in general. To conclude this section, we will demonstrate that if the columns of a matrix <span class="process-math">\(A\)</span> are linearly independent, then <span class="process-math">\(A^{\tr}A\)</span> is invertible.</p>
<article class="activity project-like" id="act_LS_invertible"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">26.4</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-427"><p id="p-4428">Let <span class="process-math">\(A\)</span> be an <span class="process-math">\(m \times n\)</span> matrix with linearly independent columns.</p></div>
<article class="task exercise-like" id="task-1455"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4429">What must be the relationship between <span class="process-math">\(m\)</span> and <span class="process-math">\(n\text{?}\)</span> Explain.</p></article><article class="task exercise-like" id="task-1456"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<div class="introduction" id="introduction-428"><p id="p-4430">We know that an <span class="process-math">\(n \times n\)</span> matrix <span class="process-math">\(M\)</span> is invertible if and only if the only solution to the homogeneous system <span class="process-math">\(M \vx = \vzero\)</span> is <span class="process-math">\(\vx = \vzero\text{.}\)</span> Note that <span class="process-math">\(A^{\tr}A\)</span> is an <span class="process-math">\(n \times n\)</span> matrix. Suppose that <span class="process-math">\(A^{\tr}A \vx = \vzero\)</span> for some <span class="process-math">\(\vx\)</span> in <span class="process-math">\(\R^n\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1457"><h6 class="heading"><span class="codenumber">(i)</span></h6>
<p id="p-4431">Show that <span class="process-math">\(||A\vx|| = 0\text{.}\)</span></p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-49" id="hint-49"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-49"><div class="hint solution-like"><p id="p-4432">What is <span class="process-math">\(\vx^{\tr} (A^{\tr}A \vx)\text{?}\)</span></p></div></div>
</div></article><article class="task exercise-like" id="task-1458"><h6 class="heading"><span class="codenumber">(ii)</span></h6>
<p id="p-4433">What does <span class="process-math">\(||A\vx|| = 0\)</span> tell us about <span class="process-math">\(\vx\)</span> in relation to <span class="process-math">\(\Nul A\text{?}\)</span> Why?</p></article><article class="task exercise-like" id="task-1459"><h6 class="heading"><span class="codenumber">(iii)</span></h6>
<p id="p-4434">What is <span class="process-math">\(\Nul A\text{?}\)</span> Why? What does this tell us about <span class="process-math">\(\vx\)</span> and then about <span class="process-math">\(A^{\tr}A\text{?}\)</span></p></article></article></article><p id="p-4435">We summarize the result of <a href="" class="xref" data-knowl="./knowl/act_LS_invertible.html" title="Activity 26.4">Activity 26.4</a> in the following theorem.</p>
<article class="theorem theorem-like" id="thm_6_f_least_squares_2"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">26.6</span><span class="period">.</span>
</h4>
<p id="p-4436">If the columns of <span class="process-math">\(A\)</span> are linearly independent, then the least squares solution <span class="process-math">\(\vx^*\)</span> to the system <span class="process-math">\(A \vx = \vb\)</span> is</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\vx^* = \left(A^{\tr}A\right)^{-1}A^{\tr}\vb\text{.}
\end{equation*}
</div></article><p id="p-4437">If the columns of <span class="process-math">\(A\)</span> are linearly dependent, we can still solve the normal equations, but will obtain more than one solution. In a later section we will see that we can also use a pseudoinverse in these situations.</p></section><section class="section" id="sec_ls_exam"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber"></span> <span class="title">Examples</span>
</h3>
<p id="p-4438">What follows are worked examples that use the concepts from this section.</p>
<article class="example example-like" id="example-52"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">26.7</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-429">
<p id="p-4439">According to the <a class="external" href="https://www.cdc.gov/growthcharts/html_charts/lenageinf.htm" target="_blank">Centers for Disease Control and Prevention</a><a href="" data-knowl="" class="id-ref fn-knowl original" data-refid="hk-fn-45" id="fn-45"><sup> 45 </sup></a>, the average length of a male infant (in centimeters) in the US as it ages (with time in months from 1.5 to 8.5) is given in <a href="" class="xref" data-knowl="./knowl/T_lengths.html" title="Table 26.8: Average lengths of male infants">Table 26.8</a>.</p>
<figure class="table table-like" id="T_lengths"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">26.8<span class="period">.</span></span><span class="space"> </span>Average lengths of male infants</figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="c m b1 r2 l1 t1 lines">Age (months)</td>
<td class="c m b1 r1 l0 t1 lines">1.5</td>
<td class="c m b1 r1 l0 t1 lines">2.5</td>
<td class="c m b1 r1 l0 t1 lines">3.5</td>
<td class="c m b1 r1 l0 t1 lines">4.5</td>
<td class="c m b1 r1 l0 t1 lines">5.5</td>
<td class="c m b1 r1 l0 t1 lines">6.5</td>
<td class="c m b1 r1 l0 t1 lines">7.5</td>
<td class="c m b1 r1 l0 t1 lines">8.5</td>
</tr>
<tr>
<td class="c m b1 r2 l1 t0 lines">Average Length (cm)</td>
<td class="c m b1 r1 l0 t0 lines">56.6</td>
<td class="c m b1 r1 l0 t0 lines">59.6</td>
<td class="c m b1 r1 l0 t0 lines">62.1</td>
<td class="c m b1 r1 l0 t0 lines">64.2</td>
<td class="c m b1 r1 l0 t0 lines">66.1</td>
<td class="c m b1 r1 l0 t0 lines">67.9</td>
<td class="c m b1 r1 l0 t0 lines">69.5</td>
<td class="c m b1 r1 l0 t0 lines">70.9</td>
</tr>
</table></div></figure><p id="p-4440">In this problem we will find the line and the quadratic of best fit in the least squares sense to this data. We treat age in months as the independent variable and length in centimeters as the dependent variable.</p>
</div>
<article class="task exercise-like" id="task-1460"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4441">Find a line that is the best fit to the data in the least squares sense. Draw a picture of your least squares solution against a scatterplot of the data.</p>
<div class="solution solution-like" id="solution-155">
<h5 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h5>
<p id="p-4442">We assume that a line of the form <span class="process-math">\(f(x) = a_1x+a_0\)</span> contains all of the data points. The first data point would satisfy <span class="process-math">\(1.5a_1+a_0 = 56.6\text{,}\)</span> the second <span class="process-math">\(2.5a_1+a_0 = 59.6\text{,}\)</span> and so on, giving us the linear system</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/F_LS_linear.html" id="md-179">
\begin{alignat*}{2}
{3} {1.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 56.6{}\\
{2.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 59.6{}\\
{3.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 62.1{}\\
{4.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 64.2{}\\
{5.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 66.1{}\\
{6.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 67.9{}\\
{7.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 69.5{}\\
{8.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 70.9{.}
\end{alignat*}
</div>
<p class="continuation">Letting</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/F_LS_linear.html">
\begin{equation*}
A = \left[ \begin{array}{cc} 1.5\amp 1 \\ 2.5\amp 1 \\ 3.5\amp 1 \\ 4.5\amp 1 \\ 5.5\amp 1 \\ 6.5\amp 1 \\ 7.5\amp 1 \\ 8.5\amp 1 \end{array}  \right], \ \vx = \left[ \begin{array}{c} a_1\\a_0 \end{array}  \right], \ \text{ and }  \ \vb=\left[ \begin{array}{c} 56.6\\59.6\\62.1\\64.2\\66.1\\67.9\\69.5\\70.9 \end{array}  \right]\text{,}
\end{equation*}
</div>
<p class="continuation">we can write this system in the matrix form  <span class="process-math">\(A \vx = \vb\text{.}\)</span> Neither column of <span class="process-math">\(A\)</span> is a multiple of the other, so the columns of <span class="process-math">\(A\)</span> are linearly independent. The least squares solution <span class="process-math">\(\vx^*\)</span> to the system is then found by</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/F_LS_linear.html">
\begin{equation*}
\vx^* = \left(A^{\tr}A\right)^{-1}A^{\tr} \vb\text{.}
\end{equation*}
</div>
<p class="continuation">Technology shows that (with entries rounded to 3 decimal places), <span class="process-math">\(\left(A^{\tr}A\right)^{-1}A^{\tr}\)</span> is</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/F_LS_linear.html">
\begin{equation*}
\left[ \begin{array}{rrrrcrrr} - 0.083\amp - 0.060\amp - 0.036\amp - 0.012\amp  0.012\amp  0.036\amp  0.060\amp  0.083\\ 0.542\amp  0.423\amp  0.304\amp  0.185\amp  0.065\amp - 0.054\amp - 0.173\amp - 0.292 \end{array}  \right]\text{,}
\end{equation*}
</div>
<p class="continuation">and</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/F_LS_linear.html">
\begin{equation*}
\vx^* \approx \left[ \begin{array}{cc} 2.011 \\ 54.559 \end{array}  \right]\text{.}
\end{equation*}
</div>
<p class="continuation">So the least squares linear function to the data is <span class="process-math">\(f(x) \approx 2.011x + 54.559\text{.}\)</span> A graph of <span class="process-math">\(f\)</span> against the data points is shown at left in <a href="" class="xref" data-knowl="./knowl/F_LS_linear.html" title="Figure 26.9">Figure 26.9</a>. <figure class="figure figure-like" id="F_LS_linear"><div class="image-box" style="width: 50%; margin-left: 25%; margin-right: 25%;"><img src="external/7_d_least_squares_line.svg" role="img" class="contained"></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">26.9<span class="period">.</span></span><span class="space"> </span>Left: Least squares line. Right: Least squares quadratic.</figcaption></figure></p>
</div></article><article class="task exercise-like" id="task-1461"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-4443">Now find the least squares quadratic of the form <span class="process-math">\(q(x) = a_2x^2+a_1x+a_0\)</span> to the data. Draw a picture of your least squares solution against a scatterplot of the data.</p>
<div class="solution solution-like" id="solution-156">
<h5 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h5>
<p id="p-4444">The first data point would satisfy <span class="process-math">\((1.5^2)a_2+1.5a_1+a_0 = 56.6\text{,}\)</span> the second <span class="process-math">\((2.5)^2a_2+2.5a_1+a_0 = 59.6\text{,}\)</span> and so on, giving us the linear system</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/F_LS_linear.html" id="md-180">
\begin{alignat*}{3}
{1.5^2}a_2  \amp {}+{}  \amp {1.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 56.6{}\\
{2.5^2}a_2  \amp {}+{}  \amp {2.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 59.6{}\\
{3.5^2}a_2  \amp {}+{}  \amp {3.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 62.1{}\\
{4.5^2}a_2  \amp {}+{}  \amp {4.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 64.2{}\\
{5.5^2}a_2  \amp {}+{}  \amp {5.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 66.1{}\\
{6.5^2}a_2  \amp {}+{}  \amp {6.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 67.9{}\\
{7.5^2}a_2  \amp {}+{}  \amp {7.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 69.5{}\\
{8.5^2}a_2  \amp {}+{}  \amp {8.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 70.9{.}
\end{alignat*}
</div>
<p class="continuation">Letting</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/F_LS_linear.html">
\begin{equation*}
A = \left[ \begin{array}{ccc} 1.5^2\amp 1.5\amp 1 \\ 2.5^2\amp 2.5\amp 1 \\ 3.5^2\amp 3.5\amp 1 \\ 4.5^2\amp 4.5\amp 1 \\ 5.5^2\amp 5.5\amp 1 \\ 6.5^2\amp 6.5\amp 1 \\ 7.5^2\amp 7.5\amp 1 \\ 8.5^2\amp 8.5\amp 1 \end{array}  \right], \ \vx = \left[ \begin{array}{c} a_2\\a_1\\a_0 \end{array}  \right], \ \text{ and }  \ \vb=\left[ \begin{array}{c} 56.6\\59.6\\62.1\\64.2\\66.1\\67.9\\69.5\\70.9 \end{array}  \right]\text{,}
\end{equation*}
</div>
<p class="continuation">we can write this system in the matrix form  <span class="process-math">\(A \vx = \vb\text{.}\)</span> Technology shows that every column of the reduced row echelon form of <span class="process-math">\(A\)</span> contains a pivot, so the columns of <span class="process-math">\(A\)</span> are linearly independent. The least squares solution <span class="process-math">\(\vx^*\)</span> to the system is then found by</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/F_LS_linear.html">
\begin{equation*}
\vx^* = \left(A^{\tr}A\right)^{-1}A^{\tr} \vb\text{.}
\end{equation*}
</div>
<p class="continuation">Technology shows that (with entries rounded to 3 decimal places) <span class="process-math">\(\left(A^{\tr}A\right)^{-1}\)</span> is</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/F_LS_linear.html">
\begin{equation*}
\left[   \begin{array}{rrrrrrrr}  0.042\amp  0.006\amp - 0.018\amp - 0.030\amp - 0.030\amp - 0.018\amp  0.006\amp  0.042\\ - 0.500\amp - 0.119\amp  0.143\amp  0.286\amp  0.310\amp  0.214\amp  0.000\amp -0.333\\  1.365\amp  0.540\amp - 0.049\amp - 0.403\amp -0.522\amp - 0.406\amp - 0.055\amp  0.531 \end{array}  \right]\text{,}
\end{equation*}
</div>
<p class="continuation">and</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/F_LS_linear.html">
\begin{equation*}
\vx^* \approx \left[ \begin{array}{r} -0.118 \\ 3.195 \\ 52.219 \end{array}  \right]\text{.}
\end{equation*}
</div>
<p class="continuation">So the least squares quadratic function to the data is <span class="process-math">\(q\)</span> defined by <span class="process-math">\(q(x) \approx -0.118x^2+3.195x + 52.219\text{.}\)</span> A graph of <span class="process-math">\(q\)</span> against the data points is shown at right in <a href="" class="xref" data-knowl="./knowl/F_LS_linear.html" title="Figure 26.9">Figure 26.9</a>.</p>
</div></article></article><article class="example example-like" id="example-53"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">26.10</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-430">
<p id="p-4445">Least squares solutions can be found through a QR factorization, as we explore in this example. Let <span class="process-math">\(A\)</span> be an <span class="process-math">\(m \times n\)</span> matrix with linearly independent columns and QR factorization <span class="process-math">\(A = QR\text{.}\)</span> Suppose that <span class="process-math">\(\vb\)</span> is not in <span class="process-math">\(\Col A\)</span> so that the system <span class="process-math">\(A \vx = \vb\)</span> is inconsistent. We know that the least squares solution <span class="process-math">\(\vx^*\)</span> to <span class="process-math">\(A \vx = \vb\)</span> is</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="eq_ls_example">
\begin{equation}
\vx^* = \left(A^{\tr}A\right)^{-1}A^{\tr}\vb\text{.}\tag{26.9}
\end{equation}
</div>
</div>
<article class="task exercise-like" id="task-1462"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4446">Replace <span class="process-math">\(A\)</span> by its QR factorization in <a href="" class="xref" data-knowl="./knowl/eq_ls_example.html" title="Equation 26.9">(26.9)</a> to show that</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_ls_example.html">
\begin{equation*}
\vx^* = R^{-1}Q^{\tr} \vb\text{.}
\end{equation*}
</div>
<div class="hint solution-like" id="hint-50">
<h5 class="heading">
<span class="type">Hint</span><span class="period">.</span>
</h5>
<p id="p-4447">Use the fact that <span class="process-math">\(Q\)</span> is orthogonal and <span class="process-math">\(R\)</span> is invertible.</p>
</div>
<div class="solution solution-like" id="solution-157">
<h5 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h5>
<p id="p-4448">Replacing <span class="process-math">\(A\)</span> with <span class="process-math">\(QR\)</span> and using the fact that <span class="process-math">\(R\)</span> is invertible and <span class="process-math">\(Q\)</span> is orthogonal to see that</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-181">
\begin{align*}
\vx^* \amp = \left(A^{\tr}A\right)^{-1}A^{\tr}\vb\\
\amp = \left((QR)^{\tr}QR\right)^{-1} (QR)^{\tr} \vb\\
\amp = \left(R^{\tr}Q^{\tr}QR\right) R^{\tr}Q^{\tr} \vb\\
\amp = \left(R^{\tr}R\right)^{-1}R^{\tr}Q^{\tr} \vb\\
\amp = R^{-1}\left(R^{\tr}\right)^{-1}R^{\tr} Q^{\tr} \vb\\
\amp = R^{-1}Q^{\tr} \vb\text{.}
\end{align*}
</div>
<p class="continuation">So if <span class="process-math">\(A = QR\)</span> is a QR factorization of <span class="process-math">\(A\text{,}\)</span> then the least squares solution to <span class="process-math">\(A \vx = \vb\)</span> is <span class="process-math">\(R^{-1}Q^{\tr} \vb\text{.}\)</span></p>
</div></article><article class="task exercise-like" id="task-1463"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<div class="introduction" id="introduction-431">
<p id="p-4449">Consider the data set in <a href="" class="xref" data-knowl="./knowl/T_life_exectancy.html" title="Table 26.11: Life expectancy in the US">Table 26.11</a>, which shows the average life expectance in years in the US for selected years from 1950 to 2010.</p>
<figure class="table table-like" id="T_life_exectancy"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">26.11<span class="period">.</span></span><span class="space"> </span>Life expectancy in the US</figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="l m b0 r1 l0 t0 lines">year</td>
<td class="l m b0 r0 l0 t0 lines">1950</td>
<td class="l m b0 r0 l0 t0 lines">1965</td>
<td class="l m b0 r0 l0 t0 lines">1980</td>
<td class="l m b0 r0 l0 t0 lines">1995</td>
<td class="l m b0 r0 l0 t0 lines">2010</td>
</tr>
<tr>
<td class="l m b0 r1 l0 t0 lines">age</td>
<td class="l m b0 r0 l0 t0 lines">68.14</td>
<td class="l m b0 r0 l0 t0 lines">70.21</td>
<td class="l m b0 r0 l0 t0 lines">73.70</td>
<td class="l m b0 r0 l0 t0 lines">75.98</td>
<td class="l m b0 r0 l0 t0 lines">78.49</td>
</tr>
</table></div></figure> (Data from <a class="external" href="https://www.macrotrends.net/countries/USA/united-states/life-expectancy" target="_blank">macrotrends</a><a href="" data-knowl="" class="id-ref fn-knowl original" data-refid="hk-fn-46" id="fn-46"><sup> 46 </sup></a>.)</div>
<article class="task exercise-like" id="task-1464"><h6 class="heading"><span class="codenumber">(i)</span></h6>
<p id="p-4450">Use <a href="" class="xref" data-knowl="./knowl/eq_ls_example.html" title="Equation 26.9">(26.9)</a> to find the least squares linear fit to the data set.</p>
<div class="solution solution-like" id="solution-158">
<h6 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h6>
<p id="p-4451">A linear fit to the data will be provided by the least squares solution to <span class="process-math">\(A \vx = \vb\text{,}\)</span> where</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A = \left[ \begin{array}{cc} 1\amp 1950 \\ 1\amp 1965 \\ 1 \amp 1980 \\  1\amp 1995 \\ 1 \amp 2010 \end{array}  \right], \ \vx = \left[ \begin{array}{c} a\\b \end{array}  \right], \ \text{ and }  \ \vb =  \left[ \begin{array}{c} 68.14\\70.21\\73.70\\75.98  \\78.49 \end{array}  \right]\text{.}
\end{equation*}
</div>
<p class="continuation">Technology shows that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\left(A^{\tr}A\right)^{-1}A^{\tr} \vb \approx [-276.1000 \ 0.1765]^{\tr}\text{.}
\end{equation*}
</div>
</div></article><article class="task exercise-like" id="task-1465"><h6 class="heading"><span class="codenumber">(ii)</span></h6>
<p id="p-4452">Use appropriate technology to find the QR factorization of an appropriate matrix <span class="process-math">\(A\text{,}\)</span> and use the QR decomposition to find the least squares linear fit to the data. Compare to what you found in part i.</p>
<div class="solution solution-like" id="solution-159">
<h6 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h6>
<p id="p-4453">Technology shows that <span class="process-math">\(A = QR\text{,}\)</span> where</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
Q = \left[  \begin{array}{cr} \frac{1}{\sqrt{5}}\amp -\frac{2}{\sqrt{10}} \\ \frac{1}{\sqrt{5}}\amp -\frac{1}{\sqrt{10}} \\ \frac{1}{\sqrt{5}}\amp 0 \\ \frac{1}{\sqrt{5}}\amp \frac{1}{\sqrt{10}} \\ \frac{1}{\sqrt{5}}\amp \frac{2}{\sqrt{10}} \end{array}  \right] \text{ and }  R = \left[ \begin{array}{cc} \sqrt{5}\amp 1980 \sqrt{5} \\ 0\amp 15 \sqrt{10} \end{array}  \right]\text{.}
\end{equation*}
</div>
<p class="continuation">Then we have that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
R^{-1} Q^{\tr} \vb \approx [-276.1000 \ 0.1765]^{\tr}\text{,}
\end{equation*}
</div>
<p class="continuation">just as in part i.</p>
</div></article></article></article></section><section class="section" id="sec_ls_summ"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber"></span> <span class="title">Summary</span>
</h3>
<ul class="disc">
<li id="li-706"><p id="p-4454">A least squares approximation to <span class="process-math">\(A \vx = \vb\)</span> is found by orthogonally projecting <span class="process-math">\(\vb\)</span> onto <span class="process-math">\(\Col A\text{.}\)</span></p></li>
<li id="li-707"><p id="p-4455">If the columns of <span class="process-math">\(A\)</span> are linearly independent, then the least squares approximation to <span class="process-math">\(A \vx = \vb\)</span> is <span class="process-math">\(\left(A^{\tr}A\right)^{-1}A^{\tr} \vb\text{.}\)</span></p></li>
<li id="li-708">
<p id="p-4456">The least squares solution to <span class="process-math">\(A \vx = \vb\text{,}\)</span> where <span class="process-math">\(A\vx = [y_1 \ y_2 \ \ldots \ y_m]\)</span> and <span class="process-math">\(\vb = [b_1 \ b_2 \ \ldots \ b_ m]^{\tr}\text{,}\)</span> minimizes the distance <span class="process-math">\(||A\vx - \vb||\text{,}\)</span> where</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
||A\vx - \vb||^2 = (y_1-b_1)^2 + (y_2-b_2)^2 + \cdots + (y_m-b_m)^2\text{.}
\end{equation*}
</div>
<p class="continuation">So the least squares solution minimizes a sum of squares.</p>
</li>
</ul></section><section class="exercises" id="sec_ls_exer"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber"></span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-257"><h4 class="heading"><span class="codenumber">1<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-432">
<p id="p-4457">The University of Denver Infant Study Center investigated whether babies take longer to learn to crawl in cold months, when they are often bundled in clothes that restrict their movement, than in warmer months. The study sought a relationship between babies' first crawling age and the average temperature during the month they first try to crawl (about 6 months after birth). Some of the data from the study is in <a href="" class="xref" data-knowl="./knowl/T_7_d_infants.html" title="Table 26.12: Crawling age">Table 26.12</a>. Let <span class="process-math">\(x\)</span> represent the temperature in degrees Fahrenheit and <span class="process-math">\(C(x)\)</span> the average crawling age in months.</p>
<figure class="table table-like" id="T_7_d_infants"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">26.12<span class="period">.</span></span><span class="space"> </span>Crawling age</figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="l m b1 r2 l1 t1 lines"><span class="process-math">\(x\)</span></td>
<td class="l m b1 r1 l0 t1 lines">33</td>
<td class="l m b1 r1 l0 t1 lines">37</td>
<td class="l m b1 r1 l0 t1 lines">48</td>
<td class="l m b1 r1 l0 t1 lines">57</td>
</tr>
<tr>
<td class="l m b1 r2 l1 t0 lines"><span class="process-math">\(C(x)\)</span></td>
<td class="l m b1 r1 l0 t0 lines">33.83</td>
<td class="l m b1 r1 l0 t0 lines">33.35</td>
<td class="l m b1 r1 l0 t0 lines">33.38</td>
<td class="l m b1 r1 l0 t0 lines">32.32</td>
</tr>
</table></div></figure>
</div>
<article class="task exercise-like" id="task-1466"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4458">Find the least squares line to fit this data. Plot the data and your line on the same set of axes. (We aren't concerned about whether a linear fit is really a good choice outside of this data set, we just fit a line to it to see what happens.)</p></article><article class="task exercise-like" id="task-1467"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-4460">Use your least squares line to predict the average crawling age when the temperature is 65.</p></article></article><article class="exercise exercise-like" id="exercise-258"><h4 class="heading"><span class="codenumber">2<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-433">
<p id="p-4462">The cost, in cents, of a first class postage stamp in years from 1981 to 1995 is shown in <a href="" class="xref" data-knowl="./knowl/T_7_d_stamps.html" title="Table 26.13: Cost of postage">Table 26.13</a>.</p>
<figure class="table table-like" id="T_7_d_stamps"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">26.13<span class="period">.</span></span><span class="space"> </span>Cost of postage</figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="c m b1 r2 l1 t1 lines">Year</td>
<td class="c m b1 r1 l0 t1 lines">1981</td>
<td class="c m b1 r1 l0 t1 lines">1985</td>
<td class="c m b1 r1 l0 t1 lines">1988</td>
<td class="c m b1 r1 l0 t1 lines">1991</td>
<td class="c m b1 r1 l0 t1 lines">1995</td>
</tr>
<tr>
<td class="c m b1 r2 l1 t0 lines">Cost</td>
<td class="c m b1 r1 l0 t0 lines">20</td>
<td class="c m b1 r1 l0 t0 lines">22</td>
<td class="c m b1 r1 l0 t0 lines">25</td>
<td class="c m b1 r1 l0 t0 lines">29</td>
<td class="c m b1 r1 l0 t0 lines">32</td>
</tr>
</table></div></figure>
</div>
<article class="task exercise-like" id="task-1468"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4463">Find the least squares line to fit this data. Plot the data and your line on the same set of axes.</p></article><article class="task exercise-like" id="task-1469"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-4464">Now find the least squares quadratic approximation to this data. Plot the quadratic function on same axes as your linear function.</p></article><article class="task exercise-like" id="task-1470"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-4465">Use your least squares line and quadratic to predict the cost of a postage stamp in this year. Look up the cost of a stamp today and determine how accurate your prediction is. Which function gives a better approximation? Provide reasons for any discrepancies.</p></article></article><article class="exercise exercise-like" id="exercise-259"><h4 class="heading"><span class="codenumber">3<span class="period">.</span></span></h4>
<p id="p-4466">According to <span class="booktitle">The Song of Insects</span> by G.W. Pierce (Harvard College Press, 1948) the sound of striped ground crickets chirping, in number of chirps per second, is related to the temperature. So the number of chirps per second could be a predictor of temperature. The data Pierce collected is shown in the table and scatterplot below, where <span class="process-math">\(x\)</span> is the (average) number of chirps per second and <span class="process-math">\(y\)</span> is the temperature in degrees Fahrenheit.</p>
<div class="sidebyside"><div class="sbsrow" style="margin-left:0%;margin-right:0%;">
<div class="sbspanel fixed-width top" style="width:50%;"><table class="tabular">
<tr>
<td class="c m b1 r1 l1 t1 lines"><span class="process-math">\(x\)</span></td>
<td class="c m b1 r1 l0 t1 lines"><span class="process-math">\(y\)</span></td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">20.0</td>
<td class="c m b1 r1 l0 t0 lines">88.6</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">16.0</td>
<td class="c m b1 r1 l0 t0 lines">71.6</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">19.8</td>
<td class="c m b1 r1 l0 t0 lines">93.3</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">18.4</td>
<td class="c m b1 r1 l0 t0 lines">84.3</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">17.1</td>
<td class="c m b1 r1 l0 t0 lines">80.6</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">15.5</td>
<td class="c m b1 r1 l0 t0 lines">75.2</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">14.7</td>
<td class="c m b1 r1 l0 t0 lines">69.7</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">17.1</td>
<td class="c m b1 r1 l0 t0 lines">82.0</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">15.4</td>
<td class="c m b1 r1 l0 t0 lines">69.4</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">16.2</td>
<td class="c m b1 r1 l0 t0 lines">83.3</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">15.0</td>
<td class="c m b1 r1 l0 t0 lines">79.6</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">17.2</td>
<td class="c m b1 r1 l0 t0 lines">82.6</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">16.0</td>
<td class="c m b1 r1 l0 t0 lines">80.6</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">17.0</td>
<td class="c m b1 r1 l0 t0 lines">83.5</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines">14.4</td>
<td class="c m b1 r1 l0 t0 lines">76.3</td>
</tr>
</table></div>
<div class="sbspanel top" style="width:50%;"><img src="external/10_7_crickets.svg" role="img" class="contained"></div>
</div></div>
<p id="p-4467">The relationship between <span class="process-math">\(x\)</span> and <span class="process-math">\(y\)</span> is not exactly linear, but looks to have a linear pattern. It could be that the relationship is really linear but experimental error causes the data to be slightly inaccurate. Or perhaps the data is not linear, but only approximately linear. Find the least squares linear approximation to the data.</p></article><article class="exercise exercise-like" id="exercise-260"><h4 class="heading"><span class="codenumber">4<span class="period">.</span></span></h4>
<p id="p-4469">We showed that if the columns of <span class="process-math">\(A\)</span> are linearly independent, then <span class="process-math">\(A^{\tr}A\)</span> is invertible. Show that the reverse implication is also true. That is, show that if <span class="process-math">\(A^{\tr}A\)</span> is invertible, then the columns of <span class="process-math">\(A\)</span> are linearly independent.</p></article><article class="exercise exercise-like" id="ex_6_f_not_li"><h4 class="heading"><span class="codenumber">5<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-434"><p id="p-4470">Consider the small data set of points <span class="process-math">\(S = \{(2,1), (2,2), (2,3)\}\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1471"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4471">Find a linear system <span class="process-math">\(A \vx = \vb\)</span> whose solution would define a least squares linear approximation to the data in set <span class="process-math">\(S\text{.}\)</span></p></article><article class="task exercise-like" id="task-1472"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-4473">Explain what happens when we attempt to find the least squares solution <span class="process-math">\(\vx^*\)</span> using the matrix <span class="process-math">\(\left(A^{\tr}A\right)^{-1}A^{\tr}\text{.}\)</span> Why does this happen?</p></article><article class="task exercise-like" id="task-1473"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-4475">Does the system <span class="process-math">\(A \vx = \vb\)</span> have a least squares solution? If so, how many and what are they? If not, why not?</p></article><article class="task exercise-like" id="task-1474"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<p id="p-4477">Fit a linear function of the form <span class="process-math">\(x = a_0 + a_1y\)</span> to the data. Why should you have expected the answer?</p></article></article><article class="exercise exercise-like" id="ex_6_f_ranks"><h4 class="heading"><span class="codenumber">6<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-435"><p id="p-4479">Let <span class="process-math">\(M\)</span> and <span class="process-math">\(N\)</span> be any matrices such that <span class="process-math">\(MN\)</span> is defined. In this exercise we investigate relationships between ranks of various matrices.</p></div>
<article class="task exercise-like" id="task-1475"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4480">Show that <span class="process-math">\(\Col MN\)</span> is a subspace of <span class="process-math">\(\Col M\text{.}\)</span> Use this result to explain why <span class="process-math">\(\rank(MN) \leq \rank(M)\text{.}\)</span></p></article><article class="task exercise-like" id="task-1476"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-4481">Show that <span class="process-math">\(\rank\left(M^{\tr}M\right) = \rank(M) = \rank\left(M^{\tr}\right)\text{.}\)</span></p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-51" id="hint-51"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-51"><div class="hint solution-like"><p id="p-4482">For part, see Exercise 12 in Section 15.</p></div></div>
</div></article><article class="task exercise-like" id="task-1477"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-4483">Show that <span class="process-math">\(\rank(MN) \leq \min\{\rank(M), \rank(N)\}\text{.}\)</span></p></article></article><article class="exercise exercise-like" id="exercise-263"><h4 class="heading"><span class="codenumber">7<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-436">
<p id="p-4484">We have seen that if the columns of a matrix <span class="process-math">\(M\)</span> are linearly independent, then</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\va^* = \left(M^{\tr}M\right)^{-1}M^{\tr} \vb
\end{equation*}
</div>
<p class="continuation">is a least squares solution to <span class="process-math">\(M \va = \vy\text{.}\)</span> What if the columns of <span class="process-math">\(M\)</span> are linearly dependent? From Activity 26.1, a least squares solution to <span class="process-math">\(M \va = \vy\)</span> is a solution to the equation <span class="process-math">\(\left(M^{\tr}M\right)\va = M^{\tr}\vy\text{.}\)</span> In this exercise we demonstrate that <span class="process-math">\(\left(M^{\tr}M\right)\va = M^{\tr}\vy\)</span> always has a solution.</p>
</div>
<article class="task exercise-like" id="task-1478"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4485">Explain why it is enough to show that the rank of the augmented matrix <span class="process-math">\([M^{\tr}M \ | \ M^{\tr}\vy]\)</span> is the same as the rank of <span class="process-math">\(M^{\tr}M\text{.}\)</span></p></article><article class="task exercise-like" id="task-1479"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-4487">Explain why <span class="process-math">\(\rank\left([M^{\tr}M \ | \ M^{\tr}\vy]\right) \geq \rank(M)\text{.}\)</span></p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-52" id="hint-52"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-52"><div class="hint solution-like"><p id="p-4488">See <a href="" class="xref" data-knowl="./knowl/ex_6_f_ranks.html" title="Exercise 6">Exercise 6</a>.</p></div></div>
</div></article><article class="task exercise-like" id="task-1480"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-4489">Explain why <span class="process-math">\([M^{\tr}M \ | \ M^{\tr}\vy] = M^{\tr}[M \ | \ \vy]\text{.}\)</span></p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-53" id="hint-53"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-53"><div class="hint solution-like"><p id="p-4490">Use the definition of the matrix product.</p></div></div>
</div></article><article class="task exercise-like" id="task-1481"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<p id="p-4491">Explain why <span class="process-math">\(\rank\left([M^{\tr}M \ | \ M^{\tr}\vy]\right) \leq \rank\left(M^{\tr} \right)\text{.}\)</span></p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-54" id="hint-54"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-54"><div class="hint solution-like"><p id="p-4492">See <a href="" class="xref" data-knowl="./knowl/ex_6_f_ranks.html" title="Exercise 6">Exercise 6</a>.</p></div></div>
</div></article><article class="task exercise-like" id="task-1482"><h5 class="heading"><span class="codenumber">(e)</span></h5>
<p id="p-4493">Finally, explain why <span class="process-math">\(\rank\left([M^{\tr}M \ | \ M^{\tr}\vy]\right) = \rank\left(M^{\tr}M\right)\text{.}\)</span></p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-55" id="hint-55"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-55"><div class="hint solution-like"><p id="p-4494">Combine parts (b) and (d).</p></div></div>
</div></article></article><article class="exercise exercise-like" id="exercise-264"><h4 class="heading"><span class="codenumber">8<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-437"><p id="p-4495">If <span class="process-math">\(A\)</span> is an <span class="process-math">\(m \times n\)</span> matrix with linearly independent columns, the least squares solution <span class="process-math">\(\vx^* = \left(A^{\tr}A\right)^{-1}A^{\tr} \vb\)</span> to <span class="process-math">\(A \vx = \vb\)</span> has the property that <span class="process-math">\(A \vx^* = A\left(A^{\tr}A\right)^{-1}A^{\tr} \vb\)</span> is the vector in <span class="process-math">\(\Col A\)</span> that is closest to <span class="process-math">\(\vb\text{.}\)</span> That is, <span class="process-math">\(A\left(A^{\tr}A\right)^{-1}A^{\tr} \vb\)</span> is the projection of <span class="process-math">\(\vb\)</span> onto <span class="process-math">\(\Col A\text{.}\)</span> The matrix <span class="process-math">\(P = A\left(A^{\tr}A\right)^{-1}A^{\tr}\)</span> is called a <dfn class="terminology">projection matrix</dfn>. Projection matrices have special properties.</p></div>
<article class="task exercise-like" id="task-1483"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4496">Show that <span class="process-math">\(P^2 = P = P^{\tr}\text{.}\)</span></p></article><article class="task exercise-like" id="task-1484"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-4497">In general, we define projection matrices as follows. <article class="definition definition-like" id="definition-59"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">26.14</span><span class="period">.</span>
</h6>
<p id="p-4498">A square matrix <span class="process-math">\(E\)</span> is a <dfn class="terminology">projection matrix</dfn> if <span class="process-math">\(E^2 = E\text{.}\)</span></p></article> Show that <span class="process-math">\(E = \left[ \begin{array}{cc} 0\amp 1\\ 0\amp 1 \end{array} \right]\)</span> is a projection matrix. Onto what does <span class="process-math">\(E\)</span> project?</p></article><article class="task exercise-like" id="task-1485"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-4499">Notice that the projection matrix from part (b) is not an orthogonal matrix. <article class="definition definition-like" id="definition-60"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">26.15</span><span class="period">.</span>
</h6>
<p id="p-4500">A square matrix <span class="process-math">\(E\)</span> is a <dfn class="terminology">orthogonal projection matrix</dfn> if <span class="process-math">\(E^2 = E = E^{\tr}\text{.}\)</span></p></article> Show that <span class="process-math">\(E = \left[ \begin{array}{cc} \frac{1}{2}\amp \frac{1}{2} \\ \frac{1}{2}\amp \frac{1}{2} \end{array} \right]\)</span> is an orthogonal projection matrix. Onto what does <span class="process-math">\(E\)</span> project?</p></article><article class="task exercise-like" id="task-1486"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<p id="p-4501">If <span class="process-math">\(E\)</span> is an <span class="process-math">\(n \times n\)</span> orthogonal projection matrix, show that if <span class="process-math">\(\vb\)</span> is in <span class="process-math">\(\R^n\text{,}\)</span> then <span class="process-math">\(\vb - E\vb\)</span> is orthogonal to every vector in <span class="process-math">\(\Col E\text{.}\)</span> (Hence, <span class="process-math">\(E\)</span> projects orthogonally onto <span class="process-math">\(\Col E\text{.}\)</span>)</p></article><article class="task exercise-like" id="task-1487"><h5 class="heading"><span class="codenumber">(e)</span></h5>
<p id="p-4502">Recall the projection <span class="process-math">\(\proj_{\vv} \vu\)</span> of a vector <span class="process-math">\(\vu\)</span> in the direction of a vector <span class="process-math">\(\vv\)</span> is give by <span class="process-math">\(\proj_{\vv} \vu = \frac{\vu \cdot \vv}{\vv \cdot \vv} \vv\text{.}\)</span> Show that <span class="process-math">\(\proj_{\vv} \vu = E\vu\text{,}\)</span> where <span class="process-math">\(E\)</span> is the orthogonal projection matrix <span class="process-math">\(\frac{1}{\vv \cdot \vv} \left(\vv \vv^{\tr} \right)\text{.}\)</span> Illustrate with <span class="process-math">\(\vv = [1 \ 1]^{\tr}\text{.}\)</span></p></article></article><article class="exercise exercise-like" id="exercise-265"><h4 class="heading"><span class="codenumber">9<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-438"><p id="p-4503">Label each of the following statements as True or False. Provide justification for your response.</p></div>
<article class="task exercise-like" id="task-1488"><h5 class="heading">
<span class="codenumber">(a)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-4504">If the columns of <span class="process-math">\(A\)</span> are linearly independent, then there is a unique least squares solution to <span class="process-math">\(A\vx = \vb\text{.}\)</span></p></article><article class="task exercise-like" id="task-1489"><h5 class="heading">
<span class="codenumber">(b)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-4506">Let <span class="process-math">\(\{\vv_1, \vv_2, \ldots, \vv_n\}\)</span> be an orthonormal basis for <span class="process-math">\(\R^n\text{.}\)</span> The least squares solution to <span class="process-math">\([\vv_1 \ \vv_2 \ \cdots \ \vv_{n-1}] \vx = \vv_n\)</span> is <span class="process-math">\(\vx = \vzero\text{.}\)</span></p></article><article class="task exercise-like" id="task-1490"><h5 class="heading">
<span class="codenumber">(c)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-4507">The least squares line to the data points <span class="process-math">\((0,0)\text{,}\)</span> <span class="process-math">\((1,0)\text{,}\)</span> and <span class="process-math">\((0,1)\)</span> is <span class="process-math">\(y=x\text{.}\)</span></p></article><article class="task exercise-like" id="task-1491"><h5 class="heading">
<span class="codenumber">(d)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-4509">If the columns of a matrix <span class="process-math">\(A\)</span> are not invertible and the vector <span class="process-math">\(\vb\)</span> is not in <span class="process-math">\(\Col A\text{,}\)</span> then there is no least squares solution to <span class="process-math">\(A \vx = \vb\text{.}\)</span></p></article><article class="task exercise-like" id="task-1492"><h5 class="heading">
<span class="codenumber">(e)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-4510">Every matrix equation of the form <span class="process-math">\(A \vx = \vb\)</span> has a least squares solution.</p></article><article class="task exercise-like" id="task-1493"><h5 class="heading">
<span class="codenumber">(f)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-4512">If the columns of <span class="process-math">\(A\)</span> are linearly independent, then the least squares solution to <span class="process-math">\(A \vx = \vb\)</span> is the orthogonal projection of <span class="process-math">\(\vb\)</span> onto <span class="process-math">\(\Col A\text{.}\)</span></p></article></article></section><section class="section" id="sec_proj_ls_approx_other"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber"></span> <span class="title">Project: Other Least Squares Approximations</span>
</h3>
<p id="p-4513">In this section we learned how to fit a polynomial function to a set of data in the least squares sense. But data takes on many forms, so it is important to be able to fit other types of functions to data sets. We investigate three different types of regression problems in this project.</p>
<article class="project project-like" id="project-82"><h4 class="heading">
<span class="type">Project Activity</span><span class="space"> </span><span class="codenumber">26.5</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-439">
<p id="p-4514">The length of a species of fish is to be represented as a function of the age and water temperature as shown in the table on the next page.<a href="" data-knowl="" class="id-ref fn-knowl original" data-refid="hk-fn-47" id="fn-47"><sup> 47 </sup></a> The fish are kept in tanks at 25, 27, 29 and 31 degrees Celsius. After birth, a test specimen is chosen at random every 14 days and its length measured. The data include:</p>
<ul class="disc">
<li id="li-709"><p id="p-4515"><span class="process-math">\(I\text{,}\)</span> the index;</p></li>
<li id="li-710"><p id="p-4516"><span class="process-math">\(x\text{,}\)</span> the age of the fish in days;</p></li>
<li id="li-711"><p id="p-4517"><span class="process-math">\(y\text{,}\)</span> the water temperature in degrees Celsius;</p></li>
<li id="li-712"><p id="p-4518"><span class="process-math">\(z\text{,}\)</span> the length of the fish.</p></li>
</ul>
<p id="p-4519">Since there are three variables in the data, we cannot perform a simple linear regression. Instead, we seek a model of the form</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
f(x,y) = ax+by+c
\end{equation*}
</div>
<p class="continuation">to fit the data, where <span class="process-math">\(f(x,y)\)</span> approximates the length. That is, we seek the best fit plane to the data. This is an example of what is called multiple linear regression. A scatterplot of the data, along with the best fit plane, is also shown.</p>
</div>
<article class="task exercise-like" id="task-1494"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4520">As we did when we fit polynomials to data, we start by considering what would happen if all of our data points satisfied our model function. In this case our data points have the form <span class="process-math">\((x_1,y_1,z_1)\text{,}\)</span> <span class="process-math">\((x_2,y_2,z_2)\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\((x_m,y_m,z_m)\text{.}\)</span> Explain what system of linear equations would result if the data points actually satisfy our model function <span class="process-math">\(f(x,y)= ax+by+c\text{.}\)</span> (You don't need to write 44 different equations, just explain the general form of the system.)</p></article><article class="task exercise-like" id="task-1495"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-4521">Write the system from (a) in the form <span class="process-math">\(M \va = \vz\text{,}\)</span> and specifically identify the matrix <span class="process-math">\(M\)</span> and the vectors <span class="process-math">\(\va\)</span> and <span class="process-math">\(\vz\text{.}\)</span></p></article><article class="task exercise-like" id="task-1496"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-4522">The same derivation as with the polynomial regression models shows that the vector <span class="process-math">\(\va^*\)</span> that minimizes <span class="process-math">\(||\vz - M\va||\)</span> is found by</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\va^* = \left(M^{\tr}M\right)^{-1}M^{\tr}\vz\text{,}
\end{equation*}
</div>
<p class="continuation">Use this to find the least squares fit of the form <span class="process-math">\(f(x,y) = ax+by+c\)</span> to the data.</p></article><article class="task exercise-like" id="task-1497"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<p id="p-4523">Provide a numeric measure of how well this model function fits the data. Explain.</p></article></article><div class="sidebyside"><div class="sbsrow" style="margin-left:0%;margin-right:0%;">
<div class="sbspanel fixed-width top" style="width:50%;"><table class="tabular">
<tr>
<td class="c m b2 r0 l0 t0 lines">Index</td>
<td class="c m b2 r0 l0 t0 lines">Age</td>
<td class="c m b2 r0 l0 t0 lines">Temp (<span class="process-math">\(^\circ\)</span>C)</td>
<td class="c m b2 r0 l0 t0 lines">Length</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">1</td>
<td class="c m b0 r0 l0 t0 lines">14</td>
<td class="c m b0 r0 l0 t0 lines">25</td>
<td class="c m b0 r0 l0 t0 lines">620</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">2</td>
<td class="c m b0 r0 l0 t0 lines">28</td>
<td class="c m b0 r0 l0 t0 lines">25</td>
<td class="c m b0 r0 l0 t0 lines">1315</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">3</td>
<td class="c m b0 r0 l0 t0 lines">41</td>
<td class="c m b0 r0 l0 t0 lines">25</td>
<td class="c m b0 r0 l0 t0 lines">2120</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">4</td>
<td class="c m b0 r0 l0 t0 lines">55</td>
<td class="c m b0 r0 l0 t0 lines">25</td>
<td class="c m b0 r0 l0 t0 lines">2600</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">5</td>
<td class="c m b0 r0 l0 t0 lines">69</td>
<td class="c m b0 r0 l0 t0 lines">25</td>
<td class="c m b0 r0 l0 t0 lines">3110</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">6</td>
<td class="c m b0 r0 l0 t0 lines">83</td>
<td class="c m b0 r0 l0 t0 lines">25</td>
<td class="c m b0 r0 l0 t0 lines">3535</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">7</td>
<td class="c m b0 r0 l0 t0 lines">97</td>
<td class="c m b0 r0 l0 t0 lines">25</td>
<td class="c m b0 r0 l0 t0 lines">3935</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">8</td>
<td class="c m b0 r0 l0 t0 lines">111</td>
<td class="c m b0 r0 l0 t0 lines">25</td>
<td class="c m b0 r0 l0 t0 lines">4465</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">9</td>
<td class="c m b0 r0 l0 t0 lines">125</td>
<td class="c m b0 r0 l0 t0 lines">25</td>
<td class="c m b0 r0 l0 t0 lines">4530</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">10</td>
<td class="c m b0 r0 l0 t0 lines">139</td>
<td class="c m b0 r0 l0 t0 lines">25</td>
<td class="c m b0 r0 l0 t0 lines">4570</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">11</td>
<td class="c m b0 r0 l0 t0 lines">153</td>
<td class="c m b0 r0 l0 t0 lines">25</td>
<td class="c m b0 r0 l0 t0 lines">4600</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">12</td>
<td class="c m b0 r0 l0 t0 lines">14</td>
<td class="c m b0 r0 l0 t0 lines">27</td>
<td class="c m b0 r0 l0 t0 lines">625</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">13</td>
<td class="c m b0 r0 l0 t0 lines">28</td>
<td class="c m b0 r0 l0 t0 lines">27</td>
<td class="c m b0 r0 l0 t0 lines">1215</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">14</td>
<td class="c m b0 r0 l0 t0 lines">41</td>
<td class="c m b0 r0 l0 t0 lines">27</td>
<td class="c m b0 r0 l0 t0 lines">2110</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">15</td>
<td class="c m b0 r0 l0 t0 lines">55</td>
<td class="c m b0 r0 l0 t0 lines">27</td>
<td class="c m b0 r0 l0 t0 lines">2805</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">16</td>
<td class="c m b0 r0 l0 t0 lines">69</td>
<td class="c m b0 r0 l0 t0 lines">27</td>
<td class="c m b0 r0 l0 t0 lines">3255</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">17</td>
<td class="c m b0 r0 l0 t0 lines">83</td>
<td class="c m b0 r0 l0 t0 lines">27</td>
<td class="c m b0 r0 l0 t0 lines">4015</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">18</td>
<td class="c m b0 r0 l0 t0 lines">97</td>
<td class="c m b0 r0 l0 t0 lines">27</td>
<td class="c m b0 r0 l0 t0 lines">4315</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">19</td>
<td class="c m b0 r0 l0 t0 lines">111</td>
<td class="c m b0 r0 l0 t0 lines">27</td>
<td class="c m b0 r0 l0 t0 lines">4495</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">20</td>
<td class="c m b0 r0 l0 t0 lines">125</td>
<td class="c m b0 r0 l0 t0 lines">27</td>
<td class="c m b0 r0 l0 t0 lines">4535</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">21</td>
<td class="c m b0 r0 l0 t0 lines">139</td>
<td class="c m b0 r0 l0 t0 lines">27</td>
<td class="c m b0 r0 l0 t0 lines">4600</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">22</td>
<td class="c m b0 r0 l0 t0 lines">153</td>
<td class="c m b0 r0 l0 t0 lines">27</td>
<td class="c m b0 r0 l0 t0 lines">4600</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">23</td>
<td class="c m b0 r0 l0 t0 lines">14</td>
<td class="c m b0 r0 l0 t0 lines">29</td>
<td class="c m b0 r0 l0 t0 lines">590</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">24</td>
<td class="c m b0 r0 l0 t0 lines">28</td>
<td class="c m b0 r0 l0 t0 lines">29</td>
<td class="c m b0 r0 l0 t0 lines">1305</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">25</td>
<td class="c m b0 r0 l0 t0 lines">41</td>
<td class="c m b0 r0 l0 t0 lines">29</td>
<td class="c m b0 r0 l0 t0 lines">2140</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">26</td>
<td class="c m b0 r0 l0 t0 lines">55</td>
<td class="c m b0 r0 l0 t0 lines">29</td>
<td class="c m b0 r0 l0 t0 lines">2890</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">27</td>
<td class="c m b0 r0 l0 t0 lines">69</td>
<td class="c m b0 r0 l0 t0 lines">29</td>
<td class="c m b0 r0 l0 t0 lines">3920</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">28</td>
<td class="c m b0 r0 l0 t0 lines">83</td>
<td class="c m b0 r0 l0 t0 lines">29</td>
<td class="c m b0 r0 l0 t0 lines">3920</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">29</td>
<td class="c m b0 r0 l0 t0 lines">97</td>
<td class="c m b0 r0 l0 t0 lines">29</td>
<td class="c m b0 r0 l0 t0 lines">4515</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">30</td>
<td class="c m b0 r0 l0 t0 lines">111</td>
<td class="c m b0 r0 l0 t0 lines">29</td>
<td class="c m b0 r0 l0 t0 lines">4520</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">31</td>
<td class="c m b0 r0 l0 t0 lines">125</td>
<td class="c m b0 r0 l0 t0 lines">29</td>
<td class="c m b0 r0 l0 t0 lines">4525</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">32</td>
<td class="c m b0 r0 l0 t0 lines">139</td>
<td class="c m b0 r0 l0 t0 lines">29</td>
<td class="c m b0 r0 l0 t0 lines">4565</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">33</td>
<td class="c m b0 r0 l0 t0 lines">153</td>
<td class="c m b0 r0 l0 t0 lines">29</td>
<td class="c m b0 r0 l0 t0 lines">4566</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">34</td>
<td class="c m b0 r0 l0 t0 lines">14</td>
<td class="c m b0 r0 l0 t0 lines">31</td>
<td class="c m b0 r0 l0 t0 lines">590</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">35</td>
<td class="c m b0 r0 l0 t0 lines">28</td>
<td class="c m b0 r0 l0 t0 lines">31</td>
<td class="c m b0 r0 l0 t0 lines">1205</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">36</td>
<td class="c m b0 r0 l0 t0 lines">41</td>
<td class="c m b0 r0 l0 t0 lines">31</td>
<td class="c m b0 r0 l0 t0 lines">1915</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">37</td>
<td class="c m b0 r0 l0 t0 lines">55</td>
<td class="c m b0 r0 l0 t0 lines">31</td>
<td class="c m b0 r0 l0 t0 lines">2140</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">38</td>
<td class="c m b0 r0 l0 t0 lines">69</td>
<td class="c m b0 r0 l0 t0 lines">31</td>
<td class="c m b0 r0 l0 t0 lines">2710</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">39</td>
<td class="c m b0 r0 l0 t0 lines">83</td>
<td class="c m b0 r0 l0 t0 lines">31</td>
<td class="c m b0 r0 l0 t0 lines">3020</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">40</td>
<td class="c m b0 r0 l0 t0 lines">97</td>
<td class="c m b0 r0 l0 t0 lines">31</td>
<td class="c m b0 r0 l0 t0 lines">3030</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">41</td>
<td class="c m b0 r0 l0 t0 lines">111</td>
<td class="c m b0 r0 l0 t0 lines">31</td>
<td class="c m b0 r0 l0 t0 lines">3040</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">42</td>
<td class="c m b0 r0 l0 t0 lines">125</td>
<td class="c m b0 r0 l0 t0 lines">31</td>
<td class="c m b0 r0 l0 t0 lines">3180</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">43</td>
<td class="c m b0 r0 l0 t0 lines">139</td>
<td class="c m b0 r0 l0 t0 lines">31</td>
<td class="c m b0 r0 l0 t0 lines">3257</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">44</td>
<td class="c m b0 r0 l0 t0 lines">153</td>
<td class="c m b0 r0 l0 t0 lines">31</td>
<td class="c m b0 r0 l0 t0 lines">3214</td>
</tr>
</table></div>
<div class="sbspanel top" style="width:50%;"><img src="external/Multi_linear.png" class="contained"></div>
</div></div>
<article class="project project-like" id="project-83"><h4 class="heading">
<span class="type">Project Activity</span><span class="space"> </span><span class="codenumber">26.6</span><span class="period">.</span>
</h4>
<p id="p-4524">Population growth is typically not well modeled by polynomial functions. Populations tend to grow at rates proportional to the population, which implies exponential growth. For example, <a href="" class="xref" data-knowl="./knowl/T_7_d_US_population.html" title="Table 26.16: U.S. population">Table 26.16</a> shows the approximate population of the United States in years between 1920 and 2000, with the population measured in millions.</p>
<figure class="table table-like" id="T_7_d_US_population"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">26.16<span class="period">.</span></span><span class="space"> </span>U.S. population</figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="l m b1 r2 l1 t1 lines">Year</td>
<td class="c m b1 r1 l0 t1 lines">1920</td>
<td class="c m b1 r1 l0 t1 lines">1930</td>
<td class="c m b1 r1 l0 t1 lines">1940</td>
<td class="c m b1 r1 l0 t1 lines">1950</td>
<td class="c m b1 r1 l0 t1 lines">1960</td>
<td class="c m b1 r1 l0 t1 lines">1970</td>
<td class="c m b1 r1 l0 t1 lines">1980</td>
<td class="c m b1 r1 l0 t1 lines">1990</td>
<td class="c m b1 r1 l0 t1 lines">2000</td>
</tr>
<tr>
<td class="l m b1 r2 l1 t0 lines">Population</td>
<td class="c m b1 r1 l0 t0 lines">106</td>
<td class="c m b1 r1 l0 t0 lines">123</td>
<td class="c m b1 r1 l0 t0 lines">142</td>
<td class="c m b1 r1 l0 t0 lines">161</td>
<td class="c m b1 r1 l0 t0 lines">189</td>
<td class="c m b1 r1 l0 t0 lines">213</td>
<td class="c m b1 r1 l0 t0 lines">237</td>
<td class="c m b1 r1 l0 t0 lines">259</td>
<td class="c m b1 r1 l0 t0 lines">291</td>
</tr>
</table></div></figure><p id="p-4525">If we assume the population grows exponentially, we would want to find the best fit function <span class="process-math">\(f\)</span> of the form <span class="process-math">\(f(t) = ae^{kt}\text{,}\)</span> where <span class="process-math">\(a\)</span> and <span class="process-math">\(k\)</span> are constants. However, an exponential function is not linear. So to apply the methods we have developed, we could instead apply the natural logarithm to both sides of <span class="process-math">\(y = ae^{kt}\)</span> to obtain the equation <span class="process-math">\(\ln(y) = \ln(a) + kt\text{.}\)</span> We can then find the best fit line to the data in the form <span class="process-math">\((t, \ln(y))\)</span> to determine the values of <span class="process-math">\(\ln(a)\)</span> and <span class="process-math">\(k\text{.}\)</span> Use this approach to find the best fit exponential function in the least squares sense to the U.S. population data. Then look up the U.S. population in 2010 (include your source) and compare to the estimate given by your model function. If your prediction is not very close, give some plausible explanations for the difference.</p></article><figure class="figure figure-like" id="F_ellipse"><div class="image-box" style="width: 30%; margin-left: 35%; margin-right: 35%;"><img src="external/ellipse_plot.svg" role="img" class="contained"></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">26.17<span class="period">.</span></span><span class="space"> </span>Best fit ellipse.</figcaption></figure><article class="project project-like" id="project-84"><h4 class="heading">
<span class="type">Project Activity</span><span class="space"> </span><span class="codenumber">26.7</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-440">
<p id="p-4526">Carl Friedrich Gauss is often credited with inventing the method of least squares. He used the method to find a best-fit ellipse which allowed him to correctly predict the orbit of the asteroid Ceres as it passed behind the sun in 1801. (Adrien-Marie Legendre appears to be the first to publish the method, though.) Here we examine the problem of fitting an ellipse to data.</p>
<p id="p-4527">An ellipse is a quadratic equation that can be written in the form</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="eq_ellipse">
\begin{equation}
x^2 + By^2 + Cxy + Dx + Ey + F = 0\tag{26.10}
\end{equation}
</div>
<p class="continuation">for constants <span class="process-math">\(B\text{,}\)</span> <span class="process-math">\(C\text{,}\)</span> <span class="process-math">\(D\text{,}\)</span> <span class="process-math">\(E\text{,}\)</span> and <span class="process-math">\(F\text{,}\)</span> with <span class="process-math">\(B &gt; 0\text{.}\)</span> We will find the best-fit ellipse in the least squares sense through the points</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
(0,2), \ (2,1), \ (1, -1), \ (-1, -2), \ (-3,1), \ \text{ and }  \ (-1, -1)\text{.}
\end{equation*}
</div>
<p id="p-4528">A picture of the best fit ellipse is shown in <a href="" class="xref" data-knowl="./knowl/F_ellipse.html" title="Figure 26.17">Figure 26.17</a>.</p>
</div>
<article class="task exercise-like" id="task-1498"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4529">Find the system of linear equations that would result if the ellipse <a href="" class="xref" data-knowl="./knowl/eq_ellipse.html" title="Equation 26.10">(26.10)</a> were to exactly pass through the given points.</p></article><article class="task exercise-like" id="task-1499"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-4530">Write the linear system from part (a) in the form <span class="process-math">\(A \vx = \vb\text{,}\)</span> where the vector <span class="process-math">\(\vx\)</span> contains the unknowns in the system. Clearly identify <span class="process-math">\(A\text{,}\)</span> <span class="process-math">\(\vx\text{,}\)</span> and <span class="process-math">\(\vb\text{.}\)</span></p></article><article class="task exercise-like" id="task-1500"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-4531">Find the least squares ellipse to this set of points. Make sure your method is clear. (Note that we are really fitting a surface of the form <span class="process-math">\(f(x,y) = x^2 + By^2 + Cxy + Dx + Ey + F\)</span> to a set of data points in the <span class="process-math">\(xy\)</span>-plane. So the error is the sum of the vertical distances from the points in the <span class="process-math">\(xy\)</span>-plane to the surface.)</p></article></article></section></section><div class="hidden-content tex2jax_ignore" id="hk-fn-45"><div class="fn"><code class="code-inline tex2jax_ignore">cdc.gov/growthcharts/html_charts/lenageinf.htm</code></div></div>
<div class="hidden-content tex2jax_ignore" id="hk-fn-46"><div class="fn"><code class="code-inline tex2jax_ignore">macrotrends.net/countries/USA/united-states/life-expectancy</code></div></div>
<div class="hidden-content tex2jax_ignore" id="hk-fn-47"><div class="fn"> Data from <span class="booktitle">Mathematical Algorithms for Linear Regression</span>, Helmut Spaeth, Academic Press, 1991, page 305, ISBN 0-12-656460-4.</div></div>
</div></main>
</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/components/prism-core.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/autoloader/prism-autoloader.min.js"></script>
</body>
</html>
