<!DOCTYPE html>
<html lang="en-US">
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="robots" content="noindex, nofollow">
</head>
<body class="ignore-math">
<h4 class="heading"><span class="type">Paragraph</span></h4>
<p>In <a href="chap_gram_schmidt.html" class="internal" title="Section 25: Projections onto Subspaces and the Gram-Schmidt Process in \R^n">Section 25</a> we saw that the projection of a vector <span class="process-math">\(\vv\)</span> in <span class="process-math">\(\R^n\)</span> onto a subspace <span class="process-math">\(W\)</span> of <span class="process-math">\(\R^n\)</span> is the best approximation to <span class="process-math">\(\vv\)</span> of all the vectors in <span class="process-math">\(W\text{.}\)</span> In fact, if <span class="process-math">\(\vv = [v_1 \ v_2 \ \ldots \ v_n]^{\tr}\)</span> and <span class="process-math">\(\proj_W \vv =  [w_1 \ w_2 \ w_3 \ \ldots \ w_m]^{\tr}\text{,}\)</span> then the error in approximating <span class="process-math">\(\vv\)</span> by <span class="process-math">\(\vw\)</span> is given by</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/chap_gram_schmidt.html ./knowl/pa_LS_1.html ./knowl/F_6_f_proj.html ./knowl/F_LS_Olympics2.html">
\begin{equation*}
|| \vv - \proj_W \vv ||^2 = \sum_{i=1}^m (v_i - w_i)^2\text{.}
\end{equation*}
</div>
<p class="continuation">In the context of <a href="" class="xref" data-knowl="./knowl/pa_LS_1.html" title="Preview Activity 26.1">Preview Activity 26.1</a>, we projected the vector <span class="process-math">\(\vb\)</span> onto the span of  the vectors <span class="process-math">\(\vv_1 = [1 \ 1 \ 1 \ 1]^{\tr}\)</span> and <span class="process-math">\(\vv_2 = [0 \ 4 \ 8 \ 12]^{\tr}\text{.}\)</span> The projection   minimizes the distance between the vectors in <span class="process-math">\(W\)</span> and the vector <span class="process-math">\(\vb\)</span> (as shown in <a href="" class="xref" data-knowl="./knowl/F_6_f_proj.html" title="Figure 26.3">Figure 26.3</a>), and also produces a line which minimizes the sums of the squares of the vertical  distances from the line to the data set as illustrated in <a href="" class="xref" data-knowl="./knowl/F_LS_Olympics2.html" title="Figure 26.4">Figure 26.4</a> with the olympics data. This is why these approximations are called least squares approximations.</p>
<span class="incontext"><a href="chap_least_squares.html#p-4414" class="internal">in-context</a></span>
</body>
</html>
