%********************************************%
%*       Generated from PreTeXt source      *%
%*       on 2022-07-12T14:06:45-04:00       *%
%*   A recent stable commit (2020-08-09):   *%
%* 98f21740783f166a773df4dc83cab5293ab63a4a *%
%*                                          *%
%*         https://pretextbook.org          *%
%*                                          *%
%********************************************%
%% We elect to always write snapshot output into <job>.dep file
\RequirePackage{snapshot}
\documentclass[oneside,10pt,]{book}
%% Custom Preamble Entries, early (use latex.preamble.early)
%% Default LaTeX packages
%%   1.  always employed (or nearly so) for some purpose, or
%%   2.  a stylewriter may assume their presence
\usepackage{geometry}
%% Some aspects of the preamble are conditional,
%% the LaTeX engine is one such determinant
\usepackage{ifthen}
%% etoolbox has a variety of modern conveniences
\usepackage{etoolbox}
\usepackage{ifxetex,ifluatex}
%% Raster graphics inclusion
\usepackage{graphicx}
%% Color support, xcolor package
%% Always loaded, for: add/delete text, author tools
%% Here, since tcolorbox loads tikz, and tikz loads xcolor
\PassOptionsToPackage{usenames,dvipsnames,svgnames,table}{xcolor}
\usepackage{xcolor}
%% begin: defined colors, via xcolor package, for styling
%% end: defined colors, via xcolor package, for styling
%% Colored boxes, and much more, though mostly styling
%% skins library provides "enhanced" skin, employing tikzpicture
%% boxes may be configured as "breakable" or "unbreakable"
%% "raster" controls grids of boxes, aka side-by-side
\usepackage{tcolorbox}
\tcbuselibrary{skins}
\tcbuselibrary{breakable}
\tcbuselibrary{raster}
%% We load some "stock" tcolorbox styles that we use a lot
%% Placement here is provisional, there will be some color work also
%% First, black on white, no border, transparent, but no assumption about titles
\tcbset{ bwminimalstyle/.style={size=minimal, boxrule=-0.3pt, frame empty,
colback=white, colbacktitle=white, coltitle=black, opacityfill=0.0} }
%% Second, bold title, run-in to text/paragraph/heading
%% Space afterwards will be controlled by environment,
%% independent of constructions of the tcb title
%% Places \blocktitlefont onto many block titles
\tcbset{ runintitlestyle/.style={fonttitle=\blocktitlefont\upshape\bfseries, attach title to upper} }
%% Spacing prior to each exercise, anywhere
\tcbset{ exercisespacingstyle/.style={before skip={1.5ex plus 0.5ex}} }
%% Spacing prior to each block
\tcbset{ blockspacingstyle/.style={before skip={2.0ex plus 0.5ex}} }
%% xparse allows the construction of more robust commands,
%% this is a necessity for isolating styling and behavior
%% The tcolorbox library of the same name loads the base library
\tcbuselibrary{xparse}
%% The tcolorbox library loads TikZ, its calc package is generally useful,
%% and is necessary for some smaller documents that use partial tcolor boxes
%% See:  https://github.com/PreTeXtBook/pretext/issues/1624
\usetikzlibrary{calc}
%% Hyperref should be here, but likes to be loaded late
%%
%% Inline math delimiters, \(, \), need to be robust
%% 2016-01-31:  latexrelease.sty  supersedes  fixltx2e.sty
%% If  latexrelease.sty  exists, bugfix is in kernel
%% If not, bugfix is in  fixltx2e.sty
%% See:  https://tug.org/TUGboat/tb36-3/tb114ltnews22.pdf
%% and read "Fewer fragile commands" in distribution's  latexchanges.pdf
\IfFileExists{latexrelease.sty}{}{\usepackage{fixltx2e}}
%% shorter subnumbers in some side-by-side require manipulations
\usepackage{xstring}
%% Footnote counters and part/chapter counters are manipulated
%% April 2018:  chngcntr  commands now integrated into the kernel,
%% but circa 2018/2019 the package would still try to redefine them,
%% so we need to do the work of loading conditionally for old kernels.
%% From version 1.1a,  chngcntr  should detect defintions made by LaTeX kernel.
\ifdefined\counterwithin
\else
    \usepackage{chngcntr}
\fi
%% Text height identically 9 inches, text width varies on point size
%% See Bringhurst 2.1.1 on measure for recommendations
%% 75 characters per line (count spaces, punctuation) is target
%% which is the upper limit of Bringhurst's recommendations
\geometry{letterpaper,total={340pt,9.0in}}
%% Custom Page Layout Adjustments (use latex.geometry)
\geometry{top=1in, bottom=1in, outer=1in, inner=1in}
%% This LaTeX file may be compiled with pdflatex, xelatex, or lualatex executables
%% LuaTeX is not explicitly supported, but we do accept additions from knowledgeable users
%% The conditional below provides  pdflatex  specific configuration last
%% begin: engine-specific capabilities
\ifthenelse{\boolean{xetex} \or \boolean{luatex}}{%
%% begin: xelatex and lualatex-specific default configuration
\ifxetex\usepackage{xltxtra}\fi
%% realscripts is the only part of xltxtra relevant to lualatex 
\ifluatex\usepackage{realscripts}\fi
%% end:   xelatex and lualatex-specific default configuration
}{
%% begin: pdflatex-specific default configuration
%% We assume a PreTeXt XML source file may have Unicode characters
%% and so we ask LaTeX to parse a UTF-8 encoded file
%% This may work well for accented characters in Western language,
%% but not with Greek, Asian languages, etc.
%% When this is not good enough, switch to the  xelatex  engine
%% where Unicode is better supported (encouraged, even)
\usepackage[utf8]{inputenc}
%% end: pdflatex-specific default configuration
}
%% end:   engine-specific capabilities
%%
%% Fonts.  Conditional on LaTex engine employed.
%% Default Text Font: The Latin Modern fonts are
%% "enhanced versions of the [original TeX] Computer Modern fonts."
%% We use them as the default text font for PreTeXt output.
%% Default Monospace font: Inconsolata (aka zi4)
%% Sponsored by TUG: http://levien.com/type/myfonts/inconsolata.html
%% Loaded for documents with intentional objects requiring monospace
%% See package documentation for excellent instructions
%% fontspec will work universally if we use filename to locate OTF files
%% Loads the "upquote" package as needed, so we don't have to
%% Upright quotes might come from the  textcomp  package, which we also use
%% We employ the shapely \ell to match Google Font version
%% pdflatex: "varl" package option produces shapely \ell
%% pdflatex: "var0" package option produces plain zero (not used)
%% pdflatex: "varqu" package option produces best upright quotes
%% xelatex,lualatex: add OTF StylisticSet 1 for shapely \ell
%% xelatex,lualatex: add OTF StylisticSet 2 for plain zero (not used)
%% xelatex,lualatex: add OTF StylisticSet 3 for upright quotes
%%
%% Automatic Font Control
%% Portions of a document, are, or may, be affected by defined commands
%% These are perhaps more flexible when using  xelatex  rather than  pdflatex
%% The following definitions are meant to be re-defined in a style, using \renewcommand
%% They are scoped when employed (in a TeX group), and so should not be defined with an argument
\newcommand{\divisionfont}{\relax}
\newcommand{\blocktitlefont}{\relax}
\newcommand{\contentsfont}{\relax}
\newcommand{\pagefont}{\relax}
\newcommand{\tabularfont}{\relax}
\newcommand{\xreffont}{\relax}
\newcommand{\titlepagefont}{\relax}
%%
\ifthenelse{\boolean{xetex} \or \boolean{luatex}}{%
%% begin: font setup and configuration for use with xelatex
%% Generally, xelatex is necessary for non-Western fonts
%% fontspec package provides extensive control of system fonts,
%% meaning *.otf (OpenType), and apparently *.ttf (TrueType)
%% that live *outside* your TeX/MF tree, and are controlled by your *system*
%% (it is possible that a TeX distribution will place fonts in a system location)
%%
%% The fontspec package is the best vehicle for using different fonts in  xelatex
%% So we load it always, no matter what a publisher or style might want
%%
\usepackage{fontspec}
%%
%% begin: xelatex main font ("font-xelatex-main" template)
%% Latin Modern Roman is the default font for xelatex and so is loaded with a TU encoding
%% *in the format* so we can't touch it, only perhaps adjust it later
%% in one of two ways (then known by NFSS names such as "lmr")
%% (1) via NFSS with font family names such as "lmr" and "lmss"
%% (2) via fontspec with commands like \setmainfont{Latin Modern Roman}
%% The latter requires the font to be known at the system-level by its font name,
%% but will give access to OTF font features through optional arguments
%% https://tex.stackexchange.com/questions/470008/
%% where-and-how-does-fontspec-sty-specify-the-default-font-latin-modern-roman
%% http://tex.stackexchange.com/questions/115321
%% /how-to-optimize-latin-modern-font-with-xelatex
%%
%% end:   xelatex main font ("font-xelatex-main" template)
%% begin: xelatex mono font ("font-xelatex-mono" template)
%% (conditional on non-trivial uses being present in source)
\IfFontExistsTF{Inconsolatazi4-Regular.otf}{}{\GenericError{}{The font "Inconsolatazi4-Regular.otf" requested by PreTeXt output is not available.  Either a file cannot be located in default locations via a filename, or a font is not known by its name as part of your system.}{Consult the PreTeXt Guide for help with LaTeX fonts.}{}}
\IfFontExistsTF{Inconsolatazi4-Bold.otf}{}{\GenericError{}{The font "Inconsolatazi4-Bold.otf" requested by PreTeXt output is not available.  Either a file cannot be located in default locations via a filename, or a font is not known by its name as part of your system.}{Consult the PreTeXt Guide for help with LaTeX fonts.}{}}
\usepackage{zi4}
\setmonofont[BoldFont=Inconsolatazi4-Bold.otf,StylisticSet={1,3}]{Inconsolatazi4-Regular.otf}
%% end:   xelatex mono font ("font-xelatex-mono" template)
%% begin: xelatex font adjustments ("font-xelatex-style" template)
%% end:   xelatex font adjustments ("font-xelatex-style" template)
%%
%% Extensive support for other languages
\usepackage{polyglossia}
%% Set main/default language based on pretext/@xml:lang value
%% document language code is "en-US", US English
%% usmax variant has extra hypenation
\setmainlanguage[variant=usmax]{english}
%% Enable secondary languages based on discovery of @xml:lang values
%% Enable fonts/scripts based on discovery of @xml:lang values
%% Western languages should be ably covered by Latin Modern Roman
%% end:   font setup and configuration for use with xelatex
}{%
%% begin: font setup and configuration for use with pdflatex
%% begin: pdflatex main font ("font-pdflatex-main" template)
\usepackage{lmodern}
\usepackage[T1]{fontenc}
%% end:   pdflatex main font ("font-pdflatex-main" template)
%% begin: pdflatex mono font ("font-pdflatex-mono" template)
%% (conditional on non-trivial uses being present in source)
\usepackage[varqu,varl]{inconsolata}
%% end:   pdflatex mono font ("font-pdflatex-mono" template)
%% begin: pdflatex font adjustments ("font-pdflatex-style" template)
%% end:   pdflatex font adjustments ("font-pdflatex-style" template)
%% end:   font setup and configuration for use with pdflatex
}
%% Micromanage spacing, etc.  The named "microtype-options"
%% template may be employed to fine-tune package behavior
\usepackage{microtype}
%% Symbols, align environment, commutative diagrams, bracket-matrix
\usepackage{amsmath}
\usepackage{amscd}
\usepackage{amssymb}
%% allow page breaks within display mathematics anywhere
%% level 4 is maximally permissive
%% this is exactly the opposite of AMSmath package philosophy
%% there are per-display, and per-equation options to control this
%% split, aligned, gathered, and alignedat are not affected
\allowdisplaybreaks[4]
%% allow more columns to a matrix
%% can make this even bigger by overriding with  latex.preamble.late  processing option
\setcounter{MaxMatrixCols}{30}
%%
%%
%% Division Titles, and Page Headers/Footers
%% titlesec package, loading "titleps" package cooperatively
%% See code comments about the necessity and purpose of "explicit" option.
%% The "newparttoc" option causes a consistent entry for parts in the ToC 
%% file, but it is only effective if there is a \titleformat for \part.
%% "pagestyles" loads the  titleps  package cooperatively.
\usepackage[explicit, newparttoc, pagestyles]{titlesec}
%% The companion titletoc package for the ToC.
\usepackage{titletoc}
%% Fixes a bug with transition from chapters to appendices in a "book"
%% See generating XSL code for more details about necessity
\newtitlemark{\chaptertitlename}
%% begin: customizations of page styles via the modal "titleps-style" template
%% Designed to use commands from the LaTeX "titleps" package
%% Plain pages should have the same font for page numbers
%% Custom template to change chapter and section titles in header
%% No longer in all caps or italicized
\renewpagestyle{plain}{%
\setfoot{}{\pagefont\thepage}{}%
}%
%% Two-page spread as in default LaTeX
%% Custom
\renewpagestyle{headings}{%
\sethead%
[\pagefont\thepage]%
[]
[\pagefont{\ifthechapter{\chaptertitlename\space\thechapter.\space}{}\chaptertitle}]%
{\pagefont{\ifthesection{\thesection.\space\sectiontitle}{}}}%
{}%
{\pagefont\thepage}\setheadrule{.8pt}%
}%
\pagestyle{headings}
%% end: customizations of page styles via the modal "titleps-style" template
%%
%% Create globally-available macros to be provided for style writers
%% These are redefined for each occurence of each division
\newcommand{\divisionnameptx}{\relax}%
\newcommand{\titleptx}{\relax}%
\newcommand{\subtitleptx}{\relax}%
\newcommand{\shortitleptx}{\relax}%
\newcommand{\authorsptx}{\relax}%
\newcommand{\epigraphptx}{\relax}%
%% Create environments for possible occurences of each division
%% Environment for a PTX "preface" at the level of a LaTeX "chapter"
\NewDocumentEnvironment{preface}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Preface}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\chapter*{#1}%
\addcontentsline{toc}{chapter}{#3}
\label{#6}%
}{}%
%% Environment for a PTX "part" at the level of a LaTeX "part"
\NewDocumentEnvironment{partptx}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Chapter}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\part[{#3}]{#1}%
\label{#6}%
}{}%
%% Environment for a PTX "chapter" at the level of a LaTeX "chapter"
\NewDocumentEnvironment{chapterptx}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Section}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\chapter[{#3}]{#1}%
\label{#6}%
}{}%
%% Environment for a PTX "section" at the level of a LaTeX "section"
\NewDocumentEnvironment{sectionptx}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Subsection}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\section[{#3}]{#1}%
\label{#6}%
}{}%
%% Environment for a PTX "exercises" at the level of a LaTeX "section"
\NewDocumentEnvironment{exercises-section}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Exercises}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\section[{#3}]{#1}%
\label{#6}%
}{}%
%% Environment for a PTX "exercises" at the level of a LaTeX "section"
\NewDocumentEnvironment{exercises-section-numberless}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Exercises}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\section*{#1}%
\addcontentsline{toc}{section}{#3}
\label{#6}%
}{}%
%% Environment for a PTX "subsection" at the level of a LaTeX "subsection"
\NewDocumentEnvironment{subsectionptx}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Subsection}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\subsection[{#3}]{#1}%
\label{#6}%
}{}%
%% Environment for a PTX "appendix" at the level of a LaTeX "chapter"
\NewDocumentEnvironment{appendixptx}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Appendix}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\chapter[{#3}]{#1}%
\label{#6}%
}{}%
%% Environment for a PTX "solutions" at the level of a LaTeX "chapter"
\NewDocumentEnvironment{solutions-chapter}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Appendix}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\chapter[{#3}]{#1}%
\label{#6}%
}{}%
%% Environment for a PTX "solutions" at the level of a LaTeX "chapter"
\NewDocumentEnvironment{solutions-chapter-numberless}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Appendix}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\chapter*{#1}%
\addcontentsline{toc}{chapter}{#3}
\label{#6}%
}{}%
%% Environment for a PTX "index" at the level of a LaTeX "chapter"
\NewDocumentEnvironment{indexptx}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Index}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\chapter*{#1}%
\addcontentsline{toc}{chapter}{#3}
\label{#6}%
}{}%
%%
%% Styles for six traditional LaTeX divisions
\titleformat{\part}[display]
{\divisionfont\Huge\bfseries\centering}{\divisionnameptx\space\thepart}{30pt}{\Huge#1}
[{\Large\centering\authorsptx}]
\titleformat{\chapter}[display]
{\divisionfont\huge\bfseries}{\divisionnameptx\space\thechapter}{20pt}{\Huge#1}
[{\Large\authorsptx}]
\titleformat{name=\chapter,numberless}[display]
{\divisionfont\huge\bfseries}{}{0pt}{#1}
[{\Large\authorsptx}]
\titlespacing*{\chapter}{0pt}{50pt}{40pt}
\titleformat{\section}[hang]
{\divisionfont\Large\bfseries}{\thesection}{1ex}{#1}
[{\large\authorsptx}]
\titleformat{name=\section,numberless}[block]
{\divisionfont\Large\bfseries}{}{0pt}{#1}
[{\large\authorsptx}]
\titlespacing*{\section}{0pt}{3.5ex plus 1ex minus .2ex}{2.3ex plus .2ex}
\titleformat{\subsection}[hang]
{\divisionfont\large\bfseries}{\thesubsection}{1ex}{#1}
[{\normalsize\authorsptx}]
\titleformat{name=\subsection,numberless}[block]
{\divisionfont\large\bfseries}{}{0pt}{#1}
[{\normalsize\authorsptx}]
\titlespacing*{\subsection}{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\titleformat{\subsubsection}[hang]
{\divisionfont\normalsize\bfseries}{\thesubsubsection}{1em}{#1}
[{\small\authorsptx}]
\titleformat{name=\subsubsection,numberless}[block]
{\divisionfont\normalsize\bfseries}{}{0pt}{#1}
[{\normalsize\authorsptx}]
\titlespacing*{\subsubsection}{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\titleformat{\paragraph}[hang]
{\divisionfont\normalsize\bfseries}{\theparagraph}{1em}{#1}
[{\small\authorsptx}]
\titleformat{name=\paragraph,numberless}[block]
{\divisionfont\normalsize\bfseries}{}{0pt}{#1}
[{\normalsize\authorsptx}]
\titlespacing*{\paragraph}{0pt}{3.25ex plus 1ex minus .2ex}{1.5em}
%%
%% Styles for five traditional LaTeX divisions
\titlecontents{part}%
[0pt]{\contentsmargin{0em}\addvspace{1pc}\contentsfont\bfseries}%
{\Large\thecontentslabel\enspace}{\Large}%
{}%
[\addvspace{.5pc}]%
\titlecontents{chapter}%
[0pt]{\contentsmargin{0em}\addvspace{1pc}\contentsfont\bfseries}%
{\large\thecontentslabel\enspace}{\large}%
{\hfill\bfseries\thecontentspage}%
[\addvspace{.5pc}]%
\dottedcontents{section}[3.8em]{\contentsfont}{2.3em}{1pc}%
\dottedcontents{subsection}[6.1em]{\contentsfont}{3.2em}{1pc}%
\dottedcontents{subsubsection}[9.3em]{\contentsfont}{4.3em}{1pc}%
%%
%% Begin: Semantic Macros
%% To preserve meaning in a LaTeX file
%%
%% \mono macro for content of "c", "cd", "tag", etc elements
%% Also used automatically in other constructions
%% Simply an alias for \texttt
%% Always defined, even if there is no need, or if a specific tt font is not loaded
\newcommand{\mono}[1]{\texttt{#1}}
%%
%% Following semantic macros are only defined here if their
%% use is required only in this specific document
%%
%% Used to markup initialisms, text or titles
\newcommand{\initialism}[1]{\textsc{\MakeLowercase{#1}}}
\DeclareRobustCommand{\initialismintitle}[1]{\texorpdfstring{#1}{#1}}
%% Used for warnings, typically bold and italic
\newcommand{\alert}[1]{\textbf{\textit{#1}}}
%% Used for inline definitions of terms
\newcommand{\terminology}[1]{\textbf{#1}}
%% Titles of longer works (e.g. books, versus articles)
\newcommand{\pubtitle}[1]{\textsl{#1}}
%% Used for fillin answer blank in text
%% Relies on calc package, loaded via tcolorbox
%% Argument is intended number of characters of blank
%% Length may compress for output to fit in one line
\newlength{\fillinmaxwidth}
\newlength{\fillincontract}
\newlength{\fillinheight}
\newcommand{\fillintext}[1]{%
\setlength{\fillinmaxwidth}{#1em*\real{0.5}}%
\setlength{\fillincontract}{#1em*\real{0.5}*\real{0.2}}%
\setlength{\fillinheight}{\heightof{\strut}+1.2pt}%
\strut\nobreak\leaders\vbox{\hrule width 0.3pt height 0.3pt \vskip -1.2pt}\hskip 1\fillinmaxwidth minus \fillincontract\nobreak\strut%
}
%% Style of a title on a list item, for ordered and unordered lists
%% Also "task" of exercise, PROJECT-LIKE, EXAMPLE-LIKE
\newcommand{\lititle}[1]{{\slshape#1}}
%% End: Semantic Macros
%% begin: environments for duplicates in solutions divisions
%% Solutions to division exercises, not in exercise group
\tcbset{ divisionsolutionstyle/.style={bwminimalstyle, runintitlestyle, exercisespacingstyle, after title={\space}, breakable, parbox=false } }
\newtcolorbox{divisionsolution}[3]{divisionsolutionstyle, title={\hyperlink{#3}{#1}.\notblank{#2}{\space#2}{}}}
\tcbset{ explorationsolutionstyle/.style={bwminimalstyle, runintitlestyle, exercisespacingstyle, after title={\space}, breakable, parbox=false } }
\newtcolorbox{explorationsolution}[3]{explorationsolutionstyle, title={\hyperref[#3]{Preview Activity~#1}\notblank{#2}{\space#2}{}}}
\tcbset{ activitysolutionstyle/.style={bwminimalstyle, runintitlestyle, exercisespacingstyle, after title={\space}, breakable, parbox=false } }
\newtcolorbox{activitysolution}[3]{activitysolutionstyle, title={\hyperref[#3]{Activity~#1}\notblank{#2}{\space#2}{}}}
\tcbset{ projectsolutionstyle/.style={bwminimalstyle, runintitlestyle, exercisespacingstyle, after title={\space}, breakable, parbox=false } }
\newtcolorbox{projectsolution}[3]{projectsolutionstyle, title={\hyperref[#3]{Project Activity~#1}\notblank{#2}{\space#2}{}}}
%% Divisional exercises (and worksheet) as LaTeX environments
%% Third argument is option for extra workspace in worksheets
%% Hanging indent occupies a 5ex width slot prior to left margin
%% Experimentally this seems just barely sufficient for a bold "888."
%% Division exercises, not in exercise group
\tcbset{ divisionexercisestyle/.style={bwminimalstyle, runintitlestyle, exercisespacingstyle, left=5ex, breakable, parbox=false } }
\newtcolorbox{divisionexercise}[4]{divisionexercisestyle, before title={\hspace{-5ex}\makebox[5ex][l]{#1.}}, title={\notblank{#2}{#2\space}{}}, phantom={\hypertarget{#4}{}}, after={\notblank{#3}{\newline\rule{\workspacestrutwidth}{#3}\newline\vfill}{\par}}}
%% Localize LaTeX supplied names (possibly none)
\renewcommand*{\appendixname}{Appendix}
\renewcommand*{\partname}{Chapter}
\renewcommand*{\chaptername}{Section}
%% Equation Numbering
%% Controlled by  numbering.equations.level  processing parameter
%% No adjustment here implies document-wide numbering
\numberwithin{equation}{chapter}
%% "tcolorbox" environment for a single image, occupying entire \linewidth
%% arguments are left-margin, width, right-margin, as multiples of
%% \linewidth, and are guaranteed to be positive and sum to 1.0
\tcbset{ imagestyle/.style={bwminimalstyle} }
\NewTColorBox{image}{mmm}{imagestyle,left skip=#1\linewidth,width=#2\linewidth}
%% For improved tables
\usepackage{array}
%% Some extra height on each row is desirable, especially with horizontal rules
%% Increment determined experimentally
\setlength{\extrarowheight}{0.2ex}
%% Define variable thickness horizontal rules, full and partial
%% Thicknesses are 0.03, 0.05, 0.08 in the  booktabs  package
\newcommand{\hrulethin}  {\noalign{\hrule height 0.04em}}
\newcommand{\hrulemedium}{\noalign{\hrule height 0.07em}}
\newcommand{\hrulethick} {\noalign{\hrule height 0.11em}}
%% We preserve a copy of the \setlength package before other
%% packages (extpfeil) get a chance to load packages that redefine it
\let\oldsetlength\setlength
\newlength{\Oldarrayrulewidth}
\newcommand{\crulethin}[1]%
{\noalign{\global\oldsetlength{\Oldarrayrulewidth}{\arrayrulewidth}}%
\noalign{\global\oldsetlength{\arrayrulewidth}{0.04em}}\cline{#1}%
\noalign{\global\oldsetlength{\arrayrulewidth}{\Oldarrayrulewidth}}}%
\newcommand{\crulemedium}[1]%
{\noalign{\global\oldsetlength{\Oldarrayrulewidth}{\arrayrulewidth}}%
\noalign{\global\oldsetlength{\arrayrulewidth}{0.07em}}\cline{#1}%
\noalign{\global\oldsetlength{\arrayrulewidth}{\Oldarrayrulewidth}}}
\newcommand{\crulethick}[1]%
{\noalign{\global\oldsetlength{\Oldarrayrulewidth}{\arrayrulewidth}}%
\noalign{\global\oldsetlength{\arrayrulewidth}{0.11em}}\cline{#1}%
\noalign{\global\oldsetlength{\arrayrulewidth}{\Oldarrayrulewidth}}}
%% Single letter column specifiers defined via array package
\newcolumntype{A}{!{\vrule width 0.04em}}
\newcolumntype{B}{!{\vrule width 0.07em}}
\newcolumntype{C}{!{\vrule width 0.11em}}
%% tcolorbox to place tabular outside of a sidebyside
\tcbset{ tabularboxstyle/.style={bwminimalstyle,} }
\newtcolorbox{tabularbox}[3]{tabularboxstyle, left skip=#1\linewidth, width=#2\linewidth,}
%% Footnote Numbering
%% Specified by numbering.footnotes.level
%% Undo counter reset by chapter for a book
\counterwithout{footnote}{chapter}
%% Global numbering, since numbering.footnotes.level = 0
%% Program listing support: for listings, programs, consoles, and Sage code
\ifthenelse{\boolean{xetex} \or \boolean{luatex}}%
  {\tcbuselibrary{listings}}%
  {\tcbuselibrary{listingsutf8}}%
%% We define the listings font style to be the default "ttfamily"
%% To fix hyphens/dashes rendered in PDF as fancy minus signs by listing
%% http://tex.stackexchange.com/questions/33185/listings-package-changes-hyphens-to-minus-signs
\makeatletter
\lst@CCPutMacro\lst@ProcessOther {"2D}{\lst@ttfamily{-{}}{-{}}}
\@empty\z@\@empty
\makeatother
%% We define a null language, free of any formatting or style
%% for use when a language is not supported, or pseudo-code, or consoles
%% Not necessary for Sage code, so in limited cases included unnecessarily
\lstdefinelanguage{none}{identifierstyle=,commentstyle=,stringstyle=,keywordstyle=}
\ifthenelse{\boolean{xetex}}{}{%
%% begin: pdflatex-specific listings configuration
%% translate U+0080 - U+00F0 to their textmode LaTeX equivalents
%% Data originally from https://www.w3.org/Math/characters/unicode.xml, 2016-07-23
%% Lines marked in XSL with "$" were converted from mathmode to textmode
\lstset{extendedchars=true}
\lstset{literate={ }{{~}}{1}{¡}{{\textexclamdown }}{1}{¢}{{\textcent }}{1}{£}{{\textsterling }}{1}{¤}{{\textcurrency }}{1}{¥}{{\textyen }}{1}{¦}{{\textbrokenbar }}{1}{§}{{\textsection }}{1}{¨}{{\textasciidieresis }}{1}{©}{{\textcopyright }}{1}{ª}{{\textordfeminine }}{1}{«}{{\guillemotleft }}{1}{¬}{{\textlnot }}{1}{­}{{\-}}{1}{®}{{\textregistered }}{1}{¯}{{\textasciimacron }}{1}{°}{{\textdegree }}{1}{±}{{\textpm }}{1}{²}{{\texttwosuperior }}{1}{³}{{\textthreesuperior }}{1}{´}{{\textasciiacute }}{1}{µ}{{\textmu }}{1}{¶}{{\textparagraph }}{1}{·}{{\textperiodcentered }}{1}{¸}{{\c{}}}{1}{¹}{{\textonesuperior }}{1}{º}{{\textordmasculine }}{1}{»}{{\guillemotright }}{1}{¼}{{\textonequarter }}{1}{½}{{\textonehalf }}{1}{¾}{{\textthreequarters }}{1}{¿}{{\textquestiondown }}{1}{À}{{\`{A}}}{1}{Á}{{\'{A}}}{1}{Â}{{\^{A}}}{1}{Ã}{{\~{A}}}{1}{Ä}{{\"{A}}}{1}{Å}{{\AA }}{1}{Æ}{{\AE }}{1}{Ç}{{\c{C}}}{1}{È}{{\`{E}}}{1}{É}{{\'{E}}}{1}{Ê}{{\^{E}}}{1}{Ë}{{\"{E}}}{1}{Ì}{{\`{I}}}{1}{Í}{{\'{I}}}{1}{Î}{{\^{I}}}{1}{Ï}{{\"{I}}}{1}{Ð}{{\DH }}{1}{Ñ}{{\~{N}}}{1}{Ò}{{\`{O}}}{1}{Ó}{{\'{O}}}{1}{Ô}{{\^{O}}}{1}{Õ}{{\~{O}}}{1}{Ö}{{\"{O}}}{1}{×}{{\texttimes }}{1}{Ø}{{\O }}{1}{Ù}{{\`{U}}}{1}{Ú}{{\'{U}}}{1}{Û}{{\^{U}}}{1}{Ü}{{\"{U}}}{1}{Ý}{{\'{Y}}}{1}{Þ}{{\TH }}{1}{ß}{{\ss }}{1}{à}{{\`{a}}}{1}{á}{{\'{a}}}{1}{â}{{\^{a}}}{1}{ã}{{\~{a}}}{1}{ä}{{\"{a}}}{1}{å}{{\aa }}{1}{æ}{{\ae }}{1}{ç}{{\c{c}}}{1}{è}{{\`{e}}}{1}{é}{{\'{e}}}{1}{ê}{{\^{e}}}{1}{ë}{{\"{e}}}{1}{ì}{{\`{\i}}}{1}{í}{{\'{\i}}}{1}{î}{{\^{\i}}}{1}{ï}{{\"{\i}}}{1}{ð}{{\dh }}{1}{ñ}{{\~{n}}}{1}{ò}{{\`{o}}}{1}{ó}{{\'{o}}}{1}{ô}{{\^{o}}}{1}{õ}{{\~{o}}}{1}{ö}{{\"{o}}}{1}{÷}{{\textdiv }}{1}{ø}{{\o }}{1}{ù}{{\`{u}}}{1}{ú}{{\'{u}}}{1}{û}{{\^{u}}}{1}{ü}{{\"{u}}}{1}{ý}{{\'{y}}}{1}{þ}{{\th }}{1}{ÿ}{{\"{y}}}{1}}
%% end: pdflatex-specific listings configuration
}
%% End of generic listing adjustments
%% Program listings via new tcblisting environment
%% First a universal color scheme for parts of any language
%% Colors match a subset of Google prettify "Default" style
%% Full colors for "electronic" version
%% http://code.google.com/p/google-code-prettify/source/browse/trunk/src/prettify.css
\definecolor{identifiers}{rgb}{0.375,0,0.375}
\definecolor{comments}{rgb}{0.5,0,0}
\definecolor{strings}{rgb}{0,0.5,0}
\definecolor{keywords}{rgb}{0,0,0.5}
%% Options passed to the listings package via tcolorbox
\lstdefinestyle{programcodestyle}{identifierstyle=\color{identifiers},commentstyle=\color{comments},stringstyle=\color{strings},keywordstyle=\color{keywords}, breaklines=true, breakatwhitespace=true, columns=fixed, extendedchars=true, aboveskip=0pt, belowskip=0pt}
\tcbset{ programboxstyle/.style={left=3ex, right=0pt, top=0ex, bottom=0ex, middle=0pt, toptitle=0pt, bottomtitle=0pt, boxsep=0pt, 
listing only, fontupper=\small\ttfamily,
colback=white, sharp corners, boxrule=-0.3pt, leftrule=0.5pt, toprule at break=-0.3pt, bottomrule at break=-0.3pt,
breakable, parbox=false,
} }
\newtcblisting{program}[4]{programboxstyle, left skip=#2\linewidth, width=#3\linewidth, listing options={language=#1, style=programcodestyle}}
%% More flexible list management, esp. for references
%% But also for specifying labels (i.e. custom order) on nested lists
\usepackage{enumitem}
%% Description lists as tcolorbox sidebyside
%% "dli" short for "description list item"
\newlength{\dlititlewidth}
\newlength{\dlimaxnarrowtitle}\setlength{\dlimaxnarrowtitle}{11ex}
\newlength{\dlimaxmediumtitle}\setlength{\dlimaxmediumtitle}{18ex}
\tcbset{ dlistyle/.style={sidebyside, sidebyside align=top seam, lower separated=false, bwminimalstyle, bottomtitle=0.75ex, after skip=1.5ex, boxsep=0pt, left=0pt, right=0pt, top=0pt, bottom=0pt} }
\tcbset{ dlinarrowstyle/.style={dlistyle, lefthand width=\dlimaxnarrowtitle, sidebyside gap=1ex, halign=flush left, righttitle=10ex} }
\tcbset{ dlimediumstyle/.style={dlistyle, lefthand width=\dlimaxmediumtitle, sidebyside gap=4ex, halign=flush right} }
\NewDocumentEnvironment{descriptionlist}{}{\par\vspace*{1.5ex}}{\par\vspace*{1.5ex}}%
%% begin enviroment has an if/then to open the tcolorbox
\NewDocumentEnvironment{dlinarrow}{mm}{%
\settowidth{\dlititlewidth}{{\textbf{#1}}}%
\ifthenelse{\dlititlewidth > \dlimaxnarrowtitle}%
{\begin{tcolorbox}[title={\textbf{#1}}, phantom={\hypertarget{#2}{}}, dlinarrowstyle]\tcblower}%
{\begin{tcolorbox}[dlinarrowstyle, phantom={\hypertarget{#2}{}}]\textbf{#1}\tcblower}%
}%
{\end{tcolorbox}}%
%% medium option is simpler
\NewDocumentEnvironment{dlimedium}{mm}%
{\begin{tcolorbox}[dlimediumstyle, phantom={\hypertarget{#2}{}}]\textbf{#1}\tcblower}%
{\end{tcolorbox}}%
%% Support for index creation
%% imakeidx package does not require extra pass (as with makeidx)
%% Title of the "Index" section set via a keyword
%% Language support for the "see" and "see also" phrases
\usepackage{imakeidx}
\makeindex[title=Index, intoc=true]
\renewcommand{\seename}{See}
\renewcommand{\alsoname}{See also}
%% hyperref driver does not need to be specified, it will be detected
%% Footnote marks in tcolorbox have broken linking under
%% hyperref, so it is necessary to turn off all linking
%% It *must* be given as a package option, not with \hypersetup
\usepackage[hyperfootnotes=false]{hyperref}
%% configure hyperref's  \href{}{}  and  \nolinkurl  to match listings' inline verbatim
\renewcommand\UrlFont{\small\ttfamily}
%% Hyperlinking active in electronic PDFs, all links without surrounding boxes and blue
\hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue,filecolor=blue,urlcolor=blue}
\hypersetup{pdftitle={An Inquiry-Based Introduction to Linear Algebra and Applications}}
%% If you manually remove hyperref, leave in this next command
%% This will allow LaTeX compilation, employing this no-op command
\providecommand\phantomsection{}
%% Division Numbering: Chapters, Sections, Subsections, etc
%% Division numbers may be turned off at some level ("depth")
%% A section *always* has depth 1, contrary to us counting from the document root
%% The latex default is 3.  If a larger number is present here, then
%% removing this command may make some cross-references ambiguous
%% The precursor variable $numbering-maxlevel is checked for consistency in the common XSL file
\setcounter{secnumdepth}{0}
%%
%% AMS "proof" environment is no longer used, but we leave previously
%% implemented \qedhere in place, should the LaTeX be recycled
\newcommand{\qedhere}{\relax}
%%
%% A faux tcolorbox whose only purpose is to provide common numbering
%% facilities for most blocks (possibly not projects, 2D displays)
%% Controlled by  numbering.theorems.level  processing parameter
\newtcolorbox[auto counter, number within=chapter]{block}{}
%%
%% This document is set to number PROJECT-LIKE on a separate numbering scheme
%% So, a faux tcolorbox whose only purpose is to provide this numbering
%% Controlled by  numbering.projects.level  processing parameter
\newtcolorbox[auto counter, number within=chapter]{project-distinct}{}
%% A faux tcolorbox whose only purpose is to provide common numbering
%% facilities for 2D displays which are subnumbered as part of a "sidebyside"
\makeatletter
\newtcolorbox[auto counter, number within=tcb@cnt@block, number freestyle={\noexpand\thetcb@cnt@block(\noexpand\alph{\tcbcounter})}]{subdisplay}{}
\makeatother
%%
%% tcolorbox, with styles, for THEOREM-LIKE
%%
%% theorem: fairly simple numbered block/structure
\tcbset{ theoremstyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, } }
\newtcolorbox[use counter from=block]{theorem}[3]{title={{Theorem~\thetcbcounter\notblank{#1#2}{\space}{}\notblank{#1}{\space#1}{}\notblank{#2}{\space(#2)}{}}}, phantomlabel={#3}, breakable, parbox=false, after={\par}, fontupper=\itshape, theoremstyle, }
%% lemma: fairly simple numbered block/structure
\tcbset{ lemmastyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, } }
\newtcolorbox[use counter from=block]{lemma}[3]{title={{Lemma~\thetcbcounter\notblank{#1#2}{\space}{}\notblank{#1}{\space#1}{}\notblank{#2}{\space(#2)}{}}}, phantomlabel={#3}, breakable, parbox=false, after={\par}, fontupper=\itshape, lemmastyle, }
%% corollary: fairly simple numbered block/structure
\tcbset{ corollarystyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, } }
\newtcolorbox[use counter from=block]{corollary}[3]{title={{Corollary~\thetcbcounter\notblank{#1#2}{\space}{}\notblank{#1}{\space#1}{}\notblank{#2}{\space(#2)}{}}}, phantomlabel={#3}, breakable, parbox=false, after={\par}, fontupper=\itshape, corollarystyle, }
%%
%% tcolorbox, with styles, for PROOF-LIKE
%%
%% proof: title is a replacement
\tcbset{ proofstyle/.style={bwminimalstyle, fonttitle=\blocktitlefont\itshape, attach title to upper, after title={\space}, after upper={\space\space\hspace*{\stretch{1}}\(\blacksquare\)},
} }
\newtcolorbox{proof}[2]{title={\notblank{#1}{#1}{Proof.}}, phantom={\hypertarget{#2}{}}, breakable, parbox=false, after={\par}, proofstyle }
%%
%% tcolorbox, with styles, for DEFINITION-LIKE
%%
%% definition: fairly simple numbered block/structure
\tcbset{ definitionstyle/.style={{bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, }} }
\newtcolorbox[use counter from=block]{definition}[2]{title={{Definition~\thetcbcounter\notblank{#1}{\space\space#1}{}}}, phantomlabel={#2}, breakable, parbox=false, after={\par}, definitionstyle, }
%%
%% tcolorbox, with styles, for REMARK-LIKE
%%
%% remark: fairly simple numbered block/structure
\tcbset{ remarkstyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, } }
\newtcolorbox[use counter from=block]{remark}[2]{title={{Reflection~\thetcbcounter\notblank{#1}{\space\space#1}{}}}, phantomlabel={#2}, breakable, parbox=false, after={\par}, remarkstyle, }
%%
%% tcolorbox, with styles, for EXAMPLE-LIKE
%%
%% example: fairly simple numbered block/structure
\tcbset{ examplestyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, after upper={\space\space\hspace*{\stretch{1}}\(\square\)}, } }
\newtcolorbox[use counter from=block]{example}[2]{title={{Example~\thetcbcounter\notblank{#1}{\space\space#1}{}}}, phantomlabel={#2}, breakable, parbox=false, after={\par}, examplestyle, }
%%
%% tcolorbox, with styles, for PROJECT-LIKE
%%
%% exploration: fairly simple numbered block/structure
\tcbset{ explorationstyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, } }
\newtcolorbox[use counter from=project-distinct]{exploration}[2]{title={{Preview Activity~\thetcbcounter\notblank{#1}{\space\space#1}{}}}, phantomlabel={#2}, breakable, parbox=false, after={\par}, explorationstyle, }
%% activity: fairly simple numbered block/structure
\tcbset{ activitystyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, } }
\newtcolorbox[use counter from=project-distinct]{activity}[2]{title={{Activity~\thetcbcounter\notblank{#1}{\space\space#1}{}}}, phantomlabel={#2}, breakable, parbox=false, after={\par}, activitystyle, }
%% project: fairly simple numbered block/structure
\tcbset{ projectstyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, } }
\newtcolorbox[use counter from=project-distinct]{project}[2]{title={{Project Activity~\thetcbcounter\notblank{#1}{\space\space#1}{}}}, phantomlabel={#2}, breakable, parbox=false, after={\par}, projectstyle, }
%%
%% tcolorbox, with styles, for GOAL-LIKE
%%
%% objectives: early in a subdivision, introduction/list/conclusion
\tcbset{ objectivesstyle/.style={{size=normal, colback=white, colbacktitle=white, coltitle=black, colframe=black, blockspacingstyle, sharp corners, fonttitle=\blocktitlefont\large\bfseries, titlerule=0ex, toprule=0.3ex, toptitle=0.5ex, top=2ex, bottom=0.5ex, bottomrule=0.3ex}} }
\newtcolorbox{objectives}[2]{title={#1}, phantomlabel={#2}, breakable, parbox=false, objectivesstyle}
%%
%% tcolorbox, with styles, for FIGURE-LIKE
%%
\tcbset{ figureptxstyle/.style={bwminimalstyle, middle=1ex, blockspacingstyle, fontlower=\blocktitlefont} }
\newtcolorbox[use counter from=block]{figureptx}[3]{lower separated=false, before lower=\begin{center} {{\textbf{Figure~\thetcbcounter}\space#1}} \end{center}, phantomlabel={#2}, unbreakable, parbox=false, figureptxstyle, }%% tableptx: 2-D display structure
\tcbset{ tableptxstyle/.style={bwminimalstyle, middle=1ex, blockspacingstyle, coltitle=black, bottomtitle=2ex, titlerule=-0.3pt, fonttitle=\blocktitlefont} }
\newtcolorbox[use counter from=block]{tableptx}[3]{title={{\textbf{Table~\thetcbcounter}\space#1}}, phantomlabel={#2}, unbreakable, parbox=false, tableptxstyle, }
%%
%% xparse environments for introductions and conclusions of divisions
%%
%% introduction: in a structured division
\NewDocumentEnvironment{introduction}{m}
{\notblank{#1}{\noindent\textbf{#1}\space}{}}{\par\medskip}
%%
%% tcolorbox, with styles, for miscellaneous environments
%%
%% paragraphs: the terminal, pseudo-division
%% We use the lowest LaTeX traditional division
\titleformat{\subparagraph}[runin]{\normalfont\normalsize\bfseries}{\thesubparagraph}{1em}{#1}
\titlespacing*{\subparagraph}{0pt}{3.25ex plus 1ex minus .2ex}{1em}
\NewDocumentEnvironment{paragraphs}{mm}
{\subparagraph*{#1}\hypertarget{#2}{}}{}
%% Graphics Preamble Entries
\usepackage{tikz}
\usepackage{tkz-graph}
\usepackage{tkz-euclide}
\usetikzlibrary{patterns}
\usetikzlibrary{positioning}
\usetikzlibrary{matrix,arrows}
\usetikzlibrary{calc}
\usetikzlibrary{shapes}
\usetikzlibrary{through,intersections,decorations,shadows,fadings}
\usepackage{pgfplots}
\usepackage{graphics}
\setlength{\unitlength}{0.75cm}
%% If tikz has been loaded, replace ampersand with \amp macro
%% tcolorbox styles for sidebyside layout
\tcbset{ sbsstyle/.style={raster before skip=2.0ex, raster equal height=rows, raster force size=false} }
\tcbset{ sbspanelstyle/.style={bwminimalstyle, fonttitle=\blocktitlefont} }
%% Enviroments for side-by-side and components
%% Necessary to use \NewTColorBox for boxes of the panels
%% "newfloat" environment to squash page-breaks within a single sidebyside
%% "xparse" environment for entire sidebyside
\NewDocumentEnvironment{sidebyside}{mmmm}
  {\begin{tcbraster}
    [sbsstyle,raster columns=#1,
    raster left skip=#2\linewidth,raster right skip=#3\linewidth,raster column skip=#4\linewidth]}
  {\end{tcbraster}}
%% "tcolorbox" environment for a panel of sidebyside
\NewTColorBox{sbspanel}{mO{top}}{sbspanelstyle,width=#1\linewidth,valign=#2}
%% extpfeil package for certain extensible arrows,
%% as also provided by MathJax extension of the same name
%% NB: this package loads mtools, which loads calc, which redefines
%%     \setlength, so it can be removed if it seems to be in the 
%%     way and your math does not use:
%%     
%%     \xtwoheadrightarrow, \xtwoheadleftarrow, \xmapsto, \xlongequal, \xtofrom
%%     
%%     we have had to be extra careful with variable thickness
%%     lines in tables, and so also load this package late
\usepackage{extpfeil}
%% Custom Preamble Entries, late (use latex.preamble.late)
%This should load all the style information that ptx does not.
\usepackage{setspace} % Add line break before and after some elements
\AtBeginEnvironment{example}{\vskip\baselineskip}
\AfterEndEnvironment{example}{\vskip\baselineskip}
\AfterEndEnvironment{divisionexercise}{\vskip\baselineskip}
\AtBeginEnvironment{definition}{\vskip\baselineskip}
\AfterEndEnvironment{definition}{\vskip\baselineskip}
\setlength{\parskip}{0.27\baselineskip}

%% Begin: Author-provided packages
%% (From  docinfo/latex-preamble/package  elements)
\usepackage{colortbl}%% End: Author-provided packages
%% Begin: Author-provided macros
%% (From  docinfo/macros  element)
%% Plus three from PTX for XML characters
\usepackage{amsmath}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\E}{\mathbb{E}}
\DeclareMathOperator{\ch}{char}
\newcommand{\N}{\mathbb{N}}
\newcommand{\W}{\mathbb{W}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\J}{\mathbb{J}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathbb{D}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\NE}{\mathcal{E}}
\newcommand{\Mn}[1]{\mathcal{M}_{#1 \times #1}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\Rn}{\R^n}
\newcommand{\Mat}{\mathbf}
\newcommand{\Seq}{\boldsymbol}
\newcommand{\seq}[1]{\boldsymbol{#1}}
\newcommand{\set}[1]{\mathcal{#1}}
\newcommand{\abs}[1]{\left\lvert{}#1\right\rvert}
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\cq}{\scalebox{.34}{\pscirclebox{ \textbf{?}}}}
\newcommand{\cqup}{\,$^{\text{\scalebox{.2}{\pscirclebox{\textbf{?}}}}}$}
\newcommand{\cqupmath}{\,^{\text{\scalebox{.2}{\pscirclebox{\textbf{?}}}}}}
\newcommand{\cqupmathnospace}{^{\text{\scalebox{.2}{\pscirclebox{\textbf{?}}}}}}
\newcommand{\uspace}[1]{\underline{}}
\newcommand{\muspace}[1]{\underline{\mspace{#1 mu}}}
\newcommand{\bspace}[1]{}
\newcommand{\ie}{\emph{i}.\emph{e}.}
\newcommand{\nq}[1]{\scalebox{.34}{\pscirclebox{\textbf{#1}}}}
\newcommand{\equalwhy}{\stackrel{\cqupmath}{=}}
\newcommand{\notequalwhy}{\stackrel{\cqupmath}{\neq}}
\newcommand{\Ker}{\text{Ker}}
\newcommand{\Image}{\text{Im}}
\newcommand{\polyp}{p(x) = a_nx^n + a_{n-1}x^{n-1} + \cdots a_2x^2 + a_1x + a_0}
\newcommand{\GL}{\text{GL}}
\newcommand{\SL}{\text{SL}}
\newcommand{\lcm}{\text{lcm}}
\newcommand{\Aut}{\text{Aut}}
\newcommand{\Inn}{\text{Inn}}
\newcommand{\Hol}{\text{Hol}}
\newcommand{\cl}{\text{cl}}
\newcommand{\bsq}{\hfill $\blacksquare$}
\newcommand{\NIMdot}{{ $\cdot$ }}
\newcommand{\eG}{e_{\scriptscriptstyle{G}}}
\newcommand{\eGroup}[1]{e_{\scriptscriptstyle{#1}}}
\newcommand{\Gdot}[1]{\cdot_{\scriptscriptstyle{#1}}}
\newcommand{\rbar}{\overline{r}}
\newcommand{\nuG}[1]{\nu_{\scriptscriptstyle{#1}}}
\newcommand{\rightarray}[2]{\left[ \begin{array}{#1} #2 \end{array} \right]}
\newcommand{\polymod}[1]{\mspace{5 mu}(\text{mod} #1)}
\newcommand{\ts}{\mspace{2 mu}}
\newcommand{\ds}{\displaystyle}
\newcommand{\adj}{\text{adj}}
\newcommand{\pol}{\mathbb{P}}
\newcommand{\CS}{\mathcal{S}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\CB}{\mathcal{B}}
\renewcommand{\CD}{\mathcal{D}}
\newcommand{\CP}{\mathcal{P}}
\newcommand{\CQ}{\mathcal{Q}}
\newcommand{\CW}{\mathcal{W}}
\newcommand{\rank}{\text{rank}}
\newcommand{\nullity}{\text{nullity}}
\newcommand{\trace}{\text{trace}}
\newcommand{\Area}{\text{Area}}
\newcommand{\Vol}{\text{Vol}}
\newcommand{\cov}{\text{cov}}
\newcommand{\var}{\text{var}}
\newcommand{\va}{\mathbf{a}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\vc}{\mathbf{c}}
\newcommand{\vd}{\mathbf{d}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\vf}{\mathbf{f}}
\newcommand{\vg}{\mathbf{g}}
\newcommand{\vh}{\mathbf{h}}
\newcommand{\vm}{\mathbf{m}}
\newcommand{\vn}{\mathbf{n}}
\newcommand{\vp}{\mathbf{p}}
\newcommand{\vq}{\mathbf{q}}
\newcommand{\vr}{\mathbf{r}}
\newcommand{\vs}{\mathbf{s}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\vzero}{\mathbf{0}}
\newcommand{\vi}{\mathbf{i}}
\newcommand{\vj}{\mathbf{j}}
\newcommand{\vk}{\mathbf{k}}
\newcommand{\vF}{\mathbf{F}}
\newcommand{\vP}{\mathbf{P}}
\newcommand{\nin}{}
\newcommand{\Dom}{\text{Dom}}
\newcommand{\tr}{\mathsf{T}}
\newcommand{\proj}{\text{proj}}
\newcommand{\comp}{\text{comp}}
\newcommand{\Row}{\text{Row }}
\newcommand{\Col}{\text{Col }}
\newcommand{\Nul}{\text{Nul }}
\newcommand{\Span}{\text{Span}}
\newcommand{\Range}{\text{Range}}
\newcommand{\Domain}{\text{Domain}}
\newcommand{\hthin}{\hlinewd{.1pt}}
\newcommand{\hthick}{\hlinewd{.7pt}}
\newcommand{\pbreaks}{1}
\newcommand{\pbreak}{
}
\newcommand{\lint}{\underline{}
\int}
\newcommand{\uint}{ \underline{}
\int}


\newcommand{\Si}{\text{Si}}
\newcommand{\lt}{<}
\newcommand{\gt}{>}
\newcommand{\amp}{&}
%% End: Author-provided macros
\begin{document}
%% bottom alignment is explicit, since it normally depends on oneside, twoside
\raggedbottom
\frontmatter
%% begin: half-title
\thispagestyle{empty}
{\titlepagefont\centering
\vspace*{0.28\textheight}
{\Huge An Inquiry-Based Introduction to Linear Algebra and Applications}\\}
\clearpage
%% end:   half-title
%% begin: title page
%% Inspired by Peter Wilson's "titleDB" in "titlepages" CTAN package
\thispagestyle{empty}
{\titlepagefont\centering
\vspace*{0.14\textheight}
%% Target for xref to top-level element is ToC
\addtocontents{toc}{\protect\hypertarget{g:book:idp8247064}{}}
{\Huge An Inquiry-Based Introduction to Linear Algebra and Applications}\\[3\baselineskip]
{\Large Feryal Alayont}\\[0.5\baselineskip]
{\Large Grand Valley State University}\\[3\baselineskip]
{\Large Steven Schlicker}\\[0.5\baselineskip]
{\Large Grand Valley State University}\\}
\clearpage
%% end:   title page
%% begin: copyright-page
\thispagestyle{empty}
\vspace*{\stretch{2}}
\vspace*{\stretch{1}}
\null\clearpage
%% end:   copyright-page
%
%
\typeout{************************************************}
\typeout{Preface  Preface}
\typeout{************************************************}
%
\begin{preface}{Preface}{}{Preface}{}{}{x:preface:fm_preface}
\begin{paragraphs}{A Free and Open-Source Linear Algebra Text.}{g:paragraphs:idp8253336}%
Mathematics is for everyone \textemdash{} whether as a gateway to other fields or as background for higher level mathematics. With linear algebra gaining importance in many applications, we feel that access to or the cost of a textbook should not stand in the way of a successful experience in learning linear algebra. Therefore, we made our textbook available to everyone for free download for their own non-commercial use. We especially encourage its use in linear algebra classrooms for instructors who are looking for an inquiry-based textbook or a supplemental resource to accompany their course.If an instructor would like to make changes to any of the files to better suit your students' needs, we offer source files for the text by making a request to the authors.%
\par
This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. The graphic \begin{image}{0.4}{0.2}{0.4}%
\includegraphics[width=\linewidth]{external/CClicense.pdf}
\end{image}%
 that appears throughout the text shows that the work is licensed with the Creative Commons, that the work may be used for free by any party so long as attribution is given to the author(s), that the work and its derivatives are used in the spirit of ``share and share alike,'' and that no party may sell this work or any of its derivatives for profit. Full details may be found by visiting \href{http://creativecommons.org/licenses/by-nc-sa/3.0/}{the Creative Commons Website}\footnote{\nolinkurl{creativecommons.org/licenses/by-nc-sa/3.0}\label{g:fn:idp8255896}} or sending a letter to Creative Commons, 444 Castro Street, Suite 900, Mountain View, California, 94041, USA.%
\end{paragraphs}%
\begin{paragraphs}{Motivation.}{g:paragraphs:idp8262936}%
The material in this text was developed with a strong focus on linear algebra in \(\R^n\) for the first two-thirds of the course, along with an emphasis on applications. We believe that the main ideas of linear algebra are more accessible to students if they are introduced in the more familiar context of vectors in \(\R^n\) rather than in abstract vector spaces. For this reason, vector spaces do no arise until late in the text.%
\end{paragraphs}%
\begin{paragraphs}{Goals.}{g:paragraphs:idp8260504}%
\pubtitle{An Inquiry-Based Introduction to Linear Algebra and Applications} provides a novel inquiry-based learning approach to linear algebra, as well as incorporating aspects of an inverted classroom. The impetus for this book lies in our approach to teaching linear algebra. We place an emphasis on active learning and on developing students' intuition through their investigation of examples. For us, active learning involves students \textemdash{} they are DOING something instead of being passive learners. What students are doing when they are actively learning might include discovering, processing, discussing, applying information, writing intensive assignments, engaging in common intellectual in-class experiences or collaborative assignments and projects. Although it is difficult to capture the essence of active learning in a textbook, this book is our attempt to do just that.%
\par
Additionally, linear algebra is one of the most applicable branches of mathematics to other disciplines. In fact, the Linear Algebra Curriculum Study Group (\initialism{LACSG}) (formed in 1990 to ``initiate substantial and sustained national interest in improving the undergraduate linear  algebra curriculum'' and funded by the National Science Foundation) made recommendations for the linear algebra curriculum, one of which was making linear algebra a matrix-oriented course that includes applications to give an indication of the ``pervasive use'' of linear algebra in other disciplines. \footnote{David Carlson, Charles R. Johnson, David Lay, Duane Porter. ``The Linear Algebra Curriculum Study Group Recommendations for the First Course in Linear Algebra.'' \pubtitle{The College Mathematics Journal}, Vol. 24, No. 1, 1993, 41-46.\label{g:fn:idp8262040}} Also, the most recent Committee on the Undergraduate Program in Mathematics (\initialism{CUPM}) report \footnote{\pubtitle{2015 CUPM Curriculum Guide to Majors in the Mathematical Sciences}, Carol S. Schumacher and Martha J. Siegel, Co-Chairs, Paul Zorn, Editor.\label{g:fn:idp8259608}} to the Mathematical Association of America recommends ``every Linear Algebra course should incorporate interesting applications, both to highlight the broad usefulness of linear algebra and to help students see the role of the theory in the subject as it is applied.'' An important component of this text is its inclusion of significant ``real-life'' applications of linear algebra to help students see that linear algebra is widely applicable in  many disciplines. Our goals for these materials are several.%
\begin{itemize}[label=\textbullet]
\item{}To carefully introduce the ideas behind the definitions and theorems to help students develop intuition and understand the logic behind them.%
\item{}To help students understand that mathematics is not done as it is often presented. We expect students to experiment through examples, make conjectures, and then refine their conjectures. We believe it is important for students to learn that definitions and theorems don't pop up completely formed in the minds of most mathematicians, but are the result of much thought and work.%
\item{}To help students develop their communication skills in mathematics. We expect our students to read and complete activities before class and come prepared with questions. While in class, students work to discover many concepts on their own through guided activities. Of course, students also individually write solutions to exercises on which they receive significant feedback. Communication skills are essential in any discipline and we place a heavy focus on their development.%
\item{}To have students actively involved in each of these items through in-class and out-of-class activities, in-class presentations (this is of course up to the instructor), and problem sets.%
\item{}To expose students to significant real-life applications of linear algebra.%
\end{itemize}
%
\end{paragraphs}%
\begin{paragraphs}{Layout.}{g:paragraphs:idp8263320}%
This text is formatted into sections, each of which contains preview activities, in-class activities, worked examples, and exercises. Most sections conclude with an application project \textemdash{} an application of the material in the section. The various types of activities serve different purposes.%
\begin{itemize}[label=\textbullet]
\item{}Preview activities are designed for students to complete before class to motivate the upcoming topic and prepare them with the background and information they need for the class activities and discussion.%
\item{}We generally use the regular activities to engage students during class in critical thinking experiences. These activities are used to provide motivation for the material,opportunities for students to develop course material on their own, or examples to help reinforce the meanings of definitions or theorems. The ultimate goal is to help students develop their intuition for and understanding of linear algebra concepts.%
\item{}Worked examples are included in each section. Part of the philosophy of this text is that students will develop the tools and understanding they need to work homework assignments through the preview, in-class activities, and class discussions. But some students express a desire for fully-worked examples in the text to reference during homework. In order to preserve the flow of material within a section, we include worked examples at the end of each section.%
\item{}The suggestion by the Linear Algebra Curriculum Study Group (\initialism{LACSG}) that, ``Mathematics departments should seriously consider making their first course in linear algebra a matrix-oriented course.'' is followed in this text. They suggest that this approach ``implies less emphasis on abstraction and more emphasis on problem solving and motivating applications.'' and that ``Some applications of linear algebra should be included to give an indication of the pervasive use of linear algebra in many client disciplines. Such applications necessarily will be limited by the need to minimize technical jargon and information from outside the course. Students should see the course as one of the most potentially useful mathematics courses they will take as an undergraduate.'' The focus of this text for the first two semesters of linear algebra (\hyperref[x:chapter:chap_intro_linear_systems]{Section~{\xreffont\ref{x:chapter:chap_intro_linear_systems}}\textendash{}{\xreffont\ref{x:chapter:chap_orthogonal_diagonalization}}})  is on \(\R^n\), and applications of linear algebra in real spaces. At our institution, these sections comprise a two-semester entry-level sequence into our major, suitable for freshmen without a calculus prerequisite. Abstract vector spaces do not appear until Section 28, which we consider to be the beginning of a third semester of linear algebra, a junior or senior level mathematics course. Also, the 2015 Committee on the Undergraduate Program in Mathematics (\initialism{CUPM}) report to the Mathematical Association of America recommends that ``Every Linear Algebra course should incorporate interesting applications,both to highlight the broad usefulness of linear algebra and to help students see the role of the theory in the subject as it is applied. Attractive applications may also entice students majoring in other disciplines to choose a minor or additional major in mathematics.'' All but two sections in this text include a substantial application. Each section begins with a short description of an application that uses the material from the section, then concludes with a project that develops the application in more detail. (The two sections that do not include projects are sections that are essentially long proofs \textemdash{} one section that contains formal proofs of the equivalences of the different parts of the Invertible Matrix Theorem, and the other contains algebraic proofs of the properties of the determinant.) The projects are independent of the material in the text \textemdash{} the text can be used without the applications. The applications are written in such a way that they could also be used with other textbooks. The projects are written following an inquiry-based style similar to the text, with important parts of the applications developed through activities. So the projects can be assigned outside of class as independent work for students. Several of the projects are accompanied by GeoGebra applets or Sage worksheets which are designed to help the students better understand the applications.%
\item{}Each investigation contains a collection of exercises. The exercises occur at a variety of levels of difficulty and most force students to extend their knowledge in different ways. While there are some standard, classic problems that are included in the exercises,many problems are open ended and expect a student to develop and then verify conjectures.%
\end{itemize}
%
\end{paragraphs}%
\begin{paragraphs}{Acknowledgements.}{g:paragraphs:idp8278424}%
The many beautiful images that are in this text were created by our colleague David Austin, to whom we owe a debt of gratitude. The original \LaTeX{} version of the book was partially supported by Grand Valley State University through an internal grant and a sabbatical.%
\par
The \initialism{HTML} version of the text is possible because of the work of Rob Beezer and the PreTeXt team for the development of the PreTeXt platform. David Farmer at the American Institute of Mathematics provided an initial PreTeXt conversion. From this version, Ian Curtis, Editorial Assistant for the GVSU Libraries, as part of the \href{https://www.gvsu.edu/library/sc/AcceleratingOER}{Accelerating Open Educational Resources Initiative at Grand Valley State University}\footnote{\nolinkurl{gvsu.edu/library/sc/AcceleratingOER}\label{g:fn:idp8277400}}, invested a significant amount of time and effort to create the PreTeXt version that you see. This work was conducted with support from the University Libraries and the \initialism{GVSU} President's Innovation Fund.%
\par
The code used to generate this book can be found in the \href{https://github.com/gvsuoer/linalg-applications}{\initialism{GVSU} \initialism{OER} GitHub repository}\footnote{\nolinkurl{github.com/gvsuoer/linalg-applications}\label{g:fn:idp8282392}}. This includes the PreTeXt \initialism{XML} files, figures, customization files (\initialism{XSL} and \initialism{CSS}), and the output (\initialism{HTML} and \initialism{PDF}). Contributors are encouraged to submit an issue or a pull request with changes, especially regarding accessibility (as mentioned below in \hyperlink{x:paragraphs:accessibility}{Accessibility}).%
\end{paragraphs}%
\begin{paragraphs}{Accessibility.}{x:paragraphs:accessibility}%
In creating this PreTeXt version we (the authors and the Grand Valley State University Libraries) strive to ensure our tools, devices, services, and environments are available to and usable by as many people as possible.%
\par
The web version of \pubtitle{An Inquiry-Based Introduction to Linear Algebra and Applications} incorporates the following features to support accessibility:%
\begin{itemize}[label=\textbullet]
\item{}All content can be navigated by use of a keyboard%
\item{}Links, headings, and tables have been designed to work with screen readers%
\item{}Mathematics in PreTeXt are rendered with MathJax so they can be understood by people using screen readers and\slash{}or other assistive devices.%
\end{itemize}
%
\par
We have identified some accessibility issues which were beyond the author's and \initialism{GVSU} Libraries' capacity to address in the course of this adaptation. These present an opportunity for future adaptation or revision by educators with expertise in accessibility issues in mathematics documents. Known accessibility issues include:%
\begin{itemize}[label=\textbullet]
\item{}A small number of exercises and activities in this text require students to visually interpret diagrams or figures. These activities cannot be made accessible through alt text alone, and may need to be redesigned or replaced for full accessibility.%
\item{}Some visual representations of concepts (see, for example, \hyperref[x:chapter:chap_determinants]{Section~{\xreffont\ref{x:chapter:chap_determinants}}}) are not accessible to screen readers. However, they are fully described in text.%
\item{}The matrix table represented in \hyperref[x:chapter:chap_SVD]{Section~{\xreffont\ref{x:chapter:chap_SVD}}} is too complex to encode with the current functionality of PreTeXt and MathJax. It is presented as an image and is not accessible to screen readers. Full accessibility likely requires redesigning the project or waiting for expanded features in PreTeXt and MathJax.%
\end{itemize}
%
\par
We are always looking for how we can make our resources more accessible. If you are having problems accessing this resource, please \href{mailto:oer@gvsu.edu}{contact us by email}\footnote{\nolinkurl{mailto:oer@gvsu.edu}\label{g:fn:idp8292504}} to let us know.%
\end{paragraphs}%
\begin{paragraphs}{To the Student.}{g:paragraphs:idp8295448}%
The inquiry-based format of this book means that you can be in charge of your own learning. The guidance of your instructor and support of your fellow classmates can help you develop your understanding of the topic. Your learning will benefit best if you engage in the material by completing all preview and in-class activities in order to fully develop your own intuition and understanding of the material, which can be achieved only by reflecting on the ideas and concepts and communicating your reflections with others. Don't be afraid to ask questions or to be wrong at times, for this is how people learn. Good luck! We will be happy to hear your comments about the book.%
\end{paragraphs}%
\end{preface}
%% begin: table of contents
%% Adjust Table of Contents
\setcounter{tocdepth}{0}
\renewcommand*\contentsname{Contents}
\tableofcontents
%% end:   table of contents
\mainmatter
%
%
\typeout{************************************************}
\typeout{Chapter I Systems of Linear Equations}
\typeout{************************************************}
%
\begin{partptx}{Systems of Linear Equations}{}{Systems of Linear Equations}{}{}{x:part:part-systems}
 %
%
\typeout{************************************************}
\typeout{Section 1 Introduction to Systems of Linear Equations}
\typeout{************************************************}
%
\begin{chapterptx}{Introduction to Systems of Linear Equations}{}{Introduction to Systems of Linear Equations}{}{}{x:chapter:chap_intro_linear_systems}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp8294552}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What is a linear equation?%
\item{}What is a system of linear equations?%
\item{}What is a solution set of a system of linear equations?%
\item{}What are equivalent systems of linear equations?%
\item{}What operations can we use to solve a system of linear equations?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Electrical Circuits}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Electrical Circuits}{}{Application: Electrical Circuits}{}{}{x:section:sec_appl_elec_circuits}
Linear algebra is concerned with the study of systems of linear equations. There are two important aspects to linear systems. One is to use given information to set up a system of equations that represents the information (this is called \terminology{modeling}), and the other is to solve the system. As an example of modeling, we consider the application to the very simple electrical circuit. An electrical circuit consists of%
\begin{itemize}[label=\textbullet]
\item{}one or more electrical sources, denoted by \begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/1_a_pa_source.pdf}
\end{image}%
%
\item{}one or more resistors, denoted by \begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/1_a_pa_resistor.pdf}
\end{image}%
~.%
\end{itemize}
%
\par
\index{circuits!source}\index{circuits!resistor}\index{circuits!junctions} A source is a power supply like a battery, and a resistor is an object that consumes the electricity, like a lamp or a computer. A simple circuit consists of one or more sources connected to resistors, like the one shown in \hyperref[x:figure:F_circuit1]{Figure~{\xreffont\ref{x:figure:F_circuit1}}}. The straight lines in the circuit indicate wires through which current flows. The points labeled P and Q are called \terminology{junctions} or \terminology{nodes}.%
\begin{figureptx}{A circuit.}{x:figure:F_circuit1}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/1_a_pa_circuit1.pdf}
\end{image}%
\tcblower
\end{figureptx}%
The source creates a charge that produces potential energy \(E\) measured in volts (V). Current flows out of the positive terminal of a source and runs through each branch of the circuit. Let \(I_1\), \(I_2\), and \(I_3\) be the currents as illustrated in \hyperref[x:figure:F_circuit1]{Figure~{\xreffont\ref{x:figure:F_circuit1}}}. The goal is to find the current flowing in each branch of the circuit.%
\par
Linear algebra comes into play when analyzing a circuit based on the relationship between current \(I\), resistance \(R\), and voltage \(E\). There are laws governing electrical circuits that state that \(E = IR\) across a resistor. Additionally, Kirchoff's Current and Voltage Laws indicate how current behaves within the whole circuit. Using all these laws together, we derive the system%
\begin{alignat*}{3}
{}I_1 \  \amp - \  \amp {}I_2 \  \amp + \  \amp {}I_3 \  \amp = 0\\
5I_1 \  \amp + \  \amp 2I_2 \    \amp {} \   \amp {}  \    \amp = 8\\
\  \amp {} \   \amp 2I_2 \    \amp + \  \amp 4I_3 \    \amp = 5\text{,}
\end{alignat*}
where \(I_1\), \(I_2\), and \(I_3\) are the currents at the points indicated in \hyperref[x:figure:F_circuit1]{Figure~{\xreffont\ref{x:figure:F_circuit1}}}. To finish analyzing the circuit, we now need to solve this system. In this section we will begin to learn systematic methods for solving systems of linear equations. More details about the derivation of these circuit equations can be found at the end of this section.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_intro_le_intro}
Systems of linear equations arise in almost every field of study: mathematics, statistics, physics, chemistry, biology, economics, sociology, computer science, engineering, and many, many others. We will study the theory behind solving systems of linear equations, implications of this theory, and applications of linear algebra as we proceed throughout this text.%
\begin{exploration}{}{x:exploration:pa_1_a}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item\label{x:task:ex_system_1}Consider the following system of two linear equations in two unknowns, \(x_1, x_2\):%
\begin{alignat*}{2}
2x_1  \amp {}-{}  \amp 3x_2   \amp = 0\\
x_1   \amp {}-{}   \amp x_2    \amp = 1\text{.}
\end{alignat*}
%
\par
One way to solve such a system of linear equations is the method of substitution (where one equation is solved for one variable and then the resulting expression is substituted into the remaining equations). This method works well for simple systems of two equations in two unknowns, but becomes complicated if the number or complexity of the equations is increased.%
\par
Another method is elimination \textemdash{} the method that we will adopt in this book. Recall that the elimination method works by multiplying each equation by a suitable constant so that the coefficients of one of the variables in each equation is the same. Then we subtract corresponding sides of these equations to eliminate that variable.%
\par
Use the method of elimination to show that this system has the unique solution \(x_1=3\) and \(x_2=2\). Explain the specific steps you perform when using elimination.%
\item{}Recall that a linear equation in two variables can be represented as a line in \(\R^2\), the Cartesian plane, where one variable corresponds to the horizontal axis and the other to the vertical axis. Represent the two equations \(2x_1 -3x_2 = 0\) and \(x_1 - x_2= 1\) in \(\R^2\) and illustrate the solution to the system in your picture.%
\item{}The previous example should be familiar to you as a system of two equations in two unknowns. Now we consider a system of three equations in three unknowns%
\begin{alignat}{4}
{}I_1   \amp {}-{}   \amp {}I_2  \amp {}+{}  \amp {}I_3   \amp {}={}   \amp 0\amp {}\label{x:mrow:eq_PAcircuit_a}\\
{5}I_1   \amp {}+{}  \amp {2}I_2   \amp {}     \amp {}     \amp {}={}  \amp 8\amp {}\label{x:mrow:eq_PAcircuit_b}\\
{}    \amp {}      \amp {2}I_2   \amp {}+{}  \amp  {4}I_3  \amp {}={}  \amp 5\amp {}\label{x:mrow:eq_PAcircuit_c}
\end{alignat}
that arises from our electrical circuit in \hyperref[x:figure:F_circuit1again]{Figure~{\xreffont\ref{x:figure:F_circuit1again}}}, with currents \(I_1\), \(I_2\), and \(I_3\) as indicated in the circuit.%
\begin{figureptx}{A circuit.}{x:figure:F_circuit1again}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/1_a_pa_circuit1.pdf}
\end{image}%
\tcblower
\end{figureptx}%
In the remainder of this preview activity we will apply the method of elimination to solve the system of linear equations \hyperref[x:mrow:eq_PAcircuit_a]{({\xreffont\ref{x:mrow:eq_PAcircuit_a}})}, \hyperref[x:mrow:eq_PAcircuit_b]{({\xreffont\ref{x:mrow:eq_PAcircuit_b}})}, and \hyperref[x:mrow:eq_PAcircuit_c]{({\xreffont\ref{x:mrow:eq_PAcircuit_c}})}.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Replace equation \hyperref[x:mrow:eq_PAcircuit_b]{({\xreffont\ref{x:mrow:eq_PAcircuit_b}})} with the new equation obtained by multiplying both sides of equation \hyperref[x:mrow:eq_PAcircuit_a]{({\xreffont\ref{x:mrow:eq_PAcircuit_a}})} by 5 and then subtracting corresponding sides of this equation from the appropriate sides of equation \hyperref[x:mrow:eq_PAcircuit_b]{({\xreffont\ref{x:mrow:eq_PAcircuit_b}})}. Show that the resulting system is%
\begin{alignat}{3}
{}I_1 \  \amp - \  \amp {}I_2 \  \amp + \  \amp {}I_3 \  \amp = 0\notag\\
\  \amp   \  \amp 7I_2  \    \amp {-} \   \amp {5}I_3  \    \amp = 8\label{x:mrow:eq_PAcircuit_d}\\
\  \amp {} \   \amp 2I_2 \    \amp + \  \amp 4I_3 \    \amp = 5\text{.}\notag
\end{alignat}
%
\item{}Now eliminate the variable \(I_2\) from the last two equations in the system in part (a) by using equations \hyperref[x:mrow:eq_PAcircuit_c]{({\xreffont\ref{x:mrow:eq_PAcircuit_c}})} and \hyperref[x:mrow:eq_PAcircuit_d]{({\xreffont\ref{x:mrow:eq_PAcircuit_d}})} to show that \(I_3=0.5\). Explain your process.%
\item{}Once you know the value for \(I_3\), how can you find \(I_2\)? Then how do you find \(I_1\)? Use your method to show that the solution to this system is the ordered triple (1,1.5,0.5). Interpret the result in terms of currents.%
\end{enumerate}
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Notation and Terminology}
\typeout{************************************************}
%
\begin{sectionptx}{Notation and Terminology}{}{Notation and Terminology}{}{}{x:section:sec_notation}
To study linear algebra, we will need to agree on some general notation and terminology to represent our systems.%
\par
An equation like \(4x_1 + x_2 = 8\) is called a linear equation because the variables (\(x_1\) and \(x_2\) in this case) are raised to the first power, and there are no products of variables. The equation \(4x_1 + x_2 = 8\) is a linear equation in two variables, but we can make a linear equation with any number of variables we like.%
\begin{definition}{}{g:definition:idp8431752}%
\index{linear equation}%
\index{linear equation!coefficients}%
A \terminology{linear equation} in the variables \(x_1\), \(x_2\), \(\ldots\), \(x_n\) is an equation of the form%
\begin{equation*}
a_1x_1 + a_2x_2 + \cdots + a_nx_n = b\text{,}
\end{equation*}
where \(n\) is a positive integer and \(a_1\), \(a_2\), \(\ldots\), \(a_n\) and \(b\) are constants. The constants \(a_1\), \(a_2\), \(\ldots\), \(a_n\) are called the \terminology{coefficients} of the equation.%
\end{definition}
We can use any labels for the variables in a linear equation that we like, e.g., \(I_1\), \(x_1\), \(t_1\), and you should become comfortable working with variables in any form. We will usually use subscripts, as in \(x_1, x_2, x_3, \ldots\), to represent the variables as this notation allows us to have any number of variables. Other examples of linear equations are%
\begin{equation*}
x+2y=4 \qquad \text{ and } \qquad \sqrt{2}x_1-3x_2=\frac{1}{4}x_3+\pi \,\text{.}
\end{equation*}
%
\par
On the other hand, the equations%
\begin{equation*}
\frac{1}{x}+y-z=0 \qquad \text{ and } \qquad 2x_1=\sqrt{x_2}-5
\end{equation*}
are non-linear equations.%
\begin{definition}{}{g:definition:idp8440328}%
\index{system of linear equations}%
A \terminology{system of linear equations} is a collection of one or more linear equations in the same variables.%
\end{definition}
For example, the two equations%
\begin{alignat}{2}
x_1   \amp {}-{}  \amp x_2  \amp = 1\notag\\
2x_1   \amp {}+{}   \amp x_2  \amp = 5\label{x:mrow:eq_1_a_PA_1}
\end{alignat}
form a system of two linear equations in variables \(x_1\), \(x_2\).%
\begin{definition}{}{g:definition:idp8435848}%
\index{system of linear equations!solution}%
A \terminology{solution} to a system of linear equations is an ordered \(n\)-tuple \((s_1, s_2, \ldots,
s_n)\) of numbers so that we obtain all true statements in the system when we replace the variable in order with \(s_1\), \(s_2\), \(\ldots\), and \(s_n\).%
\end{definition}
\index{system of linear equations!solution set} For example, \(x_1=2, x_2=1\), or simply \((2,1)\), is a solution to the above system of linear equations in \hyperref[x:mrow:eq_1_a_PA_1]{({\xreffont\ref{x:mrow:eq_1_a_PA_1}})} as can be checked by substituting the variables into each equation. In solving a system of linear equations, we are interested in finding the set of all solutions, which we will call the \terminology{solution set of the system}. For the above system in \hyperref[x:mrow:eq_1_a_PA_1]{({\xreffont\ref{x:mrow:eq_1_a_PA_1}})}, the solution set is the set containing the single point \((2,1)\), denoted \(\{(2,1)\}\), because there is only one solution. If we consider just the equation \(x_1-x_2=0\) as our system, the solution set is the line \(x_1=x_2\) in the plane. More generally, a set of solutions is a collection of ordered \(n\)-tuples of numbers. We denote the set of all ordered \(n\)-tuples of numbers as \(\R^n\). So, for example, \(\R^2\) is the set of all ordered pairs, or just the standard coordinate plane, and \(\R^3\) is the set of all ordered triples, or the three-dimensional space.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Solving Systems of Linear Equations}
\typeout{************************************************}
%
\begin{sectionptx}{Solving Systems of Linear Equations}{}{Solving Systems of Linear Equations}{}{}{x:section:sec_solve_systems}
\index{system of linear equations!equivalent systems}%
\index{elementary operations}%
\index{system of linear equations!operations}%
In \hyperref[x:exploration:pa_1_a]{Preview Activity~{\xreffont\ref{x:exploration:pa_1_a}}}, we were introduced to linear systems and the method of elimination for a system of two or three variables. Our goal now is to come up with a systematic method that will reduce any linear system to one that is easy to solve without changing the solution set of the system. Two linear systems will be called \terminology{equivalent} if they have the same solution set.%
\par
The operations we used in \hyperref[x:exploration:pa_1_a]{Preview Activity~{\xreffont\ref{x:exploration:pa_1_a}}} to systematically eliminate variables so that we can solve a linear system are called \terminology{elementary operations on a system of linear equations} or just \terminology{elementary operations}. In the exercises you will argue that elementary operations do not change the solution set to a system of linear equations, a fact that is summarized in the following theorem.%
\begin{theorem}{}{}{g:theorem:idp8453512}%
The \terminology{elementary operations} on a system of linear equations:%
\begin{enumerate}[label=(\arabic*)]
\item{}replacing one equation by the sum of that equation and a scalar multiple of another equation;%
\item{}interchanging two equations;%
\item{}replacing an equation by a nonzero scalar multiple of itself;%
\end{enumerate}
do not change the solution set to the system of equations.%
\end{theorem}
When we apply these elementary operations our ultimate goal is to produce a system of linear equations in a simplified form with the same solution set, where the number of variables eliminated from the equations increase as we move from top to bottom. This method is called the \emph{elimination}\index{system of linear equations!elimination} method.%
\begin{activity}{}{g:activity:idp8463112}%
For systems of linear equations with a small number of variables, many different methods could be used to find a solution. However, when a system gets large, ad-hoc methods become unwieldy. One of our goals is to develop an algorithmic approach to solving systems of linear equations that can be programmed and applied to any linear system, so we want to work in a very prescribed method as indicated in this activity. Ultimately, once we understand how the algorithm works, we will use calculators\slash{}computers to do the work. Apply the elimination method as described to show that the solution set of the following system is \((2, -1, 1)\):%
\begin{alignat*}{3}
x_1 \amp {}+{}\amp  x_2 \amp {}-{}\amp  x_3 \amp = 0\\
2x_1 \amp {}+{}\amp  x_2 \amp {}-{}\amp  x_3 \amp = 2\\
x_1\amp {}-{}\amp x_2 \amp {}+{}\amp  2x_3 \amp = 5\text{.}
\end{alignat*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Use the first equation to eliminate the variable \(x_1\) in the second and third equations.%
\item{}Use the new second equation to eliminate the variable \(x_2\) in the third equation and find the value of \(x_3\).%
\item{}Find values of \(x_2\) and then \(x_1\).%
\end{enumerate}
\end{activity}%
\begin{paragraphs}{Important Note.}{g:paragraphs:idp8463880}%
Technically, we don't really add two equations or multiply an equation by a scalar. When we refer to a scalar multiple of an equation, we mean the equation obtained by equating the scalar multiple of the expression on the left side of the equation and the same scalar multiple of the expression on the right side of the equation. Similarly, when we refer to a sum of two equations, we don't really add the equations themselves. Instead, we mean the equation obtained by equating the sum of the expressions on the left sides of the equations to the sum of the expressions on the right sides of the equations. We will use the terminology ``scalar multiple of an equation'' and ``sum of two equations'' as shorthand to mean what is described here.%
\end{paragraphs}%
\begin{paragraphs}{Another Important Note.}{g:paragraphs:idp8475912}%
There is an important and subtle point to consider here. When we use these operations to find a solution to a system of equations, we are assuming that the system has a solution. The application of these operations then tells us what a solution must look like. However, there is no guarantee that the outcome is actually a solution \textemdash{} to be safe we should check to make sure that our result is a solution to the system. In the case of linear systems, though, every one of our operations on equations is reversible (if applied correctly), so the result will always be a solution (but this is not true in general for non-linear systems).%
\end{paragraphs}%
\begin{paragraphs}{Terminology.}{g:paragraphs:idp8468232}%
\index{system of linear equations!consistent}%
\index{system of linear equations!inconsistent}%
A system of equations is called \terminology{consistent} if the system has at least one solution. If a system has no solutions, then it is said to be \terminology{inconsistent}.%
\end{paragraphs}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Geometry of Solution Sets of Linear Systems}
\typeout{************************************************}
%
\begin{sectionptx}{The Geometry of Solution Sets of Linear Systems}{}{The Geometry of Solution Sets of Linear Systems}{}{}{x:section:sec_geom_solu_sets}
We are familiar with linear equations in two variables from basic algebra and calculus (through linear approximations). The set of solutions to a system of linear equations in two variables has some geometry connected to it.%
\begin{activity}{}{x:activity:act_1_a_2}%
Recall that we examined the geometry of the system%
\begin{alignat*}{2}
2x_1   \amp {}-{}  \amp 3x_2   \amp = 0\\
x_1   \amp {}-{}   \amp x_2    \amp = 1
\end{alignat*}
in \hyperref[x:exploration:pa_1_a]{Preview Activity~{\xreffont\ref{x:exploration:pa_1_a}}} to show that the resulting solution set consists of a single point in the plane.%
\par
In this activity we examine the geometry of the system%
\begin{alignat}{3}
{2}x_1   \amp {}-{}  \amp {}x_2   \amp = 1\amp {}\notag\\
{2}x_1   \amp {}-{}   \amp {2}x_2    \amp = 2\amp\text{.}\label{x:mrow:eq_1_a_2}
\end{alignat}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Consider the linear equation \(2x_1-2x_2=2\) (or, equivalently \(2x-2y=2\)). What is the graph of the solution set (the set of points \((x_1, x_2)\) satisfying this equation) of this single equation in the plane? Draw the graph to illustrate.%
\item{}How can we represent the solution set of the system \hyperref[x:mrow:eq_1_a_2]{({\xreffont\ref{x:mrow:eq_1_a_2}})} of two equations graphically? How is this solution set related to the solution set of the single equation \(2x_1-2x_2=2\)? Why? How many solutions does the system \hyperref[x:mrow:eq_1_a_2]{({\xreffont\ref{x:mrow:eq_1_a_2}})} have?%
\item{}There are exactly three possibilities for the number of solutions to a general system of two linear equations in two unknowns. Describe the geometric representations of solution sets for each of the possibilities. Illustrate each with a specific example (of your own) using a system of equations and sketching its geometric representation.%
\end{enumerate}
\end{activity}%
\hyperref[x:activity:act_1_a_2]{Activity~{\xreffont\ref{x:activity:act_1_a_2}}} shows that there are three options for the solution set of a system: A system can have no solutions, one solution, or infinitely many solutions.%
\par
Now we consider systems of three variables. As an example, let us look at the linear equation \(x+y+z=1\) in the three variables \(x\), \(y\), and \(z\). Notice that the points \((1,1,-2)\), \((0,0,0)\), and \((-1,-1,2)\) all satisfy this equation. As a linear equation, the graph of \(x+y+z=0\) will be a plane in three dimensions that contains these three points, as shown in \hyperref[x:figure:F_1_a_plane]{Figure~{\xreffont\ref{x:figure:F_1_a_plane}}}. Hence when we consider a linear system in three unknowns, we are looking for a point in the three dimensional space that lies on all the planes described by the equations.%
\begin{figureptx}{The plane \(x+y+z=1\).}{x:figure:F_1_a_plane}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/1_a_plane.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\begin{activity}{}{x:activity:act_1_a_3}%
In this activity we examine the geometry of linear systems of three equations in three unknowns. Recall that each linear equation in three variables has a plane as its solution set. Use a piece of paper to represent each plane.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Is it possible for a general system of three linear equations in three unknowns to have no solutions? If so, geometrically describe this situation and then illustrate each with a specific example using a system of equations. If not, explain why not.%
\item{}Is it possible for a general system of three linear equations in three unknowns to have exactly one solution? If so, geometrically describe this situation and then illustrate each with a specific example using a system of equations. If not, explain why not.%
\item{}Is it possible for a general system of three linear equations in three unknowns to have infinitely many solutions? If so, geometrically describe this situation and then illustrate each with a specific example using a system of equations. If not, explain why not.%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_intro_le_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp8487560}%
Apply the allowable operations on equations to solve the system%
\begin{alignat*}{5}
{}x_1     \amp {}+{}  \amp {2}x_2   \amp {+}  \amp {}x_3  \amp {-}  \amp {}x_4  \amp = 4\amp {}\\
{}     \amp {}-{}  \amp {}x_2   \amp {-}  \amp {}x_3  \amp {+}  \amp {3}x_4  \amp = 6\amp {}\\
{}x_1   \amp {}{}    \amp {}     \amp {+}  \amp {2}x_3  \amp {-}  \amp {}x_4  \amp = 1\amp {}\\
{2}x_1   \amp {}-{}  \amp {3}x_2   \amp {+}  \amp {}x_3  \amp {+}  \amp {}x_4  \amp = 2\amp {.}
\end{alignat*}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp8499208}{}\quad{}We begin by eliminating the variable \(x_1\) from all but the first equation. To do so, we replace the third equation with the third equation minus the first equation to obtain the equivalent system%
\begin{alignat*}{5}
{}x_1     \amp {}+{}  \amp {2}x_2   \amp {+}    \amp {}x_3  \amp {-}  \amp {}x_4  \amp = {}\amp 4\amp {}\\
{}     \amp {}-{}  \amp {}x_2   \amp {-}    \amp {}x_3  \amp {+}  \amp {3}x_4  \amp = {}\amp 6\amp {}\\
{}     \amp {}-{}  \amp {2}x_2   \amp {+}    \amp {}x_3  \amp {}  \amp {}    \amp = {-}\amp 3\amp {}\\
{2}x_1   \amp {}-{}  \amp {3}x_2   \amp {+}    \amp {}x_3  \amp {+}  \amp {}x_4  \amp = {}\amp 2\amp {.}
\end{alignat*}
%
\par
Then we replace the fourth equation with the fourth equation minus 2 times the first to obtain the equivalent system%
\begin{alignat*}{5}
{}x_1     \amp {}+{}  \amp {2}x_2   \amp {+}    \amp {}x_3  \amp {-}  \amp {}x_4  \amp = {}\amp 4\amp {}\\
{}     \amp {}-{}  \amp {}x_2   \amp {-}    \amp {}x_3  \amp {+}  \amp {3}x_4  \amp = {}\amp 6\amp {}\\
{}     \amp {}-{}  \amp {2}x_2   \amp {+}    \amp {}x_3  \amp {}  \amp {}    \amp = {-}\amp 3\amp {}\\
{}     \amp {}-{}  \amp {7}x_2   \amp {-}    \amp {}x_3  \amp {+}  \amp {3}x_4  \amp = {-}\amp 6\amp {.}
\end{alignat*}
%
\par
To continue the elimination process, we want to eliminate the \(x_2\) variable from our latest third and fourth equations. To do so, we use the second equation so that we do not reinstate an \(x_1\) variable in our new equations. We replace equation three with equation 3 minus 2 times equation 2 to produce the equivalent system%
\begin{alignat*}{5}
{}x_1     \amp {}+{}  \amp {2}x_2   \amp {+}    \amp {}x_3  \amp {-}  \amp {}x_4  \amp = {}\amp 4\amp {}\\
{}     \amp {}-{}  \amp {}x_2   \amp {-}    \amp {}x_3  \amp {+}  \amp {3}x_4  \amp = {}\amp 6\amp {}\\
{}     \amp {}{}    \amp {}    \amp { }    \amp {3}x_3  \amp {-}  \amp {6}x_4  \amp = {-}\amp 15\amp {}\\
{}     \amp {}-{}  \amp {7}x_2   \amp {-}    \amp {}x_3  \amp {+}  \amp {3}x_4  \amp = {-}\amp 6\amp {.}
\end{alignat*}
%
\par
Then we replace equation four with equation four minus 7 times equation 2, giving us the equivalent system%
\begin{alignat*}{5}
{}x_1     \amp {}+{}  \amp {2}x_2   \amp {+}    \amp {}x_3  \amp {-}  \amp {}x_4  \amp = {}\amp 4\amp {}\\
{}     \amp {}-{}  \amp {}x_2   \amp {-}    \amp {}x_3  \amp {+}  \amp {3}x_4  \amp = {}\amp 6\amp {}\\
{}     \amp {}{}    \amp {}    \amp { }    \amp {3}x_3  \amp {-}  \amp {6}x_4  \amp = {-}\amp 15\amp {}\\
{}     \amp {}{}    \amp {}     \amp { }    \amp {6}x_3  \amp {-}  \amp {18}x_4  \amp = {-}\amp 48\amp {.}
\end{alignat*}
%
\par
With one more step we can determine the value of \(x_4\). We use the last two equations to eliminate \(x_3\) from the fourth equation by replacing equation four with equation four minus 2 times equation 3. This results in the equivalent system%
\begin{alignat*}{5}
{}x_1     \amp {}+{}  \amp {2}x_2   \amp {+}    \amp {}x_3  \amp {-}  \amp {}x_4  \amp = {}\amp 4\amp {}\\
{}     \amp {}-{}  \amp {}x_2   \amp {-}    \amp {}x_3  \amp {+}  \amp {3}x_4  \amp = {}\amp 6\amp {}\\
{}     \amp {}{}    \amp {}    \amp { }    \amp {3}x_3  \amp {-}  \amp {6}x_4  \amp = {-}\amp 15\amp {}\\
{}     \amp {}{}    \amp {}     \amp {}    \amp {}    \amp {-}  \amp {6}x_4  \amp = {-}\amp 18\amp {.}
\end{alignat*}
%
\par
The last equation tells us that \(-6x_4 = -18\), or \(x_4 =3\). Substituting into the third equation shows that%
\begin{alignat*}{1}
3x_3 - 6\left(3\right) \amp = -15\\
3x_3 \amp = 3\\
x_3 \amp = 1\text{.}
\end{alignat*}
%
\par
The second equation shows that%
\begin{alignat*}{1}
-x_2 - 1 + 3\left(3\right) \amp = 6\\
-x_2 \amp = -2\\
x_2 \amp =2\text{.}
\end{alignat*}
%
\par
Finally, the first equation tells us that%
\begin{alignat*}{1}
x_1 + 2\left(2\right) + 1 - 3 \amp = 4\\
x_1 \amp = 2\text{.}
\end{alignat*}
%
\par
So the solution to our system is \(x_1 =2\), \(x_2 = 2\), \(x_3 = 1\), and \(x_4 =3\). It is worth substituting back into our original system to check to make sure that we have not made any arithmetic mistakes.%
\end{example}
\begin{example}{}{g:example:idp8510856}%
A mining company has three mines. One day of operation at the mines produces the following output.%
\begin{itemize}[label=\textbullet]
\item{}Mine 1 produces 25 tons of copper, 600 kilograms of silver and 15 tons of manganese.%
\item{}Mine 2 produces 30 tons of copper, 500 kilograms of silver and 10 tons of manganese.%
\item{}Mine 3 produces 20 tons of copper, 550 kilograms of silver and 12 tons of manganese.%
\end{itemize}
%
\par
Suppose the company has orders for 550 tons of copper, 11350 kilograms of silver and 250 tons of manganese.%
\par
Write a system of equations to answer the question: how many days should the company operate each mine to exactly fill the orders? State clearly what the variables in your system represent. Then find the general solution of your system.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp8514312}{}\quad{}For our system, let \(x_1\) be the number of days mine 1 operates, \(x_2\) be the number of days mine 2 operates, and \(x_3\) be the number of days mine 3 operates. Since mine 1 produces 25 tons of copper each day, in \(x_1\) days mine 1 will produce \(25x_1\) tons of copper. Mine 2 produces 30 tons of copper each day, so in \(x_2\) days mine 2 will produce \(30x_2\) tons of copper. Also, mine 3 produces 20 tons of copper each day, so in \(x_3\) days mine 3 will produce \(20x_3\) tons of copper. Since the company needs to supply a total of 550 tons of copper, we need to have \(25x_1+30x_2+20x_3 = 550\). Similar analyses of silver and manganese give us the system%
\begin{alignat*}{4}
{25}x_1    \amp {}+{}  \amp {30}x_2  \amp {}+{}   \amp {20}x_3  \amp {}={}   \amp 550\amp {}\\
{600}x_1    \amp {}+{}  \amp {500}x_2  \amp {}+{}  \amp {550}x_3   \amp {}={}   \amp 11350\amp {}\\
{15}x_1      \amp {}+{}  \amp {10}x_2    \amp {}+{}  \amp {12}x_3   \amp {}={}  \amp 250\amp {.}
\end{alignat*}
%
\par
To solve the system, we eliminate the variable \(x_2\) from the second and third equations by replacing equation two with equation two minus 24 times equation one and replacing equation three with equation three minus \(\frac{3}{5}\) times equation one. This produces the equvalent system%
\begin{alignat*}{4}
{25}x_1    \amp {}+{}  \amp {30}x_2  \amp {}+{}   \amp {20}x_3  \amp {}={}   \amp {}550\amp {}\\
{}      \amp {}-{}    \amp {220}x_2  \amp {}+{}    \amp {70}x_3   \amp {}={}   \amp {-}1850\amp {}\\
{}        \amp {}-{}  \amp {8}x_2    \amp {}{}    \amp {}       \amp {}={}  \amp {-}80\amp {.}
\end{alignat*}
%
\par
We are fortunate now that we can determine the value of \(x_2\) from the third equation, which tells us that \(x_2 = 10\). Substituting into the second equation shows that%
\begin{alignat*}{1}
-220(10) + 70x_3 \amp = -1850\\
70x_3 \amp = 350\\
x_3 \amp = 5\text{.}
\end{alignat*}
%
\par
Substituting into the first equation allows us to determine the value for \(x_1\):%
\begin{alignat*}{1}
25x_1 + 30(10) + 20(5) \amp = 550\\
25x_1 \amp = 150\\
x_1 \amp = 6\text{.}
\end{alignat*}
%
\par
So the company should run mine 1 for 6 days, mine 2 for 10 days, and mine 3 for 5 days to meet this demand.%
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_intro_le_summ}
In this section we introduced linear equations and systems of linear equations.%
\begin{itemize}[label=\textbullet]
\item{}Informally, a linear equation is an equation in which each term is either a constant or a constant times a variable. More formally, a linear equation in the variables \(x_1\), \(x_2\), \(\ldots\), \(x_n\) is an equation of the form%
\begin{equation*}
a_1x_1 + a_2x_2 + \cdots + a_nx_n = b\text{,}
\end{equation*}
where \(n\) is a positive integer and \(a_1\), \(a_2\), \(\ldots\), \(a_n\) and \(b\) are constants.%
\item{}A system of linear equations is a collection of one or more linear equations in the same variables.%
\item{}Informally, a solution to a system of linear equations is a point that satisfies all of the equations in the system. More formally, a solution to a system of linear equation in \(n\) variables \(x_1\), \(x_2\), \(\ldots\), \(x_n\) is an ordered \(n\)-tuple \((s_1, s_2, \ldots,
s_n)\) of numbers so that we obtain all true statements in the system when we replace \(x_1\) with \(s_1\), \(x_2\) with \(s_2\), \(\ldots\), and \(x_n\) with \(s_n\).%
\item{}Two linear systems are equivalent if they have the same solution set.%
\item{}The following operations on a system of equations do not change the solution set:%
\begin{enumerate}[label=(\arabic*)]
\item{}Replace one equation by the sum of that equation and a scalar multiple of another equation.%
\item{}Interchange two equations.%
\item{}Replace an equation by a nonzero scalar multiple of itself.%
\end{enumerate}
%
\end{itemize}
%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_intro_le_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp8544264}%
In the method of elimination there are three operations we can apply to solve a system of linear equations. For this exercise we focus on a system of equations in three unknowns \(x_1\), \(x_2\), and \(x_3\), but the arguments generalize to a system with any number of variables. Consider the general system of three equations in three unknowns%
\begin{alignat*}{3}
4x_1   \amp {}-{}  \amp 4x_2  \amp {}+{}  \amp 4x_3   \amp = 0\\
4x_1   \amp {}+{}  \amp 2x_2  \amp {}    \amp {}     \amp = 8\\
{}   \amp {}    \amp 2x_2  \amp {}+{}  \amp 5x_3  \amp = 9\text{.}
\end{alignat*}
The goal of this exercise is to understand why the three operations on a system do not change the solutions to the system. Recall that a solution to a system with unknowns \(x_1\), \(x_2\), and \(x_3\) is a set of three numbers, one for \(x_1\), one for \(x_2\), and one for \(x_3\) that satisfy all of the equations in the system.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why, if we have a solution to this system, then that solution is also a solution to any constant \(k\) times the second equation.%
\item{}Explain why, if we have a solution to this system, then that solution is also a solution to the sum of the first equation and \(k\) times the third equation for any constant \(k\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp8553608}%
Alice stopped by a coffee shop two days in a row at a conference to buy drinks and pastries. On the first day, she bought a cup of coffee and two muffins for which she paid \textdollar{}6.87. The next day she bought two cups of coffee and three muffins (for herself and a friend). Her bill was \textdollar{}11.25. Use the method of elimination to determine the price of a cup of coffee, and the price of a muffin. Clearly explain your set-up for the problem. (Assume you are explaining your solution to someone who has not solved the problem herself\slash{}himself).%
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp8551944}%
Alice stopped by a coffee shop three days in a row at a conference to buy drinks and pastries. On the first day, she bought a cup of coffee, a muffin and a scone for which she paid \textdollar{}6.15. The next day she bought two cups of coffee, three muffins and a scone (for herself and friends). Her bill was \textdollar{}12.20. The last day she bought a cup of coffee, two muffins and two scones, and paid \textdollar{}10.35. Determine the price of a cup of coffee, the price of a muffin and the price of a scone. Clearly explain your set-up for the problem. (Assume you are explaining your solution to someone who has not solved the problem herself\slash{}himself).%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp8553864}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find an example of a system of two linear equations in variables \(x\), \(y\) for each of the following three cases:%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}where the equations correspond to two non-parallel lines,%
\item{}two parallel distinct lines,%
\item{}two identical lines (represented with different equations).%
\end{enumerate}
\item{}Describe how the relationship between the coefficients of the variables of the two equations in parts (ii) and (iii) are different than the relationship between those coefficients in part (i) (Note: Please make sure your system examples are different than the examples in the activities, and that they are your own examples.)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp8558600}%
In a grid of wires in thermal equilibrium, the temperature at interior nodes is the average of the temperatures at adjacent nodes. Consider the grid as shown in \hyperref[x:figure:F_Grid]{Figure~{\xreffont\ref{x:figure:F_Grid}}}, with \(x_1\), \(x_2\), and \(x_3\) the temperatures (in degrees Centigrade) at the indicated interior nodes, and fixed temperatures at the other nodes as shown. For example, the nodes adjacent to the node with temperature \(x_1\) have temperatures of \(x_2\), \(200\), \(0\), and \(0\), so when the grid is in thermal equilibrium \(x_1\) is the average of these temperatures:%
\begin{equation*}
x_1 = \frac{x_2+200+0+0}{4}\text{.}
\end{equation*}
%
\begin{figureptx}{A grid of wires.}{x:figure:F_Grid}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/Grid.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Determine equations for the temperatures \(x_2\) and \(x_3\) if the grid is in thermal equilibrium to construct a system of three linear equations in \(x_1\), \(x_2\), and \(x_3\) that models node temperatures in the grid in thermal equilibrium.%
\item{}Use the method of elimination to find a specific solution to the system that makes sense in context.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp8571144}%
We have seen that a linear system of two equations in two unknowns can have no solutions, one solution, or infinitely many solutions. Find, if possible, a specific example of each of the following. If not possible, explain why.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}A linear system of three equations in two unknowns with no solutions.%
\item{}A linear system of three equations in two unknowns with exactly one solution.%
\item{}A linear system of three equations in two unknowns with exactly two solutions.%
\item{}A linear system of three equations in two unknowns with infinitely many solutions.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp8575624}%
We have seen that a linear system of three equations in three unknowns can have no solutions, one solution, or infinitely many solutions. Find, if possible, a specific example of each of the following. If not possible, explain why.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}A linear system of two equations in three unknowns with no solutions.%
\item{}A linear system of two equations in three unknowns with exactly one solution.%
\item{}A linear system of two equations in three unknowns with exactly two solutions.%
\item{}A linear system of two equations in three unknowns with infinitely many solutions.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp8580872}%
Find a system of three linear equations in two variables \(u\), \(v\) whose solution is \(u=2\), \(v=1\).%
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp8584712}%
Consider the system of linear equations%
\begin{alignat*}{3}
{3} x_1   \amp  {}+{} \amp  hx_2 \amp  {}={} \amp  2\\
3x_1   \amp  {}+{} \amp  5x_2 \amp  {}={} \amp  1
\end{alignat*}
where \(h\) is an unknown constant.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Determine the solution(s) of this system for all possible \(h\) values, if a solution exists. (Note: Your answers for the variables will depend on the \(h\).)%
\item{}How do your answers change if the second equation in the system above is changed to \(3x_1+5x_2=6\)?%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{10}{}{}{g:exercise:idp8592392}%
Suppose we are given a system of two linear equations%
\begin{alignat}{4}
{4} x_1   \amp  {}+{} \amp  2x_2   \amp  {}-{}   \amp  x_3{}={}   \amp  { } \amp 1\amp {}\label{x:mrow:eq_1}\\
3x_1   \amp  {}+{} \amp  x_2   \amp  {}+{}   \amp  2x_3{}={}   \amp  {-} \amp 1\amp \text{.}\label{x:mrow:eq_2}
\end{alignat}
Find another system of two linear equations \(E_1\) and \(E_2\) in the variables \(x_1\), \(x_2\), and \(x_3\) that are not multiples of each other or of equations \hyperref[x:mrow:eq_1]{({\xreffont\ref{x:mrow:eq_1}})} or \hyperref[x:mrow:eq_2]{({\xreffont\ref{x:mrow:eq_2}})} so that any solution \((x_1, x_2, x_3)\) to the system \hyperref[x:mrow:eq_1]{({\xreffont\ref{x:mrow:eq_1}})} and \hyperref[x:mrow:eq_2]{({\xreffont\ref{x:mrow:eq_2}})} is a solution to the system \(E_1\) and \(E_2\).%
\end{divisionexercise}%
\begin{divisionexercise}{11}{True\slash{}False Questions.}{}{x:exercise:sec_intro_le_tf}%
In many sections you will be given True\slash{}False questions. In each of the True\slash{}False questions, you will be given a statement, such as ``if we add corresponding sides of two linear equations, then the resulting equation is a linear equation'' and ``one can find a system of two equations in two unknowns that has infinitely many solutions.'' Your task will be to determine the truth value of the statement and to give a brief justification for your choice.%
\par
Note that a \emph{general} statement is considered \emph{true} only when it is always true. For example, the first of the above statements \textemdash{} ``if we add corresponding sides of two linear equations, then the resulting equation is a linear equation'' \textemdash{} is a general statement. For this statement to be true, the equation we obtain by adding corresponding sides of any two linear equations has to be linear. If we can find two equations that do not give a linear equation when combined in this way, then this statement is false.%
\par
Note that an \emph{existential} statement is considered \emph{true} if there is at least one example which makes is true. For example, the latter of the above statements \textemdash{} ``one can find a system of two equations in two unknowns that has infinitely many solutions'' \textemdash{} is an existential statement. For this statement to be true, existence of a system of two equations in two unknowns with infinitely many solutions should suffice. If it is impossible to find two such equations, then this statement is false.%
\par
To justify that something always happens or never happens, one would need to refer to other statements whose truth is known, such as theorems, definitions. In particular, giving an \emph{example} of two linear equations that produce a linear equation when we add corresponding sides \emph{does not justify} why the sum of \emph{any} two linear equations is also linear. Using the definition of linear equations, however, we can justify why this new equation will always be linear: each side of a linear equation is linear, and adding linear expressions always produces a linear sum.%
\par
To justify that there are examples of something happening or not happening, one would need to give a specific example. For example, in justifying the claim that there is a system of two equations in two unknowns with infinitely many solutions, it is not enough to say ``An equation in two unknowns is a line in the \(xy\)-plane, so there can be two equations with the same line as their solution.'' In general, you should avoid the words ``can,'' ``possibly,'' ``maybe,'' etc., in your justifications. Instead, giving an example such as ``The linear system \(x+y=1\) and \(2x+2y=2\) of two equations in two unknowns has infinitely many solutions since the second equation gives the same line as the first in the \(xy\)-plane.'' provides complete justification beyond a reasonable doubt.%
\par
Each response to a True\slash{}False statement should be more than just True or False. It is important that you provide \emph{justification} for your responses.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
The set of all solutions of a linear equation can be represented graphically as a line.%
\item{}\lititle{True\slash{}False.}\par%
The set of all solutions of a linear equation in two variables can be represented graphically as a line.%
\item{}\lititle{True\slash{}False.}\par%
The set of all solutions of an equation in two variables can be represented graphically as a line.%
\item{}\lititle{True\slash{}False.}\par%
A system of three linear equations in two unknowns cannot have a unique solution.%
\item{}\lititle{True\slash{}False.}\par%
A system of three linear equations in three unknowns has a unique solution.%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Modeling an Electrical Circuit and the Wheatstone Bridge Circuit}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Modeling an Electrical Circuit and the Wheatstone Bridge Circuit}{}{Project: Modeling an Electrical Circuit and the Wheatstone Bridge Circuit}{}{}{x:section:sec_1_a_circuits}
\index{Ohm's Law}%
\index{Kirchoff's Current Law}%
\index{Kirchoff's Voltage Law}%
Mathematical modeling, or the act of creating equations to model given information, is an important part of problem solving. In this section we will see how we derived the system of equations%
\begin{alignat*}{3}
{}I_1 \  \amp - \  \amp {}I_2 \  \amp + \  \amp {}I_3 \  \amp = 0\\
5I_1 \  \amp + \  \amp 2I_2 \    \amp {} \   \amp {}  \    \amp = 8\\
\  \amp {} \   \amp 2I_2 \    \amp + \  \amp 4I_3 \    \amp = 5
\end{alignat*}
to represent the electrical current in the circuit shown in \hyperref[x:figure:F_circuit1]{Figure~{\xreffont\ref{x:figure:F_circuit1}}}. Recall that a circuit consists of%
\begin{itemize}[label=\textbullet]
\item{}one or more electrical sources (like a battery), denoted by \begin{image}{0.3}{0.4}{0.3}%
\includegraphics[width=\linewidth]{external/1_a_pa_source.pdf}
\end{image}%
%
\item{}one or more resistors (like any appliance that you plug into a wall outlet), denoted by \begin{image}{0.3}{0.4}{0.3}%
\includegraphics[width=\linewidth]{external/1_a_pa_resistor.pdf}
\end{image}%
 .%
\end{itemize}
%
\par
The source creates a charge that produces potential energy \(E\) measured in volts (V). No substance conducts electricity perfectly, there is always some price to pay (energy loss) to moving electricity. Electrical current \(I\) in amperes (A) is the flow of the electric charge in the circuit. (A current of 1 ampere means that \(6.2 \times 10^{18}\) electrons pass through the circuit per second.) Current flows out of the positive terminal of a source and runs through each branch of the circuit. Let \(I_1\) be the current flowing through the upper branch, \(I_2\) the current through middle branch, and \(I_3\) the current through the lower branch as illustrated in \hyperref[x:figure:F_circuit1]{Figure~{\xreffont\ref{x:figure:F_circuit1}}}. The goal is to find the current flowing in each branch of the circuit.%
\par
Linear algebra comes into play when analyzing a circuit based on the relationship between current, resistance, and potential. Three basic principles govern current low in a circuit.%
\begin{enumerate}
\item{}Resistance \(R\) in ohms (\(\Omega\)) can be thought of as a measure of how difficult it is to move a charge along a circuit. When a current flows through a resistor, it must expend some energy, called a \emph{voltage drop}. Ohm's Law states that the voltage drop \(E\) across a resistor is the product of the current \(I\) passing through the resistor and the resistance \(R\). That is,%
\begin{equation*}
E = IR\text{.}
\end{equation*}
%
\item{}Kirchoff's Current Law states that at any point in an electrical circuit, the sum of currents flowing into that point is equal to the sum of currents flowing out of that point.%
\item{}Kirchoff's Voltage Law says that around any closed loop the sum of the voltage drops is equal to the sum of the voltage rises.%
\end{enumerate}
%
\par
To see how these laws allow us to model the circuit in \hyperref[x:figure:F_circuit1]{Figure~{\xreffont\ref{x:figure:F_circuit1}}}, we will need three equations in \(I_1\), \(I_2\), and \(I_3\) to determine the values of these currents. Let us first apply Kirchoff's Current Law to the point P. The currents flowing into point P are \(I_1\) and \(I_3\), and the current flowing out is \(I_2\). This produces the equation \(I_1+I_3 = I_2\), or%
\begin{equation*}
I_1 - I_2 + I_3 = 0\text{.}
\end{equation*}
%
\begin{project}{}{g:project:idp8632840}%
Apply Kirchoff's Current Law to the point \(Q\) to obtain an equation in \(I_1\), \(I_2\), and \(I_3\). What do you notice?%
\end{project}%
We have three variables to determine, so we still need two more equations in \(I_1\), \(I_2\), and \(I_3\). Next we apply Kirchoff's Voltage Law to the top loop in the circuit in \hyperref[x:figure:F_circuit1]{Figure~{\xreffont\ref{x:figure:F_circuit1}}}. We will assume the following sign conventions:%
\begin{itemize}[label=\textbullet]
\item{}A current passing through a resistor produces a voltage drop if it flows in the direction of loop (and a voltage rise if the current passes in the opposite direction of the loop).%
\item{}A current passing through a source in the direction of the loop produces a voltage drop if it flows from \(+\) to \(-\) and a voltage rise if it flows from \(-\) to \(+\), while a current passing through a source in the opposite direction of the loop produces a voltage rise if it flows from \(+\) to \(-\) and a voltage drop if it flows from \(-\) to \(+\).%
\end{itemize}
%
\par
(The directions chosen in \hyperref[x:figure:F_circuit1]{Figure~{\xreffont\ref{x:figure:F_circuit1}}} for the voltage flow are arbitrary \textemdash{} if we reverse the flow then we just replace voltage drops with voltage rises and obtain the same equations. If a solution shows that a current is negative, then that current flows in the direction opposite of what is shown.)%
\par
If we move in the counterclockwise direction around the top loop in the circuit in \hyperref[x:figure:F_circuit1]{Figure~{\xreffont\ref{x:figure:F_circuit1}}}, there is a voltage rise through the source of 8 volts. This must equal the voltage drop in this loop. The current \(I_1\) passing though the resistor of resistance 2 \(\Omega\) produces a voltage drop of \(2I_1\) volts. Similarly, the current \(I_1\) passing through the resistor of resistance \(3 \Omega\) produces a voltage drop of \(3I_1\) volts. Finally, the current \(I_2\) passing through the resistor of resistance 2 \(\Omega\) produces a voltage drop of \(2I_2\) volts. So Kirchoff's Voltage Law applied to the top loop in the circuit in \hyperref[x:figure:F_circuit1]{Figure~{\xreffont\ref{x:figure:F_circuit1}}} gives us the equation \(2I_1 + 3I_1 + 2I_2 = 8\) or%
\begin{equation*}
5I_1 + 2I_2 = 8\text{.}
\end{equation*}
%
\begin{project}{}{g:project:idp8649736}%
Apply Kirchoff's Voltage Law to the bottom loop in the circuit in \hyperref[x:figure:F_circuit1]{Figure~{\xreffont\ref{x:figure:F_circuit1}}} to obtain an equation in \(I_1\), \(I_2\), and \(I_3\). Compare the three equations we have found to those in the introduction.%
\end{project}%
\begin{figureptx}{A Wheatstone bridge circuit.}{x:figure:F_Wheatstone}{}%
\begin{image}{0.3}{0.4}{0.3}%
\includegraphics[width=\linewidth]{external/Wheatstone.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\begin{project}{}{g:project:idp8655240}%
Consider the circuit as shown in \hyperref[x:figure:F_Wheatstone]{Figure~{\xreffont\ref{x:figure:F_Wheatstone}}}, with a single source and five resistors with resistances \(R_1\), \(R_2\), \(R_3\), \(R_4\), and \(R_5\) as labeled.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Use Kirchoff's Current Law to show that \(I_0 = I_1+I_2\), \(I_3 = I_1-I_5\), and \(I_4 = I_2 + I_5\). Thus, we reduce the problem to three variables.%
\item{}Apply Kirchoff's Voltage Law to three loops to show that the currents must satisfy the linear system%
\begin{alignat}{4}
{4} {2}I_1  \amp {}{}    \amp {}     \amp {}-{}  \amp {}I_5   \amp = \amp  \ 13\amp {}\label{x:mrow:eq_Wheatstone_1}\\
{}     \amp {}{}    \amp {3}I_2   \amp {}+{}  \amp {2}I_5   \amp = \amp  \ 13\amp {}\label{x:mrow:eq_Wheatstone_2}\\
{}I_1   \amp {}-{}  \amp {}I_2   \amp {}+{}  \amp {}I_5   \amp = \amp  \ 0\amp {.}\label{x:mrow:eq_Wheatstone_3}
\end{alignat}
%
\item{}Solve the system to find the unknown currents.%
\end{enumerate}
\item{}The circuit pictured in \hyperref[x:figure:F_Wheatstone]{Figure~{\xreffont\ref{x:figure:F_Wheatstone}}} is called a \emph{Wheatstone bridge} (invented by Samuel Hunter Christie in 1833 and popularized by Sir Charles Wheatstone in 1843). The Wheatstone bridge is a circuit designed to determine an unknown resistance by balancing two paths in a circuit. It is set up so that the resistances of resistors \(R_1\) and \(R_2\) are known, \(R_3\) is a variable resistor and we want to find the resistance of \(R_4\). The resistor \(R_5\) is replaced with a voltmeter, and the resistance of \(R_3\) is varied until the voltmeter reads \(0\). This balances the circuit and tells the resistance of resistor \(R_4\). Show that if the current \(I_5\) in \hyperref[x:figure:F_Wheatstone]{Figure~{\xreffont\ref{x:figure:F_Wheatstone}}} is \(0\) (so the circuit is balanced), then \(R_4 = \frac{R_2R_3}{R_1}\) (which is how we calculate the unknown resistance \(R_4\)). Do this in general and do not use any specific values for the resistances or the voltage.%
\end{enumerate}
\end{project}%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 2 The Matrix Representation of a Linear System}
\typeout{************************************************}
%
\begin{chapterptx}{The Matrix Representation of a Linear System}{}{The Matrix Representation of a Linear System}{}{}{x:chapter:chap_matrix_representation}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp8677512}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What is a matrix?%
\item{}How do we associate a matrix to a system of linear equations?%
\item{}What row operations can we perform on an augmented matrix of a linear system to solve the system of linear equations?%
\item{}What are pivots, basic variables, and free variables?%
\item{}How many solutions can a system of linear equations have?%
\item{}When is a linear system consistent?%
\item{}When does a linear system have infinitely many solutions? A unique solution?%
\item{}How can we represent the set of solutions to a consistent system if the system has infinitely many solutions?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Approximating Area Under a Curve}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Approximating Area Under a Curve}{}{Application: Approximating Area Under a Curve}{}{}{x:section:sec_appl_area_curve}
We know from basic geometry how to find areas of circles and triangles. However, it is much more difficult to find areas of other geometric objects. In fact, it is generally an impossible problem to determine the exact area bounded by a complicated curve. For this reason, approximation methods are used. One such method involves approximating curves using quadratic functions.%
\par
Unless you have learned some calculus, you have probably never calculated the area under a parabola. In the ancient work \pubtitle{Quadrature of the Parabola} (3rd century BC), Archimedes determined a method for finding the area of a region bounded by a parabola by using mechanics and then by geometric methods. Once we know how to calculate the area of a region bounded by a parabola, Simpson's Rule uses parabolas to approximate a function, and then approximates the area under the graph of the graph of the function by using the areas under the parabolas. In order to use Simpsons Rule, we need to know how to exactly fit a quadratic function to three points. More details about this process can be found at the end of this section. This idea of fitting a polynomial to a set of data points has uses in other areas as well. For example, two common applications of Bézier curves are font design and drawing tools. When fitting a polynomial to a large set of data points, our systems of equations can become quite large, and can be difficult to solve by hand. In this section we will see how to use matrices to more conveniently represent systems of equations of any size. We also consider how the elimination process works on the matrix representation of a linear system and how we can determine the existence of solutions and the form of solutions of a linear system.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_mtx_lin_intro}
When working with a linear system, the labels for the variables are irrelevant to the solution \textemdash{} the only thing that matters is the coefficients of the variables in the equations and the constants on the other side of the equations. For example, given a linear system of the form%
\begin{alignat}{3}
a_2   \amp {}-{}   \amp a_1   \amp {}+{}  \amp a_0 \amp = 2\notag\\
a_2   \amp {}+{}   \amp a_1   \amp {}+{}   \amp a_0 \amp = 6\label{x:mrow:eq_PA_1}\\
4a_2  \amp {}+{}   \amp 2a_1   \amp {}+{}   \amp a_0  \amp = 5\text{,}\notag
\end{alignat}
the important information in the system can be represented as%
\begin{align*}
1 \amp \amp -1 \amp \amp 1 \amp \amp 2\\
1 \amp \amp 1 \amp \amp 1 \amp \amp 6\\
4 \amp \amp 2 \amp \amp 1 \amp \amp 5
\end{align*}
where we interpret the first three numbers in each horizontal row to represent the coefficients of the variables \(a\), \(b\) and \(c\), respectively, and the last number to be the constant on the right hand side of the equation. This tells us that we can record all the necessary information about our system in a rectangular array of numbers. Such an array is called a \terminology{matrix}.%
\begin{definition}{}{g:definition:idp8419336}%
\index{matrix}%
A \terminology{matrix} is a rectangular array of quantities or expressions.%
\end{definition}
We usually delineate a matrix by enclosing its entries in square brackets \([ * ]\). For the system in \hyperref[x:mrow:eq_PA_1]{({\xreffont\ref{x:mrow:eq_PA_1}})}, there are two corresponding matrices:%
\begin{sidebyside}{2}{0}{0}{0}%
\end{sidebyside}%
\par
\index{coefficient matrix}\index{augmented matrix} The matrix on the left is the matrix of the coefficients of the system, and is called the \terminology{coefficient matrix} of the system. The matrix on the right is the matrix of coefficients and the constants, and is called the \terminology{augmented matrix} of the system (where we say we augment the coefficient matrix with the additional column of constants). We will separate the augmented column from the coefficient matrix with a vertical line to keep it clear that the last column is an augmented column of constants and not a column of coefficients.\footnote{You should note that not every author uses this convention \textemdash{} when they do not, it is important that you be careful to understand if the matrix has an augmented column or not.\label{g:fn:idp8424968}}%
\begin{paragraphs}{Terminology.}{g:paragraphs:idp8426504}%
\index{matrix!entry}%
\index{matrix!row}%
\index{matrix!column}%
\index{matrix!size}%
There is some important terminology related to matrices.%
\begin{itemize}[label=\textbullet]
\item{}Any number in a matrix is called an \terminology{entry} of the matrix.%
\item{}The collection of entries in an augmented matrix that corresponds to a given equation (that is reading the entries from left to right, or a horizontal set of entries) is called a \terminology{row} of the matrix. We number the rows from top to bottom in a matrix. For example, \(\left[ \begin{array}{crc} 1\amp -1\amp 1 \end{array} \right]\) is the first row and \(\left[ \begin{array}{ccc} 1\amp 1\amp 1 \end{array} \right]\) is the second row of the coefficient matrix of the system \hyperref[x:mrow:eq_PA_1]{({\xreffont\ref{x:mrow:eq_PA_1}})}.%
\item{}The set of entries as we read from top to bottom (or a vertical set of entries that correspond to one fixed variable or the constants on the right hand sides of the equations) is called a \terminology{column} of the matrix. We number the columns from left to right in a matrix. For example, \(\left[ \begin{array}{c} 1 \\ 1 \\ 4 \end{array} \right]\) is the first column and \(\left[ \begin{array}{c} 1 \\ 1 \\ 1 \end{array} \right]\) is the third column of the coefficient matrix of the system \hyperref[x:mrow:eq_PA_1]{({\xreffont\ref{x:mrow:eq_PA_1}})}.%
\item{}The \terminology{size} of a matrix is given as \(m\times n\) where \(m\) is the number of rows and \(n\) is the number of columns. The coefficient matrix above is a \(3\times 3\) matrix since it has 3 rows and 3 columns, while the augmented matrix is a \(3\times 4\) matrix as it has 4 columns.%
\end{itemize}
%
\end{paragraphs}%
\begin{exploration}{}{x:exploration:pa_1_b}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Write the augmented matrix for the following linear system. If needed, rearrange an equation to ensure that the variables appear in the same order on the left side in each equation with the constants being on the right hand side of each equation.%
\begin{align}
-x_3 + 3 + 2x_2\amp = -x_1\notag\\
-3 + 2x_3 \amp = -x_2\label{g:mrow:idp8865800}\\
-2x_2 + x_1 \amp = 3x_3-7\notag
\end{align}
%
\item{}Write the linear system in variables \(x_1, x_2\) and \(x_3\), appearing in the natural order that corresponds to the following augmented matrix. Then solve the linear system using the elimination method.%
\begin{equation*}
\left[ \begin{array}{rrr|r} 1 \amp  1 \amp  -1 \amp  4 \\ 1 \amp  2 \amp  2 \amp  3 \\ 2 \amp  3 \amp  -3 \amp  11 \end{array}  \right]
\end{equation*}
%
\item{}Consider the three types of elementary operations on systems of equations introduced in \hyperref[x:chapter:chap_intro_linear_systems]{Section~{\xreffont\ref{x:chapter:chap_intro_linear_systems}}}. Each row of an augmented matrix of a system corresponds to an equation, so each elementary operation on equations corresponds to an operation on rows (called row operations).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Describe the row operation that corresponds to interchanging two equations.%
\item{}Describe the row operation that corresponds to multiplying an equation by a nonzero scalar.%
\item{}Describe the row operation that corresponds to replacing one equation by the sum of that equation and a scalar multiple of another equation.%
\end{enumerate}
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Simplifying Linear Systems Represented in Matrix Form}
\typeout{************************************************}
%
\begin{sectionptx}{Simplifying Linear Systems Represented in Matrix Form}{}{Simplifying Linear Systems Represented in Matrix Form}{}{}{x:section:sec_simp_mtx_sys}
Once we have stored the information about a linear system in an augmented matrix, we can perform the elementary operations directly on the augmented matrix.%
\par
Recall that the allowable operations on a system of equations are the following:%
\begin{enumerate}
\item{}Replacing one equation by the sum of that equation and a scalar multiple of another equation.%
\item{}Interchanging the positions of two equations.%
\item{}Replacing an equation by a nonzero scalar multiple of itself.%
\end{enumerate}
%
\par
\index{row operations} Recall that we use these elementary operations to transform a system, with the ultimate goal of finding a simpler, equivalent system that we can solve. Since each row of an augmented matrix corresponds to an equation, we can translate these operations on equations to corresponding operations on rows (called \terminology{row operations} or \terminology{elementary row operations}):%
\begin{enumerate}
\item{}Replacing one row by the sum of that row and a scalar multiple of another row.%
\item{}Interchanging two rows.%
\item{}Replacing a row by a nonzero scalar multiple of itself.%
\end{enumerate}
%
\begin{activity}{}{x:activity:act_A1_2_1}%
Consider the system%
\begin{alignat*}{3}
{}a_2  \amp {}-{}   \amp {}a_1   \amp {}+{}  \amp {}a_0   \amp = 2\\
{}a_2   \amp {}+{}  \amp {}a_1   \amp {}+{}  \amp {}a_0  \amp = 6\\
4a_2     \amp {}+{}   \amp 2a_1   \amp {}+{}   \amp {}a_0   \amp = 5
\end{alignat*}
with corresponding augmented matrix%
\begin{equation*}
\left[ \begin{array}{crc|c} 1   \amp -1    \amp 1   \amp 2 \\ 1  \amp 1    \amp 1  \amp 6 \\ 4  \amp 2    \amp 1  \amp 5 \end{array}  \right]
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}As a first step in solving our system, we might eliminate \(a_2\) from the second equation. This means that the corresponding entry in the second row and first column of the augmented matrix will become 0. Find a row operation that adds a multiple of the first row to the second row to achieve this goal. Then write the system of equations that corresponds to this new augmented matrix.%
\item{}Now that we have eliminated the \(a_2\) terms from the second equation, we eliminate the \(a_2\) term from the third equation. Find an appropriate row operation that does that, and write the corresponding system of linear equations that corresponds to the new augmented matrix.%
\item{}Now you should have a system in which the last two rows correspond to a system of 2 linear equations in two unknowns. Use a row operation that adds a multiple of the second row to the third row to turn the coefficient of \(a_1\) in the third row to 0. Then write the corresponding system of linear equations.%
\item{}Your simplified system and its augment matrix are in \terminology{row echelon form} and this system is solvable using \terminology{back-substitution} (substituting the known variable values into the previous equation to find the value of another variable). Solve the system.%
\end{enumerate}
\end{activity}%
\begin{remark}{}{g:remark:idp8885768}%
Do you see how this standard elimination process can be generalized to any linear system with any number of variables to produce a simplified system? Do you see why the process does not change the solutions of the system? If needed, can you modify the standard elimination process to obtain a simplified system in which the last equation contains only the variable \(a_2\), the next to last equation contains only the variables \(a_1, a_2\), etc.? Understanding the standard process will enable you to be able to modify it, if needed, in a problem.%
\end{remark}
\hyperref[x:activity:act_A1_2_1]{Activity~{\xreffont\ref{x:activity:act_A1_2_1}}} illustrates how we can perform all of the operations on equations with operations on the rows of augmented matrices to reduce a system to a solvable form. Each time we perform an operation on the system of equations (or on the rows of an augmented matrix) we obtain an equivalent system (or an augmented matrix corresponding to an equivalent system). For completeness, we list the operations on equations and the corresponding row operations below that can be used to solve our polynomial fitting system. Throughout the process we will let \(E_1\), \(E_2\), and \(E_3\) be the first, second, and third equations in the system and \(R_1\), \(R_2\), and \(R_3\) the first, second, and third rows of the augmented matrices. The notation \(E_1+E_2\) placed next to equation \(E_2\) means means that we replace the second equation in the system with the sum of the first two equations. We start with the system%
\begin{alignat*}{3}
{}a_2  \amp {}-{}   \amp {}a_1   \amp {}+{}  \amp {}a_0   \amp = 2\\
{}a_2   \amp {}+{}  \amp {}a_1   \amp {}+{}  \amp {}a_0  \amp = 6\\
4a_2     \amp {}+{}   \amp 2a_1   \amp {}+{}   \amp {}a_0   \amp = 5
\end{alignat*}
%
\par
On the left we demonstrate the operations on equations and on the right the corresponding operations on rows of the augmented matrix.%
\begin{sidebyside}{4}{0.01}{0.06}{0.0433333333333333}%
\begin{sbspanel}{0.15}[center]%
\(\begin{array}{c}  \text{ }  \\  E_2-E_1\to E_2   \\ \text{ } \end{array}\)%
\end{sbspanel}%
\begin{sbspanel}{0.25}[center]%
\par
%
\begin{alignat*}{3}
{}a_2  \amp {}-{}   \amp {}a_1   \amp {}+{}  \amp {}a_0   \amp {}= 2\\
{}     \amp {}    \amp 2a_1   \amp {}    \amp {}     \amp {}= 4\\
4a_2   \amp {}+{}   \amp 2a_1   \amp {}+{}   \amp {}a_0   \amp {}= 5
\end{alignat*}
%
\end{sbspanel}%
\begin{sbspanel}{0.15}[center]%
\par
\(R_2-R_1\to R_2\)%
\end{sbspanel}%
\begin{sbspanel}{0.25}[center]%
\par
%
\begin{equation*}
\left[ \begin{array}{rrr|r} 1 \amp -1 \amp 1 \amp 2 \\ 0 \amp 2 \amp 0 \amp 4 \\ 4 \amp 2\amp 1 \amp 5
\end{array}  \right]
\end{equation*}
%
\end{sbspanel}%
\end{sidebyside}%
\begin{sidebyside}{4}{0.01}{0.06}{0.0433333333333333}%
\begin{sbspanel}{0.15}[center]%
\(\begin{array}{c}  \text{ }  \\  \text{ }   \\ E_3-4E_1\to E_3 \end{array}\)%
\end{sbspanel}%
\begin{sbspanel}{0.25}[center]%
\par
%
\begin{alignat*}{4}
{}a_2  \amp {}-{}   \amp {}a_1     \amp {}+{}  \amp {}a_0   \amp {}= \amp  \ {}\amp 2\\
{}       \amp {}    \amp 2a_1     \amp {}    \amp {}     \amp {}= \amp  \ {}\amp 4\\
{}     \amp {}     \amp 6a_1     \amp {}-{}   \amp 3a_0   \amp {}= \amp  \ {-}\amp 3
\end{alignat*}
%
\end{sbspanel}%
\begin{sbspanel}{0.15}[center]%
\par
\(\begin{array}{c}  \text{ }  \\  \text{ }   \\ R_3-4R_1\to R_3 \end{array}\)%
\end{sbspanel}%
\begin{sbspanel}{0.25}[center]%
\par
%
\begin{equation*}
\left[ \begin{array}{rrr|r} 1    \amp -1   \amp 1 \amp 2   \\ 0  \amp 2    \amp 0 \amp 4  \\ 0  \amp 6    \amp -3 \amp -3
\end{array}  \right]
\end{equation*}
%
\end{sbspanel}%
\end{sidebyside}%
\begin{sidebyside}{4}{0.01}{0.06}{0.0433333333333333}%
\begin{sbspanel}{0.15}[center]%
\(\begin{array}{c}  \text{ }  \\  \text{ }   \\ E_3-3E_2\to E_3 \end{array}\)%
\end{sbspanel}%
\begin{sbspanel}{0.25}[center]%
\par
%
\begin{alignat*}{5}
{}a_2  \amp {}-{}   \amp {}a_1     \amp {}+{}  \amp {}a_0   \amp {}= \amp  \ {}\amp \amp {}2\\
{}       \amp {}    \amp 2a_1     \amp {}    \amp {}     \amp {}= \amp  \ {}\amp \amp {}4\\
{}     \amp {}     \amp {}       \amp {}-{}  \amp 3a_0   \amp {}= \amp  \ {-}\amp \amp {15}
\end{alignat*}
%
\end{sbspanel}%
\begin{sbspanel}{0.15}[center]%
\par
\(\begin{array}{c}  \text{ }  \\  \text{ }   \\ R_3-3R_2\to R_3 \end{array}\)%
\end{sbspanel}%
\begin{sbspanel}{0.25}[center]%
\par
%
\begin{equation*}
\left[ \begin{array}{rrr|r} 1    \amp -1   \amp 1 \amp 2   \\ 0  \amp 2    \amp 0 \amp 4  \\ 0  \amp 0    \amp -3 \amp -15
\end{array}  \right]
\end{equation*}
%
\end{sbspanel}%
\end{sidebyside}%
Now we can solve the last equation for \(a_0\) to find that \(a_0=5\). The second equation gives us \(a_1 = 2\).\footnote{If there had been an \(a_0\) term in the second equation, we could have substituted \(a_0=5\) and solved for \(a_1\)\label{g:fn:idp8902408}} Finally, using the first equation with the already determined values of \(a_0\) and \(a_1\) gives us \(a_2=-1\). Thus we have found the solution to the polynomial fitting system to be \(a_2=-1\), \(a_1=2\), and \(a_0=5\).%
\par
We summarize the steps of the (partial) elimination on matrices we used above to solve a general linear system in the variables \(x_1\), \(x_2\), \(\ldots\), \(x_n\).%
\begin{enumerate}
\item{}Interchange equations if needed to ensure that the coefficient of \(x_1\) (or, more generally, the first non-zero variable) in the first equation is non-zero.%
\item{}Use the first equation to eliminate \(x_1\) (or, the first non-zero variable) from other equations by adding a multiple of the first equation to the others.%
\item{}After \(x_1\) is eliminated from all equations but the first equation, focus on the rest of the equations. Repeat the process of elimination on these equations to eliminate \(x_2\) (or, the next non-zero variable) all but the second equation.%
\item{}Once the process of eliminating variables recursively is finished, solve for the variables in a backwards fashion starting with the last equation and substituting known values in the equations above as they become known.%
\end{enumerate}
%
\par
\index{forward elimination}\index{back substitution} This elimination method where the variables are eliminated from lower equations is called the \terminology{forward elimination phase} as it eliminates variables in the forward direction. Solving for variables using substitution into upper equations is called \terminology{back substitution}. The matrix representation of a linear system after the forward elimination process is said to be in \terminology{row echelon form}. We will define this form and the elimination process on the matrices more precisely in the next section.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Linear Systems with Infinitely Many Solutions}
\typeout{************************************************}
%
\begin{sectionptx}{Linear Systems with Infinitely Many Solutions}{}{Linear Systems with Infinitely Many Solutions}{}{}{x:section:sec_sys_inf_sols}
Each of the systems that we solved so far have had a unique (exactly one) solution. The geometric representation of linear systems with two equations in two variables shows that this does not always have to be the case. We also have linear systems with no solution and systems with infinitely many solutions. We now consider the problem of how to represent the set of solutions of a linear system that has infinitely many solutions. (Systems with infinitely many solutions will also be of special interest to us a bit later when we study eigenspaces of a matrix.)%
\begin{activity}{}{x:activity:act_A1_2_3}%
Consider the system%
\begin{alignat*}{4}
{}x_1  \amp {}+{}   \amp {2}x_2    \amp {}-{}  \amp {}x_3   \amp {}= \amp  \ 1\amp {}\\
{}x_1  \amp {}+{}  \amp {}x_2     \amp {}-{}  \amp {3}x_3  \amp {}= \amp  \ 0\amp {}\\
{2}x_1  \amp {}+{}  \amp {3}x_2    \amp {}-{}  \amp {4}x_3 \amp {}= \amp  \ 1\amp {.}
\end{alignat*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Without explicitly solving the system, check that \((-1,1,0)\) and \((4,-1,1)\) are solutions to this system.%
\item{}Without explicitly solving the system, show that \(x_1 = -1+5t\), \(x_2 = 1-2t\), and \(x_3 = t\) is a solution to this system for any value of \(t\). What values of \(t\) yield the solutions \((-1,1,0)\) and \((4,-1,1)\) from part (a)? The equations \(x_1 = -1+5t\), \(x_2 = 1-2t\), and \(x_3 = t\) form what is called a \terminology{parametric solution} to the system with \terminology{parameter \(t\)}.%
\item{}Part (b) shows that our system has infinitely many solutions. We were given solutions in part (b) \textemdash{} but how do we find these solutions and how do we know that these are all of the solutions? We address those questions now. If we apply row operations to the augmented matrix%
\begin{equation*}
\left[ \begin{array}{ccr|c} 1\amp 2\amp -1\amp 1 \\ 1\amp 1\amp -3\amp 0 \\ 2\amp 3\amp -4\amp 1 \end{array}  \right]
\end{equation*}
of this system, we can reduce this system to one with augmented matrix%
\begin{equation*}
\left[ \begin{array}{ccr|c} 1\amp 2\amp -1\amp 1 \\ 0\amp 1\amp 2\amp 1 \\ 0\amp 0\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}What is it about this reduced form of the augmented matrix that indicates that the system has infinitely many solutions?%
\item{}Since the system has infinitely many solutions, we will not be able to explicitly determine values for each of the variables. Instead, at least one of the variables can be chosen arbitrarily. What is it about the reduced form of the augmented matrix that indicates that \(x_3\) is convenient to choose as the arbitrary variable?%
\item{}Letting \(x_3\) be arbitrary (we call \(x_3\) a \emph{free} variable), use the second row to show that \(x_2 = 1-2x_3\) (so that we can write \(x_2\) in terms of the arbitrary variable \(x_3\)).%
\item{}Use the first row to show that \(x_1 = 5x_3-1\) (and we can write \(x_1\) in terms of the arbitrary variable \(x_3\)). Compare this to the solutions from part (b).%
\end{enumerate}
\end{enumerate}
\end{activity}%
After using the elimination method, the first non-zero coefficient (from the left) of each equation in the linear system is in a different position. We call each such coefficient a \terminology{pivot} and a variable corresponding to a pivot a \terminology{basic variable}. In the system%
\begin{alignat*}{5}
{}a_2  \amp {}-{}   \amp {}a_1     \amp {}+{}  \amp {}a_0   \amp {}= \amp  \ {}\amp \amp {}2\\
{}       \amp {}    \amp {2}a_1    \amp {}    \amp {}     \amp {}= \amp  \ {}\amp \amp {}4\\
{}     \amp {}     \amp {}       \amp {}-{}  \amp {3}a_0  \amp {}= \amp  \ {-}\amp \amp {15}
\end{alignat*}
the basic variables are \(a_2, a_1, a_0\) for the first, second, and third equations, respectively. In the system,%
\begin{alignat*}{4}
{}x_1  \amp {}+{}   \amp {2}x_2   \amp {}-{}  \amp {}x_3   \amp {}= \amp  \ 1\\
{}       \amp {}    \amp {}x_2     \amp {}+{}  \amp {2}x_3  \amp {}= \amp  \ 1\\
{}     \amp {}     \amp {}       \amp {}    \amp {}0   \amp {}= \amp  0
\end{alignat*}
the basic variables are \(x_1\) and \(x_2\) for the first and second equations, respectively, while the third equation does not have a basic variable. Through back-substitution, we can solve for each variable in a unique way if each appears as the basic variable in an equation. If, however, a variable is \terminology{free}, meaning that it is not the basic variable of an equation, we cannot solve for that variable explicitly. We instead assign a distinct parameter to each such free variable and solve for the basic variables in terms of these parameters.%
\begin{definition}{}{g:definition:idp8938504}%
\index{pivot}%
\index{basic variable}%
\index{free variable}%
The first non-zero coefficient (from the left) in an equation in a linear system after elimination is called a \terminology{pivot}. A variable corresponding to a pivot is a \terminology{basic variable} and while a variable not corresponding to a pivot is a \terminology{free variable}.%
\end{definition}
\begin{activity}{}{x:activity:act_A1_2_4}%
Each matrix is an augmented matrix for a linear system after elimination. Identify the basic variables (if any) and free variables (if any). Then write the general solution (if there is a solution) expressing all variables in terms of the free variables. Use any symbols you like for the variables.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\left[ \begin{array}{ccc|c} 1\amp 0\amp 2\amp 1 \\ 0\amp 3\amp 1\amp 0 \\ 0\amp 0\amp 0\amp 0 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{ccr|c} 1\amp 0\amp -1\amp 1 \\ 0\amp 0\amp 1\amp 2 \\ 0\amp 0\amp 0\amp 0 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{ccrc|c} 1\amp 2\amp -1\amp 1\amp 1 \\ 0\amp 1\amp 0\amp 2\amp 1 \\ 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0 \end{array} \right]\)%
\end{enumerate}
\end{activity}%
\begin{remark}{}{g:remark:idp8949640}%
Does the existence of a row of 0's always mean a free variable? Can you think of an example where there is a row of 0's but none of the variables is free? How do the numbers of equations and the variables compare in that case?%
\end{remark}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Linear Systems with No Solutions}
\typeout{************************************************}
%
\begin{sectionptx}{Linear Systems with No Solutions}{}{Linear Systems with No Solutions}{}{}{x:section:sec_sys_no_sols}
We saw in the previous section that geometrically two parallel and distinct lines represent a linear system with two equations in two unknowns which has no solution. Similarly, two parallel and distinct planes in three dimensions represent a linear system with two equations in three unknowns which has no solution. We can have at least four different geometric configurations of three planes in three dimensions representing a system with no solution. But how do these geometrical configurations manifest themselves algebraically?%
\begin{activity}{}{g:activity:idp8947080}%
Consider the linear system%
\begin{alignat*}{4}
x_1     \amp {}-{}  \amp x_2   \amp {}+{}  \amp {}x_3  \amp {}={} \amp  \ 2 \amp {}\\
x_1    \amp {}+{}  \amp x_2  \amp {}-{}  \amp {3}x_3  \amp {}={} \amp  \ 1 \amp {}\\
{3}x_1  \amp {}-{}  \amp x_2  \amp {}-{}  \amp {}x_3  \amp {}={}  \amp  \ 6 \amp {.}
\end{alignat*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Apply the elimination process to the augmented matrix of this system. Write the system of equations that corresponds to the final reduced matrix.%
\item{}Discuss which feature in the final simplified system makes it easy to determine that the system has no solution. Similarly, what features in the matrix representation makes is easy to see the system has no solution?%
\end{enumerate}
\end{activity}%
We summarize our observations about when a system has a solution, and which of those cases has a unique solution.%
\begin{theorem}{}{}{g:theorem:idp8955656}%
A linear system is consistent if after the elimination process there is no equation of the form \(0=b\) where \(b\) is a non-zero number. If a linear system is consistent and has a free variable, then it has infinitely many solutions. If it is consistent and has no free variables, then there is a unique solution.%
\end{theorem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_mtx_sys_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp8953096}%
Consider the linear system%
\begin{alignat*}{5}
{}x_1    \amp {}-{}   \amp { }x_2    \amp {}{}    \amp {}    \amp {}+{}  \amp {2}x_4   \amp {}= \amp  \ 1\amp {}\\
{2}x_1  \amp {}+{}   \amp {3}x_2    \amp {}-{}  \amp {2}x_3  \amp {}+{}  \amp {5}x_4   \amp {}= \amp  \ 4\amp {}\\
{}x_1    \amp {}-{}   \amp {}x_2    \amp {}+{}  \amp {}x_3  \amp {}-{}  \amp { }x_4   \amp {}= \amp  \ 0\amp {}\\
x_1  \amp {}+{}   \amp {}x_2    \amp {}-{}  \amp {}x_3  \amp {}+{}  \amp {6}x_4   \amp {}= \amp  \ 5\amp {.}
\end{alignat*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Set up the augmented matrix for this linear system.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp8955144}{}\quad{}The augmented matrix for this system is%
\begin{equation*}
\left[ \begin{array}{crrr|c} 1\amp -1\amp 0\amp 2\amp 1 \\ 2\amp 3\amp -2\amp 5\amp 4 \\ 1\amp -1\amp 1\amp -1\amp 0 \\ 4\amp 1\amp -1\amp 6\amp 5 \end{array}  \right]\text{.}
\end{equation*}
%
\item{}Find all solutions to the system using forward elimination.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp8959496}{}\quad{}We apply forward elimination, first making the entries below the 1 in the upper left all 0. We do this by replacing row two with row two minus 2 times row 1, row three with row three minus row 1, and row four with row four minus 4 row one. This produces the augmented matrix%
\begin{equation*}
\left[ \begin{array}{crrr|r} 1\amp -1\amp 0\amp 2\amp 1 \\ 0\amp 5\amp -2\amp 1\amp 2 \\ 0\amp 0\amp 1\amp -3\amp -1 \\ 0\amp 5\amp -1\amp -2\amp 1 \end{array}  \right]\text{.}
\end{equation*}
Now we eliminate the leading 5 in the fourth row by replacing row four with row four minus row two to obtain the augmented matrix%
\begin{equation*}
\left[ \begin{array}{crrr|r} 1\amp -1\amp 0\amp 2\amp 1 \\ 0\amp 5\amp -2\amp 1\amp 2 \\ 0\amp 0\amp 1\amp -3\amp -1 \\ 0\amp 0\amp 1\amp -3\amp -1 \end{array}  \right]\text{.}
\end{equation*}
When we replace row four with row four minus row three, we wind up with a row of zeros:%
\begin{equation*}
\left[ \begin{array}{crrr|r} 1\amp -1\amp 0\amp 2\amp 1 \\ 0\amp 5\amp -2\amp 1\amp 2 \\ 0\amp 0\amp 1\amp -3\amp -1 \\ 0\amp 0\amp 0\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
We see that there is no pivot in column four, so \(x_4\) is a free variable. We can solve for the other variables in terms of \(x_4\). The third row shows us that%
\begin{align*}
x_3 - 3x_4 \amp = -1\\
x_3 \amp = 3x_4 - 1\text{.}
\end{align*}
The second row tells us that%
\begin{alignat*}{1}
5x_2 - 2x_3 + x_4 \amp = 2\\
5x_2 \amp = 2x_3 - x_4 + 2\\
5x_2 \amp = 2(3x_4-1) - x_4 + 2\\
5x_2 \amp = 5x_4\\
x_2 \amp = x_4\text{.}
\end{alignat*}
Finally, the first row gives us%
\begin{alignat*}{1}
x_1-x_2+2x_4 \amp = 1\\
x_1 \amp = x_2 - 2x_4 + 1\\
x_1 \amp = x_4 - 2x_4 + 1\\
x_1 \amp = -x_4 + 1\text{.}
\end{alignat*}
So this system has infinitely many solutions, with \(x_1 = -x_4 + 1\), \(x_2 = x_4\), \(x_3 = 3x_4 - 1\), and \(x_4\) is arbitrary. As a check, notice that%
\begin{equation*}
(-x_4+1) - x_4 + 2x_4 = 1
\end{equation*}
and so this solution satisfies the first equation in our system. You should check to verify that it also satisfies the other three equations.%
\item{}Suppose, after forward elimination, the augmented matrix of the system%
\begin{alignat*}{5}
{}x_1    \amp {}-{}   \amp { }x_2    \amp {}{}    \amp {}    \amp {}+{}  \amp {2}x_4   \amp {}= \amp  \ 1\amp {}\\
{2}x_1  \amp {}+{}   \amp {3}x_2    \amp {}-{}  \amp {2}x_3  \amp {}+{}  \amp {5}x_4   \amp {}= \amp  \ 4\amp {}\\
{}x_1    \amp {}-{}   \amp {}x_2    \amp {}+{}  \amp {}x_3  \amp {}-{}  \amp { }x_4   \amp {}= \amp  \ 0\amp {}\\
x_1  \amp {}+{}   \amp {}x_2    \amp {}-{}  \amp {}x_3  \amp {}+{}  \amp {6}x_4   \amp {}= \amp  \ h\amp {.}
\end{alignat*}
has the form%
\begin{equation*}
\left[ \begin{array}{crrr|c} 1\amp -1\amp 0\amp 2\amp 1 \\ 0\amp 5\amp -2\amp 1\amp 2 \\ 0\amp 0\amp 1\amp -3\amp -1 \\ 0\amp 0\amp 0\amp 0\amp h-5 \end{array}  \right]\text{.}
\end{equation*}
For which values of \(h\) does this system have:%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}No solutions?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp8968840}{}\quad{}The system has no solutions when there is an equation of the form \(0 = b\) for some nonzero number \(b\). The last row will correspond to an equation of the form \(0 = h-5\). So our system will have no solutions when \(h \neq 5\).%
\item{}A unique solution? Find the solution.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp8971400}{}\quad{}When \(h \neq 5\), the system has no solutions. When \(h = 5\), the variable \(x_4\) is a free variable and the system has infinitely many solutions. So there are no values of \(h\) for which the system has exactly one solution.%
\item{}Infinitely many solution? Determine all solutions?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp8976392}{}\quad{}When \(h = 5\), the variable \(x_4\) is a free variable and the system has infinitely many solutions. The solutions were already found in part (a).%
\end{enumerate}
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp8980616}%
After applying row operations to the augmented matrix of a system of linear equations, each of which describes a plane in 3-space, the following augmented matrix was obtained:%
\begin{equation*}
\left[ \begin{array}{ccc|r} 1\amp a\amp 0\amp 2 \\ 0\amp 2-2a\amp b\amp -4 \\ 0\amp 0\amp 3-\frac{1}{2}b\amp 1 \end{array}  \right]\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Describe, algebraically and geometrically, all solutions (if any), to this system when \(a=0\) and \(b=2\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp8981640}{}\quad{}Throughout, we will let the variables \(x\), \(y\), and \(z\) correspond to the first, second, and third columns, respectively, of our augmented matrix.%
\par
When \(a=0\) and \(b=2\) our augmented matrix has the form%
\begin{equation*}
\left[ \begin{array}{ccc|r} 1\amp 0\amp 0\amp 2 \\ 0\amp 2\amp 2\amp -4 \\ 0\amp 0\amp 2\amp 1 \end{array}  \right]\text{.}
\end{equation*}
This matrix corresponds to the system%
\begin{alignat*}{4}
{}x  \amp {}{}     \amp {}    \amp {}{}    \amp {}     \amp {}= \amp  \ 2\amp {}\\
{}  \amp {}{}     \amp {2}y  \amp {}+{}  \amp {2}z   \amp {}= \amp  \ -4\amp {}\\
{}  \amp {}{}     \amp {}    \amp {}{}    \amp {2}z  \amp {}= \amp  \ 1\amp {.}
\end{alignat*}
There are no equations of the form \(0 = b\) for a nonzero constant \(b\), so the system is consistent. There are no free variables, so the system has a unique solution. Algebraically, the solution is \(x = 2\), \(z = \frac{1}{2}\), and \(y = -\frac{5}{2}\). Geometrically, this tells us that the three planes given by the original system intersect in a single point.%
\item{}Describe, algebraically and geometrically, all solutions (if any), to this system when \(a=0\) and \(b=6\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp8985352}{}\quad{}Throughout, we will let the variables \(x\), \(y\), and \(z\) correspond to the first, second, and third columns, respectively, of our augmented matrix.%
\par
When \(a=0\) and \(b=6\) our augmented matrix has the form%
\begin{equation*}
\left[ \begin{array}{ccc|r} 1\amp 0\amp 0\amp 2 \\ 0\amp 2\amp 6\amp -4 \\ 0\amp 0\amp 0\amp 1 \end{array}  \right]\text{.}
\end{equation*}
The last row corresponds to the equation \(0 = 1\), so our system is inconsistent and has no solution. Geometrically, this tells us that the three planes given by the original system do not all intersect at any common points.%
\item{}Describe, algebraically and geometrically, all solutions (if any), to this system when \(a=1\) and \(b=12\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp8990984}{}\quad{}Throughout, we will let the variables \(x\), \(y\), and \(z\) correspond to the first, second, and third columns, respectively, of our augmented matrix.%
\par
When \(a=1\) and \(b=12\) our augmented matrix reduces to%
\begin{equation*}
\left[ \begin{array}{ccc|r} 1\amp 1\amp 0\amp 2 \\ 0\amp 0\amp 1\amp -\frac{1}{3} \\ 0\amp 0\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
There are no rows that correspond to equations of the form \(0 = c\) for a nonzero constant \(c\), so the system is consistent. The variable \(y\) is a free variable, so the system has infinitely many solutions. Algebraically, the solutions are \(y\) is free, is \(z = -\frac{1}{3}\), and \(x = 2-y\). Geometrically, this tells us that the three planes given by the original system intersect in the line with \(z = -\frac{1}{3}\), and \(x = 2-y\).%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_mtx_sys_summ}
%
\begin{itemize}[label=\textbullet]
\item{}A matrix is just a rectangular array of numbers or objects.%
\item{}Given a system of linear equations, with the variables listed in the same order in each equation, we represent the system by writing the coefficients of the first equation as the first row of a matrix, the coefficients of the second equation as the second row, and so on. This creates the coefficient matrix of the system. We then augment the coefficient matrix with a column of the constants that appear in the equations. This gives us the augmented matrix of the system.%
\item{}The operations that we can perform on equations translate exactly to row operations that we can perform on an augmented matrix:%
\begin{enumerate}
\item{}Replacing one row by the sum of that row and a scalar multiple of another row.%
\item{}Interchanging two rows.%
\item{}Replacing a row by a nonzero scalar multiple of itself.%
\end{enumerate}
%
\item{}The forward elimination phase of the elimination method recursively eliminates the variables in a linear system to reach an equivalent but simplified system.%
\item{}The first non-zero entry in an equation in a linear system after elimination is called a pivot.%
\item{}A basic variable in a linear system corresponds to a pivot of the system. A free variable is a variable that is not basic.%
\item{}A linear system can be inconsistent (no solutions), have a unique solution (if consistent and every variable is a basic variable), or have infinitely many solutions (if consistent and there is a free variable).%
\item{}A linear system has no solutions if, after elimination, there is an equation of the form \(0=b\) where \(b\) is a nonzero number.%
\item{}A linear system after the elimination method can be solved using back-substitution. The free variables can be chosen arbitrarily and the basic variables can be solved in terms of the free variables through the back-substitution process.%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_mtx_sys_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp9009288}%
Consider the system of linear equations whose augmented matrix is%
\begin{equation*}
\left[ \begin{array}{cc|r} 1 \amp  3 \amp  -1 \\ 2\amp  h \amp  k \end{array}  \right]
\end{equation*}
where \(h\) and \(k\) are unknown constants. For which values of \(h\) and \(k\) does this system have%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}a unique solution,%
\item{}infinitely many solutions,%
\item{}no solution?%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp9021704}%
Consider the following system:%
\begin{alignat*}{5}
{}x   \amp {}-{}  \amp {2}y  \amp {}+{}  \amp {}z    \amp {}={}  \amp \ {-}\amp 1\amp {}\\
{-}x  \amp {}+{}  \amp {}y    \amp {}-{}  \amp {3}z  \amp {}={} \amp \ {}\amp 2\amp {}\\
{}x  \amp {}+{}  \amp {h}y  \amp {}-{}  \amp {}z    \amp {}={}  \amp  \ {}\amp 0\amp {.}
\end{alignat*}
Check that when \(h=-3\) the system has infinitely many solutions, while when \(h\neq -3\) the system has a unique solution.%
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp9019528}%
If possible, find a system of three equations (not in reduced form) in three variables whose solution set consists only of the point \(x_1=2, x_2=-1, x_3=0\).%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp9019272}%
What are the possible geometrical descriptions of the solution set of two linear equations in \(\R^3\)? (Recall that \(\R^3\) is the three-dimensional \(xyz\)-space \textemdash{} that is, the set of all ordered triples of the form \((x,y,z)\)).%
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp9028232}%
Two students are talking about when a linear system has infinitely many solutions. \begin{quote}%
Student 1: So, if we have a linear system whose augmented matrix has a row of zeros, then the system has infinitely many solutions, doesn't it?\end{quote}
 \begin{quote}%
Student 2: Well, but what if there is a row of the form \([\, 0\, 0\, \ldots\, 0\, |\, b\, ]\) with a non-zero \(b\) right above the row of 0's?\end{quote}
 \begin{quote}%
Student 1: OK, maybe I should ask ``If we have a consistent linear system whose augmented matrix has a row of zeros, then the system has infinitely many solutions, doesn't it?''\end{quote}
 \begin{quote}%
Student 2: I don't know. It still doesn't sound enough to me, but I'm not sure why.\end{quote}
 Is Student 1 right? Or is Student 2's hunch correct? Justify your answer with a specific example if possible.%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp9029000}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
A system of linear equations in two unknowns can have exactly five solutions.%
\item{}\lititle{True\slash{}False.}\par%
A system of equations with all the right hand sides equal to 0 has at least one solution.%
\item{}\lititle{True\slash{}False.}\par%
A system of equations where there are fewer equations than the number of unknowns (known as an underdetermined system) cannot have a unique solution.%
\item{}\lititle{True\slash{}False.}\par%
A system of equations where there are more equations than the number of unknowns (known as an overdetermined system) cannot have a unique solution.%
\item{}\lititle{True\slash{}False.}\par%
A consistent system of two equations in three unknowns cannot have a unique solution.%
\item{}\lititle{True\slash{}False.}\par%
If a system with three equations and three unknowns has a solution, then the solution is unique.%
\item{}\lititle{True\slash{}False.}\par%
If a system of equations has two different solutions, then it has infinitely many solutions.%
\item{}\lititle{True\slash{}False.}\par%
If there is a row of zeros in the row echelon form of the augmented matrix of a system of equations, the system has infinitely many solutions.%
\item{}\lititle{True\slash{}False.}\par%
If there is a row of zeros in the row echelon form of the augmented matrix of a system of \(n\) equations in \(n\) variables, the system has infinitely many solutions.%
\item{}\lititle{True\slash{}False.}\par%
If a system has no free variables, then the system has a unique solution.%
\item{}\lititle{True\slash{}False.}\par%
If a system has a free variable, then the system has infinitely many solutions.%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Polynomial Interpolation to Approximate the Area Under a Curve}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Polynomial Interpolation to Approximate the Area Under a Curve}{}{Project: Polynomial Interpolation to Approximate the Area Under a Curve}{}{}{x:section:sec_1_b_polynomial}
Suppose we want to approximate the area of the region shown in \hyperref[x:figure:F_1_b_area]{Figure~{\xreffont\ref{x:figure:F_1_b_area}}}. As discussed in the introduction, we can approximate the area under a curve by approximating the curve by quadratics. First, we will see how Archimedes approached the problem of finding the area of a quadratic region, then we will determine how to determine a quadratic function that passes through three points, then we put it all together to approximate the area under a curve as in \hyperref[x:figure:F_1_b_area]{Figure~{\xreffont\ref{x:figure:F_1_b_area}}}.%
\begin{figureptx}{A region whose area we want to approximate.}{x:figure:F_1_b_area}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/1_b_area.pdf}
\end{image}%
\tcblower
\end{figureptx}%
Archimedes approached the problem of calculating the area of a quadratic region in the following way. Given a quadratic \(q(x)\) on an interval \([a,b]\), Archimedes drew in a base given by the secant line connecting the points \((a,q(a))\) and \((b,q(b))\) as illustrated at left in \hyperref[x:figure:F_1_b_Archimedes]{Figure~{\xreffont\ref{x:figure:F_1_b_Archimedes}}}. Then he found the point in the interval \([a,b]\) at which the tangent line to the curve is parallel to the secant line. Archimedes gave an argument using mechanics (based on balance points), and then another using geometry (through a method of exhaustion) to show that the area of the parabolic region is \(\frac{4}{3}\) times the area of the triangle determined by the endpoints and the point of tangency as shown at right in \hyperref[x:figure:F_1_b_Archimedes]{Figure~{\xreffont\ref{x:figure:F_1_b_Archimedes}}}.%
\begin{figureptx}{Archimedes method.}{x:figure:F_1_b_Archimedes}{}%
\begin{sidebyside}{2}{0}{0}{0}%
\begin{sbspanel}{0.5}%
\includegraphics[width=\linewidth]{external/1_b_Archimedes_1.pdf}
\end{sbspanel}%
\begin{sbspanel}{0.5}%
\includegraphics[width=\linewidth]{external/1_b_Archimedes_2.pdf}
\end{sbspanel}%
\end{sidebyside}%
\tcblower
\end{figureptx}%
Although we won't go through the details, a conclusion we can draw from Archimedes argument, using the formula for the area of a rectangle and the area of a triangle, is that the area between the graph of a quadratic with equation \(q(x) = ax^2+bx+c\) and the \(x\)-axis on an interval \([x_1, x_2]\) as illustrated in \hyperref[x:figure:F_1_b_quad_area]{Figure~{\xreffont\ref{x:figure:F_1_b_quad_area}}} is%
\begin{equation}
\frac{a}{3}(x_2^3-x_1^3) + \frac{b}{2}(x_2^2-x_1^2) + c(x_2-x_1)\text{.}\label{x:men:eq_quad_area}
\end{equation}
%
\begin{figureptx}{Region between a parabola and the \(x\)-axis.}{x:figure:F_1_b_quad_area}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/1_b_quad_area.pdf}
\end{image}%
\tcblower
\end{figureptx}%
To approximate the area under the graph of a function, we will approximate the function itself with a collection of quadratics, and then use equation \hyperref[x:men:eq_quad_area]{({\xreffont\ref{x:men:eq_quad_area}})} repeatedly. To do this, we need to know how to fit a quadratic curve to a three points. We consider that question now.%
\par
\index{polynomial curve fitting} Suppose we are given a collection of three points in the plane: \((x_1, y_1), (x_2, y_2)\) and \((x_3, y_3)\). There is exactly one quadratic polynomial \(p(x)\) which goes through these points, i.e. there is exactly one quadratic \(p(x)\) such that for each \(x_i\), \(p(x_i)=y_i\). This is an example of \terminology{polynomial curve fitting}.%
\par
As an example, we use the points \((-1, 2)\), \((1,6)\), \((2,5)\). To fit a quadratic to these points, consider a general quadratic of the form \(p(x)=a_2x^2+a_1x+a_0\). By substituting the \(x\) value of each of the given points and setting that equal to the \(y\) value of that point, we find three equations%
\begin{equation*}
(-1)^2a_2-a_1+a_0=2 \; , \;  a_2+a_1+a_0=6 \; , \;  (2)^2a_2 +2a_1+a_0=5
\end{equation*}
that give us a system of three equations in the three unknowns \(a_2\), \(a_1\), and \(a_0\):%
\begin{alignat*}{3}
{}a_2  \amp {}-{}   \amp {}a_1   \amp {}+{}  \amp {}a_0   \amp = 2\\
{}a_2   \amp {}+{}  \amp {}a_1   \amp {}+{}  \amp {}a_0  \amp = 6\\
a_2  \amp {}+{}   \amp {2}a_1  \amp {}+{}   \amp {}a_0   \amp = 5\text{.}
\end{alignat*}
%
\par
This system is the example we considered in \hyperref[x:exploration:pa_1_b]{Preview Activity~{\xreffont\ref{x:exploration:pa_1_b}}}, whose solution is \(a_2 = -1\), \(a_1=2\), and \(a_0 = 5\). A graph of \(q(x) = -x^2+2x+5\) along with the three points \((-1, 2)\), \((1,6)\), \((2,5)\) is shown in \hyperref[x:figure:F_1_b_quadratic_fit]{Figure~{\xreffont\ref{x:figure:F_1_b_quadratic_fit}}}.%
\begin{figureptx}{A quadratic fit to the points \((-1, 2)\), \((1,6)\), \((2,5)\).}{x:figure:F_1_b_quadratic_fit}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/1_b_quadratic_fit.pdf}
\end{image}%
\tcblower
\end{figureptx}%
Now that we know how to fit a quadratic to three points, we next approximate a curve with a collection of quadratics. The method we use is to break the interval on which our curve is defined into several subintervals and create quadratics on each subinterval. The basic idea is contained in our first project activity.%
\begin{project}{}{x:project:act_1_b_Simpson_ex_1}%
In this activity we model the function \(f\) defined by \(f(x) = \sin(2x)+2\) on the interval \([a,b]\), where \(a = -\frac{\pi}{2}\) and \(b = \pi\) with a collection of quadratics. Let \(f(x) = \sin(x)\). We divide the interval \([a,b]\) into three subintervals using the six points \(x_0 = -\frac{\pi}{2}\), \(x_1 = -\frac{\pi}{4}\), \(x_2 = 0\), \(x_3 = \frac{\pi}{4}\), \(x_4 = \frac{\pi}{2}\), \(x_5 = \frac{3 \pi}{4}\), and \(x_6 = \pi\). We need three points to determine a quadratic, so the three subintervals of the interval \([a,b]\) will be the intervals \([x_0, x_2]\), \([x_2, x_4]\), and \([x_4,x_6]\). An illustration of the process of dividing our interval \([a,b]\) and approximating by quadratics can be found at \href{https://www.geogebra.org/m/spd4hhbw}{\nolinkurl{geogebra.org/m/spd4hhbw}}. Round all calculations in this activity to the nearest thousandth.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Set up a system of linear equations to fit a quadratic \(q_1(x) = r_1x^2+s_1x+t_1\) to the three points \((x_0, f(x_0))\), \((x_1, f(x_1))\), and \((x_2, f(x_2))\). (The solution to this system to the nearest thousandth is \(r_1 = 2\), \(s_1 \approx 2.546\), and \(t_1 = 1.621\).)%
\item{}Set up a system of linear equations to fit a quadratic \(q_2(x) = r_2x^2+s_2x+t_2\) to the three points \((x_2, f(x_2))\), \((x_3, f(x_3))\), and \((x_4, f(x_4))\). (The solution to this system to the nearest thousandth is \(r_2 = 2\), \(s_2 \approx 2.546\), and \(t_2 \approx -1.621\).)%
\item{}Set up a system of linear equations to fit a quadratic \(q_3(x) = r_3x^2+s_3x+t_3\) to the 3 points \((x_4, f(x_4))\), \((x_5, f(x_5))\), and \((x_6, f(x_6))\). (The solution to this system to the nearest thousandth is \(r_3 \approx 10.000\), \(s_3 \approx -7.639\), and \(t_3 \approx 1.621\).)%
\item{}Use the GeoGebra applet at \href{https://www.geogebra.org/m/spd4hhbw}{\nolinkurl{geogebra.org/m/spd4hhbw}} to graph the three quadratics on their intervals on the same axes as the graph of \(f\). Explain what you see.%
\end{enumerate}
\end{project}%
\hyperref[x:project:act_1_b_Simpson_ex_1]{Project Activity~{\xreffont\ref{x:project:act_1_b_Simpson_ex_1}}} illustrates how we can model a function on an interval using a sequence of quadratic functions. Now we apply this polynomial curve fitting technique to derive the general formula for approximating the area between a graph of a function \(f\) and the \(x\)-axis. We use parabolic arcs to approximate the graph of \(f\) on each subinterval.%
\par
We start by dividing the interval \([a,b]\) over which our function is defined into some number of subintervals. We need an even number of subintervals, since we have to use three points to define each parabola. Let \(n = 2m\) be the number of subintervals we use. In order to make the calculations a bit easier, let the subintervals all have the same length, which we denote by \(\Delta x\) (the symbol \(\Delta\) is often used in mathematics to indicate a change in a quantity). Since we have \(n\) subintervals, the length of each subinterval will be \(\Delta x = \frac{b-a}{n}\). For each \(k\) we let \(x_k = a+k \Delta x\) and \(y_k = f(x_k)\). Note that \(x_0 = a\) and \(x_n=b\). This labeling scheme is illustrated in \hyperref[x:figure:F_1_b_partition]{Figure~{\xreffont\ref{x:figure:F_1_b_partition}}}%
\begin{figureptx}{Subdividing the interval \([a,b]\).}{x:figure:F_1_b_partition}{}%
\begin{image}{0.25}{0.5}{0.25}%
\includegraphics[width=\linewidth]{external/1_b_partition.pdf}
\end{image}%
\tcblower
\end{figureptx}%
We approximate \(f\) on each subinterval using a quadratic. So we need to find the quadratic \(Q(x) = c_2x^2+c_1x+c_0\) that passes through two consecutive end points as well as the midpoint of a subinterval. That is, we need to find the coefficients of \(Q\) so that \(Q\) passes through the points \((x_k,y_k)\), \((x_{k+2}, y_{k+2})\), and the midpoint \((x_{k+1},y_{k+1})\) on the interval \([x_k, x_{k+2}]\) (so that we have three points to which to fit a parabola) as shown at left in \hyperref[x:figure:F_1_b_quad_fit]{Figure~{\xreffont\ref{x:figure:F_1_b_quad_fit}}}. Note that the length of the interval \([x_k, x_{k+2}]\) is \(2\Delta x\). To make the calculations easier, we will translate our function so that our leftmost point is \((-r, y_k)\). Then the middle point is \((0, y_{k+1})\) and the rightmost point is \((r, y_{k+2})\) as illustrated at right in \hyperref[x:figure:F_1_b_quad_fit]{Figure~{\xreffont\ref{x:figure:F_1_b_quad_fit}}}, where \(r = \Delta x\).%
\begin{figureptx}{Left: Three points. Right: Translated points.}{x:figure:F_1_b_quad_fit}{}%
\begin{sidebyside}{2}{0}{0}{0}%
\begin{sbspanel}{0.5}%
\includegraphics[width=\linewidth]{external/1_b_quad_fit.pdf}
\end{sbspanel}%
\begin{sbspanel}{0.5}%
\includegraphics[width=\linewidth]{external/1_b_translate.pdf}
\end{sbspanel}%
\end{sidebyside}%
\tcblower
\end{figureptx}%
\begin{project}{}{g:project:idp9104520}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item\label{x:task:lin_sys_proj_ryk}Set up a linear system that will determine the coefficients \(c_2\), \(c_1\), and \(c_0\) so that the polynomial \(Q(x) = c_2x^2+c_1x+c_0\) passes through the points \((-r, y_k)\), \((0, y_{k+1})\), and \((r, y_{k+2})\) with \(r \neq 0\). Remember that the unknowns in this system are \(c_2\), \(c_1\), and \(c_0\).%
\item{}Explain why the coefficient matrix of the system in \hyperref[x:task:lin_sys_proj_ryk]{Task~{\xreffont\ref{g:project:idp9104520}}.{\xreffont\ref{x:task:lin_sys_proj_ryk}}} is \(\left[ \begin{array}{crc} r^2\amp -r\amp 1 \\ 0\amp 0\amp 1 \\ r^2\amp r\amp 1 \end{array}  \right]\). Then explain why row reducing the matrix \(\left[ \begin{array}{crcc} r^2\amp -r\amp 1\amp y_k \\ 0\amp 0\amp 1\amp y_{k+1} \\ r^2\amp r\amp 1\amp y_{k+2} \end{array}  \right]\) will find the coefficients we want. Assume that a row echelon form of the matrix \(\left[ \begin{array}{crcc} r^2\amp -r\amp 1\amp y_k \\ 0\amp 0\amp 1\amp y_{k+1} \\ r^2\amp r\amp 1\amp y_{k+2} \end{array}  \right]\) is%
\begin{equation*}
\left[ \begin{array}{crcc} r^2\amp -r\amp 1\amp y_k \\ 0\amp 2r\amp 0\amp y_{k+2}-y_k \\ 0\amp 0\amp 1\amp y_{k+1} \end{array}  \right]\text{.}
\end{equation*}
Use these matrices to explain why \(c_2=\frac{y_k-2y_{k+1}+y_{k+2}}{2r^2}\), \(c_1=\frac{y_{k+2}-y_k}{2r}\), and \(c_0 = y_{k+1}\).%
\item{}Our goal is to ultimately approximate the area under the curve on the interval \([a,b]\) by approximating \(f\) with quadratics on each subinterval. Use the Archimedean formula \hyperref[x:men:eq_quad_area]{({\xreffont\ref{x:men:eq_quad_area}})} to show that the area under the quadratic \(Q(x)\) from part (a) is%
\begin{equation*}
\frac{1}{3}  \left(y_k+4y_{k+1}+y_{k+2}\right) \Delta x\text{.}
\end{equation*}
%
\item{}Now add up all of the area approximations on each subinterval to show that the approximate area under the graph is given by the formula%
\begin{equation}
S(n) = \left(y_0 + 4y_1 + 2y_2 + 4y_3 + 2y_4 + \cdots 2y_{n-2} + 4y_{n-1} + y_n\right) \frac{\Delta x}{3}\text{.}\label{x:men:eq_1_b_Simpson_sum}
\end{equation}
%
\end{enumerate}
\end{project}%
We conclude with an example.%
\begin{project}{}{g:project:idp8851208}%
Let \(f(x) = \left(\frac{1}{2}\right)^x\) on the interval \([1,6]\). A graph of \(f\) is shown in \hyperref[x:figure:F_1_b_area]{Figure~{\xreffont\ref{x:figure:F_1_b_area}}}. Use our approximation formula with \(n=8\) to approximate the area of the shaded region in \hyperref[x:figure:F_1_b_area]{Figure~{\xreffont\ref{x:figure:F_1_b_area}}}. Show all of your work and round all calculations to the nearest thousandth.%
\end{project}%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 3 Row Echelon Forms}
\typeout{************************************************}
%
\begin{chapterptx}{Row Echelon Forms}{}{Row Echelon Forms}{}{}{x:chapter:chap_row_echelon_forms}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp9262728}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What is the row echelon form of a matrix?%
\item{}What is the procedure to obtain the row echelon form of any matrix?%
\item{}What is the reduced row echelon form of a matrix?%
\item{}What is the procedure to obtain the reduced row echelon form of any matrix?%
\item{}What do the echelon forms of the augmented matrix for a linear system tell us about the solutions to the system?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Balancing Chemical Reactions}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Balancing Chemical Reactions}{}{Application: Balancing Chemical Reactions}{}{}{x:section:sec_appl_chem_react}
Linear systems have applications in chemistry when balancing chemical equations. When a chemical reaction occurs, molecules of different substances combine to create molecules of other substances. Chemists represent such reactions with chemical equations. To balance a chemical equation means to find the number of atoms of each element involved that will preserve the number of atoms in the reaction. As an example, consider the chemical equation%
\begin{equation}
\text{C} _2\text{H} _6 + \text{O} _2 \to \text{ CO } _2 + \text{H} _2\text{O}\text{.}\label{x:men:eq_reaction1}
\end{equation}
%
\par
This equation asks about what will happen when the chemicals ethane (\(\text{C} _2\text{H} _6\)) and oxygen (\(\text{O} _2\)), called the \terminology{reactants} of the reaction, combine to produce carbon dioxide (\(\text{ CO } _2\)) and water (\(\text{H} _2\text{O}\)), called the \terminology{products} of the reaction (note that oxygen gas is \terminology{diatomic}, so that oxygen atoms are paired). The arrow indicates that it is the reactants that combine to form the products. Any chemical reaction has to obey the Law of Conservation of Mass that says that mass can neither be created nor destroyed in a chemical reaction. Consequently, a chemical reaction requires the same number of atoms on both sides of the reaction. In other words, the total mass of the reactants must equal the total mass of the products. In reaction \hyperref[x:men:eq_reaction1]{({\xreffont\ref{x:men:eq_reaction1}})} the chemicals involved are made up of carbon (C), hydrogen (H), and oxygen (O) atoms. To balance the equation, we need to know how many molecules of each chemical are combined to preserve the number of atoms of C, H, and O. This can be done by setting up a linear system of equations of the form%
\begin{alignat*}{5}
{2}x_1   \amp {}    \amp {}    \amp {}-{}  \amp {}x_3  \amp {}    \amp {}       \amp {}={}  \amp  \ 0\amp {}\\
x_1  \amp {}     \amp {}     \amp {}    \amp {}    \amp {}-{}  \amp {2}x_4   \amp {}={}   \amp  \ 0\amp {}\\
{}      \amp {}    \amp {2}x_2  \amp {}-{}   \amp {2}x_3  \amp {}-{}  \amp {}x_4    \amp {}={}   \amp  \ 0\amp {,}
\end{alignat*}
where \(x_1\), \(x_2\), \(x_3\), and \(x_4\) represent the number of molecules of \(\text{C} _2\text{H} _6\), \(\text{O} _2\), \(\text{ CO } _2\), and \(\text{H} _2\text{O}\), respectively, in the reaction and then solving the system. Specific details can be found at the end of this section.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_row_ech_intro}
In the previous sections, we identified operations on a given linear system with corresponding equivalent operations on the matrix representations which simplify the system and its matrix representation without changing the solutions of the system. Our end goal was to obtain a system which could be solved using back substitution, such as%
\begin{alignat*}{4}
{}x_1   \amp {}-{}   \amp {}x_2   \amp {}+{}   \amp {}x_3 \amp {}={}  \amp 0\amp {}\\
{}      \amp {}     \amp {6}x_2 \amp {}-{}  \amp {}x_3 \amp {}={}   \amp 8\amp {}\\
{}      \amp {}    \amp {}    \amp {}     \amp {}x_3 \amp {}={}   \amp 1\amp {.}
\end{alignat*}
%
\par
The augmented matrix for this system is%
\begin{equation*}
\left[ \begin{array}{crr|c} 1 \amp -1 \amp 1 \amp 0 \\ 0\amp  6 \amp -1 \amp 8 \\ 0\amp 0 \amp 1 \amp 1 \end{array}  \right]\text{.}
\end{equation*}
%
\par
The matrices of linear systems which can be solved via back substitution are said to be in \terminology{row echelon form} (or simply \terminology{echelon form}). We will define the properties of matrices in this form precisely in this section. Our goal will be to prescribe a precise procedure for converting any matrix to an equivalent one in row echelon form without having to convert back to the system representation.%
\begin{exploration}{}{x:exploration:pa_1_c}%
We want to determine a suitable form for an augmented matrix that can be obtained from row operations so that it is straightforward to find the solutions to the system. We begin with some examples.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Write the linear system corresponding to each of the following augmented matrices. Use the linear system to determine which systems have their variables eliminated completely in the forward direction, or equivalently determine for which systems the next step in the solution process is back substitution (possibly using free variables). Explain your reasoning. You do not need to solve the systems.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}\(\ds \left[ \begin{array}{rrc|r} 1 \amp -1 \amp 2 \amp -2 \\ 0 \amp 1 \amp 2\amp -1 \\ 0 \amp 0 \amp 3 \amp 1 \end{array} \right]\)%
\item{}\(\ds \left[ \begin{array}{crc|r} 1 \amp 1 \amp 0 \amp -2 \\ 0 \amp 1 \amp 0 \amp 3 \\ 0 \amp 0 \amp 0\amp 0 \end{array} \right]\)%
\item{}\(\ds \left[ \begin{array}{ccc|c} 1 \amp 1 \amp 1 \amp 2 \\ 1 \amp 2 \amp 2 \amp 2 \\ 0 \amp 0 \amp 2\amp 2 \end{array} \right]\)%
\item{}\(\ds \left[ \begin{array}{ccr|r} 0 \amp 1 \amp 1 \amp 2 \\ 0 \amp 0 \amp 3 \amp 3 \\ 0 \amp 0 \amp -2\amp -2 \end{array} \right]\)%
\end{enumerate}
\item{}Shown below are two row reduced forms of the system%
\begin{alignat*}{5}
{2}x_1   \amp {}    \amp {}    \amp {}-{}  \amp {}x_3  \amp {}    \amp {}       \amp {}={}  \amp  \ 0\amp {}\\
x_1  \amp {}     \amp {}     \amp {}    \amp {}    \amp {}-{}  \amp {2}x_4   \amp {}={}   \amp  \ 0\amp {}\\
{}      \amp {}    \amp {2}x_2  \amp {}-{}   \amp {2}x_3  \amp {}-{}  \amp {}x_4    \amp {}={}   \amp  \ 0\amp {.}
\end{alignat*}
Of the systems that correspond to these augmented matrices, which is easier to solve and why?%
\begin{equation*}
\left[ \begin{array}{cccr|c} 2 \amp  0 \amp  -1 \amp  0 \amp  0 \\ 0 \amp  2 \amp  -2 \amp  -1 \amp  0 \\ 0 \amp  0 \amp  3 \amp  -2 \amp  0 \end{array}  \right]  \left[  \begin{array}{cccr|c} 1 \amp  0 \amp  0 \amp  -\frac{1}{3} \amp  0 \\ 0 \amp  1 \amp  0 \amp  -\frac{7}{6} \amp  0 \\ 0 \amp  0 \amp  1 \amp  -\frac{2}{3} \amp  0 \end{array}  \right]
\end{equation*}
%
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Echelon Forms of a Matrix}
\typeout{************************************************}
%
\begin{sectionptx}{The Echelon Forms of a Matrix}{}{The Echelon Forms of a Matrix}{}{}{x:section:sec_mtx_ech_forms}
In the previous sections we saw how to simplify a linear system and its matrix representation via the elimination method without changing the solution set. This process is more efficient when performed on the matrix representation rather than on the system itself. Furthermore, the process of applying row operations to any augmented matrix is one that can be automated. In order to write an algorithm that can be used with any size augmented matrix to the extent that it can be applied even by a computer program, it is necessary to have a consistent procedure and a stopping point for the simplification process. The two main properties that we want the simplified augmented matrix to satisfy are that it should be easy to see if the system has solutions from the simplified matrix, and in cases when there are solutions, the general form of the solutions can be easily found. Hence the topic of this section is to define the process of elimination completely and generally.%
\par
We begin by discussing the \terminology{row echelon} or, simply, \terminology{echelon} form of a matrix. We know that the forward phase of the elimination on a linear system produces a system which can be solved by back substitution. The matrix representation of such a simplified system is said to be in \terminology{row echelon} or simply \terminology{echelon} form. Note that matrices in this form have the first nonzero entry in each row to the right of and below the first nonzero entry in the preceding row. Our next step is to formally describe this form \textemdash{} one that you tried to explain in problem 3 of \hyperref[x:exploration:pa_1_c]{Preview Activity~{\xreffont\ref{x:exploration:pa_1_c}}}.%
\begin{definition}{}{g:definition:idp9295624}%
\index{row echelon form}%
\index{pivot}%
A rectangular matrix is in \terminology{row echelon form} (or simply \terminology{echelon form}) if it has the following properties:%
\begin{enumerate}
\item{}All nonzero rows are above any rows of all zeros.%
\item{}Each \terminology{pivot} (the first non-zero entry reading from left to right) in a row is in a column to the right of the pivot of the row above it.%
\end{enumerate}
%
\end{definition}
\index{leading entry of a row}\index{pivot positions}\index{pivot column} A pivot is also called a \terminology{leading entry} of a row. Note that properties (1) and (2) above imply that all entries in a column below a pivot are zeros. It can be shown that the positions of these pivots, called \terminology{pivot positions}, are unique and tell us quite a bit about a matrix and the solutions of the linear system it corresponds to. The columns that the pivots are in, called \terminology{pivot columns}, will also have useful properties as we will see soon.%
\begin{remark}{}{g:remark:idp9298568}%
Compare the row echelon form of an augmented matrix to the corresponding system. Do you clearly see the correspondence between the requirements of the row echelon form and the properly eliminated variables in the system? Can you quickly come up with a system which will be in row echelon form when represented in augmented matrix form? Can you modify the standard row echelon form definition to cover cases where the elimination process eliminates the variables from last to first? For example, in a system with three equations in three unknowns, the last variable, say \(x_3\), can be eliminated from the second equation, and the last two variables, say \(x_2, x_3\) can be eliminated from the last equation. How would you define this modified row echelon form for a general system with this modified elimination process?%
\end{remark}
Once an augmented matrix is in row echelon form, we can use back substitution to solve the corresponding system. However, we can make solving much easier with just a little more elimination work.%
\par
Row operations are easy to apply, so if we are inclined, there is no reason to stop at the row echelon form. For example, starting with the following matrix%
\begin{equation*}
\left[ \begin{array}{crcr|r} 2\amp -1\amp 2\amp 2\amp 7 \\ 0\amp 1\amp 3\amp -1\amp -1 \\ 0\amp 0\amp 0\amp 2\amp 4 \end{array}  \right]
\end{equation*}
in row echelon form, we could take the row operations even farther and avoid the process of back substitution altogether. First, we multiply the last row by \(1/2\) to simplify that row:%
\begin{equation*}
\begin{array}{c} \text{ } \\ \text{ } \\  \frac{1}{2} R_3 \to R_3 \end{array}  \left[ \begin{array}{crcr|r} 2\amp -1\amp 2\amp 2\amp 7 \\ 0\amp 1\amp 3\amp -1\amp -1 \\ 0\amp 0\amp 0\amp 1\amp 2 \end{array}  \right]\text{.}
\end{equation*}
Then we use the third row to eliminate entries above the third pivot:%
\begin{equation*}
\begin{array}{c}  R_1-2R_3 \to R_1  \\  R_2+R_3 \to R_2   \\ \text{ } \end{array}   \left[ \begin{array}{crcr|r} 2\amp -1\amp 2\amp 0\amp 3 \\ 0\amp 1\amp 3\amp 0\amp 1 \\ 0\amp 0\amp 0\amp 1\amp 2 \end{array}  \right]\text{.}
\end{equation*}
%
\par
We can continue in this manner (we call this process \terminology{backward elimination}) to make 0 all of the entries above the pivots (one in the second column, and one in the fourth) with the pivots being 1, to ultimately obtain the equivalent augmented matrix%
\begin{equation*}
\left[ \begin{array}{crcr|r} 1\amp 0\amp \frac{5}{2} \amp 0\amp 2 \\ 0\amp 1\amp 3\amp 0\amp 1 \\ 0\amp 0\amp 0\amp 1\amp 2 \end{array}  \right]\text{.}
\end{equation*}
%
\par
The system corresponding to this augmented matrix is%
\begin{alignat*}{5}
{}x_1   \amp {}     \amp {}    \amp {}+{}  \amp {\frac{5}{2}}x_3    \amp {}    \amp {}     \amp {}={}  \amp  \ 2\\
{}      \amp {}     \amp {}x_2   \amp {}+{}  \amp {3}x_3    \amp { }              \amp {}     \amp {}={}   \amp  \ 1\\
{}      \amp {}    \amp {}    \amp {}     \amp {}      \amp { }              \amp {}x_4  \amp {}={}   \amp  \ 2
\end{alignat*}
so we can just directly read off the solution to the system: \(x_3\) free and \(x_1=2- \frac{5}{2}x_3\), \(x_2=1-3x_3\), \(x_4=2\). This final row reduced form makes solving the system very easy, and this form is called the \terminology{reduced row echelon} form.%
\begin{definition}{}{g:definition:idp9309832}%
\index{row echelon form!reduced}%
A rectangular matrix is in \terminology{reduced row echelon form} (or \terminology{reduced echelon form}) if the matrix is in row echelon form and%
\begin{enumerate}
\item{}The pivot in each nonzero row is 1.%
\item{}Each pivot is the only nonzero entry in its column.%
\end{enumerate}
%
\end{definition}
In short, the reduced row echelon form of a matrix is a row echelon form in which all the pivots are 1 and any entries \emph{below and above} the pivots are 0.%
\par
If we use either of these two row echelon forms, solving the original system becomes straightforward and, as a result, these matrix forms are stopping points for the row operation algorithm to solve a system. It is also very easy to write a computer program to perform row operations to obtain and row echelon or reduced row echelon form of the matrix, making hand computations unnecessary. We will discuss this shortly.%
\begin{remark}{}{g:remark:idp9316232}%
Compare the reduced row echelon form of an augmented matrix to the corresponding system. Do you clearly see the correspondence between the requirements of the reduced row echelon form and the way the variables appear in the equations in the system? Can you quickly come up with a system which will be in reduced row echelon form when represented in augmented matrix form?%
\end{remark}
\begin{paragraphs}{Note.}{g:paragraphs:idp9312136}%
We have used the elimination method on augmented matrices so far. However, the elimination method can be applied on just the coefficient matrix, or other matrices that will arise in other contexts, and will provide useful information in each of those cases. Therefore, the row echelon form and reduced row echelon form is defined for \emph{any matrix}, and from now on, a matrix will be a general matrix unless explicitly specified to be an augmented matrix.%
\end{paragraphs}%
\begin{activity}{}{x:activity:act_1_c_1}%
Identify which of the following matrices is in row echelon form (REF) and\slash{}or reduced row echelon form (RREF). For those in row and\slash{}or reduced row echelon form, identify the pivots clearly by circling them. For those that are not in a given form, state which properties the matrix fails to satisfy.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\left[ \begin{array}{ccrc} 2 \amp 4 \amp -3 \amp 6 \\ 0 \amp 0 \amp 0 \amp 7 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{cc} 1 \amp 0 \\ 0 \amp 1 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{cccc} 0 \amp 1 \amp 2 \amp 3 \\ 0 \amp 0 \amp 1 \amp 0 \\ 0 \amp 1 \amp 0 \amp 5 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{cccc} 1 \amp 2 \amp 3 \amp 4 \\ 0 \amp 0 \amp 0 \amp 0 \\ 0 \amp 0 \amp 0 \amp 0 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{cc} 0 \amp 0 \\ 0 \amp 0 \end{array} \right]\)%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Determining the Number of Solutions of a Linear System}
\typeout{************************************************}
%
\begin{sectionptx}{Determining the Number of Solutions of a Linear System}{}{Determining the Number of Solutions of a Linear System}{}{}{x:section:sec_num_sols_ls}
Consider the system%
\begin{alignat*}{5}
{}x_1   \amp {+}    \amp {2}x_2    \amp {}-{}  \amp {}x_3  \amp {}    \amp {}       \amp {}={}  \amp  \ 0\amp {}\\
{}    \amp {}     \amp {}x_2     \amp {}    \amp {}    \amp {}-{}  \amp {}x_4   \amp {}={}   \amp  \ 2\amp {}\\
{}      \amp {}    \amp {}  \amp {}{}       \amp {}x_3  \amp {}-{}  \amp {2}x_4    \amp {}={}   \amp  \ 4\amp {.}
\end{alignat*}
%
\par
The augmented matrix for this system is%
\begin{equation*}
\left[ \begin{array}{ccrr|c} 1 \amp  2 \amp  -1 \amp  0 \amp  0 \\ 0 \amp  1 \amp  0 \amp  -1 \amp  2 \\ 0 \amp  0 \amp  1 \amp  -2 \amp  4 \end{array}  \right]\text{.}
\end{equation*}
%
\par
Note that this matrix is already in row echelon form. The reduced row echelon form of this augmented matrix is%
\begin{equation}
\left[ \begin{array}{cccr|c} 1 \amp  0 \amp  0 \amp  0 \amp  0 \\ 0 \amp  1 \amp  0 \amp  -1 \amp  2 \\ 0 \amp  0 \amp  1 \amp  -2 \amp  4 \end{array}  \right]\text{.}\label{x:men:eq_Ex_rref}
\end{equation}
%
\par
Since there are leading 1s in the first three columns, we can use those entries to write \(x_1\), \(x_2\), and \(x_3\) in terms of \(x_4\). We then choose \(x_4\) to be arbitrary and write the remaining variables in terms of \(x_4\). Let \(x_4 = t\). Solving the third equation for \(x_3\) gives us \(x_3 = 4+2t\). The second equation shows that \(x_2 = 2+t\), and the first that \(x_1 = 0\). Each value of \(t\) provides a solution to the system, so our system has infinitely many solutions. These solutions are%
\begin{equation*}
x_1 = 0, \ x_2 = 2+t, \ x_3 = 4+2t, \ \text{ and }  \ x_4 = t\text{,}
\end{equation*}
where \(t\) can have any value.%
\begin{activity}{}{x:activity:act_1_c_2}%
We have seen examples of systems with no solutions, one solution, and infinitely many solutions. As we will see in this activity, we can recognize the number of solutions to a system by analyzing the pivot positions in the augmented matrix of the system.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Write an example of an augmented matrix in row echelon form so that the last column of the (whole) matrix is a pivot column. What is the system of equations corresponding to your augmented matrix? How many solutions does your system have? Why?%
\item{}Consider the reduced row echelon form \hyperref[x:men:eq_Ex_rref]{({\xreffont\ref{x:men:eq_Ex_rref}})}. Based on the columns of this matrix, explain how we know that the system it represents is consistent.%
\item{}The system with reduced row echelon form \hyperref[x:men:eq_Ex_rref]{({\xreffont\ref{x:men:eq_Ex_rref}})} is consistent. What is it about the columns of the coefficient matrix that tells us that this system has infinitely many solutions?%
\item{}Suppose that a linear system is consistent and that the coefficient matrix has \(m\) rows and \(n\) columns.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}If every column of the coefficient matrix is a pivot column, how many solutions must the system have? Why? What relationship must exist between \(m\) and \(n\)? Explain.%
\item{}If the coefficient matrix has at least one non-pivot column, how many solutions must the system have? Why?%
\end{enumerate}
\end{enumerate}
\end{activity}%
When solving a linear system of equations, the free variables can be chosen arbitrarily and we can write the basic variables in terms of the free variables. Therefore, the existence of a free variable leads to infinitely many solutions for consistent systems. However, it is possible to have a system with free variables which is inconsistent. (Can you think of an example?)%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Producing the Echelon Forms}
\typeout{************************************************}
%
\begin{sectionptx}{Producing the Echelon Forms}{}{Producing the Echelon Forms}{}{}{x:section:sec_prod_ech_forms}
In this part, we consider the formal process of creating the row and reduced row echelon forms of matrices. The process of creating the row echelon form is the equivalent of the elimination method on systems of linear equations.%
\begin{activity}{}{g:activity:idp9340680}%
Each of the following matrices is at most a few steps away from being in the requested echelon form. Determine what row operations need to be completed to turn the matrix into the required form.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Turn into REF: \(\left[ \begin{array}{cc} 0 \amp 2 \\ 2 \amp 1 \end{array} \right]\)%
\item{}Turn into REF: \(\left[ \begin{array}{cc} 1 \amp 2 \\ 2 \amp 5 \end{array} \right]\)%
\item{}Turn into RREF: \(\left[ \begin{array}{ccc} 2 \amp 0 \amp 0 \\ 0 \amp 3 \amp 0 \\ 0\amp 0\amp 1 \end{array} \right]\)%
\item{}Turn into RREF: \(\left[ \begin{array}{cr} 1\amp -1 \\ 0 \amp 1 \end{array} \right]\)%
\item{}Turn into RREF: \(\left[ \begin{array}{cc} 1\amp 1 \\ 0 \amp 2 \end{array} \right]\)%
\item{}Turn into RREF: \(\left[ \begin{array}{ccr} 1\amp 0 \amp -1 \\ 0 \amp 1 \amp 3 \\ 0\amp 0\amp 2 \end{array} \right]\)%
\end{enumerate}
\end{activity}%
The complete process of applying row operations to reduce an augmented matrix to a row or reduced row echelon form can be expressed as a recursive process in an algorithmic fashion, making it possible to program computers to solve linear systems. Here are the steps to do so:%
\begin{descriptionlist}
\begin{dlimedium}{Step 1}{g:li:idp9348360}%
Begin with the leftmost nonzero column (if there is one). This will be a pivot column.%
\end{dlimedium}%
\begin{dlimedium}{Step 2}{g:li:idp9350792}%
Select a nonzero entry in this pivot column as a pivot. If necessary, interchange rows to move this entry to the first row (this entry will be a pivot).%
\end{dlimedium}%
\begin{dlimedium}{Step 3}{g:li:idp9351176}%
Use row operations to create zeros in all positions below the pivot.%
\end{dlimedium}%
\begin{dlimedium}{Step 4}{g:li:idp9345928}%
Cover (or ignore) the row containing the pivot position and cover all rows, if any, above it. Apply steps 1-3 to the submatrix that remains. Repeat the process until there are no more nonzero rows to modify.%
\par
To obtain the reduced row echelon form we need one more step.%
\end{dlimedium}%
\begin{dlimedium}{Step 5}{g:li:idp9344776}%
Beginning with the rightmost pivot and working upward and to the left, create zeros above each pivot. If a pivot is not 1, make it 1 by an appropriate row multiplication.%
\end{dlimedium}%
\end{descriptionlist}
%
\par
The algorithm described in steps 1-4 will produce the row echelon form of the matrix. This algorithm is called \terminology{Gaussian elimination}. When we add step 5 to produce the reduced row echelon form, the algorithm is called \terminology{Gauss-Jordan elimination}.%
\begin{activity}{}{x:activity:act_1_c_3}%
Consider the matrix \(\left[ \begin{array}{rrcr} 0\amp 2\amp 4\amp 1\\ -1\amp 3\amp 0\amp 6 \\ 0\amp 4\amp 8\amp 2\\ 1\amp -3\amp 0\amp -2 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Perform Gaussian elimination to reduce the matrix to row echelon form. Clearly identify each step used. Compare your row echelon form to that of another group. Do your results agree? If not, who is right?%
\item{}Now continue applying row operations to obtain the reduced row echelon form of the matrix. Clearly identify each step. Compare your row echelon form to that of another group. Do your results agree? If not, who is right?%
\end{enumerate}
\end{activity}%
If we compare row echelon forms from \hyperref[x:activity:act_1_c_3]{Activity~{\xreffont\ref{x:activity:act_1_c_3}}}, it is likely that different groups or individuals produced different row echelon forms. That is because the row echelon form of a matrix is not unique. (Is the row echelon form ever unique?)%
\par
However, if row operations are applied correctly, then we will all arrive at the same reduced row echelon form in \hyperref[x:activity:act_1_c_3]{Activity~{\xreffont\ref{x:activity:act_1_c_3}}}:%
\begin{equation*}
\left[ \begin{array}{cccc} 1\amp 0\amp 6\amp 0\\ 0\amp 1\amp 2\amp 0 \\ 0\amp 0\amp 0\amp 1\\ 0\amp 0\amp 0\amp 0 \end{array}   \right]\text{.}
\end{equation*}
It turns out that the reduced row echelon form of a matrix is unique.%
\par
Two matrices who are connected by row operations are said to be \terminology{row equivalent}.%
\begin{definition}{}{g:definition:idp9356936}%
\index{row equivalent matrices}%
A matrix \(B\) is \terminology{row equivalent} to a matrix \(A\) if \(B\) can be obtained by applying elementary row operations to \(A\).%
\end{definition}
Since every elementary row operation is reversible, if \(B\) is row equivalent to \(A\), then \(A\) is also row equivalent to \(B\). Thus, we just say that \(A\) and \(B\) are row equivalent. While the row echelon form of a matrix is not unique, it is the case that the reduced row echelon form of a matrix is unique.%
\begin{theorem}{}{}{g:theorem:idp9364232}%
Every matrix is row equivalent to a unique matrix in reduced row echelon form.%
\end{theorem}
The reduced row echelon form of a matrix that corresponds to a system of linear equations provides us with an equivalent system whose solutions are easy to find. As an example, consider the system%
\begin{alignat*}{5}
{}     \amp {}     \amp {2}x_2  \amp {}+{}  \amp {4}x_3  \amp {}+{}  \amp {}x_4  \amp {}={}  \amp  \ 0\\
{-}x_1  \amp {}+{}  \amp {3}x_2  \amp {}    \amp {}    \amp {}+{}  \amp {6}x_4  \amp {}={}   \amp  \ 0\\
{}      \amp {}    \amp {4}x_2  \amp {}+{}   \amp {8}x_3  \amp {}+{}  \amp {2}x_4  \amp {}={}   \amp  \ 0\\
{}x_1  \amp {}-{}  \amp {3}x_2  \amp {}     \amp {}    \amp {}-{}  \amp {2}x_4  \amp {}={}   \amp  \ 0
\end{alignat*}
with augmented matrix%
\begin{equation*}
\left[ \begin{array}{rrcr|c} 0\amp 2\amp 4\amp 1\amp 0 \\ -1\amp 3\amp 0\amp 6\amp 0 \\ 0\amp 4\amp 8\amp 2\amp 0 \\ 1\amp -3\amp 0\amp -2\amp 0 \end{array}   \right]\text{.}
\end{equation*}
Notice that the coefficient matrix (the left hand side portion of the augmented matrix) of this system is same as the matrix we considered in \hyperref[x:activity:act_1_c_3]{Activity~{\xreffont\ref{x:activity:act_1_c_3}}}. Since we are augmenting with a column of zeros, no row operations will change those zeros in the augmented column. So the row operations applied in \hyperref[x:activity:act_1_c_3]{Activity~{\xreffont\ref{x:activity:act_1_c_3}}} will give us the reduced row echelon form of this augmented matrix as%
\begin{equation*}
\left[ \begin{array}{cccc|c} 1\amp 0\amp 6\amp 0\amp 0\\ 0\amp 1\amp 2\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 1\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
%
\par
Note that the third column is not a pivot column. That means that the variable \(x_3\) is a free variable. There are pivots in the other three columns of the coefficient matrix, so we can solve for \(x_1\), \(x_2\), and \(x_4\) in terms of \(x_3\). These variables are the basic variables. The third row of the augmented matrix tells us that \(x_4=0\). The second row corresponds to the equation \(x_2+2x_3 = 0\), and solving for \(x_2\) shows that \(x_2 = -2x_3\). Finally, the first row tells us that \(x_1+6x_3 = 0\), so \(x_1 = -6x_3\). Therefore, the general solution to this system of equations is%
\begin{equation*}
x_1 = -6x_3, \ \ \ \ x_2 = -2x_3, \ \ \ \ x_3 \text{ is free } , \ \ \ \  x_4 = 0\text{.}
\end{equation*}
%
\par
The fact that \(x_3\) is free means that we can choose any value for \(x_3\) that we like and obtain a specific solution to the system. For example, if \(x_3=-1\), then we have the solution \(x_1 = 6\), \(x_2 = 2\), \(x_3 = -1\), and \(x_4 = 0\). Check this to be sure.%
\begin{activity}{}{g:activity:idp9373704}%
Each matrix below is an augmented matrix for a linear system after elimination with variables \(x_1, x_2, \ldots\) in that order. Identify the basic variables (if any) and free variables (if any). Then find the general solution (if there is a solution) expressing all variables in terms of the free variables.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\left[ \begin{array}{ccc|c} 1\amp 0\amp 2\amp 1 \\ 0\amp 3\amp 1\amp 0 \\ 0\amp 0\amp 0\amp 0 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{ccr|c} 1\amp 1\amp 0\amp 1 \\ 0\amp 0\amp 1\amp 2 \\ 0\amp 0\amp 0\amp 0 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{ccrc|c} 1\amp 2\amp -1\amp 1\amp 1 \\ 0\amp 1\amp 0\amp 2\amp 1 \\ 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{ccr|c} 1\amp 0\amp 1\amp 1 \\ 0\amp 1\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 2 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{cc|c} 1\amp 0\amp 1 \\ 0\amp 1\amp 0 \\ 0\amp 0\amp 0 \end{array} \right]\)%
\end{enumerate}
\end{activity}%
Recall that in the previous section, we determined the criteria for when a system has a unique solution, or infinitely many solutions, or no solution. With the use of the row echelon form of the augmented matrix, we can rewrite these criteria as follows:%
\begin{theorem}{}{}{g:theorem:idp9377544}%
%
\begin{enumerate}
\item{}A linear system is consistent if in the row echelon form of the augmented matrix representing the system no pivot is in the rightmost column.%
\item{}If a linear system is consistent and the row echelon form of the coefficient matrix does not have a pivot in every column, then the system has infinitely many solutions.%
\item{}If a linear system is consistent and there is a pivot in every column of the row echelon form of the coefficient matrix, then the system has a unique solution.%
\end{enumerate}
%
\end{theorem}
\begin{figureptx}{Figures for \hyperref[x:activity:act_1_c_4]{Activity~{\xreffont\ref{x:activity:act_1_c_4}}}.}{x:figure:F_1_c_1}{}%
\begin{sidebyside}{3}{0}{0}{0}%
\begin{sbspanel}{0.333333333333333}%
\includegraphics[width=\linewidth]{external/1_c_echelon_plot_1.pdf}
\end{sbspanel}%
\begin{sbspanel}{0.333333333333333}%
\includegraphics[width=\linewidth]{external/1_c_echelon_plot_2.pdf}
\end{sbspanel}%
\begin{sbspanel}{0.333333333333333}%
\includegraphics[width=\linewidth]{external/1_c_echelon_plot_3.pdf}
\end{sbspanel}%
\end{sidebyside}%
\tcblower
\end{figureptx}%
\begin{activity}{}{x:activity:act_1_c_4}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}For each part, the reduced row echelon form of the augmented matrix of a system of equations in variables \(x\), \(y\), and \(z\) (in that order) is given. Use the reduced row echelon form to find the solution set to the original system of equations.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}\(\left[ \begin{array}{ccc|r} 1 \amp 0 \amp 0 \amp -1 \\ 0 \amp 1 \amp 0 \amp 3 \\ 0 \amp 0 \amp 0 \amp 0 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{ccr|r} 1 \amp 0 \amp 2 \amp -1 \\ 0 \amp 1 \amp -1 \amp 3 \\ 0 \amp 0 \amp 0 \amp 0 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{ccc|r} 1 \amp 0 \amp 0 \amp 2 \\ 0 \amp 1 \amp 0 \amp -1 \\ 0 \amp 0 \amp 1 \amp 3 \end{array} \right]\)%
\item{}Each of the three systems above is represented as one of the graphs in \hyperref[x:figure:F_1_c_1]{Figure~{\xreffont\ref{x:figure:F_1_c_1}}}. Match each figure with a system.%
\end{enumerate}
\item{}The reduced row echelon form of the augmented matrix of a system of equations in variables \(x\), \(y\), \(z\), and \(t\) (in that order) is given. Use the reduced row echelon form to find the solution set to the original system of equations:%
\begin{equation*}
\left[ \begin{array}{cccc|r} 1 \amp  3 \amp  0 \amp  0 \amp  -1 \\ 0 \amp  0 \amp  1 \amp  2 \amp  4 \\ 0 \amp  0 \amp  0 \amp  0 \amp  1 \end{array}  \right]\text{.}
\end{equation*}
%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_row_ech_exam}
\begin{introduction}{}%
What follows are worked examples that use the concepts from this section.%
\end{introduction}%
\begin{example}{}{g:example:idp9393672}%
Consider the linear system%
\begin{alignat*}{1}
2x_1 + 6x_3 \amp = x_2 + 2\\
2x_3 - 4x_1  \amp =  2x_2\\
x_2 + 4x_3 - 2 \amp = 2x_1 + 6\text{.}
\end{alignat*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the augmented matrix for this system.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp9395976}{}\quad{}Before we can find the augmented matrix of this system, we need to rewrite the system so that the variables are all on one side and the constant terms are on the other side of the equations. Doing so yields the equivalent system%
\begin{alignat*}{4}
{2}x_1   \amp {}-{}    \amp {}x_2    \amp {}+{}  \amp {6}x_3  \amp {}={}  \amp  \ 2\amp {}\\
{-4}x_1  \amp {}-{}     \amp {2}x_2     \amp {}+{}  \amp {2}x_3   \amp {}={}   \amp  \ 0\amp {}\\
{-2}x_1  \amp {}+{}    \amp {}x_2    \amp {}+{}   \amp {4}x_3  \amp {}={}   \amp  \ 8\amp {.}
\end{alignat*}
%
\par
Note that this is not the only way to rearrange the system. For example, for the second equation, could be written instead as \(4x_1+2x_2-2x_3=0\) to minimize the number of negative signs in the equation.%
\par
The augmented matrix for this system is%
\begin{equation*}
\left[ \begin{array}{rrc|c} 2\amp -1\amp 6\amp 2 \\ -4\amp -2\amp 2\amp 0 \\ -2\amp 1\amp 4\amp 8 \end{array}  \right]\text{.}
\end{equation*}
%
\item{}Use row operations to find a row echelon form of the augmented matrix of this system.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp9401480}{}\quad{}Our first steps to row echelon form are to eliminate the entries below the leading entry in the first row. To do this we replace row two with row two plus 2 times row 1 and we replace row three with row three plus row one. This produces the row equivalent matrix%
\begin{equation*}
\left[ \begin{array}{crc|c} 2\amp -1\amp 6\amp 2 \\ 0\amp -4\amp 14\amp 4 \\ 0\amp 0\amp 10\amp 10 \end{array}  \right]\text{.}
\end{equation*}
This matrix is now in row echelon form.%
\item{}Use row operations to find the reduced row echelon form of the augmented matrix of this system.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp9406728}{}\quad{}To continue to find the reduced row echelon form, we replace row two with row two times \(-\frac{1}{4}\) to get a leading 1 in the second row, and we replace row three with row three times \(\frac{1}{10}\) to get a leading 1 in the third row and obtain the row equivalent matrix%
\begin{equation*}
\left[ \begin{array}{crr|r} 2\amp -1\amp 6\amp 2 \\ 0\amp 1\amp -\frac{7}{2}\amp -1 \\ 0\amp 0\amp 1\amp 1 \end{array}  \right]\text{.}
\end{equation*}
Now we perform backwards elimination to make the entries above the leading \(1\)s equal to \(0\), starting with the third column and working backwards. Replace row one with row one minus \(6\) times row three and replace row two with row two plus \(\frac{7}{2}\) row three to obtain the row equivalent matrix%
\begin{equation*}
\left[ \begin{array}{crc|r} 2\amp -1\amp 0\amp -4 \\ 0\amp 1\amp 0\amp \frac{5}{2} \\ 0\amp 0\amp 1\amp 1 \end{array}  \right]\text{.}
\end{equation*}
For the second column, we replace row one with row one plus row two to obtain the row equivalent matrix%
\begin{equation*}
\left[ \begin{array}{ccc|r} 2\amp 0\amp 0\amp -\frac{3}{2} \\ 0\amp 1\amp 0\amp \frac{5}{2} \\ 0\amp 0\amp 1\amp 1 \end{array}  \right]\text{.}
\end{equation*}
Since the leading entry in row one is not a one, we have one more step before we have the reduced row echelon form. Finally, we replace row one with row one times \(\frac{1}{2}\). This gives us the reduced row echelon form%
\begin{equation*}
\left[ \begin{array}{ccc|r} 1\amp 0\amp 0\amp -\frac{3}{4} \\ 0\amp 1\amp 0\amp \frac{5}{2} \\ 0\amp 0\amp 1\amp 1 \end{array}  \right]\text{.}
\end{equation*}
%
\item{}Find the solution(s), if any, to the system.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp9404680}{}\quad{}We can read off the solution to the system from the reduced row echelon form: \(x_1 = -\frac{3}{4}\), \(x_2 = \frac{5}{2}\), and \(x_3 = 1\). You should check in the original equations to make sure we have the correct solution.%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp9417864}%
In this example, \(a\) and \(b\) are unknown scalars. Consider the system with augmented matrix%
\begin{equation*}
\left[ \begin{array}{ccc|c} 1\amp 2\amp a\amp 3 \\ 1\amp 0\amp 0\amp b \\ 0\amp 1\amp 1\amp 0 \end{array}  \right]\text{.}
\end{equation*}
Find all values of \(a\) and \(b\) so that the system has:%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Exactly one solution (and find the solution)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp9410440}{}\quad{}Let \(x_1\), \(x_2\), and \(x_3\) be the variables corresponding to the first, second, and third columns, respectively, of the augmented matrix. To answer these questions, we row reduce the augmented matrix. We interchange rows one and two and then also rows two and three to obtain the matrix%
\begin{equation*}
\left[ \begin{array}{ccc|c} 1\amp 0\amp 0\amp b  \\ 0\amp 1\amp 1\amp 0 \\ 1\amp 2\amp a\amp 3 \end{array}  \right]\text{.}
\end{equation*}
%
\par
Now we replace row three with row three minus row one to produce the row equivalent matrix%
\begin{equation*}
\left[ \begin{array}{crc|r} 1\amp 0\amp 0\amp b  \\ 0\amp 1\amp 1\amp 0 \\ 0\amp 2\amp a\amp 3-b \end{array}  \right]\text{.}
\end{equation*}
%
\par
Next, replace row three with row three minus \(2\) times row two. This yields the row equivalent matrix%
\begin{equation*}
\left[ \begin{array}{ccc|c} 1\amp 0\amp 0\amp b  \\ 0\amp 1\amp 1\amp 0 \\ 0\amp 0\amp a-2\amp 3-b \end{array}  \right]\text{.}
\end{equation*}
%
\par
We now have a row echelon form.%
\par
The system will have exactly one solution when the last row has the form \([0 \ 0 \ u \ v]\) where \(u\) is not zero. Thus, the system has exactly one solution when \(a-2 \neq 0\), or when \(a \neq 2\). In this case, the solution is%
\begin{alignat*}{1}
x_3 \amp = \frac{3-b}{a-2},\\
x_2 \amp = -x_3 = \frac{b-3}{a-2}\\
x_1 \amp = b\text{.}
\end{alignat*}
You should check to ensure that this solution is correct. The other cases occur when \(a = 2\).%
\item{}No solutions%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp9420680}{}\quad{}When \(a=2\) and \(3-b \neq 0\) (or \(b \neq 3\)), then we have a row of the form \([0 \ 0 \ 0 \ t]\), where \(t\) is not \(0\). In these cases there are no solutions.%
\item{}Infinitely many solutions (and find all solutions)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp9425288}{}\quad{}When \(a=2\) and \(b = 3\), then the last row is a row of all zeros. In this case, the system is consistent and \(x_3\) is a free variable, so the system has infinitely many solutions. The solutions are%
\begin{alignat*}{1}
x_1 \amp = b\\
x_2 \amp = -x_3\\
x_3 \amp \text{ is free. }
\end{alignat*}
You should check to ensure that this solution is correct.%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_row_ech_summ}
In this section we learned about the row echelon and reduced row echelon forms of a matrix and some of the things these forms tell us about solutions to systems of linear equations.%
%
\begin{itemize}[label=\textbullet]
\item{}A matrix is in row echelon form if%
%
\begin{enumerate}
\item{}All nonzero rows are above any rows of all zeros.%
\item{}Each \terminology{pivot} (the first nonzero entry) of a row is in a column to the right of the pivot of the row above it.%
\end{enumerate}
\item{}Once an augmented matrix is in row echelon form, we can use back substitution to solve the corresponding linear system.%
\item{}To reduce a matrix to row echelon form we do the following:%
%
\begin{itemize}[label=$\circ$]
\item{}Begin with the leftmost nonzero column (if there is one). This will be a pivot column.%
\item{}Select a nonzero entry in this pivot column as a pivot. If necessary, interchange rows to move this entry to the first row (this entry will be a pivot).%
\item{}Use row operations to create zeros in all positions below the pivot.%
\item{}Cover (or ignore) the row containing the pivot position and cover all rows, if any, above it. Apply the preceding steps to the submatrix that remains. Repeat the process until there are no more nonzero rows to modify.%
\end{itemize}
\item{}A matrix is in reduced row echelon form if it is in row echelon form and%
\begin{enumerate}
\item{}The pivot in each nonzero row is 1.%
\item{}Each pivot is the only nonzero entry in its column.%
\end{enumerate}
%
\item{}To obtain the reduced row echelon form from the row echelon form, beginning with the rightmost pivot and working upward and to the left, create zeros above each pivot. If a pivot is not 1, make it 1 by an appropriate row multiplication.%
\item{}Both row echelon forms of an augmented matrix tell us about the number of solutions to the corresponding linear system.%
\begin{itemize}[label=$\circ$]
\item{}A linear system is inconsistent if and only if a row echelon form of the augmented matrix of the system contains a row of the form%
\begin{equation*}
[0 \ 0 \ 0 \ \cdots \ 0 \ *]\text{,}
\end{equation*}
where \(*\) is not zero. Another way to say this is that a linear system is inconsistent if and only if the last column of the augmented matrix of the system is a pivot column.%
\item{}A consistent linear system will have a unique solution if and only if each column but the last in the augmented matrix of the system is a pivot column. This is equivalent to saying that a consistent linear system will have a unique solution if and only if the consistent system has no free variables.%
\item{}A consistent linear system will have infinitely many solutions if and only if the coefficient matrix of the system contains a non-pivot column. In that case, the free variables corresponding to the non-pivot columns can be chosen arbitrarily and the basic variables corresponding to pivot columns can be written in terms of the free variables.%
\item{}A linear system can have no solutions, exactly one solution, or infinitely many solutions.%
\end{itemize}
%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_row_ech_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp9435912}%
Represent the following linear system in variables \(x_1, x_2, x_3\) in augmented matrix form and use row reduction to find the general solution of the system.%
\begin{alignat*}{4}
{}x_1   \amp {}+{}  \amp {}x_2  \amp {}-{}  \amp {}x_3   \amp = \amp {}4\amp {}\\
{}x_1  \amp {}+{}  \amp {2}x_2 \amp {}+{}  \amp {2}x_3 \amp = \amp {}3\amp {}\\
{2}x_1  \amp {}+{}  \amp {3}x_2 \amp {}-{}  \amp {3}x_3 \amp = \amp {}11\amp {.}
\end{alignat*}
%
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp9447304}%
Represent the following linear system in variables \(x_1, x_2, x_3\) in augmented matrix form after rearranging the terms and use row reduction to find all solutions to the system.%
\begin{alignat*}{1}
x_1 - x_3 - 2x_2 \amp = 3\\
2x_3 + 2 \amp = x_1 +x_2\\
4x_2 + 2x_1 - 2 \amp = 5x_3\text{.}
\end{alignat*}
%
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp9448456}%
Check that the reduced row echelon form of the matrix%
\begin{equation*}
\left[ \begin{array}{rrrr} 1 \amp -1\amp  3\amp  2 \\ -1\amp  2\amp -4\amp -1 \\ 2\amp 0\amp 6\amp 8 \end{array}  \right]
\end{equation*}
is%
\begin{equation*}
\left[ \begin{array}{cccc} 1 \amp 0\amp 0\amp 1 \\ 0\amp 1\amp 0\amp 2 \\ 0\amp 0\amp 1\amp 1 \end{array}  \right] \,\text{.}
\end{equation*}
%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp9452552}%
Consider the following system:%
\begin{alignat*}{5}
{}x   \amp {}-{}   \amp {2}y   \amp {}+{}   \amp {}z     \amp {}={}  \amp \ {-}\amp 1\amp {}\\
{}    \amp {}    \amp {2}y  \amp {}-{}   \amp {4}z  \amp {}={}   \amp  \ {}\amp 6\amp {}\\
{}    \amp {}    \amp {h}y  \amp {}-{}   \amp {2}z  \amp {}={}   \amp  \ {}\amp 1\amp {.}
\end{alignat*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find a row echelon form of the augmented matrix for this system.%
\item{}For which values of \(h\), if any, does the system have (i.) no solutions, (ii.) exactly one solution, (iii.) infinitely many solutions? Find the solutions in each case.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp9455368}%
Find the general solution of the linear system corresponding to the following augmented matrix:%
\begin{equation*}
\left[ \begin{array}{rrcr|r} 1\amp -1\amp 2\amp 1\amp 2 \\ -1\amp 2\amp 2\amp -1\amp -5\\ 1\amp 1\amp 10\amp 2\amp -1 \end{array}   \right]\text{.}
\end{equation*}
%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp9451784}%
What are the conditions, if any, on the \(a,
b, c\) values so that the following augmented matrix corresponds to a consistent linear system? How many solutions will the consistent system have? Explain.%
\begin{equation*}
\left[ \begin{array}{rrr|c} 1 \amp  2 \amp  3 \amp a \\ 2\amp 3\amp 7\amp b \\ -1\amp -4\amp -1\amp c \end{array}   \right]\text{.}
\end{equation*}
%
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp9464456}%
In this exercise the symbol \(\blacksquare\) denotes a non-zero number and the symbol \textasteriskcentered{} denotes any real number (including \(0\)).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Is the augmented matrix%
\begin{equation*}
\left[ \begin{array}{cc|c} \blacksquare\amp *\amp * \\ 0\amp \blacksquare\amp * \end{array}  \right]
\end{equation*}
in a form to which back substitution will easily give the solutions to the system? Explain your reasoning.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp9461256}{}\quad{}In order to help see what happens in the general case, substitute some numbers in place of the \(\blacksquare\)'s and \textasteriskcentered{}'s and answer the question for that specific system first. Then determine if your answer generalizes.%
\item{}The above matrix is a possible form of an augmented matrix with 2 rows and 3 columns corresponding to a linear system after forward elimination, i.e., a linear system for which back substitution will easily give the solutions. Determine the other possible such forms of the nonzero augmented matrices with 2 rows and 3 columns. As in part (a), use the symbol \(\blacksquare\) to denote a non-zero number and \textasteriskcentered{} to denote any real number.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp9465224}%
Give an example of a linear system with a unique solution for which a row echelon form of the augmented matrix of the system has a row of 0's.%
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp9461512}%
Come up with an example of an augmented matrix with 0's in the rightmost column corresponding to an inconsistent system, if possible. If not, explain why not.%
\end{divisionexercise}%
\begin{divisionexercise}{10}{}{}{g:exercise:idp9459976}%
Find two different row echelon forms which are equivalent to the same matrix not given in row echelon form.%
\end{divisionexercise}%
\begin{divisionexercise}{11}{}{}{g:exercise:idp9469832}%
Determine all possible row echelon forms of a \(2\times 2\) matrix. Use the symbol \(\blacksquare\) to denote a non-zero number and \textasteriskcentered{} to denote a real number with no condition on being 0 or not to represent entries.%
\end{divisionexercise}%
\begin{divisionexercise}{12}{}{}{g:exercise:idp9475336}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
The number of pivots of an \(m \times n\) matrix cannot exceed \(m\). (Note: Here \(m\), \(n\) are some unknown numbers.)%
\item{}\lititle{True\slash{}False.}\par%
The row echelon form of a matrix is unique.%
\item{}\lititle{True\slash{}False.}\par%
The reduced row echelon form of a matrix is unique.%
\item{}\lititle{True\slash{}False.}\par%
A system of equations where there are fewer equations than the number of unknowns (known as an underdetermined system) cannot have a unique solution.%
\item{}\lititle{True\slash{}False.}\par%
A system of equations where there are more equations than the number of unknowns (known as an overdetermined system) cannot have a unique solution.%
\item{}\lititle{True\slash{}False.}\par%
If a row echelon form of the \terminology{augmented matrix} of a system of three equations in two unknowns has three pivots, then the system is inconsistent.%
\item{}\lititle{True\slash{}False.}\par%
If the coefficient matrix of a system has pivots in every row, then the system is consistent.%
\item{}\lititle{True\slash{}False.}\par%
If there is a row of zeros in a row echelon form of the augmented matrix of a system of equations, the system has infinitely many solutions.%
\item{}\lititle{True\slash{}False.}\par%
If there is a row of zeros in a row echelon form of the augmented matrix of a system of \(n\) equations in \(n\) variables, the system has infinitely many solutions.%
\item{}\lititle{True\slash{}False.}\par%
If a linear system has no free variables, then the system has a unique solution.%
\item{}\lititle{True\slash{}False.}\par%
If a linear system has a free variable, then the system has infinitely many solutions.%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Modeling a Chemical Reaction}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Modeling a Chemical Reaction}{}{Project: Modeling a Chemical Reaction}{}{}{x:section:sec_prof_chem_react}
Recall the chemical equation%
\begin{equation*}
\text{C} _2\text{H} _6 + \text{O} _2 \to \text{ CO } _2 + \text{H} _2\text{O}
\end{equation*}
from the beginning of this section. This equation illustrates the reaction between ethane (\(\text{C} _2\text{H} _6\)) and oxygen (\(\text{O} _2\)),called the \terminology{reactants}, to produce carbon dioxide (\(\text{ CO } _2\)) and water (\(\text{H} _2\text{O}\)), called the \terminology{products} of the reaction. In any chemical reaction, the total mass of the reactants must equal the total mass of the products. In our reaction the chemicals involved are made up of carbon (C), hydrogen (H), and oxygen (O) atoms. To balance the equation, we need to know how many molecules of each chemical are combined to preserve the number of atoms of C, H, and O.%
\par
Let \(x_1\) be the number of molecules of \(\text{C} _2\text{H} _6\), \(x_2\) the number of molecules of \(\text{O} _2\), \(x_3\) the number of molecules of \(\text{ CO } _2\), and \(x_4\) the number of molecules of \(\text{H} _2\text{O}\) in the reaction. We can then represent this reaction as%
\begin{equation*}
x_1 \text{C} _2\text{H} _6 + x_2 \text{O} _2 \to x_3 \text{ CO } _2 + x_4 \text{H} _2\text{O}\text{.}
\end{equation*}
%
\par
In each molecule (e.g., ethane \(\text{C} _2\text{H} _6\)), the subscripts indicate the number of atoms of each element in the molecule. So 1 molecule of ethane contains 2 atoms of carbon and 6 atoms of hydrogen. Thus, there are 2 atoms of carbon in \(\text{C} _2\text{H} _6\) and 0 atoms of carbon in \(\text{O} _2\), giving us \(2x_1\) carbon atoms in \(x_1\) molecules of \(\text{C} _2\text{H} _6\) and 0 carbon atoms in \(x_2\) molecules of \(\text{O} _2\). On the product side of the reaction there is 1 carbon atom in \(\text{ CO } _2\) and 0 carbon atoms in \(\text{H} _2\text{O}\). To balance the reaction, we know that the number of carbon atoms in the products must equal the number of carbon atoms in the reactants.%
\begin{project}{}{x:project:act_1_c_reaction_1}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Set up an equation that balances the number of carbon atoms on both sides of the reaction.%
\item{}Balance the numbers of hydrogen and oxygen atoms in the reaction to explain why%
\begin{alignat*}{1}
6x_1 \amp = 2x_4\\
2x_2 \amp = 2x_3 + x_4\text{.}
\end{alignat*}
%
\item{}So the system of linear equations that models this chemical reaction is%
\begin{alignat*}{5}
{2}x_1   \amp {}    \amp {}    \amp {}-{}  \amp {}x_3  \amp {}    \amp {}       \amp {}={}  \amp  \ 0\amp {}\\
x_1  \amp {}     \amp {}     \amp {}    \amp {}    \amp {}-{}  \amp {2}x_4   \amp {}={}   \amp  \ 0\amp {}\\
{}      \amp {}    \amp {2}x_2  \amp {}-{}   \amp {2}x_3  \amp {}-{}  \amp {}x_4    \amp {}={}   \amp  \ 0\amp {.}
\end{alignat*}
Find all solutions to this system and then balance the reaction. Note that we cannot have a fraction of a molecule in our reaction.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp9508104}{}\quad{}Some of the work needed is done in \hyperref[x:exploration:pa_1_c]{Preview Activity~{\xreffont\ref{x:exploration:pa_1_c}}}.%
\end{enumerate}
\end{project}%
\begin{project}{}{g:project:idp9501064}%
Chemical reactions can be very interesting.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Carbon dioxide, \(\text{ CO } _2\), is a familiar product of combustion. For example, when we burn glucose, \(\text{C} _6\text{H} _{12}\text{O} _6\), the products of the reaction are carbon dioxide and water:%
\begin{equation}
\text{C} _6\text{H} _{12}\text{O} _6 + \text{O} _2 \to \text{ CO } _2 + \text{H} _2\text{O}\text{.}\label{x:men:eq_reaction2}
\end{equation}
Use the techniques developed in this project to balance this reaction.%
\item{}To burn glucose, we need to add oxygen to make the combustion happen. Carbon dioxide is different in that it can burn without the presence of oxygen. For example, when we mix magnesium (Mg) with dry ice (\(\text{ CO } _2\)), the products are magnesium oxide (MgO) and carbon (C). This is an interesting reaction to watch: you can see it at many websites, e.g.\@, \href{http://www.ebaumsworld.com/video/watch/404311/}{\nolinkurl{ebaumsworld.com/video/watch/404311/}} or \href{https://www.youtube.com/watch?v=-6dfi8LyRLA}{\nolinkurl{youtube.com/watch?v=-6dfi8LyRLA}} Use the method determined above to balance the chemical reaction%
\begin{equation}
\text{ Mg }  + \text{ CO } _2 \to \text{ MgO }  + \text{C}\text{.}\label{x:men:eq_reaction3}
\end{equation}
%
\end{enumerate}
\end{project}%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 4 Vector Representation}
\typeout{************************************************}
%
\begin{chapterptx}{Vector Representation}{}{Vector Representation}{}{}{x:chapter:chap_vector_representation}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp9510920}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What is a vector?%
\item{}How do we define operations on vectors?%
\item{}What is a linear combination of vectors?%
\item{}How do we determine if one vector is a linear combination of a given set of vectors?%
\item{}How do we represent a linear system as a vector equation?%
\item{}What is the span of a set of vectors?%
\item{}What are possible geometric representations of the span of a vector, or the span of two vectors?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: The Knight's Tour}
\typeout{************************************************}
%
\begin{sectionptx}{Application: The Knight's Tour}{}{Application: The Knight's Tour}{}{}{x:section:sec_appl_knight}
Chess is a game played on an \(8 \times 8\) grid which utilizes a variety of different pieces. One piece, the knight, is different from the other pieces in that it can jump over other pieces. However, the knight is limited in how far it can move in a given turn. For these reasons, the knight is a powerful, but often under-utilized, piece.%
\par
A knight can move two units either horizontally or vertically, and one unit perpendicular to that. Four knight moves are as illustrated in \hyperref[x:figure:F_knight_1]{Figure~{\xreffont\ref{x:figure:F_knight_1}}}, and the other four moves are the opposites of these.%
\begin{figureptx}{Moves a knight can make.}{x:figure:F_knight_1}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/knight_moves_1.pdf}
\end{image}%
\tcblower
\end{figureptx}%
The knight's tour problem is the mathematical problem of finding a knight's tour, that is a sequence of knight moves so the the knight visits each square exactly once. While we won't consider a knight's tour in this text, we will see using linear combinations of vectors that a knight can move from its initial position to any other position on the board, and that it is possible to determine a sequence of moves to make that happen.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_vec_rep_intro}
So far we learned of a convenient method to represent a linear system using matrices. We now consider another representation of a linear system using \terminology{vectors}. Vectors can represent concepts in the physical world like velocity, acceleration, and force \textemdash{} but we will be interested in vectors as algebraic objects in this class. Vectors will form the foundation for everything we will do in linear algebra. For now, the following definition will suffice.%
\begin{definition}{}{g:definition:idp9519880}%
\index{vector}%
\index{vector!entry}%
\index{vector!component}%
A (real) \terminology{vector} is a finite list of real numbers in a specified order. Each number in the list is referred to as an \terminology{entry} or \terminology{component} of the vector.%
\end{definition}
\begin{paragraphs}{Note.}{g:paragraphs:idp9260936}%
For the majority of this text, we will work with real vectors. However, a vector does not need to be restricted to have real entries. At times we will use complex vectors and even vectors in other types of sets. The types of sets we use will be ones that have structure just like the real numbers. Recall that a real number is a number that has a decimal representation, either finite or repeating (rational numbers) or otherwise (irrational numbers). We can add and multiply real numbers as we have done throughout our mathematical careers, and the real numbers have a certain structure given in the following theorem that we will treat as an axiom \textemdash{} that is, we assume these properties without proof. We denote the set of real numbers with the symbol \(\R\).%
\end{paragraphs}%
\begin{theorem}{}{}{x:theorem:thm_1_d_reals}%
Let \(x\), \(y\), and \(z\) be real numbers. Then%
\begin{itemize}[label=\textbullet]
\item{}\(x + y \in \R\) and \(xy \in \R\) (The name given to this property is closure. That is, the set \(\R\) is closed under addition and multiplication.)%
\item{}\(x + y = y + x\) and \(xy=yx\) (The name given to this property is commutativity. That is addition and multiplication are commutative operations in \(\R\).)%
\item{}\((x + y) + z = x + (y + z)\) and \((xy)z = x(yz)\) (The name given to this property is associativity. That is, addition and multiplication is associative operations in \(\R\).)%
\item{}There is an element \(0\) in \(\R\) such that \(x+ 0 = x\) (The element \(0\) is called the additive identity in \(\R\).)%
\item{}There is an element \(1\) in \(\R\) such that \((1)x = x\) (The element \(1\) is called the multiplicative identity in \(\R\).)%
\item{}There is an element \(-x\) in \(\R\) such that \(x+(-x) = 0\) (The element \(-x\) is the additive inverse of \(x\) in \(\R\).)%
\item{}If \(x \neq 0\), there is an element \(\frac{1}{x}\) in \(\R\) such that \(x\left(\frac{1}{x}\right) = 1\) (The element \(\frac{1}{x}\) is the multiplicative inverse of the nonzero element \(x\) in \(\R\).)%
\item{}\(x (y + z) = (x y) + (x z)\) (The is the distributive property. That is, multiplication distributes over addition in \(\R\).)%
\end{itemize}
%
\end{theorem}
\index{field}\index{scalars} Any set that satisfies the properties listed in \hyperref[x:theorem:thm_1_d_reals]{Theorem~{\xreffont\ref{x:theorem:thm_1_d_reals}}} is called a \terminology{field}. Our vectors are made from elements of a field, we call those elements of the field \terminology{scalars}.%
\par
We will algebraically represent a vector as a matrix with one column. For example, \(\vv = \left[ \begin{array}{c} 1\\ 2 \end{array} \right]\) is a vector with 2 entries, and we say that \(\vv\) is a vector in 2-space. By 2-space we mean \(\R^2\), which can be geometrically modeled as the plane. Here the symbol \(\R\) indicates that the entries of \(\vv\) are real numbers and the superscript 2 tells us that \(\vv\) has two entries. Similarly, vectors in \(\R^3\) have three entries, e.g., \(\left[ \begin{array}{r} 1\\ 3\\ -1 \end{array} \right]\). The collection of column vectors with three entries can be geometrically modeled as three-dimensional space. If a vector \(\vv\) has \(n\) entries we say that \(\vv\) is a vector in \(\R^n\) (or \(n\)-space). Vectors are also often indicated with arrows, so we might also see a vector \(\vv\) written as \(\overrightarrow{v}\). It is important when writing to differentiate between a vector \(\vv\) and a scalar \(v\). These are quite different objects and it is up to us to make sure we are clear what a symbol represents. We will use boldface letters to represent vectors.%
\par
\index{vector!column} A vector like \(\left[ \begin{array}{c} 1\\ 2 \end{array}  \right]\) is called a \terminology{column vector} of \emph{size} \(2 \times 1\) (two rows, one column). We can define an addition operation on two vectors of the same size by adding corresponding components, such as%
\begin{equation*}
\left[ \begin{array}{r} 1 \\ -2 \end{array}  \right] + \left[ \begin{array}{c} 3 \\ 4 \end{array}  \right] = \left[ \begin{array}{c} 4 \\ 2 \end{array}  \right]\text{.}
\end{equation*}
%
\par
Similarly, we can define scalar multiplication of a vector by multiplying each component of the vector by the scalar. For example,%
\begin{equation*}
3\left[ \begin{array}{c} 1\\ 2 \end{array}  \right] = \left[ \begin{array}{c} 3\\ 6 \end{array}  \right]\text{.}
\end{equation*}
%
\par
Since we can add vectors and multiply vectors by scalars, we can then add together scalar multiples of vectors. For completeness, we define vector subtraction as adding a scalar multiple:%
\begin{equation*}
\vv - \vu = \vv + (-1)\vu\text{.}
\end{equation*}
%
\par
This definition is equivalent to defining subtraction of \(\vu\) from \(\vv\) by subtracting components of \(\vu\) from the corresponding components of \(\vv\).%
\begin{exploration}{}{x:exploration:pa_1_d}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Given vectors%
\begin{equation*}
\vv =\left[ \begin{array}{r} 1 \\ -2 \\ 2 \end{array}  \right] \, , \, \vu=\left[ \begin{array}{c} 0 \\ 1\\ 3 \end{array}  \right] \, , \, \vw= \left[ \begin{array}{c} 1\\ 1\\ 4 \end{array}  \right]\,\text{,}
\end{equation*}
determine the components of the vector \(3\vv + \vu - 2\vw\) using the operations defined above.%
\item{}In mathematics, any time we define operations on objects, such as addition of vectors, we ask which properties the operation has. For example, one might wonder if \(\vu+\vv=\vv+\vu\) for any two vectors \(\vu, \vv\) of the same size. If this property holds, we say that the \terminology{addition of vectors is a commutative operation}. However, to verify this property we cannot use examples since the property must hold for any two vectors. For simplicity, we focus on two-dimensional vectors \(\vu = \left[ \begin{array}{c} u_1\\ u_2 \end{array} \right]\) and \(\vv = \left[ \begin{array}{c} v_1\\ v_2 \end{array} \right]\). Using these arbitrary vectors, can we say that \(\vu+\vv=\vv+\vu\)? If so, justify. If not, give a counterexample. (Note: Giving a counterexample is the best way to justify why a general statement is not true.)%
\item{}One way to geometrically represent vectors with two components uses a point in the plane to correspond to a vector. Specifically, the vector \(\left[ \begin{array}{c} x \\ y \end{array} \right]\) corresponds to the point \((x, y)\) in the plane. As a specific example, the vector \(\left[ \begin{array}{c} 1 \\ 2 \end{array} \right]\) corresponds to the point \((1, 2)\) in the plane. This representation will be especially handy when we consider infinite collections of vectors as we will do in this problem.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}On the same set of axes, plot the points that correspond to 5-6 scalar multiples of the vector \(\left[ \begin{array}{c} 1 \\ 2 \end{array} \right]\). Make sure to use variety of scalar multiples covering possibilities with \(c>0,
c\lt 0, c>1, 0\lt c\lt 1, -1\lt c\lt 0\). If we consider the collection of all possible scalar multiples of this vector, what do we obtain?%
\item{}What would the collection of all scalar multiples of the vector \(\left[ \begin{array}{c} 0 \\ 0 \end{array} \right]\) form in the plane?%
\item{}What would the collection of all scalar multiples of the vector \(\left[ \begin{array}{c} 1 \\ 1\\ 1 \end{array} \right]\) form in the three-dimensional space?%
\end{enumerate}
\item{}Let \(\vu=\left[ \begin{array}{c} 1\\ 2 \end{array} \right]\) and \(\vv=\left[ \begin{array}{r} 1\\ -1 \end{array} \right]\) in \(\R^2\). We are interested in finding all vectors that can be formed as a sum of scalar multiples of \(\vu\) and \(\vv\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}On the same set of axes, plot the points that correspond to the vectors \(\vu, \vv, \vu+\vv, 1.5\vu, 2\vv, -\vu, -\vv, -\vu+2\vv\). Plot other random sums of scalar multiples of \(\vu\) and \(\vv\) using several scalar multiples (including those less than 1 or negative) (that is, find other vectors of the form \(a\vu + b\vv\) where \(a\) and \(b\) are any scalars.).%
\item{}If we considered sums of all scalar multiples of \(\vu, \vv\), which vectors will we obtain? Can we obtain any vector in \(\R^2\) in this form?%
\end{enumerate}
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Vectors and Vector Operations}
\typeout{************************************************}
%
\begin{sectionptx}{Vectors and Vector Operations}{}{Vectors and Vector Operations}{}{}{x:section:sec_vec_ops}
As discussed in \hyperref[x:exploration:pa_1_d]{Preview Activity~{\xreffont\ref{x:exploration:pa_1_d}}}, a vector is simply a list of numbers. We can add vectors of like size and multiply vectors by scalars. These operations define a structure on the set of all vectors with the same number of components that will be our major object of study in linear algebra. Ultimately we will expand our idea of vectors to a more general context and study what we will call \terminology{vector spaces}.%
\par
In \hyperref[x:exploration:pa_1_d]{Preview Activity~{\xreffont\ref{x:exploration:pa_1_d}}} we saw how to add vectors and multiply vectors by scalars in \(\R^2\), and this idea extends to \(\R^n\) for any \(n\). Before we do so, one thing we didn't address in \hyperref[x:exploration:pa_1_d]{Preview Activity~{\xreffont\ref{x:exploration:pa_1_d}}} is what it means for two vectors to be equal. It should seem reasonable that two vectors are equal if and only if they have the same corresponding components. More formally, if we let%
\begin{equation*}
\vu = \left[ \begin{array}{c} u_1 \\ u_2 \\ \vdots \\ u_n \end{array}  \right] \ \text{ and }  \ \vv = \left[ \begin{array}{c} v_1 \\ v_2 \\ \vdots \\ v_n \end{array}  \right]
\end{equation*}
be vectors in \(\R^n\), then \(\vu = \vv\) if \(u_i = v_i\) for every \(i\) between 1 and \(n\). Note that this statement implies that a vector in \(\R^2\) cannot equal a vector in \(\R^3\) because they don't have the same number of components. With this in mind we can now define the sum \(\vu + \vv\) of the vectors \(\vu\) and \(\vv\) to be the vector in \(\R^n\) defined by%
\begin{equation*}
\vu + \vv = \left[ \begin{array}{c} u_1+v_1 \\ u_2+v_2 \\ \vdots \\ u_n+v_n \end{array}  \right]\text{.}
\end{equation*}
%
\par
In other words, to add two vectors of the same size, we add corresponding components.%
\par
Similarly, we can define scalar multiplication of a vector. If \(c\) is a scalar, then the scalar multiple \(c \vv\) of the vector \(\vv\) is the vector in \(\R^n\) defined by%
\begin{equation*}
c\vv = \left[ \begin{array}{c} cv_1 \\ cv_2 \\ \vdots \\ cv_n \end{array}  \right]\text{.}
\end{equation*}
%
\par
In other words, the scalar multiple \(c\vv\) of the vector \(\vv\) is the vector obtained by multiplying each component of the vector \(\vv\) by the scalar \(c\). Since we can add vectors and multiply vectors by scalars, we can then add together scalar multiples of vectors. For completeness, we define vector subtraction as adding a scalar multiple:%
\begin{equation*}
\vv - \vu = \vv + (-1)\vu\text{.}
\end{equation*}
%
\par
This definition is equivalent to defining subtraction of \(\vu\) from \(\vv\) by subtracting components of \(\vu\) from the corresponding components of \(\vv\).%
\par
After defining operations on objects, we should wonder what kinds of properties these operations have. For example, with the operation of addition of real numbers we know that \(1+2\) is equal to \(2+1\). This is called the \emph{commutative} property of scalar addition and says that order does not matter when we add real numbers. It is natural for us to ask if similar properties hold for the vector operations, addition and scalar multiplication, we defined. You showed in \hyperref[x:exploration:pa_1_d]{Preview Activity~{\xreffont\ref{x:exploration:pa_1_d}}} that the addition operation is also commutative on vectors in \(\R^2\).%
\par
In the activity below we consider how the two operations, addition and scalar multiplication, interact with each other. In real numbers, we know that multiplication is distributive over addition. Is that true with vectors as well?%
\begin{activity}{}{x:activity:act_A1_3_1}%
We work with vectors in \(\R^2\) to make the notation easier.%
\par
Let \(a\) be an arbitrary scalar, and \(\vu = \left[ \begin{array}{c} u_1\\ u_2 \end{array} \right]\) and \(\vv = \left[ \begin{array}{c} v_1\\ v_2 \end{array} \right]\) be two \emph{arbitrary} vectors in \(\R^2\). Is \(a(\vu + \vv)\) equal to \(a\vu + a\vv\)? What property does this imply about the scalar multiplication and addition operations on vectors?%
\end{activity}%
Similar arguments can be used to show the following properties of vector addition and multiplication by scalars.%
\begin{theorem}{}{}{x:theorem:thm_vector_properties}%
Let \(\vv\), \(\vu\), and \(\vw\) be vectors in \(\R^n\) and let \(a\) and \(b\) be scalars. Then%
\begin{enumerate}
\item{}\(\displaystyle \vv + \vu = \vu + \vv\)%
\item{}\(\displaystyle (\vv + \vu) + \vw = \vv + (\vu + \vw)\)%
\item{}The vector \(\vz = \left[ \begin{array}{c} 0 \\ 0 \\ \vdots \\ 0 \end{array} \right]\) has the property that \(\vv + \vz = \vv\). The vector \(\vz\) is called the \terminology{zero vector}.%
\item{}\((-1)\vv + \vv = \vz\). The vector \((-1)\vv = -\vv\) is called the \terminology{additive inverse} of the vector \(\vv\).%
\item{}\(\displaystyle (a+b) \vv = a\vv + b\vv\)%
\item{}\(\displaystyle a(\vv + \vu) = a\vv + a\vu\)%
\item{}\(\displaystyle (ab) \vv = a(b\vv)\)%
\item{}\(1 \vv = \vv\).%
\end{enumerate}
%
\end{theorem}
We will later see that the above properties make the set \(\R^n\) a \terminology{vector space}. These properties just say that, for the most part, we can manipulate vectors just as we do real numbers. Please note, though, that there is no multiplication or division of vectors.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Geometric Representation of Vectors and Vector Operations}
\typeout{************************************************}
%
\begin{sectionptx}{Geometric Representation of Vectors and Vector Operations}{}{Geometric Representation of Vectors and Vector Operations}{}{}{x:section:sec_geom_vec_ops}
We can geometrically represent a vector \(\vv = \left[ \begin{array}{c} v_1\\ v_2 \end{array} \right]\) in \(\R^2\) as the point \((v_1, v_2)\) in the plane as we did in \hyperref[x:exploration:pa_1_d]{Preview Activity~{\xreffont\ref{x:exploration:pa_1_d}}}. We can similarly represent a vector \(\vv = \left[ \begin{array}{c} v_1\\ v_2\\ v_3 \end{array} \right]\) in \(\R^3\) as the point \((v_1, v_2, v_3)\) in the three-dimensional space. This geometric representation will be handy when we consider collections of infinitely many vectors, as we will do when we consider the span of a collection of vectors later in this section.%
\par
We can also represent the vector \(\vv = \left[ \begin{array}{c} v_1\\ v_2 \end{array} \right]\) in \(\R^2\) as the directed line segment (or arrow) from the origin to the point \((v_1, v_2)\) as shown in \hyperref[x:figure:F_Vector1]{Figure~{\xreffont\ref{x:figure:F_Vector1}}} to aid in the visualization.%
\begin{figureptx}{The vector \(\left[ \begin{array}{c} 4\\ 6 \end{array}  \right]\) in \(\R^2\).}{x:figure:F_Vector1}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/1_d_a_1.pdf}
\end{image}%
\tcblower
\end{figureptx}%
The fact that the vector in \hyperref[x:figure:F_Vector1]{Figure~{\xreffont\ref{x:figure:F_Vector1}}} is represented by the directed line segment from the origin to the point (4,6) means that this vector is the vector \(\vv = \left[ \begin{array}{c} 4\\ 6 \end{array}  \right]\). If \(O\) is the origin and \(P\) is the point \((4,6)\), we will also denote this vector as \(\overrightarrow{OP}\) \textemdash{} so%
\begin{equation*}
\overrightarrow{OP} = \left[ \begin{array}{c} 4\\ 6 \end{array}  \right]\text{.}
\end{equation*}
%
\par
\index{vector!length in \(\R^n\)} In this way we can think of vectors as having direction and length. With the Pythagorean Theorem, we can see that the length of a vector \(\vv = \left[ \begin{array}{c} v_1\\v_2 \end{array}  \right]\) is \(\sqrt{v_1^2+v_2^2}\). This idea can be applied to vectors in any space. If \(\vv = \left[ \begin{array}{c} v_1 \\ v_2 \\ v_3 \\ \vdots \\ \ v_n \end{array}  \right]\) is a vector in \(\R^n\), then the \terminology{length} of \(\vv\), denoted \(|\vv|\) is the scalar%
\begin{equation*}
||\vv|| = \sqrt{v_1^2+v_2^2+ \cdots + v_n^2}\text{.}
\end{equation*}
%
\par
Thinking of vectors having direction and length is especially useful in visualizing the addition of vectors. The geometric interpretation of the sum of two vectors can be seen in \hyperref[x:figure:F_vector_sum_1]{Figure~{\xreffont\ref{x:figure:F_vector_sum_1}}} and \hyperref[x:figure:F_vector_sum_2]{Figure~{\xreffont\ref{x:figure:F_vector_sum_2}}}.%
\begin{sidebyside}{2}{0}{0}{0}%
\begin{sbspanel}{0.5}%
\begin{figureptx}{A vector sum.}{x:figure:F_vector_sum_1}{}%
\includegraphics[width=\linewidth]{external/1_d_a_3.pdf}
\tcblower
\end{figureptx}%
\end{sbspanel}%
\begin{sbspanel}{0.5}%
\begin{figureptx}{Geometric interpretation.}{x:figure:F_vector_sum_2}{}%
\includegraphics[width=\linewidth]{external/1_d_a_4.pdf}
\tcblower
\end{figureptx}%
\end{sbspanel}%
\end{sidebyside}%
\par
Let \(\vu = \left[ \begin{array}{c} 4\\ 6 \end{array} \right]\) and \(\vv = \left[ \begin{array}{r} 3\\ -2 \end{array} \right]\). Then \(\vu + \vv = \left[ \begin{array}{c} 7\\ 4 \end{array} \right]\) as shown in \hyperref[x:figure:F_vector_sum_1]{Figure~{\xreffont\ref{x:figure:F_vector_sum_1}}}. \hyperref[x:figure:F_vector_sum_2]{Figure~{\xreffont\ref{x:figure:F_vector_sum_2}}} provides a context to interpret this vector sum geometrically. Using the parallelogram imposed on the three vectors, we see that if vectors \(\vu\) and \(\vv\) are both placed to start at the origin, then the vector sum \(\vu + \vv\) can be visualized geometrically as the directed line segment from the origin to the fourth corner of the parallelogram.%
\par
In \hyperref[x:exploration:pa_1_d]{Preview Activity~{\xreffont\ref{x:exploration:pa_1_d}}} we considered scalar multiples of a vector in \(\R^2\). The arrow representation helps in visualizing scalar multiples as well. Geometrically, a scalar multiple \(c\vv\) of a nonzero vector \(\vv\) is a vector in the same direction as \(\vv\) if \(c>0\) and in the opposite direction as \(\vv\) if \(c\lt 0\). If \(c>1\), scalar multiplication stretches the vector, while \(0\lt c\lt 1\) shrinks the vector. We also saw that the collection of all scalar multiples of a vector \(\vv\) in \(\R^2\) gives us a line through the origin and \(\vv\), except when \(\vv=\vzero\) in which case we only obtain \(\vzero\). In other words, for a nonzero vector \(\vv\), the set \(S = \{c\vv : c \text{ is a scalar } \}\) is the line through the origin and \(\vv\) in \(\R^2\).%
\par
All of these properties generalize to vectors in \(\R^3\). Specifically, the scalar multiple \(c\vv\) is a vector in the same or opposite direction as \(\vv\) based on the sign of \(c\), and is a stretched or shrunken version of \(\vv\) based on whether \(|c|>1\) or \(|c|\lt 1\). Also, the collection of all multiples of a non-zero vector \(\vv\) in \(\R^3\) form a line through the origin.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Linear Combinations of Vectors}
\typeout{************************************************}
%
\begin{sectionptx}{Linear Combinations of Vectors}{}{Linear Combinations of Vectors}{}{}{x:section:sec_lin_comb_vec}
The concept of linear combinations is one of the fundamental ideas in linear algebra. We will use linear combinations to describe almost every important concept in linear algebra \textemdash{} the span of a set of vectors, the range of a linear transformation, bases, the dimension of a vector space \textemdash{} to name just a few.%
\par
In \hyperref[x:exploration:pa_1_d]{Preview Activity~{\xreffont\ref{x:exploration:pa_1_d}}}, we considered the sets of all scalar multiples of a single nonzero vector in \(\R^2\) and in \(\R^3\). We also considered the set of all sums of scalar multiples of two nonzero vectors. These results so far gives us an idea of geometrical descriptions of sets of vectors generated by one or two vectors. Oftentimes we are interested in what vectors can be made from a given collection of vectors. For example, suppose we have two different water-benzene-acetic acid chemical solutions, one with 40\% water, 50\% benzene and 10\% acetic acid, the other with 52\% water, 42\% benzene and 6\% acid. An experiment we want to conduct requires a chemical solution with 43\% water, 48\% benzene and 9\% acid. We would like to know if we make this new chemical solution by mixing the first two chemical solutions, or do we have to run to the chemical solutions market to get the chemical solution we want.%
\par
We can set up a system of equations for each ingredient and find the answer. But we can also consider each chemical solution as a vector, where the components represent the water, benzene and acid percentages. So the two chemical solutions we have are represented by the vectors \(\vv_1 = \left[ \begin{array}{c} 40\\50\\10 \end{array}  \right]\) and \(\vv_2 = \left[ \begin{array}{c} 52\\42\\6 \end{array}  \right]\). If we mix the two chemical solutions with varying amounts of each ingredient, then the question of whether we can make the desired chemical solution becomes the question of whether the equation%
\begin{equation*}
c_1\left[ \begin{array}{c} 40\\50\\10 \end{array}  \right]+ c_2\left[ \begin{array}{c} 52\\42\\6 \end{array}  \right] = \left[ \begin{array}{c} 43\\48\\9 \end{array}  \right]
\end{equation*}
has a solution. (You will determine if this equation has a solution in \hyperlink{x:exercise:ex_1_d_acid}{Exercise~{\xreffont 5}}.)%
\par
We might also be interested in what other chemical solutions we can make from the two given solutions. This amounts to determining which vectors can be written in the form \(c_1\left[ \begin{array}{c} 40\\50\\10 \end{array} \right]+ c_2\left[ \begin{array}{c} 52\\42\\6 \end{array} \right]\) for scalars \(c_1\) and \(c_2\). Vectors that are created from sums of scalar multiples of given vectors are called linear combinations of those vectors. More formally,%
\begin{definition}{}{x:definition:x1_d_linear_combination}%
\index{linear combination}%
\index{linear combination!weights}%
A \terminology{linear combination} of vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_m\) in \(\R^n\) is any vector of the form%
\begin{equation}
c_1 \vv_1 + c_2 \vv_2 + \cdots + c_m \vv_m\text{,}\label{x:men:eq_lin_comb}
\end{equation}
where \(c_1\), \(c_2\), \(\ldots\), \(c_m\) are scalars that we will refer to as the \terminology{weights}.%
\end{definition}
In the chemical solutions example, the vector \(c_1\left[ \begin{array}{c} 40\\50\\10 \end{array} \right]+ c_2\left[ \begin{array}{c} 52\\42\\6 \end{array} \right]\) for scalars \(c_1\) and \(c_2\) is a linear combination of the vectors \(\left[ \begin{array}{c} 40\\50\\10 \end{array} \right]\) and \(\left[ \begin{array}{c} 52\\42\\6 \end{array} \right]\) with weights \(c_1\) and \(c_2\), and the set of linear combinations of the given chemical solution vectors tells us exactly which chemical solutions we can make from the given ones. This is one example of how linear combinations can arise in applications.%
\par
The set of all linear combinations of a fixed collection of vectors has a very nice algebraic structure and, in small dimensions, allows us to use a geometrical description to aid our understanding. In the above example, this collection gives us the type of chemical solutions we can make by combining the first two solutions in varying amounts.%
\begin{activity}{}{x:activity:act_A1_3_7}%
Our chemical solution example illustrates that it can be of interest to determine whether certain vectors can be written as a linear combination of given vectors. We explore that idea in more depth in this activity. Let \(\vv_1=\left[ \begin{array}{c} 1 \\ 1 \\ 1 \end{array}  \right]\) and \(\vv_2=\left[ \begin{array}{r} 2 \\ -1 \\ 3 \end{array}  \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Calculate the linear combination of \(\vv_1\) and \(\vv_2\) with corresponding weights (scalar multiples) 1 and 2. The resulting vector is a vector which can be written as a linear combination of \(\vv_1\) and \(\vv_2\).%
\item{}Can \(\vw = \left[ \begin{array}{c} 3 \\ 0 \\ 4 \end{array} \right]\) be written as a linear combination of \(\vv_1\) and \(\vv_2\)? If so, which linear combination? If not, explain why not.%
\item{}Can \(\vw = \left[ \begin{array}{c} 2 \\ 0 \\ 2 \end{array} \right]\) be written as a linear combination of \(\vv_1\) and \(\vv_2\)? If so, which linear combination? If not, explain why not.%
\item{}Let \(\vw = \left[ \begin{array}{r} 0 \\ 6 \\ -2 \end{array}  \right]\). The problem of determining if \(\vw\) is a linear combination of \(\vv_1\) and \(\vv_2\) is equivalent to the problem of finding scalars \(x_1\) and \(x_2\) so that%
\begin{equation}
\vw = x_1 \vv_1 + x_2 \vv_2\text{.}\label{x:men:eq_vect_eq1}
\end{equation}
%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Combine the vectors on the right hand side of equation \hyperref[x:men:eq_vect_eq1]{({\xreffont\ref{x:men:eq_vect_eq1}})} into one vector, and then set the components of the vectors on both sides equal to each other to convert the vector equation \hyperref[x:men:eq_vect_eq1]{({\xreffont\ref{x:men:eq_vect_eq1}})} to a linear system of three equations in two variables.%
\item{}Use row operations to find a solution, if it exists, to the system you found in the previous part of this activity. If you find a solution, verify in \hyperref[x:men:eq_vect_eq1]{({\xreffont\ref{x:men:eq_vect_eq1}})} that you have found appropriate weights to produce the vector \(\vw\) as a linear combination of \(\vv_1\) and \(\vv_2\).%
\end{enumerate}
\end{enumerate}
\end{activity}%
Note that to find the weights that make \(\vw\) a linear combination of the vectors \(\vv_1\) and \(\vv_2\), we simply solved the linear system corresponding to the augmented matrix%
\begin{equation*}
[\vv_1 \  \vv_2 \ | \ \vw]\text{,}
\end{equation*}
where the vectors \(\vv_1\), \(\vv_2\), and \(\vw\) form the columns of an augmented matrix, and the solution of the system gave us the weights of the linear combination. In general, if we want to find weights \(c_1\), \(c_2\), \(\ldots\), \(c_m\) so that a vector \(\vw\) in \(\R^n\) is a linear combination of the vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_m\) in \(\R^n\), we solve the system corresponding to the augmented matrix%
\begin{equation*}
[\vv_1 \  \vv_2 \  \vv_3 \ \cdots \ \vv_m | \ \vw]\text{.}
\end{equation*}
%
\par
Any solution to this system will gives us the weights. If this system has no solutions, then \(\vw\) cannot be written as a linear combination of the vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_m\). This shows us the equivalence of the linear system and its vector equation representation. Specifically, we have the following result.%
\begin{theorem}{}{}{g:theorem:idp9777528}%
The vector equation%
\begin{equation*}
x_1 \vv_1 + x_2\vv_2 + x_3\vv_3 + \cdots + x_m\vv_m = \vw
\end{equation*}
has the same solution set as the linear system represented by the augmented matrix%
\begin{equation*}
[\vv_1 \  \vv_2 \  \vv_3 \ \cdots \ \vv_m | \ \vw]\text{.}
\end{equation*}
%
\par
In particular, the system has a solution if and only if \(\vw\) is a linear combination of the vectors \(\vv_1, \vv_2, \vv_3, \ldots, \vv_m\).%
\end{theorem}
\begin{activity}{}{x:activity:act_A1_3_8}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Represent the following linear system as a vector equation. After finding the vector equation, compare your vector equation to the matrix representation you found in \hyperref[x:exploration:pa_1_d]{Preview Activity~{\xreffont\ref{x:exploration:pa_1_d}}}. (Note that this is the same linear system from \hyperref[x:exploration:pa_1_c]{Preview Activity~{\xreffont\ref{x:exploration:pa_1_c}}}.)%
\begin{equation*}
\begin{split} -x_3 + 3 + 2x_2\amp = -x_1   \\ -3 + 2x_3 \amp = -x_2  \\ -2x_2 + x_1 \amp = 3x_3-7 \end{split}
\end{equation*}
%
\item{}Represent the following vector equation as a linear system and solve the linear system.%
\begin{equation*}
x_1 \left[ \begin{array}{c} 1 \\ 1 \\ 2 \end{array}  \right] +x_2 \left[ \begin{array}{c} 1 \\ 2\\ 3 \end{array}  \right] + x_3 \left[ \begin{array}{r} -1 \\ 2 \\ -3 \end{array}  \right] = \left[ \begin{array}{c} 4 \\ 3\\ 11 \end{array}  \right]
\end{equation*}
%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Span of a Set of Vectors}
\typeout{************************************************}
%
\begin{sectionptx}{The Span of a Set of Vectors}{}{The Span of a Set of Vectors}{}{}{x:section:sec_vec_span}
As we saw in the previous section, the question of whether a system of linear equations has a solution is equivalent to the question of whether the vector obtained by the non-coefficient constants in the system is a linear combination of the vectors obtained from the columns of the coefficient matrix of the system. So if we were interested in finding for which constants the system has a solution, we would look for the collection of all linear combinations of the columns. We call this collection the \terminology{span} of these vectors. In this section we investigate the concept of span both algebraically and geometrically.%
\par
Our work in \hyperref[x:exploration:pa_1_d]{Preview Activity~{\xreffont\ref{x:exploration:pa_1_d}}} seems to indicate that the span of a set of vectors, i.e., the collection of all linear combinations of this set of vectors, has a nice structure. As we mentioned above, the span of a set of vectors represents the collection of all constant vectors for which a linear system has a solution, but we will also see that other important objects in linear algebra can be represented as the span of a set of vectors.%
\begin{definition}{}{x:definition:def_1_d_span}%
\index{span}%
The \terminology{span} of the vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_m\) in \(\R^n\) is the collection of all linear combinations of the vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_m\).%
\end{definition}
\begin{paragraphs}{Notation.}{g:paragraphs:idp9789176}%
We denote the span of a set of vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_m\) as%
\begin{equation*}
\Span \{\vv_1, \vv_2, \ldots, \vv_m\}\text{.}
\end{equation*}
%
\par
So%
\begin{equation*}
\Span \{\vv_1, \vv_2, \ldots, \vv_m\}  = \{c_1\vv_1+c_2\vv_2 + \cdots + c_m \vv_m : c_1, c_2, \ldots, c_m \text{ are scalars } \}\text{.}
\end{equation*}
%
\par
The curly braces, \(\{ \; \}\), are used in denoting sets. They represent the whole set formed by the objects included between them. So \(\{\vv_1, \vv_2, \ldots, \vv_m\}\) represents the collection of the vectors formed by \(\vv_1, \vv_2, \ldots, \vv_m\) for an arbitrary number \(m\). Note that \(m\) can be 1, meaning that the collection can contain only one vector \(\vv_1\).%
\end{paragraphs}%
\par
We now investigate what the span of a set of one or two vectors is, both from an algebraic and geometric perspective, and consider what happens for more general spanning sets.%
\begin{activity}{}{x:activity:act_A1_3_9}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}By definition, \(\Span\left\{\left[ \begin{array}{r} 1 \\ -2 \end{array} \right]\right\}\) is the collection of all vectors which are scalar multiples of \(\left[ \begin{array}{r} 1 \\ -2 \end{array} \right]\). Determine which vectors are in this collection. If we plot all these vectors with each vector being represented as a point in the plane, what do they form?%
\item{}Let \(\vv_1 = \left[ \begin{array}{c} 1 \\ 0\\ 1 \end{array}  \right]\) and \(\vv_2 = \left[ \begin{array}{c} 0\\1 \\1 \end{array}  \right]\) in \(\R^3\). By definition,%
\begin{equation*}
\Span\left\{\left[ \begin{array}{c} 1 \\ 0\\ 1 \end{array}  \right], \left[ \begin{array}{c} 0\\1 \\1 \end{array}  \right] \right\}
\end{equation*}
is the collection of all linear combinations of the form%
\begin{equation*}
x_1 \left[ \begin{array}{c} 1 \\ 0\\ 1 \end{array}  \right] + x_2 \left[ \begin{array}{c} 0\\1 \\1 \end{array}  \right]\text{,}
\end{equation*}
where \(x_1\) and \(x_2\) are any scalars.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Find four different vectors in \(\Span \{\vv_1, \vv_2\}\) and indicate the weights (the values of \(x_1\) and \(x_2\)) for each linear combination.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp9798904}{}\quad{}It is really easy to find 3 vectors in \(\Span \{\vv_1, \vv_2\}\) for any \(\vv_1, \vv_2\).%
\item{}Are there any vectors in \(\R^3\) that are not in \(\Span\{\vv_1, \vv_2\}\)? Explain. Verify your result.%
\item{}Set up a linear system to determine which vectors \(\vw= \left[ \begin{array}{c} w_1 \\ w_2\\ w_3 \end{array} \right]\) are in \(\Span\{\vv_1, \vv_2\}\). Specifically, which \(\vw\) can be expressed as a linear combination of \(\vv_1\) and \(\vv_2\)?%
\item{}Geometrically, what shape do the vectors in \(\Span\{\vv_1, \vv_2\}\) form inside \(\R^3\)?%
\end{enumerate}
\item{}Is it possible for \(\Span\{\vz_1, \vz_2\}\) to be a line for two vectors \(\vz_1, \vz_2\) in \(\R^3\)?%
\item{}What do you think are the possible geometric descriptions of a span of a set of vectors in \(\R^2\)? Explain.%
\item{}What do you think are the possible spans of a set of vectors in \(\R^3\)? Explain.%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_vec_rep_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp9819384}%
For each of the following systems,%
\begin{itemize}[label=\textbullet]
\item{}express an arbitrary solution to the system algebraically as a linear combination of vectors,%
\item{}find a set of vectors that spans the solution set,%
\item{}describe the solution set geometrically.%
\end{itemize}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}%
\begin{alignat*}{5}
{}x_1   \amp {}{}     \amp {}     \amp {}+{}    \amp {}x_3     \amp = \amp {}   \amp 0\amp {}\\
{2}x_1   \amp {}+{}  \amp {}x_2   \amp {}+{}     \amp {3}x_3     \amp = \amp {}   \amp 0\amp {}\\
x_1  \amp {}-{}   \amp {}x_2   \amp {}+{}     \amp {3}x_3    \amp = \amp {}   \amp 0\amp {.}
\end{alignat*}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp9816184}{}\quad{}In each example, we use technology to find the reduced row echelon form of the augmented matrix.%
\par
The reduced row echelon form of the augmented matrix%
\begin{equation*}
\left[ \begin{array}{crc|c} 1\amp 0\amp 1\amp 0 \\ 2\amp 1\amp 3\amp 0 \\ 4\amp -1\amp 3\amp 0 \end{array}  \right]
\end{equation*}
is%
\begin{equation*}
\left[ \begin{array}{ccc|c} 1\amp 0\amp 1\amp 0 \\ 0\amp 1\amp 1\amp 0 \\ 0\amp 0\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
%
\begin{itemize}[label=\textbullet]
\item{}There is no pivot in the \(x_3\) column, so \(x_3\) is a free variable. Since the system is consistent, it has infinitely many solutions. We can write both \(x_1\) and \(x_2\) in terms of \(x_3\) as \(x_2 = -x_3\) and \(x_1 = -x_3\). So the general solution to the system has the algebraic form%
\begin{equation*}
\left[ \begin{array}{c} x_1\\x_2\\x_3 \end{array}  \right] = \left[ \begin{array}{r} -x_3\\-x_3\\x_3 \end{array}  \right] = x_3\left[ \begin{array}{r} -1\\-1\\1 \end{array}  \right]\text{.}
\end{equation*}
So every solution to this system is a scalar multiple (linear combination) of the vector \(\left[ \begin{array}{r} -1\\-1\\1 \end{array}  \right]\).%
\item{}Since every solution to the system is a scalar multiple of the vector \(\left[ \begin{array}{r} -1\\-1\\1 \end{array} \right]\), the solution set to the system is \(\Span\left\{\left[ \begin{array}{r} -1\\-1\\1 \end{array} \right]\right\}\).%
\item{}As the set of scalar multiples of a single vector, the solution set to this system is a line in \(\R^3\) through the origin and the point \((-1,-1,1)\).%
\end{itemize}
%
\item{}~%
\begin{alignat*}{5}
{}x_1   \amp {}+{}   \amp {2}x_2   \amp {}+{}    \amp {3}x_3     \amp = \amp {}   \amp 0\amp {}\\
{2}x_1   \amp {}+{}  \amp {4}x_2   \amp {}+{}     \amp {6}x_3     \amp = \amp {}   \amp 0\amp {}\\
x_1  \amp {}+{}   \amp {8}x_2   \amp {}+{}     \amp {12}x_3    \amp = \amp {}   \amp 0\amp {.}
\end{alignat*}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp9825784}{}\quad{}The reduced row echelon form of the augmented matrix%
\begin{equation*}
\left[ \begin{array}{ccc|c} 1\amp 2\amp 3\amp 0 \\ 2\amp 4\amp 6\amp 0 \\ 4\amp 8\amp 12\amp 0 \end{array}  \right]
\end{equation*}
is%
\begin{equation*}
\left[ \begin{array}{ccc|c} 1\amp 2\amp 3\amp 0 \\ 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
%
\begin{itemize}[label=\textbullet]
\item{}There are no pivots in the \(x_2\) and \(x_3\) columns, so \(x_2\) and \(x_3\) are free variables. Since the system is consistent, it has infinitely many solutions. We can write \(x_1\)  in terms of  \(x_2\) and \(x_3\) as \(x_1 = -2x_2-3x_3\). So the general solution to the system has the algebraic form%
\begin{equation*}
\left[ \begin{array}{c} x_1\\x_2\\x_3 \end{array}  \right] = \left[ \begin{array}{c} -2x_2-3x_3\\x_2\\x_3 \end{array}  \right] = x_2\left[ \begin{array}{r} -2\\1\\0 \end{array}  \right] + x_3\left[ \begin{array}{r} -3\\0\\1 \end{array}  \right]\text{.}
\end{equation*}
So every solution to this system is a linear combination of the vectors \(\left[ \begin{array}{r} -2\\1\\0 \end{array}  \right]\) and \(\left[ \begin{array}{r} -3\\0\\1 \end{array}  \right]\).%
\item{}Since every solution to the system is a linear combination of the vectors \(\left[ \begin{array}{r} -2\\1\\0 \end{array}  \right]\) and \(\left[ \begin{array}{r} -3\\0\\1 \end{array}  \right]\), the solution set to the system is%
\begin{equation*}
\Span\left\{\left[ \begin{array}{r} -2\\1\\0 \end{array}  \right], \left[ \begin{array}{r} -3\\0\\1 \end{array}  \right] \right\}\text{.}
\end{equation*}
%
\item{}As the set of linear combinations of two vectors, the solution set to this system is a plane in \(\R^3\) through the origin and the points \((-2,1,0)\) and \((-3,0,1)\).%
\end{itemize}
%
\end{enumerate}
\end{example}
\begin{example}{}{x:example:example_1_d_span}%
Let \(W = \left\{\left[ \begin{array}{c} s+t \\ r+2s \\ r-3t \\ r+s+t \end{array} \right] : r,s,t \in \R \right\}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find three vectors \(\vv_1\), \(\vv_2\), and \(\vv_3\) such that \(W = \Span\{\vv_1, \vv_2, \vv_3\}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp9836152}{}\quad{}Every vector in \(W\) has the form%
\begin{alignat*}{1}
\left[ \begin{array}{c} s+t\\
r+2s\\
r-3t\\
r+s+t \end{array} \right] \amp = \left[ \begin{array}{c} 0\\
r\\
r\\
r\end{array} \right] + \left[ \begin{array}{c} s\\
2s\\
0\\
s \end{array} \right] + \left[ \begin{array}{r} t\\
0\\
-3t\\
t \end{array} \right]\\
\amp = r\left[ \begin{array}{c} 0\\
1\\
1\\
1 \end{array} \right] + s\left[ \begin{array}{r} 1\\
2\\
0\\
1 \end{array} \right] + t\left[ \begin{array}{r} 1\\
0\\
-3\\
1 \end{array} \right]
\end{alignat*}
for some real numbers \(r\), \(s\), and \(t\). Thus, \(W = \Span\{\vv_1, \vv_2, \vv_3\}\) where \(\vv_1 = \left[ \begin{array}{c} 0 \\ 1 \\ 1 \\ 1 \end{array}  \right]\), \(\vv_2 = \left[ \begin{array}{r} 1 \\ 2 \\ 0 \\ 1 \end{array}  \right]\), and \(\vv_3 = \left[ \begin{array}{r} 1 \\ 0 \\ -3 \\ 1 \end{array}  \right]\).%
\item{}Can \(\vw = \left[ \begin{array}{r} -2\\-4\\-1\\0 \end{array} \right]\) be written as a linear combination of the vectors \(\vv_1\), \(\vv_2\), \(\vv_3\)? If so, find such a linear combination. If not, justify your response. What does your result tell us about the relationship between \(\vw\) and \(W\)? Explain.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp9857400}{}\quad{}To determine if \(\vw\) is a linear combination of \(\vv_1\), \(\vv_2\), and \(\vv_3\), we row reduced the augmented matrix \([\vv_1 \ \vv_2 \ \vv_3 \ | \ \vw]\). The reduced row echelon form of the matrix \([\vv_1 \ \vv_2 \ \vv_3 \ | \ \vw]\) is%
\begin{equation*}
\left[ \begin{array}{ccc|r} 1\amp 0\amp 0\amp 2 \\ 0\amp 1\amp 0\amp -3\\ 0\amp 0\amp 1\amp 1 \\ 0\amp 0\amp 0\amp 0 \end{array} \right]\text{.}
\end{equation*}
The system with this as augmented matrix is consistent. If we let \(x_1\), \(x_2\), and \(x_3\) be the variables corresponding to the first three columns, respectively, of this augmented matrix, then we see that \(x_1 = 2\), \(x_2 = -3\), and \(x_3 = 1\). So \(\vw\) can be written as a linear combination of \(\vv_1\), \(\vv_2\), and \(\vv_3\) as%
\begin{equation*}
\vw = 2\vv_1 -3\vv_2 + \vv_3\text{.}
\end{equation*}
Since \(W = \Span\{\vv_1, \vv_2, \vv_3\}\), it follows that \(\vw \in W\).%
\item{}Can \(\vu = \left[ \begin{array}{r} 3\\-4\\1\\-1 \end{array} \right]\) be written as a linear combination of the vectors \(\vv_1\), \(\vv_2\), \(\vv_3\)? If so, find such a linear combination. If not, justify your response. What does your result tell us about the relationship between \(\vu\) and \(W\)? Explain.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp9868792}{}\quad{}To determine if \(\vu\) is a linear combination of \(\vv_1\), \(\vv_2\), and \(\vv_3\), we row reduced the augmented matrix \([\vv_1 \ \vv_2 \ \vv_3 \ | \ \vu]\). The reduced row echelon form of the matrix \([\vv_1 \ \vv_2 \ \vv_3 \ | \ \vu]\) is%
\begin{equation*}
\left[ \begin{array}{ccc|c} 1\amp 0\amp 0\amp 0 \\ 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 1\amp 0 \\ 0\amp 0\amp 0\amp 1 \end{array} \right]\text{.}
\end{equation*}
The last row shows that the system with this as augmented matrix is inconsistent. So \(\vu\) cannot be written as a linear combination of \(\vv_1\), \(\vv_2\), and \(\vv_3\). Since \(W = \Span\{\vv_1, \vv_2, \vv_3\}\), it follows that \(\vu \not\in W\).%
\item{}What relationship, if any, exists between \(\Span\{\vv_1, \vv_2, \vv_3\}\) and \(\Span \ W\)? Explain.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp9872248}{}\quad{}We know that \(\Span\{\vv_1, \vv_2, \vv_3\} = W\). Now \(\Span \ W\) contains the linear combinations of vectors in \(W\), which are all linear combinations of the vectors \(\vv_1\), \(\vv_2\), and \(\vv_3\). Thus, \(\Span \ W\) is just the set of linear combinations of \(\vv_1\), \(\vv_2\), and \(\vv_3\). We conclude that \(\Span \ W = \Span\{\vv_1, \vv_2, \vv_3\} = W\).%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_vec_rep_summ}
%
\begin{itemize}[label=\textbullet]
\item{}A vector is a list of numbers in a specified order.%
\item{}We add two vectors of the same size by adding corresponding components. In other words, if \(\vu\) and \(\vv\) are vectors of the same size and \(u_i\) and \(v_i\) are the \(i\) components of \(\vu\) and \(\vv\), respectively, then \(\vu +\vv\) is the vector whose \(i\)th component is \(u_i+v_i\) for each \(i\). Geometrically, we represent the sum of two vectors using the Parallelogram Rule: The vector \(\vu+\vv\) is the directed line segment from the origin to the 4th point of the parallelogram formed by the origin and the vectors \(\vu, \vv\).%
\item{}A scalar multiple of a vector is found by multiplying each component of the vector by that scalar. In other words, if \(v_i\) is the \(i\) component of the vector \(\vv\) and \(c\) is any scalar, then \(c\vv\) is the vector whose \(i\) component is \(cv_i\) for each \(i\). Geometrically, a scalar multiple of a nonzero vector \(\vv\) is a vector in the same direction as \(\vv\) if \(c>0\) and in the opposite direction if \(c\lt 0\). If \(|c|>1\), the vector is stretched, and if \(|c|\lt 1\), the vector is shrunk.%
\item{}An important concept is that of a linear combination of vectors. In words, a linear combination of a collection of vectors is a sum of scalar multiples of the vectors. More formally, we defined a linear combination of vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_m\) in \(\R^n\) is any vector of the form \(c_1 \vv_1 + c_2 \vv_2 + \cdots + c_m \vv_m\), where \(c_1\), \(c_2\), \(\ldots\), \(c_m\) are scalars.%
\item{}To find weights \(c_1\), \(c_2\), \(\ldots\), \(c_m\) so that a vector \(\vw\) in \(\R^n\) is a linear combination of the vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_m\) in \(\R^n\), we simply solve the system corresponding to the augmented matrix%
\begin{equation*}
[\vv_1 \  \vv_2 \  \vv_3 \ \cdots \ \vv_m | \ \vw]\text{.}
\end{equation*}
%
\item{}The collection of all linear combinations of a set of vectors is called the span of the set of vectors. More formally, the span of the vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_m\) in \(\R^n\) is the set%
\begin{equation*}
\{c_1\vv_1 + c_2\vv_2 + \cdots + c_m\vv_m : c_1, c_2, \ldots, c_m \text{ are scalars } \}\text{,}
\end{equation*}
which we denote as \(\Span \{\vv_1, \vv_2, \ldots, \vv_m\}\). Geometrically, the span of a single nonzero vector \(\vv\) in any dimension is the line through the origin and the vector \(\vv\). The span of two vectors \(\vv_1, \vv_2\) in any dimension neither of which is a multiple of the other is a plane through the origin containing both vectors.%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{g:exercises:idp9907080}
\begin{divisionexercise}{1}{}{}{x:exercise:sec_vec_rep_exer}%
Given vectors \(\vu=\left[ \begin{array}{c} 1 \\ 2 \end{array} \right]\) and \(\vv=\left[ \begin{array}{r} -1 \\ 2 \end{array} \right]\) in \(\R^2\), determine if \(\vw=\left[ \begin{array}{r} -4 \\ -1 \end{array} \right]\) can be written as a linear combination of \(\vu\) and \(\vv\). If so, determine the weights of \(\vu\) and \(\vv\) which produce \(\vw\).%
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp9911176}%
Given vectors \(\vv_1=\left[ \begin{array}{c} 1 \\ 2 \\1 \end{array} \right]\), \(\vv_2=\left[ \begin{array}{r} -2 \\ 1 \\2 \end{array} \right]\) and \(\vv_3=\left[ \begin{array}{r} -1 \\ 3\\3 \end{array} \right]\) in \(\R^3\), determine if \(\vw=\left[ \begin{array}{c} 5\\ 5\\ 1 \end{array} \right]\) can be written as a linear combination of \(\vv_1\), \(\vv_2\) and \(\vv_3\). If so, determine the weights of \(\vv_1\), \(\vv_2\) and \(\vv_3\) which produce \(\vw\). Reflect on the result. Is there anything special about the given vectors \(\vv_1\), \(\vv_2\) and \(\vv_3\)?%
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp9918344}%
Let \(\vu=\left[ \begin{array}{c} 1 \\ 2 \\1 \end{array} \right]\) and \(\vv=\left[ \begin{array}{r} -1 \\ 1 \\ 1 \end{array} \right]\) in \(\R^3\). Determine which vectors \(\vw= \left[ \begin{array}{c} w_1 \\ w_2 \\ w_3 \end{array} \right]\) in \(\R^3\) can be written as a linear combination of \(\vu\) and \(\vv\). Does the set of \(\vw\)'s include the 0 vector? If so, determine which weights in the linear combination produce the 0 vector. If not, explain why not.%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp9920264}%
Consider vectors \(\vu=\left[ \begin{array}{c} 0\\ 2\\0 \end{array} \right]\) and \(\vv=\left[ \begin{array}{c} 1\\ 1\\1 \end{array} \right]\) in \(\R^3\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find four specific linear combinations of the vectors \(\vu\) and \(\vv\).%
\item{}Explain why the zero vector must be a linear combination of \(\vu\) and \(\vv\).%
\item{}What kind of geometric shape does the set of all linear combinations of \(\vu\) and \(\vv\) have in \(\R^3\)?%
\item{}Can we obtain any vector in \(\R^3\) as a linear combination of \(\vu\) and \(\vv\)? Explain.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{x:exercise:ex_1_d_acid}%
Suppose we have two different water-benzene-acetic acid solutions, one with 40\% water, 50\% benzene and 10\% acetic acid, the other with 52\% water, 42\% benzene and 6\% acid.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}An experiment we want to conduct requires a solution with 43\% water, 48\% benzene and 9\% acid. Representing each acid solution as a vector, determine if we can we make this new acid solution by mixing the first two solutions, or do we have to run to the chemical solutions market to get the solution we want?%
\item{}Using the water-benzene-acetic acid solutions in the previous problem, can we obtain an acid solution which contains 50\% water, 43\% benzene and 7\% acid?%
\item{}Determine the relationship between the percentages of water, benzene, and acid in solutions which can be obtained by mixing the two given water-benzene-acetic acid solutions above.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp9938312}%
Is the vector \(\vb = \left[ \begin{array}{c} 0\\1\\2 \end{array} \right]\) in \(\Span\left\{ \left[ \begin{array}{r} 2\\-1\\0 \end{array} \right], \left[ \begin{array}{r} -3\\0\\-5 \end{array} \right], \left[ \begin{array}{c} 1\\1\\0 \end{array} \right] \right\}\)? Justify your answer.%
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp9936008}%
Describe geometrically each of the following sets.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\Span\left\{\left[ \begin{array}{c} 1\\1 \end{array} \right], \left[ \begin{array}{r} -1\\-1 \end{array} \right]\right\}\) in \(\R^2\)%
\item{}\(\Span\left\{\left[ \begin{array}{c} 1\\1\\1 \end{array} \right], \left[ \begin{array}{r} -1\\-1\\-1 \end{array} \right], \left[ \begin{array}{c} 2\\0\\1 \end{array} \right]\right\}\) in \(\R^3\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp9944712}%
Consider the linear system%
\begin{alignat*}{5}
{2}x_1  \amp {}+{}  \amp {3}x_2 \amp {}+{}  \amp {3}x_3  \amp {}    \amp {}    \amp {}={} \amp 0\amp {}\\
x_1  \amp {}    \amp {}     \amp {}+{}  \amp {6}x_3  \amp {}+{}  \amp {6}x_4  \amp {}={} \amp 0\amp {}\\
{2}x_1  \amp {}+{}  \amp {4}x_2  \amp {}+{}  \amp {3}x_3  \amp {}-{}  \amp {}x_4  \amp {}={} \amp 0\amp {.}
\end{alignat*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the general solution to this system.%
\item{}Find two specific vectors \(\vv_1\) and \(\vv_2\) so that the solution set to this system is \(\Span\{\vv_1, \vv_2\}\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp9958792}%
Answer the following question as yes or no. Verify your answer. If \(\vu\) and \(\vv\) are vectors in \(\R^n\), then \(\vv\) is in \(\Span\{\vu, \vu-\vv\}\).%
\end{divisionexercise}%
\begin{divisionexercise}{10}{}{}{g:exercise:idp9952392}%
Let \(\vv\), \(\vu\), and \(\vw\) be vectors in \(\R^n\) and let \(a\) and \(b\) be scalars. Verify \hyperref[x:theorem:thm_vector_properties]{Theorem~{\xreffont\ref{x:theorem:thm_vector_properties}}}. That is, show that%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\vv + \vu = \vu + \vv\)%
\item{}\((\vv + \vu) + \vw = \vv + (\vu + \vw)\)%
\item{}The vector \(\vz = \left[ \begin{array}{c} 0 \\ 0 \\ \vdots \\ 0 \end{array} \right]\) has the property that \(\vv + \vz = \vv\).%
\item{}\((-1)\vv + \vv = \vz\).%
\item{}\((a+b) \vv = a\vv + b\vv\)%
\item{}\(a(\vv + \vu) = a\vv + a\vu\)%
\item{}\((ab) \vv = a(b\vv)\)%
\item{}\(1 \vv = \vv\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{11}{}{}{g:exercise:idp9964936}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
A vector in \(\R^2\), i.e. a two-dimensional vector, is also a vector in \(\R^3\).%
\item{}\lititle{True\slash{}False.}\par%
Any vector in \(\R^2\) can be visualized as a vector in \(\R^3\) by adding a 0 as the last coordinate.%
\item{}\lititle{True\slash{}False.}\par%
The zero vector is a scalar multiple of any other vector (of the same size).%
\item{}\lititle{True\slash{}False.}\par%
The zero vector cannot be a linear combination of two non-zero vectors.%
\item{}\lititle{True\slash{}False.}\par%
Given two vectors \(\vu\) and \(\vv\), the vector \(\frac{1}{2}\vu\) is a linear combination of \(\vu\) and \(\vv\).%
\item{}\lititle{True\slash{}False.}\par%
Given any two non-zero vectors \(\vu\) and \(\vv\) in \(\R^2\), we can obtain any vector in \(\R^2\) as a linear combination of \(\vu\) and \(\vv\).%
\item{}\lititle{True\slash{}False.}\par%
Given any two distinct vectors \(\vu\) and \(\vv\) in \(\R^2\), we can obtain any vector in \(\R^2\) as a linear combination of \(\vu\) and \(\vv\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\vu\) can be expressed as a linear combination of \(\vv_1\) and \(\vv_2\), then \(2\vu\) can also be expressed as a linear combination of \(\vv_1\) and \(\vv_2\).%
\item{}\lititle{True\slash{}False.}\par%
The span of any two vectors neither of which is a multiple of the other can be visualized as a plane through the origin.%
\item{}\lititle{True\slash{}False.}\par%
Given any vector, the collection of all linear combinations of this vector can be visualized as a line through the origin.%
\item{}\lititle{True\slash{}False.}\par%
The span of any collection of vectors includes the \(\vzero\) vector.%
\item{}\lititle{True\slash{}False.}\par%
If the span of \(\vv_1\) and \(\vv_2\) is all of \(\R^2\), then so is the span of \(\vv_1\) and \(\vv_1+\vv_2\).%
\item{}\lititle{True\slash{}False.}\par%
If the span of \(\vv_1, \vv_2\) and \(\vv_3\) is all of \(\R^3\), then so is the span of \(\vv_1+\vv_2\) and \(\vv_2+\vv_3\).%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Analyzing Knight Moves}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Analyzing Knight Moves}{}{Project: Analyzing Knight Moves}{}{}{x:section:sec_proj_knight}
To understand where a knight can move in a chess game, we need to know the initial setup. A chess board is an \(8 \times 8\) grid. To be able to refer to the individual positions on the board, we will place the board so that its lower left corner is at the origin, make each square in the grid have side length \(1\), and label each square with the point at the lower left corner. This is illustrated at left in \hyperref[x:figure:F_knight_2]{Figure~{\xreffont\ref{x:figure:F_knight_2}}}.%
\begin{figureptx}{Initial knight placement and moves.}{x:figure:F_knight_2}{}%
\begin{sidebyside}{2}{0}{0}{0}%
\begin{sbspanel}{0.5}%
\includegraphics[width=\linewidth]{external/knight_grid.pdf}
\end{sbspanel}%
\begin{sbspanel}{0.5}%
\includegraphics[width=\linewidth]{external/knight_move_2.pdf}
\end{sbspanel}%
\end{sidebyside}%
\tcblower
\end{figureptx}%
Each player has two knights to start the game, for one player the knights would begin in positions \((1,0)\) and \((6,0)\). Because of the symmetry of the knight's moves, we will only analyze the moves of the knight that begins at position \((1,0)\). This knight has only three allowable moves from its starting point (assuming that the board is empty), as shown at right in \hyperref[x:figure:F_knight_2]{Figure~{\xreffont\ref{x:figure:F_knight_2}}}. The questions we will ask are: given any position on the board, can the knight move from its start position to that position using only knight moves and, what sequence of moves will make that happen. To answer these questions we will use linear combinations of knight moves described as vectors.%
\par
Each knight move can be described by a vector. A move one position to the right and two up can be represented as \(\vn_1 = \left[ \begin{array}{c} 1\\2 \end{array}  \right]\). Three other moves are \(\vn_2 = \left[ \begin{array}{r} -1\\2 \end{array}  \right]\), \(\vn_3 = \left[ \begin{array}{c} 2\\1 \end{array}  \right]\), and \(\vn_4 = \left[ \begin{array}{r} -2\\1 \end{array}  \right]\). The other four knight moves are the additive inverses of these four. Any sequence of moves by the knight is given by the linear combination%
\begin{equation*}
x_1 \vn_1 + x_2 \vn_2 + x_3 \vn_3 + x_4 \vn_4\text{.}
\end{equation*}
%
\par
A word of caution: the knight can only make complete moves, so we are restricted to integer (either positive or negative) values for \(x_1\), \(x_2\), \(x_3\), and \(x_4\). You can use the GeoGebra app at \href{https://www.geogebra.org/m/dfwtskrj}{\nolinkurl{geogebra.org/m/dfwtskrj}} to see the effects the weights have on the knight moves. We should note here that since addition of vectors is commutative, the order in which we apply our moves does not matter. However, we may need to be careful with the order so that our knight does not leave the chess board.%
\begin{project}{}{x:project:act_knight_1}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why the vector equation%
\begin{equation*}
\left[ \begin{array}{c} 1\\0 \end{array}  \right] + x_1 \vn_1 + x_2 \vn_2 + x_3 \vn_3 + x_4 \vn_4 = \left[ \begin{array}{c} 5\\2 \end{array}  \right]
\end{equation*}
will tell us if it is possible for the knight to move from its initial position at \((1,0)\) to the position (5,2).%
\item{}Find all solutions, if any, to the system from part (a). If it is possible to find a sequence of moves that take the knight from its initial position to position \((5,2)\), find weights \(x_1\), \(x_2\), \(x_3\), and \(x_4\) to accomplish this move. (Be careful \textemdash{} we must have solutions in which \(x_1\), \(x_2\), \(x_3\), and \(x_4\) are integers.) Is there more than one sequence of possible moves? You can check your solution with the GeoGebra app at \href{https://www.geogebra.org/m/dfwtskrj}{\nolinkurl{geogebra.org/m/dfwtskrj}}.%
\end{enumerate}
\end{project}%
\hyperref[x:project:act_knight_1]{Project Activity~{\xreffont\ref{x:project:act_knight_1}}} shows that it is possible for our knight to move to position \((5,2)\) on the board. We would like to know if it is possible to move to any position on the board. That is, we would like to know if the integer span of the four moves \(\vn_1\), \(\vn_2\), \(\vn_3\), and \(\vn_4\) will allow our knight to cover the entire board. This takes a bit more work.%
\begin{project}{}{x:project:act_knight_2}%
Given any position \((a,b)\), we want to know if our knight can move from its start position \((1,0)\) to position \((a,b)\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Write a vector equation whose solution will tell us if it is possible for our knight to move from its start position \((1,0)\) to position \((a,b)\).%
\item{}Show that the solution to part (a) can be written in the form%
\begin{align}
x_1 \amp = \frac{1}{4}\left(-5x_3+3x_4+b+2(a-1)\right)\label{x:mrow:eq_knight_x1}\\
x_2 \amp = \frac{1}{4}\left(3x_3-5x_4+b-2(a-1)\right)\label{x:mrow:eq_knight_x2}\\
x_3 \amp \text{ is free }\notag\\
x_4 \amp \text{ is free. }\notag
\end{align}
%
\end{enumerate}
\end{project}%
To answer our question if our knight can reach any position, we now need to determine if we can always find integer values of \(x_3\) and \(x_4\) to make equations \hyperref[x:mrow:eq_knight_x1]{({\xreffont\ref{x:mrow:eq_knight_x1}})} and \hyperref[x:mrow:eq_knight_x2]{({\xreffont\ref{x:mrow:eq_knight_x2}})} have integer solutions. In other words, we need to find values of \(x_3\) and \(x_4\) so that \(-5x_3+3x_4+b+2(a-1)\) and \(3x_3-5x_4+b-2(a-1)\) are multiples of \(4\). How we do this could depend on the parity (even or odd) of \(a\) and \(b\). For example, if \(a\) is odd and \(b\) is even, say \(a = 2r+1\) and \(b = 2s\) for some integers \(r\) and \(s\), then%
\begin{align*}
x_1 \amp = \frac{1}{4}\left( -5x_3 + 3x_4 + 2s + 4r\right)\\
x_2 \amp = \frac{1}{4}\left( 3x_3 - 5x_4 + 2s - 4r \right)\text{.}
\end{align*}
%
\par
With a little trial and error we can see that if we let \(x_3 = x_4 = s\), then \(x_1 = r\) and \(x_2 = -r\) is a solution with integer weights. For example, when \(a=5\) and \(b=2\) we have \(r=2\) and \(s = 1\). This makes \(x_1 = 2\), \(x_2 = -2\), \(x_3 = 1 = x_4\). Compare this to the solution(s) you found in \hyperref[x:project:act_knight_1]{Project Activity~{\xreffont\ref{x:project:act_knight_1}}}. This analysis shows us how to move our knight to any position \((a,b)\) where \(a\) is odd and \(b\) is even.%
\begin{project}{}{x:project:act_knight_3}%
Complete the analysis as above to determine if there are integer solutions to our knight's move system in the following cases.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(a\) odd and \(b\) odd%
\item{}\(a\) even and \(b\) even%
\item{}\(a\) even and \(b\) odd.%
\end{enumerate}
\end{project}%
\hyperref[x:project:act_knight_3]{Project Activity~{\xreffont\ref{x:project:act_knight_3}}} shows that for any position on the chess board, using linear combinations of move vectors, we can find a sequence of moves that takes our knight to that position. (We actually haven't shown that these moves can be made so that our knight always stays on the board \textemdash{} we leave that question to you.)%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 5 The Matrix-Vector Form of a Linear System}
\typeout{************************************************}
%
\begin{chapterptx}{The Matrix-Vector Form of a Linear System}{}{The Matrix-Vector Form of a Linear System}{}{}{x:chapter:chap_matrix_vector}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp10036872}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}How and when is the matrix-vector product \(A \vx\) defined?%
\item{}How can a system of linear equations be written in matrix-vector form?%
\item{}How can we tell if the system \(A \vx = \vb\) is consistent for a given vector \(\vb\)?%
\item{}How can we tell if the system \(A \vx = \vb\) is consistent for every vector \(\vb\)?%
\item{}What is a homogeneous system? What can we say about the solution set to a homogeneous system?%
\item{}What must be true about pivots in the coefficient matrix \(A\) in order for the homogeneous system \(A \vx = \vzero\) to have a unique solution?%
\item{}How are the solutions to the nonhomogeneous system \(A \vx = \vb\) related to the solutions of the corresponding homogeneous system \(A \vx = \vzero\)?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Modeling an Economy}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Modeling an Economy}{}{Application: Modeling an Economy}{}{}{x:section:sec_appl_model_econ}
An economy is a very complex system. An economy is not a well-defined object, there are many factors that influence an economy, and it is often unclear how the factors influence each other. Mathematical modeling plays an important role in attempting to understand an economy.%
\par
In 1941 Wassily Leontief developed the first empirical model of a national economy. Around 1949 Leontief used data from the U.S. Bureau of Labor Statistics to divide the U.S. economy into 500 sectors. He then set up linear equations for each sector. This system was too large for the computers at the time to solve, so he then aggregated the information into 42 sectors. The Harvard Mark II computer was used to solve this system, one of the first significant uses of computers for mathematical modeling. Leontief won the 1973 Nobel Prize in economics for his work.%
\par
With such large models (Leontief's models are called \terminology{input-output} models) it is important to find a shorthand way to represent the resulting systems. In this section we will see how to represent any size system of linear equations in a very convenient way. Later, we will analyze a small economy using input-output models.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_mv_form_intro}
There is another useful way to represent a system of linear equations using a matrix-vector product that we investigate in this section. To understand how this product comes about, recall that we can represent the linear system%
\begin{alignat*}{5}
x_1     \amp {}+{}   \amp {4}x_2   \amp {}+{}  \amp {2}x_3   \amp {}+{}  \amp {4}x_4  \amp {}={}  \amp 1\\
{2}x_1   \amp {}-{}   \amp {}x_2     \amp {}-{}  \amp {5}x_3   \amp {}-{}  \amp {}x_4  \amp {}={}  \amp 2\\
{3}x_1   \amp {}+{}   \amp {7}x_2   \amp {}+{}  \amp {}x_3     \amp {}+{}  \amp {7}x_4  \amp {}={}  \amp 3
\end{alignat*}
as a vector equation as%
\begin{equation}
x_1\left[ \begin{array}{c} 1\\2\\3 \end{array}  \right] + x_2\left[ \begin{array}{r} 4\\-1\\7 \end{array}  \right] + x_3\left[ \begin{array}{r} 2\\-5\\1 \end{array}  \right] + x_4\left[ \begin{array}{r} 4\\-1\\7 \end{array}  \right]= \left[ \begin{array}{c} 1\\2\\3 \end{array}  \right]\text{.}\label{x:men:eq_PA1e_1}
\end{equation}
%
\par
We can view the left hand side of Equation \hyperref[x:men:eq_PA1e_1]{({\xreffont\ref{x:men:eq_PA1e_1}})} as a \terminology{matrix-vector product}. Specifically, if \(A = \left[ \begin{array}{crrr} 1\amp 4\amp 2\amp 4 \\ 2\amp -1\amp -5\amp -1 \\ 3\amp 7\amp 1\amp 7 \end{array} \right]\) and \(\vx = \left[ \begin{array}{c} x_1 \\x_2 \\x_3 \\x_4 \end{array} \right]\), then we define the \terminology{matrix-vector product} \(A\vx\) as the left hand side Equation \hyperref[x:men:eq_PA1e_1]{({\xreffont\ref{x:men:eq_PA1e_1}})}. So the matrix-vector product \(A\vx\) is the linear combination of the columns of \(A\) with weights from the vector \(\vx\) in order.%
\par
With this definition, the vector equation in \hyperref[x:men:eq_PA1e_1]{({\xreffont\ref{x:men:eq_PA1e_1}})} can be expressed as a matrix-vector equation as%
\begin{equation*}
\left[ \begin{array}{crrr} 1\amp 4\amp 2\amp 4 \\ 2\amp -1\amp -5\amp -1 \\ 3\amp 7\amp 1\amp 7 \end{array}  \right] \left[ \begin{array}{c} x_1 \\x_2 \\x_3\\x_4 \end{array}  \right] = \left[ \begin{array}{c} 1\\2\\3 \end{array}  \right] \,\text{.}
\end{equation*}
%
\par
We call this representation the \terminology{matrix-vector form} of the system. Note that the matrix \(A\) in this expression is the same as the coefficient matrix that appears in the augmented matrix representation of the system.%
\par
We can use the above definition of the matrix-vector product as a linear combination with any matrix and any vector, as long as it is meaningful to use the entries in the vector as weights for the columns of the matrix. For example, for \(A=\left[ \begin{array}{cc} 1\amp 2\\3\amp 1\\1\amp 1 \end{array}  \right]\) and \(\vv = \left[ \begin{array}{c} 3\\4 \end{array}  \right]\), then we can define \(A\vv\) to be the linear combination of the columns of \(A\) with weights 3 and 4:%
\begin{equation*}
A\vv= 3 \left[ \begin{array}{c} 1\\3\\1 \end{array}  \right] + 4 \left[ \begin{array}{c} 2\\1\\1 \end{array}  \right] = \left[ \begin{array}{c} 11\\13\\7 \end{array}  \right] \,\text{.}
\end{equation*}
%
\par
However, note that if \(\vv\) had three entries, this definition would not make sense since we do not have three columns in \(A\). In those cases, we say \(A\vv\) is not defined. We will later see that this definition can be generalized to matrix-matrix products, by treating the vector as a special case of a matrix with one column.%
\begin{exploration}{}{x:exploration:pa_1_e}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Write the vector equation%
\begin{equation*}
x_1 \left[ \begin{array}{c} 1 \\ 1 \\ 2 \end{array}  \right] +x_2 \left[ \begin{array}{c}  1\\ 2\\ 3 \end{array}  \right] + x_3 \left[ \begin{array}{r} -1 \\ 2 \\ -3 \end{array}  \right] = \left[ \begin{array}{c} 4 \\ 3\\ 11 \end{array}  \right]
\end{equation*}
in matrix-vector form. Note that this is the vector equation whose augmented matrix representation was given in Problem 2 in \hyperref[x:exploration:pa_1_b]{Preview Activity~{\xreffont\ref{x:exploration:pa_1_b}}}. Compare your matrix \(A\) and the right hand side vector to the augmented matrix. Do not solve the system.%
\item{}Given the matrix-vector equation%
\begin{equation*}
\left[ \begin{array}{crr} 1 \amp  2 \amp  -1 \\ 0\amp 1\amp 2\\1\amp -2\amp -3 \end{array}  \right] \vx = \left[\begin{array}{r} -3\\3\\-7 \end{array} \right]
\end{equation*}
represent the system corresponding to this equation. Note that this should correspond to the system (or an equivalent system where an equation might be multiplied by \((-1)\)) in Problem 1 of \hyperref[x:exploration:pa_1_b]{Preview Activity~{\xreffont\ref{x:exploration:pa_1_b}}}.%
\item{}Find the indicated matrix-vector products, if possible. Express as one vector.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}\(\left[ \begin{array}{cr} 2\amp -2 \\ 1\amp 2 \end{array} \right] \left[ \begin{array}{r} 1 \\ -1 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{rrr} 1 \amp 0 \amp 2\\ 2 \amp -2 \amp 3 \end{array} \right] \left[ \begin{array}{r} 2\\1 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{rrc} -6 \amp -2 \amp 1\\ 2 \amp -2 \amp 1 \end{array} \right] \left[ \begin{array}{r} 1\\-3 \\1 \end{array} \right]\)%
\end{enumerate}
\item{}As you might have noticed, systems with all the constants being 0 are special in that they always have a solution. (Why?) So we might consider grouping systems into two types: Those of the form \(A \vx = \vb\), where not all of the entries of the vector \(\vb\) are \(0\), and those of the form \(A \vx = \vzero\), where \(\vzero\) is the vector of all zeros. Systems like \(A \vx = \vb\), where \(\vb\) contains at least one non-zero entry, are called \terminology{nonhomogeneous} systems, and systems of the form \(A \vx = \vzero\) are called \terminology{homogeneous} systems. For every nonhomogeneous system \(A \vx = \vb\) there is a corresponding homogeneous system \(A \vx = \vzero\), and there is a useful connection between the solutions to the nonhomogeneous system and the corresponding homogeneous system. For example, consider the nonhomogeneous system%
\begin{equation*}
A \vx = \vb
\end{equation*}
with%
\begin{equation}
A = \left[ \begin{array}{ccc} 1 \amp  1 \amp  2  \\ 1 \amp  2 \amp  1 \end{array}  \right], \ \vx = \left[ \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array}  \right], \ \text{ and }  \ \vb =  \left[ \begin{array}{r} 0 \\ -2 \end{array}  \right]\text{.}\label{x:men:eq_PA1e_2}
\end{equation}
The augmented matrix representation of this system is \([A \ | \ \vb]\). If we reduce this augmented matrix, we find%
\begin{equation*}
\left[ \begin{array}{ccr|r} 1 \amp  0 \amp  3 \amp  2 \\ 0 \amp  1 \amp  -1 \amp  -2 \end{array}  \right]\text{.}
\end{equation*}
From this RREF, we immediately see that the general solution is that \(x_3\) is free, \(x_2 = x_3-2\), and \(x_1 = 2-3x_3\). In vector form, we can represent this general solution as%
\begin{equation}
\left[ \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array}  \right] = \left[ \begin{array}{c} 2-3x_3 \\ x_3-2 \\ x_3 \end{array}  \right] = \left[ \begin{array}{r} 2 \\ -2 \\ 0 \end{array}  \right] + x_3\left[ \begin{array}{r} -3 \\ 1 \\ 1 \end{array}  \right]\text{.}\label{x:men:eq_PA1e_3}
\end{equation}
The rightmost expression above is called the \terminology{parametric vector form} of the solution. If we had a system where the general solution involved more than one free variable, then we would write the parametric vector form to include one vector multiplying each free variable. For example, if the general solution of a system were that \(x_2\) and \(x_3\) are free and \(x_1=2+x_2+3x_3\), then the parametric vector form would be%
\begin{equation*}
\vx = \left[ \begin{array}{c} 2+ x_2+3x_3 \\ x_2 \\ x_3 \end{array}  \right] = \left[ \begin{array}{c} 2 \\ 0 \\ 0 \end{array}  \right] + x_2 \left[ \begin{array}{c} 1 \\ 1 \\ 0 \end{array}  \right] + x_3 \left[ \begin{array}{c} 3 \\ 0 \\ 1 \end{array}  \right] \,\text{.}
\end{equation*}
Note that the parametric vector form expresses the solutions as a linear combination of a number of vectors, depending on the number of free variables, with an added constant vector. This expression helps us in interpreting the solution set geometrically, as we will see in this section.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Find the general solution to the homogeneous system%
\begin{equation*}
A \vx = \vzero
\end{equation*}
with \(A\) and \(\vx\) as in \hyperref[x:men:eq_PA1e_2]{({\xreffont\ref{x:men:eq_PA1e_2}})} and compare it to the solution to the nonhomogeneous system in \hyperref[x:men:eq_PA1e_3]{({\xreffont\ref{x:men:eq_PA1e_3}})}. What do you notice?%
\item{}Find the general solution to the nonhomogeneous system%
\begin{equation*}
A \vx = \vb
\end{equation*}
with%
\begin{equation*}
A = \left[ \begin{array}{ccr} 1\amp 2\amp -1 \\ 2\amp 4\amp -2 \end{array}  \right], \ \vx =  \left[ \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array}  \right], \ \text{ and }  \ \vb =  \left[ \begin{array}{r} -1 \\ 1 \end{array}  \right]\text{.}
\end{equation*}
and express it in parametric vector form. Then find the general solution to the corresponding homogeneous system and express it in parametric vector form. How are the two solution sets related?%
\item{}Make a conjecture about the relationship between the solutions to a consistent nonhomogeneous system \(A \vx = \vb\) and the corresponding homogeneous system \(A \vx = \vzero\). Be as specific as possible.%
\end{enumerate}
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Matrix-Vector Product}
\typeout{************************************************}
%
\begin{sectionptx}{The Matrix-Vector Product}{}{The Matrix-Vector Product}{}{}{x:section:sec_mv_prod}
The matrix-vector product we defined in \hyperref[x:exploration:pa_1_e]{Preview Activity~{\xreffont\ref{x:exploration:pa_1_e}}} for a specific example generalizes in a very straightforward manner, and provides a convenient way to represent a system of linear equations of any size using matrices and vectors. In addition to providing us with an algebraic approach to solving systems via matrices and vectors \textemdash{} leading to a powerful geometric relationship between solution sets of homogeneous and non-homogeneous systems \textemdash{} this representation allows us to think of a linear system from a dynamic perspective, as we will see later in the section on matrix transformations.%
\par
\index{matrix!rows}\index{matrix!columns}\index{matrix!size} The matrix-vector product \(A \vx\) is a linear combination of the columns of \(A\) with weights from \(\vx\). To define this product in general, we will need a little notation. Recall that a matrix is made of rows and columns \textemdash{} the entries reading from left to right form the \terminology{rows} of the matrix and the entries reading from top to bottom form the \terminology{columns}. For example, the matrix%
\begin{equation*}
A = \left[ \begin{array}{cccc} 1 \amp  2 \amp  3 \amp  4 \\ 5 \amp  6 \amp  7 \amp  8  \\ 9 \amp  10 \amp  11 \amp  12 \end{array}  \right]\text{.}
\end{equation*}
has three rows and four columns. The number of rows and columns of a matrix is called the \terminology{size} of the matrix, so \(A\) is a 3 by 4 matrix (also written as \(3 \times 4\)). We often need to have a way to reference the individual entries of a matrix \(A\), and to do so we typically give a label, say \(a_{ij}\) to the entry in the \(i\)th row and \(j\)th column of \(A\). So in our example we have \(a_{23}=7\). We also write \(A = [a_{ij}]\) to indicate a matrix whose \(i,j\)th entry is \(a_{ij}\). At times it is convenient to write a matrix in terms of its rows or columns. If \(A = [a_{ij}]\) is an \(m \times n\) matrix, then we will write%
\begin{equation*}
A = \left[ \begin{array}{ccccc} a_{11} \amp  a_{12}  \amp  \cdots    \amp  a_{1n-1} \amp  a_{1n} \\ a_{21} \amp  a_{22}  \amp  \cdots    \amp  a_{2n-1} \amp  a_{2n} \\ \vdots \amp          \amp  \ddots    \amp      \amp \vdots \\ a_{m1} \amp  a_{m2}  \amp  \cdots    \amp  a_{mn-1} \amp  a_{mn} \end{array}  \right]
\end{equation*}
or, if we let \(\vr_1, \vr_2, \ldots, \vr_m\) denote the rows of the matrix \(A\), then we can write \(A\) as \footnote{Technically, the rows of \(A\) are made from the entries of the row vectors, but we use this notation as a shorthand.\label{g:fn:idp10102280}}%
\begin{equation*}
A = \left[ \begin{array}{c} \vr_1 \\ \vr_2 \\ \vdots \\ \vr_m \end{array}  \right]\text{.}
\end{equation*}
%
\par
We can also write \(A\) in terms of its columns, \(\vc_1, \vc_2, \ldots, \vc_n\), as%
\begin{equation*}
A = [\vc_1 \ \vc_2 \ \cdots \ \vc_n]\text{.}
\end{equation*}
%
\par
In general, the product of a matrix with a vector is defined as follows.%
\begin{definition}{}{g:definition:idp10105864}%
\index{matrix-vector product}%
Let \(A\) be an \(m \times n\) matrix with columns \(\vc_1\), \(\vc_2\), \(\ldots\), \(\vc_n\), and let \(\vx = \left[ \begin{array}{c} x_1 \\ x_2 \\ \vdots \\ x_n \end{array}  \right]\) be a vector in \(\R^n\). The \terminology{matrix-vector product} \(A\vx\) is%
\begin{equation*}
A \vx = x_1 \vc_1 + x_2 \vc_2 + \cdots + x_n \vc_n\text{.}
\end{equation*}
%
\end{definition}
\begin{paragraphs}{Important Note.}{g:paragraphs:idp10106376}%
The matrix-vector product \(A \vx\) is defined only when the number of entries of the vector \(\vx\) is equal to the number of columns of the matrix \(A\). That is, if \(A\) is an \(m \times n\) matrix, then \(A\vx\) is defined only if \(\vx\) is a column vector with \(n\) entries.%
\end{paragraphs}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Matrix-Vector Form of a Linear System}
\typeout{************************************************}
%
\begin{sectionptx}{The Matrix-Vector Form of a Linear System}{}{The Matrix-Vector Form of a Linear System}{}{}{x:section:sec_mv_form}
\begin{introduction}{}%
As we saw in \hyperref[x:exploration:pa_1_e]{Preview Activity~{\xreffont\ref{x:exploration:pa_1_e}}}, the matrix-vector product provides us with a short hand way of representing a system of linear equations. In general, every linear system can be written in matrix-vector form as follows.%
\par
The linear system%
\begin{alignat*}{5}
{a_{11}}x_1   \amp {}+{}   \amp {a_{12}}x_2   \amp {}+{}  \amp \cdots       \amp {}+{}  \amp {a_{1n}}x_n    \amp {}={}  \amp b_1\\
{a_{21}}x_1   \amp {}+{}   \amp {a_{22}}x_2    \amp {}+{}  \amp \cdots      \amp {}+{}  \amp {a_{2n}}x_n    \amp {}={}  \amp b_2\\
{}         \amp {}     \amp {}          \amp {}    \amp \vdots \ \    \amp {}    \amp {}          \amp {}    \amp {}\\
{a_{m1}}x_1   \amp {}+{}   \amp {a_{2m}}x_2  \amp {}+{}  \amp \cdots      \amp {}+{}  \amp {a_{mn}}x_n  \amp {}={}  \amp b_m
\end{alignat*}
of \(m\) equations in \(n\) unknowns can be written in matrix-vector form as \(A\vx = \vb\), where%
\begin{equation*}
A = \left[ \begin{array}{cccc} a_{11} \amp  a_{12} \amp  \cdots \amp  a_{1n} \\ a_{21} \amp  a_{22} \amp  \cdots \amp  a_{2n} \\ \vdots \amp         \amp  \ddots    \amp  \vdots \\ a_{m1} \amp  a_{m2} \amp  \cdots \amp  a_{mn} \end{array}  \right],  \ \ \ \vx = \left[ \begin{array}{c} x_1 \\ x_2 \\ \vdots \\ x_n \end{array}  \right],  \ \ \ \text{ and }  \ \ \ \vb = \left[ \begin{array}{c} b_1 \\ b_2 \\ \vdots \\ b_m \end{array}  \right]\text{.}
\end{equation*}
%
\par
This general system can also be written in the vector form%
\begin{equation*}
x_1  \left[ \begin{array}{c} a_{11}  \\ a_{21} \\ \vdots \\ a_{m1} \end{array}  \right] + x_2 \left[ \begin{array}{c} a_{12}  \\ a_{22} \\ \vdots \\ a_{m2} \end{array}  \right]  + \cdots + x_n \left[ \begin{array}{c} a_{1n}  \\ a_{2n}  \\ \vdots \\ a_{mn} \end{array}  \right] = \left[ \begin{array}{c} b_1 \\ b_2 \\ \vdots \\ b_m \end{array}  \right]\text{.}
\end{equation*}
%
\par
With this last representation, we now have four different ways to represent a system of linear equations (as a system of linear equations, as an augmented matrix, in vector equation form, and in matrix-vector equation form), and it is important to be able to translate between them. As an example, the system%
\begin{alignat*}{5}
x_1     \amp {}+{}   \amp {4}x_2   \amp {}+{}  \amp {2}x_3   \amp {}+{}  \amp {4}x_4  \amp {}={}  \amp 1\\
{2}x_1   \amp {}-{}   \amp {}x_2     \amp {}-{}  \amp {5}x_3   \amp {}-{}  \amp {}x_4  \amp {}={}  \amp 2\\
{3}x_1   \amp {}+{}   \amp {7}x_2   \amp {}+{}  \amp {}x_3     \amp {}+{}  \amp {7}x_4  \amp {}={}  \amp 3
\end{alignat*}
from the introduction to this section has corresponding augmented matrix%
\begin{equation*}
\left[ \begin{array}{crrr|c} 1\amp 4\amp 2\amp 4\amp 1 \\ 2\amp -1\amp -5\amp -1\amp 2 \\ 3\amp 7\amp 1\amp 7\amp 3 \end{array}  \right]\text{,}
\end{equation*}
is expressed in vector form as%
\begin{equation*}
x_1\left[ \begin{array}{c} 1\\2\\3 \end{array}  \right] + x_2\left[ \begin{array}{r} 4\\-1\\7 \end{array}  \right] + x_3\left[ \begin{array}{r} 2\\-5\\1 \end{array}  \right] + x_4\left[ \begin{array}{r} 4\\-1\\7 \end{array}  \right]= \left[ \begin{array}{c} 1\\2\\3 \end{array}  \right]\text{,}
\end{equation*}
and has matrix-vector form%
\begin{equation*}
\left[ \begin{array}{crrr} 1\amp 4\amp 2\amp 4 \\ 2\amp -1\amp -5\amp -1 \\ 3\amp 7\amp 1\amp 7 \end{array}  \right] \left[ \begin{array}{c} x_1 \\x_2 \\ x_3 \\ x_4 \end{array}  \right] = \left[ \begin{array}{c} 1 \\ 2 \\ 3 \end{array}  \right]\text{.}
\end{equation*}
%
\begin{activity}{}{g:activity:idp10130568}%
In this activity, we will use the equivalence of the different representations of a system to make useful observations about when a system represented as \(A\vx=\vb\) has a solution.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Consider the system%
\begin{equation*}
\left[ \begin{array}{ccr} 1\amp 2\amp -1 \\ 2\amp 1\amp 3 \end{array}  \right] \left[ \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array}  \right] = \left[ \begin{array}{c} 2 \\ 6 \end{array}  \right]\text{.}
\end{equation*}
Write the matrix-vector product on the left side of this equation as a linear combination of the columns of the coefficient matrix. Find weights that make the vector \(\left[ \begin{array}{c} 2 \\ 6 \end{array}  \right]\) a linear combination of the columns of the coefficient matrix.%
\item{}From this point on we consider the general case where \(A\) is an \(m \times n\) matrix. Use the vector equation representation to explain why the system \(A \vx = \vb\) has a solution if and only if \(\vb\) is a linear combination of the columns of \(A\). (Note that `if and only if' is an expression to mean that if one side of the expression is true, then the other side must also be true.) (Hint: Compare to what you did in part (a).)%
\item{}Use part (b) and the definition of span to explain why the system \(A \vx = \vb\) has a solution if and only if the vector \(\vb\) is in the span of the columns of \(A\).%
\item{}Use part (c) to explain why the system \(A \vx = \vb\) always has a solution for any vector \(\vb\) in \(\R^m\) if and only if the span of the columns of \(A\) is all of \(\R^m\).%
\item{}Use the augmented matrix representation and the criterion for a consistent system to explain why the system \(A \vx = \vb\) is consistent for all vectors \(\vb\) if and only if \(A\) has a pivot position in every row.%
\end{enumerate}
\end{activity}%
We summarize our observations from the above activity in the following theorem. \begin{theorem}{}{}{x:theorem:thm_nm_mtx}%
Let \(A\) be an \(m \times n\) matrix. The following statements are equivalent:%
\begin{enumerate}
\item{}The matrix equation \(A \vx = \vb\) has a solution for every vector \(\vb\) in \(\R^m\).%
\item{}Every vector \(\vb\) in \(\R^m\) can be written as a linear combination of the columns of \(A\).%
\item{}The span of the columns of \(A\) is \(\R^m\).%
\item{}The matrix \(A\) has a pivot position in each row.%
\end{enumerate}
%
\end{theorem}
 In the future, if we need to determine whether a system has a solution for every \(\vb\), we can refer to this theorem without having to argue our reasoning from scratch.%
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Properties of the Matrix-Vector Product}
\typeout{************************************************}
%
\begin{subsectionptx}{Properties of the Matrix-Vector Product}{}{Properties of the Matrix-Vector Product}{}{}{g:subsection:idp10139784}
As we have done before, we have a new operation (the matrix-vector product), so we should wonder what properties it has.%
\begin{activity}{}{x:activity:act_A1_4_8}%
In this activity, we consider whether the matrix-vector product distributes vector addition. In other words: Is \(A(\vu + \vv)\) equal to \(A\vu + A\vv\)?%
\par
We work with arbitrary vectors \(\vu, \vv\) in \(\R^3\) and an arbitrary matrix \(A\) with 3 columns (so that \(A\vu\) and \(A\vv\) are defined) to simplify notation. Let \(A = [\vc_1 \ \vc_2 \ \vc_3]\) (note that each \(\vc_i\) represents a column of \(A\)), \(\vu = \left[ \begin{array}{c} u_1 \\ u_2 \\ u_3 \end{array}  \right]\), and \(\vv = \left[ \begin{array}{c} v_1 \\ v_2 \\ v_3 \end{array}  \right]\). Use the definition of the matrix-vector product along with the properties of vector operations to show that%
\begin{equation*}
A(\vu + \vv)= A\vu + A\vv\text{.}
\end{equation*}
%
\end{activity}%
Similar arguments using the definition of matrix-vector product along with the properties of vector operations can be used to show the following theorem:%
\begin{theorem}{}{}{x:theorem:thm_IMT_1_e}%
Let \(A\) be an \(m \times n\) matrix, \(\vu\) and \(\vv\) \(n \times 1\) vectors, and \(c\) a scalar. Then%
\begin{enumerate}
\item{}\(\displaystyle A(\vu + \vv) = A\vu + A\vv\)%
\item{}\(\displaystyle c(A\vv) = A(c\vv)\)%
\end{enumerate}
%
\end{theorem}
\end{subsectionptx}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Homogeneous and Nonhomogeneous Systems}
\typeout{************************************************}
%
\begin{sectionptx}{Homogeneous and Nonhomogeneous Systems}{}{Homogeneous and Nonhomogeneous Systems}{}{}{x:section:sec_homog_sys}
As we saw before, the systems with all the right hand side constants being 0 are special in that they always have a solution. (Why?) So we might consider grouping systems into two types: Those of the form \(A \vx = \vb\), where not all of the entries of the vector \(\vb\) are \(0\), and those of the form \(A \vx = \vzero\), where \(\vzero\) is the vector of all zeros. Systems like \(A \vx = \vb\), where \(\vb\) contains at least one non-zero entry, are called \terminology{nonhomogeneous} systems, and systems of the form \(A \vx = \vzero\) are called \terminology{homogeneous} systems. For every nonhomogeneous system \(A \vx = \vb\) there is a corresponding homogeneous system \(A \vx = \vzero\). We now investigate the connection between the solutions to the nonhomogeneous system and the corresponding homogeneous system.%
\begin{activity}{}{g:activity:idp10259080}%
In this activity we will consider the relationship between the solution sets of nonhomogeneous systems and those of the corresponding homogeneous systems.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the solution sets of the system%
\begin{equation*}
A \vx = \vb
\end{equation*}
where%
\begin{equation*}
A = \left[ \begin{array}{ccc} 1 \amp  1 \amp  2  \\ 1 \amp  2 \amp  1 \end{array}  \right], \ \vx = \left[ \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array}  \right], \ \text{ and }  \ \vb =  \left[ \begin{array}{r} 0 \\ -2 \end{array}  \right]
\end{equation*}
and the corresponding homogeneous system (i.e. where we replace \(\vb\) with \(\vzero\).)%
\item{}Find the solution sets of the system%
\begin{equation*}
A \vx = \vb
\end{equation*}
where%
\begin{equation*}
A = \left[ \begin{array}{ccr} 1\amp 2\amp -1 \\ 2\amp 4\amp -2 \end{array}  \right], \ \vx =  \left[ \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array}  \right], \ \text{ and }  \ \vb =  \left[ \begin{array}{r} -1 \\ 1 \end{array}  \right]
\end{equation*}
and the corresponding homogeneous system.%
\item{}What are the similarities\slash{}differences between solutions of the nonhomogeneous system and its homogeneous counterpart?%
\end{enumerate}
\end{activity}%
As we saw in the above activity, there is a relationship between solutions of a nonhomogeneous and the corresponding homogeneous system. Let us formalize this relationship. If the general solution of a system involves free variables, we can represent the solutions in \terminology{parametric vector form} to have a better idea about the geometric representation of the solution set. Suppose the solution is that \(x_3\) is free, \(x_2 = -2+x_3\), and \(x_1 = 2-3x_3\). In vector form, we can represent this general solution as%
\begin{equation}
\left[ \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array}  \right] = \left[ \begin{array}{c} 2-3x_3 \\ x_3-2 \\ x_3 \end{array}  \right] = \left[ \begin{array}{r} 2 \\ -2 \\ 0 \end{array}  \right] + x_3\left[ \begin{array}{r} -3 \\ 1 \\ 1 \end{array}  \right]\text{.}\label{g:men:idp10268424}
\end{equation}
%
\par
From this representation, we see that the solution set is a line through the origin (formed by multiples of \(\left[ \begin{array}{r} -3 \\ 1 \\ 1 \end{array} \right]\)) shifted by the added vector \(\left[ \begin{array}{r} 2 \\ -2 \\ 0 \end{array} \right]\). The solution to the homogeneous system on the other does not have the shift.%
\par
Algebraically, we see that every solution to the nonhomogeneous system \(A\vx = \vb\) can be written in the form \(\vp + \vv_h\), where \(\vp\) is a particular solution to \(A\vx = \vb\) and \(\vv_h\) is a solution to the corresponding homogeneous system \(A\vx = \vzero\).%
\par
To understand why this \emph{always} happens, we will verify the result algebraically for an arbitrary \(A\) and \(\vb\). Assuming that \(\vp\) is a particular solution to the nonhomogeneous system \(A\vx=\vb\), we need to show that:%
\begin{itemize}[label=\textbullet]
\item{}if \(\vv\) is an arbitrary solution to the nonhomogeneous system, then \(\vv = \vp + \vv_h\), where \(\vv_h\) is some solution to the homogeneous system \(A\vx = \vzero\), and%
\item{}if \(\vv_h\) is an arbitrary solution to the homogeneous system, then \(\vp + \vv_h\) is a solution to the nonhomogeneous system.%
\end{itemize}
%
\par
To verify the first condition, suppose that \(\vv\) is a solution to the nonhomogeneous system \(A \vx = \vb\). Since we want \(\vv=\vp+\vv_h\), we need to verify that \(\vv-\vp\) is a solution for the homogeneous system so that we can assign \(\vv_h=\vv-\vp\). Note that%
\begin{equation*}
A(\vv - \vp) = A\vv - A\vp = \vb - \vb = \vzero \,\text{,}
\end{equation*}
using the distributive property of matrix-vector product over vector addition. Hence \(\vv\) is of the form \(\vp + \vv_h\) with \(\vv_h = \vzero\).%
\par
To verify the second condition, consider a vector of the form \(\vp + \vv_h\), where \(\vv_h\) is a homogeneous solution. We have%
\begin{equation*}
A(\vp + \vv_h) = A\vp + A\vv_h = \vb + \vzero = \vb\text{,}
\end{equation*}
and so \(\vp + \vv_h\) is a solution to \(A \vx = \vb\).%
\par
Our work above proves the following theorem.%
\begin{theorem}{}{}{x:theorem:thm_1_e_1}%
Suppose the equation \(A\vx=\vb\) is consistent for some \(\vb\) and \(\vp\) is a solution. Then the solution set of \(A\vx=\vb\) consists of all vectors of the form \(\vv=\vp+\vv_h\) where \(\vv_h\) is a solution to \(A\vx=\vzero\).%
\end{theorem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Geometry of Solutions to the Homogeneous System}
\typeout{************************************************}
%
\begin{sectionptx}{The Geometry of Solutions to the Homogeneous System}{}{The Geometry of Solutions to the Homogeneous System}{}{}{x:section:sec_geom_homog_sys}
There is a simple geometric interpretation to the solution set of the homogeneous system \(A \vx = \vzero\) based on the number of free variables that imposes a geometry on the solution set of the corresponding nonhomogeneous system \(A \vx = \vb\) (when consistent) due to \hyperref[x:theorem:thm_1_e_1]{Theorem~{\xreffont\ref{x:theorem:thm_1_e_1}}}.%
\begin{activity}{}{x:activity:ex_Homogeneous2}%
In this activity we consider geometric interpretations of the solution sets of homogeneous and nonhomogeneous systems.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Consider the system \(A \vx = \vb\) where \(A = \left[ \begin{array}{rr} 1 \amp -3 \\ -3 \amp 9 \\ -1 \amp 3 \end{array} \right]\) and \(\vb = \left[ \begin{array}{r} 2 \\ -6 \\ -2 \end{array} \right]\). The general solution to this system has the form \(\left[ \begin{array}{c} 2 \\ 0 \end{array} \right] + x_2\left[ \begin{array}{c} 3 \\ 1 \end{array} \right]\), where \(x_2\) is any real number.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Let \(\vv = \left[ \begin{array}{c} 3 \\ 1 \end{array} \right]\). What does the set of all vectors of the form \(x_2\vv\) look like geometrically? Draw a picture in \(\R^2\) to illustrate. (Recall that we refer to all the vectors of the form \(x_2\vv\) simply as \(\Span \{\vv\}\).)%
\item{}Let \(\vp = \left[ \begin{array}{r} 2 \\ 0 \end{array} \right]\). What effect does adding the vector \(\vp\) to each vector in \(\Span \{\vv\}\) have on the geometry of \(\Span \{\vv\}\)? Finally, what does this mean about the geometry of the solution set to the nonhomogeneous system \(A \vx = \vb\)?%
\end{enumerate}
\item{}Consider the system \(A \vx = \vb\) where \(A =\left[ \begin{array}{ccr} 1\amp 2\amp -1 \\ 3\amp 6\amp -3 \end{array} \right]\) and \(\vb = \left[ \begin{array}{r} -2 \\ -6 \end{array} \right]\). The general solution to this system has the form \(\left[ \begin{array}{r} -2 \\ 0 \\0 \end{array} \right] + x_2\left[ \begin{array}{r} -2 \\ 1 \\0 \end{array} \right] +x_3\left[ \begin{array}{c} 1 \\ 0 \\1 \end{array} \right]\), where \(x_2, x_3\) are any real numbers.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Let \(\vu = \left[ \begin{array}{r} -2 \\ 1 \\0 \end{array} \right], \vv= \left[ \begin{array}{c} 1 \\ 0 \\1 \end{array} \right]\). Use our results from Section 4 to determine the geometric shape of \(\Span \{\vu, \vv\}\), the set of all vectors of the form \(x_2\left[ \begin{array}{r} -2 \\ 1 \\0 \end{array} \right] +x_3\left[ \begin{array}{c} 1 \\ 0 \\1 \end{array} \right]\), where \(x_2, x_3\) are any real numbers.%
\item{}Let \(\vp = \left[ \begin{array}{r} -2 \\ 0 \\0 \end{array} \right]\). What's the geometric effect of adding the vector \(\vp\) to each vector in \(\Span \{\vu, \vv\}\)? Finally, what does this mean about the geometry of the solution set to the nonhomogeneous system \(A \vx = \vb\)?%
\end{enumerate}
\end{enumerate}
\end{activity}%
Our work in the above activity shows the geometric shape of the solution set of a consistent nonhomogeneous system is the same as the geometric shape of the solution set of the corresponding homogeneous system. The only difference between the two solution sets is that one is a shifted version of the other.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_mv_form_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp10302344}%
We now have several different ways to represent a system of linear equations. Rewrite the system in an equivalent form%
\begin{alignat*}{6}
{11}x_1  \amp {}+{}   \amp {4}x_2   \amp {}-{}  \amp {5}x_3   \amp {}-{}  \amp {2}x_4  \amp = \amp {} \amp 63\amp {}\\
{15}x_1   \amp {}+{}   \amp {5}x_2   \amp {}+{}  \amp {2}x_3   \amp {}-{}  \amp {2}x_4  \amp = \amp {} \amp 68\amp {}\\
x_1   \amp {}+{}   \amp {2}x_2   \amp {}+{}  \amp {}x_3   \amp {}-{}  \amp {}x_4  \amp = \amp {} \amp 26\amp {}\\
{9}x_1   \amp {}+{}   \amp {3}x_2   \amp {}+{}  \amp {2}x_3   \amp {}-{}  \amp {}x_4  \amp = \amp {} \amp 40\amp {.}
\end{alignat*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}as an augmented matrix%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp10304136}{}\quad{}The augmented matrix for this system is%
\begin{equation*}
\left[ \begin{array}{ccrr|c} 11\amp 4\amp -5\amp -2\amp 63 \\ 15\amp 5\amp 2\amp -2\amp 68 \\ 6\amp 2\amp 1\amp -1\amp 26 \\ 9\amp 3\amp 2\amp -1\amp 40 \end{array}  \right]\text{.}
\end{equation*}
%
\item{}as an equation involving a linear combination of vectors%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp10310152}{}\quad{}If we make vectors from the columns of the augmented matrix, we can write this system in vector form as%
\begin{equation*}
x_1 \left[ \begin{array}{c} 11 \\ 15 \\ 6 \\ 9 \end{array}  \right] + x_2 \left[ \begin{array}{c} 4 \\ 5 \\ 2 \\ 3 \end{array}  \right] + x_3 \left[ \begin{array}{r} -5 \\ 2\\ 1 \\ 2 \end{array}  \right] + x_4 \left[ \begin{array}{r} -2\\ -2 \\ -1 \\ -1 \end{array}  \right] = \left[ \begin{array}{c} 63 \\ 68 \\ 26 \\ 40 \end{array}  \right]\text{.}
\end{equation*}
%
\item{}using a matrix-vector product%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp10304264}{}\quad{}The coefficient matrix for this system is \(\left[ \begin{array}{ccrr} 11\amp 4\amp -5\amp -2 \\ 15\amp 5\amp 2\amp -2 \\ 6\amp 2\amp 1\amp -1 \\ 9\amp 3\amp 2\amp -1 \end{array}  \right]\), and the matrix-vector form of the system is%
\begin{equation*}
\left[ \begin{array}{ccrr} 11\amp 4\amp -5\amp -2 \\ 15\amp 5\amp 2\amp -2 \\ 6\amp 2\amp 1\amp -1 \\ 9\amp 3\amp 2\amp -1 \end{array}  \right] \left[ \begin{array}{c} x_1\\x_2\\x_3\\x_4 \end{array}  \right] = \left[ \begin{array}{c} 63 \\ 68 \\ 26 \\ 40 \end{array}  \right]\text{.}
\end{equation*}
%
\item{}Then solve the system.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp10306312}{}\quad{}Using technology, we find that the reduced row echelon form of the augmented matrix for this system is%
\begin{equation*}
\left[ \begin{array}{cccc|r} 1\amp 0\amp 0\amp 0\amp 3 \\ 0\amp 1\amp 0\amp 0\amp 7 \\ 0\amp 0\amp 1\amp 0\amp -2  \\ 0\amp 0\amp 0\amp 1\amp 4 \end{array}  \right]\text{.}
\end{equation*}
%
\par
So the solution to this system is \(x_1 = 3\), \(x_2 = 7\), \(x_3 = -2\), and \(x_4 = 4\).%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp10313736}%
Consider the homogeneous system%
\begin{alignat*}{5}
{}x_1    \amp {}+{}   \amp {8}x_2   \amp {}-{}  \amp {}x_3     \amp = \amp {} \amp 0\amp {}\\
{}x_1   \amp {}-{}   \amp {7}x_2   \amp {}+{}  \amp {2}x_3     \amp = \amp {} \amp 0\amp {}\\
{3}x_1   \amp {}+{}   \amp {4}x_2   \amp {}+{}  \amp {}x_3     \amp = \amp {} \amp 0\amp {.}
\end{alignat*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the general solution to this homogeneous system and express the system in parametric vector form.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp10312328}{}\quad{}The augmented matrix of the homogeneous system is%
\begin{equation*}
\left[ \begin{array}{crr|c} 1\amp 8\amp -1\amp 0 \\ 1\amp -7\amp 2\amp 0 \\ 3\amp 4\amp 1\amp 0 \end{array}  \right]\text{,}
\end{equation*}
and the reduced row echelon form of this augmented matrix is%
\begin{equation*}
\left[  \begin{array}{ccr|c} 1\amp 0\amp \frac{3}{5}\amp 0 \\ \amp 1\amp -\frac{1}{5}\amp 0 \\ 0\amp 0\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
Since there is no corresponding equation of the form \(0 = b\) for a nonzero constant \(b\), this system is consistent. The third column contains no pivot, so the variable \(x_3\) is free, \(x_2 = \frac{1}{5}x_3\) and \(x_1 =  -\frac{3}{5}x_3\). In parametric vector form the general solution to the homogeneous system is%
\begin{equation*}
\left[\begin{array}{c} x_1\\x_2\\x_3 \end{array}  \right] = \left[ \begin{array}{c} -\frac{3}{5}x_3\\ \frac{1}{5}x_3\\x_3 \end{array}  \right]  = x_3 \left[ \begin{array}{r} -\frac{3}{5}\\ \frac{1}{5}\\1 \end{array}  \right]\text{.}
\end{equation*}
%
\item{}Let \(A = \left[ \begin{array}{crr} 1\amp 8\amp -1 \\ 1\amp -7\amp 2 \\ 3\amp 4\amp 1 \end{array} \right]\), and let \(\vb = \left[ \begin{array}{r} -6\\9\\2 \end{array} \right]\). Show that \(\left[ \begin{array}{r} -1\\0\\5 \end{array} \right]\) is a solution to the non-homogeneous system \(A \vx = \vb\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp10323976}{}\quad{}Since%
\begin{align*}
A \left[ \begin{array}{r} -1\\
0\\
5 \end{array} \right] \amp = (-1)\left[ \begin{array}{c} 1\\
1\\
3 \end{array} \right] + (0)\left[ \begin{array}{r} 8\\
-7\\
4 \end{array} \right] + (5) \left[ \begin{array}{r} -1\\
2\\
1 \end{array} \right]\\
\amp = \left[ \begin{array}{c} -1-5\\
-1+10\\
-3+5 \end{array} \right] = \left[ \begin{array}{r} -6\\
9\\
2 \end{array} \right]\text{,}
\end{align*}
we conclude that \(\left[ \begin{array}{r} -1\\0\\5 \end{array}  \right]\) is a solution to the non-homogeneous system \(A \vx = \vb\).%
\item{}Use the results from part (a) and (b) to write the parametric vector form of the general solution to the non-homogeneous system \(A \vx = \vb\). (Do this without directly solving the system \(A \vx = \vb\).)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp10326152}{}\quad{}We know that every solution to the non-homogeneous system \(A \vx = \vb\) has the form of the general solution to the homogeneous system plus a particular solution to the non-homogeneous system. Combining the results of (a) and (b) we see that the general solution to the non-homogeneous system \(A \vx = \vb\) is%
\begin{equation*}
\left[\begin{array}{c} x_1\\x_2\\x_3 \end{array}  \right] = \left[\begin{array}{r} -1\\0\\5 \end{array}  \right]  +  x_3 \left[ \begin{array}{r} -\frac{3}{5}\\ \frac{1}{5}\\1 \end{array}  \right]\text{,}
\end{equation*}
where \(x_3\) can be any real number.%
\item{}Describe what the general solution to the homogeneous system \(A \vx = \vzero\) and the general solution to the non-homogeneous system \(A \vx = \vb\) look like geometrically.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp10334728}{}\quad{}The solution to the homogeneous system \(A \vx = \vzero\) is the span of the vector \(\left[ \begin{array}{r} -\frac{3}{5}\\ \frac{1}{5}\\1 \end{array} \right]\). Geometrically, this set of points is a line through the origin and the point \((-3, 1, 5)\) in \(\R^3\). The solution to the non-homogeneous system \(A \vx = \vb\) is the translation of the line through the origin and \((-3, 1, 5)\) by the vector \(\left[ \begin{array}{r} -1\\0\\5 \end{array} \right]\). In other words, the solution to the non-homogeneous system \(A \vx = \vb\) is the line in \(\R^3\) through the points \((-1,0,5)\) and \((-4,1,10)\).%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_mv_form_summ}
%
\begin{itemize}[label=\textbullet]
\item{}If \(A = [\vc_1 \ \vc_2 \ \cdots \ \vc_n]\) is an \(m \times n\) matrix with columns \(\vc_1\), \(\vc_2\), \(\ldots\), \(\vc_n\), and if \(\vx = \left[ \begin{array}{c} x_1\\x_2\\ \vdots \\ x_n \end{array}  \right]\) is a vector in \(\R^n\), then the matrix-vector product \(A \vx\) is defined to be the linear combination of the columns of \(A\) with corresponding weights from \(\vx\) \textemdash{} that is%
\begin{equation*}
A \vx = x_1 \vc_1 + x_2 \vc_2 + \cdots + x_n \vc_n\text{.}
\end{equation*}
%
\item{}A linear system%
\begin{alignat*}{5}
{a_{11}}x_1   \amp {}+{}   \amp {a_{12}}x_2   \amp {}+{}  \amp \cdots       \amp {}+{}  \amp {a_{1n}}x_n    \amp {}={}  \amp b_1\\
{a_{21}}x_1   \amp {}+{}   \amp {a_{22}}x_2    \amp {}+{}  \amp \cdots      \amp {}+{}  \amp {a_{2n}}x_n    \amp {}={}  \amp b_2\\
{}         \amp {}     \amp {}          \amp {}    \amp \vdots \ \    \amp {}    \amp {}          \amp {}    \amp {}\\
{a_{m1}}x_1   \amp {}+{}   \amp {a_{2m}}x_2  \amp {}+{}  \amp \cdots      \amp {}+{}  \amp {a_{mn}}x_n  \amp {}={}  \amp b_m
\end{alignat*}
can be written in matrix form as%
\begin{equation*}
A \vx = \vb\text{,}
\end{equation*}
where%
\begin{equation*}
A = \left[ \begin{array}{cccc} a_{11} \amp  a_{12} \amp  \cdots \amp  a_{1n} \\ a_{21} \amp  a_{22} \amp  \cdots \amp  a_{2n} \\ \vdots \amp         \amp  \ddots    \amp  \vdots \\ a_{m1} \amp  a_{m2} \amp  \cdots \amp  a_{mn} \end{array}  \right],  \ \ \ \vx = \left[ \begin{array}{c} x_1 \\ x_2 \\ \vdots \\ x_n \end{array}  \right],  \ \ \ \text{ and }  \ \ \ \vb = \left[ \begin{array}{c} b_1 \\ b_2 \\ \vdots \\ b_m \end{array}  \right]\text{.}
\end{equation*}
%
\item{}The matrix equation \(A \vx = \vb\) has a solution if and only if \(\vb\) is a linear combination of the columns of \(A\).%
\item{}The system \(A \vx = \vb\) is consistent for every vector \(\vb\) if every row of \(A\) contains a pivot.%
\item{}A homogeneous system is a system of the form \(A \vx = \vzero\) for some \(m \times n\) matrix \(A\). Since the zero vector in \(\R^n\) satisfies \(A \vx = \vzero\), a homogeneous system is always consistent.%
\item{}A homogeneous system can have one or infinitely many different solutions. The homogeneous system \(A \vx = \vzero\) has exactly one solution if and only if each column of \(A\) is a pivot column.%
\item{}The solutions to the consistent nonhomogeneous system \(A \vx = \vb\) have the form \(\vp + \vv_h\), where \(\vp\) is a particular solution to the nonhomogeneous system \(A \vx = \vb\) and \(\vv_h\) is a solution to the homogeneous system \(A \vx = \vzero\). In other words, the solution space to a consistent nonhomogeneous system \(A \vx = \vb\) is a translation of the solution space of the homogeneous system \(A \vx = \vzero\) by a particular solution to the nonhomogeneous system.%
\end{itemize}
Finally, we argued an important theorem.%
\begin{paragraphs}{Theorem~{\xreffont\ref*{x:theorem:thm_nm_mtx}}.}{g:paragraphs:idp10353160}%
Let \(A\) be an \(m \times n\) matrix. The following statements are equivalent.%
\begin{enumerate}
\item{}The matrix equation \(A \vx = \vb\) has a solution for every vector \(\vb\) in \(\R^m\).%
\item{}Every vector \(\vb\) in \(\R^m\) can be written as a linear combination of the columns of \(A\).%
\item{}The span of the columns of \(A\) is \(\R^m\).%
\item{}The matrix \(A\) has a pivot position in each row.%
\end{enumerate}
%
\end{paragraphs}%
\par
We will continue to add to this theorem, so it is a good idea for you to begin now to remember the equivalent conditions of this theorem.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_mv_form_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp10360968}%
Write the system%
\begin{alignat*}{5}
x_1     \amp {}+{}   \amp {2}x_2   \amp {}+{}  \amp {2}x_3   \amp {}+{}  \amp {}x_4    \amp {}={}  \amp {}- \amp 1\\
x_1   \amp {}-{}   \amp {8}x_2    \amp {}+{}  \amp {3}x_3   \amp {}-{}  \amp {9}x_4    \amp {}={}  \amp {} \amp 2\\
{}x_1   \amp {}+{}   \amp {6}x_2   \amp {}-{}  \amp {4}x_3    \amp {}+{}  \amp {12}x_4  \amp {}={}  \amp {}- \amp 1
\end{alignat*}
in matrix-vector form. Explicitly identify the coefficient matrix and the vector of constants.%
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp10367752}%
Write the linear combination%
\begin{equation*}
x_1\left[ \begin{array}{c} 1\\5 \end{array}  \right] + x_2\left[ \begin{array}{r} -3\\10 \end{array}  \right] + x_3\left[ \begin{array}{c} 2\\2 \end{array}  \right]
\end{equation*}
as a matrix-vector product.%
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp10371208}%
Represent the following matrix-vector equation as a linear system and find its solution.%
\begin{equation*}
\left[ \begin{array}{crc} 2 \amp  3 \amp  4 \\ 1 \amp  -2 \amp  3 \end{array}  \right] \left[ \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array}  \right] = \left[ \begin{array}{r} 4\\-6 \end{array}  \right]
\end{equation*}
%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp10368904}%
Represent the following matrix-vector equation as a linear system and find its solution.%
\begin{equation*}
\left[ \begin{array}{crr} 1 \amp  -2 \amp  -1 \\ 2 \amp  2 \amp  -2 \\ 3 \amp  1 \amp  1 \end{array}  \right] \left[ \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array}  \right] = \left[ \begin{array}{r} 1\\ -4\\8 \end{array}  \right]
\end{equation*}
%
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{x:exercise:ex_1_e_scalar_product}%
Another way of defining the matrix-vector product uses the concept of the \terminology{scalar product} of vectors.\footnotemark{}  Given a \(1 \times n\) matrix \(\vu = [u_1 \ u_2 \ \ldots \ u_n]\)\footnotemark{} and an \(n \times 1\) vector \(\vv=\left[ \begin{array}{c} v_1\\ v_2\\ \vdots \\ v_n \end{array}  \right]\), we define the scalar product \(\vu \cdot \vv\) as%
\begin{equation*}
\vu \cdot \vv = u_1v_1 + u_2v_2 + u_3v_3 + \cdots + u_nv_n\text{.}
\end{equation*}
We then define the matrix-vector product \(A\vx\) as the vector whose entries are the scalar products of the rows of \(A\) with \(\vx\). As an example, if \(A= \left[ \begin{array}{crc} 2 \amp  3 \amp  4 \\ 1 \amp  -2 \amp  3 \end{array}  \right]\) and \(\vx = \left[ \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array}  \right]\), then%
\begin{equation*}
A \vx = \left[ \begin{array}{c} 2x_1+3x_2 +4x_3 \\ x_1 +(-2)x_2 + 3x_3 \end{array}  \right]\text{.}
\end{equation*}
Calculate the matrix-vector product \(A\vx\) where \(A=\left[ \begin{array}{cc} a \amp  b\\ c\amp  d \end{array}  \right]\) and \(\vx=\left[ \begin{array}{c} x_1 \\ x_2 \end{array}  \right]\) using both methods of finding the matrix-vector product to show that the two definitions are equivalent for size \(2\times 2\) matrices.%
\end{divisionexercise}%
\footnotetext[10]{Note that some authors refer to the scalar product as the \terminology{dot product}.\label{g:fn:idp10371464}}%
\footnotetext[11]{We can identify a \(1 \times n\) matrix \(\vu = [u_1 \ u_2 \ \ldots \ u_n]\) with the \(n \times 1\) vector \(\vu=\left[ \begin{array}{c} u_1\\ u_2\\ \vdots \\ u_n \end{array}  \right]\), so we ofter refer to \([u_1 \ u_2 \ \ldots \ u_n]\) as a vector.\label{g:fn:idp10380552}}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp10385288}%
Find the value of \(a\) such that%
\begin{equation*}
\left[ \begin{array}{crc} 1 \amp  2 \amp  2 \\1 \amp  -1 \amp  3 \\1 \amp  2 \amp  4 \end{array}  \right] \left[ \begin{array}{r} 1 \\ -1 \\ a \end{array}  \right] = \left[ \begin{array}{r} * \\ -5 \\ * \end{array}  \right]
\end{equation*}
where \(*\)'s represent unknown values.%
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp10389128}%
Suppose we have%
\begin{equation*}
\left[ \begin{array}{rccc} 1 \amp  2 \amp  1 \amp  2\\-1 \amp  2 \amp  3 \amp  1\\2\amp 3 \amp  1 \amp  a \end{array}  \right] \left[ \begin{array}{r} 1 \\ 2 \\ -2 \\ 3 \end{array}  \right] = \left[ \begin{array}{c} b_1 \\ b_2\\ b_3 \end{array}  \right]
\end{equation*}
where \(b_i\)'s represent unknown values.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}In order to find the value of \(a\), which of the \(b_i\)'s do we need to know? Why?%
\item{}Suppose the \(b_i\)(s) that we need to know is(are) equal to 9. What is the value of \(a\)?%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp10395784}%
Suppose we are given%
\begin{equation*}
A \vu = \left[ \begin{array}{c} 1 \\ 1 \end{array}  \right] \; \text{ and }  \; A \vv = \left[ \begin{array}{c} 1 \\ 3 \end{array}  \right]
\end{equation*}
for an unknown \(A\) and two unknown vectors \(\vu, \vv\) in \(\R^3\). Using matrix-vector product properties, evaluate \(A\vw\) where \(\vw=2\vu-3\vv\).%
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp10398472}%
Suppose we are given%
\begin{equation*}
A \left[ \begin{array}{c} 1 \\ 2\\ 1 \end{array}  \right] = \left[ \begin{array}{c} 1 \\ 1 \end{array}  \right] \; \text{ and }  \; A \left[ \begin{array}{c} 1 \\ 0 \\2 \end{array}  \right] = \left[ \begin{array}{c} 0 \\ 2 \end{array}  \right] \,\text{.}
\end{equation*}
After expressing \(\left[ \begin{array}{r} -1 \\ 6 \\ -5 \end{array}  \right]\) as a linear combination of \(\left[ \begin{array}{c} 1 \\ 2\\ 1 \end{array}  \right]\) and \(\left[ \begin{array}{c} 1 \\ 0 \\2 \end{array}  \right]\), use the matrix-vector product properties to determine \(A\left[ \begin{array}{r} -1 \\ 6 \\ -5 \end{array}  \right]\).%
\end{divisionexercise}%
\begin{divisionexercise}{10}{}{}{g:exercise:idp10399880}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}The non-homogeneous system (with unknown constants \(a\) and \(b\))%
\begin{alignat*}{4}
{}x   \amp {}+{}   \amp {}y   \amp {}-{}  \amp {}z    \amp {}={}  \amp 2\\
{2}x   \amp {}+{}   \amp {a}y   \amp {}+{}  \amp {b}z  \amp {}={}  \amp 4
\end{alignat*}
has a solution which lies on the \(x\)-axis (i.e. \(y=z=0\)). Find this solution.%
\item{}If the corresponding homogeneous system%
\begin{alignat*}{4}
{}x   \amp {}+{}   \amp {}y   \amp {}-{}  \amp {}z    \amp {}={}  \amp 0\\
{2}x   \amp {}+{}   \amp {a}y   \amp {}+{}  \amp {b}z  \amp {}={}  \amp 0
\end{alignat*}
has its general solution expressed in parametric vector form as \(z\cdot \left[ \begin{array}{c} 0\\1\\1 \end{array}  \right]\), find the general solution for the non-homogeneous system using your answer to part (a).%
\item{}Find the conditions on \(a\) and \(b\) that make the system from (a) have the general solution you found in (b).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{11}{}{}{g:exercise:idp10406152}%
Find the general solution to the non-homogeneous system%
\begin{alignat*}{5}
{}x     \amp {}-{}   \amp {2}y   \amp {}+{}  \amp {}z    \amp {}={}  \amp {}\amp 3\amp {}\\
{-2}x  \amp {}+{}   \amp {4}y   \amp {}-{}  \amp {2}z  \amp {}={}  \amp {-}\amp 6\amp {.}
\end{alignat*}
Using the parametric vector form of the solutions, determine what the solution set to this non-homogeneous system looks like geometrically. Be as specific as possible. (Include information such as whether the solution set is a point, a line, or a plane, etc.; whether the solution set passes through the origin or is shifted from the origin in a specific direction by a specific number of units; and how the solution is related to the corresponding homogeneous system.)%
\end{divisionexercise}%
\begin{divisionexercise}{12}{}{}{g:exercise:idp10409864}%
Come up with an example of a \(3\times 3\) matrix \(A\) for which the solution set of \(A\vx=\vzero\) is a line, and a \(3\times 3\) matrix \(A\) for which the solution set of \(A\vx=\vzero\) is a plane.%
\end{divisionexercise}%
\begin{divisionexercise}{13}{}{}{g:exercise:idp10410120}%
Suppose we have three vectors \(\vv_1, \vv_2\) and \(\vv_3\) satisfying \(\vv_3=2\vv_1-\vv_2\). Let \(A\) be the matrix with vectors \(\vv_1, \vv_2\) and \(\vv_3\) as the columns in that order. Find a non-zero \(\vx\) such that \(A\vx=\vzero\) using this information.%
\end{divisionexercise}%
\begin{divisionexercise}{14}{}{}{g:exercise:idp10419080}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
If the system \(A\vx = \vzero\) has infinitely many solutions, then so does the system \(A\vx=\vb\) for \emph{any} right-hand-side \(\vb\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\vx_1\) is a solution for \(A\vx=\vb_1\) and \(\vx_2\) is a solution for \(A\vx=\vb_2\), then \(\vx_1+\vx_2\) is a solution for \(A\vx=\vb_1+\vb_2\).%
\item{}\lititle{True\slash{}False.}\par%
If an \(m \times n\) matrix \(A\) has a pivot in every row, then the equation \(A\vx=\vb\) has a unique solution for every \(\vb\).%
\item{}\lititle{True\slash{}False.}\par%
If an \(m \times n\) matrix \(A\) has a pivot in every row, then the equation \(A\vx=\vb\) has a solution for every \(\vb\).%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) and \(B\) are row equivalent matrices and the columns of \(A\) span \(\R^m\), then so do the columns of \(B\).%
\item{}\lititle{True\slash{}False.}\par%
All homogeneous systems have either a unique solution or infinitely many solutions.%
\item{}\lititle{True\slash{}False.}\par%
If a linear system is not homogeneous, then the solution set does not include the origin.%
\item{}\lititle{True\slash{}False.}\par%
If a solution set of a linear system does not include the origin, the system is not homogeneous.%
\item{}\lititle{True\slash{}False.}\par%
If the system \(A\vx=\vb\) has a unique solution for some \(\vb\), then the homogeneous system has only the trivial solution.%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is a \(3 \times 4\) matrix, then the homogeneous equation \(A \vx = \vzero\) has non-trivial solutions.%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is a \(3 \times 2\) matrix, then the homogeneous equation \(A \vx = \vzero\) has non-trivial solutions.%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Input-Output Models}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Input-Output Models}{}{Project: Input-Output Models}{}{}{x:section:sec_proj_io_models}
\index{input-output models}%
There are two basic types of input-output models: closed and open. The closed model assumes that all goods produced are consumed within the economy \textemdash{} no trading takes place with outside entities. In the open model, goods produced within the economy can be traded outside the economy.%
\par
To work with a closed model, we use an example (from \pubtitle{Input-Output Economics} by Wassily Leontief). Assume a simple three-sector economy consisting of agriculture (growing wheat), manufacturing (producing cloth), and households (supplying labor). Each sector of the economy relies on goods from the other sectors to operate (e.g., people must eat to work and need to be clothed). To model the interactions between the sectors, we consider how many units of product is needed as input from one sector to another to produce one unit of product in the second sector. For example, assume the following:%
\begin{itemize}[label=\textbullet]
\item{}to produce one unit (say dollars worth) of agricultural goods requires 25\% of a unit of agricultural output, 28\% of a unit of manufacturing output, and 27\% of a unit of household output;%
\item{}to produce one unit of manufactured goods requires 20\% of a unit of agricultural output, 60\% of a unit of manufacturing output, and 60\% of a unit of household output;%
\item{}to produce one unit of household goods requires 55\% of a unit of agricultural output, 12\% of a unit of manufacturing output, and 13\% of a unit of household output.%
\end{itemize}
%
\par
These assumptions are summarized in \hyperref[x:table:T_ThreeSectorTable]{Table~{\xreffont\ref{x:table:T_ThreeSectorTable}}}.%
\begin{tableptx}{\textbf{Summary of simple three sector economy}}{x:table:T_ThreeSectorTable}{}%
\centering%
{\tabularfont%
\begin{tabular}{lccc}
\multicolumn{1}{lA}{\textbf{into\textbackslash{}from}}&\textbf{Agriculture}&\textbf{Manufacture}&\textbf{Households}\tabularnewline\hrulethin
\multicolumn{1}{lA}{Agriculture}&0.25&0.28&0.27\tabularnewline[0pt]
\multicolumn{1}{lA}{Manufacture}&0.20&0.60&0.60\tabularnewline[0pt]
\multicolumn{1}{lA}{Households}&0.55&0.12&0.13
\end{tabular}
}%
\end{tableptx}%
This model is said to be \terminology{closed} because all good produced are used up within the economy. If there are goods that are not used within the economy the model is said to be \terminology{open}. Open models will be examined later.%
\par
The economist's goal is to determine what level of production in each section meets the following requirements:%
\begin{itemize}[label=\textbullet]
\item{}the production from each sector meets the needs of all of the sectors and%
\item{}there is no overproduction.%
\end{itemize}
%
\begin{project}{}{x:project:act_Closed_model}%
We can use techniques from linear algebra to determine the levels of production that precisely meet the two goals of the economist.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Suppose that the agricultural output is \(x_1\) units, the manufacturing output is \(x_2\) units, and the household output is \(x_3\) units. We represent this data as a \terminology{production vector} \(\left[ \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array}  \right]\). To produce a unit of agriculture requires 0.25 units of agriculture, \(0.28\) units of manufacturing, and \(0.27\) units of household. If \(x_1\) units of agriculture, \(x_2\) units of manufacturing, and \(x_3\) units of household products are are produced, then agriculture can produce%
\begin{equation*}
0.25x_1 + 0.28x_2 + 0.27x_3
\end{equation*}
units. In order to meet the needs of agriculture and for there to be no overproduction, we must then have%
\begin{equation*}
0.25x_1 + 0.28x_2 + 0.27x_3 = x_1\text{.}
\end{equation*}
Write similar equations for the manufacturing and household sectors of the economy.%
\item{}Find the augmented matrix for the system of linear equations that represent production of the three sectors from part (a), and then solve the system to find the production levels that meet the economist's two goals.%
\item{}Suppose the production level of the household sector is 200 million units (dollars). Find the production levels of the agricultural and manufacturing sectors that meet the economist's two goals.%
\end{enumerate}
\end{project}%
In general, a matrix derived from a table like \hyperref[x:table:T_ThreeSectorTable]{Table~{\xreffont\ref{x:table:T_ThreeSectorTable}}} is called a \terminology{consumption} matrix, which we will denote as \(C\). (In the example discussed here \(C = \left[\begin{array}{ccc} 0.25 \amp 0.28 \amp 0.27 \\ 0.20 \amp 0.60 \amp 0.60 \\ 0.55 \amp 0.12 \amp 0.13 \end{array} \right]\).) A consumption matrix \(C = [c_{ij}]\), where \(c_{ij}\) represents the proportion of the output of sector \(j\) that is consumed by sector \(i\), satisfies two important properties.%
\begin{itemize}[label=\textbullet]
\item{}Since no sector can consume a negative amount or an amount that exceeds the output of another sector, we must have \(0 \leq c_{ij} \leq 1\) for all \(i\) and \(j\).%
\item{}If there are \(n\) sectors in the economy, the fact that all output is consumed within the economy implies that \(c_{1j}+c_{2j}+ \cdots + c_{nj} = 1\). In other words, the column sums of \(C\) are all 1.%
\end{itemize}
%
\par
In our example, if we let \(x = \left[\begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array} \right]\), then we can write the equations that guarantee that the production levels satisfy the two economists' goal in matrix form as%
\begin{equation}
\vx = C\vx\text{.}\label{x:men:eq_closed}
\end{equation}
%
\par
Now we can rephrase the question to be answered as which production vectors \(\vx\) satisfy equation \hyperref[x:men:eq_closed]{({\xreffont\ref{x:men:eq_closed}})}. When \(C\vx = \vx\), then the system is in equilibrium, that is output exactly meets needs. Any solution \(\vx\) that satisfies \hyperref[x:men:eq_closed]{({\xreffont\ref{x:men:eq_closed}})} is called a \terminology{steady state} solution.%
\begin{project}{}{g:project:idp10480008}%
Is there a steady state solution for the closed system of Agriculture, Manufacturing, and Households? If so, find the general steady state solution. If no, explain why.%
\end{project}%
So far, we considered the case where the economic system was \terminology{closed}. This means that the industries that were part of the system sold products only to each other. However, if we want to represent the demand from other countries, from households, capital building, etc., we need an \terminology{open model}. In an article in the \pubtitle{Scientific American} Leontief organized the 1958 American economy into 81 sectors. The production of each of these sectors relied on production from the all of the sectors. Here we present a small sample from Leontief's 81 sectors, using Petroleum, Textiles, Transportation, and Chemicals as our sectors of the economy. Leontief's model assumed that the production of 1 unit of output of%
\begin{itemize}[label=\textbullet]
\item{}petroleum requires 0.1 unit of petroleum, 0.2 units of transportation, and 0.4 units of chemicals;%
\item{}textiles requires 0.4 units of petroleum, 0.1 unit of textiles, 0.15 units of transportation, and 0.3 units of chemicals;%
\item{}transportation requires 0.6 units of petroleum, 0.1 unit of transportation, and 0.25 units of chemicals;%
\item{}chemicals requires 0.2 units of petroleum, 0.1 unit of textiles, 0.3 units of transportation, and 0.2 units of chemicals.%
\end{itemize}
%
\par
A summary of this information is in \hyperref[x:table:T_FourSectors]{Table~{\xreffont\ref{x:table:T_FourSectors}}}. Assume the units are measured in dollars.%
\begin{tableptx}{\textbf{Summary of four sector economy}}{x:table:T_FourSectors}{}%
\centering%
{\tabularfont%
\begin{tabular}{lcccc}
\multicolumn{1}{lA}{into\textbackslash{}from}&Petroleum&Textiles&Transportation&Chemicals\tabularnewline\hrulethin
\multicolumn{1}{lA}{Petroleum}&0.10&0.00&0.20&0.40\tabularnewline[0pt]
\multicolumn{1}{lA}{Textiles}&0.40&0.10&0.15&0.30\tabularnewline[0pt]
\multicolumn{1}{lA}{Transportation}&0.60&0.00&0.10&0.25\tabularnewline[0pt]
\multicolumn{1}{lA}{Chemicals}&0.20&0.10&0.30&0.20
\end{tabular}
}%
\end{tableptx}%
In the open model, there is another part of the economy, called the \terminology{open sector}, that does not produce goods or services but only consumes them. If this sector (think end consumers, for example) demands\slash{}consumes \(d_1\) units of Petroleum, \(d_2\) units of Textiles, \(d_3\) units of Transportation, and \(d_4\) units of Chemicals, we put this into a \terminology{final demand vector} \(\vd = \left[ \begin{array}{c} d_1 \\d_2 \\ d_3 \\ d_4 \end{array} \right]\).%
\par
An economist would want to find the production level where the demand from the good\slash{}service producing sectors of the economy plus the final demand from the open sector exactly matches the output in each of the sectors. Let \(x_1\) represent the number of units of petroleum output, \(x_2\) the number of units of textiles output, \(x_3\) the number of units of transportation output, and \(x_4\) the number of units of chemical output during any time period. Then the production vector is \(\vx = \left[\begin{array}{c} x_1 \\ x_2 \\ x_3 \\x_4 \end{array} \right]\). So an economist wants to find the production vectors \(\vx\) such that%
\begin{alignat*}{6}
{0.10}x_1    \amp {}{}  \amp {}        \amp {}+{}  \amp {0.20}x_3 \amp {}+{}  \amp {0.40}x_4 \amp {}+{}  \amp {}d_1  \amp = x_1\amp {}\\
{0.40}x_1    \amp {}+{}  \amp {0.10}x_2    \amp {}+{}  \amp {0.15}x_3 \amp {}+{}  \amp {0.30}x_4 \amp {}+{}  \amp {}d_2  \amp = x_2\amp {}\\
{0.60}x_1    \amp {}{}  \amp {}        \amp {}+{}  \amp {0.10}x_3 \amp {}+{}  \amp {0.25}x_4 \amp {}+{}  \amp {}d_3  \amp = x_3\amp {}\\
{0.20}x_1    \amp {}+{}  \amp {0.10}x_2    \amp {}+{}  \amp {0.30}x_3 \amp {}+{}  \amp {0.20}x_4 \amp {}+{}  \amp {}d_4  \amp = x_4\amp {,}
\end{alignat*}
where \(\vd = \left[ \begin{array}{c} d_1 \\d_2 \\ d_3 \\ d_4 \end{array} \right]\) is the demand vector from the open market. The matrix%
\begin{equation*}
E = \left[ \begin{array}{cccc} 0.10 \amp 0.00 \amp 0.20 \amp 0.40    \\ 0.40  \amp 0.10 \amp 0.15 \amp  0.30    \\ 0.60 \amp 0.00 \amp 0.10 \amp 0.25 \\  0.20 \amp 0.10 \amp 0.30 \amp 0.20 \end{array}  \right]
\end{equation*}
derived from \hyperref[x:table:T_FourSectors]{Table~{\xreffont\ref{x:table:T_FourSectors}}}, is called the \terminology{exchange} matrix.%
\begin{project}{}{g:project:idp10503432}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Suppose the final demand vector in our four sector economy is \(\left[ \begin{array}{c} 500 \\200 \\ 400 \\ 100 \end{array} \right]\). Find the production levels that satisfy our system.%
\item{}Does this economy defined by the exchange matrix \(E\) have production levels that exactly meet internal and external demands regardless of the external demands? That is, does the system of equations%
\begin{alignat*}{6}
{0.10}x_1    \amp {}{}  \amp {}        \amp {}+{}  \amp {0.20}x_3 \amp {}+{}  \amp {0.40}x_4 \amp {}+{}  \amp {}d_1  \amp = x_1\amp {}\\
{0.40}x_1    \amp {}+{}  \amp {0.10}x_2    \amp {}+{}  \amp {0.15}x_3 \amp {}+{}  \amp {0.30}x_4 \amp {}+{}  \amp {}d_2  \amp = x_2\amp {}\\
{0.60}x_1    \amp {}{}  \amp {}        \amp {}+{}  \amp {0.10}x_3 \amp {}+{}  \amp {0.25}x_4 \amp {}+{}  \amp {}d_3  \amp = x_3\amp {}\\
{0.20}x_1    \amp {}+{}  \amp {0.10}x_2    \amp {}+{}  \amp {0.30}x_3 \amp {}+{}  \amp {0.20}x_4 \amp {}+{}  \amp {}d_4  \amp = x_4\amp {}
\end{alignat*}
have a solution regardless of the values of \(d_1\), \(d_2\), \(d_3\), and \(d_4\)? Explain.%
\end{enumerate}
\end{project}%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 6 Linear Dependence and Independence}
\typeout{************************************************}
%
\begin{chapterptx}{Linear Dependence and Independence}{}{Linear Dependence and Independence}{}{}{x:chapter:chap_independence}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp10505352}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What are two ways to describe what it means for a set of vectors in \(\R^n\) to be linearly independent?%
\item{}What are two ways to describe what it means for a set of vectors in \(\R^n\) to be linearly dependent?%
\item{}If \(S\) is a set of vectors, what do we mean by a basis for \(\Span \ S\)?%
\item{}Given a nonzero set \(S\) of vectors, how can we find a linearly independent subset of \(S\) that has the same span as \(S\)?%
\item{}How do we recognize if the columns of a matrix \(A\) are linearly independent?%
\item{}How can we use a matrix to determine if a set \(\{\vv_1, \vv_2, \ldots, \vv_k\}\) of vectors is linearly independent?%
\item{}How can we use a matrix to find a minimal spanning set for a set \(\{\vv_1, \vv_2, \vv_3, \ldots, \vv_k\}\) of vectors in \(\R^n\)?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Bézier Curves}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Bézier Curves}{}{Application: Bézier Curves}{}{}{x:section:sec_appl_bezier}
Bézier curves are simple curves that were first developed in 1959 by French mathematician Paul de Casteljau, who was working at the French automaker Citroën. The curves were made public in 1962 by Pierre Bézier who used them in his work designing automobiles at the French car maker Renault. In addition to automobile design, Bézier curves have many other uses. Two of the most common applications of Bézier curves are font design and drawing tools. As an example, the letter ``S'' in Palatino font is shown using Bézier curves in \hyperref[x:figure:F_Letter_S]{Figure~{\xreffont\ref{x:figure:F_Letter_S}}}. If you've used Adobe Illustrator, Photoshop, Macromedia Freehand, Fontographer, or any other of a number of drawing programs, then you've used Bézier curves. At the end of this section we will see how Bézier curves can be defined using linearly independent vectors and linear combinations of vectors.%
\begin{figureptx}{A letter \(S\).}{x:figure:F_Letter_S}{}%
\begin{image}{0.3}{0.4}{0.3}%
\includegraphics[width=\linewidth]{external/letter.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_indep_intro}
In \hyperref[x:chapter:chap_vector_representation]{Section~{\xreffont\ref{x:chapter:chap_vector_representation}}} we saw how to represent water-benzene-acetic acid chemical solutions with vectors, where the components represent the water, benzene and acid percentages. We then considered a problem of determining if a given chemical solution could be made by mixing other chemical solutions. Suppose we now have three different water-benzene-acetic acid chemical solutions, one with 40\% water, 50\% benzene and 10\% acetic acid, the second with 52\% water, 42\% benzene and 6\% acid, and a third with 46\% water, 46\% benzene and 8\% acid. We represent the first chemical solution with the vector \(\vv_1 = \left[ \begin{array}{c} 40\\50\\10 \end{array}  \right]\), the second with the vector \(\vv_2 = \left[ \begin{array}{c} 52\\42\\6 \end{array}  \right]\), and the third with the vector \(\vv_3 = \left[ \begin{array}{c} 46\\46\\8 \end{array}  \right]\). By combining these three chemical solutions we can make a chemical solution with 43\% water, 48\% benzene and 9\% acid as follows%
\begin{equation*}
\frac{7}{12}\vv_1 + \frac{1}{12} \vv_2 + \frac{1}{3}\vv_3 = \left[ \begin{array}{c} 43\\48\\9 \end{array}  \right]\text{.}
\end{equation*}
%
\par
However, if we had noticed that the third chemical solution can actually be made from the first two, that is,%
\begin{equation*}
\frac{1}{2}\vv_1 + \frac{1}{2} \vv_2 = \vv_3\text{,}
\end{equation*}
we might have realized that we don't need the third chemical solution to make the 43\% water, 48\% benzene and 9\% acid chemical solution. In fact,%
\begin{equation*}
\frac{3}{4}\vv_1 + \frac{1}{4} \vv_2 = \left[ \begin{array}{c} 43\\48\\9 \end{array}  \right]\text{.}
\end{equation*}
%
\par
(See \hyperlink{x:exercise:ex_1_d_acid}{Exercise~{\xreffont 5}} of \hyperref[x:chapter:chap_vector_representation]{Section~{\xreffont\ref{x:chapter:chap_vector_representation}}}.) Using the third chemical solution (represented by \(\vv_3\)) uses more information than we actually need to make the desired 43\% water, 48\% benzene and 9\% acid chemical solution because the vector \(\vv_3\) is redundant \textemdash{} all of the material we need to make \(\vv_3\) is contained in \(\vv_1\) and \(\vv_2\). This is the basic idea behind linear independence \textemdash{} representing information in the most efficient way.%
\par
Information is often contained in and conveyed through vectors \textemdash{} especially linear combinations of vectors. In this section we will investigate the concepts of linear dependence and independence of a set of vectors. Our goal is to be able to efficiently determine when a given set of vectors forms a \terminology{minimal spanning set}. A minimal spanning set is a spanning set that contains the smallest number of vectors to obtain all of the vectors in the span. An important aspect of a minimal spanning set is that every vector in the span can be written in one and only one way as a linear combination of the vectors in the minimal spanning set. This will allow us to define the important notion of the dimension of a vector space.%
\begin{paragraphs}{Review of useful information.}{g:paragraphs:idp10618888}%
Recall that a linear combination of vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) in \(\R^n\) is a sum of scalar multiples of \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\). That is, a linear combination of the vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) is a vector of the form%
\begin{equation*}
c_1\vv_1 + c_2\vv_2 + \cdots + c_k\vv_k\text{,}
\end{equation*}
where \(c_1\), \(c_2\), \(\ldots\), \(c_k\) are scalars.%
\par
Recall also that the collection of all linear combinations of a set \(\{\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\}\) of vectors in \(\R^n\) is called the span of the set of vectors. That is, the span \(\Span \{\vv_1, \vv_2, \ldots, \vv_k\}\) of the set \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) of vectors in \(\R^n\) is the set%
\begin{equation*}
\{c_1\vv_1 + c_2\vv_2 + \cdots + c_k\vv_k : \text{ where }  c_1, c_2, \ldots, c_k \text{ are scalars } \}\text{.}
\end{equation*}
%
\par
For example, a linear combination of vectors \(\vv_1 = \left[ \begin{array}{c} 1\\1\\2 \end{array}  \right]\) and \(\vv_2 = \left[ \begin{array}{r} 0\\-2\\1 \end{array}  \right]\) is \(2 \vv_1-3\vv_2 = \left[ \begin{array}{c} 2\\8\\1 \end{array}  \right]\). All linear combinations of these two vectors can be expressed as the collection of vectors of the form \(\left[ \begin{array}{c} c_1\\c_1-2c_2\\2c_1+c_2 \end{array}  \right]\) where \(c_1, c_2\) are scalars. Suppose we want to determine whether \(\vw=\left[ \begin{array}{c} 1\\2\\3 \end{array}  \right]\) is in the span, in other words if \(\vw\) is a linear combination of \(\vv_1, \vv_2\). This means we are looking for \(c_1, c_2\) such that%
\begin{equation*}
\left[ \begin{array}{c} c_1\\c_1-2c_2\\2c_1+c_2 \end{array}  \right] = \left[ \begin{array}{c} 1\\2\\3 \end{array}  \right] \,\text{.}
\end{equation*}
we solve for the system represented with the augmented matrix%
\begin{equation*}
\left[\begin{array}{crc|c} 1\amp 0\amp \amp 1\\ 1\amp -2\amp \amp 2 \\ 2\amp 1\amp \amp 3 \end{array}  \right] \,\text{.}
\end{equation*}
%
\par
By reducing this matrix, we find that there are no solutions of the system, which implies that \(\vw\) is not a linear combination of \(\vv_1, \vv_2\). Note that we can use any names we please for the scalars, say \(x_1, x_2\), if we prefer.%
\end{paragraphs}%
\begin{exploration}{}{x:exploration:pa_1_f}%
Let \(\vv_1 = \left[ \begin{array}{r} 2\\1\\-3 \end{array}  \right]\), \(\vv_2 =  \left[ \begin{array}{c} 1\\1\\0 \end{array}  \right]\), and \(\vv_3 = \left[ \begin{array}{r} 1\\-1\\-6 \end{array}  \right]\), and let \(\vb = \left[ \begin{array}{c} 0 \\ 1 \\ 3 \end{array}  \right]\). If \(\vb\) is in \(\Span \{\vv_1, \vv_2, \vv_3\}\), we are interested in the most efficient way to represent \(\vb\) as a linear combination of \(\vv_1\), \(\vv_2\), and \(\vv_3\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item\label{x:task:act_PA1_f_1}The vector \(\vb\) is in \(\Span \{\vv_1, \vv_2, \vv_3\}\) if there exist \(x_1\), \(x_2\), and \(x_3\) so that%
\begin{equation*}
x_1 \vv_1 + x_2 \vv_2 + x_3 \vv_3 = \vb\text{.}
\end{equation*}
(Recall that we can use any letters we want for the scalars. They are simply unknown scalars we want to solve for.)%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Explain why \(\vb\) is in \(\Span \{\vv_1, \vv_2, \vv_3\}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp10639496}{}\quad{}What is the matrix we need to reduce?%
\item{}Write \(\vb\) as a linear combination of \(\vv_1\), \(\vv_2\), and \(\vv_3\). In how many ways can \(\vb\) be written as a linear combination of the vectors \(\vv_1\), \(\vv_2\), and \(\vv_3\)? Explain.%
\end{enumerate}
\item\label{x:task:act_PA1_f_2}In \hyperref[x:task:act_PA1_f_1]{Task~{\xreffont\ref{x:exploration:pa_1_f}}.{\xreffont\ref{x:task:act_PA1_f_1}}} we saw that the vector \(\vb\) could be written in infinitely many different ways as linear combinations of \(\vv_1\), \(\vv_2\), and \(\vv_3\). We now ask the question if we really need all of the vectors \(\vv_1\), \(\vv_2\), and \(\vv_3\) to make \(\vb\) as a linear combination in a unique way.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Can the vector \(\vb\) be written as a linear combination of the vectors \(\vv_1\) and \(\vv_2\)? If not, why not? If so, in how many ways can \(\vb\) be written as a linear combination of \(\vv_1\) and \(\vv_2\)? Explain.%
\item{}If possible, write \(\vb\) as a linear combination of \(\vv_1\) and \(\vv_2\).%
\end{enumerate}
\item{}In \hyperref[x:task:act_PA1_f_1]{Task~{\xreffont\ref{x:exploration:pa_1_f}}.{\xreffont\ref{x:task:act_PA1_f_1}}} we saw that \(\vb\) could be written in infinitely many different ways as a linear combination of the vectors \(\vv_1\), \(\vv_2\), and \(\vv_3\). However, the vector \(\vb\) could only be written in one way as a linear combination of \(\vv_1\) and \(\vv_2\). So \(\vb\) is in \(\Span \{\vv_1, \vv_2, \vv_3\}\) and \(\vb\) is also in \(\Span \{\vv_1, \vv_2\}\). This raises a question \textemdash{} is \emph{any} vector in \(\Span \{\vv_1, \vv_2, \vv_3\}\) also in \(\Span\{\vv_1, \vv_2\}\). If so, then the vector \(\vv_3\) is redundant in terms of forming the span of \(\vv_1\), \(\vv_2\), and \(\vv_3\). For the sake of efficiency, we want to recognize and eliminate this redundancy.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Can \(\vv_3\) be written as a linear combination of the vectors \(\vv_1\) and \(\vv_2\)? If not, why not? If so, write \(\vv_3\) as a linear combination of \(\vv_1\) and \(\vv_2\).%
\item{}Use the result of part (a) to decide if \emph{any} vector in \(\Span\{\vv_1, \vv_2, \vv_3\}\) is also in \(\Span\{\vv_1, \vv_2\}\).%
\end{enumerate}
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Linear Independence}
\typeout{************************************************}
%
\begin{sectionptx}{Linear Independence}{}{Linear Independence}{}{}{x:section:lin_indep_intro_new}
In this section we will investigate the concepts of linear independence of a set of vectors. Our goal is to be able to efficiently determine when a given set of vectors forms a \terminology{minimal spanning set}. This will involve the concepts of span and linear independence. Minimal spanning sets are important in that they provide the most efficient way to represent vectors in a space, and will later allow us to define the dimension of a vector space.%
\par
In \hyperref[x:exploration:pa_1_f]{Preview Activity~{\xreffont\ref{x:exploration:pa_1_f}}} we considered the case where we had a set \(\{ \vv_1, \vv_2, \vv_3 \}\) of three vectors, and the vector \(\vv_3\) was in the span of \(\{ \vv_1, \vv_2 \}\). So the vector \(\vv_3\) did not add anything to the span of \(\{ \vv_1, \vv_2 \}\). In other words, the set \(\{ \vv_1, \vv_2, \vv_3 \}\) was larger than it needed to be in order to generate the vectors in its span \textemdash{} that is, \(\Span \{ \vv_1, \vv_2, \vv_3 \} = \Span \{ \vv_1, \vv_2 \}\). However, neither of the vectors in the set \(\{ \vv_1, \vv_2 \}\) could be removed without changing its span. In this case, the set \(\{ \vv_1, \vv_2 \}\) is what we will call a \terminology{minimal spanning set} or \terminology{basis} for \(\Span S\). There are two important properties that make \(\{ \vv_1, \vv_2 \}\) a basis for \(\Span S\). The first is that every vector in \(\Span S\) can be written as linear combinations of \(\vv_1\) and \(\vv_2\) (we also use the terminology that the vectors \(\vv_1\) and \(\vv_2\) span \(\Span S\)), and the second is that every vector in \(\Span S\) can be written in exactly one way as a linear combination of \(\vv_1\) and \(\vv_2\). This second property is the property of linear independence, and it is the property that makes the spanning set \terminology{minimal}.%
\par
To make a spanning set minimal, we want to be able to write every vector in the span in a unique way in terms of the spanning vectors. Notice that the zero vector can always be written as a linear combination of any set of vectors using 0 for all of the weights. So to have a \terminology{minimal} or \terminology{linearly independent} spanning set, that is, to have a unique representation for each vector in the span, it will need to be the case that the \terminology{only} way we can write the zero vector as a linear combination of a set of vectors is if all of the weights are 0. This leads us to the definition of a linearly independent set of vectors.%
\begin{definition}{}{x:definition:def_linear_independence_Rn}%
\index{linear independence}%
\index{independence!linear}%
\index{linear dependence}%
\index{dependence!linear}%
A set \(\{ \vv_1, \vv_2, \cdots, \vv_k \}\) of vectors in \(\R^n\) is \terminology{linearly independent} if the vector equation%
\begin{equation*}
x_1 \vv_1 + x_2 \vv_2 + \cdots + x_k \vv_k = \vzero
\end{equation*}
for the scalars \(x_1, x_2, \cdots, x_k\) has only the trivial solution%
\begin{equation*}
x_1 = x_2 = x_3 = \cdots x_k = 0\text{.}
\end{equation*}
If a set is not linearly independent, then the set is \terminology{linearly dependent}.%
\end{definition}
Alternatively, we say that the vectors \(\vv_1, \vv_2, \cdots, \vv_k\) are linearly independent (or dependent) if the set \(\{ \vv_1, \vv_2, \cdots, \vv_k \}\) is linearly independent (or dependent).%
\par
Note that the definition tells us that a set \(\{ \vv_1, \vv_2, \cdots, \vv_k \}\) of vectors in \(\R^n\) is linearly dependent if there are scalars \(x_1, x_2, \cdots, x_n\), not all of which are 0 so that%
\begin{equation*}
x_1 \vv_1 + x_2 \vv_s + \cdots + x_k \vv_k = \vzero\text{.}
\end{equation*}
%
\begin{activity}{}{x:activity:act_1_f_1}%
Which of the following sets in \(\R^2\) or \(\R^3\) is linearly independent and which is linearly dependent? Why? For the linearly dependent sets, write one of the vectors as a linear combination of the others, if possible.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item\label{x:task:act_1_f_1a}\(S_1 = \left\{\left[\begin{array}{c} 2 \\ 0 \\ 1 \end{array}\right], \left[\begin{array}{r} -2 \\ 8 \\ 1 \end{array}\right], \left[\begin{array}{r} -4 \\ 8 \\ 0 \end{array}\right]\right\}\)%
\item{}\(S_2 = \left\{\left[\begin{array}{c} 1 \\ 2 \\ 1 \end{array}\right], \left[\begin{array}{c} 0 \\ 2 \\ 3 \end{array}\right]\right\}\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp10689544}{}\quad{}What relationship must exist between two vectors if they are linearly dependent?%
\item\label{x:task:act_1_f_1c}The vectors \(\vu\), \(\vv\), and \(\vw\) as shown in \hyperref[x:figure:F_1_f_1]{Figure~{\xreffont\ref{x:figure:F_1_f_1}}}.%
\begin{figureptx}{Vectors \(\vu\), \(\vv\), and \(\vw\).}{x:figure:F_1_f_1}{}%
\begin{image}{0.25}{0.5}{0.25}%
\includegraphics[width=\linewidth]{external/1_f_lin_dependence.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\end{enumerate}
\end{activity}%
\hyperref[x:task:act_1_f_1a]{Task~{\xreffont\ref{x:activity:act_1_f_1}}.{\xreffont\ref{x:task:act_1_f_1a}}} and \hyperref[x:task:act_1_f_1c]{Task~{\xreffont\ref{x:activity:act_1_f_1}}.{\xreffont\ref{x:task:act_1_f_1c}}} illustrate how we can write one of the vectors in a linearly dependent set as a linear combination of the others. This would allow us to write at least one of the vectors in the span of the set in more than one way as a linear combination of vectors in this set. We prove this result in general in the following theorem.%
\begin{theorem}{}{}{x:theorem:thm_dependence}%
A set \(\{ \vv_1, \vv_2, \cdots, \vv_k \}\) of vectors in \(\R^n\) is linearly dependent if and only if at least one of the vectors in the set can be written as a linear combination of the remaining vectors in the set.%
\end{theorem}
The next activity is intended to help set the stage for the proof of \hyperref[x:theorem:thm_dependence]{Theorem~{\xreffont\ref{x:theorem:thm_dependence}}}.%
\begin{activity}{}{x:activity:act_1_f_1b}%
The statement of \hyperref[x:theorem:thm_dependence]{Theorem~{\xreffont\ref{x:theorem:thm_dependence}}} is a bi-conditional statement (an if and only if statement). To prove this statement about the set \(S\) we need to show two things about \(S\). One: we must demonstrate that if \(S\) is a linearly dependent set, then at least one vector in \(S\) is a linear combination of the other vectors (this is the ``only if'' part ofthe biconditional statement) and Two: if at least one vector in \(S\) is a linear combination of the others, then \(S\) is linearly dependent (this is the ``if'' part of the biconditional statement). We illustrate the main idea of the proof using a three vector set \(S = \{ \vv_1, \vv_2, \vv_3 \}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}First let us assume that \(S\) is a linearly dependent set and show that at least one vector in \(S\) is a linear combination of the other vectors. Since \(S\) is linearly dependent we can write the zero vector as a linear combination of \(\vv_1\), \(\vv_2\), and \(\vv_3\) with at least one nonzero weight. For example, suppose%
\begin{equation}
2 \vv_1 + 3 \vv_2 + 4 \vv_3 = \vzero\label{x:men:eq_1_f_dependence_thm_1}
\end{equation}
Solve Equation \hyperref[x:men:eq_1_f_dependence_thm_1]{({\xreffont\ref{x:men:eq_1_f_dependence_thm_1}})} for the vector \(\vv_2\) to show that \(\vv_2\) can be written as a linear combination of \(\vv_1\) and \(\vv_3\). Conclude that \(\vv_2\) is a linear combination of the other vectors in the set \(S\).%
\item{}Now we assume that at least one of the vectors in \(S\) is a linear combination of the others. For example, suppose that%
\begin{equation}
\vv_3 = \vv_1 + 5 \vv_2\text{.}\label{x:men:eq_1_f_dependence_thm_2}
\end{equation}
Use vector algebra to rewrite Equation \hyperref[x:men:eq_1_f_dependence_thm_2]{({\xreffont\ref{x:men:eq_1_f_dependence_thm_2}})} so that \(\vzero\) is expressed as a linear combination of \(\vv_1\), \(\vv_2\), and \(\vv_3\) such that the weight on \(\vv_3\) is not zero. Conclude that the set \(S\) is linearly dependent.%
\end{enumerate}
\end{activity}%
Now we provide a formal prof of \hyperref[x:theorem:thm_dependence]{Theorem~{\xreffont\ref{x:theorem:thm_dependence}}}, using the ideas from \hyperref[x:activity:act_1_f_1b]{Activity~{\xreffont\ref{x:activity:act_1_f_1b}}}.%
\begin{proof}{Proof of Theorem~{\xreffont\ref*{x:theorem:thm_dependence}}.}{g:proof:idp10719112}
Let \(s = \{ \vv_1, \vv_2, \cdots, \vv_k \}\) be a set of vectors in \(\R^n\). We will begin by verifying the first statement.%
\par
We assume that \(S\) is a linearly dependent set and show that at least one vector in \(S\) is a linear combination of the others. Since \(S\) is linearly dependent, there are scalarc \(x_1, x_2, \cdots, x_n\), not all of which are 0, so that%
\begin{equation}
x_1\vv_1 + x_2\vv_2 + \cdots +  x_{k}\vv_{k} = \vzero\text{.}\label{x:men:eq_1_f_dependence_1}
\end{equation}
We don't know which scalar(s) are not zero, but there is at least one. So let us assume that \(x_i\) is not zero for some \(i\) between 1 and \(k\). we can then subtract \(x_i \vv_i\) from both sides of Equation \hyperref[x:men:eq_1_f_dependence_1]{({\xreffont\ref{x:men:eq_1_f_dependence_1}})} and divide by \(x_i\) to obtain%
\begin{equation*}
\vv_i = \frac{x_1}{x_i}\vv_1 + \frac{x_2}{x_i}\vv_2 + \cdots + \frac{x_{i-1}}{x_i}\vv_{i-1} +  \frac{x_{i+1}}{x_i}\vv_{i+1} + \frac{x_{i+2}}{x_i}\vv_{i+2} + \cdots + \frac{x_{k}}{x_i}\vv_{k}\text{.}
\end{equation*}
Thus, the vector \(\vv_i\) is a linear combination of \(\vv_1\), \(\vv_2\), \(\cdots\), \(\vv_{i-1}\), \(\vv_{i+1}\), \(\cdots\), \(\vv_k\), and at least one of the vectors in \(S\)is a linear combination of the other vectors in \(S\).%
\par
To verify the second statement, we assume that at least one of the vectors in \(S\) can be written as a linear combination of the others and show that \(S\) is then a linearly dependent set. We don't know which vector(s) in \(S\) can be written as a linear combination of the others, but there is at least one. Let us suppose that \(\vv_i\) is a linear combination of the others, but there is at least one. Let us suppose that \(\vv_i\) is a linear combination of the vectors \(\vv_1\), \(\vv_2\), \(\cdots\), \(\vv_{i-1}\), \(\vv_{i+1}\), \(\cdots\), \(\vv_k\), for some \(i\) between 1 and \(k\). Then there exist scalars \(x_1\), \(x_2\), \(\cdots\), \(x_{i-1}\), \(x_{i+1}\), \(\cdots\), \(x_n\) so that%
\begin{equation*}
\vv_i = x_1\vv_1 + x_2\vv_2 + \cdots + x_{i-1}\vv_{i-1} +  x_{i+1}\vv_{i+1} + x_{i+2}\vv_{i+2} + \cdots + x_{k}\vv_{k}\text{.}
\end{equation*}
It follows that%
\begin{equation*}
\vzero = x_1\vv_1 + x_2\vv_2 + \cdots + x_{i-1}\vv_{i-1} +  (-1)\vv_i + x_{i+1}\vv_{i+1} + x_{i+2}\vv_{i+2} + \cdots + x_{k}\vv_{k}\text{.}
\end{equation*}
So there are scalars \(x_1\), \(x_2\), \(\cdots\), \(x_n\) (with \(x_i = -1\)), not all of which are 0, so that%
\begin{equation*}
x_1\vv_1 + x_2\vv_2 + \cdots +  x_{k}\vv_{k} = \vzero\text{.}
\end{equation*}
This makes \(S\) a linearly dependent set.%
\end{proof}
With a linearly dependent set, at least one of the vectors in the set is a linear combination of the others. With a linearly independent set, this cannot happen \textemdash{} no vector in the set can be written as a linear combination of the others. This result is given in the next theorem. You may be able to see how \hyperref[x:theorem:thm_dependence]{Theorem~{\xreffont\ref{x:theorem:thm_dependence}}} and \hyperref[x:theorem:thm_Independence]{Theorem~{\xreffont\ref{x:theorem:thm_Independence}}} are logically equivalent.%
\begin{theorem}{}{}{x:theorem:thm_Independence}%
A set \(\{\vv_1, \vv_2, \ldots, \vv_k\}\) of vectors in \(\R^n\) is linearly independent if and only if no vector in the set can be written as a linear combination of the remaining vectors in the set.%
\end{theorem}
\begin{activity}{}{x:activity:act_1_f_2}%
As was hinted at in \hyperref[x:exploration:pa_1_f]{Preview Activity~{\xreffont\ref{x:exploration:pa_1_f}}}, an important consequence of a linearly independent set is that every vector in the span of the set can be written in one and only one way as a linear combination of vectors in the set. It is this uniqueness that makes linearly independent sets so useful. We explore this idea in this activity for a linearly independent set of three vectors. Let \(S = \{ \vv_1, \vv_2, \vv_3 \}\) be a linearly independent set of vectors in \(\R^n\) for some \(n\), and let \(\vb\) be a vector in \(\Span S\). To show that \(\vb\) can be written in exactly one way as a linear combination of vectors in \(S\), we assume that%
\begin{equation*}
\vb = x_1 \vv_1 + x_2 \vv_2 + x_3 \vv_3 \ \ \text{ and } \ \ \vb = y_1\vv_1 + y_2 \vv_2 + y_3 \vv_3
\end{equation*}
for some scalars \(x_1\), \(x_2\), \(x_3\), \(y_1\), \(y_2\), and \(y_3\). We need to demonstrate that \(x_1 = y_1\), \(x_2 = y_2\), and \(x_3 = y_3\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}use the two different ways of writing \(\vb\) as a linear combination of \(\vv_1\), \(\vv_2\), and \(\vv_3\) to come up with a linear combination expressing \(\vzero\) as a linear combination of these vectors.%
\item{}Use the linear independence of the vectors \(\vv_1\), \(\vv_2\), and \(\vv_3\) to explain why \(x_1 = y_1\), \(x_2 = y_2\), and \(x_3 = y_3\).%
\end{enumerate}
\end{activity}%
\hyperref[x:activity:act_1_f_2]{Activity~{\xreffont\ref{x:activity:act_1_f_2}}} contains the general ideas to show that any vector in the span of a linearly independent set can be written in one and only one way as a linear combination of the vectors in the set. The weights of such a linear combination provide us a \terminology{coordinate system} for the vectors in terms of the basis. Two familiar concepts of coordinate systems are the Cartesian coordinates and \(xy\)-plane, and \(xyz\)-space. We will revisit the coordinate system idea in a later chapter.%
\par
In the next theorem we state and prove the general case of any number of linearly independent vectors productin unique representations as linear combinations.%
\begin{theorem}{}{}{x:theorem:thm_1_f_unique_representation}%
Let \(S = \{\vv_1, \vv_2, \ldots, \vv_k\}\) be a linearly independent set of vectors in \(\R^n\). Any vector in \(\Span S\) can be written in one and only one way as as linear combination of the vectors \(\vv_1\), \(\vv_2\), and \(\vv_3\).%
\end{theorem}
\begin{proof}{}{g:proof:idp10769800}
Let \(S = \{\vv_1, \vv_2, \ldots, \vv_k\}\) be a linearly independent set of vectors in \(\R^n\) and let \(\vb\) be a vector in \(\Span S\). By definition, it follows that \(\vb\) can be written as a linear combination of the vectors in \(S\). It remains for us to show that this representation is unique. So assume that%
\begin{equation}
\vb = x_1 \vv_1 + x_2 \vv_2 + \cdots + x_k \vv_k \ \ \text{ and } \ \ \vb = y_1\vv_1 + y_2 \vv_2 + \cdots + y_k \vv_k\label{x:men:eq_1_f_1}
\end{equation}
for some scalars \(x_1\), \(x_2\), \(\cdots\), \(x_k\) and \(y_1\), \(y_2\), \(\cdots\), \(y_k\). Then%
\begin{equation*}
x_1 \vv_1 + x_2 \vv_2 + \cdots + x_k \vv_k = y_1\vv_1 + y_2 \vv_2 + \cdots + y_k \vv_k\text{.}
\end{equation*}
Subtracting all terms from the right side and using a little vector algebra gives us%
\begin{equation*}
(x_1-y_1) \vv_1 + (x_2-y_2) \vv_2 + \cdots + (x_k-y_k) \vv_k = \vzero\text{.}
\end{equation*}
The fact that \(S\) is a linearly independent set implies that%
\begin{equation*}
x_1-y_1=0, \ x_2-y_2 = 0, \ \ldots, \ x_k-y_k=0\text{,}
\end{equation*}
showing that \(x_i = y_i\) for every \(i\) between 1 and \(k\). We conclude that the representation of \(\vb\) as a linear combination of the linearly independent vectors in \(S\) is unique.%
\end{proof}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Determining Linear Independence}
\typeout{************************************************}
%
\begin{sectionptx}{Determining Linear Independence}{}{Determining Linear Independence}{}{}{x:section:sec_determ_lin_ind}
The definition and our previous work give us a straightforward method for determining when a set of vectors in \(\R^n\) is linearly independent or dependent.%
\begin{activity}{}{x:activity:act_1_f_3}%
In this activity we learn how to use a matrix to determine in general if a set of vectors in \(\R^n\) is linearly independent or dependent. Suppose we have \(k\) vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) in \(\R^n\). To see if these vectors are linearly independent, we need to find the solutions to the vector equation%
\begin{equation}
x_1 \vv_1 + x_2 \vv_2 + \cdots + x_k \vv_k = \vzero\text{.}\label{x:men:eq_lin_indep}
\end{equation}
%
\par
If we let \(A = [\vv_1 \ \vv_2 \ \vv_3 \ \cdots \ \vv_k]\) and \(\vx = \left[ \begin{array}{c} x_1 \\ x_2 \\ \vdots \\ x_k \end{array} \right]\), then we can write the vector equation \hyperref[x:men:eq_lin_indep]{({\xreffont\ref{x:men:eq_lin_indep}})} in matrix form \(A \vx = \vzero\). Let \(B\) be the reduced row echelon form of \(A\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}What can we say about the pivots of \(B\) in order for \(A \vx = \vzero\) to have exactly one solution? Under these conditions, are the vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) linearly independent or dependent?%
\item{}What can we say about the rows or columns of \(B\) in order for \(A \vx = \vzero\) to have infinitely many solutions? Under these conditions, are the vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) linearly independent or dependent?%
\item{}Use the result of parts (a) and (b) to determine if the vectors \(\vv_1 = \left[ \begin{array}{r} 1 \\ -1 \\ 2 \\ 0 \end{array} \right]\), \(\vv_2 = \left[ \begin{array}{c} 1 \\ 0 \\ 2 \\ 3 \end{array} \right]\), and \(\vv_3 = \left[ \begin{array}{c} 0 \\ 0 \\ 2 \\ 1 \end{array} \right]\) in \(\R^4\) are linearly independent or dependent. If dependent, write one of the vectors as a linear combination of the others. You may use the fact that the matrix \(\left[ \begin{array}{rcc} 1\amp 1\amp 0 \\ -1\amp 0\amp 0 \\ 2\amp 2\amp 2 \\ 0\amp 3\amp 1 \end{array} \right]\) is row equivalent to \(\left[ \begin{array}{rcc} 1\amp 0\amp 0 \\ 0\amp 1\amp 0 \\ 0\amp 0\amp 1 \\ 0\amp 0\amp 0 \end{array} \right]\).%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Minimal Spanning Sets}
\typeout{************************************************}
%
\begin{sectionptx}{Minimal Spanning Sets}{}{Minimal Spanning Sets}{}{}{x:section:sec_min_span_set}
It is important to note the differences and connections between linear independence, span, and minimal spanning set.%
\begin{itemize}[label=\textbullet]
\item{}The set \(S = \left\{ \left[ \begin{array}{c} 1 \\ 0 \\ 0 \end{array} \right], \left[ \begin{array}{c} 0 \\ 1 \\ 0 \end{array} \right] \right\}\) is not a minimal spanning set for \(\R^3\) even though \(S\) is a linearly independent set. Note that \(S\) does not span \(\R^3\) since the vector \(\left[ \begin{array}{c} 0 \\ 0 \\ 1 \end{array} \right]\) is not in \(\Span \ S\).%
\item{}The set \(T = \left\{ \left[ \begin{array}{c} 1 \\ 0 \\ 0 \end{array}  \right], \left[ \begin{array}{c} 0 \\ 1 \\ 0 \end{array}  \right] , \left[ \begin{array}{c} 0 \\ 0 \\ 1 \end{array}  \right] , \left[ \begin{array}{c} 1 \\ 1 \\ 1 \end{array}  \right] \right\}\) is not a minimal spanning set for \(\R^3\) even though \(\Span \ T = \R^3\). Note that%
\begin{equation*}
\left[ \begin{array}{c} 1 \\ 1 \\ 1 \end{array}  \right] = \left[ \begin{array}{c} 1 \\ 0 \\ 0 \end{array}  \right] + \left[ \begin{array}{c} 0 \\ 1 \\ 0 \end{array}  \right] + \left[ \begin{array}{c} 0 \\ 0 \\ 1 \end{array}  \right]\text{,}
\end{equation*}
so \(T\) is not a linearly independent set.%
\item{}The set \(U = \left\{ \left[ \begin{array}{c} 1 \\ 0 \\ 0 \end{array} \right], \left[ \begin{array}{c} 0 \\ 1 \\ 0 \end{array} \right] , \left[ \begin{array}{c} 0 \\ 0 \\ 1 \end{array} \right] \right\}\) is a minimal spanning set for \(\R^3\) since it satisfies both characteristics of a minimal spanning set: \(\Span \ U = \R^3\) AND \(U\) is linearly independent.%
\end{itemize}
%
\par
The three concepts \textemdash{} linear independence, span, and minimal spanning set \textemdash{} are different. The important point to note is that minimal spanning set must be both linearly independent and span the space.%
\par
To find a minimal spanning set we will often need to find a smallest subset of a given set of vectors that has the same span as the original set of vectors. In this section we determine a method for doing so.%
\begin{activity}{}{x:activity:act_1_f_4}%
Let \(\vv_1 = \left[ \begin{array}{r} -1 \\ 0 \\ 2 \end{array} \right]\), \(\vv_2 = \left[ \begin{array}{r} 2 \\ 0 \\ -4 \end{array} \right]\), \(\vv_3 = \left[ \begin{array}{c} 0 \\ 1 \\ 3 \end{array} \right]\), and \(\vv_4 = \left[ \begin{array}{r} -3 \\ 4 \\ 18 \end{array} \right]\) in \(\R^3\). Assume that the reduced row echelon form of the matrix \(A = \left[ \begin{array}{rrcr} -1\amp 2\amp 0\amp -3 \\ 0\amp 0\amp 1\amp 4 \\ 2\amp -4\amp 3\amp 18 \end{array} \right]\) is \(\left[ \begin{array}{crcc} 1\amp -2\amp 0\amp 3 \\ 0\amp 0\amp 1\amp 4 \\ 0\amp 0\amp 0\amp 0 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Write the general solution to the homogeneous system \(A \vx = \vzero\), where \(\vx = \left[ \begin{array}{c} x_1 \\ x_2 \\ x_3 \\ x_4 \end{array} \right]\). Write all linear combinations of \(\vv_1\), \(\vv_2\), \(\vv_3\), and \(\vv_4\) that are equal to \(\vzero\), using weights that only involve \(x_2\) and \(x_4\).%
\item{}Explain how we can conveniently choose the weights in the general solution to \(A \vx = \vzero\) to show that the vector \(\vv_4\) is a linear combination of \(\vv_1\), \(\vv_2\), and \(\vv_3\). What does this tell us about \(\Span \{\vv_1, \vv_2, \vv_3\}\) and \(\Span \{\vv_1, \vv_2, \vv_3, \vv_4\}\)?%
\item{}Explain how we can conveniently choose the weights in the general solution to \(A \vx = \vzero\) to show why the vector \(\vv_2\) is a linear combination of \(\vv_1\) and \(\vv_3\). What does this tell us about \(\Span \{\vv_1, \vv_3\}\) and \(\Span \{\vv_1, \vv_2, \vv_3 \}\)?%
\item{}Is \(\{\vv_1, \vv_3\}\) a minimal spanning set for \(\Span \{\vv_1, \vv_2, \vv_3, \vv_4\}\)? Explain your response.%
\end{enumerate}
\end{activity}%
\hyperref[x:activity:act_1_f_4]{Activity~{\xreffont\ref{x:activity:act_1_f_4}}} illustrates how we can use a matrix to determine a minimal spanning set for a given set of vectors \(\{\vv_1, \vv_2, \ldots, \vv_k\}\) in \(\R^n\).%
\begin{itemize}[label=\textbullet]
\item{}Form the matrix \(A = [\vv_1 \ \vv_2 \ \cdots \ \vv_k]\).%
\item{}Find the reduced row echelon form \([B \ | \ \vzero]\) of \([A \ | \ \vzero]\). If \(B\) contains non-pivot columns, say for example that the \(i\)th column is a non-pivot column, then we can choose the weight \(x_i\) corresponding to the \(i\)th column to be 1 and all weights corresponding to the other non-pivot columns to be 0 to make a linear combination of the columns of \(A\) that is equal to \(\vzero\). This allows us to write \(\vv_i\) as a linear combination of the vectors corresponding to the pivot columns of \(A\) as we did in the proof of \hyperref[x:theorem:thm_Independence]{Theorem~{\xreffont\ref{x:theorem:thm_Independence}}}. So every vector corresponding to a non-pivot column is in the span of the set of vectors corresponding to the pivot columns. The vectors corresponding to the pivot columns are linearly independent, since the matrix with those columns has every column as a pivot column. Thus, the set of vectors corresponding to the pivot columns of \(A\) forms a minimal spanning set for \(\{\vv_1, \vv_2, \ldots, \vv_k\}\).%
\end{itemize}
%
\begin{paragraphs}{IMPORTANT NOTE.}{g:paragraphs:idp10826248}%
The set of pivot columns of the reduced row echelon form of \(A\) will normally not have the same span as the set of columns of \(A\), so it is critical that we use columns of \(A\), \alert{not} \(B\) in our minimal spanning set.%
\end{paragraphs}%
\begin{activity}{}{x:activity:act_1_f_5}%
Find a minimal spanning set for the span of the set%
\begin{equation*}
\left\{ \left[ \begin{array}{c} 1 \\ 1 \\ 0 \\ 0 \end{array}  \right], \left[ \begin{array}{c} 2 \\ 3 \\ 0 \\ 0 \end{array}  \right], \left[ \begin{array}{c} 0 \\ 1 \\ 2 \\ 0 \end{array}  \right], \left[ \begin{array}{c} 4 \\ 1 \\ 0 \\ 0 \end{array}  \right] \right\}\text{.}
\end{equation*}
%
\end{activity}%
\hyperref[x:activity:act_1_f_4]{Activity~{\xreffont\ref{x:activity:act_1_f_4}}} also illustrates a general process by which we can find a minimal spanning set \textemdash{} that is the smallest subset of vectors that has the same span. This process will be useful later when we consider vectors in arbitrary vector spaces. The idea is that if we can write one of the vectors in a set \(S\) as a linear combination of the remaining vectors, then we can remove that vector from the set and maintain the same span. In other words, begin with the span of a set \(S\) and follow these steps:%
\begin{descriptionlist}
\begin{dlimedium}{Step 1}{g:li:idp10835336}%
If \(S\) is a linearly independent set, we already have a minimal spanning set.%
\end{dlimedium}%
\begin{dlimedium}{Step 2}{g:li:idp10831752}%
If \(S\) is not a linearly independent set, then one of the vectors in \(S\) is a linear combination of the others. Remove that vector from \(S\) to obtain a new set \(T\). It will be the case that \(\Span \ T = \Span \ S\).%
\end{dlimedium}%
\begin{dlimedium}{Step 3}{g:li:idp10833544}%
If \(T\) is a linearly independent set, then \(T\) is a minimal spanning set. If not, repeat steps 2 and 3 for the set \(T\) until you arrive at a linearly independent set.%
\end{dlimedium}%
\end{descriptionlist}
%
\par
This process is guaranteed to stop as long as the set contains at least one nonzero vector. A verification of the statement in Step 2 that \(\Span \ T = \Span \ S\) is given in the next theorem.%
\begin{theorem}{}{}{x:theorem:thm_minimal_spanning_set}%
Let \(\{\vv_1, \vv_2, \ldots, \vv_k\}\) be a set of vectors in \(\R^n\) so that for some \(i\) between 1 and \(k\), \(\vv_i\) is in \(\Span \{\vv_1, \vv_2, \ldots, \vv_{i-1}, \vv_{i+1}, \ldots, \vv_k\}\). Then%
\begin{equation*}
\Span \{\vv_1, \vv_2, \ldots, \vv_k\} = \Span \{\vv_1, \vv_2, \ldots, \vv_{i-1}, \vv_{i+1}, \ldots, \vv_k\}\text{.}
\end{equation*}
%
\end{theorem}
\begin{proof}{}{g:proof:idp10838280}
Let \(\{\vv_1, \vv_2, \ldots, \vv_k\}\) be a set of vectors in \(\R^n\) so that \(\vv_i\) is in the span of \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_{i-1}\), \(\vv_{i+1}\), \(\ldots\), and \(\vv_k\) for some \(i\) between 1 and \(k\). To show that%
\begin{equation*}
\Span \{\vv_1, \vv_2, \ldots, \vv_k\} = \Span \{\vv_1,  \ldots, \vv_{i-1}, \vv_{i+1}, \ldots, \vv_k\}\text{,}
\end{equation*}
we need to show that%
\begin{enumerate}
\item{}every vector in \(\Span \{\vv_1, \vv_2, \ldots, \vv_k\}\) is in \(\Span\{\vv_1, \ldots, \vv_{i-1}, \vv_{i+1}, \ldots, \vv_k\}\), and%
\item{}every vector in \(\Span \{\vv_1, \ldots, \vv_{i-1}, \vv_{i+1}, \ldots, \vv_k\}\) is in \(\Span \{\vv_1, \ldots, \vv_k\}\).%
\end{enumerate}
%
\par
Let us consider the second containment. Let \(\vx\) be a vector in the span of \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_{i-1}\), \(\vv_{i+1}\), \(\ldots\), and \(\vv_k\). Then%
\begin{equation*}
\vx = x_1\vv_1 + x_2\vv_2 + \cdots + x_{i-1}\vv_{i-1} + x_{i+1}\vv_{i+1} + \cdots + x_k\vv_k
\end{equation*}
for some scalars \(x_1\), \(x_2\), \(\ldots\), \(x_{i-1}\), \(x_{i+1}\), \(\ldots\), \(x_k\). Note that%
\begin{equation*}
\vx = x_1\vv_1 + x_2\vv_2 + \cdots + x_{i-1}\vv_{i-1} + (0)\vv_i + x_{i+1}\vv_{i+1} + \cdots + x_k\vv_k
\end{equation*}
as well, so \(\vx\) is in \(\Span \{\vv_1, \vv_2, \ldots, \vv_k\}\). Thus,%
\begin{equation*}
\Span \{\vv_1, \vv_2, \ldots, \vv_{i-1}, \vv_{i+1}, \ldots, \vv_k\} \subseteq  \Span \{\vv_1, \vv_2, \ldots, \vv_k\}\text{.}
\end{equation*}
%
\par
(This same argument shows a more general statement that if \(S\) is a subset of \(T\), then \(\Span \ S \subseteq \Span \ T\).)%
\par
Now we demonstrate the first containment. Here we need the assumption that \(\vv_i\) is in \(\Span \{\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_{i-1}\), \(\vv_{i+1}\), \(\ldots\), \(\vv_k\}\) for some \(i\) between 1 and \(k\). That assumption gives us%
\begin{equation}
\vv_i = c_1 \vv_1 + c_2\vv_2 + \cdots + c_{i-1}\vv_{i-1} + c_{i+1}\vv_{i+1} + \cdots + c_k\vv_k\label{x:men:eq_lin_depend_containment}
\end{equation}
for some scalars \(c_1\), \(c_2\), \(\ldots\), \(c_{i-1}\), \(c_{i+1}\), \(\ldots\), \(c_k\). Now let \(\vx\) be a vector in the span of \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\). Then%
\begin{equation*}
\vx = x_1\vv_1 + x_2\vv_2 + \cdots + x_k\vv_k
\end{equation*}
for some scalars \(x_1\), \(x_2\), \(\ldots\), \(x_k\). Substituting from \hyperref[x:men:eq_lin_depend_containment]{({\xreffont\ref{x:men:eq_lin_depend_containment}})} shows that%
\begin{align*}
\vx \amp = x_1\vv_1 + x_2\vv_2 + \cdots + x_k\vv_k\\
\amp = x_1\vv_1 + x_2\vv_2 + \cdots + x_{i-1} \vv_{i-1} + x_i \vv_i + x_{i+1}\vv_{i+1} + \cdots + x_k\vv_k\\
\amp = x_1\vv_1 + x_2\vv_2 + \cdots + x_{i-1} \vv_{i-1}\\
\amp \qquad + x_i[c_1 \vv_1 + c_2\vv_2 + \cdots + c_{i-1}\vv_{i-1} + c_{i+1}\vv_{i+1} + \cdots + c_k\vv_k]\\
\amp \qquad + x_{i+1}\vv_{i+1} + \cdots + x_k\vv_k\\
\amp = (x_1+x_ic_1)\vv_1 + (x_2+x_ic_2)\vv_2 + \cdots + (x_{i-1}+x_ic_{i-1}) \vv_{i-1}\\
\amp \qquad + (x_{i+1}+x_ic_{i+1}) \vv_{i+1} \cdots + (x_k+x_ic_k)\vv_k\text{.}
\end{align*}
%
\par
So \(\vx\) is in \(\Span \{\vv_1, \vv_2, \ldots, \vv_{i-1}, \vv_{i+1}, \ldots, \vv_k\}\) and%
\begin{equation*}
\Span \{\vv_1, \vv_2, \ldots, \vv_k\} \subseteq \Span \{\vv_1, \vv_2, \ldots, \vv_{i-1}, \vv_{i+1}, \ldots, \vv_k\}\text{.}
\end{equation*}
%
\par
Since the two sets are subsets of each other, they must be equal sets. We conclude that%
\begin{equation*}
\Span \{\vv_1, \vv_2, \ldots, \vv_k\} = \Span \{\vv_1, \vv_2, \ldots, \vv_{i-1}, \vv_{i+1}, \ldots, \vv_k\}\text{.}\qedhere
\end{equation*}
%
\end{proof}
The result of \hyperref[x:theorem:thm_minimal_spanning_set]{Theorem~{\xreffont\ref{x:theorem:thm_minimal_spanning_set}}} is that if we have a finite set \(S\) of vectors in \(\R^n\), we can eliminate those vectors that are linear combinations of others until we obtain a smallest set of vectors that still has the same span. As mentioned earlier, we call such a minimal spanning set a basis.%
\begin{definition}{}{x:definition:def_1_f_basis}%
\index{basis}%
Let \(S\) be a set of vectors in \(\R^n\). A subset \(B\) of \(S\) is a \terminology{basis} for \(\Span \ S\) if \(B\) is linearly independent and \(\Span \ B = \Span \ S\).%
\end{definition}
\begin{paragraphs}{IMPORTANT NOTE.}{g:paragraphs:idp10876056}%
A basis is defined by two characteristics. A basis must span the space in question and a basis must be a linearly independent set. It is the linear independence that makes a basis a \emph{minimal} spanning set.%
\end{paragraphs}%
\par
We have worked with a familiar basis in \(\R^2\) throughout our mathematical careers. A vector \(\left[ \begin{array}{c} a \\ b \end{array}  \right]\) in \(\R^2\) can be written as%
\begin{equation*}
\left[ \begin{array}{c} a \\ b \end{array}  \right] = a\left[ \begin{array}{c} 1 \\ 0 \end{array}  \right] + b\left[ \begin{array}{c} 0 \\ 1 \end{array}  \right]\text{.}
\end{equation*}
%
\par
So the set \(\{\ve_1, \ve_2\}\), where \(\ve_1 = \left[ \begin{array}{c} 1 \\ 0 \end{array} \right]\) and \(\ve_2 = \left[ \begin{array}{c} 0 \\ 1 \end{array} \right]\) spans \(\R^2\). Since the columns of \([\ve_1 \ \ve_2]\) are linearly independent, so is the set \(\{\ve_1, \ve_2\}\). Therefore, the set \(\{\ve_1, \ve_2\}\) is a basis for \(\R^2\). The vector \(\ve_1\) is in the direction of the positive \(x\)-axis and the vector \(\ve_2\) is in the direction of the positive \(y\)-axis, so decomposing a vector \(\left[ \begin{array}{c} a \\ b \end{array} \right]\) as a linear combination of \(\ve_1\) and \(\ve_2\) is akin to identifying the vector with the point \((a,b)\) as we discussed earlier. The set \(\{\ve_1, \ve_2\}\) is called the \terminology{standard basis} for \(\R^2\).%
\par
\index{standard basis!for \(\R^n\)} This idea is not restricted to \(\R^2\). Consider the vectors%
\begin{equation*}
\ve_1 = \left[ \begin{array}{c} 1\\0\\0 \\ \vdots \\ 0 \\ 0 \end{array}  \right], \ \ve_2 = \left[ \begin{array}{c} 0\\1\\0 \\ \vdots \\ 0 \\ 0 \end{array}  \right], \ \cdots, \ \ve_n = \left[ \begin{array}{c} 0\\0\\0 \\ \vdots \\ 0 \\ 1 \end{array}  \right]
\end{equation*}
in \(\R^n\). That is, the vector \(\ve_i\) is the vector with a 1 in the \(i\)th position and 0s everywhere else. Since the matrix \([\ve_1 \ \ve_2 \ \cdots \ \ve_n]\) has a pivot in each row and column, the set \(\{\ve_1, \ve_2, \ldots, \ve_n\}\) is a basis for \(\R^n\). The set \(\{\ve_1, \ve_2, \ldots, \ve_n\}\) is called the \terminology{standard basis} for \(\R^n\).%
\par
As we will see later, bases\footnote{The plural of basis is bases.\label{g:fn:idp10888216}} are of fundamental importance in linear algebra in that bases will allow us to define the dimension of a vector space and will provide us with coordinate systems.%
\par
We conclude this section with an important theorem that is similar to \hyperref[x:theorem:thm_IMT_1_e]{Theorem~{\xreffont\ref{x:theorem:thm_IMT_1_e}}}.%
\begin{theorem}{}{}{x:theorem:thm_IMT_1_f}%
Let \(A\) be an \(m \times n\) matrix. The following statements are equivalent.%
\begin{enumerate}
\item{}The matrix equation \(A \vx = \vb\) has a unique solution for every vector \(\vb\) in the span of the columns of \(A\).%
\item{}The matrix equation \(A \vx = \vzero\) has the unique solution \(\vx = \vzero\).%
\item{}The columns of \(A\) are linearly independent.%
\item{}The matrix \(A\) has a pivot position in each column.%
\end{enumerate}
%
\end{theorem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_indep_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp10907288}%
Let \(\vv_1 = \left[ \begin{array}{c} 1\\2\\0\\1 \end{array} \right]\), \(\vv_2 = \left[ \begin{array}{r} 0\\6\\-1\\5 \end{array} \right]\), \(\vv_3 = \left[ \begin{array}{r} 3\\-6\\2\\-7 \end{array} \right]\), and \(\vv_4 = \left[ \begin{array}{r} 5\\-2\\2\\-5 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Is the set \(S = \{\vv_1, \vv_2, \vv_3, \vv_4\}\) linearly independent or dependent. If independent, explain why. If dependent, write one of the vectors in \(S\) as a linear combination of the other vectors in \(S\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp10905112}{}\quad{}We need to know the solutions to the vector equation%
\begin{equation*}
x_1 \vv_1 + x_2 \vv_2 + x_3 \vv_3 + x_4 \vv_4 = \vzero\text{.}
\end{equation*}
If the equation has as its only solution \(x_1 = x_2 = x_3 = x_4 = 0\) (the trivial solution), then the set \(S\) is linearly independent. Otherwise the set \(S\) is linearly dependent. To find the solutions to this system, we row reduce the augmented matrix%
\begin{equation*}
\left[ \begin{array}{crrr|c} 1\amp 0\amp 3\amp 5\amp 0 \\ 2\amp 6\amp -6\amp -2\amp 0 \\ 0\amp -1\amp 2\amp 2\amp 0 \\ 1\amp 5\amp -7\amp -5\amp 0 \end{array}  \right]\text{.}
\end{equation*}
(Note that we really don't need the augmented column of zeros \textemdash{} row operations won't change that column at all. We just need to know that the column of zeros is there.) Technology shows that the reduced row echelon form of this augmented matrix is%
\begin{equation*}
\left[ \begin{array}{cccr|c} 1\amp 0\amp 3\amp 5\amp 0 \\ 0\amp 1\amp -2\amp -2\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
The reduced row echelon form tells us that the vector equation is consistent, and the fact that there is no pivot in the fourth column shows that the system has a free variable and more than just the trivial solution. We conclude that \(S\) is linearly dependent. Moreover, the general solution to our vector equation is%
\begin{align*}
x_1 \amp = -3x_3 - 5x_4\\
x_2 \amp = 2x_3 + 2x_4\\
x_3 \amp \text{ is free }\\
x_4 \amp \text{ is free } \text{.}
\end{align*}
Letting \(x_4 = 0\) and \(x_3 = 1\) shows that one non-trivial solution to our vector equation is%
\begin{equation*}
x_1 = -3, \ x_2 = 2, \ x_3 = 1, \ \text{ and }  \ x_4 = 0\text{.}
\end{equation*}
Thus,%
\begin{equation*}
-3\vv_1 + 2\vv_2 + \vv_3 = \vzero\text{,}
\end{equation*}
or%
\begin{equation*}
\vv_3 = 3\vv_1 - 2\vv_2
\end{equation*}
and we have written one vector in \(S\) as a linear combination of the other vectors in \(S\).%
\item{}Find a subset \(B\) of \(S\) that is a basis for \(\Span \ S\). Explain how you know you have a basis.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp10913944}{}\quad{}We have seen that the pivot columns in a matrix \(A\) form a minimal spanning set (or basis) for the span of the columns of \(A\). From part (a) we see that the pivot columns in the reduced row echelon form of \(A = [\vv_1 \ \vv_2 \ \vv_3 \ \vv_4]\) are the first and second columns. So a basis for the span of the columns of \(A\) is \(\{\vv_1, \vv_2\}\). Since the elements of \(S\) are the columns of \(A\), we conclude that the set \(B = \{\vv_1, \vv_2\}\) is a subset of \(S\) that is a basis for \(\Span \ S\).%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp10920856}%
Let \(\vv_1 = \left[ \begin{array}{c} 1 \\ 1 \\ 0 \end{array} \right]\), \(\vv_2 = \left[ \begin{array}{r} 3 \\ -7 \\ 2 \end{array} \right]\), and \(\vv_3 = \left[ \begin{array}{r} -5 \\ 6 \\ 10 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Is the set \(S = \left\{\vv_1, \vv_2, \vv_3\right\}\) a basis for \(\R^3\)? Explain.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp10921240}{}\quad{}We need to know if the vectors in \(S\) are linearly independent and span \(\R^3\). Technology shows that the reduced row echelon form of%
\begin{equation*}
A=\left[ \begin{array}{crr} 1\amp 3\amp -5 \\ 1\amp -7\amp 6 \\ 0\amp 2\amp 10 \end{array}  \right]
\end{equation*}
is%
\begin{equation*}
\left[ \begin{array}{ccc} 1\amp 0\amp 0 \\ 0\amp 1\amp 0 \\ 0\amp 0\amp 1 \end{array}  \right]\text{.}
\end{equation*}
Since every column of \([\vv_1 \ \vv_2 \ \vv_3]\) is a pivot column, the set \(\{\vv_1, \vv_2, \vv_3\}\) is linearly independent. The fact that there is a pivot in every row of the matrix \(A\) means that the equation \(A \vx = \vb\) is consistent for every \(\vb\) in \(\R^3\). Since \(A \vx\) is a linear combination of the columns of \(A\) with weights from \(\vx\), tt follows that the columns of \(A\) span \(\R^3\). We conclude that the set \(S\) is a basis for \(\R^3\).%
\item{}Let \(\vv_4 = \left[ \begin{array}{r} -5 \\ 6 \\ h \end{array} \right]\), where \(h\) is a scalar. Are there any values of \(h\) for which the set \(S' = \{\vv_1, \vv_2, \vv_4\}\) is not a basis for \(\R^3\)? If so, find all such values of \(h\) and explain why \(S'\) is not a basis for \(\R^3\) for those values of \(h\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp10932504}{}\quad{}Technology shows that a row echelon form of \(A = [\vv_1 \ \vv_2 \ \vv_4]\) is%
\begin{equation*}
\left[  \begin{array}{crc} 1\amp 0\amp 0 \\ 0\amp -10\amp 11 \\ 0\amp 0\amp h+\frac{11}{5} \end{array}  \right]\text{.}
\end{equation*}
The columns of \(A\) are all pivot columns (hence linearly independent) as long as \(h \neq -\frac{11}{5}\), and are linearly dependent when \(h = -\frac{11}{5}\). So the only value of \(h\) for which \(S'\) is not a basis for \(\R^3\) is \(h = -\frac{11}{5}\).%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_indep_summ}
%
\begin{itemize}[label=\textbullet]
\item{}A set \(\{\vv_1, \vv_2, \ldots, \vv_k\}\) of vectors in \(\R^n\) is linearly independent if the vector equation%
\begin{equation*}
x_1 \vv_1 + x_2 \vv_2 + \cdots + x_k \vv_k = \vzero
\end{equation*}
for scalars \(x_1, x_2, \ldots,
x_k\) has only the trivial solution%
\begin{equation*}
x_1 = x_2 = x_3 = \cdots = x_k = 0\text{.}
\end{equation*}
Another way to think about this is that a set of vectors is linearly independent if no vector in the set can be written as a linear combination of the other vectors in the set.%
\item{}A set \(\{\vv_1, \vv_2, \ldots, \vv_k\}\) of vectors in \(\R^n\) is linearly dependent if the vector equation%
\begin{equation*}
x_1 \vv_1 + x_2 \vv_2 + \cdots + x_k \vv_k = \vzero
\end{equation*}
has a nontrivial solution. That is, we can find scalars \(x_1, x_2, \ldots,
x_k\) that are not all 0 so that%
\begin{equation*}
x_1 \vv_1 + x_2 \vv_2 + \cdots + x_k \vv_k = \vzero\text{.}
\end{equation*}
Another way to think about this is that a set of vectors is linearly dependent if at least one vector in the set can be written as a linear combination of the other vectors in the set.%
\item{}If \(S\) is a set of vectors, a subset \(B\) of \(S\) is a basis for \(\Span \ S\) if \(B\) is a linearly independent set and \(\Span \ B = \Span \ S\).%
\item{}Given a nonzero set \(S\) of vectors, we can remove vectors from \(S\) that are linear combinations of remaining vectors in \(S\) to obtain a linearly independent subset of \(S\) that has the same span as \(S\).%
\item{}The columns of a matrix \(A\) are linearly independent if the equation \(A \vx = \vzero\) has only the trivial solution \(\vx = \vzero\).%
\item{}The set \(\{\vv_1, \vv_2, \ldots, \vv_k\}\) is linearly independent if and only if every column of the matrix \(A = [\vv_1 \ \vv_2 \ \vv_3 \ \cdots \ \vv_k]\), is a pivot column.%
\item{}If \(A = [\vv_1 \ \vv_2 \ \vv_3 \ \cdots \ \vv_k]\), then the vectors in the pivot columns of \(A\) form a minimal spanning set for \(\Span \{\vv_1, \vv_2, \ldots, \vv_k\}\).%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_indep_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp10955544}%
Consider the following vectors in \(\R^3\):%
\begin{equation*}
\vv_1 = \left[ \begin{array}{c} 1 \\ 1\\ 1 \end{array}  \right]\; , \; \vv_2 = \left[ \begin{array}{c} 1 \\ 2\\ 1 \end{array}  \right] \; , \; \vv_3= \left[ \begin{array}{c} 1 \\ 3\\ 1 \end{array}  \right]
\end{equation*}
Is the set consisting of these vectors linearly independent? If so, explain why. If not, make a single change in one of the vectors so that the set is linearly independent.%
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp10950808}%
Consider the following vectors in \(\R^3\):%
\begin{equation*}
\vv_1 = \left[ \begin{array}{c} 1 \\ 2\\ 1 \end{array}  \right]\; , \; \vv_2 = \left[ \begin{array}{r} 1 \\ -1\\ 2 \end{array}  \right] \; , \; \vv_3= \left[ \begin{array}{c} 1 \\ 1\\ c \end{array}  \right]
\end{equation*}
For which values of \(c\) is the set consisting of these vectors linearly independent?%
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp10963224}%
In a lab, there are three different water-benzene-acetic acid solutions: The first one with 36\% water, 50\% benzene and 14\% acetic acid; the second one with 44\% water, 46\% benzene and 10\% acetic acid; and the last one with 38\% water, 49\% benzene and 13\% acid. Since the lab needs space, the lab coordinator wants to determine whether all solutions are needed, or if it is possible to create one of the solutions using the other two. Can you help the lab coordinator?%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp10961944}%
Given vectors \(\vv_1= \left[ \begin{array}{c} 1 \\ 2\\ 3 \end{array} \right]\) and \(\vv_2 = \left[ \begin{array}{c} 0 \\ 2\\ 1 \end{array} \right]\), find a vector \(\vv_3\) in \(\R^3\) so that the set consisting of \(\vv_1, \vv_2\) and \(\vv_3\) is linearly independent.%
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp10960536}%
Consider the span of \(S=\{\vv_1, \vv_2, \vv_3, \vv_4\}\) where%
\begin{equation*}
\vv_1= \left[ \begin{array}{c} 1\\ 1\\ 1\\ 4 \end{array}  \right]\, ,\, \vv_2 = \left[ \begin{array}{c} 2\\1\\0\\3 \end{array}  \right]\, ,\, \vv_3 = \left[ \begin{array}{r} 3\\2\\-1\\1 \end{array}  \right]\, ,\, \vv_4 = \left[ \begin{array}{c} 3\\3\\1\\6 \end{array}  \right]\,\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Is the set \(S\) a minimal spanning set of \(\Span \ S\)? If not, determine a minimal spanning set, i.e. a basis, of \(\Span\ S\).%
\item{}Check that the vector \(\vu=\left[ \begin{array}{r} 6\\5\\-2\\1 \end{array} \right]\) is in \(\Span \ S\). Find the unique representation of \(\vu\) in terms of the basis vectors.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp10969368}%
Come up with a \(4\times 3\) matrix with linearly independent columns, if possible. If not, explain why not.%
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp10969880}%
Come up with a \(3\times 4\) matrix with linearly independent columns, if possible. If not, explain why not.%
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp10966936}%
Give an example of vectors \(\vv_1, \vv_2, \vv_3\) such that a minimal spanning set for \(\Span\{\vv_1, \vv_2, \vv_3\}\) is equal to that of \(\Span\{\vv_1, \vv_2\}\); and an example of three vectors \(\vv_1, \vv_2, \vv_3\) such that a minimal spanning set for \(\Span\{\vv_1, \vv_2, \vv_3\}\) is equal to that of \(\Span\{\vv_1, \vv_3\}\).%
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp10980120}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
If \(\vv_1\), \(\vv_2\) and \(\vv_3\) are three vectors none of which is a multiple of another, then these vectors form a linearly independent set.%
\item{}\lititle{True\slash{}False.}\par%
If \(\vv_1\), \(\vv_2\) and \(\vv_3\) in \(\R^n\) are linearly independent vectors, then so are \(\vv_1\), \(\vv_2\), \(\vv_3\) and \(\vv_4\) for any \(\vv_4\) in \(\R^n\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\vv_1\), \(\vv_2\), \(\vv_3\) and \(\vv_4\) in \(\R^n\) are linearly independent vectors, then so are \(\vv_1\), \(\vv_2\) and \(\vv_3\).%
\item{}\lititle{True\slash{}False.}\par%
A \(3\times 4\) matrix cannot have linearly independent columns.%
\item{}\lititle{True\slash{}False.}\par%
If two vectors span \(\R^2\), then they are linearly independent.%
\item{}\lititle{True\slash{}False.}\par%
The space \(\R^3\) cannot contain four linearly independent vectors.%
\item{}\lititle{True\slash{}False.}\par%
If two vectors are linearly dependent, then one is a scalar multiple of the other.%
\item{}\lititle{True\slash{}False.}\par%
If a set of vectors in \(\R^n\) is linearly dependent, then the set contains more than \(n\) vectors.%
\item{}\lititle{True\slash{}False.}\par%
The columns of a matrix \(A\) are linearly independent if the equation \(A\vx=\vzero\) has only the trivial solution.%
\item{}\lititle{True\slash{}False.}\par%
Let \(W = \Span\{\vv_1, \vv_2, \vv_3, \vv_4\}\). If \(\{\vv_1, \vv_2, \vv_3\}\) is a minimal spanning set for \(W\), then \(\{\vv_1, \vv_2, \vv_4\}\) cannot also be a minimal spanning set for \(W\).%
\item{}\lititle{True\slash{}False.}\par%
Let \(W = \Span\{\vv_1, \vv_2, \vv_3, \vv_4\}\). If \(\{\vv_1, \vv_2, \vv_3\}\) is a minimal spanning set for \(W\), then \(\{\vv_1, \vv_2\}\) cannot also be a minimal spanning set for \(W\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\vv_3=2\vv_1-3\vv_2\), then \(\{\vv_1, \vv_2\}\) is a minimal spanning set for \(\Span\{\vv_1, \vv_2, \vv_3\}\).%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Generating Bézier Curves}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Generating Bézier Curves}{}{Project: Generating Bézier Curves}{}{}{x:section:sec_proj_bezier}
Bézier curves can be created as linear combinations of vectors. In this section we will investigate how cubic Bézier curves (the ones used for fonts) can be realized through linear and quadratic Bézier curves. We begin with linear Bézier curves.%
\begin{project}{}{x:project:act_1_d_linear_Bezier}%
Start with two vectors \(\vp_0\) and \(\vp_1\). Linear Bézier curves are linear combinations%
\begin{equation*}
\vq = (1-t)\vp_0 + t\vp_1
\end{equation*}
of the vectors \(\vp_0\) and \(\vp_1\) for scalars \(t\) between 0 and 1. (You can visualize these linear combinations using the GeoGebra file \mono{Linear Bezier} at \href{https://www.geogebra.org/m/HvrPhh86}{\nolinkurl{geogebra.org/m/HvrPhh86}}. With this file you can draw the vectors \(\vq\) for varying values of \(t\). You can move the points \(\vp_0\) and \(\vp_1\) in the GeoGebra file, and the slider controls the values of \(t\). The point identified with \(\vq\) is traced as \(t\) is changed.) For this activity, we will see what the curve \(\vq\) corresponds to by evaluating certain points on the curve in a specific example. Let \(\vp_0 = \left[ \begin{array}{c} 2 \\ 1 \end{array}  \right]\) and \(\vp_1 = \left[ \begin{array}{c} 6 \\ 3 \end{array}  \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}What are the components of the vector \((1-t)\vp_0 + t\vp_1\) if \(t = \frac{1}{2}\)? Where is this vector in relation to \(\vp_0\) and \(\vp_1\)? Explain.%
\item{}What are the components of the vector \((1-t)\vp_0 + t\vp_1\) if \(t = \frac{1}{3}\)? Where is this vector in relation to \(\vp_0\) and \(\vp_1\)? Explain.%
\item{}What are the components of the vector \((1-t)\vp_0 + t\vp_1\) for an arbitrary \(t\)? Where is this vector in relation to \(\vp_0\) and \(\vp_1\)? Explain.%
\end{enumerate}
\end{project}%
For each value of \(t\), the vector \(\vq = (1-t)\vp_0 + t\vp_1\) is a linear combination of the vectors \(\vp_0\) and \(\vp_1\). Note that when \(t=0\), we have \(\vq = \vp_0\) and when \(t=1\) we have \(\vq = \vp_1\), and for \(0 \leq t \leq 1\) \hyperref[x:project:act_1_d_linear_Bezier]{Project Activity~{\xreffont\ref{x:project:act_1_d_linear_Bezier}}} shows that the vectors \(\vq\) trace out the line segment from \(\vp_0\) to \(\vp_1\). The span \(\{(1-t)\vp_0 + t\vp_1\}\) of the vectors \(\vp_0\) and \(\vp_1\) for \(0 \leq t \leq 1\) is a linear Bézier curve. Once we have a construction like this, it is natural in mathematics to extend it and see what happens. We do that in the next activity to construct quadratic Bézier curves.%
\begin{project}{}{x:project:act_1_d_quadratic_Bezier}%
Let \(\vp_0\), \(\vp_1\), and \(\vp_2\) be vectors in the plane. We can then let%
\begin{equation*}
\vq_0 = (1-t)\vp_0 + t\vp_1 \ \ \text{ and }  \ \  \vq_1 = (1-t)\vp_1 + t\vp_2
\end{equation*}
be the linear Bézier curves as defined in \hyperref[x:project:act_1_d_linear_Bezier]{Project Activity~{\xreffont\ref{x:project:act_1_d_linear_Bezier}}}. Since \(\vq_0\) and \(\vq_1\) are vectors, we can define \(\vr\) as%
\begin{equation*}
\vr = (1-t)\vq_0 + t\vq_1\text{.}
\end{equation*}
%
\par
(You can visualize these linear combinations using the GeoGebra file \mono{Quadratic Bezier} at \href{https://www.geogebra.org/m/VWCZZBXz}{\nolinkurl{geogebra.org/m/VWCZZBXz}}. With this file you can draw the vectors \(\vr\) for varying values of \(t\). You can move the points \(\vp_0\), \(\vp_1\), and \(\vp_2\) in the GeoGebra file, and the slider controls the values of \(t\). The point identified with \(\vr\) is traced as \(t\) is changed.) In this activity we investigate how the vectors \(\vr\) change as \(t\) changes. For the remainder of this activity, let \(\vp_0 = \left[ \begin{array}{c} 2 \\ 3 \end{array} \right]\), \(\vp_1 = \left[ \begin{array}{c} 8 \\ 4 \end{array} \right]\), and \(\vp_2 = \left[ \begin{array}{r} 6 \\ -3 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}At what point (in terms of \(\vp_0\), \(\vp_1\), and \(\vp_2\)) is the vector \(\vr = (1-t)\vq_0 + t\vq_1\) when \(t=0\)? Explain using the definition of \(\vr\).%
\item{}At what point (in terms of \(\vp_0\), \(\vp_1\), and \(\vp_2\)) is the vector \(\vr = (1-t)\vq_0 + t\vq_1\) when \(t=1\)? Explain using the definition of \(\vr\).%
\item{}Find by hand the components of the vector \((1-t)\vq_0 + t\vq_1\) with \(t = \frac{1}{4}\). Compare with the result of the GeoGebra file.%
\end{enumerate}
\end{project}%
The span \(\{(1-t)\vq_0 + t \vq_1\}\) of the vectors \(\vq_0\) and \(\vq_1\), or the set of points traced out by the vectors \(\vr\) for \(0 \leq t \leq 1\), is a quadratic Bézier curve. To understand why this curve is called quadratic, we examine the situation in a general context in the following activity.%
\begin{project}{}{x:project:act_1_d_quadratic_Bezier_general}%
Let \(\vp_0\), \(\vp_1\), and \(\vp_2\) be arbitrary vectors in the plane. Write \(\vr = (1-t)\vq_0 + t\vq_1\) as a linear combination of \(\vp_0\), \(\vp_1\), and \(\vp_2\). That is, write \(\vr\) in the form \(a_0 \vp_0 + a_1 \vp_1 + a_2 \vp_2\) for some scalars (that may depend on \(t\)) \(a_0\), \(a_1\), and \(a_2\). Explain why the result leads us to call these vectors \terminology{quadratic} Bézier curves.%
\end{project}%
Notice that if any one of the \(\vp_i\) lies on the line determined by the other two vectors, then the quadratic Bézier curve is just a line segment. So to obtain something non-linear we need to choose our vectors so that that doesn't happen.%
\par
Quadratic Bézier curves are limited, because their graphs are parabolas. For applications we need higher order Bézier curves. In the next activity we consider cubic Bézier curves.%
\begin{project}{}{x:project:act_1_d_cubic_Bezier}%
Start with four vectors \(\vp_0\), \(\vp_1\), \(\vp_2\), \(\vp_3\) \textemdash{} the points defined by these vectors are called \emph{control points} for the curve. As with the linear and quadratic Bézier curves, we let%
\begin{equation*}
\vq_0 = (1-t)\vp_0 + t\vp_1, \ \ \vq_1 = (1-t)\vp_1+t\vp_2,  \ \ \text{ and }  \ \  \vq_2 = (1-t)\vp_2 + t\vp_3\text{.}
\end{equation*}
%
\par
Then let%
\begin{equation*}
\vr_0 = (1-t)\vq_0 + t\vq_1 \ \ \text{ and }  \ \ \vr_1 = (1-t)\vq_1 + t\vq_2\text{.}
\end{equation*}
%
\par
We take this one step further to generate the cubic Bézier curves by letting%
\begin{equation*}
\vs = (1-t)\vr_0 + t\vr_1\text{.}
\end{equation*}
%
\par
(You can visualize these linear combinations using the GeoGebra file \mono{Cubic Bezier} at \href{https://www.geogebra.org/m/EDAhudy9}{\nolinkurl{geogebra.org/m/EDAhudy9}}. With this file you can draw the vectors \(\vs\) for varying values of \(t\). You can move the points \(\vp_0\), \(\vp_1\), \(\vp_2\), and \(\vp_3\) in the GeoGebra file, and the slider controls the values of \(t\). The point identified with \(\vs\) is traced as \(t\) is changed.) In this activity we investigate how the vectors \(\vs\) change as \(t\) changes. For the remainder of this activity, let \(\vp_0 = \left[ \begin{array}{c} 1 \\ 3 \end{array} \right]\), \(\vp_1 = \left[ \begin{array}{c} 4 \\ 5 \end{array} \right]\), \(\vp_2 = \left[ \begin{array}{r} 9 \\ -3 \end{array} \right]\), and \(\vp_3 = \left[ \begin{array}{c} 2 \\ 0 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}At what point (in terms of \(\vp_0\), \(\vp_1\), \(\vp_2\), and \(\vp_3\)) is the vector \(\vs = (1-t)\vr_0 + t\vr_1\) when \(t=0\)? Explain using the definition of \(\vs\).%
\item{}At what point (in terms of \(\vp_0\), \(\vp_1\), \(\vp_2\), and \(\vp_3\)) is the vector \(\vs = (1-t)\vr_0 + t\vr_1\) when \(t=1\)? Explain using the definition of \(\vs\).%
\item{}Find by hand the components of the vector \((1-t)\vr_0 + t\vr_1\) with \(t = \frac{3}{4}\). Compare with the result of the GeoGebra file.%
\end{enumerate}
\end{project}%
The span \(\{(1-t)\vr_0 + t \vr_1\}\) of the vectors \(\vr_0\) and \(\vr_1\), or the set of points traced out by the vectors \(\vs\) for \(0 \leq t \leq 1\), is a cubic Bézier curve. To understand why this curve is called cubic, we examine the situation in a general context in the following activity.%
\begin{project}{}{x:project:act_1_d_cubic_Bezier_general}%
Let \(\vp_0\), \(\vp_1\), \(\vp_2\), and \(\vp_3\) be arbitrary vectors in the plane. Write \(\vs = (1-t)\vr_0 + t\vr_1\) as a linear combination of \(\vp_0\), \(\vp_1\), \(\vp_2\), and \(\vp_3\). That is, write \(\vs\) in the form \(b_0 \vp_0 + b_1 \vp_1 + b_2 \vp_2 + b_3 \vp_3\) for some scalars (that may depend on \(t\)) \(b_0\), \(b_1\), \(b_2\), and \(b_3\). Explain why the result leads us to call these vectors \terminology{cubic} Bézier curves.%
\end{project}%
Just as with the quadratic case, we need certain subsets of the set of control vectors to be linearly independent so that the cubic Bézier curve does not degenerate to a quadratic or linear Bézier curve.%
\par
More complicated and realistic shapes can be represented by piecing together two or more Bézier curves as illustrated with the letter ``S'' in \hyperref[x:figure:F_Letter_S]{Figure~{\xreffont\ref{x:figure:F_Letter_S}}}. Suppose we have two cubic Bézier curves, the first with control points \(\vp_0\), \(\vp_1\), \(\vp_2\), and \(\vp_3\) and the second with control points \(\vp_0'\), \(\vp_1'\), \(\vp_2'\), and \(\vp_3'\). You may have noticed that \(\vp_1\) lies on the tangent line to the first Bézier curve at \(\vp_0\) and that \(\vp_2\) lies on the tangent line to the first Bézier curve at \(\vp_3\). (Play around with the program \mono{Cubic Bezier} to convince yourself of these statements. This can be proved in a straightforward manner using vector calculus.) So if we want to make a smooth curve from these two Bézier curves, the curves will need to join together smoothly at \(\vp_3\) and \(\vp_0'\). This will force \(\vp_3 = \vp_0'\) and the tangents at \(\vp_3 = \vp_0'\) will have to match. This implies that \(\vp_2\), \(\vp_3\), and \(\vp_1'\) all have to lie on this common tangent line. Keeping this idea in mind, use the GeoGebra file \mono{Cubic Bezier Pair} at \href{https://www.geogebra.org/m/UwxQ6RPk}{\nolinkurl{geogebra.org/m/UwxQ6RPk}} to find control points for the pair of Bézier curves that create your own letter S.%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 7 Matrix Transformations}
\typeout{************************************************}
%
\begin{chapterptx}{Matrix Transformations}{}{Matrix Transformations}{}{}{x:chapter:chap_matrix_transformations}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp11090584}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What is a matrix transformation?%
\item{}What properties do matrix transformations have? (In particular, what properties make matrix transformations \emph{linear}?)%
\item{}What is the domain of a matrix transformation defined by an \(m \times n\) matrix? Why?%
\item{}What are the range and codomain of a matrix transformation defined by an \(m \times n\) matrix? Why?%
\item{}What does it mean for a matrix transformation to be one-to-one? If \(T\) is a matrix transformation represented as \(T(\vx) = A \vx\), what are the conditions on \(A\) that make \(T\) a one-to-one transformation?%
\item{}What does it mean for a matrix transformation to be onto? If \(T\) is a matrix transformation represented as \(T(\vx) = A \vx\), what are the conditions on \(A\) that make \(T\) an onto transformation?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Computer Graphics}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Computer Graphics}{}{Application: Computer Graphics}{}{}{x:section:sec_appl_graphics}
As we will discuss, left multiplication by an \(m \times n\) matrix defines a function from \(\R^n\) to \(\R^m\). Such a function defined by matrix multiplication is called a matrix transformation. In this section we study some of the properties of matrix transformations and understand how, using the pivots of the matrix, to determine when the output of a matrix transformation covers the whole space \(R^m\) or when a transformation maps distinct vectors to distinct outputs.%
\par
Matrix transformations are used extensively in computer graphics to produce animations as seen in video games and movies. For example, consider the dancing figure at left in \hyperref[x:figure:F_Rotate_dance]{Figure~{\xreffont\ref{x:figure:F_Rotate_dance}}}. We can identify certain control points (e.g., the point at the neck, where the arms join the torso, etc.) to mark the locations of important points. Using just the control points we can reconstruct the figure. Each control point can be represented as a vector, and so we can manipulate the figure by manipulating the control points with matrix transformations. We will explore this idea in more detail later in this section.%
\begin{figureptx}{A dancing figure and a rotated dancing figure.}{x:figure:F_Rotate_dance}{}%
\begin{image}{0.25}{0.5}{0.25}%
\includegraphics[width=\linewidth]{external/1_g_dancer_rotate.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_mtx_trans_intro}
In this section we will consider special functions which take vectors as inputs and produce vectors as outputs. We will use matrix multiplication to produce the output vectors.%
\par
If \(A\) is an \(m \times n\) matrix and \(\vx\) is a vector in \(\R^n\), then the matrix-vector product \(A \vx\) is a vector in \(\R^m\). (Pick some specific \(n, m\) values to understand this statement better.) Therefore, left multiplication by the matrix \(A\) takes an input vector \(\vx\) in \(\R^n\) and produces an output vector \(A\vx\) in \(\R^m\), which we will refer to as the \terminology{image} of \(\vx\) under the transformation. This defines a function \(T\) from \(\R^n\) to \(\R^m\) where%
\begin{equation*}
T(\vx) = A \vx \,\text{.}
\end{equation*}
%
\par
These functions are the matrix transformations.%
\begin{definition}{}{g:definition:idp11119000}%
\index{matrix transformation}%
\index{transformation!matrix}%
A \terminology{matrix transformation} is a function \(T: \R^n \to \R^m\) defined by%
\begin{equation*}
T(\vx) = A\vx
\end{equation*}
for some \(m \times n\) matrix \(A\).%
\end{definition}
Many of the transformations we consider in this section are from \(\R^2\) to \(\R^2\) so that we can visualize the transformations. As an example, let us consider the transformation \(T\) defined by%
\begin{equation*}
T\left(\left[ \begin{array}{c} x_1 \\ x_2 \end{array}  \right] \right) = \left[ \begin{array}{cr} 1\amp 0 \\ 0\amp -1 \end{array}  \right]\left[ \begin{array}{c} x_1 \\ x_2 \end{array}  \right]\text{.}
\end{equation*}
%
\par
If we plot the input vectors \(\vu_1 = \left[ \begin{array}{c}  1 \\ 0 \end{array}  \right]\), \(\vu_2 = \left[ \begin{array}{c}  0\\1 \end{array}  \right]\), \(\vu_3 = \left[ \begin{array}{c} 1  \\ 2 \end{array}  \right]\), and \(\vu_4 = \left[ \begin{array}{r} -1  \\ 1 \end{array}  \right]\) (as (blue) circles) and their images \(T(\vu_1) = \left[ \begin{array}{cr} 1\amp 0 \\ 0\amp -1 \end{array}  \right]\left[ \begin{array}{c} 1 \\ 0 \end{array}  \right] = \left[ \begin{array}{c} 1 \\ 0 \end{array}  \right]\), \(T(\vu_2) = \left[ \begin{array}{r} 0 \\ -1 \end{array}  \right]\), \(T(\vu_3) = \left[ \begin{array}{r} 1 \\ -2 \end{array}  \right]\), and \(T(\vu_4) = \left[ \begin{array}{r} -1 \\ -1 \end{array}  \right]\) (as (red) \texttimes{}'s) on the same set of axes as shown in \hyperref[x:figure:F_PA_1g_1]{Figure~{\xreffont\ref{x:figure:F_PA_1g_1}}}, we see that this transformation reflects the input vectors across the \(x\)-axis. We can also see this algebraically since the reflection of the point \((x_1,x_2)\) around the \(x\)-axis is the point \((x_1,-x_2)\), and%
\begin{equation*}
T\left( \left[ \begin{array}{c} x_1 \\ x_2 \end{array}  \right] \right) = \left[ \begin{array}{r} x_1 \\ -x_2 \end{array}  \right]\text{.}
\end{equation*}
%
\begin{figureptx}{Inputs and outputs of the transformation \(T\).}{x:figure:F_PA_1g_1}{}%
\begin{image}{0.25}{0.5}{0.25}%
\includegraphics[width=\linewidth]{external/1g_pa1_b.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\begin{exploration}{}{x:exploration:pa_1_g}%
We now consider other transformations from \(\R^2\) to \(\R^2\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Suppose a transformation \(T\) is defined by%
\begin{equation*}
T\left( \left[ \begin{array}{c}  x_1\\x_2 \end{array}  \right] \right) = \left[ \begin{array}{cc} 2\amp 0 \\ 0\amp 2 \end{array}  \right] \left[ \begin{array}{c}  x_1 \\ x_2 \end{array}  \right]\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Find \(T(\vu_i)\) for each of \(\vu_1 = \left[ \begin{array}{c} 1 \\ 0 \end{array} \right]\), \(\vu_2 = \left[ \begin{array}{c} 0\\1 \end{array} \right]\), \(\vu_3 = \left[ \begin{array}{c} 1 \\ 2 \end{array} \right]\), and \(\vu_4 = \left[ \begin{array}{r} -1 \\ 1 \end{array} \right]\). (In other words, substitute \(\vu_1, \vu_2, \vu_3, \vu_4\) into the formula above to see what output is obtained.)%
Plot all input vectors and their images on the same axes in \(\R^2\). Clearly identify which image corresponds to which input vector. Then give a geometric description of what this transformation does.%
\end{enumerate}
\item{}The transformation in the introduction performs a reflection across the \(x\)-axis. Find a matrix transformation that performs a reflection across the \(y\)-axis.%
\item{}Suppose a transformation \(T\) is defined by%
\begin{equation*}
T(\vx) = A\vx\text{,}
\end{equation*}
where%
\begin{equation*}
A = \left[ \begin{array}{cc} 1\amp 0 \\ 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Find \(T(\vu_i)\) for each of \(\vu_1 = \left[ \begin{array}{c} 1 \\ 0 \end{array} \right]\), \(\vu_2 = \left[ \begin{array}{c} 0\\1 \end{array} \right]\), \(\vu_3 = \left[ \begin{array}{c} 1 \\ 2 \end{array} \right]\), and \(\vu_4 = \left[ \begin{array}{r} -1 \\ 1 \end{array} \right]\).%
\item{}Plot all input vectors and their images on the same axes in \(\R^2\). Give a geometric description of this transformation.%
\item{}Is there an input vector which produces \(\vb = \left[ \begin{array}{c} 1 \\ 1 \end{array} \right]\) as an output vector?%
\item{}Find all input vectors that produce the output vector \(\vb = \left[ \begin{array}{c} 1 \\ 0 \end{array} \right]\). Is there a unique input vector, or multiple input vectors?%
\end{enumerate}
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Properties of Matrix Transformations}
\typeout{************************************************}
%
\begin{sectionptx}{Properties of Matrix Transformations}{}{Properties of Matrix Transformations}{}{}{x:section:sec_mtx_trans_prop}
\index{domain of a matrix transformation}%
\index{codomain of a matrix transformation}%
\index{range of a matrix transformation}%
\index{image of an element under a transformation}%
A matrix transformation is a function. When dealing with functions in previous mathematics courses we have used the terms domain and range with our functions. Recall that the domain of a function is the set of all allowable inputs into the function and the range of a function is the set of all outputs of the function. We do the same with transformations. If \(T\) is the matrix transformation \(T(\vx) = A \vx\) for some \(m \times n\) matrix \(A\), then \(T\) maps vectors from \(\R^n\) into \(\R^m\). So \(\R^n\) is the \terminology{domain} of \(T\) \textemdash{} the set of all input vectors. However, the set \(\R^m\) is only the target set for \(T\) and not necessarily the range of \(T\). We call \(\R^m\) the \terminology{codomain} of \(T\), while the \terminology{range} of \(T\) is the set of all output vectors. The range of a transformation will come up later, so we make a formal definition.%
\begin{definition}{}{g:definition:idp11349896}%
\index{range of a matrix transformation}%
The \terminology{range} of a matrix transformation \(T\) from \(\R^n\) to \(\R^m\) is the set%
\begin{equation*}
\Range{T} = \{T(\vx) : \vx \in \R^n\}\text{.}
\end{equation*}
%
\end{definition}
\index{image of an element under a transformation}\index{image of a matrix transformation} The range is always a subset of the codomain, but the two sets do not have to be equal. In addition, if a vector \(\vb\) in \(\R^m\) satisfies \(\vb = T(\vx)\) for some \(\vx\) in \(\R^n\), then we say that \(\vb\) is the \terminology{image} of \(\vx\) under the transformation \(T\). For this reason we also use the term \terminology{image} to also mean the range of a transformation, with the corresponding notation \(\Image(T)\).%
\par
Because of the properties of the matrix-vector product, if the matrix transformation \(T\) is defined by \(T(\vx) = A\vx\) for some \(m \times n\) matrix \(A\), then%
\begin{equation*}
T(\vu + \vv) =  A(\vu + \vv) = A\vu + A\vv
\end{equation*}
and%
\begin{equation*}
T(c\vu) = A(c\vu) = cA\vu = cT(\vu)
\end{equation*}
for any vectors \(\vu\) and \(\vv\) in \(\R^n\) and any scalar \(c\). So every matrix transformation \(T\) satisfies the following two important properties:%
\begin{enumerate}
\item{}\(T(\vu + \vv) = T(\vu) + T(\vv)\) and%
\item{}\(T(c\vu) = c T(\vv)\).%
\end{enumerate}
%
\par
The first property says that a matrix transformation \(T\) preserves sums of vectors and the second that \(T\) preserves scalar multiples of vectors.%
\begin{activity}{}{x:activity:act_1_g_2}%
Let \(T\) be a matrix transformation, and let \(\vu\) and \(\vv\) be vectors in the domain of \(T\) so that \(T(\vu) = \left[ \begin{array}{c} 1 \\ 2 \\ 0 \end{array} \right]\) and \(T(\vv) = \left[ \begin{array}{r} -3 \\ 1 \\ 4 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Exactly which vector is \(T(2\vu - 3\vv)\)? Explain.%
\item{}If \(a\) and \(b\) are any scalars, what is the vector \(T(a\vu + b\vv)\)? Why?%
\end{enumerate}
\end{activity}%
As we saw in \hyperref[x:activity:act_1_g_2]{Activity~{\xreffont\ref{x:activity:act_1_g_2}}}, we can combine the two properties of a matrix transformation \(T\) into one: for any scalars \(a\) and \(b\) and any vectors \(\vu\) and \(\vv\) in the domain of \(T\) we have%
\begin{equation}
T(a\vu+b\vv) = aT(\vu) + bT(\vv)\text{.}\label{x:men:eq_1_g_1}
\end{equation}
%
\par
We can then extend equation \hyperref[x:men:eq_1_g_1]{({\xreffont\ref{x:men:eq_1_g_1}})} (by mathematical induction) to any finite linear combination of vectors. That is, if \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) are any vectors in the domain of a matrix transformation \(T\) and if \(x_1\), \(x_2\), \(\ldots\), \(x_k\) are any scalars, then%
\begin{equation}
T(x_1 \vv_1 + x_2 \vv_2 + \cdots + x_k \vv_k) = x_1 T(\vv_1) + x_2 T(\vv_2) + \cdots + x_k T(\vv_k)\text{.}\label{x:men:eq_1_g_2}
\end{equation}
%
\par
In other words, a matrix transformation preserves linear combinations. For this reason matrix transformations are examples of a larger set of transformation that are called \terminology{linear} transformations. We will discuss general linear transformations in a later section.%
\par
There is one other important property of a matrix transformation for us to consider. The functions we encountered in earlier mathematics courses, e.g., \(f(x) = 2x+1\), could send the input 0 to any output. However, as a consequence of the definition, any matrix transformation \(T\) maps the zero vector to the zero vector because%
\begin{equation*}
T(\vzero) = A\vzero = \vzero \,\text{.}
\end{equation*}
%
\par
Note that the two vectors \(\vzero\) in the last equation may not be the same vector \textemdash{} if \(T : \R^n \to \R^m\), then the first \(\vzero\) is in \(\R^n\) and the second in \(\R^m\). It should be clear from the context which vector \(\vzero\) is meant.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Onto and One-to-One Transformations}
\typeout{************************************************}
%
\begin{sectionptx}{Onto and One-to-One Transformations}{}{Onto and One-to-One Transformations}{}{}{x:section:sec_trans_onto_oto}
The problems we have been asking about solutions to systems of linear equations can be rephrased in terms of matrix transformations. The question about whether a system \(A \vx = \vb\) is consistent for any vector \(\vb\) is also a question about the existence of a vector \(\vx\) so that \(T(\vx) = \vb\), where \(T\) is the matrix transformation defined by \(T(\vx) = A \vx\).%
\begin{activity}{}{x:activity:act_1_g_4}%
Let \(T\) be the matrix transformation defined by \(T(\vx) = A\vx\) where \(A\) is%
\begin{equation*}
\left[ \begin{array}{rc} 1\amp 0 \\ 0\amp 1 \\ 0\amp 2 \end{array}  \right]\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find \(T\left(\left[\begin{array}{c} 1\\1 \end{array} \right]\right)\) and \(T\left(\left[\begin{array}{c} 1\\1\\1 \end{array} \right]\right)\). If it is not possible to find one or both of the output vectors, indicate why.%
\item{}What are the domain and codomain of \(T\)? Why? (Recall that the domain is the space of all input vectors, while the codomain is the space in which the output vectors are contained.)%
\item{}Can you find a vector \(\vx\) for which \(T(\vx)= \left[\begin{array}{c} 2\\3\\6 \end{array} \right]\)? Can you find a vector \(\vx\) for which \(T(\vx)=\left[\begin{array}{c} 2\\3\\1 \end{array} \right]\)?%
\item{}Which \(\vb=\left[\begin{array}{c} a\\b\\c \end{array} \right]\) are the image vectors for this transformation? Is the range of \(T\) equal to the codomain of \(T\)? Explain.%
\item{}The previous question can be rephrased as a matrix equation question. We are asking whether \(A \vx = \vb\) is consistent for every \(\vb\). How is the answer to this question related to the pivots of \(A\)?%
\end{enumerate}
\end{activity}%
If \(T\) is a matrix transformation, \hyperref[x:activity:act_1_g_4]{Activity~{\xreffont\ref{x:activity:act_1_g_4}}} illustrates that the range of a matrix transformation \(T\) may not equal its codomain. In other words, there may be vectors \(\vb\) in the codomain of \(T\) that are not the image of any vector in the domain of \(T\). If it is the case for a matrix transformation \(T\) that there is always a vector \(\vx\) in the domain of \(T\) such that \(T(\vx) = \vb\) for any vector \(\vb\) in the codomain of \(T\), then \(T\) is given a special name.%
\begin{definition}{}{g:definition:idp11398152}%
\index{onto}%
A matrix transformation \(T\) from \(\R^n\) to \(\R^m\) is \terminology{onto} if each \(\vb\) in \(\R^m\) is the image of \terminology{at least one} \(\vx\) in \(\R^n\).%
\end{definition}
So the matrix transformation \(T\) from \(\R^n\) to \(\R^m\) defined by \(T(\vx) = A\vx\) is onto if the equation \(A \vx = \vb\) has a solution for each vector \(\vb\) in \(\R^m\). Since the vectors \(A\vx\) are linear combinations of the columns of \(A\), \(T\) is onto exactly when the span of the columns of \(A\) is all of \(\R^m\). \hyperref[x:activity:act_1_g_4]{Activity~{\xreffont\ref{x:activity:act_1_g_4}}} shows us that \(T\) is onto if every row of \(A\) contains a pivot.%
\par
Another question to ask about matrix transformations is how many vectors there can be that map onto a given output vector.%
\begin{activity}{}{x:activity:act_1_g_5}%
Let \(T\) be the matrix transformation defined by \(T(\vx) = A \vx\) where \(A\) is%
\begin{equation*}
\left[ \begin{array}{ccc} 1 \amp  3 \amp  0 \\ 0 \amp  0 \amp  1 \end{array}  \right]\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find \(T\left(\left[\begin{array}{c} 1\\1 \end{array} \right]\right)\) and \(T\left(\left[\begin{array}{c} 1\\1\\1 \end{array} \right]\right)\). If it is not possible to find one or both of the output vectors, indicate why.%
\item{}What are the domain and codomain of \(T\)? Why?%
\item{}Find \(T\left(\left[\begin{array}{c} 1\\1\\2 \end{array} \right]\right)\). Are there any other \(\vx\)'s for which \(T(\vx)\) is this same output vector?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp11421064}{}\quad{}Set up an equation to solve for such \(\vx\)'s.%
\item{}Assume more generally that for some vector \(\vb\), there is a vector \(\vx\) so that \(T(\vx) = \vb\). Write this as a matrix equation to determine how many solutions this equation has. Explain. How is the answer to this question related to the pivots of \(A\)?%
\end{enumerate}
\end{activity}%
The uniqueness of a solution to \(A \vx = \vb\) is the same as saying that the matrix transformation \(T\) defined by \(T(\vx) = A\vx\) maps exactly one vector to \(\vb\). A matrix transformation \(T\) that has the property that every image vector is an image in exactly one way is also a special type of transformation.%
\begin{definition}{}{g:definition:idp11417224}%
\index{one-to-one}%
A matrix transformation \(T\) from \(\R^n\) to \(\R^m\) is \terminology{one-to-one} if each \(\vb\) in \(\R^m\) is the image of \terminology{at most} one \(\vx\) in \(\R^n\).%
\end{definition}
So the matrix transformation \(T\) from \(\R^n\) to \(\R^m\) defined by \(T(\vx) = A\vx\) is one-to-one if the equation \(A \vx = \vb\) has a unique solution whenever \(A \vx = \vb\) is consistent. Since the vectors \(A\vx\) are linear combinations of the columns of \(A\), the unique solution requirement indicates that any output vector can be written in exactly one way as a linear combination of the columns of \(A\). This implies that the columns of \(A\) are linearly independent. \hyperref[x:activity:act_1_g_5]{Activity~{\xreffont\ref{x:activity:act_1_g_5}}} indicates that this happens when every column of \(A\) is a pivot column.%
\par
To summarize, if \(T\) is a matrix transformation defined by \(T(\vx) = A \vx\), then \(T\) is onto if every row of \(A\) contains a pivot, and \(T\) is one-to-one if every column of \(A\) is a pivot column. It is important to note the difference: being one-to-one depends on the rows of \(A\) and being onto depends on the columns of \(A\).%
\par
Having a matrix transformation from \(\R^n\) to \(\R^m\) can tell us things about \(m\) and \(n\). For example, when a matrix transformation from \(\R^n\) to \(\R^m\) is one-to-one, it means that there is a unique input vector for every output vector. Since a matrix transformation preserves the algebraic structure of \(\R^n\), this implies that the collection of the images of the vectors in the domain of \(T\) form a copy of \(\R^n\) inside of \(\R^m\). If we think of \(T\) as a one-to-one matrix transformation with \(T(\vx) = A \vx\) for some \(m \times n\) matrix, then every column of \(A\) will have to be a pivot column. It follows that if there is a one-to-one matrix transformation from \(\R^n\) to \(\R^m\), we must have \(m \geq n\). Similarly, if a matrix transformation \(T\) from \(\R^n\) to \(\R^m\) is onto, then for each \(\vb\) in \(\R^m\), if we select one vector in the domain of \(T\) whose image is \(\vb\), then the collection of these vectors in the domain of \(T\) is a copy of \(\R^m\) inside of \(\R^n\). So if there is an onto matrix transformation from \(\R^n\) to \(\R^m\), then \(n \geq m\). As a consequence, the only way a matrix transformation from \(\R^n\) to \(\R^m\) is both one-to-one and onto is if \(n = m\).%
\par
We conclude this section by adding new equivalent conditions to \hyperref[x:theorem:thm_IMT_1_e]{Theorem~{\xreffont\ref{x:theorem:thm_IMT_1_e}}} and \hyperref[x:theorem:thm_IMT_1_f]{Theorem~{\xreffont\ref{x:theorem:thm_IMT_1_f}}} from \hyperref[x:chapter:chap_matrix_vector]{Section~{\xreffont\ref{x:chapter:chap_matrix_vector}}} and \hyperref[x:chapter:chap_independence]{Section~{\xreffont\ref{x:chapter:chap_independence}}}.%
\begin{theorem}{}{}{x:theorem:thm_IMT_1_g_a}%
Let \(A\) be an \(m \times n\) matrix. The following statements are equivalent.%
\begin{enumerate}
\item{}The matrix equation \(A \vx = \vb\) has a solution for every vector \(\vb\) in \(\R^m\).%
\item{}Every vector \(\vb\) in \(\R^m\) can be written as a linear combination of the columns of \(A\).%
\item{}The span of the columns of \(A\) is \(\R^m\).%
\item{}The matrix \(A\) has a pivot position in each row.%
\item{}The matrix transformation \(T\) from \(\R^n\) to \(\R^m\) defined by \(T(\vx) = A\vx\) is onto.%
\end{enumerate}
%
\end{theorem}
\begin{theorem}{}{}{x:theorem:thm_IMT_1_g_b}%
Let \(A\) be an \(m \times n\) matrix. The following statements are equivalent.%
\begin{enumerate}
\item{}The matrix equation \(A \vx = \vb\) has a unique solution for every vector \(\vb\) in the span of the columns of \(A\).%
\item{}The matrix equation \(A \vx = \vzero\) has the unique solution \(\vx = \vzero\).%
\item{}The columns of \(A\) are linearly independent.%
\item{}The matrix \(A\) has a pivot position in each column.%
\item{}The matrix transformation \(T\) from \(\R^n\) to \(\R^m\) defined by \(T(\vx) = A\vx\) is one-to-one.%
\end{enumerate}
%
\end{theorem}
We will continue to add to these theorems, which will eventually give us many different but equivalent perspectives to look at a linear algebra problem. Please keep these equivalent criteria in mind when considering the best possible approach to a problem.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_mtx_trans_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp11466632}%
Let \(A = \left[ \begin{array}{crrr} 1\amp 1\amp -1\amp -1 \\ 3\amp 6\amp 0\amp 3 \\ 2\amp -1\amp -5\amp -8 \end{array} \right]\) and let \(T(\vx) = A\vx\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Identify the domain of \(T\). Explain your reasoning.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp11473928}{}\quad{}Since \(A\) is a \(3 \times 4\) matrix, \(A\) has four columns. Now \(A \vx\) is a linear combination of the columns of \(A\) with weights from \(\vx\), so \(\vx\) must have four entries to correspond to the columns of \(A\). We conclude that the domain of \(T\) is \(\R^4\).%
\item{}Is \(T\) one-to-one. Explain.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp11482504}{}\quad{}Technology shows that the reduced row echelon form of \(A\) is%
\begin{equation*}
\left[ \begin{array}{ccrr} 1\amp 0\amp -2\amp -3 \\ 0\amp 1\amp 1\amp 2 \\ 0\amp 0\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
Since \(A\) contains non-pivot columns, the homogeneous system \(A \vx = \vzero\) has infinitely many solutions. So \(T\) is not one-to-one. In other words, if there is a column of \(A\) that is a non-pivot column, then \(A\) is not one-to-one.%
\item{}Is \(T\) onto? If yes, explain why. If no, describe the range of \(T\) as best you can, both algebraically and graphically.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp11482120}{}\quad{}Since the reduced row echelon form of \(A\) has rows of zeros, there will be vectors \(\vb\) in \(\R^3\) such that the reduced row echelon form of \([A \ \vb]\) will have a row of the form \([0 \ 0 \ 0 \ 0 \ c]\) for some nonzero scalar \(c\). This means that \(T(\vx) = A\vx = \vb\) will have no solution and \(T\) is not onto. In other words, if there is a row of \(A\) that does not contain a pivot, then \(T\) is not onto.%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp11487240}%
A matrix transformation \(T: \R^2 \to \R^2\) defined by%
\begin{equation*}
T\left(\left[ \begin{array}{c} x\\y \end{array}  \right] \right) = \left[ \begin{array}{c} cx\\y \end{array}  \right]
\end{equation*}
is a contraction in the \(x\) direction if \(0 \lt  c \lt  1\) and a dilation in the \(x\) direction if \(c>1\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find a matrix \(A\) such that \(T(\vx) = A\vx\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp11498632}{}\quad{}Since%
\begin{equation*}
\left[ \begin{array}{cc} c\amp 0\\0\amp 1 \end{array} \right] \left[ \begin{array}{c} x\\y \end{array}  \right] = \left[ \begin{array}{c} cx\\y \end{array}  \right]\text{,}
\end{equation*}
the matrix \(A = \left[ \begin{array}{cc} c\amp 0\\0\amp 1 \end{array} \right]\) has the property that \(T(\vx) = A\vx\).%
\item{}Sketch the square \(S\) with vertices \(\vu_1 = \left[ \begin{array}{c} 0\\0 \end{array} \right]\), \(\vu_2 = \left[ \begin{array}{c} 1\\0 \end{array} \right]\), \(\vu_3 = \left[ \begin{array}{c} 1\\1 \end{array} \right]\), and \(\vu_4 = \left[ \begin{array}{c} 0\\1 \end{array} \right]\). Determine and sketch the image of \(S\) under \(T\) if \(c = 2\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp11492744}{}\quad{}We can determine the image of \(S\) under \(T\) by calculating what \(T\) does to the vertices of \(S\). Notice that%
\begin{align*}
T(\vu_1) \amp = \left[ \begin{array}{cc} 2\amp 0\\
0\amp 1 \end{array}\right] \left[ \begin{array}{c} 0\\
0 \end{array} \right] = \left[ \begin{array}{c} 0\\
0 \end{array} \right]\\
T(\vu_2) \amp = \left[ \begin{array}{cc} 2\amp 0\\
0\amp 1 \end{array}\right] \left[ \begin{array}{c} 1\\
0 \end{array} \right] = \left[ \begin{array}{c} 2\\
0 \end{array} \right]\\
T(\vu_3) \amp = \left[ \begin{array}{cc} 2\amp 0\\
0\amp 1 \end{array}\right] \left[ \begin{array}{c} 1\\
1 \end{array} \right] = \left[ \begin{array}{c} 2\\
1 \end{array} \right]\\
T(\vu_4) \amp = \left[ \begin{array}{cc} 2\amp 0\\
0\amp 1 \end{array}\right] \left[ \begin{array}{c} 0\\
1 \end{array} \right] = \left[ \begin{array}{c} 0\\
1 \end{array} \right]
\end{align*}
Since \(T\) is a linear map, the image of \(S\) under \(T\) is the polygon with vertices \((0,0)\), \((1,0)\), \((2,1)\), and \((0,1)\) as shown in \hyperref[x:figure:F_ex_1_g_polygons]{Figure~{\xreffont\ref{x:figure:F_ex_1_g_polygons}}}. From \hyperref[x:figure:F_ex_1_g_polygons]{Figure~{\xreffont\ref{x:figure:F_ex_1_g_polygons}}} we can see that \(T\) stretches the figure in the \(x\) direction only by a factor of 2. \begin{figureptx}{The input square \(S\) and the output \(T(S)\).}{x:figure:F_ex_1_g_polygons}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/1_g_polygons.pdf}
\end{image}%
\tcblower
\end{figureptx}%
%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_mtx_trans_summ}
In this section we determined how to represent any matrix transformation from \(\R^n\) to \(\R^m\) as a matrix transformation, and what it means for a matrix transformation to be one-to-one and onto.%
\begin{itemize}[label=\textbullet]
\item{}A matrix transformation is a function \(T: \R^n \to \R^m\) defined by \(T(\vx) = A\vx\) for some \(m \times n\) matrix \(A\).%
\item{}A matrix transformation \(T\) from \(\R^n\) to \(\R^m\) satisfies%
\begin{equation*}
T(a\vu + b\vv) = aT(\vu) + bT(\vv)
\end{equation*}
for any scalars \(a\) and \(b\) and any vectors \(\vu\) and \(\vv\) in \(\R^n\). The fact that \(T\) preserves linear combinations is why we say that \(T\) is a linear transformation.%
\item{}An \(m \times n\) matrix \(A\) defines the matrix transformation \(T\) via%
\begin{equation*}
T(\vx) = A \vx\text{.}
\end{equation*}
The domain of this transformation is \(\R^n\) because the matrix-vector product \(A \vx\) is only defined if \(\vx\) is an \(n \times 1\) vector.%
\item{}If \(A\) is an \(m \times n\) matrix, then the codomain of the matrix transformation \(T\) defined by \(T(\vx) = A \vx\) is \(\R^m\). This is because the matrix-vector product \(A \vx\) with \(\vx\) an \(n \times 1\) vector is an \(m \times 1\) vector. The range of \(T\) is the subset of the codomain of \(T\) consisting of all vectors of the form \(T(\vx)\) for vectors \(\vx\) in the domain of \(T\).%
\item{}A matrix transformation \(T\) from \(\R^n\) to \(\R^m\) is \terminology{one-to-one} if each \(\vb\) in \(\R^m\) is the image of \emph{at most} one \(\vx\) in \(\R^n\). If \(T\) is a matrix transformation represented as \(T(\vx) = A\vx\), then \(T\) is one-to-one if each column of \(A\) is a pivot column, or if the columns of \(A\) are linearly independent.%
\item{}A matrix transformation \(T\) from \(\R^n\) to \(\R^m\) is \terminology{onto} if each \(\vb\) in \(\R^m\) is the image of \emph{at least one} \(\vx\) in \(\R^n\). If \(T\) is a matrix transformation represented as \(T(\vx) = A\vx\), then \(T\) is onto if each row of \(A\) contains a pivot position, or if the span of the columns of \(A\) is all of \(\R^m\).%
\end{itemize}
%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_mtx_trans_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp11547784}%
Given matrix \(A=\left[ \begin{array}{ccr} 1\amp 2 \amp 1 \\ 1 \amp 0 \amp -3 \end{array} \right]\), write the coordinate form of the transformation \(T\) defined by \(T(\vx)=A\vx\). (Note: Writing a transformation in coordinate form refers to writing the transformation in terms of the entries of the input and output vectors.)%
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp11543048}%
Suppose the transformation \(T\) is defined by \(T(\vx)=A\vx\) where%
\begin{equation*}
A=\left[ \begin{array}{ccr} 1\amp 1\amp -1 \\ 2\amp 1\amp 1 \\ 4\amp 1\amp 4 \end{array}  \right]\text{.}
\end{equation*}
Determine if \(\vb=\left[ \begin{array}{c} 1\\0\\0 \end{array}  \right]\) is in the range of \(T\). If so, find all \(\vx\)'s which map to \(\vb\).%
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp11547528}%
Suppose \(T\) is a matrix transformation and%
\begin{equation*}
T(\vv_1) = \left[ \begin{array}{c} 1 \\ 2 \end{array}  \right] \, , \, T(\vv_2) = \left[ \begin{array}{r} -2 \\ 3 \end{array}  \right]
\end{equation*}
Find \(T(2\vv_1 -5 \vv_2)\).%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp11548552}%
Given a matrix transformation defined as%
\begin{equation*}
T\left( \left[ \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array}  \right] \right) = \left[ \begin{array}{c} 2x_1 -x_3 \\ -x_1 +2x_2 +x_3 \\ 3x_2 - 4x_3 \end{array}  \right]
\end{equation*}
determine the matrix \(A\) for which \(T(\vx) = A\vx\).%
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp11554440}%
Suppose a matrix transformation \(T\) defined by \(T(\vx)=A\vx\) for some unknown \(A\) matrix satisfies%
\begin{equation*}
T\left( \left[ \begin{array}{c} 1 \\ 0 \end{array}  \right] \right) = \left[ \begin{array}{c} 2\\ 1 \end{array}  \right] \, \text{ and }  \, T\left( \left[ \begin{array}{c} 0 \\ 1 \end{array}  \right] \right) = \left[ \begin{array}{r} -1\\ 3 \end{array}  \right] \,\text{.}
\end{equation*}
Use the matrix transformation properties to determine \(T(\vx)\) where \(\vx= \left[ \begin{array}{c} x_1\\x_2 \end{array}  \right]\). Use the expression for \(T(\vx)\) to determine the matrix \(A\).%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp11556360}%
For each of the following matrices, determine if the transformation \(T\) defined by \(T(\vx)=A\vx\) is onto and if \(T\) is one-to-one.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(A=\left[ \begin{array}{ccr} 1\amp 1 \amp 1 \\ 1 \amp 2 \amp -3 \end{array} \right]\)%
\item{}\(A=\left[ \begin{array}{ccc} 1\amp 1 \amp 2 \\ 2 \amp 2 \amp 4 \end{array} \right]\)%
\item{}\(A=\left[ \begin{array}{rcc} 1\amp 1 \amp 2 \\ 1 \amp 2 \amp 3 \\ -1 \amp 1 \amp 2 \end{array} \right]\)%
\item{}\(A=\left[ \begin{array}{cc} 1\amp 1 \\ 2 \amp 3 \\ 3 \amp 0 \end{array} \right]\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp11561224}%
Come up with an example of a one-to-one transformation from \(\R^3\) to \(\R^4\), if possible. If not, explain why not.%
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp11570568}%
Come up with an example of an onto transformation from \(\R^3\) to \(\R^4\), if possible. If not, explain why not.%
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp11567624}%
Come up with an example of a one-to-one but not onto transformation from \(\R^4\) to \(\R^4\), if possible. If not, explain why not.%
\end{divisionexercise}%
\begin{divisionexercise}{10}{}{}{g:exercise:idp11566472}%
Two students are talking about when a matrix transformation is one-to-one. \begin{quote}%
Student 1: If we have a matrix transformation, then we need to check that \(A\vx=\vb\) has a unique solution for every \(\vb\) for which \(A\vx=\vb\) has a solution, right?\end{quote}
 \begin{quote}%
Student 2: Well, that's the definition. Each \(\vb\) in the codomain has to be the image of at most one \(\vx\) in the domain. So when \(\vb\) is in the range, corresponding to \(A\vx=\vb\) having a solution, then there is exactly one solution \(\vx\).\end{quote}
 \begin{quote}%
Student 1: But wouldn't it be enough to check that \(A\vx=\vzero\) has a unique solution? Doesn't that translate to the other \(\vb\) vectors? If there is a unique solution for one \(\vb_1\), then there can't be infinitely many solutions for another \(\vb_2\).\end{quote}
 \begin{quote}%
Student 2: I don't know. It feels to me as if changing the right hand side could change whether there is a unique solution, or infinitely many solutions, or no solution.\end{quote}
 Which part of the above conversation do you agree with? Which parts need fixing?%
\end{divisionexercise}%
\begin{divisionexercise}{11}{}{}{g:exercise:idp11572744}%
Show that if \(T\) is a matrix transformation from \(\R^n\) to \(\R^m\) and \(L\) is a line in \(\R^n\), then \(T(L)\), the image of \(L\), is a line or a single vector. (Note that a line in \(\R^n\) is the set of all vectors of the form \(\vv+c\vw\) where \(c\) is a scalar, and \(\vv, \vw\) are two fixed vectors in \(\R^n\).)%
\end{divisionexercise}%
\begin{divisionexercise}{12}{}{}{g:exercise:idp11312136}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
The range of a transformation is the same as the codomain of the transformation.%
\item{}\lititle{True\slash{}False.}\par%
The codomain of a transformation \(T\) defined by \(T(\vx)=A\vx\) is the span of the columns of \(A\).%
\item{}\lititle{True\slash{}False.}\par%
A one-to-one transformation is a transformation where each input has a unique output.%
\item{}\lititle{True\slash{}False.}\par%
A one-to-one transformation is a transformation where each output can only come from a unique input.%
\item{}\lititle{True\slash{}False.}\par%
If a matrix transformation from \(\R^n\) to \(\R^n\) is one-to-one, then it is also onto.%
\item{}\lititle{True\slash{}False.}\par%
A matrix transformation from \(\R^2\) to \(\R^3\) cannot be onto.%
\item{}\lititle{True\slash{}False.}\par%
A matrix transformation from \(\R^3\) to \(\R^2\) cannot be onto.%
\item{}\lititle{True\slash{}False.}\par%
A matrix transformation from \(\R^3\) to \(\R^2\) cannot be one-to-one.%
\item{}\lititle{True\slash{}False.}\par%
If the columns of a matrix \(A\) are linearly independent, then the transformation \(T\) defined by \(T(\vx)=A\vx\) is onto.%
\item{}\lititle{True\slash{}False.}\par%
If the columns of a matrix \(A\) are linearly independent, then the transformation \(T\) defined by \(T(\vx)=A\vx\) is one-to-one.%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is an \(m \times n\) matrix with \(n\) pivots, then the transformation \(\vx \mapsto A\vx\) is onto.%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is an \(m \times n\) matrix with \(n\) pivots, then the transformation \(\vx \mapsto A\vx\) is one-to-one.%
\item{}\lititle{True\slash{}False.}\par%
If \(\vu\) is in the range of a matrix transformation \(T\), then there is an \(\vx\) in the domain of \(T\) such that \(T(\vx)=\vu\).%
\item{}\lititle{True\slash{}False.}\par%
If \(T\) is a one-to-one matrix transformation, then \(T(\vx)=\vzero\) has a non-trivial solution.%
\item{}\lititle{True\slash{}False.}\par%
If the transformations \(T_1: \R^m \to \R^n\) and \(T_2:\R^n \to \R^p\) are onto, then the transformation \(T_2 \circ T_1\) defined by \(T_2\circ T_1(\vx)=T_2(T_1(\vx))\) is also onto.%
\item{}\lititle{True\slash{}False.}\par%
If the transformations \(T_1: \R^m \to \R^n\) and \(T_2:\R^n \to \R^p\) are one-to-one, then the transformation \(T_2 \circ T_1\) defined by \(T_2\circ T_1(\vx)=T_2(T_1(\vx))\) is also one-to-one.%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: The Geometry of Matrix Transformations}
\typeout{************************************************}
%
\begin{sectionptx}{Project: The Geometry of Matrix Transformations}{}{Project: The Geometry of Matrix Transformations}{}{}{x:section:sec_proj_geom_mtx}
In this section we will consider certain types of matrix transformations and analyze their geometry. Much more would be needed for real computer graphics, but the essential ideas are contained in our examples. A GeoGebra applet is available at \href{https://www.geogebra.org/m/rh4bzxee}{\nolinkurl{geogebra.org/m/rh4bzxee}} for you to use to visualize the transformations in this project.%
\begin{project}{}{x:project:act_1_g_rotation}%
\index{rotation matrix}%
We begin with transformations that produce the rotated dancing image in \hyperref[x:figure:F_Rotate_dance]{Figure~{\xreffont\ref{x:figure:F_Rotate_dance}}}. Let \(R\) be the matrix transformation from \(\R^2\) to \(\R^2\) defined by%
\begin{equation*}
R\left(\left[ \begin{array}{c} x \\ y \end{array}  \right] \right) = \left[\begin{array}{lr} \cos(\theta) \amp  -\sin(\theta) \\ \sin(\theta) \amp  \cos(\theta) \end{array}  \right]\left[ \begin{array}{c} x \\ y \end{array}  \right]\text{.}
\end{equation*}
These matrices are the rotation matrices.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Suppose \(\theta = \frac{\pi}{2}\). Then%
\begin{equation*}
R\left(\left[ \begin{array}{c} x \\ y \end{array}  \right] \right) = \left[ \begin{array}{cr} 0\amp -1 \\ 1\amp 0 \end{array}  \right] \left[ \begin{array}{c} x \\ y \end{array}  \right]\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Find the images of \(\vu_1 = \left[ \begin{array}{c} 1 \\ 0 \end{array} \right]\), \(\vu_2 = \left[ \begin{array}{c} \frac{\sqrt{2}}{2} \\ \frac{\sqrt{2}}{2} \end{array} \right]\), and \(\vu_3 = \left[ \begin{array}{c} 0 \\ 1 \end{array} \right]\) under \(R\).%
\item{}Plot the points determined by the vectors from part i. The matrix transformation \(R\) performs a rotation. Based on this small amount of data, what would you say the angle of rotation is for this transformation \(R\)?%
\end{enumerate}
\item{}\textgreater{} Now let \(R\) be the general matrix transformation defined by the matrix%
\begin{equation*}
\left[\begin{array}{lr} \cos(\theta) \amp  -\sin(\theta) \\ \sin(\theta) \amp  \cos(\theta) \end{array}  \right]\text{.}
\end{equation*}
Follow the steps indicated to show that \(R\) performs a counterclockwise rotation of an angle \(\theta\) around the origin. Let \(P\) be the point defined by the vector \(\left[ \begin{array}{c} x\\y \end{array}  \right] = \left[ \begin{array}{c}\cos(\alpha) \\ \sin(\alpha) \end{array}  \right]\) and \(Q\) the point defined by the vector \(\left[ \begin{array}{c} w\\z \end{array}  \right] = \left[ \begin{array}{c} \cos(\alpha+\theta) \\ \sin(\alpha+\theta) \end{array}  \right]\) as illustrated in \hyperref[x:figure:fig_Rotation_matrix]{Figure~{\xreffont\ref{x:figure:fig_Rotation_matrix}}}. \begin{figureptx}{A rotation in the plane.}{x:figure:fig_Rotation_matrix}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/Rotation_matrix.pdf}
\end{image}%
\tcblower
\end{figureptx}%
%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Use the angle sum trigonometric identities%
\begin{align*}
\cos(A+B) \amp = \cos(A) \cos(B) - \sin(A) \sin(B)\\
\sin(A+B) \amp = \cos(A) \sin(B) + \cos(B) \sin(A)
\end{align*}
to show that%
\begin{align*}
w \amp = \cos(\theta)x - \sin(\theta)y\\
z \amp =  \sin(\theta)x + \cos(\theta)y\text{.}
\end{align*}
%
\item{}Now explain why the counterclockwise rotation around the origin by an angle \(\theta\) can be represented by left multiplication by the matrix%
\begin{equation*}
\left[ \begin{array}{rr} \cos(\theta) \amp  -\sin(\theta) \\ \sin(\theta) \amp  \cos(\theta) \end{array}  \right]\text{.}
\end{equation*}
%
\end{enumerate}
\end{enumerate}
\end{project}%
\hyperref[x:project:act_1_g_rotation]{Project Activity~{\xreffont\ref{x:project:act_1_g_rotation}}} presented the rotation matrices. Other matrices have different effects.%
\begin{project}{Different matrix transformations.}{x:project:act_1_g_shear}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(S\) be the matrix transformation from \(\R^2\) to \(\R^2\) defined by%
\begin{equation*}
S\left(\left[ \begin{array}{c} x \\ y \end{array}  \right] \right) = \left[ \begin{array}{cr} 1\amp 0.5 \\ 0\amp 1 \end{array}  \right]\left[ \begin{array}{c} x \\ y \end{array}  \right]\text{.}
\end{equation*}
Determine the entries of the output vector \(S\left(\left[ \begin{array}{c} x \\ y \end{array}  \right] \right)\) and explain the action of the transformation \(S\) on the dancing figure as illustrated in \hyperref[x:figure:F_Shear_x_Dance]{Figure~{\xreffont\ref{x:figure:F_Shear_x_Dance}}}. (The transformation \(S\) is called a \terminology{shear} in the \(x\) direction.) \begin{figureptx}{A dancing figure and a sheared dancing figure.}{x:figure:F_Shear_x_Dance}{}%
\begin{image}{0.25}{0.5}{0.25}%
\includegraphics[width=\linewidth]{external/1_g_shear_x.pdf}
\end{image}%
\tcblower
\end{figureptx}%
%
\item{}Let \(C\) be the matrix transformation from \(\R^2\) to \(\R^2\) defined by%
\begin{equation*}
C\left(\left[ \begin{array}{c} x \\ y \end{array}  \right] \right) = \left[ \begin{array}{cr} 0.65\amp 0 \\ 0\amp 0.65 \end{array}  \right]\left[ \begin{array}{c} x \\ y \end{array}  \right]\text{.}
\end{equation*}
Determine the entries of the output vector \(C\left(\left[ \begin{array}{c} x \\ y \end{array}  \right] \right)\) and explain the action of the transformation \(C\) on the dancing figure as illustrated in \hyperref[x:figure:F_Contract_Dance]{Figure~{\xreffont\ref{x:figure:F_Contract_Dance}}}. (The transformation \(C\) is called a \terminology{contraction}.) How would your response change if each \(0.65\) was changed to \(2\) in the matrix \(C\)? \begin{figureptx}{A dancing figure and a contracted dancing figure.}{x:figure:F_Contract_Dance}{}%
\begin{image}{0.25}{0.5}{0.25}%
\includegraphics[width=\linewidth]{external/1_g_dancer_contract.pdf}
\end{image}%
\tcblower
\end{figureptx}%
%
\end{enumerate}
\end{project}%
So far we have seen specific matrix transformations perform a rotations, shears, and contractions. We can combine these, and other, matrix transformations by composition to change figures in different ways, and to created animations of geometric figures. (As we will see later, combining transformations needs to be done carefully in order to obtain the result we want. For example, if we want to first rotate then translate, in what order should the matrices be applied?)%
\end{sectionptx}
\end{chapterptx}
\end{partptx}
%
%
\typeout{************************************************}
\typeout{Chapter II Matrices}
\typeout{************************************************}
%
\begin{partptx}{Matrices}{}{Matrices}{}{}{x:part:part-matrices}
 %
%
\typeout{************************************************}
\typeout{Section 8 Matrix Operations}
\typeout{************************************************}
%
\begin{chapterptx}{Matrix Operations}{}{Matrix Operations}{}{}{x:chapter:chap_matrix_operations}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp11645080}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}Under what conditions can we add two matrices and how is the matrix sum defined?%
\item{}Under what conditions can we multiply a matrix by a scalar and how is a scalar multiple of a matrix defined?%
\item{}Under what conditions can we multiply two matrices and how is the matrix product defined?%
\item{}What properties do matrix addition, scalar multiplication of matrices and matrix multiplication satisfy? Are these properties similar to properties that are satisfied by vector operations?%
\item{}What are two properties that make matrix multiplication fundamentally different than our standard product of real numbers?%
\item{}What is the interpretation of matrix multiplication from the perspective of linear transformations?%
\item{}How is the transpose of a matrix defined?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Algorithms for Matrix Multiplication}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Algorithms for Matrix Multiplication}{}{Application: Algorithms for Matrix Multiplication}{}{}{x:section:sec_appl_mtx_mult}
Matrix multiplication is widely used in applications ranging from scientific computing and pattern recognition to counting paths in graphs. As a consequence, much work is being done in developing efficient algorithms for matrix multiplication.%
\par
We will see that a matrix product can be calculated through the row-column method. Recall that the product of two \(2 \times 2\) matrices \(A = \left[ \begin{array}{cc} a_{11}\amp a_{12}\\a_{21}\amp a_{22} \end{array}  \right]\) and \(B = \left[ \begin{array}{cc} b_{11}\amp b_{12}\\b_{21}\amp b_{22} \end{array}  \right]\) is given by%
\begin{equation*}
AB = \left[ \begin{array}{cc} a_{11}b_{11}+a_{12}b_{21} \amp  a_{11}b_{12}+a_{12}b_{22} \\ a_{21}b_{11}+a_{22}b_{21} \amp  a_{21}b_{12}+a_{22}b_{22} \end{array}  \right]\text{,}
\end{equation*}
%
\par
This product involves eight scalar multiplications and some scalar additions. As we will see, multiplication is more computationally expensive than addition, so we will focus on multiplication. In 1969, a German mathematician named Volker Strassen showed\footnote{Strassen, Volker, Gaussian Elimination is not Optimal, Number. Math. 13, p. 354-356, 1969\label{g:fn:idp11658008}} that the product of two \(2 \times 2\) matrices can be calculated using only seven multiplications. While this is not much of an improvement, the Strassen algorithm can be applied to larger matrices, using matrix partitions (which allow for parallel computation), and its publication led to additional research on faster algorithms for matrix multiplication. More details are provided later in this section.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_mtx_ops_intro}
A vector is a list of numbers in a specified order and a matrix is an ordered array of objects. In fact, a vector can be thought of as a matrix of size \(n \times 1\). Vectors and matrices are so alike in this way that it would seem natural that we can define operations on matrices just as we did with vectors.%
\par
Recall that a matrix is made of rows and columns \textemdash{} the entries reading from left to right form the \emph{rows} of the matrix and the entries reading from top to bottom form the \terminology{columns}. The number of rows and columns of a matrix is called the \terminology{size} of the matrix, so an \(m \times n\) matrix has \(m\) rows and \(n\) columns. If we label the entry in the \(i\)th row and \(j\)th column of a matrix \(A\) as \(a_{ij}\), then we write \(A = [a_{ij}]\).%
\par
We can generalize the operations of addition and scalar multiplication on vectors to matrices similarly. Given two matrices \(A=[a_{ij}]\) and \(B=[b_{ij}]\) of the same size, we define the sum \(A+B\) by%
\begin{equation*}
A+B= [ a_{ij}+b_{ij} ]
\end{equation*}
when the sizes of the matrices \(A\) and \(B\) match. In other words, for matrices of the same size the matrix addition is defined by adding corresponding entries in the matrices. For example,%
\begin{equation*}
\left[ \begin{array}{rc} 1 \amp  2 \\ -2 \amp  3 \end{array}  \right] + \left[ \begin{array}{cc} 1 \amp  3 \\ 2 \amp  4 \end{array}  \right] = \left[ \begin{array}{cc} 2 \amp  5 \\ 0 \amp  7 \end{array}  \right]  \,\text{.}
\end{equation*}
%
\par
\index{matrix!scalar multiple} We define the scalar multiple of a matrix \(A=[a_{ij}]\) by scalar \(c\) to be the matrix \(cA\) defined by%
\begin{equation*}
cA= [ ca_{ij}]\text{,}
\end{equation*}
%
\par
This means that we multiply each entry of the matrix \(A\) by the scalar \(c\). As an example,%
\begin{equation*}
3 \left[ \begin{array}{rc} 1 \amp  2 \\ -2 \amp  3 \end{array}  \right] = \left[ \begin{array}{rc} 3 \amp  6 \\ -6 \amp  9 \end{array}  \right] \,\text{.}
\end{equation*}
%
\par
Even though we did not have a multiplication operation on vectors, we had a matrix-vector product, which is a special case of a matrix-matrix product since a vector is a matrix with one column. However, generalizing the matrix-vector product to a matrix-matrix product is not immediate as it is not immediately clear what we can do with the other columns. We will consider this question in this section.%
\par
Note that all of the matrix operations can be performed on a calculator. After entering each matrix in the calculator, just use \(+\), \(-\) and \texttimes{} operations to find the result of the matrix operation. (Just for fun, try using \(\div\) with matrices to see if it will work.)%
\begin{exploration}{}{x:exploration:pa_2_a}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Pick three different varying sizes of pairs of \(A, B\) matrices which can be added. For each pair:%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Find the matrices \(A+B\) and \(B+A\).%
\item{}How are the two matrices \(A+B\) and \(B+A\) related? What does this tell us about matrix addition?%
\end{enumerate}
\item{}Let \(A = \left[ \begin{array}{rc} 1 \amp 0 \\ -2 \amp 8 \end{array} \right]\), \(B = \left[ \begin{array}{cc} 1 \amp 1 \\ 3 \amp 4 \end{array} \right]\), and \(C = \left[ \begin{array}{cr} 0 \amp -5 \\ 1 \amp 6 \end{array} \right]\). Determine the entries of the matrix \(A + 2B - 7C\).%
\item\label{x:task:p_matrix_multiplication}Now we turn to multiplication of matrices. Our first goal is to find out what conditions we need on the sizes of matrices \(A\) and \(B\) if the matrix-matrix product \(AB\) is defined and what the size of the resulting product matrix is. We know the condition and the size of the result in the special case of \(B\) being a vector, i.e., a matrix with one column. So our conjectures for the general case should match what we know in the special case. In each part of this problem, use any appropriate tool (e.g., your calculator, Maple, Mathematica, Wolfram\(|\)Alpha) to determine the matrix product \(AB\), if it exists. If you obtain a product, write it down and explain how its size is related to the sizes of \(A\) and \(B\). If you receive an error, write down the error and guess why the error occurred and\slash{}or what it means.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}\(A = \left[ \begin{array}{ccc} 1\amp 2\amp 0 \\ 0\amp 1\amp 1 \end{array} \right] \ \ \ \text{ and } \ \ \ B = \left[ \begin{array}{crc} 3\amp 5\amp 0\\0\amp -2\amp 1 \end{array} \right]\)%
\item{}\(A = \left[ \begin{array}{ccc} 1\amp 2\amp 0 \\ 0\amp 1\amp 1 \end{array} \right] \ \ \ \text{ and } \ \ \ B = \left[ \begin{array}{crc} 3\amp 0\\5\amp -2 \\ 0\amp 1 \end{array} \right]\)%
\item{}\(A = \left[ \begin{array}{cc} 1 \amp 2 \\ 3 \amp 4 \end{array} \right] \ \ \ \text{ and } \ \ \ B = \left[ \begin{array}{ccc} 1 \amp 1 \amp 1 \\1 \amp 0 \amp 1 \\ 0 \amp 2 \amp 0 \end{array} \right]\)%
\item{}\(A = \left[ \begin{array}{cc} 1 \amp 2 \\ 3 \amp 4 \\ 5 \amp 6 \\ 7 \amp 8 \end{array} \right] \ \ \ \text{ and } \ \ \ B = \left[ \begin{array}{rcc} 1 \amp 2 \amp 3 \\ -1 \amp 1 \amp 1 \end{array} \right]\)%
\item{}Make a guess for the condition on the sizes of two matrices \(A, B\) for which the product \(AB\) is defined. How is the size of the product matrix related to the sizes of \(A\) and \(B\)?%
\end{enumerate}
\item{}The final matrix products, when defined, in \hyperref[x:task:p_matrix_multiplication]{problem~{\xreffont\ref{x:exploration:pa_2_a}}.{\xreffont\ref{x:task:p_matrix_multiplication}}} might seem unrelated to the individual matrices at first. In this problem, we will uncover this relationship using our knowledge of the matrix-vector product. Let \(A = \left[ \begin{array}{rr} 3 \amp -1 \\ -2 \amp 3 \end{array} \right]\) and \(B = \left[ \begin{array}{ccc} 0 \amp 2 \amp 1 \\ 1 \amp 3 \amp 2 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Calculate \(AB\) using any tool.%
\item{}Using the matrix-vector product, calculate \(A \vx\) where \(\vx\) is the first column (i.e., calculate \(A\begin{bmatrix}0 \\ 1 \end{bmatrix}\)), and then the second column of \(B\) (i.e., calculate \(A\begin{bmatrix}2 \\ 3 \end{bmatrix}\)), and then the third column of \(B\) (i.e., calculate \(A\begin{bmatrix}1 \\ 2 \end{bmatrix}\)). Do you notice these output vectors within \(AB\)?%
\item{}Describe as best you can a definition of \(AB\) using the matrix-vector product.%
\end{enumerate}
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Properties of Matrix Addition and Multiplication by Scalars}
\typeout{************************************************}
%
\begin{sectionptx}{Properties of Matrix Addition and Multiplication by Scalars}{}{Properties of Matrix Addition and Multiplication by Scalars}{}{}{x:section:sec_mtx_add_smult}
Just as we were able to define an algebra of vectors with addition and multiplication by scalars, we can define an algebra of matrices. We will see that the properties of these operations on matrices are immediate generalizations of the properties of the operations on vectors. We will then see how the matrix product arises through the connection of matrices to linear transformations. Finally, we define the transpose of a matrix. The transpose of a matrix will be useful in applications such as graph theory and least-squares fitting of curves, as well as in advanced topics such as inner product spaces and the dual space of a vector space.%
\par
\index{matrix!sum} We learned in \hyperref[x:exploration:pa_2_a]{Preview Activity~{\xreffont\ref{x:exploration:pa_2_a}}} that we can add two matrices of the same size together by adding corresponding entries and we can multiply any matrix by a scalar by multiplying each entry of the matrix by that scalar. More generally, if \(A = [a_{ij}]\) and \(B = [b_{ij}]\) are \(m \times n\) matrices and \(c\) is any scalar, then%
\begin{equation*}
A + B = [a_{ij}+b_{ij}] \ \ \text{ and }  \ \ cA = [ca_{ij}]\text{.}
\end{equation*}
%
\par
As we have done each time we have introduced a new operation, we ask what properties the operation has. For example, you determined in \hyperref[x:exploration:pa_2_a]{Preview Activity~{\xreffont\ref{x:exploration:pa_2_a}}} that addition of matrices is a commutative operation. More specifically, for every two \(m\times n\) matrices \(A\) and \(B\), \(A+B=B+A\). We can use similar arguments to verify the following properties of matrix addition and multiplication by scalars. Notice that these properties are very similar to the properties of addition and scalar multiplication of vectors we discussed earlier. This should come as no surprise since the \(n\)-dimensional vectors are \(n\times 1\) matrices. In a strange twist, we will see that matrices themselves can be considered as vectors when we discuss vector spaces in a later section.%
\begin{theorem}{}{}{x:theorem:thm_matrix_sum_properties}%
Let \(A\), \(B\), and \(C\) be \(m \times n\) matrices and let \(a\) and \(b\) be scalars. Then%
\begin{enumerate}
\item{}\(A+B = B+A\) (this property tells us that matrix addition is \terminology{commutative})%
\item{}\((A+B) + C = A + (B+C)\) (this property tells us that matrix addition is \terminology{associative})%
\item{}The \(m \times n\) matrix \(0\) whose entries are all 0 has the property that \(A + 0 = A\). The matrix \(0\) is called the \terminology{zero matrix} (It is generally clear from the context what the size of the 0 matrix is.).%
\item{}The scalar multiple \((-1)A\) of the matrix \(A\) has the property that \((-1)A + A = 0\). The matrix \((-1)A = -A\) is called the \terminology{additive inverse} of the matrix \(A\).%
\item{}\((a+b) A = aA + bA\) (this property tells us that \emph{scalar multiplication of matrices distributes over scalar addition})%
\item{}\(a(A+B) = aA + aB\) (this property tells us that \emph{scalar multiplication of matrices distributes over matrix addition})%
\item{}\(\displaystyle (ab) A = a(bA)\)%
\item{}\(1A=A\).%
\end{enumerate}
%
\end{theorem}
Later on, we will see that these properties define the set of all \(m \times n\) matrices as a \terminology{vector space}. These properties just say that, regarding addition and multiplication by scalars, we can manipulate matrices just as we do real numbers. Note, however, we have not yet defined an operation of multiplication on matrices. That is the topic for the next section.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  A Matrix Product}
\typeout{************************************************}
%
\begin{sectionptx}{A Matrix Product}{}{A Matrix Product}{}{}{x:section:sec_mtx_prod}
\begin{introduction}{}%
As we saw in \hyperref[x:exploration:pa_2_a]{Preview Activity~{\xreffont\ref{x:exploration:pa_2_a}}}, a matrix-matrix product can be found in a way which makes use of and also generalizes the matrix-vector product.%
\begin{definition}{}{g:definition:idp11725592}%
\index{matrix!product}%
The \terminology{matrix product} of a \(k \times m\) matrix \(A\) and an \(m \times n\) matrix \(B = [\vb_1 \ \vb_2 \ \cdots \ \vb_n]\) with columns \(\vb_1\), \(\vb_2\), \(\ldots\), \(\vb_n\) is the \(k \times n\) matrix%
\begin{equation*}
[A\vb_1 \ A\vb_2 \ \cdots \ A\vb_n]\text{.}
\end{equation*}
%
\end{definition}
We now consider the motivation behind this definition by thinking about the matrix transformations corresponding to each of the matrices \(A, B\) and \(AB\). Recall that left multiplication by an \(m \times n\) matrix \(B\) defines a transformation \(T\) from \(\R^n\) to \(\R^m\) by \(T(\vx)=B\vx\). The domain of \(T\) is \(\R^n\) because the number of components of \(\vx\) have to match the number of entries in each of row of \(B\) in order for the matrix-vector product \(B\vx\) to be defined. Similarly, a \(k \times m\) matrix \(A\) defines a transformation \(A\) from \(\R^m\) to \(\R^k\). Since transformations are functions, we can compose them as long as the output vectors of the inside transformation lie in the domain of the outside transformation. Therefore if \(T\) is the inside transformation and \(S\) is the outside transformation, the composition \(S\circ T\) is defined. So a natural question to ask is if we are given%
\begin{itemize}[label=\textbullet]
\item{}a transformation \(T\) from \(\R^n\) to \(\R^m\) where \(T(\vx) = B \vx\) for an \(m \times n\) matrix \(B\) and%
\item{}a transformation \(S\) from \(\R^m\) to \(\R^k\) with \(S(\vy) = A \vy\) for some \(k \times m\) matrix \(A\),%
\end{itemize}
is there a matrix that represents the transformation \(S \circ T\) defined by \((S\circ T)(\vx)=S(T(\vx))\)? We investigate this question in the next activity in the special case of a \(2\times 3\) matrix \(A\) and a \(3\times 2\) matrix \(B\).%
\begin{activity}{}{x:activity:act_A2_1_1}%
In this activity, we look for the meaning of the matrix product from a transformation perspective. Let \(S\) and \(T\) be matrix transformations defined by%
\begin{equation*}
S(\vy) = A \vy \ \ \ \text{ and }  \ \ \ T(\vx) = B \vx\text{,}
\end{equation*}
where%
\begin{equation*}
A = \left[ \begin{array}{ccc} 1\amp 2\amp 0 \\ 0\amp 1\amp 1 \end{array}  \right]  \ \ \ \text{ and }  \ \ \ B = \left[ \begin{array}{cr} 3\amp 0\\5\amp -2 \\ 0\amp 1 \end{array}  \right]\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}What are the domains and codomains of \(S\) and \(T\)? Why is the composite transformation \(S \circ T\) defined? What is the domain of \(S\circ T\)? What is the codomain of \(S\circ T\)? (Recall that \(S \circ T\) is defined by \((S \circ T)(\vx) = S(T(\vx))\), i.e., we substitute the output \(T(\vx)\) as the input into the transformation \(S\).)%
\item{}Let \(\vx = \left[ \begin{array}{c} x \\ y \end{array} \right]\). Determine the components of \(T(\vx)\).%
\item{}Find the components of \(S\circ T(\vx)=S(T(\vx))\).%
\item{}Find a matrix \(C\) so that \(S(T(\vx)) = C\vx\).%
\item{}Use the definition of composition of transformations and the definitions of the \(S\) and \(T\) transformations to explain why it is reasonable to define \(AB\) to be the matrix \(C\). Does the matrix \(C\) agree with the%
\begin{equation*}
AB = \left[ \begin{array}{cr} 13 \amp  -4 \\ 5 \amp  -1 \end{array}  \right]
\end{equation*}
you found in \hyperref[x:exploration:pa_2_a]{Preview Activity~{\xreffont\ref{x:exploration:pa_2_a}}} using technology?%
\end{enumerate}
\end{activity}%
We now consider this result in the general case of a \(k\times m\) matrix \(A\) and an \(m \times n\) matrix \(B\), where \(A\) and \(B\) define matrix transformations \(S\) and \(T\), respectively. In other words, \(S\) and \(T\) are matrix transformations defined by \(S(\vx) = A\vx\) and \(T(\vx) = B\vx\). The domain of \(S\) is \(\R^m\) and the codomain is \(\R^k\). The domain of \(T\) is \(\R^n\) and the codomain is \(\R^m\). The composition \(S\circ T\) is defined because the output vectors of \(T\) are in \(\R^m\) and they lie in the domain of \(S\). The domain of \(S\circ T\) is the same as the domain of \(T\) since the input vectors first go through the \(T\) transformation. The codomain of \(S\circ T\) is the same as the codomain of \(S\) since the final output vectors are produced by applying the \(S\) transformation.%
\par
Let us see how we can obtain the matrix corresponding to the transformation \(S\circ T\). Let \(B = \left[ \vb_1 \ \vb_2  \  \cdots \ \vb_n  \right]\), where \(\vb_j\) is the \(j\)th column of \(B\), and let \(\vx = \left[ \begin{array}{c} x_1 \\ x_2 \\ \vdots \\ x_n \end{array}  \right]\). Recall that the matrix vector product \(B\vx\) is the linear combination of the columns of \(B\) with the corresponding weights from \(\vx\). So%
\begin{equation*}
T(\vx) = B\vx = x_1 \vb_1 + x_2 \vb_2 + \cdots + x_n \vb_n\text{.}
\end{equation*}
%
\par
Note that each of the \(\vb_j\) vectors are in \(\R^m\) since \(B\) is an \(m\times n\) matrix. Therefore, each of these vectors can be multiplied by matrix \(A\) and we can evaluate \(S(B\vx)\). Therefore, \(S\circ T\) is defined and%
\begin{equation}
(S \circ T)(\vx) = S(T(\vx))= A(B\vx) = A\left( x_1 \vb_1 + x_2 \vb_2 + \cdots + x_n \vb_n\right)\text{.}\label{x:men:eq_2_a_1}
\end{equation}
%
\par
The properties of matrix-vector products show that%
\begin{equation}
A\left( x_1 \vb_1 + x_2 \vb_2 + \cdots + x_n \vb_n\right) = x_1 A\vb_1 + x_2 A\vb_2 + \cdots + x_n A\vb_n\text{.}\label{x:men:eq_2_a_2}
\end{equation}
%
\par
This expression is a linear combination of \(A\vb_i\)'s with \(x_i\)'s being the weights. Therefore, if we let \(C\) be the matrix with columns \(A \vb_1\), \(A\vb_2\), \(\ldots\), \(A\vb_n\), that is%
\begin{equation*}
C = [A \vb_1 \ A\vb_2 \ \cdots \ A\vb_n]\text{,}
\end{equation*}
then%
\begin{equation}
x_1 A\vb_1 + x_2 A\vb_2 + \cdots + x_n A\vb_n = C \vx\label{x:men:eq_2_a_3}
\end{equation}
by definition of the matrix-vector product. Combining equations \hyperref[x:men:eq_2_a_1]{({\xreffont\ref{x:men:eq_2_a_1}})}, \hyperref[x:men:eq_2_a_2]{({\xreffont\ref{x:men:eq_2_a_2}})}, and \hyperref[x:men:eq_2_a_3]{({\xreffont\ref{x:men:eq_2_a_3}})} shows that%
\begin{equation*}
(S \circ T)(\vx) = C \vx
\end{equation*}
where \(C = [A \vb_1 \ A\vb_2 \ \cdots \ A\vb_n]\).%
\par
Also note that since \(T(\vx)=B\vx\) and \(S(\vy)=A\vy\), we find%
\begin{equation}
(S\circ T)(\vx)= S(T(\vx))= S(B\vx)=A(B(\vx)) \,\text{.}\label{x:men:eq_2_a_4}
\end{equation}
%
\par
Since the matrix representing the transformation \(S\circ T\) is the matrix%
\begin{equation*}
[A \vb_1 \ A\vb_2 \ \cdots \ A\vb_n]
\end{equation*}
where \(\vb_1\), \(\vb_2\), \(\ldots\), \(\vb_n\) are the columns of the matrix \(B\), it is natural to define \(AB\) to be this matrix in light of equation \hyperref[x:men:eq_2_a_4]{({\xreffont\ref{x:men:eq_2_a_4}})}.%
\par
Matrix multiplication has some properties that are unfamiliar to us as the next activity illustrates.%
\begin{activity}{}{x:activity:act_A2_1_2}%
Let \(A~=~\left[ \begin{array}{rr} 3 \amp -1 \\ -2 \amp 6 \end{array} \right]\), \(B~=~\left[ \begin{array}{cc} 0 \amp 2 \\ 1 \amp 3 \end{array} \right]\), \(C~=~\left[ \begin{array}{cc} 1 \amp 1 \\ 1 \amp 1 \end{array} \right]\), \(D~=~\left[ \begin{array}{rr} 3 \amp -3 \\ -3 \amp 3 \end{array} \right]\) and \(E~=~\left[ \begin{array}{cc} 1 \amp 0 \\ 0 \amp 1 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the indicated products (by hand or using a calculator).%
\begin{equation*}
AB \qquad BA \qquad DC \qquad AC \qquad BC \qquad AE \qquad EB
\end{equation*}
%
\item{}Is matrix multiplication commutative? Explain.%
\item{}Is there an identity element for matrix multiplication? In other words, is there a matrix \(I\) for which \(AI=IA=A\) for any matrix \(A\)? Explain.%
\item{}If \(a\) and \(b\) are real numbers with \(ab=0\), then we know that either \(a=0\) or \(b=0\). Is this same property true with matrix multiplication? Explain.%
\item{}If \(a\), \(b\), and \(c\) are real numbers with \(c \neq 0\) and \(ac = bc\), we know that \(a=b\). Is this same property true with matrix multiplication? Explain.%
\end{enumerate}
\end{activity}%
As we saw in \hyperref[x:activity:act_A2_1_2]{Activity~{\xreffont\ref{x:activity:act_A2_1_2}}}, there are matrices \(A, B\) for which \(AB\neq BA\). On the other hand, there are matrices for which \(AB=BA\). For example, this equality will always hold for a square matrix \(A\) and if \(B\) is the identity matrix of the same size. It also holds if \(A=B\). If the equality \(AB=BA\) holds, we say that matrices \(A\) and \(B\) \terminology{commute}. So the identity matrix commutes with all square matrices of the same size and every matrix \(A\) commutes with \(A^k\) for any power \(k\).%
\par
There is an alternative method of calculating a matrix product that we will often use that we illustrate in the next activity. This alternate version depends on the product of a row matrix with a vector. Suppose \(A = [a_1 \ a_2 \ \cdots \ a_n]\) is a \(1 \times n\) matrix and \(\vx = \left[ \begin{array}{c} x_1 \\ x_2 \\ \vdots \\ x_n \end{array}  \right]\) is an \(n \times 1\) vector. Then the product \(A \vx\) is the \(1 \times 1\) vector%
\begin{equation*}
[a_1 \ a_2 \ \cdots \ a_n]\left[ \begin{array}{c} x_1 \\ x_2 \\ \vdots \\ x_n \end{array}  \right] = [a_1x_1+a_2x_2 + \cdots + a_nx_n]\text{.}
\end{equation*}
%
\par
In this situation, we usually identify the \(1 \times 1\) matrix with its scalar entry and write%
\begin{equation}
[a_1 \ a_2 \ \cdots \ a_n] \cdot \left[ \begin{array}{c} x_1 \\ x_2 \\ \vdots \\ x_n \end{array}  \right] = a_1x_1+a_2x_2 + \cdots + a_nx_n\text{.}\label{x:men:eq_2_a_5}
\end{equation}
%
\par
\index{dot product} The product \(\cdot\) in \hyperref[x:men:eq_2_a_5]{({\xreffont\ref{x:men:eq_2_a_5}})} is called the \terminology{scalar} or \terminology{dot} product of \([a_1 \ a_2 \ \cdots \ a_n]\) with \(\left[ \begin{array}{c} x_1 \\ x_2 \\ \vdots \\ x_n \end{array} \right]\).%
\begin{activity}{}{x:activity:act_A2_1_3}%
Let \(A = \left[ \begin{array}{crr} 1\amp -1\amp 2 \\ 3\amp 0\amp -4 \\ 2\amp -5\amp 1 \end{array} \right]\) and \(B = \left[ \begin{array}{cr} 4\amp -2 \\ 6\amp 0 \\ 1\amp 3 \end{array} \right]\).%
\par
Let \(\va_i\) be the \(i\)th row of \(A\) and \(\vb_j\) the \(j\)th column of \(B\). For example, \(\va_1=[ \, 1 \; -1 \; 2 \, ]\) and \(\vb_2 = \left[ \begin{array}{r} -2 \\ 0 \\ 3 \end{array} \right]\).%
\par
Calculate the entries of the matrix \(C\), where%
\begin{equation*}
C = \left[ \begin{array}{cc} \va_1 \cdot \vb_1 \amp  \va_1 \cdot \vb_2 \\ \va_2 \cdot \vb_1 \amp  \va_2 \cdot \vb_2 \\ \va_3 \cdot \vb_1 \amp  \va_3 \cdot \vb_2 \end{array}  \right]\,\text{,}
\end{equation*}
where \(\va_i \cdot \vb_j\) refers to the scalar product of row \(i\) of \(A\) with column \(j\) of \(B\).\footnotemark{} Compare your result with the result of \(AB\) calculated via the product of \(A\) with the columns of \(B\).%
\end{activity}%
\footnotetext[14]{Recall from \hyperlink{x:exercise:ex_1_e_scalar_product}{Exercise~{\xreffont 5}} of \hyperref[x:chapter:chap_matrix_vector]{Section~{\xreffont\ref{x:chapter:chap_matrix_vector}}} that the scalar product \(\vu \cdot \vv\) of a \(1 \times n\) matrix \(\vu = [u_1 \ u_2 \ \ldots \ u_n]\) and an \(n \times 1\) vector \(\vv=\left[ \begin{array}{c} v_1\\ v_2\\ \vdots \\ v_n \end{array}  \right]\) is \(\vu \cdot \vv = u_1v_1 + u_2v_2 + u_3v_3 + \cdots + u_nv_n\).\label{g:fn:idp11822744}}%
\hyperref[x:activity:act_A2_1_3]{Activity~{\xreffont\ref{x:activity:act_A2_1_3}}} shows that these is an alternate way to calculate a matrix product. To see how this works in general, let \(A = [a_{ij}]\) be a \(k \times m\) matrix and \(B = [\vb_1 \ \vb_2 \ \cdots \ \vb_n]\) an \(m \times n\) matrix. We know that%
\begin{equation*}
AB = [A\vb_1 \ A\vb_2 \ \cdots \ A\vb_n]\text{.}
\end{equation*}
%
\par
Now let \(\vr_1\), \(\vr_2\), \(\ldots\), \(\vr_k\) be the rows of \(A\) so that \(A = \left[ \begin{array}{c} \vr_1 \\ \vr_2 \\ \vdots \\ \vr_k \end{array}  \right]\). First we argue that if \(\vx = \left[ \begin{array}{c} x_1 \\ x_2 \\ \vdots \\ x_m \end{array}  \right]\), then%
\begin{equation*}
A \vx = \left[ \begin{array}{c} \vr_1 \cdot \vx \\ \vr_2 \cdot \vx \\ \vdots \\ \vr_k \cdot \vx \end{array}  \right]\text{.}
\end{equation*}
%
\par
This is the \terminology{scalar product} (or \terminology{dot product}) definition of the matrix-vector product.%
\par
To show that this definition gives the same result as the linear combination definition of matrix-vector product, we first let \(A = [\vc_1 \ \vc_2 \ \cdots \ \vc_m]\), where \(\vc_1\), \(\vc_2\), \(\ldots\), \(\vc_m\) are the columns of \(A\). By our linear combination definition of the matrix-vector product, we obtain%
\begin{align*}
A \vx \amp = x_1\vc_1 + x_2 \vc_2 + \cdots + x_m \vc_m\\
\amp = x_1 \left[ \begin{array}{c} a_{11}\\
a_{21}\\
\vdots\\
a_{k1} \end{array} \right] + x_2 \left[ \begin{array}{c} a_{12}\\
a_{22}\\
\vdots\\
a_{k2} \end{array} \right] + \cdots + x_m \left[ \begin{array}{c} a_{1m}\\
a_{2m}\\
\vdots\\
a_{km} \end{array} \right]\\
\amp = \left[ \begin{array}{c} a_{11}x_1+a_{12}x_2+ \cdots + a_{1m}x_m\\
a_{21}x_1+a_{22}x_2+ \cdots + a_{2m}x_m\\
\vdots\\
a_{k1}x_1+a_{k2}x_2+ \cdots + a_{km}x_m \end{array} \right]\\
\amp = \left[ \begin{array}{c} \vr_1 \cdot \vx\\
\vr_2 \cdot \vx\\
\vdots\\
\vr_k \cdot \vx \end{array} \right]\text{.}
\end{align*}
%
\par
Therefore, the above work shows that both linear combination and scalar product definitions give the same matrix-vector product.%
\par
Applying this to the matrix product \(AB\) defined in terms of the matrix-vector product, we see that%
\begin{equation*}
A \vb_j = \left[ \begin{array}{c} \vr_1 \cdot \vb_j \\ \vr_2 \cdot \vb_j \\ \vdots \\ \vr_k \cdot \vb_j \end{array}  \right]\text{.}
\end{equation*}
%
\par
So the \(i,j\)th entry of the matrix product \(AB\) is found by taking the scalar product of the \(i\)th row of \(A\) with the \(j\)th column of \(B\). In other words,%
\begin{equation*}
(AB)_{ij} = \vr_i \cdot \vb_j
\end{equation*}
where \(\vr_i\) is the \(i\)th row of \(A\) and \(\vb_j\) is the \(j\)th column of \(B\).%
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Properties of Matrix Multiplication}
\typeout{************************************************}
%
\begin{subsectionptx}{Properties of Matrix Multiplication}{}{Properties of Matrix Multiplication}{}{}{g:subsection:idp11844888}
\hyperref[x:activity:act_A2_1_2]{Activity~{\xreffont\ref{x:activity:act_A2_1_2}}} shows that we must be very careful not to assume that matrix multiplication behaves like multiplication of real numbers. However, matrix multiplication does satisfy some familiar properties. For example, we now have an addition and multiplication of matrices under certain conditions, so we might ask if matrix multiplication distributes over matrix addition. To answer this question we take two \terminology{arbitrary} \(k \times m\) matrices \(A\) and \(B\) and an \terminology{arbitrary} \(m \times n\) matrix \(C =  [\vc_1 \ \vc_2 \ \cdots \ \vc_n]\). Then%
\begin{align*}
(A+B)C \amp = [(A+B)\vc_1 \ (A+B)\vc_2 \ \cdots \ (A+B)\vc_n]\\
\amp = [A\vc_1+B\vc_1 \ A\vc_2+B\vc_2 \ \cdots \ A\vc_n+B\vc_n]\\
\amp = [A\vc_1 \ A\vc_2 \ \cdots \ A\vc_n] + [B\vc_1 \ B\vc_2 \ \cdots \ B\vc_n]\\
\amp = AC + BC\text{.}
\end{align*}
%
\par
Similar arguments can be used to show the following properties of matrix multiplication.%
\begin{theorem}{}{}{x:theorem:thm_matrix_product_properties}%
Let \(A\), \(B\), and \(C\) be matrices of the appropriate sizes for all sums and products to be defined and let \(a\) be a scalar. Then%
\begin{enumerate}
\item{}\((AB)C = A(BC)\) (this property tells us that matrix multiplication is \terminology{associative})%
\item{}\((A+B)C = AC + BC\) (this property tells us that matrix multiplication on the right \terminology{distributes over matrix addition})%
\item{}\(A(B+C) = AB + AC\) (this property tells us that matrix multiplication on the left \terminology{distributes over matrix addition})%
\item{}There is a square matrix \(I_n\) with the property that \(AI_n = A\) or \(I_nA = A\) for whichever product is defined.%
\item{}\(\displaystyle a(AB) = (aA)B = A(aB)\)%
\end{enumerate}
%
\end{theorem}
We verified the second part of this theorem and will assume that all of the properties of this theorem hold. The matrix \(I_n\) introduced in \hyperref[x:theorem:thm_matrix_product_properties]{Theorem~{\xreffont\ref{x:theorem:thm_matrix_product_properties}}} is called the \terminology{(multiplicative) identity matrix}. We usually omit the word multiplicative and refer to the \(I_n\) simply as the identity matrix. This does not cause any confusion since we refer to the additive identity matrix as simply the zero matrix.%
\begin{definition}{}{g:definition:idp11875336}%
\index{identity matrix}%
Let \(n\) be a positive integer. The \(n \times n\) \terminology{identity matrix} \(I_n\) is the matrix \(I_n = [a_{ij}]\), where \(a_{ii} = 1\) for each \(i\) and \(a_{ij} = 0\) if \(i \neq j\).%
\end{definition}
We also write the matrix \(I_n\) as%
\begin{equation*}
I_n = \left[ \begin{array}{ccccccc} 1 \amp  0 \amp  0 \amp  0 \amp  \cdots \amp  0 \amp  0 \\ 0 \amp  1 \amp  0 \amp  0 \amp  \cdots \amp  0 \amp  0 \\ 0 \amp  0 \amp  1 \amp  0 \amp  \cdots \amp  0 \amp  0 \\ \vdots \amp  \amp  \amp  \amp  \ddots \amp  \amp  \vdots \\ 0 \amp  0 \amp  0 \amp  0 \amp  \cdots \amp  1 \amp  0 \\ 0 \amp  0\amp  0 \amp  0 \amp  \cdots \amp  0 \amp  1 \end{array}  \right]\text{.}
\end{equation*}
%
\par
The matrix \(I_n\) has the property that for any \(n \times n\) matrix \(A\),%
\begin{equation*}
AI_n = I_n A = A\,\text{.}
\end{equation*}
so \(I_n\) is a multiplicative identity in the set of all \(n \times n\) matrices. More generally, for an \(m\times n\) matrix \(A\),%
\begin{equation*}
AI_n = I_mA = A \,\text{.}
\end{equation*}
%
\end{subsectionptx}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Transpose of a Matrix}
\typeout{************************************************}
%
\begin{sectionptx}{The Transpose of a Matrix}{}{The Transpose of a Matrix}{}{}{x:section:sec_mtx_transpose}
One additional operation on matrices is the transpose. The transpose of a matrix occurs in many useful formulas in linear algebra and in applications of linear algebra.%
\begin{definition}{}{g:definition:idp11889416}%
\index{matrix!transpose}%
The \terminology{transpose} of an \(m \times n\) matrix \(A = [a_{ij}]\) is the \(n \times m\) matrix \(A^{\tr}\) whose \(i,j\)th entry is \(a_{ji}\).%
\end{definition}
Written out, the transpose of the \(m \times n\) matrix%
\begin{equation*}
A = \left[ \begin{array}{ccccc} a_{11} \amp  a_{12} \amp  \cdots    \amp  a_{1n-1} \amp  a_{1n} \\ a_{21} \amp  a_{22} \amp  \cdots    \amp  a_{2n-1} \amp  a_{2n} \\ \vdots \amp        \amp  \ddots    \amp            \amp \vdots \\ a_{m1} \amp  a_{m2} \amp  \cdots    \amp  a_{mn-1} \amp  a_{mn} \end{array}  \right]
\end{equation*}
is the \(n \times m\) matrix%
\begin{equation*}
A^{\tr} = \left[ \begin{array}{ccccc} a_{11} \amp  a_{21} \amp  \cdots \amp  a_{m-11} \amp  a_{m1} \\ a_{12} \amp  a_{22} \amp  \cdots \amp  a_{m-12} \amp  a_{m2} \\ \vdots \amp        \amp  \ddots \amp           \amp \vdots \\ a_{1n} \amp  a_{2n} \amp  \cdots \amp  a_{m-1n} \amp  a_{mn} \end{array}  \right]\text{.}
\end{equation*}
%
\par
\index{diagonal of a matrix} In other words, the transpose of a matrix \(A\) is the matrix \(A^{\tr}\) whose rows are the columns of \(A\). Alternatively, the transpose of \(A\) is the matrix \(A^{\tr}\) whose columns are the rows of \(A\). We can also view the transpose of \(A\) as the reflection of \(A\) across its main diagonal, where the \terminology{diagonal} of a matrix \(A = [a_{ij}]\) consists of the entries of the form \([a_{ii}]\).%
\begin{activity}{}{x:activity:act_A2_1_4}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the transpose of each of the indicated matrices.%
\begin{equation*}
\left[ \begin{array}{cccc} 1 \amp 2 \amp 3 \amp 4 \\ 5 \amp 6 \amp 7 \amp 8 \end{array} \right] \qquad
\left[ \begin{array}{r} 1 \\ -1 \\ 0 \end{array} \right] \qquad \left[ \begin{array}{cr} 1 \amp 2 \\ 4 \amp -3 \\ 0 \amp -1 \end{array} \right]
\end{equation*}
%
\item{}Find the transpose of the new matrix for each part above. What can you conjecture based on your results? There are certain special types of matrices that are given names.%
\item{}There are certain special types of matrices that are given names.%
\begin{definition}{}{x:definition:def_special_matrices}%
Let \(A\) be a square matrix whose \(ij\)th entry is \(a_{ij}\).%
\begin{enumerate}
\item{}The matrix \(A\) is a \emph{diagonal matrix} \index{diagonal matrix} if \(a_{ij} = 0\) whenever \(i \neq j\).%
\item{}The matrix \(A\) is a \emph{symmetric} \index{symmetric matrix} matrix if \(A^{\tr} = A\).%
\item{}The matrix \(A\) is an \emph{upper triangular} \index{upper triangular matrix} if \(a_{ij} = 0\) whenever \(i > j\).%
\item{}The matrix \(A\) is a \emph{lower triangular} \index{lower triangular matrix} if \(a_{ij} = 0\) whenever \(i \lt j\).%
\end{enumerate}
%
\end{definition}
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Find an example of a diagonal matrix \(A\). What can you say about \(A^{\tr}\)?%
\item{}Find an example of a non-diagonal symmetric matrix \(B\). If \(B^{\tr} = B\), must \(B\) be a square matrix?%
\item{}Find an example of an upper triangular matrix \(C\). What kind of a matrix is \(C^{\tr}\)?%
\end{enumerate}
\end{enumerate}
\end{activity}%
We will see later that diagonal matrices are important in that their powers are easy to calculate. Symmetric matrices arise frequently in applications such as in graph theory as adjacency matrices and in quantum mechanics as observables, and have many useful properties including being diagonalizable and having real eigenvalues, as we will also see later.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Properties of the Matrix Transpose}
\typeout{************************************************}
%
\begin{sectionptx}{Properties of the Matrix Transpose}{}{Properties of the Matrix Transpose}{}{}{x:section:sec_mtx_transpose_prop}
As with every other operation, we want to understand what properties the matrix transpose has. Properties of transposes are shown in the following theorem.%
\begin{theorem}{}{}{x:theorem:thm_transpose_props}%
Let \(A\) and \(B\) be matrices of the appropriate sizes and let \(a\) be a scalar. Then%
\begin{enumerate}
\item{}\(\displaystyle \left(A^{\tr}\right)^{\tr} = A\)%
\item{}\(\displaystyle (A+B)^{\tr} = A^{\tr} + B^{\tr}\)%
\item{}\(\displaystyle (AB)^{\tr} = B^{\tr}A^{\tr}\)%
\item{}\(\displaystyle (aA)^{\tr} = aA^{\tr}\)%
\end{enumerate}
%
\end{theorem}
The one property that might seem strange is the third one. To understand this property, suppose \(A\) is an \(m \times n\) matrix and \(B\) an \(n \times k\) matrix so that the product \(AB\) is defined. We will argue that \((AB)^{\tr} = B^{\tr}A^{\tr}\) by comparing the \(i,j\)th entry of each side.%
\begin{itemize}[label=\textbullet]
\item{}First notice that the \(i,j\)th entry of \((AB)^{\tr}\) is the \(j,i\)th entry of \(AB\). The \(j,i\)th entry of \(AB\) is found by taking the scalar product of the \(j\)th row of \(A\) with the \(i\)th column of \(B\). Thus, the \(i,j\)th entry of \((AB)^{\tr}\) is the scalar product of the \(j\)th row of \(A\) with the \(i\)th column of \(B\).%
\item{}The \(i,j\)th entry of \(B^{\tr}A^{\tr}\) is the scalar product of the \(i\)th row of \(B^{\tr}\) with the \(j\)th column of \(A^{\tr}\). But the \(i\)th row of \(B^{\tr}\) is the \(i\)th column of \(B\) and the \(j\)th column of \(A^{\tr}\) is the \(j\)th row of \(A\). So the \(i,j\)th entry of \(B^{\tr}A^{\tr}\) is the scalar product of the \(j\)th row of \(A\) with the \(i\)th column of \(B\).%
\end{itemize}
%
\par
Since the two matrices \((AB)^{\tr}\) and \(B^{\tr}A^{\tr}\) have the same size and same corresponding entries, they are the same matrix.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_mtx_ops_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp11948424}%
Let%
\begin{equation*}
\begin{array}{ccc} A = \left[ \begin{array}{ccrc} 1\amp 2\amp 0\amp 1\\3\amp 0\amp -4\amp 5\\7\amp 6\amp -1\amp 0 \end{array}  \right] \amp \amp   B = \left[ \begin{array}{rcr} -2\amp 4\amp -3\\5\amp 1\amp 9\\1\amp 1\amp -2 \end{array}  \right] \\ \amp \amp  \\ C = \left[ \begin{array}{crc} 0\amp -1\amp 6\\3\amp -2\amp 5\\1\amp 0\amp 4 \end{array}  \right] 
\amp \amp  D = \left[ \begin{array}{cr} 10\amp -4\\5\amp 2\\8\amp -1 \end{array}  \right] \\ \amp \amp  \\ E = \left[ \begin{array}{cr} 1\amp 0\\4\amp -3\\5\amp -1 \end{array}  \right] \amp \text{ and }   \amp  F = \left[ \begin{array}{rcr} -2\amp 1\amp 5\\6\amp 3\amp -8\\1\amp 0\amp -1\\ 7\amp 0\amp -5 \end{array}  \right]. \end{array}\text{.}
\end{equation*}
%
\par
Determine the results of the following operations, if defined. If not defined, explain why.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(AF\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp11944328}{}\quad{}Since \(A\) is a \(3 \times 4\) matrix and \(F\) is a \(4 \times 3\) matrix, the number of columns of \(A\) equals the number of rows of \(F\) and the matrix produce \(AF\) is defined. Recall that if \(F = [\vf_1 \ \vf_2 \ \vf_3]\), where \(\vf_1\), \(\vf_2\), \(\vf_3\) are the columns of \(F\), then \(AF = [A\vf_1 \ A\vf_2 \ A\vf_3]\). Recall also that \(A \vf_1\) is the linear combination of the columns of \(A\) with weights from \(\vf_1\), so%
\begin{align*}
A\vf_1 \amp = \left[ \begin{array}{ccrc} 1\amp 2\amp 0\amp 1\\
3\amp 0\amp -4\amp 5\\
7\amp 6\amp -1\amp 0 \end{array} \right] \left[ \begin{array}{r} -2\\
6\\
1\\
7 \end{array} \right]\\
\amp = (-2) \left[ \begin{array}{c} 1\\
3\\
7 \end{array} \right]  + (6)  \left[ \begin{array}{c} 2\\
0\\
6 \end{array} \right]  + (1)  \left[ \begin{array}{r} 0\\
-4\\
-1 \end{array} \right] + (7)  \left[ \begin{array}{c} 1\\
5\\
0 \end{array} \right]\\
\amp =  \left[ \begin{array}{c} -2+12+0+7\\
-6+0-4+35\\
-14+36-1+0\end{array} \right]\\
\amp =  \left[ \begin{array}{c} 17\\
25\\
21 \end{array} \right]\text{,}
\end{align*}
%
\begin{align*}
A\vf_2 \amp = \left[ \begin{array}{ccrc} 1\amp 2\amp 0\amp 1\\
3\amp 0\amp -4\amp 5\\
7\amp 6\amp -1\amp 0 \end{array} \right] \left[ \begin{array}{c} 1\\
3\\
0\\
0 \end{array} \right]\\
\amp = (1) \left[ \begin{array}{c} 1\\
3\\
7 \end{array} \right]  + (3)  \left[ \begin{array}{c} 2\\
0\\
6 \end{array} \right]  + (0)  \left[ \begin{array}{r} 0\\
-4\\
-1 \end{array} \right] + (0)  \left[ \begin{array}{c} 1\\
5\\
0 \end{array} \right]\\
\amp =  \left[ \begin{array}{c} 1+6+0+0\\
3+0+0+0\\
7+18+0+0\end{array} \right]\\
\amp =  \left[ \begin{array}{c} 7\\
3\\
25 \end{array} \right]\text{,}
\end{align*}
and%
\begin{align*}
A\vf_3 \amp = \left[ \begin{array}{ccrc} 1\amp 2\amp 0\amp 1\\
3\amp 0\amp -4\amp 5\\
7\amp 6\amp -1\amp 0 \end{array} \right] \left[ \begin{array}{r} 5\\
-8\\
-1\\
-5 \end{array} \right]\\
\amp = (5) \left[ \begin{array}{c} 1\\
3\\
7 \end{array} \right]  - (8)  \left[ \begin{array}{c} 2\\
0\\
6 \end{array} \right]  - (1)  \left[ \begin{array}{r} 0\\
-4\\
-1 \end{array} \right] - (5)  \left[ \begin{array}{c} 1\\
5\\
0 \end{array} \right]\\
\amp =  \left[ \begin{array}{c} 5-16-0-5\\
15-0+4-25\\
35-48+1-0\end{array} \right]\\
\amp =  \left[ \begin{array}{r} -16\\
-6\\
-12 \end{array} \right]\text{.}
\end{align*}
So \(AF = \left[ \begin{array}{ccr} 17\amp 7\amp -16\\25\amp 3\amp -6\\21\amp 25\amp -12 \end{array}  \right]\). Alternatively, if \(A = \left[ \begin{array}{c} \va_1\\ \va_2 \\ \va_3 \\ \va_4 \end{array} \right]\), then the matrix product \(AF\) is the matrix whose \(ij\) entry is \(\va_i \cdot \vf_j\). Using this method we have%
\begin{equation*}
AF = \left[ \begin{array}{ccc} \va_1 \cdot \vf_1\amp \va_1 \cdot \vf_2 \amp  \va_1 \cdot \vf_3 \\ \va_2 \cdot \vf_1\amp \va_2 \cdot \vf_2 \amp  \va_2 \cdot \vf_3 \\ \va_3 \cdot \vf_1\amp \va_3 \cdot \vf_2 \amp  \va_3 \cdot \vf_3 \end{array}  \right]\text{.}
\end{equation*}
Now%
\begin{align*}
\va_1 \cdot \vf_1 \amp = (1)(-2)+(2)(6)+(0)(1)+(1)(7) = 17\\
\va_1 \cdot \vf_2 \amp = (1)(1)+(2)(3)+(0)(0)+(1)(0) = 7\\
\va_1 \cdot \vf_3 \amp = (1)(5)+(2)(-8)+(0)(-1)+(1)(-5) = -16\\
\va_2 \cdot \vf_1 \amp = (3)(-2)+(0)(6)+(-4)(1)+(5)(7) = 25\\
\va_2 \cdot \vf_2 \amp = (3)(1)+(0)(3)+(-4)(0)+(5)(0) = 3\\
\va_2 \cdot \vf_3 \amp = (3)(5)+(0)(-8)+(-4)(-1)+(5)(-5) = -6\\
\va_3 \cdot \vf_1 \amp = (7)(-2)+(6)(6)+(-1)(1)+(0)(7) = 21\\
\va_3 \cdot \vf_2 \amp = (7)(1)+(6)(3)+(-1)(0)+(0)(0) = 25\\
\va_3 \cdot \vf_3 \amp =(7)(5)+(6)(-8)+(-1)(-1)+(0)(-5) = -12\text{,}
\end{align*}
so \(AF = \left[ \begin{array}{ccr} 17\amp 7\amp -16\\25\amp 3\amp -6\\21\amp 25\amp -12 \end{array}  \right]\).%
\item{}\(A(BC)\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp11977480}{}\quad{}Since \(BC\) is a \(3 \times 3\) matrix but \(A\) is \(3 \times 4\), the number of columns of \(A\) is not equal to the number of rows of \(BC\). We conclude that \(A(BC)\) is not defined.%
\item{}\((BC)A\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp11980808}{}\quad{}Since \(BC\) is a \(3 \times 3\) matrix and \(A\) is \(3 \times 4\), the number of columns of \(BC\) is equal to the number of rows of \(A\). Thus, the quantity \((BC)A\) is defined. First we calculate \(BC\) using the dot product of the rows of \(B\) with the columns of \(C\). Letting \(B = \left[ \begin{array}{c} \vb_1 \\ \vb_2 \\ \vb_3 \end{array}  \right]\) and \(C = [\vc_1 \ \vc_2 \ \vc_3]\), where \(\vb_1\), \(\vb_2\), and \(\vb_3\) are the rows of \(B\) and \(\vc_1\), \(\vc_2\), and \(\vc_3\) are the columns of \(C\), we have%
\begin{equation*}
BC = \left[ \begin{array}{ccc} \vb_1 \cdot \vc_1 \amp  \vb_1 \cdot \vc_2 \amp  \vb_1 \cdot \vc_3 \\ \vb_2 \cdot \vc_1 \amp  \vb_2 \cdot \vc_2 \amp  \vb_2 \cdot \vc_3 \\ \vb_3 \cdot \vc_1 \amp  \vb_3 \cdot \vc_2 \amp  \vb_3 \cdot \vc_3 \end{array}  \right]\text{.}
\end{equation*}
Now%
\begin{align*}
\vb_1 \cdot \vc_1 \amp = (-2)(0)+(4)(3)+(-3)(1) = 9\\
\vb_1 \cdot \vc_1 \amp = (-2)(-1)+(4)(-2)+(-3)(0) = -6\\
\vb_1 \cdot \vc_1 \amp = (-2)(6)+(4)(5)+(-3)(4) = -4\\
\vb_1 \cdot \vc_1 \amp = (5)(0)+(1)(3)+(9)(1) = 12\\
\vb_1 \cdot \vc_1 \amp = (5)(-1)+(1)(-2)+(9)(0) = -7\\
\vb_1 \cdot \vc_1 \amp = (5)(6)+(1)(5)+(9)(4) = 71\\
\vb_1 \cdot \vc_1 \amp =(1)(0)+(1)(3)+(-2)(1) = 1\\
\vb_1 \cdot \vc_1 \amp = (1)(-1)+(1)(-2)+(-2)(0) = -3\\
\vb_1 \cdot \vc_1 \amp = (1)(6)+(1)(5)+(-2)(4) = 3\text{,}
\end{align*}
so \(BC = \left[ \begin{array}{crr} 9\amp -6\amp -4 \\ 12\amp -7\amp 71 \\ 1\amp -3\amp 3 \end{array}  \right]\). If \(BC = \left[ \begin{array}{c} \vr_1 \\  \vr_2 \\ \vr_3 \end{array}  \right]\) and \(A = [\vs_1 \ \vs_2 \ \vs_3 \ \vs_4]\), where \(\vr_1\), \(\vr_2\), and \(\vr_3\) are the rows of \(BC\) and \(\vs_1\), \(\vs_2\), \(\vs_3\), and \(\vs_4\) are the columns of \(A\), then%
\begin{equation*}
(BC)A = \left[ \begin{array}{cccc} \vr_1 \cdot \vs_1 \amp  \vr_1 \cdot \vs_2 \amp  \vr_1 \cdot \vr_3 \amp  \vr_1 \cdot \vs_4 \\ \vr_2 \cdot \vs_1 \amp  \vr_2 \cdot \vs_2 \amp  \vr_2 \cdot \vs_3 \amp  \vr_2 \cdot \vs_4 \\ \vr_3 \cdot \vs_1 \amp  \vr_3 \cdot \vs_2 \amp  \vr_3 \cdot \vs_3 \amp  \vr_3 \cdot \vs_4 \end{array}  \right]\text{.}
\end{equation*}
Now%
\begin{align*}
\vr_1 \cdot \vs_1 \amp = (9)(1)+(-6)(3)+(-4)(7) = -37\\
\vr_1 \cdot \vs_2 \amp = (9)(2)+(-6)(0)+(-4)(6) = -6\\
\vr_1 \cdot \vs_3 \amp = (9)(0)+(-6)(-4)+(-4)(-1) = 28\\
\vr_1 \cdot \vs_4 \amp = (9)(1)+(-6)(5)+(-4)(0) = -21\\
\vr_2 \cdot \vs_1 \amp = (12)(1)+(-7)(3)+(71)(7) = 488\\
\vr_2 \cdot \vs_2 \amp = (12)(2)+(-7)(0)+(71)(6) = 450\\
\vr_2 \cdot \vs_3 \amp = (12)(0)+(-7)(-4)+(71)(-1) = -43\\
\vr_2 \cdot \vs_4 \amp = (12)(1)+(-7)(5)+(71)(0) = -23\\
\vr_3 \cdot \vs_1 \amp = (1)(1)+(-3)(3)+(3)(7) = 13\\
\vr_3 \cdot \vs_2 \amp = (1)(2)+(-3)(0)+(3)(6) = 20\\
\vr_3 \cdot \vs_3 \amp = (1)(0)+(-3)(-4)+(3)(-1) = 9\\
\vr_3 \cdot \vs_4 \amp = (1)(1)+(-3)(5)+(3)(0) = -14\text{,}
\end{align*}
so \((BC)A =  \left[ \begin{array}{rrrr} -37\amp -6\amp 28\amp -21 \\ 488\amp 450\amp -43\amp -23 \\ 13\amp 20\amp 9\amp -14 \end{array}  \right]\).%
\item{}\((B+C)D\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp12006280}{}\quad{}Since \(B\) and \(C\) are both \(3 \times 3\) matrices, their sum is defined and is a \(3 \times 3\) matrix. Because \(D\) is \(3 \times 2\) matrix, the number of columns of \(B+C\) is equal to the number of rows of \(D\). Thus, the quantity \((B+C)D\) is defined and, using the row-column method of matrix multiplication as earlier,%
\begin{align*}
(B+C)D \amp = \left( \left[ \begin{array}{rcr} -2\amp 4\amp -3\\
5\amp 1\amp 9\\
1\amp 1\amp -2 \end{array} \right] +  \left[ \begin{array}{crc} 0\amp -1\amp 6\\
3\amp -2\amp 5\\
1\amp 0\amp 4 \end{array} \right] \right) \left[ \begin{array}{cr} 10\amp -4\\
5\amp 2\\
8\amp -1 \end{array} \right]\\
\amp =  \left[ \begin{array}{ccc} -2+0\amp 4-1\amp -3+6\\
5+3\amp 1-2\amp 9+5\\
1+1\amp 1+0\amp -2+4 \end{array} \right]  \left[ \begin{array}{cr} 10\amp -4\\
5\amp 2\\
8\amp -1 \end{array} \right]\\
\amp = \left[ \begin{array}{rrc} -2\amp 3\amp 3\\
8\amp -1\amp 14\\
2\amp 1\amp 2 \end{array} \right]  \left[ \begin{array}{cr} 10\amp -4\\
5\amp 2\\
8\amp -1 \end{array} \right]\\
\amp =  \left[ \begin{array}{cr} 19\amp 11\\
187\amp -48\\
41\amp -8 \end{array} \right]\text{.}
\end{align*}
%
\item{}\(D^{\tr}E\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp12010376}{}\quad{}Since \(D^{\tr}\) is a \(2 \times 3\) matrix and \(E\) is \(3 \times 2\), the number of columns of \(D^{\tr}\) is equal to the number of rows of \(E\). Thus, \(D^{\tr}E\) is defined and%
\begin{align*}
D^{\tr}E \amp = \left[ \begin{array}{cr} 10\amp -4\\
5\amp 2\\
8\amp -1 \end{array} \right]^{\tr} \left[ \begin{array}{cr} 1\amp 0\\
4\amp -3\\
5\amp -1 \end{array} \right]\\
\amp = \left[ \begin{array}{rcr} 10\amp 5\amp 8\\
-4\amp 2\amp -1 \end{array} \right] \left[ \begin{array}{cr} 1\amp 0\\
4\amp -3\\
5\amp -1 \end{array} \right]\\
\amp = \left[ \begin{array}{rr} 70\amp -23\\
-1\amp -5 \end{array} \right]\text{.}
\end{align*}
%
\item{}\(\left(A^{\tr}+F\right)^{\tr}\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp12017032}{}\quad{}The fact that \(A\) is a \(3 \times 4\) matrix means that \(A^{\tr}\) is a \(4 \times 3\) matrix. Since \(F\) is also a \(4 \times 3\) matrix, the sum \(A^{\tr}+F\) is defined. The transpose of any matrix is also defined, so \(\left(A^{\tr}+F\right)^{\tr}\) is defined and%
\begin{align*}
\left(A^{\tr}+F\right)^{\tr} \amp = \left( \left[ \begin{array}{ccrc} 1\amp 2\amp 0\amp 1\\
3\amp 0\amp -4\amp 5\\
7\amp 6\amp -1\amp 0 \end{array} \right]^{\tr} +  \left[ \begin{array}{rcr} -2\amp 1\amp 5\\
6\amp 3\amp -8\\
1\amp 0\amp -1\\
7\amp 0\amp -5 \end{array} \right] \right)^{\tr}\\
\amp = \left( \left[ \begin{array}{crr} 1\amp 3\amp 7\\
2\amp 0\amp 6\\
0\amp -4\amp -1\\
1\amp 5\amp 0 \end{array} \right] + \left[ \begin{array}{rcr} -2\amp 1\amp 5\\
6\amp 3\amp -8\\
1\amp 0\amp -1\\
7\amp 0\amp -5 \end{array} \right] \right)^{\tr}\\
\amp = \left( \left[ \begin{array}{ccc} 1-2\amp 3+1\amp 7+5\\
2+6\amp 0+3\amp 6-8\\
0+1\amp -4+0\amp -1-1\\
1+7\amp 5+0\amp 0-5 \end{array} \right] \right)^{\tr}\\
\amp = \left( \left[ \begin{array}{rrr} -1\amp 4\amp 12\\
8\amp 3\amp -2\\
1\amp -4\amp -2\\
8\amp 5\amp -5 \end{array} \right] \right)^{\tr}\\
\amp = \left[ \begin{array}{rrrr} -1\amp 8\amp 1\amp 8\\
4\amp 3\amp -4\amp 5\\
12\amp -2\amp -2\amp -5 \end{array} \right]\text{.}
\end{align*}
%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp12024072}%
Let \(A = \left[ \begin{array}{cr} 2\amp -1\\7\amp -2 \end{array} \right]\) and \(B = \left[ \begin{array}{rc} 4\amp 6 \\ -3\amp 5 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Determine the matrix sum \(A+B\). Then use this sum to calculate \((A+B)^2\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp12030856}{}\quad{}Adding corresponding terms shows that \(A+B = \left[ \begin{array}{cc} 6\amp 5\\4\amp 3 \end{array} \right]\). Squaring this sum yields the result \((A+B)^2 = \left[ \begin{array}{cc} 56\amp 45 \\ 36\amp 29 \end{array} \right]\).%
\item{}Now calculate \((A+B)^2\) in a different way. Use the fact that matrix multiplication distributes over matrix addition to expand (like foiling) \((A+B)^2\) into a sum of matrix products. The calculate each summand and add to find \((A+B)^2\). You should obtain the same result as part (a). If not, what could be wrong?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp12035592}{}\quad{}Expanding \((A+B)^2\) (remember that matrix multiplication is not commutative) gives us%
\begin{align*}
(A+B)^2 \amp = (A+B)(A+B)\\
\amp = A^2 + AB + BA + B^2\\
\amp = \left[ \begin{array}{rr} -3\amp 0\\
0\amp -3 \end{array} \right] +  \left[ \begin{array}{cc} 11\amp 7\\
34\amp 32 \end{array} \right] +  \left[ \begin{array}{cr} 50\amp -16\\
29\amp -7 \end{array} \right] +  \left[ \begin{array}{rc} -2\amp 54\\
-27\amp 7 \end{array} \right]\\
\amp =  \left[ \begin{array}{cc} 56\amp 45\\
36\amp 29 \end{array} \right]
\end{align*}
just as in part (a). If instead you obtained the matrix \(\left[ \begin{array}{cc} 17\amp 68 \\41\amp 68 \end{array}  \right]\) you likely made the mistake of equating \((A+B)^2\) with \(A^2+2AB+B^2\). These two matrices are not equal in general, because we cannot say that \(AB\) is equal to \(BA\).%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_mtx_ops_summ}
In this section we defined a matrix sum, scalar multiples of matrices, the matrix product, and the transpose of a matrix.%
\begin{itemize}[label=\textbullet]
\item{}The sum of two \(m \times n\) matrices \(A = [a_{ij}]\) and \(B = [b_{ij}]\) is the \(m \times n\) matrix \(A+B\) whose \(i,j\)th entry is \(a_{ij} + b_{ij}\).%
\item{}If \(A = [a_{ij}]\) is an \(m \times n\) matrix, the scalar multiple \(kA\) of \(A\) by the scalar \(k\) is the \(m \times n\) matrix whose \(i,j\)th entry is \(ka_{ij}\).%
\item{}If \(A\) is a \(k \times m\) matrix and \(B = [\vb_1 \ \vb_2 \ \cdots \ \vb_n]\) is an \(m \times n\) matrix, then the matrix product \(AB\) of the matrices \(A\) and \(B\) is the \(k \times n\) matrix%
\begin{equation*}
[A\vb_1 \ A\vb_2 \ \cdots \ A\vb_n]\text{.}
\end{equation*}
The matrix product is defined in this way so that the matrix of a composite \(S \circ T\) of linear transformations is the product of matrices of \(S\) and \(T\).%
\item{}An alternate way of calculating the product of an \(k \times m\) matrix \(A\) with rows \(\vr_1\), \(\vr_2\), \(\ldots\), \(\vr_k\) and an \(m \times n\) matrix \(B\) with columns \(\vb_1\), \(\vb_2\), \(\ldots\), \(\vb_n\) is that the product \(AB\) is the \(k \times n\) matrix whose \(i,j\)th entry is \(\vr_i \cdot \vb_j\).%
\item{}Matrix multiplication does not behave as the standard multiplication on real numbers. For example, we can have a product of two non-zero matrices equal to the zero matrix and there is no cancellation law for matrix multiplication.%
\item{}The transpose of an \(m \times n\) matrix \(A = [a_{ij}]\) is the \(n \times m\) matrix \(A^{\tr}\) whose \(i,j\)th entry is \(a_{ji}\).%
\end{itemize}
%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_mtx_ops_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp12068616}%
Calculate \(AB\) for each of the following matrix pairs by hand in two ways.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(A = \left[ \begin{array}{cc} 1\amp 0\\0\amp 1\\0\amp 0 \end{array} \right]\), \(B = \left[ \begin{array}{cc} a\amp b \\ c\amp d \end{array} \right]\)%
\item{}\(A = \left[ \begin{array}{ccr} 1\amp 0\amp -1 \end{array} \right]\), \(B = \left[ \begin{array}{cc} 1\amp 2 \\ 2\amp 3 \\ 3\amp 4 \end{array} \right]\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp12066824}%
For each of the following \(A\) matrices, find all \(2\times 2\) matrices \(B=\left[ \begin{array}{cc} a\amp b\\c\amp d \end{array} \right]\) which commute with the given \(A\). (Two matrices \(A\) and \(B\) commute with each other if \(AB = BA\).)%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(A = \left[ \begin{array}{cc} 2\amp 0 \\ 0 \amp 2 \end{array} \right]\)%
\item{}\(A = \left[ \begin{array}{cc} 2\amp 0 \\ 0 \amp 3 \end{array} \right]\)%
\item{}\(A = \left[ \begin{array}{cc} 0\amp 1 \\ 0 \amp 0 \end{array} \right]\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp12077960}%
Find all possible, if any, \(X\) matrices satisfying each of the following matrix equations.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\left[ \begin{array}{cc} 1\amp 2 \\ 0 \amp 2 \end{array} \right] X = \left[ \begin{array}{cc} 0\amp 1 \\ 0 \amp 0 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{rr} 1\amp -2 \\ -2 \amp 4 \end{array} \right] X = \left[ \begin{array}{cc} 0\amp 1 \\ 0 \amp 0 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{rr} 1\amp -2 \\ -2 \amp 4 \end{array} \right] X = \left[ \begin{array}{cc} 0\amp 1 \\ 0 \amp -2 \end{array} \right]\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp12080776}%
For each of the following \(A\) matrices, compute \(A^2=AA, A^3=AAA, A^4\). Use your results to conjecture a formula for \(A^m\). Interpret your answer geometrically using the transformation interpretation.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(A = \left[ \begin{array}{cc} 2\amp 0 \\ 0 \amp 3 \end{array} \right]\)%
\item{}\(A = \left[ \begin{array}{cc} 1\amp 1 \\ 0 \amp 1 \end{array} \right]\)%
\item{}\(A = \left[ \begin{array}{cr} 0\amp -1 \\ 1 \amp 0 \end{array} \right]\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp12087176}%
If \(A\vv=2\vv\) for unknown \(A\) matrix and \(\vv\) vector, determine an expression for \(A^2 \vv\), \(A^3\vv\), \textellipsis{}, \(A^m\vv\).%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp12092424}%
If \(A\vv=2\vv\) and \(A\vu=3\vu\), find an expression for \(A^m(a\vv+b\vu)\) in terms of \(\vv\) and \(\vu\).%
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp12093704}%
A matrix \(A\) is a \emph{nilpotent} matrix if \(A^m=0\), i.e., \(A^m\) is the zero matrix, for some positive integer \(m\). Explain why the matrices%
\begin{equation*}
A = \left[ \begin{array}{cc} 0 \amp  a \\ 0 \amp  0 \end{array}  \right] \, , \, B = \left[ \begin{array}{ccc} 0 \amp  a \amp  b \\ 0 \amp  0\amp  c\\ 0\amp 0\amp 0 \end{array}  \right]
\end{equation*}
are nilpotent matrices.%
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp12099464}%
Suppose \(A\) is an \(n\times n\) matrix for which \(A^2=0\). Show that there is a matrix \(B\) for which \((I_n+A)B=I_n\) where \(I_n\) is the identity matrix of size \(n\).%
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp12099848}%
Let \(A\), \(B\), and \(C\) be \(m \times n\) matrices and let \(a\) and \(b\) be scalars. Verify \hyperref[x:theorem:thm_matrix_sum_properties]{Theorem~{\xreffont\ref{x:theorem:thm_matrix_sum_properties}}}. That is, show that%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(A+B = B+A\)%
\item{}\((A+B) + C = A + (B+C)\)%
\item{}The \(m \times n\) matrix \(0\) whose entries are all 0 has the property that \(A + 0 = A\).%
\item{}The scalar multiple \((-1)A\) of the matrix \(A\) has the property that \((-1)A + A = 0\).%
\item{}\((a+b) A = aA + bA\)%
\item{}\(a(A+B) = aA + aB\)%
\item{}\((ab) A = a(bA)\)%
\item{}\(1A=A\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{10}{}{}{g:exercise:idp11858696}%
Let \(A\), \(B\), and \(C\) be matrices of the appropriate sizes for all sums and products to be defined and let \(a\) be a scalar. Verify the remaining parts of \hyperref[x:theorem:thm_matrix_product_properties]{Theorem~{\xreffont\ref{x:theorem:thm_matrix_product_properties}}}. That is, show that%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\((AB)C = A(BC)\)%
\item{}\(A(B+C) = AB + AC\)%
\item{}There is a square matrix \(I_n\) with the property that \(AI_n = A\) or \(I_nA = A\) for whichever product is defined.%
\item{}\(a(AB) = (aA)B = A(aB)\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{11}{}{}{g:exercise:idp11852168}%
Let \(A = [a_{ij}]\) and \(B = [b_{ij}]\) be matrices of the appropriate sizes, and let \(a\) be a scalar. Verify the remaining parts of \hyperref[x:theorem:thm_transpose_props]{Theorem~{\xreffont\ref{x:theorem:thm_transpose_props}}}. That is, show that%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\left(A^{\tr}\right)^{\tr} = A\)%
\item{}\((A+B)^{\tr} = A^{\tr} + B^{\tr}\)%
\item{}\((aA)^{\tr} = aA^{\tr}\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{12}{}{}{x:exercise:ex_2_a_matrix_exponential}%
\index{matrix!exponential}%
The \terminology{matrix exponential} is an important tool in solving differential equations. Recall from calculus that the Taylor series expansion for \(e^x\) centered at \(x=0\) is%
\begin{equation*}
e^x = \sum_{n=0}^{\infty} \frac{x^n}{n!} = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots\text{,}
\end{equation*}
and that this Taylor series converges to \(e^x\) for every real number \(x\). We extend this idea to define the matrix exponential \(e^A\) for any square matrix \(A\) with real entries as%
\begin{equation*}
e^A = \sum_{n=0}^{\infty} \frac{1}{n!}A^n = I_n + A + \frac{1}{2!}A^2 + \frac{1}{3!}A^3 + \cdots
\end{equation*}
We explore this idea with an example. Let \(B = \left[ \begin{array}{cr} 2\amp 0\\0\amp -1 \end{array}  \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Calculate \(B^2\), \(B^3\), \(B^4\). Explain why \(B^n = \left[ \begin{array}{cc} 2^n\amp 0\\0\amp (-1)^n \end{array} \right]\) for any positive integer \(n\).%
\item{}Show that  \(I_2 + B + B^2 + B^3 + B^4\) is equal to%
\begin{equation*}
\left[ \begin{array}{cc} 1+2+\frac{2^2}{2} + \frac{2^3}{3!} + \frac{2^4}{4!}\amp 0\\0\amp 1+(-1)+\frac{(-1)^2}{2} + \frac{(-1)^3}{3!} + \frac{(-1)^4}{4!} \end{array}  \right]\text{.}
\end{equation*}
%
\item{}Explain why \(e^B = \left[ \begin{array}{cc} e^2\amp 0\\0\amp e^{-1} \end{array} \right]\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{13}{}{}{g:exercise:idp12217224}%
Show that if \(A\) and \(B\) are \(2\times 2\) rotation matrices, then \(AB\) is also a \(2\times 2\) rotation matrix.%
\end{divisionexercise}%
\begin{divisionexercise}{14}{}{}{g:exercise:idp12222728}%
Label each of the following statements as True or False. Provide justification for your response. Throughout, assume that matrices are of the appropriate sizes so that any matrix sums or products are defined.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
For any three matrices \(A, B, C\) with \(A \neq 0\), \(AB=AC\) implies \(B=C\).%
\item{}\lititle{True\slash{}False.}\par%
For any three matrices \(A, B, C\) with \(A \neq 0\), \(AB=CA\) implies \(B=C\).%
\item{}\lititle{True\slash{}False.}\par%
If \(A^2\) is the zero matrix, then \(A\) itself is the zero matrix.%
\item{}\lititle{True\slash{}False.}\par%
If \(AB=BA\) for every \(n \times n\) matrix \(B\), then \(A\) is the identity matrix \(I_n\).%
\item{}\lititle{True\slash{}False.}\par%
If matrix products \(AB\) and \(BA\) are both defined, then \(A\) and \(B\) are both square matrices of the same size.%
\item{}\lititle{True\slash{}False.}\par%
If \(\vx_1\) is a solution for \(A\vx=\vb_1\) (i.e., that \(A\vx_1 = \vb_1\)) and \(\vx_2\) is a solution for \(B\vx=\vb_2\), then \(\vx_1+\vx_2\) is a solution for \((A+B)\vx=\vb_1+\vb_2\).%
\item{}\lititle{True\slash{}False.}\par%
If \(B\) is an \(m\times n\) matrix with two equal columns, then the matrix \(AB\) has two equal columns for every \(k \times m\) matrix.%
\item{}\lititle{True\slash{}False.}\par%
If \(A^2 = I_2\), then \(A=-I_2\) or \(A=I_2\).%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Strassen's Algorithm and Partitioned Matrices}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Strassen's Algorithm and Partitioned Matrices}{}{Project: Strassen's Algorithm and Partitioned Matrices}{}{}{x:section:sec_proj_starassen}
\index{Strassen's algorithm}%
Strassen's algorithm is an algorithm for matrix multiplication that can be more efficient than the standard row-column method. To understand this method, we begin with the \(2 \times 2\) case which will highlight the essential ideas.%
\begin{project}{}{x:project:x_pact_Strassen_1}%
We first work with the \(2 \times 2\) case.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(A = [a_{ij}] = \left[ \begin{array}{cc} 1\amp 2\\3\amp 4 \end{array}  \right]\) and \(B =  [b_{ij}] = \left[ \begin{array}{cc} 5\amp 6\\7\amp 8 \end{array}  \right]\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Calculate the matrix product \(AB\).%
\item{}Rather than using eight multiplications to calculate \(AB\), Strassen came up with the idea of using the following seven products:%
\begin{align*}
h_1 \amp = (a_{11}+a_{22})(b_{11}+b_{22})\\
h_2 \amp = (a_{21}+a_{22})b_{11}\\
h_3 \amp = a_{11}(b_{12}-b_{22})\\
h_4 \amp = a_{22}(b_{21}-b_{11})\\
h_5 \amp = (a_{11}+a_{12})b_{22}\\
h_6 \amp =(a_{21}-a_{11})(b_{11}+b_{12})\\
h_7 \amp = (a_{12}-a_{22})(b_{21}+b_{22})\text{.}
\end{align*}
Calculate \(h_1\) through \(h_7\) for the given matrices \(A\) and \(B\). Then calculate the quantities%
\begin{equation*}
h_1+h_4-h_5+h_7,  \  h_3+h_5, \  h_2+h_4, \ \text{ and }   h_1+h_3-h_2+h_6\text{.}
\end{equation*}
What do you notice?%
\end{enumerate}
\item{}Now we repeat part (a) in general. Suppose we want to calculate the matrix product \(AB\) for arbitrary \(2 \times 2\) matrices \(A = \left[ \begin{array}{cc} a_{11}\amp a_{12}\\a_{21}\amp a_{22} \end{array}  \right]\) and \(B = \left[ \begin{array}{cc} b_{11}\amp b_{12}\\b_{21}\amp b_{22} \end{array}  \right]\). Let%
\begin{align*}
h_1 \amp = (a_{11}+a_{22})(b_{11}+b_{22})\\
h_2 \amp = (a_{21}+a_{22})b_{11}\\
h_3 \amp = a_{11}(b_{12}-b_{22})\\
h_4 \amp = a_{22}(b_{21}-b_{11})\\
h_5 \amp = (a_{11}+a_{12})b_{22}\\
h_6 \amp =(a_{21}-a_{11})(b_{11}+b_{12})\\
h_7 \amp = (a_{12}-a_{22})(b_{21}+b_{22})\text{.}
\end{align*}
Show that%
\begin{equation*}
AB = \left[ \begin{array}{cc} h_1+h_4-h_5+h_7 \amp  h_3+h_5 \\ h_2+h_4 \amp  h_1+h_3-h_2+h_6 \end{array}  \right]\text{.}
\end{equation*}
%
\end{enumerate}
\end{project}%
\index{partitioned matrices} The next step is to understand how Strassen's algorithm can be applied to larger matrices. This involves the idea of partitioned (or block) matrices. Recall that the matrix-matrix product of the \(k \times m\) matrix \(A\) and the \(m \times n\) matrix \(B = [\vb_1 \ \vb_2 \ \cdots \ \vb_n]\) is defined as%
\begin{equation*}
AB = [A\vb_1 \ A\vb_2 \ \cdots \ A\vb_n]\text{.}
\end{equation*}
%
\par
In this process, we think of \(B\) as being partitioned into \(n\) columns. We can expand on this idea to partition both \(A\) and \(B\) when calculating a matrix-matrix product.%
\begin{project}{}{g:project:idp12268808}%
We illustrate the idea of partitioned matrices with an example. Let \(A = \left[ \begin{array}{crcrc} 1\amp -2\amp 3\amp -6\amp 4 \\ 7\amp 5\amp 2\amp -1\amp 0 \\ 3\amp -8\amp 1\amp 0\amp 9 \end{array}  \right]\).We can partition \(A\) into smaller matrices%
\begin{equation*}
A = \left[ \begin{array}{crc|rc} 1\amp -2\amp 3\amp -6\amp 4 \\ 7\amp 5\amp 2\amp -1\amp 0  \\ \hline 3\amp -8\amp 1\amp 0\amp 9 \end{array}  \right]\text{,}
\end{equation*}
which are indicated by the vertical and horizontal lines. As a shorthand, we can describe this partition of \(A\) as%
\begin{equation*}
A = \left[ \begin{array}{cc} A_{11}\amp A_{12} \\ A_{21}\amp A_{22} \end{array}  \right]\text{,}
\end{equation*}
where \(A_{11} = \left[ \begin{array}{crc} 1\amp -2\amp 3\\ 7\amp 5\amp 2 \end{array}  \right]\), \(A_{12} = \left[ \begin{array}{rc} -6\amp 4 \\ -1\amp 0 \end{array}  \right]\), \(A_{21} = \left[ \begin{array}{crc} 3\amp -8\amp 1 \end{array}  \right]\), and \(A_{22} =  [0 \ 9 ]\). The submatrices \(A_{ij}\) are called \emph{blocks}. If \(B\) is a matrix such that \(AB\) is defined, then \(B\) must have five rows. As an example, \(AB\) is defined if \(B = \left[ \begin{array}{cc} 1\amp 3\\2\amp 0 \\ 4\amp 1\\6\amp 5\\4\amp 2 \end{array}  \right]\). The partition of \(A\) breaks \(A\) up into blocks with three and two columns, respectively. So if we partition \(B\) into blocks with three and two rows, then we can use the blocks to calculate the matrix product \(AB\). For example, partition \(B\) as%
\begin{equation*}
B = \left[ \begin{array}{cc} 1\amp 3\\2\amp 0 \\ 4\amp 1\\ \hline 6\amp 5\\4\amp 2 \end{array}  \right] = \left[ \begin{array}{c} B_{11}\\B_{21} \end{array}  \right]\text{.}
\end{equation*}
%
\par
Show that%
\begin{equation*}
AB = \left[ \begin{array}{cc} A_{11}\amp A_{12} \\ A_{21}\amp A_{22} \end{array}  \right] \left[ \begin{array}{c} B_{11}\\B_{21} \end{array}  \right] = \left[ \begin{array}{cc} A_{11}B_{11}+A_{12}B_{21} \\ A_{21}B_{11}+A_{22}B_{21} \end{array}  \right]\text{.}
\end{equation*}
%
\end{project}%
An advantage to using partitioned matrices is that computations with them can be done in parallel, which lessens the time it takes to do the work. In general, we can multiply partitioned matrices as though the submatrices are scalars. That is,%
\begin{equation*}
\left[ \begin{array}{cccc} A_{11}\amp A_{12}\amp \cdots\amp A_{1m} \\ A_{21}\amp A_{22}\amp \cdots\amp A_{2m}\\ \vdots \amp  \vdots \amp \ddots \amp  \vdots \\ A_{i1}\amp A_{i2}\amp \cdots\amp A_{im}  \\ \vdots \amp  \vdots \amp \ddots \amp  \vdots \\ A_{k1}\amp A_{k2}\amp \cdots\amp A_{km} \end{array}  \right] \left[ \begin{array}{cccccc} B_{11}\amp B_{12}\amp \cdots\amp B_{1j} \amp  \cdots \amp B_{1n} \\ B_{21}\amp B_{22}\amp \cdots\amp B_{2j}\amp \cdots \amp B_{2n}\\ \vdots \amp  \vdots \amp \ddots \amp  \vdots\amp \ddots\amp \vdots \\ B_{m1}\amp B_{m2}\amp \cdots\amp B_{mj} \amp \cdots \amp B_{mn} \end{array}  \right] = [P_{ij}]\text{,}
\end{equation*}
where%
\begin{equation*}
P_{ij} = A_{i1}B_{1j} + A_{i2}B_{2j} + \cdots + A_{im}B_{mj} = \sum_{t=1}^{m} A_{it}B_{tj}\text{,}
\end{equation*}
provided that all the submatrix products are defined.%
\par
Now we can apply Strassen's algorithm to larger matrices using partitions. This method is sometimes referred to as divide and conquer.%
\begin{project}{}{g:project:idp12275336}%
Let \(A\) and \(B\) be two \(r \times r\) matrices. If \(r\) is not a power of \(2\), then pad the rows and columns of \(A\) and \(B\) with zeros to make them of size \(2^m \times 2^m\) for some integer \(m\). (From a practical perspective, we might instead just use unequal block sizes.) Let \(n = 2^m\). Partition \(A\) and \(B\) as%
\begin{equation*}
A = \left[ \begin{array}{cc} A_{11}\amp A_{12}\\A_{21}\amp A_{22} \end{array}  \right] \ \text{ and }  \  B = \left[ \begin{array}{cc} B_{11}\amp B_{12}\\B_{21}\amp B_{22} \end{array}  \right]\text{,}
\end{equation*}
where each submatrix is of size \(\frac{n}{2} \times \frac{n}{2}\). Now we use the Strassen algorithm just as in the \(2 \times 2\) case, treating the submatrices as if they were scalars (with the additional constraints of making sure that the dimensions match up so that products are defined, and ensuring we multiply in the correct order). Letting%
\begin{align*}
M_1 \amp = (A_{11}+A_{22})(B_{11}+B_{22})\\
M_2 \amp = (A_{21}+A_{22})B_{11}\\
M_3 \amp = A_{11}(B_{12}-B_{22})\\
M_4 \amp = A_{22}(B_{21}-B_{11})\\
M_5 \amp = (A_{11}+A_{12})B_{22}\\
M_6 \amp =(A_{21}-A_{11})(B_{11}+B_{12})\\
M_7 \amp = (A_{12}-A_{22})(B_{21}+B_{22})\text{,}
\end{align*}
then the same algebra as in \hyperref[x:project:x_pact_Strassen_1]{Project Activity~{\xreffont\ref{x:project:x_pact_Strassen_1}}} shows that%
\begin{equation*}
AB = \left[ \begin{array}{cc} M_1+M_4-M_5+M_7 \amp  M_3+M_5 \\ M_2+M_4 \amp  M_1+M_3-M_2+M_6 \end{array}  \right]\text{.}
\end{equation*}
%
\par
Apply Strassen's algorithm to calculate the matrix product \(AB\), where%
\begin{equation*}
A = \left[ \begin{array}{crr} 1\amp 3\amp -1\\2\amp 4\amp 6\\7\amp -2\amp 5 \end{array}  \right] \text{ and }  B = \left[ \begin{array}{crc} 2\amp 5\amp 3\\2\amp -4\amp 1\\1\amp 6\amp 4 \end{array}  \right]\text{.}
\end{equation*}
%
\end{project}%
While Strassen's algorithm can be more efficient, it does not always speed up the process. We investigate this in the next activity.%
\begin{project}{}{g:project:idp12288264}%
We introduce a little notation to help us describe the efficiency of our calculations. We won't be formal with this notation, rather work with it in an informal way. Big O (the letter ``O'' ) notation is used to describe the complexity of an algorithm. Generally speaking, in computer science big O notation can be used to describe the run time of an algorithm, the space used by the algorithm, or the number of computations required. The letter ``O'' is used because the behavior described is also called the order. Big O measures the asymptotic time of an algorithm, not its exact time. For example, if it takes \(6n^2-n+8\) steps to complete an algorithm, then we say that the algorithm grows at the order of \(n^2\) (we ignore the constants and the smaller power terms, since they become insignificant as \(n\) increases) and we describe its growth as \(O{\left(n^2\right)}\). To measure the efficiency of an algorithm to determine a matrix product, we will measure the number of operations it takes to calculate the product.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Suppose \(A\) and \(B\) are \(n \times n\) matrices. Explain why the operation of addition (that is, calculating \(A+B\)) is \(O{\left(n^2\right)}\).%
\item{}Suppose \(A\) and \(B\) are \(n \times n\) matrices. How many multiplications are required to calculate the matrix product \(AB\)? Explain.%
\item{}The standard algorithm for calculating a matrix product of two \(n \times n\) matrices requires \(n^3\) multiplications and a number of additions. Since additions are much less costly in terms of operations, the standard matrix product is \(O{\left(n^3\right)}\). We won't show it here, but using Strassen's algorithm on a product of \(2^m \times 2^m\) matrices is \(O{\left(n^{\log_2(7)}\right)}\), where \(n = 2^m\). That means that Strassen's algorithm applied to an \(n \times n\) matrix (where \(n\) is a power of \(2\)) requires approximately \(n^{\log_2(7)}\) multiplications. We use this to analyze situations to determine when Strassen's algorithm is computationally more efficient than the standard algorithm.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Suppose \(A\) and \(B\) are \(5 \times 5\) matrices. Determine the number of multiplications required to calculate the matrix product \(AB\) using the standard matrix product. Then determine the approximate number of multiplications required to calculate the matrix product \(AB\) using Strassen's algorithm. Which is more efficient? (Remember, we can only apply Strassen's algorithm to square matrices whose sizes are powers of \(2\).)%
\item{}Repeat part i. with \(125 \times 125\) matrices. Which method is more efficient?%
\end{enumerate}
\end{enumerate}
\end{project}%
As a final note, Strassen's algorithm is approximately \(O{\left(n^{2.81}\right)}\). As of 2018, the best algorithm for matrix multiplication, developed by Virginia Williams at Stanford University, is approximately \(O{\left(n^{2.373}\right)}\).\footnote{V. V. Williams, Multiplying matrices in \(O{\left(n^{2.373}\right)}\) time, Stanford University, (2014).\label{g:fn:idp12308232}}%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 9 Introduction to Eigenvalues and Eigenvectors}
\typeout{************************************************}
%
\begin{chapterptx}{Introduction to Eigenvalues and Eigenvectors}{}{Introduction to Eigenvalues and Eigenvectors}{}{}{x:chapter:chap_intro_eigenvals_eigenvects}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp12304008}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What is an eigenvalue of a matrix?%
\item{}What is an eigenvector of a matrix?%
\item{}How do we find eigenvectors of a matrix corresponding to an eigenvalue?%
\item{}How can the action of a matrix on an eigenvector be visualized?%
\item{}Why do we study eigenvalues and eigenvectors?%
\item{}What are discrete dynamical systems and how do we analyze the long-term behavior in them?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: The Google PageRank Algorithm}
\typeout{************************************************}
%
\begin{sectionptx}{Application: The Google PageRank Algorithm}{}{Application: The Google PageRank Algorithm}{}{}{x:section:sec_appl_pagerank}
The World Wide Web is a vast collection of information, searchable via search engines. A search engine looks for pages that are of interest to the user. In order to be effective, a search engine needs to be able to identify those pages that are relevant to the search criteria provided by the user. This involves determining the relative importance of different web pages by ranking the results of thousands or millions of pages fitting the search criteria. For Google, the PageRank algorithm is their method and is ``the heart of our software'' as they say. It is this PageRank algorithm that we will learn about later in this section. Eigenvalues and eigenvectors play an important role in this algorithm.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_eigen_intro}
Given a matrix \(A\), for some special non-zero vectors \(\vv\) the action of \(A\) on \(\vv\) will be same as scalar multiplication, i.e., \(A\vv=\lambda \vv\) for some scalar \(\lambda\). Geometrically, this means that the transformation \(T\) defined by \(T(\vx) = A\vx\) simply stretches or contracts the vector \(\vv\) but does not change its direction. Such a \emph{nonzero} vector is called an \terminology{eigenvector} of \(A\), while the scalar \(\lambda\) is called the corresponding \terminology{eigenvalue} of \(A\). The eigenvectors of a matrix tell us quite a bit about the transformation the matrix defines.%
\par
Eigenvalues and eigenvectors are used in many applications. Social media like Facebook and Google use eigenvalues to determine the influence of individual members on the network (which can affect advertising) or to rank the importance of web pages. Eigenvalues and eigenvectors appear in quantum physics, where atomic and molecular orbitals can be defined by the eigenvectors of a certain operator. They appear in principal component analysis, used to study large data sets, to diagonalize certain matrices and determine the long term behavior of systems as a result, and in the important singular value decomposition of a matrix. Matrices with real entries can have real or complex eigenvalues, and complex eigenvalues reveal a rotation that is encoded in every real matrix with complex eigenvalues which allows us to better understand certain matrix transformations.%
\begin{definition}{}{x:definition:def_eigenvector}%
\index{eigenvector}%
\index{characteristic vector}%
\index{eigenvalue}%
\index{characteristic value}%
Let \(A\) be an \(n \times n\) matrix. A non-zero vector \(\vx\) is an \terminology{eigenvector} (or \terminology{characteristic vector}) of \(A\) if there is a scalar \(\lambda\) such that \(A\vx = \lambda \vx\). The scalar \(\lambda\) is an \terminology{eigenvalue} (or \terminology{characteristic value}) of \(A\).%
\end{definition}
For example, \(\vv= \left[ \begin{array}{c} 1\\1 \end{array} \right]\) is an eigenvector of \(A=\left[ \begin{array}{cc} 2 \amp 1 \\ 3 \amp 0 \end{array} \right]\) corresponding to the eigenvalue \(\lambda=3\) because \(A\vv = \left[ \begin{array}{c} 3\\3 \end{array} \right]\), which is equal to \(3\vv\). On the other hand, \(\vw= \left[ \begin{array}{c} 1\\2 \end{array} \right]\) is not an eigenvector of \(A=\left[ \begin{array}{cc} 2 \amp 1 \\ 3 \amp 0 \end{array} \right]\) because \(A\vw = \left[ \begin{array}{c} 4\\3 \end{array} \right]\), which is not a multiple of \(\vw\).%
\begin{exploration}{}{x:exploration:pa_2_b_1}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}For each of the following parts, use the definition of an eigenvector to determine whether the given vector \(\vv\) is an eigenvector for the given matrix \(A\). If it is, determine the corresponding eigenvalue.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}\(A=\left[ \begin{array}{cc} 3 \amp 2 \\ 3 \amp 8 \end{array} \right]\), \(\vv=\left[ \begin{array}{r} -2\\1 \end{array} \right]\)%
\item{}\(A=\left[ \begin{array}{cr} 2 \amp 0 \\ 0 \amp -3 \end{array} \right]\), \(\vv=\left[ \begin{array}{c} 0\\1 \end{array} \right]\)%
\item{}\(A=\left[ \begin{array}{cc} 3 \amp 2 \\ 3 \amp 8 \end{array} \right]\), \(\vv=\left[ \begin{array}{c} 1\\1 \end{array} \right]\)%
\item{}\(A=\left[ \begin{array}{cc} 1 \amp 2 \\ 2 \amp 4 \end{array} \right]\), \(\vv=\left[ \begin{array}{r} -2\\1 \end{array} \right]\)%
\end{enumerate}
\item{}We now consider how we can find the eigenvectors corresponding to an eigenvalue using the definition. Suppose \(A=\left[ \begin{array}{cr} 6 \amp -2\\2 \amp 1 \end{array} \right]\). We consider whether we can find eigenvectors corresponding to eigenvalues 3, and 5. Effectively, this will help us determine whether 3 and\slash{}or 5 are eigenvalues of \(A\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Rewrite the vector equation \(A\vv=5\vv\) where \(\vv = \left[ \begin{array}{c} x \\y \end{array} \right]\) as a vector equation.%
\item{}After writing \(5\vv\) as \(5I\vv\) where \(I\) is the identity matrix, rearrange the variables to turn this vector equation into the homogeneous matrix equation \(B\vv=\vzero\) where \(B=\left[ \begin{array}{cr} 1 \amp -2\\2 \amp -4 \end{array} \right]\). If possible, find a non-zero (i.e. a non-trivial) solution to \(B\vv=\vzero\). Explain what this means about 5 being an eigenvalue of \(A\) or not.%
\item{}Similarly, determine whether the vector equation \(A\vv=3\vv\) has non-zero solutions. Using your result, determine whether 3 is an eigenvalue of \(A\) or not.%
\end{enumerate}
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Eigenvalues and Eigenvectors}
\typeout{************************************************}
%
\begin{sectionptx}{Eigenvalues and Eigenvectors}{}{Eigenvalues and Eigenvectors}{}{}{x:section:sec_eigval_eigvec}
Eigenvectors are especially useful in understanding the long-term behavior of dynamical systems, an example of which we will see shortly. The long-term behavior of a dynamical system is quite simple when the initial state vector is an eigenvector and this fact helps us analyze the system in general.%
\par
To find eigenvectors, we are interested in determining the vectors \(\vx\) for which \(A\vx\) has the same direction as \(\vx\). This will happen when%
\begin{equation*}
A\vx = \lambda \vx
\end{equation*}
for some scalar \(\lambda\). Of course, \(A\vx = \lambda \vx\) when \(\vx = \vzero\) for every \(A\) and every \(\lambda\), but that is uninteresting. So we really want to consider when there is a \emph{non-zero} vector \(\vx\) so that \(A\vx = \lambda \vx\). This prompts the definition of eigenvectors and eigenvalues as in \hyperref[x:definition:def_eigenvector]{Definition~{\xreffont\ref{x:definition:def_eigenvector}}}%
\par
\index{matrix!square} In order for a matrix \(A\) to have an eigenvector, one condition \(A\) must satisfy is that \(A\) has to be a square matrix, i.e. an \(n \times n\) matrix. We will find that each \(n\times n\) matrix has only finitely many eigenvalues.%
\par
The terms eigenvalue and eigenvector seem to come from Hilbert, using the German ``eigen'' (roughly translated as ``own'', ``proper'', or ``characteristic'') to emphasize how eigenvectors and eigenvalues are connected to their matrices. To find the eigenvalues and eigenvectors of an \(n \times n\) matrix \(A\), we need to find the solutions to the equation%
\begin{equation*}
A \vx = \lambda \vx \,\text{.}
\end{equation*}
%
\par
In \hyperref[x:exploration:pa_2_b_1]{Preview Activity~{\xreffont\ref{x:exploration:pa_2_b_1}}}, we considered this equation for \(A=\left[ \begin{array}{cr} 6\amp -2 \\ 2\amp 1 \end{array}  \right]\) and \(\lambda=5\). The homogeneous matrix equation we came up with was%
\begin{equation*}
\left[ \begin{array}{cr} 1\amp -2 \\ 2\amp -4 \end{array} \right] \vx = \vzero \,\text{.}
\end{equation*}
%
\par
To see the relationship between this homogeneous matrix equation and the eigenvalue-eigenvector equation better, let us consider the eigenvector equation using matrix algebra:%
\begin{align*}
A\vx \amp = \lambda\vx\\
A\vx - \lambda \vx \amp = \vzero\\
A\vx - \lambda I_n\vx \amp = \vzero\\
(A-\lambda I_n)\vx \amp = \vzero\text{,}
\end{align*}
where \(I_n\) is the \(n \times n\) identity matrix. Notice that this description matches the homogenous equation matrix example above since we simply subtracted 5 from the diagonal terms of the matrix \(A\). Hence, to find eigenvalues, we need to find the values of \(\lambda\) so that the homogeneous equation \((A - \lambda I_n)\vx=\vzero\) has non-trivial solutions.%
\begin{activity}{}{g:activity:idp12363400}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Under what conditions on \(A-\lambda I_n\) will the matrix equation \((A-\lambda I_n)\vx=\vzero\) have non-trivial solutions? Describe at least two different but equivalent conditions.%
\item{}The real number 0 is an eigenvalue of \(A = \left[ \begin{array}{cc} 1\amp 2 \\ 2\amp 4 \end{array} \right]\). Check that your criteria in the previous part agrees with this result.%
\item{}Determine if 5 is an eigenvalue of the matrix \(A = \left[ \begin{array}{cc} 1\amp 2 \\ 2\amp 4 \end{array} \right]\) using your criterion above.%
\item{}What are the two eigenvalues of the matrix \(A = \left[ \begin{array}{cc} 3\amp 2\\4\amp 5 \end{array} \right]\)?%
\end{enumerate}
\end{activity}%
Since an eigenvector of \(A\) corresponding to eigenvalue \(\lambda\) is a non-trivial solution to the homogeneous equation \((A - \lambda I_n)\vx = \vzero\), the eigenvalues \(\lambda\) which work are those for which the matrix \(A-\lambda I_n\) has linearly dependent columns, or for which the row echelon form of the matrix \(A-\lambda I_n\) does not have a pivot in every column. When we need to test if a specific \(\lambda\) is an eigenvalue, this method works fine. However, finding which \(\lambda\)'s will work in general involves row reducing a matrix with \(\lambda\)'s subtracted on the diagonal algebraically. For certain types of matrices, this method still provides us the eigenvalues quickly. For general matrices though, row reducing algebraically is not efficient. We will later see an algebraic method which uses the determinants to find the eigenvalues.%
\begin{activity}{}{g:activity:idp12369288}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}For \(\lambda\) to be an eigenvalue of \(A\), we noted that \(A-\lambda I_n\) must have a non-pivot column. Use this criterion to explain why \(A=\left[ \begin{array}{rc} -2\amp 2\\0\amp 4 \end{array} \right]\) has eigenvalues \(\lambda=-2\) and \(\lambda=4\).%
\item{}Determine the eigenvalues of \(A = \left[ \begin{array}{ccrc} 3\amp 0\amp 1\amp 0 \\ 0\amp 2\amp -1\amp 0 \\ 0\amp 0\amp 2\amp 0 \\ 0\amp 0\amp 0\amp 1 \end{array} \right]\).%
\item{}Generalize your results from the above parts in the form of a theorem in the most general \(n\times n\) case.%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Dynamical Systems}
\typeout{************************************************}
%
\begin{sectionptx}{Dynamical Systems}{}{Dynamical Systems}{}{}{x:section:sec_dynam_sys}
\index{dynamical system}%
One real-life application of eigenvalues and eigenvectors is in analyzing the long-term behavior of \terminology{discrete dynamical systems}. A \terminology{dynamical system} is a system of variables whose values change with time. In discrete systems, the change is described by defining the values of the variables at time \(t+1\) in terms of the values at time \(t\). For example, the discrete dynamical system%
\begin{equation*}
y_{t+1} = y_t + t
\end{equation*}
relates the value of \(y\) at time \(t+1\) to the value of \(y\) at time \(t\). This is in contrast with a differential equation\footnote{A differential equation is an equation that involves one or more derivatives of a function.\label{g:fn:idp12377736}} such as%
\begin{equation*}
\frac{dy}{dt} = y+t\text{,}
\end{equation*}
which describes the instantaneous rate of change of \(y(t)\) in terms of \(y\) and \(t\).%
\par
Discrete dynamical systems can be used in population modeling to provide a simplified model of predator-prey interactions in biology (see \hyperref[x:activity:pa_2_b_2]{Activity~{\xreffont\ref{x:activity:pa_2_b_2}}}). Other applications include Markov chains (see \hyperlink{x:exercise:ex_2_b_Markov}{Exercise~{\xreffont 5}}), age structured population growth models, distillation of a binary ideal mixture of two liquids, cobweb model in economics concerning the interaction of supply and demand for a single good, queuing theory and traffic flow.%
\par
Eigenvectors can be used to analyze the long-term behavior of dynamical systems.%
\begin{activity}{}{x:activity:pa_2_b_2}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Consider a discrete dynamical system providing a simplified model of predator-prey interactions in biology, such as the system describing the populations of rabbits and foxes in a certain area. Suppose, for example, for a specific area the model is given by the following equations:%
\begin{equation}
\begin{alignedat}{4} r_{k+1}  \amp {}={}   \amp {1.14}r_k   \amp {}-{}  \amp {0.12}f_k \\ f_{k+1}  \amp {}={}   \amp {0.08}r_k   \amp {}+{}  \amp {0.86}f_k \end{alignedat}\label{x:men:eq_PA5_1_3}
\end{equation}
where \(r_i\) represents the number of rabbits in the area \(i\) years after a starting time value, and \(f_i\) represents the number of foxes in year \(i\). We use \(r_0, f_0\) to denote the initial population values.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Suppose \(r_k=300\) and \(f_k=100\) for one year. Calculate rabbit and fox population values for the next year. In other words, find \(r_{k+1}, f_{k+1}\) values.%
\item{}Consider the coefficients of the variables \(r_k, f_k\) in the the system of equations in \hyperref[x:men:eq_PA5_1_3]{({\xreffont\ref{x:men:eq_PA5_1_3}})}. Can you explain the reasoning behind the signs and absolute sizes of the coefficients from the story that it models?%
\item{}Let \(\vx_k=\left[ \begin{array}{c} r_k \\ f_k \end{array}  \right]\). The vector \(\vx_k\) is called the \terminology{state vector} of the system at time \(k\), because it describes the state of the whole system at time \(k\). We can rewrite the system of equations in \hyperref[x:men:eq_PA5_1_3]{({\xreffont\ref{x:men:eq_PA5_1_3}})} as a matrix-vector equation in terms of the state vectors at time \(k\) and \(k+1\). More specifically, the equation will be of the form%
\begin{equation}
\vx_{k+1} = A\vx_k\label{x:men:eq_PA5_1_4}
\end{equation}
where \(A = \left[ \begin{array}{cr} 1.14 \amp  -0.12 \\ 0.08 \amp  0.86 \end{array}  \right]\). We will call this matrix the \terminology{transition matrix} of the system. Check that \(A \left[ \begin{array}{c} 300\\100 \end{array}  \right]\) gives us the population values you calculated in the first part above.%
\item{}The transition matrix will help us simplify calculations of the population values. Note that equation \hyperref[x:men:eq_PA5_1_4]{({\xreffont\ref{x:men:eq_PA5_1_4}})} implies that \(\vx_1 = A\vx_0\), \(\vx_2=A\vx_1\), \(\vx_3=A\vx_2\), and so on. This is a recursive method to find the population values as each year's population values depend on the previous year's population values. Using this approach, calculate \(\vx_k\) for \(k\) values up to 5 corresponding to the following \emph{three different} initial rabbit-fox population values (all in thousands):%
\begin{equation*}
r_0=300 \;,\;  f_0=100
\end{equation*}
%
\begin{equation*}
r_0=100 \;,\;  f_0=200
\end{equation*}
%
\begin{equation*}
r_0=1200 \;,\; f_0=750
\end{equation*}
Can you guess the long-term behavior of the population values in each case? Are they both increasing? Decreasing? One increasing, one decreasing? How do the rabbit and fox populations compare?%
\end{enumerate}
\end{enumerate}
\end{activity}%
A dynamical system is a system of variables whose values change with time. In \hyperref[x:activity:pa_2_b_2]{Activity~{\xreffont\ref{x:activity:pa_2_b_2}}}, we considered the discrete dynamical system modeling the rabbit and fox population in an area, which is an example of a predator-prey system. The system was given by the equations from \hyperref[x:men:eq_PA5_1_3]{({\xreffont\ref{x:men:eq_PA5_1_3}})}, where \(r_i\) represented the number of rabbits in the area \(i\) years after a starting time value, and \(f_i\) represented the number of foxes in year \(i\). In this notation, \(r_0, f_0\) corresponded to the initial population values.%
\par
\index{state vector} As we saw in \hyperref[x:activity:pa_2_b_2]{Activity~{\xreffont\ref{x:activity:pa_2_b_2}}}, if we define the state vector as \(\vx_k=\begin{bmatrix}r_k \\ f_k \end{bmatrix}\), the system of equations representing the dynamical system can be expressed as%
\begin{equation}
\vx_{k+1} = A \vx_k\label{x:men:eq_state_vector_recursive_formula}
\end{equation}
where \(A=\left[ \begin{array}{cr} 1.14 \amp  -0.12 \\ 0.08 \amp  0.86 \end{array}  \right]\) represents the transition matrix. Note that equation \hyperref[x:men:eq_state_vector_recursive_formula]{({\xreffont\ref{x:men:eq_state_vector_recursive_formula}})} encodes infinitely many equations including \(\vx_1 = A\vx_0\), \(\vx_2=A\vx_1\), \(\vx_3=A\vx_2\), and so on. This is a recursive formula for the population values as each year's population values are expressed in terms of the previous year's population values. If we want to calculate \(\vx_{10}\), this formula requires first finding the population values for years 1-9. However, we can obtain a non-recursive formula using matrix algebra. If we substitute \(\vx_1 = A\vx_0\) into \(\vx_2=A\vx_1\) and simplify, we find that%
\begin{equation*}
\vx_2=A\vx_1= A(A\vx_0) = A^2 \vx_0 \,\text{.}
\end{equation*}
%
\par
Similarly, substituting \(\vx_2=A^2\vx_1\) into the formula for \(\vx_3\) gives%
\begin{equation*}
\vx_3 = A\vx_2=A(A^2 \vx_0) = A^3 \vx_0 \,\text{.}
\end{equation*}
%
\par
This process can be continued inductively to show that%
\begin{equation}
\vx_k=A^k \vx_0\label{x:men:eq_state_vector_closed_formula}
\end{equation}
for every \(k\) value. So to find the population values at any year \(k\), we only need to know the initial state vector \(\vx_0\).%
\begin{activity}{}{x:activity:act_dynamical_system}%
In this activity the matrix \(A\) is the transition matrix for the rabbit and fox population model,%
\begin{equation*}
A = \left[ \begin{array}{cr} 1.14 \amp  -0.12 \\ 0.08 \amp  0.86 \end{array}  \right]\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Suppose that the initial state vector \(\vx_0\) is an eigenvector of \(A\) corresponding to eigenvalue \(\lambda\). In this case, explain why \(\vx_1=\lambda \vx_0\) and \(\vx_2=\lambda^2 \vx_0\). Find the formula for \(\vx_k\) in terms of \(\lambda,
k\) and \(\vx_0\) by applying equation \hyperref[x:men:eq_state_vector_recursive_formula]{({\xreffont\ref{x:men:eq_state_vector_recursive_formula}})} iteratively.%
\item{}The initial state vector \(\vx_0=\left[ \begin{array}{c} 300\\100 \end{array} \right]\) is an eigenvector of \(A\). Find the corresponding eigenvalue and, using your formula from (a) for \(\vx_k\) in terms of \(\lambda,
k\) and \(\vx_0\), find the state vector \(\vx_k\) in this case.%
\item{}The initial state vector \(\vx_0=\left[ \begin{array}{c} 100\\200 \end{array} \right]\) is an eigenvector of \(A\). Find the corresponding eigenvalue and, using your formula from (a) for \(\vx_k\) in terms of \(\lambda,
k\) and \(\vx_0\), find the state vector \(\vx_k\) in this case.%
\item{}Consider now an initial state vector of the form \(\vx_0=a\vv_0+b\vw_0\) where \(a, b\) are constants, \(\vv_0\) is an eigenvector corresponding to eigenvalue \(\lambda_1\) and \(\vw_0\) corresponding to eigenvalue \(\lambda_2\) (\(\vv_0\) and \(\vw_0\) are not necessarily the eigenvectors from parts (b) and (c)). Use matrix algebra and equation \hyperref[x:men:eq_state_vector_closed_formula]{({\xreffont\ref{x:men:eq_state_vector_closed_formula}})} to explain why \(\vx_k=a\lambda_1^k\vv_0+b\lambda_2^k\vw_0\).%
\item{}Express the initial state vector \(\vx_0=\left[ \begin{array}{c} 1200 \\ 750 \end{array} \right]\) as a linear combination of the eigenvectors \(\vv_0=\left[ \begin{array}{c}300\\ 100 \end{array} \right], \vw_0=\left[ \begin{array}{c} 100\\200 \end{array} \right]\) and use your result from the previous part to find a formula for \(\vx_k\). What happens to the population values as \(k\to \infty?\)%
\end{enumerate}
\end{activity}%
As you discovered in \hyperref[x:activity:act_dynamical_system]{Activity~{\xreffont\ref{x:activity:act_dynamical_system}}}, we can use linearly independent eigenvectors of the transition matrix to find a closed formula for the state vector of a dynamical system, as long as the initial state vector can be expressed as a linear combination of the eigenvectors.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_eigen_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp12430600}%
Let \(A = \left[ \begin{array}{cc} 1\amp 2\\2\amp 4 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find all of the eigenvalues of \(A\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp12424712}{}\quad{}Recall that a scalar \(\lambda\) is a eigenvalue for \(A\) if there is a nonzero vector \(\vx\) such that \(A \vx = \lambda \vx\) or \((A-\lambda I_2) \vx = \vzero\). For this matrix \(A\), we have%
\begin{equation*}
A - \lambda I_2 = \left[ \begin{array}{cc} 1-\lambda\amp 2\\2\amp 4-\lambda \end{array}  \right]\text{.}
\end{equation*}
To solve the homogeneous system \((A-\lambda I_2) \vx = \vzero\), we row reduce \(A - \lambda I_2\). To do this, we first interchange rows to get the following matrix that is row equivalent to \(A - \lambda I_2\) (we do this to ensure that we have a nonzero entry in the first row and column)%
\begin{equation*}
\left[ \begin{array}{cc} 2\amp 4-\lambda \\ 1-\lambda\amp 2 \end{array}  \right]\text{.}
\end{equation*}
Next we replace row two with \(\frac{1}{2}(1-\lambda)\) times row one minus row two to obtain the row equivalent matrix%
\begin{equation*}
\left[  \begin{array}{cc} 2\amp 4-\lambda \\ 0\amp \frac{1}{2}(4-\lambda)(1-\lambda)-2 \end{array}  \right]\text{.}
\end{equation*}
There will be a nontrivial solution to \((A-\lambda I_2)\vx = \vzero\) if there is a row of zeros in this row echelon form. Thus, we look for values of \(\lambda\) that make%
\begin{equation*}
\frac{1}{2}(4-\lambda)(1-\lambda)-2 = 0\text{.}
\end{equation*}
Applying a little algebra shows that%
\begin{align*}
\frac{1}{2}(4-\lambda)(1-\lambda)-2 \amp = 0\\
(4-\lambda)(1-\lambda) - 4 \amp = 0\\
\lambda^2 - 5 \lambda \amp = 0\\
\lambda(\lambda-5) \amp = 0\text{.}
\end{align*}
So the eigenvalues of \(A\) are \(\lambda = 0\) and \(\lambda = 5\).%
\item{}Find a corresponding eigenvector for each eigenvalue found in part (a).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp12441096}{}\quad{}Recall that an eigenvector for the eigenvalue \(\lambda\) is a nonzero vector \(\vx\) such that \((A - \lambda I_2) \vx = \vzero\). We consider each eigenvalue in turn.%
\begin{itemize}[label=\textbullet]
\item{}When \(\lambda = 0\),%
\begin{equation*}
A - 0 I_2 = A = \left[ \begin{array}{cc} 1\amp 2\\2\amp 4 \end{array}  \right]\text{.}
\end{equation*}
Technology shows that the reduced row echelon form of \(A\) is%
\begin{equation*}
\left[ \begin{array}{cc} 1\amp 2 \\ 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
If \(\vx = \left[ \begin{array}{c} x_1\\x_2 \end{array}  \right]\), then \(A \vx = \vzero\) implies that \(x_2\) is free and \(x_1 = -2x_2\). Choosing \(x_2 = 1\) gives us the eigenvector \(\left[ \begin{array}{r} -2\\1 \end{array}  \right]\). As a check, note that%
\begin{equation*}
\left[ \begin{array}{cc} 1\amp 2\\2\amp 4 \end{array}  \right] \left[ \begin{array}{r} -2\\1 \end{array}  \right] = \left[ \begin{array}{c} 0\\0 \end{array}  \right]\text{.}
\end{equation*}
%
\item{}When \(\lambda = 5\),%
\begin{equation*}
A - 5 I_2 = \left[ \begin{array}{rr} -4\amp 2\\2\amp -1 \end{array}  \right]\text{.}
\end{equation*}
Technology shows that the reduced row echelon form of \(A-5I_2\) is%
\begin{equation*}
\left[  \begin{array}{cr} 1\amp -\frac{1}{2} \\ 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
If \(\vx = \left[ \begin{array}{c} x_1\\x_2 \end{array}  \right]\), then \((A - 5I_2)\vx = \vzero\) implies that \(x_2\) is free and \(x_1 = \frac{1}{2}x_2\). Choosing \(x_2 = 2\) gives us the eigenvector \(\left[ \begin{array}{c} 1\\2 \end{array}  \right]\). As a check, note that%
\begin{equation*}
\left[  \begin{array}{cr} 1\amp -\frac{1}{2} \\ 0\amp 0 \end{array}  \right] \left[ \begin{array}{c} 1\\2 \end{array}  \right] = \left[ \begin{array}{c} 0\\0 \end{array}  \right]\text{.}
\end{equation*}
%
\end{itemize}
%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp12451464}%
Accurately predicting the weather has long been an important task. Meteorologists use science, mathematics, and technology to construct models that help us understand weather patterns. These models are very sophisticated, but we will consider only a simple model. Suppose, for example, we want to learn something about whether it will be wet or dry in Grand Rapids, Michigan. To do this, we might begin by collecting some data about weather conditions in Grand Rapids and then use that to make predictions. Information taken over the course of 2017 from the National Weather Service Climate Data shows that if it was dry (meaning no measurable precipitation, either rain or snow) on a given day in Grand Rapids, it would be dry the next day with a probability of 64\% and wet with a probability of 36\%. Similarly, if it was wet on a given day it would be dry the next day with a probability of 47\% and dry with a probability of 53\%. Assuming that this pattern is one that continues in the long run, we can develop a mathematical model to make predictions about the weather.%
\par
This data tells us how the weather transitions from one day to the next, and we can succinctly represent this data in a \terminology{transition matrix}:%
\begin{equation}
T = \left[ \begin{array}{cc} 0.64\amp 0.47 \\ 0.36\amp 0.53 \end{array}  \right]\text{.}\label{x:men:eq_weather_transition}
\end{equation}
%
\par
Whether it is dry or wet on a given day is called the \terminology{state} of that day. So our transition matrix tells us about the transition between states. Notice that if \(T = [t_{ij}]\), then the probability of moving from state \(j\) to state \(i\) is given by \(t_{ij}\). We can represent a state by a vector: the vector \(\left[ \begin{array}{c} 1\\0 \end{array} \right]\) represents the dry state and the vector \(\left[ \begin{array}{c} 0\\1 \end{array} \right]\) represents the wet state.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Calculate \(T\left[ \begin{array}{c} 1\\0 \end{array} \right]\). Interpret the meaning of this output.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp12458632}{}\quad{}Here we have%
\begin{equation*}
T\left[ \begin{array}{c} 1\\0 \end{array} \right]  = \left[ \begin{array}{cc} 0.64\amp 0.47 \\ 0.36\amp 0.53 \end{array}  \right] \left[ \begin{array}{c} 1\\0 \end{array} \right]  = \left[ \begin{array}{c} 0.64\\0.36 \end{array} \right]\text{.}
\end{equation*}
This output tells us the different probabilities of whether it will be dry or wet the day following a dry day.%
\item{}Calculate \(T\left[ \begin{array}{c} 0\\1 \end{array} \right]\). Interpret the meaning of this output.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp12476184}{}\quad{}Here we have%
\begin{equation*}
T\left[ \begin{array}{c} 0\\1 \end{array} \right]  = \left[ \begin{array}{cc} 0.64\amp 0.47 \\ 0.36\amp 0.53 \end{array}  \right] \left[ \begin{array}{c} 0\\1 \end{array} \right]  = \left[ \begin{array}{c} 0.47\\0.53 \end{array} \right]\text{.}
\end{equation*}
This output tells us the different probabilities of whether it will be dry or wet the day following a wet day.%
\item{}Calculate \(T\left[ \begin{array}{c} 0.3\\0.7 \end{array} \right]\). Interpret the meaning of this output.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp12480408}{}\quad{}Here we have%
\begin{equation*}
T\left[ \begin{array}{c} 0.3\\0.7 \end{array} \right]  = \left[ \begin{array}{cc} 0.64\amp 0.47 \\ 0.36\amp 0.53 \end{array}  \right] \left[ \begin{array}{c} 0.3\\0.7 \end{array} \right]  \approx \left[ \begin{array}{c} 0.52\\0.48 \end{array} \right]\text{.}
\end{equation*}
This output tells us there is a 52\% chance of it being dry and a 48\% chance of it being wet following a day when there is a 30\% chance of it being dry and a 70\% chance of it being wet.%
\item{}We can use the transition matrix to build a chain of probability vectors. We begin with an initial state, say it is dry on a given day. This initial state is represented by the initial state vector \(\vx_0 = \left[ \begin{array}{c} 1\\0 \end{array} \right]\). The probabilities that it will be dry or wet the following day are given by the vector%
\begin{equation*}
\vx_1 = T\vx_0 = \left[ \begin{array}{c} 0.64\\0.36 \end{array} \right]\text{.}
\end{equation*}
This output vector tells us that the next day will be dry with a 64\% probability and wet with a 36\% probability. For each \(k \geq 1\), we let%
\begin{equation}
\vx_{k} = T\vx_{k-1}\text{.}\label{x:men:eq_weather_chain}
\end{equation}
Thus we create a sequence of vectors that tell us the probabilities of it being dry or wet on subsequent days. The vector \(\vx_k\) is called the \terminology{state vector} of the system at time \(k\), because it describes the state of the whole system at time \(k\). We can rewrite the system of equations in \hyperref[x:men:eq_PA5_1_3]{({\xreffont\ref{x:men:eq_PA5_1_3}})} as a matrix-vector equation in terms of the state vectors at time \(k\) and \(k+1\). More specifically, the equation will be of the form%
\begin{equation}
\vx_{k+1} = T\vx_k\label{x:men:eq_weather_chain2}
\end{equation}
for \(k \geq 0\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Starting with \(\vx_0 = \left[ \begin{array}{c} 1\\0 \end{array} \right]\), use appropriate technology to calculate \(\vx_k\) for \(k\) values up to 10. Round to three decimal places. What do you notice about the entries?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp12486936}{}\quad{}Technology shows that%
\begin{equation*}
\begin{array}{ll} \vx_1 = \left[ \begin{array}{c} 0.640\\0.360 \end{array}  \right] \amp \vx_2 = \left[ \begin{array}{c} 0.579\\0.421 \end{array}  \right]  \\ \vx_3 = \left[ \begin{array}{c} 0.568\\0.432 \end{array}  \right] \amp \vx_4 = \left[ \begin{array}{c} 0.567\\0.433 \end{array}  \right]    \\ \vx_5 = \left[ \begin{array}{c} 0.566\\0.434 \end{array}  \right] \amp \vx_6 = \left[ \begin{array}{c} 0.566\\0.434 \end{array}  \right]    \\ \vx_7 = \left[ \begin{array}{c} 0.566\\0.434 \end{array}  \right] \amp \vx_8 = \left[ \begin{array}{c} 0.566\\0.434 \end{array}  \right]    \\ \vx_9 = \left[ \begin{array}{c} 0.566\\0.434 \end{array}  \right] \amp \vx_{10} = \left[ \begin{array}{c} 0.566\\0.434 \end{array}  \right]. \end{array}
\end{equation*}
We can see that our vectors \(\vx_k\) are essentially the same as we let \(k\) increase.%
\item{}What does the result of the previous part tell us about eigenvalues of \(T\)? Explain.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp12482456}{}\quad{}Since our sequence seems to be converging to a vector \(\vx\) satisfying \(T \vx = \vx\), we conclude that 1 is an eigenvalue of \(T\).%
\item{}Rewrite \(T\) as%
\begin{equation*}
T = \left[  \begin{array}{cc} \frac{64}{100}\amp \frac{47}{100} \\ \frac{36}{100}\amp \frac{53}{100} \end{array}  \right]\text{.}
\end{equation*}
We do this so we can use exact arithmetic. Let \(\vx = \left[  \begin{array}{c} \frac{47}{83} \\ \frac{36}{83} \end{array}  \right]\). What is \(T\vx\)? (Use exact arithmetic, no decimals.) Explain how \(\vx\) is related to the previous two parts of this problem. What does the vector \(\vx\) tells us about weather in Grand Rapids?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp12494232}{}\quad{}A matrix vector multiplication shows that%
\begin{equation*}
T \vx = \left[  \begin{array}{cc} \frac{64}{100}\amp \frac{47}{100} \\ \frac{36}{100}\amp \frac{53}{100} \end{array}  \right]\left[  \begin{array}{c} \frac{47}{83} \\ \frac{36}{83} \end{array}  \right] = \left[  \begin{array}{c} \frac{47}{83} \\ \frac{36}{83} \end{array}  \right]\text{.}
\end{equation*}
In other words, \(\vx\) is an eigenvector for \(T\) with eigenvalue 1. Notice that%
\begin{equation*}
\frac{47}{83} \approx 0.566 \ \text{ and }  \ \frac{36}{83} \approx 0.434\text{,}
\end{equation*}
so these fractions give the same results we obtained with our sequence of vectors \(\vx_k\). These vectors provide a steady-state vector for Grand Rapids weather. In other words, if there is a \(56.6\%\) chance of it being dry on a given day in Grand Rapids, then there is a \(56.6\%\) chance it will be dry again the next day.%
\end{enumerate}
\end{enumerate}
This is an example of a Markov process. Markov processes (named after Andrei Andreevich Markov) are widely used to model phenomena in biology, chemistry, business, physics, engineering, the social sciences, and much more. More specifically,%
\begin{definition}{}{x:definition:def_Markov}%
\index{Markov process}%
A \terminology{Markov process} is a process in which the probability of the system being in a given state depends only on the previous state.%
\end{definition}
If \(\vx_0\) is a vector which represents the initial state of a Markov process, then there is a matrix \(T\) (the \terminology{transition matrix}) such that the state of the system after one iteration is given by the vector \(T\vx_0\). This produces a chain of state vectors \(T\vx_0\), \(T^2 \vx_0\), \(T^3 \vx_0\), etc., where the state of the system after \(n\) iterations is given by \(T^n \vx_0\). Such a chain of vectors is called a \terminology{Markov chain}. A Markov process is characterized by two properties:%
\begin{itemize}[label=\textbullet]
\item{}the total number of observations remains fixed (this is reflected in the fact that the sum of the entries in each column of the matrix \(T\) is 1), and%
\item{}no observation is lost (this means the entries in the matrix \(T\) cannot be negative).%
\end{itemize}
%
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_eigen_summ}
We learned about eigenvalues and eigenvectors of a matrix in this section.%
\begin{itemize}[label=\textbullet]
\item{}A scalar \(\lambda\) is an eigenvalue (or characteristic value) of a square matrix \(A\) if there is a non-zero vector \(\vx\) so that \(A\vx = \lambda \vx\).%
\item{}A non-zero vector \(\vx\) is an eigenvector (or characteristic vector) of a square matrix \(A\) if there is a scalar \(\lambda\) so that \(A\vx = \lambda \vx\).%
\item{}To find the eigenvectors of an \(n \times n\) matrix \(A\) corresponding to an eigenvalue \(\lambda\), we determine the non-trivial solutions to \((A - \lambda I_n)\vx=\vzero\) where \(I_n\) is the \(n\times n\) identity matrix.%
\item{}We study eigenvectors and eigenvalues because the eigenvectors tell us quite a bit about the transformation corresponding to the matrix. These eigenvectors arise in many applications in physics, chemistry, statistics, economics, biology, sociology and other areas, and help understand the long-term behavior of dynamical systems.%
\item{}A dynamical system is a system of variables whose values change with time. In linear dynamical systems, the change in the state vector from one time period to the next is expressed by matrix multiplication by the transition matrix \(A\). The eigenvectors of \(A\) provide us a simple method to express the state vector at any given time period in terms of the initial state vector. Specifically, if the initial state vector is \(\vx_0=a\vv_0+b\vw_0\) where \(\vv_0, \vw_0\) are eigenvectors corresponding to eigenvalues \(\lambda_1, \lambda_2\), we have%
\begin{equation*}
\vx_k = A^k \vx_0 = \lambda_1^k a \vv_0 + \lambda_2^k b \vw_0\,\text{.}
\end{equation*}
%
\end{itemize}
%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_eigen_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp12515224}%
For each of the following matrix-vector pairs, determine whether the given vector is an eigenvector of the matrix.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(A=\left[ \begin{array}{cc} 1\amp 2 \\ 4\amp 3 \end{array} \right]\), \(\vv=\left[ \begin{array}{r} 1\\-1 \end{array} \right]\)%
\item{}\(A=\left[ \begin{array}{cc} 1\amp 2 \\ 0\amp 3 \end{array} \right]\), \(\vv=\left[ \begin{array}{r} 1\\1 \end{array} \right]\)%
\item{}\(A=\left[ \begin{array}{rcc} 2\amp 1\amp 0 \\ 0\amp 1\amp 0 \\ -1\amp 0\amp 1 \end{array} \right]\), \(\vv=\left[ \begin{array}{r} -1\\0\\1 \end{array} \right]\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp12530584}%
For each of the following matrix-eigenvalue pairs, determine an eigenvector of \(A\) for the given eigenvalue.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(A=\left[ \begin{array}{rc} 1\amp 2 \\ -1\amp 4 \end{array} \right]\), \(\lambda=3\)%
\item{}\(A=\left[ \begin{array}{cc} 1\amp 4 \\ 1\amp 1 \end{array} \right]\), \(\lambda=3\)%
\item{}\(A=\left[ \begin{array}{rcc} -1\amp 4\amp 1 \\ 3\amp 3\amp 0 \\ 0\amp 0\amp 1 \end{array} \right]\), \(\lambda=5\)%
\item{}\(A=\left[ \begin{array}{cccc} 4\amp 0\amp 0\amp 0 \\ 0\amp 2\amp 0\amp 2 \\ 2\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 3 \end{array} \right]\), \(\lambda=4\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp12530328}%
For each of the following matrix-\(\lambda\) pairs, determine whether the given \(\lambda\) will work as an eigenvalue. You do not need to find an eigenvector as long you can justify if \(\lambda\) is a valid eigenvalue or not.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(A=\left[ \begin{array}{cc} 4\amp 3 \\ 4\amp 8 \end{array} \right]\), \(\lambda=2\)%
\item{}\(A=\left[ \begin{array}{cr} 4\amp -2 \\ 2\amp -1 \end{array} \right]\), \(\lambda=0\)%
\item{}\(A=\left[ \begin{array}{rc} 1\amp 2 \\ -1\amp 4 \end{array} \right]\), \(\lambda=-1\)%
\item{}\(A=\left[ \begin{array}{rr} 0\amp -2 \\ -1\amp 1 \end{array} \right]\), \(\lambda=-2\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp12541720}%
For a matrix \(A\) with eigenvector \(\vv_1=\left[ \begin{array}{c} 1\\1 \end{array} \right]\) with eigenvalue \(\lambda_1=2\), and eigenvector \(\vv_2=\left[ \begin{array}{c} 1\\2 \end{array} \right]\) with eigenvalue \(\lambda_2=-1\), determine the value of the following expressions using matrix-vector product properties:%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(A(2\vv_1+3\vv_2)\)%
\item{}\(A(A(\vv_1+2\vv_2))\)%
\item{}\(A^{20} (4\vv_1-2\vv_2)\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{x:exercise:ex_2_b_Markov}%
\index{Markov chain}%
In this problem we consider a discrete dynamical system that forms what is called a \terminology{Markov chain} (see \hyperref[x:definition:def_Markov]{Definition~{\xreffont\ref{x:definition:def_Markov}}}) which models the number of students attending and skipping a linear algebra class in a semester. Assume the course starts with 1,000,000 students on day 0. For any given class day, 90\% of the students who attend a class attend the next class (and 10\% of these students skip next class) while only 30\% of those absent are there the next time (and 70\% of these students continue skipping class).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}We know that there will be 900,000 students in class on the second day and 100,000 students skipping class. On the third day, 90\% of the 900,000 students (attenders) and 30\% of the 100,000 students (skippers) will come back to class. Therefore, 840,000 students will attend class on the third day. On the other hand, 10\% of 900,000 students and 70\% of 100,000 students skip class on the third day, for a total of 160,000 students skipping class. We can use variables to represent these numbers. Let \(a_n\) represent the number of students attending class \(n\) days after first day. So \(a_0=1,000,000, a_1=900,000, a_2=840,000\). Let \(s_n\) represent the students skipping class. So \(s_0=0, s_1=100,000, s_2=160,000\). Find \(a_3, s_3, a_4, s_4\).%
\item{}Find a linear expression for \(a_{k+1}\) in terms of the previous day values, \(a_k\) and \(s_k\), using the story given in the problem. Similarly, express \(s_{k+1}\) in terms of \(a_k\) and \(s_k\).%
\item{}Let \(\vb_k\) represent the state vector: \(\vx_k=\begin{bmatrix}a_k\\s_k \end{bmatrix}\). It describes the state of the whole system (students attending class and skipping class) in one vector. For example, \(\vx_0=\begin{bmatrix}1,000,000\\0 \end{bmatrix}\) is the initial state. The state next day is \(\vx_1=\begin{bmatrix}900,000\\100,000 \end{bmatrix}\). Using your answer to the previous part, find a matrix \(A\) which describes how the system changes from one day to the other so that \(\vx_{k+1} = A\textbf{x}_k\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp12558104}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
The number 0 cannot be an eigenvalue.%
\item{}\lititle{True\slash{}False.}\par%
The \(\vzero\) vector cannot be an eigenvector.%
\item{}\lititle{True\slash{}False.}\par%
If \(\vv\) is an eigenvector of \(A\), then so is \(2\vv\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\vv\) is an eigenvector of \(A\), then it is also an eigenvector of \(A^2\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\vv\) and \(\vu\) are eigenvectors of \(A\) with the same eigenvalue, then \(\vv+\vu\) is also an eigenvector with the same eigenvalue.%
\item{}\lititle{True\slash{}False.}\par%
If \(\lambda\) is an eigenvalue of \(A\), then \(\lambda^2\) is an eigenvalue of \(A^2\).%
\item{}\lititle{True\slash{}False.}\par%
A projection matrix satisfies \(P^2=P\). If \(P\) is a projection matrix, then the eigenvalues of \(P\) can only be 0 and 1.%
\item{}\lititle{True\slash{}False.}\par%
If \(\lambda\) is an eigenvalue of an \(n\times n\) matrix \(A\), then \(1+\lambda\) is an eigenvalue of \(I_n+A\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\lambda\) is an eigenvalue of two matrices \(A\) and \(B\) of the same size, then \(\lambda\) is an eigenvalue of \(A+B\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\vv\) is an eigenvector of two matrices \(A\) and \(B\) of the same size, then it is also an eigenvector of \(A+B\).%
\item{}\lititle{True\slash{}False.}\par%
A matrix \(A\) has 0 as an eigenvalue if and only if \(A\) has linearly dependent columns.%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Understanding the PageRank Algorithm}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Understanding the PageRank Algorithm}{}{Project: Understanding the PageRank Algorithm}{}{}{x:section:sec_proj_pagerank}
Sergey Brin and Lawrence Page, the founders of Google, decided that the importance of a web page can be judged by the number of links to it as well as the importance of those pages. It is this idea that leads to the PageRank algorithm.\footnote{Information for this project was taken from the websites \href{http://www.ams.org/samplings/feature-column/fcarc-pagerank}{\nolinkurl{ams.org/samplings/feature-column/fcarc-pagerank}} and \href{http://faculty.winthrop.edu/polaskit/spring11/math550/chapter.pdf}{\nolinkurl{faculty.winthrop.edu/polaskit/spring11/math550/chapter.pdf}}.\label{g:fn:idp12587160}} Google uses this algorithm (and others) to order search engine results. According to Google:\footnote{\href{http://web.archive.org/web/20111104131332/http://www.google.com/competition/howgooglesearchworks.html}{\nolinkurl{web.archive.org/web/20111104131332/http://www.google.com/competition/howgooglesearchworks.html}}\label{g:fn:idp12586136}}%
\begin{quote}%
PageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites.\end{quote}
To rank how ``important'' a website is, we need to make some assumptions. We assume that a person visits a page and then surfs the web by selecting a link from that page \textemdash{} all links on a given page are assigned the same probability of being chosen. As an example, assume a small set of seven pages 1, 2, 3, 4, 5, 6, and 7 with links between the pages given by the arrows as shown in \hyperref[x:figure:F_seven_page]{Figure~{\xreffont\ref{x:figure:F_seven_page}}}.\footnote{The Internet is very large and has upwards of 25 billion pages. This would leave us with an enormous transition matrix, even though most of its entries are 0. In fact, studies show that web pages have an average of about 10 links, so on average all but 10 entries of each column are 0. Working with such a large matrix is beyond what we want to do in this project, so we will just amuse ourselves with small examples that illustrate the general points.\label{g:fn:idp12592152}} So, for example, there is a hyperlink from page 4 to page 3, but no hyperlink in the opposite direction. If a web surfer starts on page 5, then there is probability of \(\frac{1}{2}\) that this person will surf to page 6 and a probability of \(\frac{1}{2}\) that the surfer will move to page 4. If there is no link leaving a page, as in the case of page 3, then the probability of remaining there is 1.%
\begin{figureptx}{A seven page internet}{x:figure:F_seven_page}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/seven_page_1.pdf}
\end{image}%
\tcblower
\end{figureptx}%
To rank pages, we need to know how likely it is that a surfer will land on a given page. In our seven page example, a person can land on page 3 from page 4 with a probability of \(\frac{1}{2}\) or from page 6 with a probability of \(\frac{1}{3}\). If there is a link from a page we assume that the surfer leaves the page, and if there are no links from a page then the surfer stays on that page. We also assume that the surfer does not use the ``Back'' key. This information for our seven page internet example can be summarized in a \terminology{transition matrix} \(T\) whose \(i,j\)th entry is the probability that a surfer lands on page \(i\) from page \(j\).%
\begin{equation*}
T= \left[  \begin{array}{ccccccc} 0\amp \frac{1}{2}\amp 0\amp 0\amp 0\amp 0\amp 0 \\ 1\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 1\amp \frac{1}{2}\amp 0\amp \frac{1}{3}\amp 0 \\ 0\amp \frac{1}{2}\amp 0\amp 0\amp \frac{1}{2}\amp 0\amp 0 \\ 0\amp 0\amp 0\amp \frac{1}{2}\amp 0\amp \frac{1}{3}\amp 1 \\ 0\amp 0\amp 0\amp 0\amp \frac{1}{2}\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp \frac{1}{3}\amp 0 \end{array}  \right]\text{.}
\end{equation*}
%
\par
Let us assume in our seven page internet that a user starts on page 6. That is, the probability that the user is initially on page 6 is 1, and so the probability that the user is on some other page is 0. This information can be encapsulated in a \terminology{state vector}%
\begin{equation*}
\vx_0 = \left[ 0 \ 0 \ 0 \ 0 \ 0 \ 1 \ 0  \right]^{\tr}\text{.}
\end{equation*}
%
\par
Since there are links from page 6 to pages 3, 5, and 7, there is a \(\frac{1}{3}\) probability that the surfer will next move to one of these pages. That means that at the next step, the state vector \(\vx_1\) for this user will be%
\begin{equation*}
\vx_1 = \left[ 0 \ 0 \ \frac{1}{3} \ 0 \ \frac{1}{3} \ 0 \ \frac{1}{3} \right]^{\tr}\text{.}
\end{equation*}
Note that%
\begin{equation*}
\vx_1 = T\vx_0\text{.}
\end{equation*}
As the user continues to surf the internet, the probabilities that the surfer is on a given page after the second, third, and fourth steps are given in the state vectors%
\begin{equation*}
\vx_2 = T\vx_1 = T^2 \vx_0, \ \ \vx_3 = T\vx_2 = T^3 \vx_0, \ \ \vx_4 = T\vx_3 = T^4 \vx_0\text{.}
\end{equation*}
%
\par
In general, the probabilities that the surfer is on a given page after the \(n\)th step is given by the state vector%
\begin{equation*}
\vx_n = T \vx_{n-1} = T^n \vx_0\text{.}
\end{equation*}
%
\par
This example illustrates the general nature of what is called a \terminology{Markov process} (see \hyperref[x:definition:def_Markov]{Definition~{\xreffont\ref{x:definition:def_Markov}}}). The two properties of the transition matrix \(T\) make \(T\) a special kind of matrix.%
\begin{definition}{}{g:definition:idp12603544}%
\index{stochastic matrix}%
A \terminology{stochastic} matrix is a matrix in which entries are nonnegative and the sum of the entries in every column is one.%
\end{definition}
In a Markov process, each generation depends only on the preceding generation and there may be a limiting value as we let the process continue indefinitely. We can test to see if that happens for this Markov process defined by \(T\) by doing some experimentation.%
\begin{project}{}{x:project:act_limit}%
Use appropriate technology to do the following. Choose several different initial state vectors \(\vx_0\) and calculate the vectors in the sequence \(\{T^n\vx_0\}\) for large values of \(n\). (Note that, as state vectors, the entries of \(\vx_0\) cannot be negative and the sum of the entries of \(\vx_0\) must be \(1\).) Explain the behavior of the sequence \(\{\vx_n\}\) as \(n\) gets large. Do you notice anything strange? What aspects of our seven page internet do you think explain this behavior? Clearly communicate all of the experimentation that you do. You may use the GeoGebra applet at \href{https://www.geogebra.org/m/b3dybnux}{\nolinkurl{geogebra.org/m/b3dybnux}}.%
\end{project}%
If there is a limit of the sequence \(\{T^n\vx_0\}\) (in other words, if there is a vector \(\vv\) such that \(\vv = \ds \lim_{n \to \infty} T^n \vx_0\)), we call this limit a \terminology{steady-state} or \terminology{equilibrium} vector. Such a steady-state vector has another important property. Since \(T\) is independent of \(n\) we have%
\begin{equation}
T \vv = T\left(\lim_{n \to \infty} T^n \vx_0 \right) = \lim_{n \to \infty} T^{n+1} \vx_0 = \vv\text{.}\label{x:men:eq_Google_evector}
\end{equation}
%
\par
Equation \hyperref[x:men:eq_Google_evector]{({\xreffont\ref{x:men:eq_Google_evector}})} shows that a steady state vector \(\vv\) is an eigenvector for \(T\) with eigenvalue 1. We can interpret the steady-state vector for \(T\) in an important way. Let \(t_j\) be the fraction of time we spend on page \(j\) and let \(l_j\) be the number of links on page \(j\). Then the fraction of the time that we end up on page \(i\) coming from page \(j\) is \(\frac{t_j}{l_j}\). If we sum over all the pages linked to page \(i\) we have that%
\begin{equation*}
t_i = \sum \frac{t_j}{l_j}\text{.}
\end{equation*}
%
\par
Notice that this is essentially the same process we used to obtain \(\vx_n\) from \(\vx_{n-1}\), and so we can interpret the steady-state vector \(\vv\) as telling us what fraction of a random web surfer's time was spent at each web page. If we assume that the time spent at a web page is a measure of its importance, then the steady-state vector tells us the relative importance of each web page. So this steady-state vector provides the page rankings for us. In other words, \begin{quote}%
The importance of a webpage may be measured by the relative size of the corresponding entry in the steady-state vector for an appropriately chosen Markov chain.\end{quote}
%
\begin{project}{}{g:project:idp12615192}%
Show that the limiting vector you found in \hyperref[x:project:act_limit]{Project Activity~{\xreffont\ref{x:project:act_limit}}} is an eigenvector of \(T\) with eigenvalue 1.%
\end{project}%
\hyperref[x:project:act_limit]{Project Activity~{\xreffont\ref{x:project:act_limit}}} illustrates one problem with our seven page internet. The steady-state vector shows that page 3 is the only important page, but that hardly seems reasonable in the example since there are other pages that must have some importance. The problem is that page 3 is a ``dangling'' page and does not lead anywhere. Once a surfer reaches that page, they are stuck there, overemphasizing its importance. So this dangling page acts like a sink, ultimately drawing all surfers to it. To adjust for dangling pages, we make the assumption that if a surfer reaches a dangling page (one with no links emanating from it), the surfer will jump to any page on the web with equal probability. So in our seven page example, once a surfer reaches page 3 the surfer will jump to any page on the web with probability \(\frac{1}{7}\).%
\begin{project}{}{x:project:LQ_G2}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Determine the transition matrix for our seven page internet with this adjustment.%
\item{}Approximate the steady-state vector for this adjusted matrix so that the entries are accurate to four decimal places. Use any appropriate technology to row reduce matrices.%
\item{}According to this adjusted model, which web page is now the most important? Why? Does this seem reasonable? Why?%
\end{enumerate}
\end{project}%
There is one more issue to address before we can consider ourselves ready to rank web pages. Consider the example of the five page internet shown in \hyperref[x:figure:F_five_page]{Figure~{\xreffont\ref{x:figure:F_five_page}}}.%
\begin{figureptx}{A five page internet}{x:figure:F_five_page}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/five_page_1.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\begin{project}{}{x:project:Q_no_limit}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why%
\begin{equation*}
\left[ \begin{array}{ccccc} 0\amp 0\amp 1\amp \frac{1}{2}\amp \frac{1}{5} \\ 1\amp 0\amp 0\amp 0\amp \frac{1}{5} \\ 0\amp 1\amp 0\amp 0\amp \frac{1}{5} \\ 0\amp 0\amp 0\amp 0\amp \frac{1}{5} \\ 0\amp 0\amp 0\amp \frac{1}{2}\amp \frac{1}{5} \end{array}  \right]\text{.}
\end{equation*}
is the transition matrix for this five page internet. (Keep in mind the adjustment we made for dangling pages.)%
\item{}Start with different initial state vectors \(\vx_0\) and determine if there is a limit to the Markov chain. Explain. You may use the GeoGebra applet at \href{https://www.geogebra.org/m/b3dybnux}{\nolinkurl{geogebra.org/m/b3dybnux}}.%
\end{enumerate}
\end{project}%
\hyperref[x:project:Q_no_limit]{Project Activity~{\xreffont\ref{x:project:Q_no_limit}}} shows that it is possible to construct an internet so that the corresponding Markov chain does not have a limit, even after adjusting for dangling pages. This is a significant problem if we want to provide a relative ranking of all web pages regardless of where a surfer starts. To fix this problem we need to make one final adjustment to arrive at a type of transition matrix that always provides a limit for our Markov chain.%
\begin{definition}{}{g:definition:idp12627480}%
\index{stochastic matrix!regular}%
A stochastic matrix is \terminology{regular} if its transition matrix \(T\) has the property that for some power \(k\), all the entries of \(T^k\) are positive.%
\end{definition}
Note that the transition matrix from \hyperref[x:project:Q_no_limit]{Project Activity~{\xreffont\ref{x:project:Q_no_limit}}} is not regular. Regular matrices have some especially nice properties, as the following theorem describes. We will not prove this theorem, but use it in the remainder of this project. The theorem shows that if we have a regular transition matrix, then there will a limit of the state vectors \(\vx_n\), and that this limit has a very interesting property.%
\begin{theorem}{}{}{x:theorem:thm_Google_1}%
Assume \(n \geq 2\) and that \(T\) is a regular \(n \times n\) stochastic matrix.%
\begin{enumerate}
\item{}\(\ds \lim_{k \to \infty} T^k\) exists and is a stochastic matrix.%
\item{}For any vector \(\vx\),%
\begin{equation*}
\lim_{k \to \infty} T^k \vx = \vc
\end{equation*}
for the same vector \(\vc\).%
\item{}The columns of \(\ds \lim_{k \to \infty} T^k\) are the same vector \(\vc\).%
\item{}The vector \(\vc\) is the unique eigenvector of \(T\) whose entries sum to 1.%
\item{}If \(\lambda\) is an eigenvalue of \(T\) not equal to 1, then \(|\lambda| \lt 1\).%
\end{enumerate}
%
\end{theorem}
Having a regular transition matrix \(T\) ensures that there is always the same limit \(\vv\) to the sequence \(T^k \vx_0\) for any starting vector \(\vx_0\). As mentioned before, the entries in \(\vv = \ds \lim_{n \to \infty} T^n \vx_0\) can be interpreted as telling us what fraction of the random surfer's time was spent at each webpage. If we interpret the amount of time a surfer spends at a page as a measure of the page's importance, then this steady-state vector \(\vv\) provides a ranking of the relative importance of each page in the web. This is the essence of Google's PageRank.%
\par
To make our final adjustment in the transition matrix to be sure that we obtain a regular matrix, we need to deal with the problems of ``loops'' in our internet. Loops, as illustrated in \hyperref[x:project:Q_no_limit]{Project Activity~{\xreffont\ref{x:project:Q_no_limit}}}, can act as sinks just like the dangling pages we saw earlier and condemn a user that enters such a loop to spend his\slash{}her time only on those pages in the loop. Quite boring! To account for this problem, we make a second adjustment.%
\par
Let \(p\) be a number between 0 and 1 (Google supposedly uses \(p=0.85\)). Suppose a surfer is on page \(i\). We assume with probability \(p\) that the surfer will chose any link on page \(i\) with equal probability. We make the additional assumption with probability \(1-p\) that the surfer will select with equal probability any page on the web.%
\par
If \(T\) is a transition matrix, incorporating the method we used to deal with dangling pages, then the adjusted transition matrix \(G\) (the Google matrix) is%
\begin{equation*}
G = pT + (1-p)Q\text{,}
\end{equation*}
where \(Q\) is the matrix all of whose entries are \(\frac{1}{n}\), where \(n\) is the number of pages in the internet (\(n=7\) in our seven page example). Since all of the entries of \(G\) are positive, \(G\) is a regular stochastic matrix.%
\begin{project}{}{g:project:idp12650520}%
Return to the seven page internet in \hyperref[x:figure:F_seven_page]{Figure~{\xreffont\ref{x:figure:F_seven_page}}}.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the Google matrix \(G\) for this internet.%
\item{}Approximate, to four decimal places, the steady-state vector for this internet.%
\item{}What is the relative rank of each page in this internet, and approximately what percentage of time does a random user spend on each page.%
\end{enumerate}
\end{project}%
We conclude with two observations. Consider the role of the parameter \(p\) in our final adjustment. Notice that if \(p=1\), then \(G = T\) and we have the original hyperlink structure of the web. However, if \(p=0\), then \(G = \frac{1}{n} I_n\), where \(I_n\) is the \(n \times n\) identity matrix with \(n\) as the number of pages in the web. In this case, every page is linked to every other page and a random surfer spends equal time on any page. Here we have lost all of the character of the linked structure of the web. Choosing \(p\) close to 1 retains much of the original hyperlink structure of the web.%
\par
Finally, the matrices that model the web are HUGE, and so the methods we used in this project to approximate the steady-state vectors are not practical. There are many methods for approximating eigenvectors that are often used in these situations, some of which we discuss in a later section.%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 10 The Inverse of a Matrix}
\typeout{************************************************}
%
\begin{chapterptx}{The Inverse of a Matrix}{}{The Inverse of a Matrix}{}{}{x:chapter:chap_matrix_inverse}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp12660376}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What does it mean for a matrix \(A\) to be invertible?%
\item{}How can we tell when an \(n \times n\) matrix \(A\) is invertible?%
\item{}If an \(n \times n\) matrix \(A\) is invertible, how do we find the inverse of \(A\)?%
\item{}If \(A\) and \(B\) are invertible \(n \times n\) matrices, why is \(AB\) invertible and what is \((AB)^{-1}\)?%
\item{}How can we use the inverse of a matrix in solving matrix equations?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Modeling an Arms Race}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Modeling an Arms Race}{}{Application: Modeling an Arms Race}{}{}{x:section:sec_appl_arms_race}
Lewis Fry Richardson was a Quaker by conviction who was deeply troubled by the major wars that had been fought in his lifetime. Richardson's training as a physicist led him to believe that the causes of war were phenomena that could be quantified, studied, explained, and thus controlled. He collected considerable data on wars and constructed a model to represent an arms race. The equations in his model caused him concern about the future as indicated by the following statement:%
\begin{quote}%
But it worried him that the equations also showed that the unilateral disarmament of Germany after 1918, enforced by the Allied Powers, combined with the persistent level of armaments of the victor countries would lead to the level of Germany's armaments growing again. In other words, the post-1918 situation was not stable. From the model he concluded that great statesmanship would be needed to prevent an unstable situation from developing, which could only be prevented by a change of policies. \footnote{\pubtitle{Nature} 135, 830-831 (18 May 1935) ``Mathematical Psychology of War'' (3420).\label{g:fn:idp12678424}}\end{quote}
Analyzing Richardson's arms race model utilizes matrix operations, including matrix inverses. We explore the basic ideas in Richardson's model later in this section.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_inverse_intro}
To this point we have solved systems of linear equations with matrix forms \(A \vx = \vb\) by row reducing the augmented matrices \([A \ | \ \vb]\). These linear matrix-vector equations should remind us of linear algebraic equations of the form \(ax = b\), where \(a\) and \(b\) are real numbers. Recall that we solved an equation of the form \(ax=b\) by dividing both sides by \(a\) (provided \(a \neq 0\)), giving the solution \(x = \frac{b}{a}\), or equivalently \(x = a^{-1}b\). The important property that the number \(a^{-1}\) has that allows us to solve a linear equation in this way is that \(a^{-1}a = 1\), so that \(a^{-1}\) is the multiplicative inverse of \(a\). We can solve certain types of matrix equations \(A \vx = \vb\) in the same way, provided we can find a matrix \(A^{-1}\) with similar properties. We investigate this situation in this section.%
\begin{exploration}{}{x:exploration:pa_2_c}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Before we define the inverse matrix, recall that the identity matrix \(I_n\) (with 1's along the diagonal and 0's everywhere else) is a multiplicative identity in the set of \(n \times n\) matrices (just like the real number 1 is the multiplicative identity in the set of real number). In particular, \(I_nA = AI_n = A\) for any \(n \times n\) matrix \(A\). Now we can generalize the inverse operation to matrices. For an \(n\times n\) matrix \(A\), we define \(A^{-1}\) to be the matrix which when multiplied by \(A\) gives us the identity matrix. In other words, \(AA^{-1}=A^{-1}A=I_n\). We can find the inverse of a matrix in a calculator by using the \(x^{-1}\) button.%
\par
For each of the following matrices, determine if the inverse exists using your calculator or other appropriate technology. If the inverse does exist, write down the inverse and check that it satisfies the defining property of the inverse matrix, that is \(AA^{-1}=A^{-1}A=I_n\). If the inverse doesn't exist, write down any error you received from the technology. Can you guess why the inverse does not exist for these matrices?%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}\(A = \left[ \begin{array}{cc} 1\amp 3 \\ 0\amp 4 \end{array} \right]\)%
\item{}\(A = \left[ \begin{array}{cc} 2\amp 3 \\ 4\amp 6 \end{array} \right]\)%
\item{}\(A = \left[ \begin{array}{rrc} 1\amp 2\amp 3 \\ -1\amp -1\amp 2\\1\amp 2\amp 2 \end{array} \right]\)%
\item{}\(A = \left[ \begin{array}{rrc} 1\amp 2\amp 3 \\ 2\amp 4\amp 6\\1\amp 2\amp 2 \end{array} \right]\)%
\item{}\(A = \left[ \begin{array}{rrc} 1\amp 0\amp 0 \\ 0\amp 2\amp 0\\0\amp 0\amp 3 \end{array} \right]\)%
\item{}\(A = \left[ \begin{array}{rrc} 1\amp 2\amp 3 \\ -1\amp -1\amp 2\\0\amp 1\amp 5 \end{array} \right]\)%
\end{enumerate}
\item{}Now we turn to the question of how to find the inverse of a matrix in general. With this approach, we will be able to determine which matrices have inverses as well. We will consider the \(2 \times 2\) case to make the calculations easier. Suppose \(A\) is a \(2 \times 2\) matrix. Our goal is to find a matrix \(B\) so that \(AB = I_2\) and \(BA = I_2\). If such a matrix exists, we will call \(B\) the inverse, \(A^{-1}\), of \(A\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}What does the equation \(AB = I_2\) tell us about the size of the matrix \(B\)?%
\item{}Now let \(A = \left[ \begin{array}{cc} 1\amp 2 \\ 1\amp 3 \end{array}  \right]\). We want to find a  matrix \(B\) so that \(AB = I_2\). Suppose \(B\) has columns \(\vb_1\) and \(\vb_2\), i.e. \(B = [\vb_1 \ \vb_2]\). Our definition of matrix multiplication shows that%
\begin{equation*}
AB = [A\vb_1 \ A\vb_2]\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\Alph*),ref=\theenumi.\theenumii.\Alph*]
\item{}If \(AB = I_2\), what must \(A\vb_1\) and \(A \vb_2\) equal?%
\item{}Use the result from part (a) to set up two matrix equations to solve to find \(\vb_1\) and \(\vb_2\). Then find \(\vb_1\) and \(\vb_2\). As a result, find the matrix \(B\).%
\item{}When we solve the two systems we have found a matrix \(B\) so that \(AB = I_2\). Is this enough to say that \(B\) is the inverse of \(A\)? If not, what else do we need to know to verify that \(B\) is in fact \(A^{-1}\)? Verify that \(B\) is \(A^{-1}\).%
\end{enumerate}
\item{}A matrix inverse is extremely useful in solving matrix equations and can help us in solving systems of equations. Suppose that \(A\) is an invertible matrix, i.e., there exists \(A^{-1}\) such that \(AA^{-1}=A^{-1}A=I_n\).%
\begin{enumerate}[font=\bfseries,label=(\Alph*),ref=\theenumi.\theenumii.\Alph*]
\item{}Consider the system \(A\vx=\vb\). Use the inverse of \(A\) to show that this system has a solution for every \(\vb\) and find an expression for this solution in terms of \(\vb\) and \(A^{-1}\). (Note that since matrix multiplication is not commutative, we have to pay attention to the order in which we multiply matrices. For example, \(A^{-1}AB=B\) while we cannot simplify \(ABA^{-1}\) to \(B\) unless \(A\) and \(B\) commute.)%
\item{}If \(A\), \(B\), and \(C\) are matrices and \(A+C = B+C\), then we can subtract the matrix \(C\) from both sides to see that \(A = B\). We saw in \hyperref[x:chapter:chap_matrix_operations]{Section~{\xreffont\ref{x:chapter:chap_matrix_operations}}} that there is no corresponding general cancellation property for matrix multiplication when we found that \(AB=AC\) could hold while \(B\neq C\). However, we can cancel \(A\) from this equation in certain circumstances. Suppose that \(AB=AC\) and that \(A\) is an invertible matrix. Show that we can cancel \(A\) in this case and conclude that \(B=C\). (Note: When simplifying the product of matrices, again keep in mind that matrix multiplication is not commutative.)%
\end{enumerate}
\end{enumerate}
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Invertible Matrices}
\typeout{************************************************}
%
\begin{sectionptx}{Invertible Matrices}{}{Invertible Matrices}{}{}{x:section:sec_mtx_invertible}
We now have an algebra of matrices in that we can add, subtract, and multiply matrices of the correct sizes. But what about division? In our early mathematics education we learned about \terminology{multiplicative inverses} (or reciprocals) of real numbers. The multiplicative inverse of a number \(a\) is the real number which when multiplied by \(a\) produces 1, the multiplicative identity of real numbers. This inverse is denoted \(a^{-1}\). For example, the multiplicative inverse of \(2\) is \(2^{-1}=\frac{1}{2}\) because%
\begin{equation*}
2\cdot \frac{1}{2} = 1 = \frac{1}{2}\cdot 2\text{.}
\end{equation*}
%
\par
Of course, we didn't have to write both products because multiplication of real numbers is a commutative operation. There are a couple of important things to note about multiplicative inverses \textemdash{} we can use the inverses of the number \(a\) to solve the simple linear equation \(ax+b = c\) for \(x\) (\(x = a^{-1}(c-b)\)), and not every real number has an inverse. The latter means that the inverse is not defined on the entire set of real numbers. We can extend the idea of inverses to matrices, although we will see that there are many more matrices than just the zero matrix that do not have inverses.%
\par
To define matrix inverses\footnote{We usually refer to a multiplicative inverse as just an inverse. Since every matrix has an additive inverse, there is no need to consider the existence of additive inverses.\label{g:fn:idp12725400}} we make an analogy with the property of inverses in the real numbers: \(x\cdot x^{-1}=1=x^{-1}\cdot x\).%
\begin{definition}{}{g:definition:idp12723224}%
\index{matrix!invertible}%
\index{matrix!inverse}%
Let \(A\) be an \(n \times n\) matrix.%
\begin{enumerate}
\item{}\(A\) is \terminology{invertible} if there is an \(n \times n\) matrix \(B\) so that \(AB = BA = I_n\).%
\item{}If \(A\) is invertible, an \terminology{inverse} of \(A\) is a matrix \(B\) such that \(AB = BA = I_n\).%
\end{enumerate}
%
\end{definition}
\index{matrix!non-singular}\index{matrix!singular} If an \(n \times n\) matrix \(A\) is invertible, its inverse will be unique (see \hyperlink{x:exercise:ex_2_c_unique_inverse}{Exercise~{\xreffont 1}}), and we denote the inverse of \(A\) as \(A^{-1}\). We also call an invertible matrix a \terminology{non-singular} matrix (with \terminology{singular} meaning non-invertible).%
\begin{activity}{}{x:activity:act_2_c_1}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(A = \left[ \begin{array}{cc} 1 \amp 0 \\ 0 \amp 0 \end{array} \right]\). Calculate \(AB\) where \(B = \left[ \begin{array}{cc} a\amp b \\ c\amp d \end{array} \right]\). Using your result, explain why it is not possible to have \(AB=I_2\), showing that \(A\) is non-invertible.%
\item{}Calculate \(AB\) where \(A = \left[ \begin{array}{cc} 1 \amp 2 \\ 2 \amp 4 \end{array} \right]\) and \(B = \left[ \begin{array}{cc} a\amp b \\ c\amp d \end{array} \right]\). Using your result, explain why the inverse of \(A\) doesn't exist.%
\end{enumerate}
\end{activity}%
We saw in \hyperref[x:activity:act_2_c_1]{Activity~{\xreffont\ref{x:activity:act_2_c_1}}} why the inverse does not exist for two specific matrices. We will find in the next section an easy criterion for determining when a matrix has an inverse. In short, when the RREF of the matrix has a pivot in every column and row, then the matrix will be invertible. We know that this condition relates to quite a few other linear algebra concepts we have seen so far, such as linear independence of columns and the columns spanning \(\R^n\). We will put these criteria together in one big theorem in the next section.%
\begin{activity}{}{x:activity:act_2_c_2}%
Suppose that \(A\) is an invertible \(n \times n\) matrix. Hence we have an inverse matrix \(A^{-1}\) for which \(AA^{-1}=A^{-1}A=I_n\). We will see how the inverse is useful in solving matrix equations involving \(A\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why the matrix expressions%
\begin{equation*}
\ A^{-1}(AB) , \ A^{-1}(A(BA)A^{-1}) \ \text{ and }  \ BA^{-1}BAA^{-1}B^{-1}A
\end{equation*}
can all be simplified to \(B\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp12771352}{}\quad{}Use the associative property of matrix multiplication.%
\item{}Suppose the system \(A \vx = \vb\) has a solution. Explain why then \(A^{-1}(A\vx)= A^{-1}\vb\). What does this equation simplify to?%
\item{}Since we found one single expression for the solution \(\vx\) in equation \(A\vx=\vb\), this implies that the equation has a unique solution. What does this imply about the matrix \(A\)?%
\end{enumerate}
\end{activity}%
As we saw in \hyperref[x:activity:act_2_c_1]{Activity~{\xreffont\ref{x:activity:act_2_c_1}}}, if the \(n \times n\) matrix \(A\) is invertible, then the equation \(A \vx = \vb\) is consistent for all \(\vb\) in \(\R^n\) and has the unique solution \(\vx = A^{-1} \vb\). This means that \(A\) has a pivot in every row and column, which is equivalent to the criterion that \(A\) reduces to \(I_n\), as we noted above.%
\par
Even though \(\vx=A^{-1}\vb\) is an explicit expression for the solution of the system \(A\vx=\vb\), using the inverse of a matrix is usually not a computationally efficient way to solve a matrix equation. Finding the RREF of a matrix computationally takes fewer steps to solve the matrix equation.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Finding the Inverse of a Matrix}
\typeout{************************************************}
%
\begin{sectionptx}{Finding the Inverse of a Matrix}{}{Finding the Inverse of a Matrix}{}{}{x:section:sec_mtx_inverse}
The next questions for us to address are how to tell when a matrix is invertible and how to find the inverse of an invertible matrix. Consider a \(2 \times 2\) matrix \(A\). To find the inverse matrix \(B = [\vb_1 \ \vb_2]\) of \(A\), we have to solve the two matrix-vector equations \(A \vb_1 = \begin{bmatrix}1 \\ 0 \end{bmatrix}\) and \(A \vb_2= \begin{bmatrix}0 \\1 \end{bmatrix}\) to find the columns of \(B\). Since \(A\) is the coefficient matrix for both systems, we apply the same row operations on both systems to reduce \(A\) to RREF. Thus, instead of solving the two matrix-vector equations separately, we could simply have found the RREF of%
\begin{equation*}
\left[\, A \ \left| \begin{array}{cc} 1 \amp  0 \\ 0 \amp  1 \end{array} \right. \right]
\end{equation*}
and done all of the work in one pass. Note that the right hand side of the augmented matrix is now \(I_2\). So we row reduce \([A \ | \ I_2]\), and if the systems are consistent, the reduced row echelon form of \([A \ | \ I_2]\) must be \([I_2 \ | \ A^{-1}]\). You should be able to see that this same process works in any dimension.%
\begin{paragraphs}{How to find the inverse of an \(n \times n\) matrix \(A\).}{g:paragraphs:idp12788760}%
%
\begin{itemize}[label=\textbullet]
\item{}Augment \(A\) with the identity matrix \(I_n\).%
\item{}Apply row operations to reduce the augmented matrix \([A \ | \ I_n]\). If the system is consistent, then the reduced row echelon form of \([A \ | \ I_n]\) will have the form \([I_n \ | \ B]\) (by \hyperref[x:activity:act_2_c_1]{Activity~{\xreffont\ref{x:activity:act_2_c_1}}} (d)). If the reduced row echelon form of \(A\) is not \(I_n\), then this step fails and \(A\) is not invertible.%
\item{}If \(A\) is row equivalent to \(I_n\), then the matrix \(B\) in the second step has the property that \(AB = I_n\). We will show later that the matrix \(B\) also satisfies \(BA = I_n\) and so \(B\) is the inverse of \(A\).%
\end{itemize}
%
\end{paragraphs}%
\begin{activity}{}{x:activity:act_2_c_3}%
Find the inverse of each matrix \emph{using the method above}, if it exists. Compare the result with the inverse that you get from using appropriate technology to directly calculate the inverse.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\left[ \begin{array}{crr} 1\amp 1\amp 1\\ 1\amp 1\amp -1\\ 1\amp -1\amp 0 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{cccc} 1\amp 1\amp 1\\ 2\amp 2\amp 2\\ 0\amp 0\amp 1 \end{array} \right]\)%
\end{enumerate}
\end{activity}%
We can use this method of finding the inverse of a matrix to derive a concrete formula for the inverse of a \(2 \times 2\) matrix:%
\begin{equation}
\left[ \begin{array}{cc} a\amp b \\ c\amp d \end{array}  \right]^{-1} = \frac{1}{ad-bc} \left[ \begin{array}{rr} d\amp -b \\ -c\amp a \end{array}  \right]\text{,}\label{x:men:eq_2_c_1}
\end{equation}
provided that \(ad-bc \neq 0\) (see \hyperlink{x:exercise:ex_2_c_2by2_inverse}{Exercise~{\xreffont 2}}). Hence, any \(2\times 2\) matrix \(\left[ \begin{array}{cc} a\amp b \\ c\amp d \end{array}  \right]\) has an inverse if and only if \(ad-bc\neq 0\). We call this quantity \terminology{determinant of \(A\)}, \(\det(A)\). We will see that the determinant of a general \(n\times n\) matrix will be essential in determining invertibility of the matrix.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Properties of the Matrix Inverse}
\typeout{************************************************}
%
\begin{sectionptx}{Properties of the Matrix Inverse}{}{Properties of the Matrix Inverse}{}{}{x:section:sec_mtx_inverse_props}
As we have done with every new operation, we ask what properties the inverse of a matrix has.%
\begin{activity}{}{x:activity:act_2_c_5}%
Consider the following questions about matrix inverses. If two \(n \times n\) matrices \(A\) and \(B\) are invertible, is the product \(AB\) invertible? If so, what is the inverse of \(AB\)? We answer these questions in this activity.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let%
\begin{equation*}
A = \left[ \begin{array}{cc} 1\amp 2\\1\amp 3 \end{array}  \right] \ \ \text{ and }   B = \left[ \begin{array}{rc} 2\amp 3\\-1\amp 2 \end{array}  \right]\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Use formula \hyperref[x:men:eq_2_c_1]{({\xreffont\ref{x:men:eq_2_c_1}})} to find the inverses of \(A\) and \(B\).%
\item{}Find the matrix product \(AB\). Is \(AB\) invertible? If so, use formula \hyperref[x:men:eq_2_c_1]{({\xreffont\ref{x:men:eq_2_c_1}})} to find the inverse of \(AB\).%
\item{}Calculate the products \(A^{-1}B^{-1}\) and \(B^{-1}A^{-1}\). What do you notice?%
\end{enumerate}
\item{}In part (a) we saw that the matrix product \(B^{-1}A^{-1}\) was the inverse of the matrix product \(AB\). Now we address the question of whether this is true in general. Suppose now that \(C\) and \(D\) are invertible \(n \times n\) matrices so that the matrix inverses \(C^{-1}\) and \(D^{-1}\) exist.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Use matrix algebra to simplify the matrix product \((CD)\left(D^{-1}C^{-1}\right)\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp12823832}{}\quad{}What do you know about \(DD^{-1}\) and \(CC^{-1}\)?%
\item{}Simplify the matrix product \(\left(D^{-1}C^{-1}\right)(CD)\) in a manner similar to part i.%
\item{}What conclusion can we draw from parts i and ii? Explain. What property of matrix multiplication requires us to reverse the order of the product when we create the inverse of \(CD\)?%
\end{enumerate}
\end{enumerate}
\end{activity}%
\hyperref[x:activity:act_2_c_5]{Activity~{\xreffont\ref{x:activity:act_2_c_5}}} gives us one important property of matrix inverses. The other properties given in the next theorem can be verified similarly.%
\begin{theorem}{}{}{x:theorem:thm_inverse_properties}%
Let \(A\) and \(B\) be invertible \(n \times n\) matrices. Then%
\begin{enumerate}
\item{}\(\left(A^{-1}\right)^{-1} = A\).%
\item{}The product \(AB\) is invertible and \((AB)^{-1} = B^{-1}A^{-1}\).%
\item{}The matrix \(A^{\tr}\) is invertible and \(\left(A^{\tr}\right)^{-1} = \left(A^{-1}\right)^{\tr}\).%
\end{enumerate}
%
\end{theorem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_inverse_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp12827800}%
For each of the following matrices \(A\),%
\begin{itemize}[label=\textbullet]
\item{}Use appropriate technology to find the reduced row echelon form of \([A \ | \ I_3]\).%
\item{}Based on the result of part (a), is \(A\) invertible? If yes, what is \(A^{-1}\)? If no, explain why.%
\item{}Let \(\vx = \left[ \begin{array}{c} x_1\\x_2\\x_3 \end{array} \right]\) and \(\vb = \left[ \begin{array}{c} 5\\4\\1 \end{array} \right]\). If \(A\) is invertible, solve the matrix equation \(A \vx = \vb\) using the inverse of \(A\). If \(A\) is not invertible, find all solutions, if any, to the equation \(A \vx = \vb\) using whatever method you choose.%
\end{itemize}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(A = \left[ \begin{array}{crr} 1\amp 2\amp 3 \\ 1\amp -1\amp -1 \\ 1\amp 0\amp 1 \end{array} \right]\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp12839448}{}\quad{}With \(A = \left[ \begin{array}{crr} 1\amp 2\amp 3 \\ 1\amp -1\amp -1 \\ 1\amp 0\amp 1 \end{array} \right]\), we have the following.%
\begin{itemize}[label=\textbullet]
\item{}The reduced row echelon form of \([A \ | \ I_3]\) is%
\begin{equation*}
\left[   \begin{array}{ccc|rrr} 1\amp 0\amp 0\amp \frac{1}{2}\amp 1\amp -\frac{1}{2} \\ 0\amp 1\amp 0\amp 1\amp 1\amp -2  \\ 0\amp 0\amp 1\amp -\frac{1}{2}\amp -1\amp \frac{3}{2} \end{array}  \right]\text{.}
\end{equation*}
%
\item{}Since \(A\) is row equivalent to \(I_3\), we conclude that \(A\) is invertible. The reduced row echelon form of \([A \ | \ I_3]\) tells us that%
\begin{equation*}
A^{-1} = \frac{1}{2} \left[ \begin{array}{rrr} 1\amp 2\amp -1 \\ 2\amp 2\amp -4 \\ -1\amp -2\amp 3 \end{array}  \right]\text{.}
\end{equation*}
%
\item{}The solution to \(A \vx = \vb\) is given by%
\begin{equation*}
vx = A^{-1} \vb = \frac{1}{2} \left[ \begin{array}{rrr} 1\amp 2\amp -1 \\ 2\amp 2\amp -4 \\ -1\amp -2\amp 3 \end{array}  \right] \left[ \begin{array}{c} 5\\4\\1 \end{array}  \right] = \left[ \begin{array}{r} 6\\7\\-5 \end{array}  \right]\text{.}
\end{equation*}
%
\end{itemize}
%
\item{}\(A = \left[ \begin{array}{crr} 1\amp 2\amp 5 \\ 1\amp -1\amp -1 \\ 1\amp 0\amp 1 \end{array} \right]\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp12846360}{}\quad{}With \(A = \left[ \begin{array}{crr} 1\amp 2\amp 5 \\ 1\amp -1\amp -1 \\ 1\amp 0\amp 1 \end{array}  \right]\), we have the following.%
\begin{itemize}[label=\textbullet]
\item{}The reduced row echelon form of \([A \ | \ I_3]\) is%
\begin{equation*}
\left[ \begin{array}{ccc|crr} 1\amp 0\amp 1\amp 0\amp 0\amp 1 \\ 0\amp 1\amp 2\amp 0\amp -1\amp 1  \\ 0\amp 0\amp 0\amp 1\amp 2\amp -3 \end{array}  \right]\text{.}
\end{equation*}
%
\item{}Since \(A\) is not row equivalent to \(I_3\), we conclude that \(A\) is not invertible.%
\item{}The reduced row echelon form of \([A \ | \ \vb]\) is%
\begin{equation*}
\left[ \begin{array}{ccc|c} 1\amp 0\amp 1\amp 0 \\ 0\amp 1\amp 2\amp 0 \\ 0\amp 0\amp 0\amp 1 \end{array}  \right]\text{.}
\end{equation*}
The fact that the augmented column is a pivot column means that the equation \(A \vx = \vb\) has no solutions.%
\end{itemize}
%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp12845848}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(A = \left[ \begin{array}{ccc} 0\amp 1\amp 0\\0\amp 0\amp 1\\0\amp 0\amp 0 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Show that \(A^2 \neq 0\) but \(A^3 = 0\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp12856600}{}\quad{}Let \(A = \left[ \begin{array}{ccc} 0\amp 1\amp 0\\0\amp 0\amp 1\\0\amp 0\amp 0 \end{array}  \right]\).%
\par
Using technology to calculate \(A^2\) and \(A^3\) we find that \(A^3=0\) while \(A^2 = \left[ \begin{array}{ccc} 0\amp 0\amp 1\\0\amp 0\amp 0\\0\amp 0\amp 0 \end{array} \right]\).%
\item{}Show that \(I - A\) is invertible and find its inverse. Compare the inverse of \(I-A\) to \(I+A+A^2\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp12855960}{}\quad{}Let \(A = \left[ \begin{array}{ccc} 0\amp 1\amp 0\\0\amp 0\amp 1\\0\amp 0\amp 0 \end{array}  \right]\).%
\par
For this matrix \(A\) we have \(I - A = \left[ \begin{array}{crr} 1\amp -1\amp 0\\0\amp 1\amp -1\\0\amp 0\amp 1 \end{array}  \right]\). The reduced row echelon form of \(I-A\) is%
\begin{equation*}
\left[ \begin{array}{ccc|ccc} 1\amp -0\amp 0\amp 1\amp 1\amp 1\\0\amp 1\amp 0\amp 0\amp 1\amp 1\\0\amp 0\amp 1\amp 0\amp 0\amp 1 \end{array}  \right]\text{,}
\end{equation*}
so \(I-A\) is invertible and \((I-A)^{-1} = \left[ \begin{array}{ccc} 1\amp 1\amp 1\\0\amp 1\amp 1\\0\amp 0\amp 1 \end{array}  \right]\). A straightforward matrix calculation also shows that%
\begin{equation*}
(I-A)^{-1} = I+A+A^2\text{.}
\end{equation*}
%
\end{enumerate}
\item{}Let \(M\) be an arbitrary square matrix such that \(M^3 = 0\). Show that \(M\) is invertible and find an inverse for \(M\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp12863640}{}\quad{}We can try to emulate the result of part (a) here. Expanding using matrix operations gives us%
\begin{align*}
(I-M)(I+M+M^2) \amp = (I+M+M^2) - (M+M^2+M^3)\\
\amp = (I+M+M^2) - (M+M^2+0)\\
\amp = I
\end{align*}
and%
\begin{align*}
(I+M+M^2)(I-M) \amp = (I+M+M^2) - (M+M^2+M^3)\\
\amp = (I+M+M^2) - (M+M^2+0)\\
\amp = I\text{.}
\end{align*}
So \(I-M\) is invertible and \((I-M)^{-1} = I+M+M^2\). This argument can be generalized to show that if \(M\) is a square matrix and \(M^n = 0\) for some positive integer \(n\), then \(I-M\) is invertible and%
\begin{equation*}
(I-M)^{-1} = I+M+M^2+ \cdots + M^{n-1}\text{.}
\end{equation*}
%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_inverse_summ}
%
\begin{itemize}[label=\textbullet]
\item{}If \(A\) is an \(n \times n\) matrix, then \(A\) is invertible if there is a matrix \(B\) so that \(AB = BA = I_n\). The matrix \(B\) is called the inverse of \(A\) and is denoted \(A^{-1}\).%
\item{}An \(n \times n\) matrix \(A\) is invertible if and only if \(A\) the reduced row echelon form of \(A\) is the \(n \times n\) identity matrix \(I_n\).%
\item{}To find the inverse of an invertible \(n \times n\) matrix \(A\), augment \(A\) with the identity and row reduce. If \([A \ | \ I_n] \sim [I_n \ | \ B]\), then \(B = A^{-1}\).%
\item{}If \(A\) and \(B\) are invertible \(n \times n\) matrices, then \((AB)^{-1}=B^{-1}A^{-1}\). Since the inverse of \(AB\) exists, the product of two invertible matrices is an invertible matrix.%
\item{}We can use the algebraic tools we have developed for matrix operations to solve equations much like we solve equations with real variables. We must be careful, though, to only multiply by inverses of invertible matrices, and remember that matrix multiplication is not commutative.%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_inverse_exer}
\begin{divisionexercise}{1}{}{}{x:exercise:ex_2_c_unique_inverse}%
Let \(A\) be an invertible \(n \times n\) matrix. In this exercise we will prove that the inverse of \(A\) is unique. To do so, we assume that both \(B\) and \(C\) are inverses of \(A\), that is \(AB=BA = I_n\) and \(AC=CA = I_n\). By considering the product \(BAC\) simplified in two different ways, show that \(B=C\), implying that the inverse of \(A\) is unique.%
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{x:exercise:ex_2_c_2by2_inverse}%
Let \(A = \left[ \begin{array}{cc} a\amp b \\ c\amp d \end{array}  \right]\) be an arbitrary \(2 \times 2\) matrix.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}If \(A\) is invertible, perform row operations to determine a row echelon form of \(A\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp12886296}{}\quad{}You may need to consider different cases, e.g., when \(a=0\) and when \(a \neq 0\).%
\item{}Under certain conditions, we can row reduce \([A \ | \ I_2]\) to \([I_2 \ | \ B]\), where%
\begin{equation*}
B=\frac{1}{ad-bc} \left[ \begin{array}{rr} d\amp -b \\ -c\amp a \end{array}  \right]\text{.}
\end{equation*}
Use the row echelon form of \(A\) from part (a) to find conditions under which the \(2 \times 2\) matrix \(A\) is invertible. Then derive the formula for the inverse \(B\) of \(A\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp12899736}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}For a few different \(k\) values, find the inverse of \(A = \left[ \begin{array}{cc} 1\amp k \\ 0\amp 1 \end{array} \right]\). From these results, make a conjecture as to what \(A^{-1}\) is in general.%
\item{}Prove your conjecture using the definition of inverse matrix.%
\item{}Find the inverse of \(A = \left[ \begin{array}{ccc} 1\amp k\amp \ell \\ 0\amp 1\amp m\\0\amp 0\amp 1 \end{array} \right]\).%
\end{enumerate}
(Note: You can combine the first two parts above by applying the inverse finding algorithm directly on \(A = \left[ \begin{array}{cc} 1\amp k \\ 0\amp 1 \end{array} \right]\).)%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp12906264}%
Solve for the matrix \(A\) in terms of the others in the following equation:%
\begin{equation*}
P^{-1}(D+CA)P=B
\end{equation*}
If you need to use an inverse, assume it exists.%
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp12901528}%
For which \(c\) is the matrix \(A=\left[ \begin{array}{ccr} 1\amp 2\amp -1\\2\amp 1\amp 1\\1\amp 5\amp c \end{array} \right]\) invertible?%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp12904088}%
For which \(c\) is the matrix \(A=\left[ \begin{array}{cc} c\amp 2\\3\amp c \end{array} \right]\) invertible?%
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp12909080}%
Let \(A\) and \(B\) be invertible \(n \times n\) matrices. Verify the remaining properties of \hyperref[x:theorem:thm_inverse_properties]{Theorem~{\xreffont\ref{x:theorem:thm_inverse_properties}}}. That is, show that%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\left(A^{-1}\right)^{-1} = A\).%
\item{}The matrix \(A^{\tr}\) is invertible and \(\left(A^{\tr}\right)^{-1} = \left(A^{-1}\right)^{\tr}\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp12914328}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is an invertible matrix, then for any two matrices \(B, C\), \(AB=AC\) implies \(B=C\).%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is invertible, then so is \(AB\) for any matrix \(B\).%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) and \(B\) are invertible \(n\times n\) matrices, then so is \(AB\).%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is an invertible \(n\times n\) matrix, then the equation \(A\vx=\vb\) is consistent for any \(\vb\) in \(\R^n\).%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is an invertible \(n\times n\) matrix, then the equation \(A\vx=\vb\) has a unique solution when it is consistent.%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is invertible, then so is \(A^2\).%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is invertible, then it reduces to the identity matrix.%
\item{}\lititle{True\slash{}False.}\par%
If a matrix is invertible, then so is its transpose.%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) and \(B\) are invertible \(n\times n\) matrices, then \(A+B\) is invertible.%
\item{}\lititle{True\slash{}False.}\par%
If \(A^2=0\), then \(I+A\) is invertible.%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: The Richardson Arms Race Model}
\typeout{************************************************}
%
\begin{sectionptx}{Project: The Richardson Arms Race Model}{}{Project: The Richardson Arms Race Model}{}{}{x:section:sec_proj_arms_race}
How and why a nation arms itself for defense depends on many factors. Among these factors are the offensive military capabilities a nation deems its enemies have, the resources available for creating military forces and equipment, and many others. To begin to analyze such a situation, we will need some notation and background. In this section we will consider a two nation scenario, but the methods can be extended to any number of nations. In fact, after World War I, Richardson collected data and created a model for the countries Czechoslovakia, China, France, Germany, England, Italy, Japan, Poland, the USA, and the USSR.\footnote{The Union of Soviet Socialist Republics (USSR), headed by Russia, was a confederation of socialist republics in Eurasia. The USSR disbanded in 1991. Czechoslovakia was a sovereign state in central Europe that peacefully split into the Czech Republic and Slovakia in 1993.\label{g:fn:idp12939032}}%
\par
Let \(N_1\) and \(N_2\) represent \(2\) different nations. Each nation has some military capability (we will call this the \terminology{armament} of the nation) at time \(n\) (think of \(n\) as representing the year). Let \(a_1(n)\) represent the armament of nation \(N_1\) at time \(n\), and \(a_2(n)\) the armament of nation \(N_2\) at time \(n\). We could measure \(a_i(n)\) in weaponry or dollars or whatever units make sense for armaments. The Richardson arms race model provides connections between the armaments of the two nations.%
\begin{project}{}{x:project:act_arms_race_1}%
We continue to analyze a two nation scenario. Let us suppose that our two nations are Iran (nation \(N_1\)) and Iraq (nation \(N_2\)). In 1980, Iraq invaded Iran resulting in a long and brutal 8 year war. Richardson was interested in analyzing data to see if such wars could be predicted by the changes in armaments of each nation. We construct the two nation model in this activity.%
\par
During each time period every nation adds or subtracts from its armaments. In our model, we will consider three main effects on the changes in armaments: the defense effect, fatigue effect and the grievance effect. In this activity we will discuss each effect in turn and then create a model to represent a two nation arms race.%
\begin{itemize}[label=\textbullet]
\item{}We first consider the defense effect. In a two nation scenario, each nation may react to the potential threat implied by an arms buildup of the other nation. For example, if nation \(N_1\) feels threatened by nation \(N_2\) (think of South and North Korea, or Ukraine and Russia, for example), then nation \(N_2\)'s level of armament might cause nation \(N_1\) to increase its armament in response. We will let \(\delta_{12}\) represent this effect of nation \(N_2\)'s armament on the armament of nation \(N_1\). Nation \(N_1\) will then increase (or decrease) its armament in time period \(n\) by the amount \(\delta_{12}a_2(n-1)\) based on the armament of nation \(N_2\) in time period \(n-1\). We will call \(\delta_{12}\) a \emph{defense coefficient}.\footnotemark{}%
\item{}Next we discuss the fatigue effect. Keeping a strong defense is an expensive and taxing enterprise, often exacting a heavy toll on the resources of a nation. For example, consider the fatigue that the U.S. experienced fighting wars in Iraq and Afghanistan, losing much hardware and manpower in these conflicts. Let \(\delta_{ii}\) represent this \terminology{fatigue factor} on nation \(i\). Think of \(\delta_{ii}\) as a measure of how much the nation has to replace each year, so a positive fatigue factor means that the nation is adding to its armament. The fatigue factor produces an effect of \(\delta_{ii}a_i(n-1)\) on the armament of nation \(i\) at time \(t=n\) that is the effect of the armament at time \(t=n-1\).%
\item{}The last factor we consider is what we will call a grievance factor. This can be thought of as the set of ambitions and\slash{}or grievances against other nations (such as the acquisition or reacquisition of territory currently belonging to another country). As an example, Argentina and Great Britain both claim the Falkland Islands as territory. In 1982 Argentina invaded the disputed Falkland Islands which resulted in a two-month long undeclared Falkland Islands war, which returned control to the British. It seems reasonable that one nation might want to have sufficient armament in place to support its claim if force becomes necessary. Assuming that these grievances and ambitions have a constant impact on the armament of a nation from year to year, let \(g_i\) be this ``grievance'' constant for nation \(i\).\footnotemark{} The effect a grievance factor \(g_i\) would have on the armament of nation \(i\) in year \(n\) would be to add \(g_i\) directly to \(a_i(n-1)\), since the factor \(g_i\) is constant from year to year (paying for arms and soldier's wages, for example) and does not depend on the amount of existing armament.%
\end{itemize}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Taking the three effects discussed above into consideration, explain why%
\begin{equation*}
a_1(n) = \delta_{11}a_1(n-1) + \delta_{12}a_2(n-1) + a_1(n-1) + g_1\text{.}
\end{equation*}
Then explain why%
\begin{equation}
a_1(n) = \left(\delta_{11}+1\right)a_1(n-1) + \delta_{12}a_2(n-1) + g_1\text{.}\label{x:men:eq_AR_1}
\end{equation}
%
\item{}Write an equation similar to equation \hyperref[x:men:eq_AR_1]{({\xreffont\ref{x:men:eq_AR_1}})} that describes \(a_2(n)\) in terms of the three effects.%
\item{}Let \(\va_n = \left[ \begin{array}{ccc} a_1(n) \\ a_2(n) \end{array}  \right]\). Explain why%
\begin{equation*}
\va_n = (D+I_2)\va_{n-1} + \vg\text{,}
\end{equation*}
where \(D = \left[ \begin{array}{cc} \delta_{11}\amp \delta_{12} \\ \delta_{21}\amp \delta_{22} \end{array}  \right]\) and \(\vg = [g_1 \ g_2]^{\tr}\).%
\end{enumerate}
\end{project}%
\footnotetext[23]{Of course, there are many other factors that have not been taken into account in the analysis. A nation may have heavily armed allies (like the U.S.) which may provide enough perceived security that this analysis is not relevant. Also, a nation might be a neutral state, such as Switzerland, and this analysis might not apply to such nations.\label{g:fn:idp12950936}}%
\footnotetext[24]{It might be possible for \(g_i\) to be negative if, for example, a nation feels that such disputes can and should only be settled by negotiation.\label{g:fn:idp12949144}}%
\begin{tableptx}{\textbf{Military Expenditures of Iran and Iraq 1966-1975}}{x:table:T_Expenditures}{}%
\centering%
{\tabularfont%
\begin{tabular}{lll}
Year&Iran&Iraq\tabularnewline\hrulethin
\(1966\)&\(662\)&\(391\)\tabularnewline\hrulethin
\(1967\)&\(903\)&\(378\)\tabularnewline\hrulethin
\(1968\)&\(1090\)&\(495\)\tabularnewline\hrulethin
\(1969\)&\(1320\)&\(615\)\tabularnewline\hrulethin
\(1970\)&\(1470\)&\(600\)\tabularnewline\hrulethin
\(1971\)&\(1970\)&\(618\)\tabularnewline\hrulethin
\(1972\)&\(2500\)&\(589\)\tabularnewline\hrulethin
\(1973\)&\(2970\)&\(785\)\tabularnewline\hrulethin
\(1974\)&\(5970\)&\(2990\)\tabularnewline\hrulethin
\(1975\)&\(7100\)&\(1690\)
\end{tabular}
}%
\end{tableptx}%
\begin{project}{}{x:project:act_arms_race_2}%
In order to analyze a specific arms race between nations, we need some data to determine values of the \(\delta_{ij}\) and the \(g_i\). \hyperref[x:table:T_Expenditures]{Table~{\xreffont\ref{x:table:T_Expenditures}}} shows the military expenditures of Iran and Iraq in the years leading up to their war in 1975. (The data is in millions of US dollars, adjusted for inflation and is taken from \textasciigrave{}\textasciigrave{}World Military Expenditures and Arms Transfers 1966-1975" by the U.S. Arms Control and Disarmament Agency.) We can perform regression (we will see how in a later section) on this data to obtain the following linear approximations:%
\begin{align}
a_1(n) \amp = 2.0780a_1(n-1) - 1.7081a_2(n-1) - 126.9954\label{x:mrow:eq_arms_race_regression_1}\\
a_2(n) \amp =  0.9419a_1(n-1) - 1.3283a_2(n-1) - 101.2980\label{x:mrow:eq_arms_race_regression_2}
\end{align}
%
\par
(Of course, the data does not restrict itself to only factors between the two countries, so our model will not be as precise as we might like. However, it is a reasonable place to start.) Use the regression equations \hyperref[x:mrow:eq_arms_race_regression_1]{({\xreffont\ref{x:mrow:eq_arms_race_regression_1}})} and \hyperref[x:mrow:eq_arms_race_regression_2]{({\xreffont\ref{x:mrow:eq_arms_race_regression_2}})} to explain why%
\begin{equation*}
D = \left[ \begin{array}{cc} 1.0780\amp -1.7081 \\  0.94194\amp -2.3283 \end{array}  \right] \ \text{ and }  \ \ \vg = [ -126.9954 \ -101.2980]^{\tr}
\end{equation*}
for our Iran-Iraq arms race.%
\end{project}%
\hyperref[x:project:act_arms_race_1]{Project Activity~{\xreffont\ref{x:project:act_arms_race_1}}} and \hyperref[x:project:act_arms_race_2]{Project Activity~{\xreffont\ref{x:project:act_arms_race_2}}} provide the basics to describe the general arms race model due to Richardson. If we have an \(m\) nation arms race with \(D = [\delta_{ij}]\) and \(\vg = [g_i]\) , then%
\begin{equation}
\va_n =  (D+I_m)\va_{n-1}+\vg\text{.}\label{x:men:eq_AR_model}
\end{equation}
%
\begin{project}{}{g:project:idp12987800}%
The idea of an arms race, theoretically, is to reach a point at which all parties feel secure and no additional money needs to be spent on armament. If such a situation ever arises, then the armament of all nations is stable, or in equilibrium. If we have an equilibrium solution, then for large values of \(n\) we will have \(\va_n = \va_{n-1}\). So to find an equilibrium solution, if it exists, we need to find a vector \(\va_E\) so that%
\begin{equation}
\va_E = (D+I)\va_E + \vg\label{x:men:eq_AR_equilibrium}
\end{equation}
where \(I\) is the appropriate size identity matrix. If \(\va_{E}\) exists, we call \(\va_E\) an \terminology{equilibrium state}.%
\par
We can apply matrix algebra to find the equilibrium state vector \(\va_E\) under certain conditions.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Assuming that \(\va_E\) exists, use matrix algebra and \hyperref[x:men:eq_AR_equilibrium]{Equation~({\xreffont\ref{x:men:eq_AR_equilibrium}})} to show that%
\begin{equation}
D\va_E + \vg = 0\text{.}\label{x:men:eq_AR_equilibrium_2}
\end{equation}
%
\item{}Under what conditions can we be assured that there will always be a unique equilibrium state \(\va_{E}\)? Explain. Under these conditions, how can we find this unique equilibrium state? Write this equilibrium state vector \(\va_{E}\) as a matrix-vector product.%
\item{}Does the arms race model for Iran and Iraq have an equilibrium solution? If so, find it. If not, explain why not. Use technology as appropriate.%
\item{}Assuming an equilibrium exists and that both nations behave in a way that supports the equilibrium, explain what the appropriate entry of the equilibrium state vector \(\va_E\) suggests about what Iran and Iraq's policies should be. What does this model say about why there might have been war between these two nations?%
\end{enumerate}
\end{project}%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 11 The Invertible Matrix Theorem}
\typeout{************************************************}
%
\begin{chapterptx}{The Invertible Matrix Theorem}{}{The Invertible Matrix Theorem}{}{}{x:chapter:chap_IMT}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp12998168}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What does it mean for two statements to be equivalent?%
\item{}How can we efficiently prove that a string of statements are all equivalent?%
\item{}What is the Invertible Matrix Theorem and why is it important?%
\item{}What are the equivalent conditions to a matrix being invertible?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_imt_intro}
This section is different than others in this book in that it contains only one long proof of the equivalence of statements that we have already discussed. As such, this is a theoretical section and there is no application connected to it.%
\par
\index{equivalent statements} The Invertible Matrix Theorem is a theorem that provides many different statements that are equivalent to having a matrix be invertible. To understand the Invertible Matrix Theorem, we need to know what it means for two statements to be \terminology{equivalent}. By equivalent, we mean that if one of the statements is true, then so is the other. We examine this idea in this preview activity.%
\begin{exploration}{}{x:exploration:pa_2_d}%
Let \(A\) be an \(n \times n\) matrix. In this activity we endeavor to understand why the two statements%
\begin{enumerate}[label=\Roman*]
\item{}The matrix \(A\) is invertible.%
\item{}The matrix \(A^{\tr}\) is invertible.%
\end{enumerate}
are equivalent. To demonstrate that statements I and II are equivalent, we need to argue that if statement I is true, then so is statement II, and if statement II is true then so is statement I.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let's first show that if statement I is true, then so is statement II. So we assume statement I. That is, we assume that \(A\) is an invertible matrix. So we know that there is an \(n \times n\) matrix \(B\) such that \(AB = BA = I_n\), where \(I_n\) is the \(n \times n\) identity matrix. To demonstrate that statement II must also be true, we need to verify that \(A^{\tr}\) is an invertible matrix.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}What is \(I_n^{\tr}\)?%
\item{}Take the transpose of both sides of the equation \(AB = I_n\) and use the properties of the transpose to write \((AB)^{\tr}\) in terms of \(A^{\tr}\) and \(B^{\tr}\).%
\item{}Take the transpose of both sides of the equation \(BA = I_n\) and use the properties of the transpose to write \((BA)^{\tr}\) in terms of \(A^{\tr}\) and \(B^{\tr}\).%
\item{}Explain how the previous two parts show that \(B^{\tr}\) is the inverse of \(A^{\tr}\), so that \(A^{\tr}\) is invertible. So we have shown that if statement I is true, so is statement II.\footnotemark{}%
\end{enumerate}
\item{}Now we prove that if statement II is true, then so is statement I. So we assume statement II. That is, we assume that the matrix \(A^{\tr}\) is invertible. We could do this in the same manner as part (a), or we could be a bit clever. Let's try to be clever.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}What matrix is \(\left(A^{\tr}\right)^{\tr}\)?%
\item{}Why can we use the result of part (a) with \(A^{\tr}\) in place of \(A\) to conclude that \(A\) is invertible? As a consequence, we have demonstrated that \(A\) is invertible if \(A^{\tr}\) is invertible. This concludes our argument that statements I and II are equivalent.%
\end{enumerate}
\end{enumerate}
\end{exploration}%
\footnotetext[25]{Note that statement I does not have to be true. We are only assuming that IF statement I is true, then statement II must also be true.\label{g:fn:idp12753304}}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Invertible Matrix Theorem}
\typeout{************************************************}
%
\begin{sectionptx}{The Invertible Matrix Theorem}{}{The Invertible Matrix Theorem}{}{}{x:section:sec_imt}
We have been introduced to many statements about existence and uniqueness of solutions to systems of linear equations, linear independence of columns of coefficient matrices, onto linear transformations, and many other items. In this section we will analyze these statements in light of how they are related to invertible matrices, with the main goal to understand the Invertible Matrix Theorem.%
\par
Recall that an \(n \times n\) matrix \(A\) is invertible if there is an \(n \times n\) matrix \(B\) such that \(AB = BA = I_n\), where \(I_n\) is the \(n \times n\) identity matrix. The Invertible Matrix Theorem is an important theorem in that it provides us with a wealth of statements that are all equivalent to the statement that an \(n \times n\) matrix \(A\) is invertible, and connects many of the topics we have been discussing so far this semester into one big picture.%
\begin{theorem}{The Invertible Matrix Theorem.}{}{g:theorem:idp13073048}%
\index{Invertible Matrix Theorem}%
Let \(A\) be an \(n \times n\) matrix. The following statements are equivalent:%
\begin{enumerate}[label=(\arabic*)]
\item{}\(A\) is an invertible matrix.%
\item\hypertarget{x:li:item_trivial_soln}{}The equation \(A \vx = \vzero\) has only the trivial solution.%
\item{}\(A\) has \(n\) pivot columns.%
\item{}The columns of \(A\) span \(\R^n\).%
\item{}\(A\) is row equivalent to the identity matrix \(I_n\).%
\item{}The columns of \(A\) are linearly independent.%
\item{}The columns of \(A\) form a basis for \(\R^n\).%
\item{}The matrix transformation \(T\) from \(\R^n\) to \(\R^n\) defined by \(T(\vx) = A\vx\) is one-to-one.%
\item{}The matrix equation \(A \vx = \vb\) has exactly one solution for each vector \(\vb\) in \(\R^n\).%
\item{}The matrix transformation \(T\) from \(\R^n\) to \(\R^n\) defined by \(T(\vx) = A\vx\) is onto.%
\item\hypertarget{x:li:item_AC_I}{}There is an \(n \times n\) matrix \(C\) so that \(AC = I_n\).%
\item\hypertarget{x:li:item_DA_I}{}There is an \(n \times n\) matrix \(D\) so that \(DA = I_n\).%
\item{}The scalar 0 is not an eigenvalue of \(A\).%
\item{}\(A^{\tr}\) is invertible.%
\end{enumerate}
%
\end{theorem}
The Invertible Matrix Theorem is a theorem that provides many different statements that are equivalent to a matrix being invertible. As discussed in \hyperref[x:exploration:pa_2_d]{Preview Activity~{\xreffont\ref{x:exploration:pa_2_d}}}, two statements are said to be \terminology{equivalent} if, whenever one of the statements is true, then the other is also true. So to demonstrate, say, statements I and II are equivalent, we need to argue that%
\begin{itemize}[label=\textbullet]
\item{}if statement I is true, then so is statement II, and%
\item{}if statement II is true then so is statement I.%
\end{itemize}
%
\par
The Invertible Matrix Theorem, however, provides a long list of statements that are equivalent. It would be inefficient to prove, one by one, that each pair of statements is equivalent. (There are \(\binom{14}{2} = 91\) such pairs.) Fortunately, there is a shorter method that we can use.%
\begin{activity}{}{x:activity:act_2_d_1}%
In this activity, we will consider certain parts of the Invertible Matrix Theorem and show that one implies another in a specific order. For all parts in this activity, we assume \(A\) is an \(n\times n\) matrix.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Consider the following implication: \((2) \implies (6)\):\footnotemark{}  If the equation \(A \vx = \vzero\) has only the trivial solution, then the columns of \(A\) are linearly independent. This shows that part 2 of the IMT implies part 6 of the IMT. Justify this implication as if it is a T\slash{}F problem.%
\item{}Justify the following implication: \((6) \implies (9)\): If the columns of \(A\) are linearly independent, then the matrix equation \(A\vx =\vb\) has exactly one solution for each vector \(\vb\) in \(\R^n\).%
\item{}Justify the following implication: \((9) \implies (4)\): If the equation \(A \vx = \vb\) has exactly one solution for every vector \(\vb\) in \(\R^n\), then the columns of \(A\) span \(\R^n\).%
\item{}Justify the following implication: \((4) \implies (2)\): If the columns of \(A\) span \(\R^n\), then the equation \(A \vx = \vzero\) has only the trivial solution.%
\item{}Using the above implications you proved, explain why we can conclude the following implication must also be true: \((2) \implies (9)\): If the equation \(A \vx = \vzero\) has only the trivial solution, then the matrix equation \(A\vx =\vb\) has exactly one solution for each vector \(\vb\) in \(\R^n\).%
\item{}Using the above implications you proved, explain why any one of the implications \((2)\), \((6)\), \((9)\), and \((4)\) implies any of the others.%
\end{enumerate}
\end{activity}%
\footnotetext[26]{The symbol \(\implies\) is the implication symbol, so \((1) \implies (12)\) is read to mean that statement \((1)\) of the theorem implies statement \((12)\).\label{g:fn:idp13100440}}%
Using a similar ordering of circular implications as in \hyperref[x:activity:act_2_d_1]{Activity~{\xreffont\ref{x:activity:act_2_d_1}}}, we can prove the Invertible Matrix Theorem by showing that each statement in the list implies the next statement, and that the last statement implies the first.%
\begin{proof}{Proof of the Invertible Matrix Theorem.}{g:proof:idp13118360}
%
\begin{descriptionlist}
\begin{dlinarrow}{Statement (1) implies Statement (2)}{g:li:idp13115928}%
This follows from work done in \hyperref[x:activity:act_2_d_1]{Activity~{\xreffont\ref{x:activity:act_2_d_1}}}.%
\end{dlinarrow}%
\begin{dlinarrow}{Statement (2) implies Statement (3)}{g:li:idp13116696}%
This was done in \hyperref[x:activity:act_2_d_1]{Activity~{\xreffont\ref{x:activity:act_2_d_1}}}.%
\end{dlinarrow}%
\begin{dlinarrow}{Statement (3) implies Statement (4)}{g:li:idp13125144}%
Suppose that every column of \(A\) is a pivot column. The fact that \(A\) is square means that every row of \(A\) contains a pivot, and hence the columns of \(A\) span \(\R^n\).%
\end{dlinarrow}%
\begin{dlinarrow}{Statement (4) implies Statement (5)}{g:li:idp13126424}%
Since the columns of \(A\) span \(\R^n\), it must be the case that every row of \(A\) contains a pivot. This means that \(A\) must be row equivalent to \(I_n\).%
\end{dlinarrow}%
\begin{dlinarrow}{Statement (5) implies Statement (6)}{g:li:idp13123992}%
If \(A\) is row equivalent to \(I_n\), there must be a pivot in every column, which means that the columns of \(A\) are linearly independent.%
\end{dlinarrow}%
\begin{dlinarrow}{Statement (6) implies Statement (7)}{g:li:idp13129624}%
If the columns of \(A\) are linearly independent, then there is a pivot in every column. Since \(A\) is a square matrix, there is a pivot in every row as well. So the columns of \(A\) span \(\R^n\). Since they are also linearly independent, the columns form a minimal spanning set, which is a basis of \(\R^n\).%
\end{dlinarrow}%
\begin{dlinarrow}{Statement (7) implies Statement (8)}{g:li:idp13131160}%
If the columns form a basis of \(\R^n\), then the columns are linearly independent. This means that each column is a pivot column, which also implies \(A\vx=\vzero\) has a unique solution and that \(T\) is one-to-one.%
\end{dlinarrow}%
\begin{dlinarrow}{Statement (8) implies Statement (9)}{g:li:idp13135256}%
If \(T\) is one-to-one, then \(A\) has a pivot in every column. Since \(A\) is square, every row of \(A\) contains a pivot. Therefore, the system \(A \vx = \vb\) is consistent for every \(\vb \in \R^n\) and has a unique solution.%
\end{dlinarrow}%
\begin{dlinarrow}{Statement (9) implies Statement (10)}{g:li:idp13144344}%
If \(A\vx=\vb\) has a unique solution for every \(\vb\), then the transformation \(T\) is onto since \(T(\vx)=\vb\) has a solution for every \(\vb\).%
\end{dlinarrow}%
\begin{dlinarrow}{Statement (10) implies Statement (11)}{g:li:idp13142296}%
Assume that \(T\) defined by \(T(\vx) = A\vx\) is onto. For each \(i\), let \(\ve_i\) be the \(i\)th column of the \(n \times n\) identity matrix \(I_n\). That is, \(\ve_i\) is the vector in \(\R^n\) with 1 in the \(i\)th component and 0 everywhere else. Since \(T\) is onto, for each \(i\) there is a vector \(\vc_i\) such that \(T(\vc_i) = A \vc_i = \ve_i\). Let \(C = [\vc_1  \ \vc_2 \ \cdots \ \vc_n]\). Then%
\begin{equation*}
AC = A[\vc_1  \ \vc_2 \ \cdots \ \vc_n] = [A\vc_1 \ A\vc_2 \ \cdots \ A\vc_n] = [\ve_1 \ \ve_2 \ \cdots \ \ve_n] = I_n\text{.}
\end{equation*}
%
\end{dlinarrow}%
\begin{dlinarrow}{Statement (11) implies Statement (12)}{g:li:idp13145752}%
Assume \(C\) is an \(n \times n\) matrix so that \(AC = I_n\). First we show that the matrix equation \(C \vx = \vzero\) has only the trivial solution. Suppose \(C \vx = \vzero\). Then multiplying both sides on the left by \(A\) gives us%
\begin{equation*}
A(C \vx) = A \vzero\text{.}
\end{equation*}
Simplifying this equation using \(AC=I_n\), we find \(\vx = \vzero\). Since \(C \vx = \vzero\) has only the trivial solution, every column of \(C\) must be a pivot column. Since \(C\) is an \(n \times n\) matrix, it follows that every row of \(C\) contains a pivot position. Thus, the matrix equation \(C \vx = \vb\) is consistent and has a unique solution for every \(\vb\) in \(\R^n\). Let \(\vv_i\) be the vector in \(\R^n\) satisfying \(C \vv_i = \ve_i\) for each \(i\) between 1 and \(n\) and let \(M = [\vv_1 \ \vv_2 \ \cdots \ \vv_n]\). Then \(CM = I_n\). Now we show that \(CA = I_n\). Since%
\begin{equation*}
AC = I_n
\end{equation*}
we can multiply both sides on the left by \(C\) to see that%
\begin{equation*}
C(AC) = CI_n\text{.}
\end{equation*}
Now we multiply both sides on the right by \(M\) and obtain%
\begin{equation*}
(C(AC))M = CM\text{.}
\end{equation*}
Using the associative property of matrix multiplication and the fact that \(CM = I_n\) shows that%
\begin{align*}
(CA)(CM) \amp = CM\\
CA = I_n\text{.}
\end{align*}
Thus, if \(A\) and \(C\) are \(n \times n\) matrices and \(AC = I_n\), then \(CA = I_n\). So we have proved our implication with \(D = C\)%
\end{dlinarrow}%
\begin{dlinarrow}{Statement (12) implies Statement (13)}{g:li:idp13164696}%
Assume that there is an \(n \times n\) matrix \(D\) so that \(DA = I_n\). Suppose \(A \vx = \vzero\). Then multiplying both sides by \(A\) on the left, we find that%
\begin{align*}
D(A\vx) \amp = D\vzero\\
(DA) \vx \amp = \vzero\\
\vx \amp = \vzero\text{.}
\end{align*}
So the equation \(A \vx = \vzero\) has only the trivial solution and \(0\) is not an eigenvalue for \(A\).%
\end{dlinarrow}%
\begin{dlinarrow}{Statement (13) implies Statement (14)}{g:li:idp13169432}%
If 0 is not an eigenvalue of \(A\), then the equation \(A \vx = \vzero\) has only the trivial solution. Since \hyperlink{x:li:item_trivial_soln}{statement~{\xreffont 2}} implies \hyperlink{x:li:item_AC_I}{statement~{\xreffont 11}}, there is an \(n \times n\) matrix \(C\) such that \(AC = I_n\). The proof that \hyperlink{x:li:item_AC_I}{statement~{\xreffont 11}} implies \hyperlink{x:li:item_DA_I}{statement~{\xreffont 12}} shows that \(CA = I_n\) as well. So \(A\) is invertible. By taking the transpose of both sides of the equation \(AA^{-1}=A^{-1}A=I_n\) (remembering \((AB)^{\tr}= B^{\tr}A^{\tr}\)) we find%
\begin{equation*}
(A^{-1})^{\tr} A^{\tr} = A^{\tr} (A^{-1})^{\tr} = I_n^{\tr}=I_n \,\text{.}
\end{equation*}
Therefore, \((A^{-1})^{\tr}\) is the inverse of \(A^{\tr}\) by definition of the inverse.%
\end{dlinarrow}%
\begin{dlinarrow}{Statement (14) implies Statement (1)}{g:li:idp13173400}%
Since statement (1) implies statement (14), we proved `If \(A\) is invertible, then \(A^{\tr}\) is invertible.'' Using this implication with \(A^{\tr}\) replaced by \(A\), we find that `If \(A^{\tr}\) is invertible, then \((A^{\tr})^{\tr}\) is invertible.'' But \((A^{\tr})^{\tr}=A\), which proves that statement (14) implies statement (1).%
\end{dlinarrow}%
\end{descriptionlist}
%
\end{proof}
This concludes our proof of the Invertible Matrix Theorem.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_imt_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp13184408}%
Let \(M = \left[ \begin{array}{cccc} 1\amp 2\amp 2\amp 1 \\ 0\amp 1\amp 0\amp 1 \\ 1\amp 3\amp 2\amp 3 \\ 0\amp 1\amp 0\amp 0 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Without doing any calculations, is \(M\) invertible? Explain your response.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp13179544}{}\quad{}The third column of \(M\) is twice the first, so the columns of \(M\) are not linearly independent. We conclude that \(M\) is not invertible.%
\item{}Is the equation \(M \vx = \vb\) consistent for every \(\vb\) in \(\R^4\)? Explain.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp13192216}{}\quad{}The equation \(M \vx = \vb\) is not consistent for every \(\vb\) in \(\R^4\). If it was, then the columns of \(M\) would span \(\R^4\) and, since there are exactly four columns, the columns of \(M\) would be a basis for \(\R^4\). Thus, \(M\) would have to be invertible, which it is not.%
\item{}Is the equation \(M \vx = \vzero\) consistent? If so, how many solutions does this equation have? Explain.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp13193240}{}\quad{}The homogeneous system is always consistent. Since the columns of \(M\) are linearly dependent, the equation \(M \vx = \vzero\) has infinitely many solutions.%
\item{}Is it possible to find a \(4 \times 4\) matrix \(P\) such that \(PM = I_4\)? Explain.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp13198232}{}\quad{}It is not possible to find a \(4 \times 4\) matrix \(P\) such that \(PM = I_4\). Otherwise \(M\) would have to be invertible.%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp13200792}%
Let \(M\) be an \(n \times n\) matrix whose eigenvalues are all nonzero.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(\vb \in \R^n\). Is the equation \(M \vx = \vb\) consistent? If yes, explain why and find all solutions in terms of \(M\) and \(\vb\). If no, explain why.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp13197720}{}\quad{}Since \(0\) is not an eigenvalue of \(M\), we know that \(M\) is invertible. Therefore, the equation \(M \vx = \vb\) has the unique solution \(\vx = M^{-1} \vb\).%
\item{}Let \(S\) be the matrix transformation defined by \(S(\vx) = M\vx\). Suppose \(S(\va) = S(\vb)\) for some vectors \(\va\) and \(\vb\) in \(\R^n\). Must there be any relationship between \(\va\) and \(\vb\)? If yes, explain the relationship. If no, explain why.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp13204760}{}\quad{}The fact that \(M\) is invertible implies that \(S\) is one-to-one. So if \(S(\va) = S(\vb)\), then it must be the case that \(\va = \vb\).%
\item{}Let \(\vm_1\), \(\vm_2\), \(\ldots\), \(\vm_n\) be the columns of \(M\). In how many ways can we write the zero vector as a linear combination of \(\vm_1\), \(\vm_2\), \(\ldots\), \(\vm_n\)? Explain.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp13212568}{}\quad{}Because \(M\) is invertible, the columns of \(M\) are linearly independent. Therefore, there is only the trivial solution to the equation%
\begin{equation*}
x_1\vm_1 + x_2\vm_2 +  \cdots + x_n\vm_n = \vzero\text{.}
\end{equation*}
%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_imt_summ}
%
\begin{itemize}[label=\textbullet]
\item{}Two statements are equivalent if, whenever one of the statements is true, then the other must also be true.%
\item{}To efficiently prove that a string of statements are all equivalent, we can prove that each statement in the list implies the next statement, and that the last statement implies the first.%
\item{}The Invertible Matrix Theorem gives us many conditions that are equivalent to an \(n \times n\) matrix being invertible. This theorem is important because it connects many of the concepts we have been studying.%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_imt_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp13212440}%
Consider the matrix \(A=\left[ \begin{array}{rcc} 1\amp 2\amp a \\ -1\amp 1\amp b \\ 1\amp 1\amp c \end{array} \right]\). Use the Invertible Matrix Theorem to geometrically describe the vectors \(\left[ \begin{array}{c} a \\ b\\ c \end{array} \right]\) which make \(A\) invertible without doing any calculations.%
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp13221784}%
Suppose \(A\) is an invertible \(n\times n\) matrix. Let \(T\) be the matrix transformation defined by \(T(\vx)=A\vx\) for \(\vx\) in \(\R^n\). Show that the matrix transformation \(S\) defined by \(S(\vx)=A^{-1}\vx\) is the inverse of the transformation \(T\) (i.e., \(S\) is the inverse function to \(T\) when the transformations are considered as functions).%
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp13231128}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
If \(A^2\) is invertible, then \(A\) is invertible.%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) and \(B\) are square matrices with \(AB\) invertible, then \(A\) and \(B\) are invertible.%
\item{}\lititle{True\slash{}False.}\par%
If the columns of an \(n \times n\) matrix \(A\) span \(\R^n\), then the equation \(A^{-1}\vx =\vzero\) has a unique solution.%
\item{}\lititle{True\slash{}False.}\par%
If the columns of \(A\) and columns of \(B\) form a basis of \(\R^n\), then so do the columns of \(AB\).%
\item{}\lititle{True\slash{}False.}\par%
If the columns of a matrix \(A\) form a basis of \(\R^n\), then so do the rows of \(A\).%
\item{}\lititle{True\slash{}False.}\par%
If the matrix transformation \(T\) defined by \(T(\vx)=A\vx\) is one-to-one for an \(n\times n\) matrix \(A\), then the columns of \(A^{-1}\) are linearly independent.%
\item{}\lititle{True\slash{}False.}\par%
If the columns of an \(n\times n\) matrix \(A\) span \(\R^n\), then so do the rows of \(A\).%
\item{}\lititle{True\slash{}False.}\par%
If there are two \(n\times n\) matrices \(A\) and \(B\) such that \(AB=I_n\), then the matrix transformation defined by \(T(\vx)=A^T\vx\) is one-to-one.%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
\end{chapterptx}
\end{partptx}
%
%
\typeout{************************************************}
\typeout{Chapter III The Vector Space \(\R^n\)}
\typeout{************************************************}
%
\begin{partptx}{The Vector Space \(\R^n\)}{}{The Vector Space \(\R^n\)}{}{}{x:part:part-vector-rn}
 %
%
\typeout{************************************************}
\typeout{Section 12 The Structure of \(\R^n\)}
\typeout{************************************************}
%
\begin{chapterptx}{The Structure of \(\R^n\)}{}{The Structure of \(\R^n\)}{}{}{x:chapter:chap_R_n}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp13251608}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What properties make \(\R^n\) a vector space?%
\item{}What is a subspace of \(\R^n\)?%
\item{}What properties do we need to verify to show that a set of vectors is a subspace of \(\R^n\)? Why?%
\item{}What important structure does the span of a set of vectors in \(\R^n\) have?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Connecting GDP and Consumption in Romania}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Connecting GDP and Consumption in Romania}{}{Application: Connecting GDP and Consumption in Romania}{}{}{x:section:sec_appl_romania}
It is common practice in the sciences to run experiments and collect data. Once data is collected it is necessary to find some way to analyze the data and predict future behavior from the data. One method is to find a curve that best ``fits'' the data, and one widely used method for curve fitting is called the \terminology{least squares} method.%
\par
For example, economists are often interested in \terminology{consumption}, which is the purchase of goods and services for use by households. In ``A Statistical Analysis of GDP and Final Consumption Using Simple Linear Regression, the Case of Romania 1990-2010'',\footnote{Bălăcescu, Aniela \& Zaharia, Marian. (2012). A STATISTICAL ANALYSIS OF GDP AND FINAL CONSUMPTION USING SIMPLE LINEAR REGRESSION. THE CASE OF ROMANIA 1990?2010. Annals - Economy Series. 4. 26-31. Available from: \href{https://www.researchgate.net/publication/227382939_A_STATISTICAL_ANALYSIS_OF_GDP_AND_FINAL_CONSUMPTION_USING_SIMPLE_LINEAR_REGRESSION_THE_CASE_OF_ROMANIA_1990-2010}{Research Gate}\footnote{\nolinkurl{researchgate.net/publication/227382939_A_STATISTICAL_ANALYSIS_OF_GDP_AND_FINAL_CONSUMPTION_USING_SIMPLE_LINEAR_REGRESSION_THE_CASE_OF_ROMANIA_1990-2010}\label{g:fn:idp13262104}}.\label{g:fn:idp13256472}} the authors collect data and then use simple linear regression to compare GDP (gross domestic product) to consumption in Romania. The data they used is seen in \hyperref[x:table:T_GDP_consumption]{Table~{\xreffont\ref{x:table:T_GDP_consumption}}}, with a corresponding scatterplot of the data (with consumption as independent variable and GDP as dependent variable). The units for GDP and consumption are milliions of leu (the currency of Romania is the leu \textemdash{} on December 21, 2018, one leu was worth approximately \textdollar{}0.25 U.S.) The authors conclude their paper with the following statement: \begin{quote}%
However, we can appreciate that linear regression model describes the correlation between the value of gross domestic product and the value of final consumption and may be transcribed following form: PIB = -3127.51+ 1.22 CF. Analysis of correlation between GDP and final consumption (private consumption and public consumption) will result in an increase of 1.22 units of monetary value of gross domestic product. We can conclude that the Gross Domestic Product of our country is strongly influenced by the private and public consumption.\end{quote}
 \begin{sidebyside}{2}{0}{0}{0}%
\begin{sbspanel}{0.5}%
\begin{tableptx}{\textbf{GDP and consumption in Romania}}{x:table:T_GDP_consumption}{}%
\resizebox{\ifdim\width > \linewidth\linewidth\else\width\fi}{!}{%
{\centering%
{\tabularfont%
\begin{tabular}{ccc}
Year&GDP&Consumption\tabularnewline\hrulethin
\(1990\)&\(85.8\)&\(68.0\)\tabularnewline\hrulethin
\(1991\)&\(220.4\)&\(167.3\)\tabularnewline\hrulethin
\(1992\)&\(602.9\)&\(464.3\)\tabularnewline\hrulethin
\(1993\)&\(2003.9\)&\(1523.6\)\tabularnewline\hrulethin
\(1994\)&\(4977.3\)&\(3845.2\)\tabularnewline\hrulethin
\(1995\)&\(7648.9\)&\(6257.7\)\tabularnewline\hrulethin
\(1996\)&\(11384.2\)&\(9713.8\)\tabularnewline\hrulethin
\(1997\)&\(25529.8\)&\(21972.2\)\tabularnewline\hrulethin
\(1998\)&\(37055.1\)&\(33311.2\)\tabularnewline\hrulethin
\(1999\)&\(55191.4\)&\(49311.9\)\tabularnewline\hrulethin
\(2000\)&\(80984.6\)&\(69587.4\)\tabularnewline\hrulethin
\(2001\)&\(117945.8\)&\(100731.7\)\tabularnewline\hrulethin
\(2002\)&\(152017.0\)&\(127118.8\)\tabularnewline\hrulethin
\(2003\)&\(197427.6\)&\(168818.7\)\tabularnewline\hrulethin
\(2004\)&\(247368.0\)&\(211054.6\)\tabularnewline\hrulethin
\(2005\)&\(288954.6\)&\(251038.1\)\tabularnewline\hrulethin
\(2006\)&\(344650.6\)&\(294867.6\)\tabularnewline\hrulethin
\(2007\)&\(416006.8\)&\(344937.0\)\tabularnewline\hrulethin
\(2008\)&\(514700.0\)&\(420917.5\)\tabularnewline\hrulethin
\(2009\)&\(498007.5\)&\(402246.0\)\tabularnewline\hrulethin
\(2010\)&\(513640.8\)&\(405422.4\)\tabularnewline\hrulethin
\end{tabular}
}%
\par}
}%
\end{tableptx}%
\end{sbspanel}%
\begin{sbspanel}{0.5}%
\begin{figureptx}{GDP and consumption data plot.}{g:figure:idp13297560}{}%
\includegraphics[width=\linewidth]{external/GDP_consumption.pdf}
\tcblower
\end{figureptx}%
\end{sbspanel}%
\end{sidebyside}%
%
\par
As we can see from the scatterplot, the relationship between the GDP and consumption is not exactly linear, but looks to be very close. To make correlations between GDP and consumption as the authors did, we need to understand how they determined their approximate linear relationship between the variables. With a good approximation function we can then compare the variables, extrapolate from the data, and make predictions or interpolate and estimate between data points. For example, we could use our approximation function to predict, as the authors did, how changes in consumption affect GDP (or vice versa). Later in this section we will see how to find the least squares line to fit this data \textemdash{} the best linear approximation to the data. This involves finding a vector in a certain subspace of \(\R^2\) that is closest to a given vector. Linear least squares approximation is a special case of a more general process that we will encounter in later sections where we learn how to project sets onto subspaces.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_rn_intro}
The set \(\R^n\) with vector addition and scalar multiplication has a nice algebraic structure. These operations satisfy a number of properties, such as associativity and commutativity of vector addition, the existence of an additive identity and additive inverse, distribution of scalar multiplication over vector addition, and others. These properties make it easier to work with the whole space since we can express the vectors as linear combinations of basis vectors in a unique way. This algebraic structure makes \(\R^n\) a \terminology{vector space}.%
\par
There are many subsets of \(\R^n\) that have this same structure. These subsets are called \terminology{subspaces} of \(\R^n\). These are the sets of vectors for which the addition of any two vectors is defined within the set, the scalar multiple of any vector by any scalar is defined within the set and the set contains the zero vector. One type of subset with this structure is the span of a set of vectors.%
\par
Recall that the span of a set of vectors \(\{\vv_1, \vv_2, \ldots, \vv_k\}\) in \(\R^n\) is the set of all linear combinations of the vectors. For example, if \(\vv_1=\left[ \begin{array}{c} 1\\1\\0 \end{array}  \right]\) and \(\vv_2=\left[ \begin{array}{c} 1\\0\\1 \end{array}  \right]\), then a linear combination of these two vectors is of the form%
\begin{equation*}
c_1 \vv_1 + c_2 \vv_2 = c_1 \left[ \begin{array}{c} 1\\1\\0 \end{array}  \right] + c_2 \left[ \begin{array}{c} 1\\0\\1 \end{array}  \right] = \left[ \begin{array}{c} c_1+c_2 \\c_1 \\c_2 \end{array}  \right] \,\text{.}
\end{equation*}
%
\par
One linear combination can be obtained by letting \(c_1=2, c_2=-3\), which gives the vector \(2\vv_1-3\vv_2=\left[ \begin{array}{c} -1\\-3\\2 \end{array} \right]\). All such linear combinations form the span of the vectors \(\vv_!\) and \(\vv_2\). In this case, these vectors will form a plane through the origin in \(\R^3\).%
\par
Now we will investigate if the span of two vectors form a subspace, i.e. if it has the same structure as a vector space.%
\begin{exploration}{}{x:exploration:pa_3_a}%
Let \(\vw_1\) and \(\vw_2\) be two vectors in \(\R^n\). Let \(W = \Span \{\vw_1, \vw_2\}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}For \(W\) to be a subspace of \(\R^n\), the sum of any two vectors in \(W\) must also be in \(W\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Pick two specific examples of vectors \(\vu, \vy\) in \(W\) (keeping \(\vw_1, \vw_2\) unknown\slash{}general vectors). For example, one specific \(\vu\) would be \(2\vw_1-3\vw_2\) as we used in the above example. Find the sum of \(\vu, \vy\). Is the sum also in \(W\)? Explain.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp13313816}{}\quad{}What does it mean for a vector to be in \(W\)?%
\item{}Now let \(\vu\) and \(\vy\) be arbitrary vectors in \(W\). Explain why \(\vu + \vy\) is in \(W\).%
\end{enumerate}
\item{}For \(W\) to be a subspace of \(\R^n\), any scalar multiple of any vector in \(W\) must also be in \(W\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Pick a specific example \(\vu\) in \(W\). Explain why \(2\vu, -3\vu, \pi\vu\) are all also in \(W\).%
\item{}Now let \(a\) be an arbitrary scalar and let \(\vu\) be an arbitrary vector in \(W\). Explain why the vector \(a \vu\) is in \(W\).%
\end{enumerate}
\item{}For \(W\) to be a subspace of \(\R^n\), the zero vector must also be in \(W\). Explain why the zero vector is in \(W\).%
\item{}Does vector addition being commutative for vectors in \(\R^n\) imply that vector addition is also commutative for vectors in \(W\)? Explain your reasoning.%
\item{}Suppose we have an arbitrary \(\vu\) in \(W\). There is an additive inverse of \(\vu\) in \(\R^n\). In other words, there is a \(\vu'\) such that \(\vu+ \vu'=\vzero\). Should this \(\vu'\) be also in \(W\)? If so, explain why. If not, give a counterexample.%
\item{}Look at the other properties of vector addition and scalar multiplication of vectors in \(\R^n\) listed in \hyperref[x:theorem:thm_vector_properties]{Theorem~{\xreffont\ref{x:theorem:thm_vector_properties}}} in \hyperref[x:chapter:chap_vector_representation]{Section~{\xreffont\ref{x:chapter:chap_vector_representation}}}. Which of these properties should also hold for vectors in \(W\)?%
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Vector Spaces}
\typeout{************************************************}
%
\begin{sectionptx}{Vector Spaces}{}{Vector Spaces}{}{}{x:section:sec_vec_spaces}
The set of \(n\)-dimensional vectors with the vector addition and scalar multiplication satisfy many properties, such as addition being commutative and associative, existence of an additive identity, and others. The set \(\R^n\) with these properties is an example of a \terminology{vector space}, a general structure examples of which include many other algebraic structures as we will see later.%
\begin{definition}{}{x:definition:def_3_a_1}%
\index{vector space}%
A set \(V\) on which an operation of addition and a multiplication by scalars is defined is a \terminology{vector space} if for all \(\vu\), \(\vv\), and \(\vw\) in \(V\) and all scalars \(a\) and \(b\):%
\begin{enumerate}
\item{}\(\vu + \vv\) is an element of \(V\) (we say that \(V\) is \terminology{closed} under the addition in \(V\)),%
\item{}\(\vu + \vv = \vv + \vu\) (we say that the addition in \(V\) is \terminology{commutative}),%
\item{}\((\vu + \vv) + \vw = \vu + (\vv + \vw)\) (we say that the addition in \(V\) is \terminology{associative}),%
\item{}there is a vector \(\vzero\) in \(V\) so that \(\vu + \vzero = \vu\) (we say that \(V\) contains an \terminology{additive identity} or \terminology{zero vector} \(\vzero\)),%
\item{}for each \(\vx\) in \(V\) there is an element \(\vy\) in \(V\) so that \(\vx + \vy = \vzero\) (we say that \(V\) contains an \terminology{additive inverse} \(\vy\) for each element \(\vx\) in \(V\)),%
\item{}\(a \vu\) is an element of \(V\) (we say that \(V\) is \terminology{closed} under multiplication by scalars),%
\item{}\((a+b) \vu = a\vu + b\vu\) (we say that \terminology{multiplication by scalars distributes over scalar addition}),%
\item{}\(a(\vu + \vv) = a\vu + a\vv\) (we say that \terminology{multiplication by scalars distributes over addition in \(V\)}),%
\item{}\((ab) \vu = a(b\vu)\),%
\item{}\(1 \vu = \vu\).%
\end{enumerate}
%
\end{definition}
\hyperref[x:theorem:thm_vector_properties]{Theorem~{\xreffont\ref{x:theorem:thm_vector_properties}}} in \hyperref[x:chapter:chap_vector_representation]{Section~{\xreffont\ref{x:chapter:chap_vector_representation}}} shows that \(\R^n\) is itself a vector space. As we will see, there are many other sets that have the same algebraic structure. By focusing on this structure and the properties of these operations, we can extend the theory of vectors we developed so far to a broad range of objects, making it easier to work with them. For example, we can consider linear combinations of functions or matrices, or define a basis for different types of sets of objects. Such algebraic tools provide us with new ways of looking at these sets of objects, including a geometric intuition when working with these sets. In this section, we will analyze subsets of \(\R^n\) which behave similar to \(\R^n\) algebraically. We will call such sets \terminology{subspaces}. In a later chapter we will encounter different kinds of sets that are also vector spaces.%
\begin{definition}{}{x:definition:def_3_a_subspaces}%
\index{subspace!of \(\R^n\)}%
A subset \(W\) of \(\R^n\) is a \terminology{subspace} of \(\R^n\) if \(W\) itself is a vector space using the same operations as in \(\R^n\).%
\end{definition}
The following example illustrates the process for demonstrating that a subset of \(\R^n\) is a subspace of \(\R^n\).%
\begin{example}{}{x:example:ex_3_a_1}%
There are many subsets of \(\R^n\) that are themselves vector spaces. Consider as an example the set \(W\) of vectors in \(\R^2\) defined by%
\begin{equation*}
W = \left\{ \left[ \begin{array}{c} x \\ 0 \end{array}  \right] \middle| x \text{ is a real number } \right\}\text{.}
\end{equation*}
%
\par
In other words, \(W\) is the set of vectors in \(\R^2\) whose second component is 0. To see that \(W\) is itself a vector space, we need to demonstrate that \(W\) satisfies all of the properties listed in \hyperref[x:definition:def_3_a_1]{Definition~{\xreffont\ref{x:definition:def_3_a_1}}}.%
\par
To prove the first property, we need to show that the sum of \emph{any} two vectors in \(W\) is again in \(W\). So we need to choose two \terminology{arbitrary} vectors in \(W\). Let \(\vu = \left[ \begin{array}{c} x \\ 0 \end{array}  \right]\) and \(\vv = \left[ \begin{array}{c} y \\ 0 \end{array}  \right]\) be vectors in \(W\). Note that%
\begin{equation*}
\vu + \vv = \left[ \begin{array}{c} x \\ 0 \end{array}  \right] + \left[ \begin{array}{c} y \\ 0 \end{array}  \right] = \left[ \begin{array}{c} x+y \\ 0 \end{array}  \right]\text{.}
\end{equation*}
%
\par
Since the second component of \(\vu + \vv\) is 0, it follows that \(\vu + \vv\) is in \(W\). Thus, the set \(W\) is closed under addition.%
\par
For the second property, that addition is commutative in \(W\), we can just use the fact that if \(\vu\) and \(\vv\) are in \(W\), they are also vectors in \(\R^2\) and \(\vu+\vv=\vv+\vu\) is satisfied in \(\R^2\). So the property also holds in \(W\).%
\par
A similar argument can be made for property (3).%
\par
Property (4) states the existence of the additive identity in \(W\). Note that \(\vzero\) is an additive identity in \(\R^2\) and if it is also an element in \(W\), then it will automatically be the additive identity of \(W\). Since the zero vector can be written as \(\vzero=\left[ \begin{array}{c} x \\ 0 \end{array} \right]\) with \(x=0\), \(\vzero\) is in \(W\). Thus, \(W\) satisfies property 4.%
\par
We will postpone property (5) for a bit since we can show that other properties imply property (5).%
\par
Property (6) is a closure property, just like property (1). We need to verify that \emph{any} scalar multiple of \emph{any} vector in \(W\) is again in \(W\). Consider an arbitrary vector \(\vu\) and an arbitrary scalar \(a\). Now%
\begin{equation*}
a \vu = a\left[ \begin{array}{c} x \\ 0 \end{array}  \right] = \left[ \begin{array}{c} ax \\ 0 \end{array}  \right]\text{.}
\end{equation*}
%
\par
Since the vector \(a \vu\) has a 0 as its second component, we see that \(a \vu\) is in \(W\). Thus, \(W\) is closed under scalar multiplication.%
\par
Properties (7), (8), (9) and (10) only depend on the operations of addition and multiplication by scalars in \(\R^2\). Since these properties depend on the operations and not the vectors, these properties will transfer to \(W\).%
\par
We still have to justify property (5) though. Note that since \(1-1=0\) in real numbers, by applying property (7) with \(a=1\), \(b=-1\), we find that%
\begin{equation*}
\vzero=0\vu=(a+b)\vu=a\vu+b\vu= \vu + (-1)\vu \,\text{.}
\end{equation*}
%
\par
Therefore, \((-1)\vu\) is an additive inverse for \(\vu\). Therefore, to show that the additive inverse of any \(\vu\) in \(W\) is also in \(W\), we simply note that any multiple of \(\vu\) is also in \(W\) and hence \((-1)\vu\) must also be in \(W\).%
\par
Since \(W\) satisfies all of the properties of a vector space, \(W\) is a vector space. Any subset of \(\R^n\) that is itself a vector space using the same operations as in \(\R^n\) is called a \terminology{subspace} of \(\R^n\).%
\end{example}
\hyperref[x:example:ex_3_a_1]{Example~{\xreffont\ref{x:example:ex_3_a_1}}} and our work \hyperref[x:exploration:pa_3_a]{Preview Activity~{\xreffont\ref{x:exploration:pa_3_a}}} bring out some important ideas. When checking that a subset \(W\) of a vector space \(\R^n\) is also a vector space, we can use the fact that all of the properties of the operations in \(\R^n\) are transferred to any closed subset \(W\). This implies that properties (2), (3), (7)-(10) are all automatically satisfied for \(W\) as well. Property (5) follows from the others. So we only need to check properties (1), (4) and (6). In fact, as we argued in the above example, property (4) also needs to be checked by simply checking that \(\vzero\) of \(\R^n\) is in \(W\). We summarize this result in the following theorem.%
\begin{theorem}{}{}{x:theorem:thm_3_a_subspace_Rn}%
A subset \(W\) of \(\R^n\) is a subspace of \(\R^n\) if%
\begin{enumerate}
\item{}whenever \(\vu\) and \(\vv\) are in \(W\) it is also true that \(\vu + \vv\) is in \(W\) (that is, \(W\) is \terminology{closed} under addition),%
\item{}whenever \(\vu\) is in \(W\) and \(a\) is a scalar it is also true that \(a\vu\) is in \(W\) (that is, \(W\) is \terminology{closed} under scalar multiplication),%
\item{}\(\vzero\) is in \(W\).%
\end{enumerate}
%
\end{theorem}
The next activity provides some practice using \hyperref[x:theorem:thm_3_a_subspace_Rn]{Theorem~{\xreffont\ref{x:theorem:thm_3_a_subspace_Rn}}}.%
\begin{activity}{}{x:activity:act_3_a_1}%
Use \hyperref[x:theorem:thm_3_a_subspace_Rn]{Theorem~{\xreffont\ref{x:theorem:thm_3_a_subspace_Rn}}} to answer the following questions. Justify your responses. For sets which lie inside \(\R^2\), sketch a pictorial representation of the set and explain why your picture confirms your answer.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Is the set \(W = \left\{ \left[ \begin{array}{c} x \\ y \end{array} \right] \middle| y = 2x\right\}\) a subspace of \(\R^2\)?%
\item{}Is the set \(W = \left\{ \left[ \begin{array}{c} x \\ 0 \\ 1 \end{array} \right] \middle| x \text{ is a scalar } \right\}\) a subspace of \(\R^3\)?%
\item{}Is the set \(W = \left\{ \left[ \begin{array}{c} x \\ x+y \end{array} \right] \middle| x, y \text{ are scalars } \right\}\) a subspace of \(\R^2\)?%
\item{}Is the set \(W = \left\{ \left[ \begin{array}{c} x \\ y \end{array} \right] \middle| y = 2x+1\right\}\) a subspace of \(\R^2\)?%
\item{}Is the set \(W = \left\{ \left[ \begin{array}{c} x \\ y \end{array} \right] \middle| y=x^2\right\}\) a subspace of \(\R^2\)?%
\item{}Is the set \(W = \left\{ \left[ \begin{array}{c} 0 \\ 0 \\ 0 \\ 0 \end{array} \right]\right\}\) a subspace of \(\R^4\)?%
\item{}Is the set \(W = \left\{ \left[ \begin{array}{c} x \\ y \\ z \end{array} \right] \middle| x^2+y^2+z^2 \leq 1\right\}\) a subspace of \(\R^3\)? Note that \(W\) is the unit sphere (a.k.a. unit ball) in \(\R^3\).%
\item{}Is the set \(W = \R^2\) a subspace of \(\R^3\)?%
\end{enumerate}
\end{activity}%
There are several important points that we can glean from \hyperref[x:activity:act_3_a_1]{Activity~{\xreffont\ref{x:activity:act_3_a_1}}}.%
\begin{itemize}[label=\textbullet]
\item{}A subspace is a vector space within a larger vector space, similar to a subset being a set within a larger set.%
\item{}The set containing the zero vector in \(\R^n\) is a subspace of \(\R^n\), and it is the only finite subspace of \(\R^n\).%
\item{}Every subspace of \(\R^n\) must contain the zero vector.%
\item{}No nonzero subspace is bounded \textemdash{} since a subspace must include all scalar multiples of its vectors, a subspace cannot be contained in a finite sphere or box.%
\item{}Since vectors in \(\R^k\) have \(k\) components, vectors in \(\R^k\) are not contained in \(\R^n\) when \(n \neq k\). However, if \(n > k\), then we can think of \(\R^n\) as containing a \emph{copy} (what we call an isomorphic image) of \(\R^k\) as the set of vectors with zeros as the last \(n-k\) components.%
\end{itemize}
%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Subspace Spanned by a Set of Vectors}
\typeout{************************************************}
%
\begin{sectionptx}{The Subspace Spanned by a Set of Vectors}{}{The Subspace Spanned by a Set of Vectors}{}{}{x:section:sec_sub_space_span}
One of the most convenient ways to represent a subspace of \(\R^n\) is as the span of a set of vectors. In \hyperref[x:exploration:pa_3_a]{Preview Activity~{\xreffont\ref{x:exploration:pa_3_a}}} we saw that the span of two vectors is a subspace of \(\R^n\). In the next theorem we verify this result for the span of an arbitrary number of vectors, extending the ideas you used in \hyperref[x:exploration:pa_3_a]{Preview Activity~{\xreffont\ref{x:exploration:pa_3_a}}}. Expressing a set of vectors as the span of some number of vectors is a quick way of justifying that this set is a subspace and it also provides us a geometric intuition for the set of vectors.%
\begin{theorem}{}{}{x:theorem:thm_3_a_span_subspace}%
Let \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) be vectors in \(\R^n\). Then \(\Span \{\vv_1, \vv_2, \ldots, \vv_k\}\) is a subspace of \(\R^n\).%
\end{theorem}
\begin{proof}{}{g:proof:idp13450008}
Let \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) be vectors in \(\R^n\). Let \(W = \Span\{\vv_1, \vv_2, \ldots, \vv_k\}\). To show that \(W\) is a subspace of \(\R^n\) we need to show that \(W\) is closed under addition and multiplication by scalars and that \(\vzero\) is in \(W\).%
\par
First we show that \(W\) is closed under addition. Let \(\vu\) and \(\vw\) be vectors in \(W\). This means that \(\vu\) and \(\vw\) are linear combinations of \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\). So there are scalars \(a_1\), \(a_2\), \(\ldots\), \(a_k\) and \(b_1\), \(b_2\), \(\ldots\), \(b_k\) so that%
\begin{equation*}
\vu = a_1\vv_1 + a_2\vv_2 + \cdots + a_k \vv_k \ \ \ \ \ \text{ and }  \ \ \ \ \ \vw = b_1\vv_1 + b_2\vv_2 + \cdots + b_k \vv_k\text{.}
\end{equation*}
%
\par
To demonstrate that \(\vu + \vw\) is in \(W\), we need to show that \(\vu + \vw\) is a linear combination of \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\). Using the properties of vector addition and scalar multiplication, we find%
\begin{align*}
\vu + \vw \amp = (a_1\vv_1 + a_2\vv_2 + \cdots + a_k \vv_k) + (b_1\vv_1 + b_2\vv_2 + \cdots + b_k \vv_k)\\
\amp = (a_1+b_1)\vv_1 + (a_2+b_2)\vv_2 + \cdots + (a_k+b_k) \vv_k\text{.}
\end{align*}
%
\par
Thus \(\vu + \vw\) is a linear combination of \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) and \(W\) is closed under vector addition.%
\par
Next we show that \(W\) is closed under scalar multiplication. Let \(\vu\) be in \(W\) and \(c\) be a scalar. Then%
\begin{equation*}
c\vu = c(a_1\vv_1 + a_2\vv_2 + \cdots + a_k \vv_k) = (ca_1)\vv_1 + (ca_2)\vv_2 + \cdots + (ca_k) \vv_k
\end{equation*}
and \(c\vu\) is a linear combination of \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) and \(W\) is closed under multiplication by scalars.%
\par
Finally, we show that \(\vzero\) is in \(W\). Since%
\begin{equation*}
\vzero = 0\vv_1 + 0\vv_2 + \cdots + 0 \vv_k \,\text{,}
\end{equation*}
%
\par
\(\vzero\) is in \(W\).%
\par
Since \(W\) satisfies all of the properties of a subspace as given in definition of a subspace, we conclude that \(W\) is a subspace of \(\R^n\).%
\end{proof}
The subspace \(W=\Span\{\vv_1, \vv_2, \ldots, \vv_k\}\) is called the \terminology{subspace of \(\R^n\) spanned by \(\vv_1, \vv_2, \ldots, \vv_k\)}. We also use the phrase ``subspace generated by \(\vv_1, \vv_2, \ldots, \vv_k\)'' since the vectors \(\vv_1, \vv_2, \ldots, \vv_k\) are the building blocks of all vectors in \(W\).%
\begin{activity}{}{x:activity:act_3_a_2}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Describe geometrically as best as you can the subspaces of \(\R^3\) spanned by the following sets of vectors. \(\left\{\left[ \begin{array}{c} 1 \\ 0 \\0 \end{array} \right]\right\}\) \(\left\{\left[ \begin{array}{c} 1 \\ 0\\0 \end{array} \right], \left[ \begin{array}{r} 0 \\ 1\\0 \end{array} \right]\right\}\)%
\item{}Express the following set of vectors as the span of some vectors to show that this set is a subspace. Can you give a geometric description of the set?%
\begin{equation*}
W = \left\{ \left[ \begin{array}{c} 2x+y-z \\ y \\ z \\ -x+3z \end{array}  \right]: x, y, z \text{ real numbers }  \right \}
\end{equation*}
%
\end{enumerate}
\end{activity}%
One additional conclusion we can draw from \hyperref[x:activity:act_3_a_1]{Activity~{\xreffont\ref{x:activity:act_3_a_1}}} and \hyperref[x:activity:act_3_a_2]{Activity~{\xreffont\ref{x:activity:act_3_a_2}}} is that subspaces of \(\R^n\) are made up of ``flat'' subsets. The span of a single nonzero vector is a line (which is flat), and the span of a set of two distinct nonzero vectors is a plane (which is also flat). So subspaces of \(\R^n\) are linear (or ``flat'') subsets of \(\R^n\). That is why we can recognize that the non-flat parabola in \hyperref[x:activity:act_3_a_1]{Activity~{\xreffont\ref{x:activity:act_3_a_1}}} is not a subspace of \(\R^2\).%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_rn_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp13497368}%
Let \(W = \left\{\left[ \begin{array}{c} 2r+s+t \\ r+t \\ r+s \end{array} \right] : r,s,t \in \R \right\}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(W\) is a subspace of \(\R^3\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp13489944}{}\quad{}Every vector in \(W\) has the form%
\begin{equation*}
\left[ \begin{array}{c}  2r+s+t \\ r+t \\ r+s \end{array}  \right] = r\left[ \begin{array}{c} 2 \\ 1 \\ 1 \end{array}  \right] + s\left[ \begin{array}{c} 1 \\ 0 \\ 1 \end{array}  \right] + t\left[ \begin{array}{c} 1 \\ 1 \\ 0 \end{array}  \right]
\end{equation*}
for some real numbers \(r\), \(s\), and \(t\). Thus,%
\begin{equation*}
W = \Span\left\{\left[ \begin{array}{c} 2 \\ 1 \\ 1 \end{array}  \right], \left[ \begin{array}{c} 1 \\ 0 \\ 1 \end{array}  \right], \left[ \begin{array}{c} 1 \\ 1 \\ 0 \end{array}  \right] \right\}\text{.}
\end{equation*}
As a span of a set of vectors, we know that \(W\) is a subspace of \(\R^3\).%
\item{}Describe in detail the geometry of the subspace \(W\) (e.g., is it a line, a union of lines, a plane, a union of planes, etc.)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp13505048}{}\quad{}Let \(\vv_1 = \left[ \begin{array}{c} 2 \\ 1 \\ 1 \end{array} \right]\), \(\vv_2 = \left[ \begin{array}{c} 1 \\ 0 \\ 1 \end{array} \right]\), and \(\vv_3 = \left[ \begin{array}{c} 1 \\ 1 \\ 0 \end{array} \right]\). The reduced row echelon form of \([\vv_1 \ \vv_2 \ \vv_3]\) is \(\left[ \begin{array}{ccr} 1\amp 0\amp 1\\0\amp 1\amp -1\\0\amp 0\amp 0 \end{array} \right]\). The pivot columns of \([\vv_1 \ \vv_2 \ \vv_3]\) form a linearly independent set with the same span as \(\{\vv_1, \vv_2, \vv_3\}\), So \(W = \Span\{\vv_1, \vv_2\}\) and \(W\) forms the plane in \(\R^3\) through the origin and the points \((2,1,1)\) and \((1,0,1)\).%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp13499800}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(X = \Span\left\{ \left[ \begin{array}{c} 1\\0\\0 \end{array}  \right] \right\}\) and let \(Y = \Span\left\{ \left[ \begin{array}{c} 0\\1\\0 \end{array}  \right] \right\}\). That is, \(X\) is the \(x\)-axis and \(Y\) the \(y\)-axis in three-space. Let%
\begin{equation*}
X+Y = \{\vx+\vy : \vx \in X \text{ and }  \vy \in Y\}\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Is \(\left[ \begin{array}{c} 2\\3\\0 \end{array} \right]\) in \(X+Y\)? Justify your answer.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp13512984}{}\quad{}We let \(X = \Span\left\{ \left[ \begin{array}{c} 1\\0\\0 \end{array} \right] \right\}\) and \(Y = \Span\left\{ \left[ \begin{array}{c} 0\\1\\0 \end{array} \right] \right\}\). T%
\par
Let \(\vw = \left[ \begin{array}{c} 2\\3\\0 \end{array} \right]\), \(\vx = 2\left[ \begin{array}{c} 1\\0\\0 \end{array} \right]\), and \(\vy = 3\left[ \begin{array}{c} 0\\1\\0 \end{array} \right]\). Since \(\vw = \vx+\vy\) with \(\vx \in X\) and \(\vy \in Y\) we conclude that \(\vw \in X + Y\).%
\item{}Is \(\left[ \begin{array}{c} 1\\1\\1 \end{array} \right]\) in \(X+Y\)? Justify your answer.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp13518104}{}\quad{}We let \(X = \Span\left\{ \left[ \begin{array}{c} 1\\0\\0 \end{array} \right] \right\}\) and \(Y = \Span\left\{ \left[ \begin{array}{c} 0\\1\\0 \end{array} \right] \right\}\). T%
\par
Every vector in \(X\) has the form \(a \ve_1\) for some scalar \(a\) (where \(\ve_1 = \left[ \begin{array}{c} 1\\0\\0 \end{array} \right]\), and every vector in \(Y\) has the form \(b \ve_2\) for some scalar \(b\) (where \(\ve_2 = \left[ \begin{array}{c} 0\\1\\0 \end{array} \right]\)). So every vector in \(X+Y\) is of the form \(a\ve_1 + b\ve_2 = \left[ \begin{array}{c} a\\b\\0 \end{array} \right]\). Since the vector \(\left[ \begin{array}{c} 1\\1\\1 \end{array} \right]\) does not have a \(0\) in the third component, we conclude that in \(\left[ \begin{array}{c} 1\\1\\1 \end{array} \right]\) is not in \(X+Y\).%
\item{}Assume that \(X+Y\) is a subspace of \(\R^3\). Describe in detail the geometry of this subspace.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp13526936}{}\quad{}We let \(X = \Span\left\{ \left[ \begin{array}{c} 1\\0\\0 \end{array} \right] \right\}\) and \(Y = \Span\left\{ \left[ \begin{array}{c} 0\\1\\0 \end{array} \right] \right\}\). T%
\par
As we just argued, every vector in \(X+Y\) has the form \(a\ve_1+b\ve_2\). So \(X+Y = \Span\{\ve_1,\ve_2\}\), which is the \(xy\)-plane in \(\R^3\).%
\end{enumerate}
\item{}Now let \(W_1\) and \(W_2\) be arbitrary subspaces of \(\R^n\) for some positive integer \(n\). Let%
\begin{equation*}
W_1+W_2 = \{\vw_1+\vw_2 : \vw_1 \in W_1 \text{ and }  \vw_2 \in W_2\}\text{.}
\end{equation*}
Show that \(W_1+W_2\) is a subspace of \(\R^n\). The set \(W_1+W_2\) is called the \terminology{sum} of the subspaces \(W_1\) and \(W_2\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp13535512}{}\quad{}To see why the set \(W_1+W_2\) is a subspace of \(\R^3\), suppose that \(\vx\) and \(\vy\) are in \(W_1+W_2\). Then \(\vx = \vu_1+\vu_2\) and \(\vy = \vz_1+\vz_2\) for some \(\vu_1, \vz_1\) in \(W_1\) and some \(\vu_2, \vz_2\) in \(W_2\). Then%
\begin{equation*}
\vx+\vy = (\vu_1+\vu_2)+(\vz_1+\vz_2) = (\vu_1+\vz_1)+(\vu_2+\vz_2)\text{.}
\end{equation*}
Since \(W_1\) is a subspace of \(\R^3\) it follows that \(\vu_1+\vz_1 \in W_1\). Similarly, \(\vu_2+\vz_2 \in W_2\). This makes \(\vx + \vy\) an element of \(W_1+W_2\). Also, suppose that \(a\) is a scalar. Then%
\begin{equation*}
a \vx = a(\vu_1+\vu_2) = a\vu_1 + a\vu_2\text{.}
\end{equation*}
Since \(W_1\) is a subspace of \(\R^3\) it follows that \(a\vu_1 \in W_1\). Similarly, \(a\vu_2 \in W_2\). This makes \(a\vx\) an element of \(W_1+W_2\). Finally, since \(\vzero\) is in both \(W_1\) and \(W_2\), and \(\vzero = \vzero + \vzero\), it follows that \(\vzero\) is an element of \(W_1+W_2\). We conclude that \(W_1+W_2\) is a subspace of \(\R^3\).%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_rn_summ}
%
\begin{itemize}[label=\textbullet]
\item{}A vector space is a set \(V\) with operations of addition and scalar multiplication defined on \(V\) such that for all \(\vu\), \(\vv\), and \(\vw\) in \(V\) and all scalars \(a\) and \(b\):%
\begin{enumerate}
\item{}\(\vu + \vv\) is an element of \(V\) (we say that \(V\) is \terminology{closed} under the addition in \(V\)),%
\item{}\(\vu + \vv = \vv + \vu\) (we say that the addition in \(V\) is \terminology{commutative}),%
\item{}\((\vu + \vv) + \vw = \vu + (\vv + \vw)\) (we say that the addition in \(V\) is \terminology{associative}),%
\item{}there is a vector \(\vzero\) in \(V\) so that \(\vu + \vzero = \vu\) (we say that \(V\) contains an \terminology{additive identity} or \terminology{zero vector} \(\vzero\)),%
\item{}for each \(\vx\) in \(V\) there is an element \(\vy\) in \(V\) so that \(\vx + \vy = \vzero\) (we say that \(V\) contains an \terminology{additive inverse} \(\vy\) for each element \(\vx\) in \(V\)),%
\item{}\(a \vu\) is an element of \(V\) (we say that \(V\) is \terminology{closed} under multiplication by scalars),%
\item{}\((a+b) \vu = a\vu + b\vu\) (we say that \terminology{multiplication by scalars distributes over scalar addition}),%
\item{}\(a(\vu + \vv) = a\vu + a\vv\) (we say that \terminology{multiplication by scalars distributes over addition in \(V\)}),%
\item{}\((ab) \vu = a(b\vu)\),%
\item{}\(1 \vu = \vu\).%
\end{enumerate}
%
\item{}For every \(n\), \(\R^n\) is a vector space.%
\item{}A subset \(W\) of \(\R^n\) is a subspace of \(\R^n\) if \(W\) is a vector space using the same operations as in \(\R^n\).%
\item{}To show that a subset \(W\) of \(\R^n\) is a subspace of \(\R^n\), we need to prove the following:%
\begin{enumerate}
\item{}\(\vu + \vv\) is in \(W\) whenever \(\vu\) and \(\vv\) are in \(W\) (when this property is satisfied we say that \(W\) is \terminology{closed} under addition),%
\item{}\(a \vu\) is in \(W\) whenever \(a\) is a scalar and \(\vu\) is in \(W\) (when this property is satisfied we say that \(W\) is \terminology{closed} under multiplication by scalars),%
\item{}\(\vzero\) is in \(W\).%
\end{enumerate}
The remaining properties of a vector space are properties of the operation, and as long as we use the same operations as in \(\R^n\), the operation properties follow the operations.%
\item{}The span of any set of vectors in \(\R^n\) is a subspace of \(\R^n\).%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_rn_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp13589016}%
Each of the following regions or graphs determines a ≈subset \(W\) of \(\R^2\). For each region, discuss each of the subspace properties of Theorem 12.4 and explain with justification if the set \(W\) satisfies each property or not.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/3_a_subspaces_a.pdf}
\end{image}%
%
\item{}\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/3_a_subspaces_b.pdf}
\end{image}%
%
\item{}\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/3_a_subspaces_e.pdf}
\end{image}%
%
\item{}\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/3_a_subspaces_g.pdf}
\end{image}%
%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp13597976}%
Determine which of the following sets \(W\) is a subspace of \(\R^n\) for the indicated value of \(n\). Justify your answer.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(W = \{[x \ 0]^{\tr} : x \text{ is a scalar } \}\)%
\item{}\(W = \{[2x+y \ x-y \ x+y]^{\tr} : x,y \text{ are scalars } \}\)%
\item{}\(W = \{[x+1 \ x-1]^{\tr} : x \text{ is a scalar } \}\)%
\item{}\(W = \{[xy \ xz \ yz ]^{\tr} : x,y,z \text{ are scalars } \}\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp13600152}%
Find a subset of \(\R^2\) that is closed under addition and scalar multiplication, but that does not contain the zero vector, or explain why no such subset exists.%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp13596312}%
Let \(\vv\) be a vector in \(\R^2\). What is the smallest subspace of \(\R^2\) that contains \(\vv\)? Explain. Describe this space geometrically.%
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp13610520}%
What is the smallest subspace of \(\R^2\) containing the first quadrant? Justify your answer.%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp13606424}%
Let \(\vu\), \(\vv\), and \(\vw\) be vectors in \(\R^3\) with \(\vw = \vu+\vv\). Let \(W_1 = \Span\{\vu,\vv\}\) and \(W_2 = \Span\{\vu,\vv,\vw\}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}If \(\vx\) is in \(W_1\), must \(\vx\) be in \(W_2\)? Explain.%
\item{}If \(\vy\) is in \(W_2\), must \(\vy\) be in \(W_1\)? Explain.%
\item{}What is the relationship between \(\Span\{\vu,\vv\}\) and \(\Span\{\vu,\vv,\vw\}\)? Be specific.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp13776008}%
Let \(m\) and \(n\) be positive integers, and let \(\vv\) be in \(\R^n\). Let \(W = \{A\vv : A \in \M_{m \times n}\}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}As an example, let \(\vv = [2 \ 1]^{\tr}\) in \(\R^2\) with \(W = \{A\vv : A \in \M_{2 \times 2}\}\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Show that the vector \([2 \ 1]^{\tr}\) is in \(W\) by finding a matrix \(A\) that places \([2 \ 1]^{\tr}\) in \(W\).%
\item{}Show that the the vector \([4 \ 2]^{\tr}\) is in \(W\) by finding a matrix \(A\) that places \([4 \ 2]^{\tr}\) in \(W\).%
\item{}Show that the vector \([6 \ -1]^{\tr}\) is in \(W\) by finding a matrix \(A\) that places \([6 \ -1]^{\tr}\) in \(W\).%
\item{}Show that \(W = \R^2\).%
\end{enumerate}
\item{}Show that, regardless of the vector \(\vv\) selected, \(W\) is a subspace of \(\R^m\).%
\item{}Characterize all of the possibilities for what the subspace \(W\) can be.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp13791240}{}\quad{}There is more than one possibility.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp13797128}%
Let \(S_1\) and \(S_2\) be subsets of \(\R^3\) such that \(\Span \ S_1 = \Span \ S_2\). Must it be the case that \(S_1\) and \(S_2\) contain at least one vector in common? Justify your answer.%
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp13798024}%
Assume \(W_1\) and \(W_2\) are two subspaces of \(\R^n\). Is \(W_1 \cap W_2\) also a subspace of \(\R^n\)? Is \(W_1 \cup W_2\) also a subspace of \(\R^n\)? Justify your answer. (Note: The notation \(W_1 \cap W_2\) refers to the vectors common to both \(W_1, W_2\), while the notation \(W_1 \cup W_2\) refers to the vectors that are in at least one of \(W_1, W_2\).)%
\end{divisionexercise}%
\begin{divisionexercise}{10}{}{}{g:exercise:idp13808904}%
Determine whether the plane defined by the equation \(5x+3y-2z=0\) is a subspace in \(\R^3\).%
\end{divisionexercise}%
\begin{divisionexercise}{11}{}{}{g:exercise:idp13807496}%
If \(W\) is a subspace of \(\R^n\) and \(\vu\) is a vector in \(\R^n\) not in \(W\), determine whether%
\begin{equation*}
\vu+W = \{\vu+\vv: \vv \text{ is a vector in } W\}
\end{equation*}
is a subspace of \(\R^n\).%
\end{divisionexercise}%
\begin{divisionexercise}{12}{}{}{g:exercise:idp13809416}%
Two students are talking about examples of subspaces. \begin{quote}%
Student 1: The \(x\)-axis in \(\R^2\) is a subspace. It is generated by the vector \(\left[ \begin{array}{c} 1\\0 \end{array} \right]\).\end{quote}
 \begin{quote}%
Student 2: Similarly \(\R^2\) is a subspace of \(\R^3\).\end{quote}
 \begin{quote}%
Student 1: I'm not sure if that will work. Can we fit \(\R^2\) inside \(\R^3\)? Don't we need \(W\) to be a subset of \(\R^3\) if it is a subspace of \(\R^3\)?\end{quote}
 \begin{quote}%
Student 2: Of course we can fit \(\R^2\) inside \(\R^3\). We can think of \(\R^2\) as vectors \(\left[ \begin{array}{c} a\\b\\0 \end{array} \right]\). That's the \(xy\)-plane.\end{quote}
 \begin{quote}%
Student 1: I don't know. The vector \(\left[ \begin{array}{c} a\\b\\0 \end{array} \right]\) is not exactly same as \(\left[ \begin{array}{c} a\\b \end{array} \right]\).\end{quote}
 \begin{quote}%
Student 2: Well, \(\R^2\) is a plane and so is the \(xy\)-plane. So they must be equal, shouldn't they?\end{quote}
 \begin{quote}%
Student 1: But there are infinitely many planes in \(\R^3\). They can't all be equal to \(\R^2\). They all ``look like''\(\R^2\) but I don't think we can say they are equal.\end{quote}
 Which student is correct? Is \(\R^2\) a subspace of \(\R^3\), or not? Justify your answer.%
\end{divisionexercise}%
\begin{divisionexercise}{13}{}{}{x:exercise:ex_3_a_sum}%
\index{subspace!sum}%
Given two subspaces \(H_1, H_2\) of \(\R^n\), define%
\begin{equation*}
H_1+H_2 = \{ \vw \mid \vw=\vu+\vv \text{ where }  \vu \text{ in } H_1, \vv \text{ in } H_2\} \,\text{.}
\end{equation*}
Show that \(H_1+H_2\) is a subspace of \(\R^n\) containing both \(H_1, H_2\) as subspaces. The space \(H_1+H_2\) is the sum of the subspaces \(H_1\) and \(H_2\).%
\end{divisionexercise}%
\begin{divisionexercise}{14}{}{}{g:exercise:idp13832712}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
Any line in \(\R^n\) is a subspace in \(\R^n\).%
\item{}\lititle{True\slash{}False.}\par%
Any line through the origin in \(\R^n\) is a subspace in \(\R^n\).%
\item{}\lititle{True\slash{}False.}\par%
Any plane through the origin in \(\R^n\) is a subspace in \(\R^n\).%
\item{}\lititle{True\slash{}False.}\par%
In \(\R^4\), the points satisfying \(xy=2t+z\) form a subspace.%
\item{}\lititle{True\slash{}False.}\par%
In \(\R^4\), the points satisfying \(x+3y=2z\) form a subspace.%
\item{}\lititle{True\slash{}False.}\par%
Any two nonzero vectors generate a plane subspace in \(\R^3\).%
\item{}\lititle{True\slash{}False.}\par%
The space \(\R^2\) is a subspace of \(\R^3\).%
\item{}\lititle{True\slash{}False.}\par%
If \(W\) is a subspace of \(\R^n\) and \(\vu\) is in \(W\), then the line through the origin and \(\vu\) is in \(W\).%
\item{}\lititle{True\slash{}False.}\par%
There are four types of subspaces in \(\R^3\): \(\{\vzero\}\), line through origin, plane through origin and the whole space \(\R^3\).%
\item{}\lititle{True\slash{}False.}\par%
There are four types of subspaces in \(\R^4\): \(\{\vzero\}\), line through origin, plane through origin and the whole space \(\R^4\).%
\item{}\lititle{True\slash{}False.}\par%
The vectors \(\left[ \begin{array}{c} 1\\1\\1 \end{array} \right]\), \(\left[ \begin{array}{c} 1\\2\\1 \end{array} \right]\) and \(\left[ \begin{array}{c} 2\\3\\2 \end{array} \right]\) form a subspace in \(\R^3\).%
\item{}\lititle{True\slash{}False.}\par%
The vectors \(\left[ \begin{array}{c} 1\\1\\1 \end{array} \right]\) and \(\left[ \begin{array}{c} 1\\2\\1 \end{array} \right]\) form a basis of a subspace in \(\R^3\).%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Least Squares Linear Approximation}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Least Squares Linear Approximation}{}{Project: Least Squares Linear Approximation}{}{}{x:section:sec_proj_ls_approx}
We return to the problem of finding the least squares line to fit the GDP-consumption data. We will start our work in a more general setting, determining the  method for fitting a linear function to fit any data set, like the GDP-consumption data, in the least squares sense. Then we will apply our result to the GDP-consumption data.%
\begin{project}{}{x:project:act_ls_no_line}%
Suppose we want to fit a linear function \(p(x) = mx+b\) to our data. For the sake of our argument, let us assume the general case where we have \(n\)data points labeled as \((x_1,y_1)\), \((x_2, y_2)\), \((x_3, y_3)\),  \(\ldots\), \((x_n, y_n)\). (In the GDP-consumption data \(n = 21\).) In the unlikely event that the graph of \(p(x)\) actually passes through these data points, then we would have the system of equations%
\begin{align}
y_1 \amp = b + mx_1\notag\\
y_2 \amp = b + mx_2\notag\\
y_3 \amp = b + mx_3 \label{x:mrow:eq_LS_system}\\
\vdots \amp \hspace{0.39in} \vdots \notag\\
y_n \amp = b + mx_n \notag\notag
\end{align}
in the unknowns \(b\) and \(m\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}As a small example to illustrate, write the system (\textbackslash{}ref\textbraceleft{}eq:LS\textunderscore{}system\textbraceright{}) using the threepoints \((x_1,y_1)= (1,2)\), \((x_2,y_2) = (3,4)\), and \((x_3,y_3) = (5,6)\). Identify the unknowns and then write this system in the form \(M \va = \vy\). Explicitly identify thematrix \(M\) and the the vectors \(\va\) and \(\vy\).%
\item{}Identify the specific matrix \(M\) and the specific vectors \(\va\) and \(\vy\) using the data inTable \textbackslash{}ref\textbraceleft{}T:GDP\textunderscore{}consumption\textbraceright{}. Explain why the system \(M \va = \vy\) is inconsistent.(Remember, we are treating consumption as the independent variable and GDP as the dependentvariable.) What does the result tell us about the data?%
\end{enumerate}
\end{project}%
\hyperref[x:project:act_ls_no_line]{Project Activity~{\xreffont\ref{x:project:act_ls_no_line}}} shows that the GDP-consumption data does not lie on a line. So instead of attempting to find coefficients \(b\) and \(m\) that give a solution to this system, which may be impossible, we instead look for a vector \(\va^*\) that provides us with something that is ``close'' to a solution.%
\par
If we could find \(b\) and \(m\) that give a solution to the system \(M\va = \vy\), then \(M\va - \vy\) would be zero. If we can't make \(M\va - \vy\) exactly equal to the vector \(\vzero\), we could instead try to minimize \(M\va-\vy\) in some way. One way is to minimize the length \(||M\va-\vy||\) of the vector \(M\va - \vy\).%
\par
If we minimize the quantity \(||M\va-\vy||\), then we will have minimized a function given by a sum of squares. That is, \(||M\va-\vy||\) is calculated to be%
\begin{equation}
\sqrt{(b+mx_1-y_1)^2 + (b+mx_2-y_2)^2 + \cdots + (b+mx_n-y_n)^2}\text{.}\label{x:men:eq_ls_equation}
\end{equation}
This is why the method we will derive is called the method of least squares. This method provides us with a vector ``solution'' in a subspace that is related to \(M\). We can visualize \(||M \va - \vy||\) as in \hyperref[x:figure:F_GDP_error]{Figure~{\xreffont\ref{x:figure:F_GDP_error}}}. In this figure the data points are shown along with a linear approximation (not the best for illustrative purposes). The lengths of the vertical line segments are the summands \((b+mx_i-y_i)\) in \hyperref[x:men:eq_ls_equation]{({\xreffont\ref{x:men:eq_ls_equation}})}. So we are trying to minimize the sum of the squares of these line segments.%
\begin{figureptx}{Error in the linear approximation.}{x:figure:F_GDP_error}{}%
\begin{image}{0.25}{0.5}{0.25}%
\includegraphics[width=\linewidth]{external/GDP_error.pdf}
\end{image}%
\tcblower
\end{figureptx}%
Suppose that \(\va^*\) minimizes \(||M \va - \vy||\). Then the vector \(M \va^*\) is the vector that is closest to \(\vy\) of all of the vectors of the form \(M \vx\). The fact that the vectors of the form \(M \vx\) make a subspace will be useful in what follows. We verify that fact in the next project activity.%
\begin{project}{}{x:project:act_ls_subspace}%
Let \(A\) be an arbitrary \(m \times k\) matrix. Explain why the set \(C = \{A \vx : \vx \in \R^k\}\) is a subspace of \(\R^m\).%
\end{project}%
\hyperref[x:project:act_ls_subspace]{Project Activity~{\xreffont\ref{x:project:act_ls_subspace}}} shows us that even though the GDP-consumption system \(M \va = \vy\) does not have a solution, we can find a vector that is close to a solution in the subspace \(\{M \vx : \vx \in \R^2\}\). That is, find a vector \(\va^*\) in \(\R^{2}\) such that \(M \va^*\) is as close (in the least squares sense) to \(\vy\) as we can get. In other words, the error \(||M \va^* - \vy||\) is as small as possible. In the following activity we see how to find \(\va^*\).%
\begin{project}{}{x:project:act_ls_minimum}%
Let%
\begin{equation*}
S = \sqrt{(b+mx_1-y_1)^2 + (b+mx_2-y_2)^2 + \cdots + (b+mx_n-y_n)^2}\text{,}
\end{equation*}
the quantity we want to minimize. The variables in \(S\) are \(m\) and \(b\), so we can think of \(S\) as a function of the two independent variables \(m\) and \(b\). The square root makes calculations more complicated, so it is helpful to notice that \(S\) will be a minimum when \(S^2\) is a minimum. Since \(S^2\) is also function of the two variables \(b\) and \(m\), the minimum value of \(S^2\) will occur when the partial derivatives of \(S^2\) with respect to \(b\) and \(m\) are both \(0\) (if you haven't yet taken a multivariable calculus course, you can just assume that this is correct). This yields the equations%
\begin{align}
0 \amp = \sum_{i=1}^n \left( mx_i + b - y_i\right)x_i\label{x:mrow:eq_ls_1}\\
0 \amp = \sum_{i=1}^n \left( mx_i+b-y_i\right)\text{.}\label{x:mrow:eq_ls_2}
\end{align}
In this activity we solve equations \hyperref[x:mrow:eq_ls_1]{({\xreffont\ref{x:mrow:eq_ls_1}})} and \hyperref[x:mrow:eq_ls_2]{({\xreffont\ref{x:mrow:eq_ls_2}})} for the unknowns \(b\) and \(m\). (Do this in a general setting without using specific values for the \(x_i\) and \(y_i\).)%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(r=\sum_{i=1}^n x_i^2\), \(s=\sum_{i=1}^n x_i\), \(t = \sum_{i=1}^n y_i\), and \(u =\sum_{i=1}^n x_iy_i\). Show that the equations \hyperref[x:mrow:eq_ls_1]{({\xreffont\ref{x:mrow:eq_ls_1}})} and \hyperref[x:mrow:eq_ls_2]{({\xreffont\ref{x:mrow:eq_ls_2}})} can be written in the form%
\begin{align*}
0 \amp = bs + mr -u\\
0 \amp = bn + ms - t\text{.}
\end{align*}
Note that this is a system of two linear equations in the unknowns \(b\) and \(m\).%
\item{}Write the system from part (a) in matrix form \(A \vx = \vb\). Then use techniques from linear algebra to solve the linear system to show that%
\begin{equation}
b = \frac{tr-us}{nr-s^2} = \frac{\left(\sum_{i=1}^n y_i\right) \left(\sum_{i=1}^n x_i^2\right) - \left(\sum_{i=1}^n x_i \right) \left(\sum_{i=1}^n x_iy_i \right)}{n\left(\sum_{i=1}^n x_i^2 \right) - \left(\sum_{i=1}^n x_i \right)^2}\label{x:men:eq_ls_b}
\end{equation}
and%
\begin{equation}
m = \frac{nu-ts}{nr-s^2} =  \frac{n\left(\sum_{i=1}^n x_iy_i\right) - \left(\sum_{i=1}^n x_i \right) \left(\sum_{i=1}^n y_i \right)}{n\left(\sum_{i=1}^n x_i^2 \right) - \left(\sum_{i=1}^n x_i \right)^2}.\label{x:men:eq_ls_m}
\end{equation}
%
\end{enumerate}
\end{project}%
\begin{project}{}{g:project:idp13901832}%
Use the formulas \hyperref[x:men:eq_ls_b]{({\xreffont\ref{x:men:eq_ls_b}})} and \hyperref[x:men:eq_ls_m]{({\xreffont\ref{x:men:eq_ls_m}})} to find the values of \(b\) and \(m\) for the regression line to fit the GDP-consumption data in \hyperref[x:table:T_GDP_consumption]{Table~{\xreffont\ref{x:table:T_GDP_consumption}}}. You may use the fact that the sum of the GDP data is \(3.5164030 \times 10^6\), the sum of the consumption data is \(2.9233750 \times 10^6\), the sum of the squares of the consumption data is \(8.806564894 \times 10^{11}\), and the sum of the products of the GDP and consumption data is \(1.069946378 \times 10^{12}\). Compare to the results the authors obtained in the paper  ``A Statistical Analysis of GDP and Final Consumption Using Simple Linear Regression, the Case of Romania 1990-2010''.\end{project}%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 13 The Null Space and Column Space of a Matrix}
\typeout{************************************************}
%
\begin{chapterptx}{The Null Space and Column Space of a Matrix}{}{The Null Space and Column Space of a Matrix}{}{}{x:chapter:chap_null_space}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp13909256}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What is the null space of a matrix?%
\item{}What is the column space of a matrix?%
\item{}What important structure do the null space and column space of a matrix have?%
\item{}What is the kernel of a matrix transformation?%
\item{}How is the kernel of a matrix transformation \(T\) defined by \(T(\vx) = A\vx\) related to the null space of \(A\)?%
\item{}What is the range of a matrix transformation?%
\item{}How is the range of a matrix transformation \(T\) defined by \(T(\vx) = A\vx\) related to the column space of \(A\)?%
\item{}How do we find a basis for \(\Nul A\)?%
\item{}How do we find a basis for \(\Col A\)?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: The Lights Out Game}
\typeout{************************************************}
%
\begin{sectionptx}{Application: The Lights Out Game}{}{Application: The Lights Out Game}{}{}{x:section:sec_appl_lights_out}
Lights Out (LO) is a commercial game released by Tiger Toys in 1995 (later bought out by Hasbro). The game consists of a \(5 \times 5\) grid in which each square is either lit or unlit. Pressing a square changes the status of the square itself and all the squares to the left, right, up, or down. The player's job is to turn all the lights out. You can play a sample game at \href{https://www.geogebra.org/m/wcmctahp}{\nolinkurl{geogebra.org/m/wcmctahp}}. There is a method to solve any solvable Lights Out game that can be uncovered through linear algebra that we will uncover later in this section. Column spaces and null spaces play important roles in this method.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_null_intro}
Recall that a subspace of \(\R^n\) is a subset of \(\R^n\) which is a vector space in itself. More specifically, a subset \(W\) or \(\R^n\) is a subspace of \(\R^n\) if%
\begin{enumerate}
\item{}whenever \(\vu\) and \(\vv\) are in \(W\) it is also true that \(\vu + \vv\) is in \(W\) (that is, \(W\) is \terminology{closed} under addition),%
\item{}whenever \(\vu\) is in \(W\) and \(a\) is a scalar it is also true that \(a\vu\) is in \(W\) (that is, \(W\) is \terminology{closed} under scalar multiplication),%
\item{}\(\vzero\) is in \(W\).%
\end{enumerate}
%
\par
Given a matrix \(A\), there are several subspaces that are connected to \(A\). Two specific such subspaces are the null space of \(A\) and the column space of \(A\). We will see that these subspaces provide answers to the big questions we have been considering since the beginning of the semester, such as ``Do columns of \(A\) span \(\R^m\)?'' ``Are the columns of \(A\) linearly independent?'' ``Is the transformation \(T\) defined by matrix multiplication by \(A\) one-to-one?'' ``Is the transformation \(T\) onto?''%
\par
In this preview activity, we start examining the \terminology{null space}.%
\begin{exploration}{}{x:exploration:pa_3_b}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(A = \left[ \begin{array}{ccc} 2\amp 1\amp 3\\ 1\amp 1\amp 4 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Find the general solution to the homogeneous equation \(A \vx = \vzero\). Write your solutions in parametric vector form. (Recall that the parametric vector form expresses the solutions to an equation as linear combinations of vectors with free variables as the weights. An example would be \(x_3 \left[ \begin{array}{r} 1\\0\\-1\\0 \end{array} \right] + x_4 \left[ \begin{array}{r} -2\\1\\0\\1 \end{array} \right]\).)%
\item{}Find two specific solutions \(\vx_1\) and \(\vx_2\) to the homogeneous equation \(A \vx = \vzero\). Is \(\vx_1 + \vx_2\) a solution to \(A \vx = \vzero\)? Explain.%
\item{}Is \(3\vx_1\) a solution to \(A \vx = \vzero\)? Explain.%
\item{}Is \(\vzero\) a solution to \(A \vx = \vzero\)?%
\item{}What does the above seem to indicate about the set of solutions to the homogeneous system \(A \vx = \vzero\)?%
\end{enumerate}
\item{}Let \(A\) be an \(m \times n\) matrix. As problem 1 implies, the set of solutions to a homogeneous matrix-vector equation \(A \vx = \vzero\) appears to be a subspace. We give a special name to this set. \begin{definition}{}{x:definition:def_null_space}%
\index{null space}The \terminology{null space} of an \(m \times n\) matrix \(A\) is the set of all solutions to \(A \vx = \vzero\).%
\end{definition}
 We denote the null space of a matrix \(A\) as \(\Nul A\). In set notation we write%
\begin{equation*}
\Nul A = \{ \vx : A \vx = \vzero\}\text{.}
\end{equation*}
Note that since \(A\vx=\vzero\) corresponds to a homogeneous system of linear equations, \(\Nul A\) also represents the solution set of a homogeneous system. Let \(A =  \left[ \begin{array}{cccc} 2\amp 1\amp 3\amp 0\\ 1\amp 1\amp 4\amp 1 \end{array}  \right]\). Find all vectors in \(\Nul A\).%
\item{}So far we considered specific examples of null spaces. But what are the properties of a null space in general? Let \(A\) be an \terminology{arbitrary} \(m \times n\) matrix.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}The null space of an \(m \times n\) matrix is a subset of \(\R^k\) for some integer \(k\). What is \(k\)?%
\item{}Now suppose \(\vu\) and \(\vv\) are two vectors in \(\Nul A\). By definition, that means \(A\vu=\vzero\), \(A\vv=\vzero\). Use properties of the matrix-vector product to show that \(\vu + \vv\) is also in \(\Nul A\).%
\item{}Now suppose \(\vu\) is a vector in \(\Nul A\) and \(a\) is a scalar. Explain why \(a\vu\) is also in \(\Nul A\).%
\item{}Explain why \(\Nul A\) is a subspace of \(\R^n\).%
\end{enumerate}
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Null Space of a Matrix and the Kernel of a Matrix Transformation}
\typeout{************************************************}
%
\begin{sectionptx}{The Null Space of a Matrix and the Kernel of a Matrix Transformation}{}{The Null Space of a Matrix and the Kernel of a Matrix Transformation}{}{}{x:section:sec_null_kernel}
In this section we explore the \terminology{null space} and see how the null space of a matrix is related to the matrix transformation defined by the matrix.%
\par
Let \(A\) be an \(m \times n\) matrix. In \hyperref[x:exploration:pa_3_b]{Preview Activity~{\xreffont\ref{x:exploration:pa_3_b}}} we defined the null space of a matrix \(A\) (see \hyperref[x:definition:def_null_space]{Definition~{\xreffont\ref{x:definition:def_null_space}}}) as the set of solutions to the matrix equation \(A \vx = \vzero\). Note that the null space of an \(m \times n\) matrix is a subset of \(\R^n\). We saw that the null space of \(A\) is closed under addition and scalar multiplication \textemdash{} that is, if \(\vu\) and \(\vv\) are in \(\Nul A\) and \(a\) and \(b\) are any scalars, then \(\vu + \vv\) and \(a \vu\) are also in \(\Nul A\). Since the zero vector is always in \(\Nul A\), we can conclude that the null space of \(A\) is a subspace of \(\R^n\).%
\par
There is a connection between the null space of a matrix and the matrix transformation it defines. Recall that any \(m \times n\) matrix \(A\) defines a matrix transformation \(T\) from \(\R^n\) to \(\R^m\) by \(T(\vx) = A \vx\). The null space of \(A\) is then the collection of vectors \(\vx\) in \(\R^n\) so that \(T(\vx) = \vzero\). So if \(T\) is a matrix transformation from \(\R^n\) to \(\R^m\), then the set%
\begin{equation*}
\{\vx \text{ in }  \R^n : T(\vx) = \vzero\}
\end{equation*}
is a subspace of \(\R^n\) equal to the null space of \(A\). This set is is given a special name.%
\begin{definition}{}{g:definition:idp13987464}%
Let \(T : \R^n \to \R^n\) be a matrix transformation. The \terminology{kernel} \index{kernel} of \(T\) is the set%
\begin{equation*}
\Ker(T) = \{\vx \in \R^n : T(\vx) = \vzero\}\text{.}
\end{equation*}
%
\end{definition}
\begin{activity}{}{x:activity:act_3_b_1}%
If \(T\) is a matrix transformation defined by a matrix \(A\), then there is a convenient way to determine if \(T\) is one-to-one.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(T\) be the matrix transformation defined by \(T(\vx) = A\vx\), where%
\begin{equation*}
A = \left[ \begin{array}{ccr} 1\amp 2\amp -1 \\ 0\amp 1\amp 4 \end{array}  \right]\text{.}
\end{equation*}
Find all of the vectors in \(\Nul A\). If \(\Nul A\) contains more than one vector, can \(T\) be one-to-one? Why?%
\item{}Let \(T\) be the matrix transformation defined by \(T(\vx) = A\vx\), where%
\begin{equation*}
A = \left[ \begin{array}{rc} 1\amp 0 \\ 2\amp 1 \\ -1\amp 4 \end{array}  \right]\text{.}
\end{equation*}
Find all of the vectors in \(\Nul A\). Is \(T\) one-to-one? Why?%
\item{}To find the vectors in the null space of a matrix \(A\) we solve the system \(A \vx = \vzero\). Since a homogeneous system is always consistent, there are two possibilities for \(\Nul A\): either \(\Nul A = \{\vzero\}\) or \(\Nul A\) contains infinitely many vectors.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Under what conditions on \(A\) is \(\Nul A = \{\vzero\}\)? What does that mean about \(T\) being one-to-one or not? Explain.%
\item{}Under what conditions is \(\Nul A\) infinite? What does that mean about \(T\) being one-to-one or not? Explain.%
\item{}Is is possible for \(\Nul A\) to be the whole space \(\R^n\)? If so, give an example. If not, explain why not.%
\end{enumerate}
\end{enumerate}
\end{activity}%
Recall that for a function to be one-to-one, each output must come from exactly one input. Since a matrix transformation \(T\) defined by \(T(\vx) = A \vx\) always maps the zero vector to the zero vector, for \(T\) to be one-to-one it must be the case that the zero vector is the only vector that \(T\) maps to the zero vector. This means that the null space of \(A\) must be \(\{\vzero\}\). \hyperref[x:activity:act_3_b_1]{Activity~{\xreffont\ref{x:activity:act_3_b_1}}} demonstrates that if the matrix \(A\) that defines the transformation \(T\) has a pivot in every column, then \(T(\vx) = \vb\) will have exactly one solution for each \(\vb\) in the range of \(T\). So a trivial null space is enough to characterize a one-to-one matrix transformation.%
\begin{theorem}{}{}{x:theorem:thm_3_b_one_to_one_kernel}%
A matrix transformation \(T\) from \(\R^n\) to \(\R^m\) defined by \(T(\vx) = A\vx\) is one-to-one if and only if%
\begin{equation*}
\Nul A = \Ker(T) = \{\vzero\}\text{.}
\end{equation*}
%
\end{theorem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Column Space of a Matrix and the Range of a Matrix Transformation}
\typeout{************************************************}
%
\begin{sectionptx}{The Column Space of a Matrix and the Range of a Matrix Transformation}{}{The Column Space of a Matrix and the Range of a Matrix Transformation}{}{}{x:section:sec_column_range}
Given an \(m \times n\) matrix \(A\), we have seen that the matrix-vector product \(A \vx\) is a linear combination of the columns of \(A\) with weights from \(\vx\). It follows that the equation \(A \vx = \vb\) has a solution if and only if \(\vb\) is a linear combination of the columns of \(A\). So the span of the columns of \(A\) tells us for which vectors the equation \(A \vx = \vb\) is consistent. We give the span of the columns of a matrix \(A\) a special name.%
\begin{definition}{}{g:definition:idp14008712}%
\index{column space}%
The \terminology{column space} of an \(m \times n\) matrix \(A\) is the span of the columns of \(A\).%
\end{definition}
We denote the column space of \(A\) as \(\Col A\). Given that \(A \vx\) is a linear combination of the columns of \(A\), we can also write the column space of an \(m \times n\) matrix \(A\) as%
\begin{equation*}
\Col A = \{A \vx : \vx \text{ is in }  \R^n\}\text{.}
\end{equation*}
%
\par
For the matrix transformation \(T\) defined by \(T(\vx) = A\vx\), the set of all vectors of the form \(A \vx\) is also the range of the transformation \(T\). So for a matrix transformation \(T\) with matrix \(A\) we have \(\Range(T) = \Col A\).%
\begin{activity}{}{x:activity:act_3_b_2}%
As a span of a set of vectors, we know that \(\Col A\) is a subspace of \(\R^k\) for an appropriate value of \(k\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(M = \left[ \begin{array}{ccccr} 1\amp 1\amp 1\amp 0\amp 2 \\ 0\amp 1\amp 0\amp 1\amp 1 \\ 1\amp 0\amp 1\amp 1\amp -1 \\ 0\amp 1\amp 0\amp 1\amp 1 \end{array} \right]\). The space \(\Col M\) is a subspace of \(\R^k\) for some positive integer \(k\). What is \(k\) in this case?%
\item{}If \(A\) is an \(m \times n\) matrix, then \(\Col A\) is a subspace of \(\R^k\) for some positive integer \(k\). What is \(k\) in this case?%
\item{}Recall that a matrix transformation \(T\) given by \(T(\vx)=A\vx\) where \(A\) is an \(m\times n\) matrix is onto if for every \(\vb\) in \(\R^m\) there exists a \(\vx\) in \(\R^n\) for which \(T(\vx)=\vb\). How does \(T\) being onto relate to the \(\Col A\)?%
\end{enumerate}
\end{activity}%
As you saw in \hyperref[x:activity:act_3_b_2]{Activity~{\xreffont\ref{x:activity:act_3_b_2}}}, a matrix transformation \(T\) defined by \(T(\vx)=A\vx\) is onto if the column space of \(A\), which consists of the image vectors under the transformation \(T\), is equal to \(\R^m\). In other words, we want the \(\Range(T)\) to equal \(\R^m\).%
\begin{theorem}{}{}{x:theorem:thm_3_b_onto_range}%
A matrix transformation \(T\) from \(\R^n\) to \(\R^m\) defined by \(T(\vx) = A\vx\) is onto if and only if%
\begin{equation*}
\Col A = \Range(T) = \R^m\text{.}
\end{equation*}
%
\end{theorem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Row Space of a Matrix}
\typeout{************************************************}
%
\begin{sectionptx}{The Row Space of a Matrix}{}{The Row Space of a Matrix}{}{}{x:section:sec_row_space}
As you might expect, if there is a column space for a matrix then there is also a row space for a matrix. The row space is defined just as the column space as the span of the rows of a matrix.%
\begin{definition}{}{g:definition:idp14054168}%
\index{row space}%
The \terminology{row space} of an \(m \times n\) matrix \(A\) is the span of the row of \(A\).%
\end{definition}
There is really nothing new here, though. Since the rows of \(A\) are the columns of \(A^{\tr}\), it follows that \(\Row A = \Col A^{\tr}\). So if we want to learn anything about the row space of \(A\), we can just translate all of our questions to the column space of \(A^{\tr}\).%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Bases for \(\Nul A\) and \(\Col A\)}
\typeout{************************************************}
%
\begin{sectionptx}{Bases for \(\Nul A\) and \(\Col A\)}{}{Bases for \(\Nul A\) and \(\Col A\)}{}{}{x:section:sec_null_col_base}
When confronted with a subspace of \(\R^n\), we will usually want to find a minimal spanning set \textemdash{} a smallest spanning set \textemdash{} for the space. Recall that a minimal spanning set is also called a basis for the space. So a basis for a space must span that space, and to be a minimal spanning set we have seen that a basis must also be linearly independent. So to prove that a set is a basis for a subspace of \(\R^n\) we need to demonstrate two things: that the set is linearly independent, and that the set spans the subspace.%
\begin{activity}{}{x:activity:act_3_b_3}%
In this activity we see how to find a basis for \(\Col A\) and \(\Nul A\) for a specific matrix \(A\). Let%
\begin{equation*}
A =  \left[ \begin{array}{rrrrr} 1\amp 1\amp 1\amp 0\amp 2 \\ 0\amp 1\amp 0\amp 1\amp 1 \\ 1\amp 0\amp 1\amp 1\amp -1 \\ 0\amp 1\amp 0\amp 1\amp 1 \end{array}  \right]\text{.}
\end{equation*}
%
\par
Assume that the reduced row echelon form of \(A\) is%
\begin{equation*}
R= \left[ \begin{array}{rrrrr} 1\amp 0\amp 1\amp 0\amp 0 \\ 0\amp 1\amp 0\amp 0\amp 2 \\ 0\amp 0\amp 0\amp 1\amp -1 \\ 0\amp 0\amp 0\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}First we examine \(\Col A\). Recall that to find a minimal spanning set of a set of vectors \(\{\vv_1, \vv_2, \ldots, \vv_k\}\) in \(\R^n\) we just select the pivot columns of the matrix \([\vv_1 \ \vv_2 \ \cdots \ \vv_k]\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Find a basis for \(\Col A\).%
\item{}Does \(\Col A\) equal \(\Col R\)? Explain.%
\end{enumerate}
\item{}Now we look at \(\Nul A\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Write the general solution to the homogeneous system \(A \vx = \vzero\) in vector form.%
\item{}Find a spanning set for \(\Nul A\).%
\item{}Find a basis for \(\Nul A\). Explain how you know you have a basis.%
\end{enumerate}
\end{enumerate}
\end{activity}%
You should have noticed that \hyperref[x:activity:act_3_b_3]{Activity~{\xreffont\ref{x:activity:act_3_b_3}}} (a) provides a process for finding a basis for \(\Col A\) \textemdash{} the pivot columns of \(A\) form a basis for \(\Col A\). Similarly, \hyperref[x:activity:act_3_b_3]{Activity~{\xreffont\ref{x:activity:act_3_b_3}}} (b) shows us that we can find a basis for \(\Nul A\) by writing the general solution to the homogeneous equation \(A\vx = \vzero\) as a linear combination of vectors whose weights are the variables corresponding to the non-pivot columns of \(A\) \textemdash{} and these vectors form a basis for \(\Nul A\). As we will argue next, these process always give us bases for \(\Col A\) and \(\Nul A\).%
\par
Let \(A\) be an \(m \times n\) matrix, and let \(R\) be the reduced row echelon form of \(A\). Suppose \(R\) has \(k\) non-pivot columns and \(n-k\) pivot columns. We can rearrange the columns so that the non-pivot columns of \(R\) are the last \(k\) columns (this just amounts to relabeling the unknowns in the system).%
\begin{paragraphs}{Basis for \(\Nul A\).}{g:paragraphs:idp14084120}%
Here we argue that the method described following \hyperref[x:activity:act_3_b_3]{Activity~{\xreffont\ref{x:activity:act_3_b_3}}} to find a spanning set for the null space always yields a basis for the null space. First note that \(\Nul R = \Nul A\), since the system \(A\vx = \vzero\) has the same solution set as \(R \vx = \vzero\). So it is enough to find a basis for \(\Nul R\). If every column of \(R\) is a pivot column, then \(R \vx = \vzero\) has only the trivial solution and the null space of \(R\) is \(\{ \vzero \}\). Let us now consider the case where \(R\) contains non-pivot columns. If we let \(\vx = [x_1 \ x_2 \ \ldots \ x_n]^{\tr}\), and if \(R\vx = \vzero\) then we can write \(x_1\), \(x_2\), \(\ldots\), \(x_{n-k}\) in terms of \(x_{n-k+1}\), \(x_{n-k+2}\), \(\ldots\), and \(x_n\). From these equations we can write \(\vx\) as a linear combination of some vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) with \(x_{n-k+1}\), \(x_{n-2+2}\), \(\ldots\), \(x_n\) as weights. By construction, each of the vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) has a component that is 1 with the corresponding component as 0 in all the other \(\vv_i\). Therefore, the vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) are linearly independent and span \(\Nul R\) (and \(\Nul A\)). In other words, the method we have developed to find the general solution to \(A \vx = \vzero\) always produces a basis for \(\Nul A\).%
\end{paragraphs}%
\begin{paragraphs}{Basis for \(\Col A\).}{g:paragraphs:idp14099992}%
Here we explain why the pivot columns of \(A\) form a basis for \(\Col A\). Recall that the product \(A \vx\) expresses a linear combination of the columns of \(A\) with weights from \(\vx\), and every such linear combination is matched with a product \(R \vx\) giving a linear combination of the columns of \(R\) \emph{using the same weights}. So if a set of columns of \(R\) is linearly independent (or dependent), then the set of corresponding columns in \(A\) is linearly independent (or dependent) and vice versa. Since each pivot column of \(R\) is a vector with 1 in one entry (a different entry for different pivot columns) and zeros elsewhere, the pivot columns of \(R\) are clearly linearly independent. It follows that the pivot columns of \(A\) are linearly independent. All that remains is to explain why the pivot columns of \(A\) span \(\Col A\). Let \(\vr_1\), \(\vr_2\), \(\ldots\), \(\vr_n\) be the columns of \(R\) so that \(R~=~[\vr_1 \ \ \vr_2 \ \ \cdots \ \ \vr_n]\), and let \(\va_1\), \(\va_2\), \(\ldots\), \(\va_n\) be the columns of \(A\) so that \(A~=~[\va_1 \ \ \va_2 \ \ \cdots \ \ \va_n]\). Suppose \(\va_i\) is a non-pivot column for \(A\) and \(\vr_i\) the corresponding non-pivot column in \(R\). Each pivot column is composed of a single 1 with the rest of its entries 0. Also, if a non-pivot column contains a nonzero entry, then there is a corresponding pivot column that contains a 1 in the corresponding position. So \(\vr_i\) is a linear combination of \(\vr_1\), \(\vr_2\), \(\ldots\), \(\vr_{n-k}\) \textemdash{} the pivot columns of \(R\). Thus,%
\begin{equation*}
\vr_i = c_1 \vr_1 + c_2\vr_2 + \cdots + c_{n-k}\vr_{n-k}
\end{equation*}
for some scalars \(c_1\), \(c_2\), \(\ldots\), \(c_{n-k}\). Let \(\vx = [c_1 \ c_2 \ \cdots \ c_{n-k} \ 0 \ \cdots 0 \ -1 \ 0 \ \cdots \ 0]^{\tr}\), where the \(-1\) is in position \(i\). Then \(R \vx = \vzero\) and so \(A \vx = \vzero\). Thus,%
\begin{equation*}
\va_i = c_1 \va_1 + c_2\va_2 + \cdots + c_{n-k}\va_{n-k}
\end{equation*}
and \(\va_i\) is a linear combination of the pivot columns of \(A\). So every non-pivot column of \(A\) is in the span of \(A\) and we conclude that the pivot columns of \(A\) form a basis for \(\Col A\).%
\end{paragraphs}%
\begin{paragraphs}{IMPORTANT POINT.}{g:paragraphs:idp14126744}%
It is the pivot columns of \(A\) that form a basis for \(\Col A\), not the pivot columns of the reduced row echelon form \(R\) of \(A\). In general, \(\Col R \neq \Col A\).%
\end{paragraphs}%
\par
We can incorporate the ideas of this section to expand the Invertible Matrix Theorem.%
\begin{theorem}{The Invertible Matrix Theorem.}{}{g:theorem:idp14125080}%
Let \(A\) be an \(n \times n\) matrix. The following statements are equivalent.%
\begin{enumerate}
\item{}The matrix \(A\) is an invertible matrix.%
\item\hypertarget{x:li:item_3_b_trivial_soln}{}The matrix equation \(A \vx = \vzero\) has only the trivial solution.%
\item{}The matrix \(A\) has \(n\) pivot columns.%
\item{}Every row of \(A\) contains a pivot.%
\item{}The columns of \(A\) span \(\R^n\).%
\item{}The matrix \(A\) is row equivalent to the identity matrix \(I_n\).%
\item{}The columns of \(A\) are linearly independent.%
\item{}The columns of \(A\) form a basis for \(\R^n\).%
\item{}The matrix transformation \(T\) from \(\R^n\) to \(\R^n\) defined by \(T(\vx) = A\vx\) is one-to-one.%
\item{}The matrix equation \(A \vx = \vb\) has exactly one solution for each vector \(\vb\) in \(\R^n\).%
\item{}The matrix transformation \(T\) from \(\R^n\) to \(\R^n\) defined by \(T(\vx) = A\vx\) is onto.%
\item\hypertarget{x:li:item_3_b_AC_I}{}There is an \(n \times n\) matrix \(C\) so that \(AC = I_n\).%
\item\hypertarget{x:li:item_3_b_DA_I}{}There is an \(n \times n\) matrix \(D\) so that \(DA = I_n\).%
\item{}The scalar 0 is not an eigenvalue of \(A\).%
\item{}The matrix \(A^{\tr}\) is invertible.%
\item{}\(\Nul A = \{\vzero\}\).%
\item{}\(\Col A = \R^n\).%
\end{enumerate}
%
\end{theorem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_null_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp14149528}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(A = \left[ \begin{array}{rrrr} 1\amp 0\amp -2\amp 3 \\ -2\amp -4\amp 0\amp -14 \\ 1\amp 3\amp 1\amp 9 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Find a basis for \(\Col A\).%
\par
Technology shows that the reduced row echelon form of \(A\) is%
\begin{equation*}
\left[ \begin{array}{ccrc} 1\amp 0\amp -2\amp 3\\ 0\amp 1\amp 1\amp 2 \\ 0\amp 0\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
The first two columns of \(A\) are the pivot columns of \(A\). Since the pivot columns of \(A\) form a basis for \(\Col A\), a basis for \(\Col A\) is%
\begin{equation*}
\left\{ \left[ \begin{array}{r} 1\\ -2 \\ 1 \end{array}  \right], \left[ \begin{array}{r} 0\\ -4 \\ 3 \end{array}  \right] \right\}\text{.}
\end{equation*}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp14154904}{}\quad{}We use \(A = \left[ \begin{array}{rrrr} 1\amp 0\amp -2\amp 3 \\ -2\amp -4\amp 0\amp -14 \\ 1\amp 3\amp 1\amp 9 \end{array}  \right]\).%
\par
Let \(\vv_1 = \left[ \begin{array}{r} 1\\ -2 \\ 1 \end{array} \right]\) and \(\vv_2 = \left[ \begin{array}{r} 0\\ -4 \\ 3 \end{array} \right]\). Since neither \(\vv_1\) nor \(\vv_2\) is a scalar multiple of the other, we see that \(\Col A\) is the span of two linearly independent vectors in \(\R^3\). Thus, we conclude that \(\Col A\) is the plane in \(\R^3\) through the origin and the points \((1,-2,1)\) and \((0,-4,3)\).%
\item{}Describe \(\Col A\) geometrically (e.g., as a line, a plane, a union of lines, etc.) in the appropriate larger space.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp14162328}{}\quad{}%
\end{enumerate}
\item{}Let \(B = \left[ \begin{array}{rrr} 0\amp -2\amp 1\\-1\amp 0\amp 1 \\ 6\amp -10\amp -1 \\ 1\amp -4\amp 1 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Find a basis for \(\Nul B\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp14167960}{}\quad{}We use \(B = \left[ \begin{array}{rrr} 0\amp -2\amp 1\\-1\amp 0\amp 1 \\ 6\amp -10\amp -1 \\ 1\amp -4\amp 1 \end{array}  \right]\).%
\par
Technology shows that the reduced row echelon form of \(B\) is%
\begin{equation*}
\left[  \begin{array}{ccr} 1\amp 0\amp -1\\ 0\amp 1\amp -\frac{1}{2} \\ 0\amp 0\amp 0 \\ 0\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
To find a basis for \(\Nul B\), we must solve the homogeneous equation \(B \vx = \vzero\). If \(\vx = \left[ \begin{array}{c}x_1\\x_2\\x_3 \end{array}  \right]\) and \(B \vx = \vzero\), the reduced row echelon form of \(B\) shows that \(x_3\) is free, \(x_2 = \frac{1}{2}x_3\), and \(x_1 = x_3\). So%
\begin{align*}
\vx \amp = \left[ \begin{array}{c}x_1\\
x_2\\
x_3 \end{array} \right]\\
\amp = \left[  \begin{array}{c}x_3\\
\frac{1}{2}x_3\\
x_3 \end{array} \right]\\
\amp = x_3\left[  \begin{array}{c}1\\
\frac{1}{2}1\\
1 \end{array} \right] \text{.}
\end{align*}
Thus, a basis for \(\Nul B\) is%
\begin{equation*}
\left\{ \left[ \begin{array}{c} 2\\ 1 \\ 2 \end{array}  \right] \right\}\text{.}
\end{equation*}
%
\item{}Describe \(\Nul B\) geometrically (e.g., as a line, a plane, a union of lines, etc.) in the appropriate larger space.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp14175640}{}\quad{}We use \(B = \left[ \begin{array}{rrr} 0\amp -2\amp 1\\-1\amp 0\amp 1 \\ 6\amp -10\amp -1 \\ 1\amp -4\amp 1 \end{array}  \right]\).%
\par
Since \(\Nul B = \Span\left\{ \left[ \begin{array}{c} 2\\ 1 \\ 2 \end{array} \right] \right\}\) is the span of one nonzero vector in \(\R^3\), we conclude that \(\Nul B\) is the line in \(\R^3\) through the origin and the point \((2,1,2)\).%
\end{enumerate}
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp14172824}%
Let \(A = \left[ \begin{array}{rr} 1\amp 3\\-1\amp 2 \\ 0\amp -2\\5\amp 6 \end{array} \right]\), and let \(T\) be the matrix transformation defined by \(T(\vx) = A\vx\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}What are the domain and codomain of \(T\)? Why?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp14182424}{}\quad{}Recall that \(A \vx\) is a linear combination of the columns of \(A\) with weights from \(\vx\). So \(A \vx\) is defined only when the number of components of \(\vx\) is equal to the number of columns of \(A\). This explains why the domain of \(T\) is \(\R^2\). Also, since each output of \(T\) is a linear combination of the columns of \(A\), the codomain of \(T\) is \(\R^4\).%
\item{}Find all vectors \(\vx\) such that \(T(\vx) = \vzero\). How is this set of vectors related to \(\Nul A\)? Explain.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp14190232}{}\quad{}The set of vectors \(\vx\) such that \(\vzero = T(\vx) = A\vx\) is the same as \(\Nul A\). The reduced row echelon form of \(A\) is%
\begin{equation*}
\left[ \begin{array}{cc} 1\amp 0\\0\amp 1\\0\amp 0\\0\amp 0 \end{array} \right]\text{.}
\end{equation*}
Since both columns of \(A\) are pivot columns, the columns of \(A\) are linearly independent. This implies that \(A \vx = \vzero\) has only the trivial solution. Therefore, the only vector \(\vx\) such that \(T(\vx) = \vzero\) is the zero vector in \(\R^2\).%
\item{}Is \(T\) one-to-one? Explain.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp14192792}{}\quad{}The previous part shows that \(\Ker(T) = \{\vzero\}\). This means that \(T\) is one-to-one by \hyperref[x:theorem:thm_3_b_one_to_one_kernel]{Theorem~{\xreffont\ref{x:theorem:thm_3_b_one_to_one_kernel}}}.%
\item{}Is \(T\) onto? If yes, explain why. If no, find a basis for the range of \(T\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp14195096}{}\quad{}Recall that the range of \(T\) is the same as \(\Col A\). The reduced row echelon form of \(A\) has a row of zeros, so \(A \vx = \vb\) is not consistent for every \(\vb\) in \(\R^4\). We conclude that \(T\) is not onto. To find a basis for the range of \(T\), we just need to find a basis for \(\Col A\). The pivot columns of \(A\) form such a basis, so a basis for the range of \(T\) is%
\begin{equation*}
\left\{ \left[ \begin{array}{r} 1\\-1 \\ 0\\5 \end{array}  \right], \left[ \begin{array}{r} 3\\2 \\ -2\\6 \end{array}  \right] \right\}\text{.}
\end{equation*}
%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_null_summ}
%
\begin{itemize}[label=\textbullet]
\item{}The null space of an \(m \times n\) matrix \(A\) is the set of vectors \(\vx\) in \(\R^n\) so that \(A \vx = \vzero\). In set notation%
\begin{equation*}
\Nul A = \{\vx : A \vx = \vzero\}\text{.}
\end{equation*}
%
\item{}The column space of a matrix \(A\) is the span of the columns of \(A\).%
\item{}A subset \(W\) of \(\R^n\) is a subspace of \(\R^n\) if%
\begin{enumerate}
\item{}\(\vu + \vv\) is in \(W\) whenever \(\vu\) and \(\vv\) are in \(W\) (when this property is satisfied we say that \(W\) is \terminology{closed} under addition),%
\item{}\(a \vu\) is in \(W\) whenever \(a\) is a scalar and \(\vu\) is in \(W\) (when this property is satisfied we say that \(W\) is \terminology{closed} under multiplication by scalars),%
\item{}\(\vzero\) is in \(W\).%
\end{enumerate}
%
\item{}The null space of an \(m \times n\) matrix is a subspace of \(\R^n\) while the column space of \(A\) is a subspace of \(\R^m\).%
\item{}The span of any set of vectors in \(\R^n\) is a subspace of \(\R^n\).%
\item{}The kernel of a matrix transformation \(T : \R^n \to \R^m\) is the set%
\begin{equation*}
\Ker(T) = \{\vx \in \R^n : T(\vx) = \vzero\}\text{.}
\end{equation*}
%
\item{}The kernel of a matrix transformation \(T\) defined by \(T(\vx) = A\vx\) is the same set as \(\Nul A\).%
\item{}The range of a matrix transformation \(T : \R^n \to \R^m\) is the set%
\begin{equation*}
\Range(T) = \{T(\vx) : \vx \in \R^n\}\text{.}
\end{equation*}
%
\item{}The range of a matrix transformation \(T\) defined by \(T(\vx) = A\vx\) is the same set as \(\Col A\).%
\item{}A basis for the null space of a matrix \(A\) can be found by writing the general solution to the homogeneous equation \(A \vx = \vzero\) as a linear combination of vectors whose weights are the variables corresponding to the non-pivot columns of \(A\). The number of vectors in a basis for \(\Nul A\) is the number of non-pivot columns of \(A\).%
\item{}The pivot columns of a matrix \(A\) form a basis for the column space of \(A\).%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_null_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp14228248}%
Find a basis for the null space and column space of the matrix%
\begin{equation*}
A=\left[ \begin{array}{cccr} 1 \amp  2\amp  3\amp  4\\ 0 \amp  0 \amp  2 \amp  -2 \\ 1 \amp  2 \amp  5 \amp  2 \end{array}  \right]\text{.}
\end{equation*}
Of which spaces are the null and column spaces of \(A\) subspaces?%
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp14233880}%
If the column space of \(\left[ \begin{array}{ccr} 1\amp 2\amp -1\\1\amp 1\amp 1\\1\amp 2\amp c \end{array} \right]\) has basis \(\left\{ \left[ \begin{array}{c} 1\\1\\1 \end{array} \right], \left[ \begin{array}{c} 2\\1\\2 \end{array} \right]\right\}\), what must \(c\) be?%
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp14237976}%
If the null space of \(\left[ \begin{array}{ccc} 2\amp 1\amp a\\ 1\amp 2\amp b \end{array} \right]\) has basis \(\left\{ \left[ \begin{array}{r} 2\\-1\\1 \end{array} \right]\right\}\), what must \(a\) and \(b\) be?%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp14239128}%
Find a matrix with at least four non-zero and distinct columns for which the column space has basis \(\left\{ \left[ \begin{array}{c} 1\\1\\1 \end{array} \right] , \left[ \begin{array}{c} 2\\2\\3 \end{array} \right] \right\}\).%
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp14236568}%
Find a matrix with at least two rows whose null space has basis \(\left\{ \left[ \begin{array}{r} 1\\1\\-1 \end{array} \right] \right\}\).%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp14236952}%
Find a matrix whose column space has basis \(\left\{ \left[ \begin{array}{c} 1\\1\\1 \end{array} \right] , \left[ \begin{array}{c} 2\\2\\3 \end{array} \right] \right\}\) and whose null space has basis \(\left\{ \left[ \begin{array}{r} 2\\1\\-1 \end{array} \right] \right\}\).%
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp14249368}%
If possible, find a \(4\times 4\) matrix whose column space does not equal \(\R^4\) but whose null space equals \(\{\vzero\}\). Explain your answer. If not possible, explain why not.%
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp14248856}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
For a \(3\times 4\) matrix, the null space contains vectors other than the zero vector.%
\item{}\lititle{True\slash{}False.}\par%
For a \(4\times 3\) matrix, the null space contains vectors other than the zero vector.%
\item{}\lititle{True\slash{}False.}\par%
If Nul \(A\) is not the zero subspace, then the transformation \(\textbf{x} \mapsto A\textbf{x}\) is not one-to-one.%
\item{}\lititle{True\slash{}False.}\par%
If the transformation \(\textbf{x} \mapsto A\textbf{x}\) is onto where \(A\) is an \(m\times n\) matrix, then Col \(A=\R^m\).%
\item{}\lititle{True\slash{}False.}\par%
For a \(4\times 3\) matrix \(A\), Col \(A\) cannot equal \(\R^4\).%
\item{}\lititle{True\slash{}False.}\par%
For a \(3\times 4\) matrix \(A\), Col \(A\) cannot equal \(\R^3\).%
\item{}\lititle{True\slash{}False.}\par%
The null space of the matrix \(\begin{bmatrix}1 \amp 1 \amp 1 \\1 \amp 1 \amp 1 \\ 1 \amp 1 \amp 1 \end{bmatrix}\) consists of the two vectors \([-1 \ 0 \ 1]^{\tr}\) and \([0 \ -1 \ 1]^{\tr}\).%
\item{}\lititle{True\slash{}False.}\par%
A basis for the null space of the matrix \(\begin{bmatrix}1 \amp 1 \amp 1 \\1 \amp 1 \amp 1 \\ 1 \amp 1 \amp 1 \end{bmatrix}\) consists of the two vectors \([-1 \ 0 \ 1]^{\tr}\) and \([0 \ -1 \ 1]^{\tr}\).%
\item{}\lititle{True\slash{}False.}\par%
There does not exist a matrix whose null space equals its column space.%
\item{}\lititle{True\slash{}False.}\par%
The column space of every \(4\times 4\) matrix is \(\R^4\) and its null space is \(\{\vzero\}\).%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Solving the Lights Out Game}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Solving the Lights Out Game}{}{Project: Solving the Lights Out Game}{}{}{x:section:sec_proj_lights_out}
The Lights Out game starts with a \(5 \times 5\) grid on which some of the squares are lit (on) and some are not lit (off). We will call such a state a \terminology{configuration}. Pressing a square that is on turns it off and changes the state of all adjacent (vertically and horizontally) squares, and pressing a square that is off turns it on and changes the state of all adjacent (vertically and horizontally) squares. To model this situation, we consider the number system \(\Z_2 = \{0,1\}\) consisting only of 0 and 1, where 0 represents the off state and 1 the on state. We can also think of 1 as the act of pressing a square and 0 as the act of not pressing \textemdash{} that is,%
\begin{itemize}[label=\textbullet]
\item{}\(0+0 = 0\) (not pressing an off square leaves it off),%
\item{}\(0+1 = 1 = 1 + 0\) (pressing an off square turns it on or not pressing a lit square leaves it lit),%
\item{}\(1+1 = 0\) (pressing a lit square turns it off).%
\end{itemize}
%
\par
The numbers 0 and 1 in \(\Z_2\) will be the only numbers we use when playing the Lights Out game, so all of our matrix entries will be in \(\Z_2\) and all of our calculations are done in \(\Z_2\).%
\par
There are two ways we can view a Lights Out game.%
\begin{itemize}[label=\textbullet]
\item{}We can view each configuration as a \(5 \times 5\) matrix. In this situation, we label the entries in the grid as shown in \hyperref[x:figure:F_LO]{Figure~{\xreffont\ref{x:figure:F_LO}}}. Each entry in the grid will be assigned a 0 or 1 according to whether the light in that entry is off or on.%
\item{}For our purposes a better way to visualize a Lights Out configuration is as a \(25 \times 1\) vector. The components in this vector correspond to the entries in the \(5 \times 5\) grid with the correspondence given by the numbering demonstrated in \hyperref[x:figure:F_LO]{Figure~{\xreffont\ref{x:figure:F_LO}}} (for the sake of space, this array is shown in a row instead of a column). Again, each component is assigned a 0 or 1 according to whether the light for that entry is off or on. In this view, each configuration is a vector with 25 components in \(\Z_2\).%
\end{itemize}
%
\begin{figureptx}{Two representations of the Lights Out game.}{x:figure:F_LO}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/LO.pdf}
\end{image}%
\begin{image}{0.05}{0.9}{0.05}%
\includegraphics[width=\linewidth]{external/LO_vector.pdf}
\end{image}%
\tcblower
\end{figureptx}%
We will take the latter perspective and view the Lights Out game as if it is played on a \(25 \times 1\) board with entries in \(\Z_2\). The space of all of these Lights Out configurations is denoted as \(\Z_2^{25}\) (similar to \(\R^{25}\), but with entries in \(\Z_2\) rather than \(\R\)). Since \(\Z_2\) is a field, the space \(\Z_2^{25}\) is a vector space just as \(\R^{25}\) is. This is the environment in which we will play the Lights Out game.%
\par
If we think about the game as played on a \(25 \times 1\) board, then pressing a square correlates to selecting one of the 25 components of a configuration vector. Each time we press a square, we make a move that changes the status of that square and all the squares vertically or horizontally adjacent to it from the \(5 \times 5\) board. Recalling that adding 1 to a square has the effect of changing its status (from on to off or off to on), and each move that we make in the game can be represented as a \(25 \times 1\) vector that is added to a configuration. For example, the move of pressing the first square is given by adding the vector%
\begin{equation*}
\vm_{1} = [ 1 \ 1 \ 0 \ 0 \ 0 \ 1 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0\  0 \ 0 \ 0 \ 0 \ 0 ]^{\tr}
\end{equation*}
to a configuration vector and the move of pressing the second square is represented by adding the vector%
\begin{equation*}
\vm_{2} = [ 1 \ 1 \ 1 \ 0 \ 0 \ 0 \ 1 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0\  0 \ 0 \ 0 \ 0 \ 0 ]^{\tr}\text{.}
\end{equation*}
%
\begin{project}{}{g:project:idp14289944}%
Let \(\vm_i\) be the move of pressing the \(i\)th square for \(i\) from 1 to 25.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find vector representations for \(\vm_9\) and \(\vm_{22}\).%
\item{}Let \(M = [m_{ij}] = \left[\vm_1 | \vm_2 | \cdots | \vm_{25} \right]\). Explain why \(m_{ij} = m_{ji}\) for each \(i\) and \(j\). In other words, explain why \(M^{\tr} = M\). (Such a matrix is called a \emph{symmetric} matrix.)%
\end{enumerate}
\end{project}%
The goal of the Lights Out game is to begin with an initial configuration \(\vc\) (a vector in \(\Z_2^{25}\)) and determine if we can apply a sequence of moves to obtain the configuration in which all the entries are 0 (or all the lights are off). The vector in \(\Z_2^{25}\) of all 0s is the zero vector in \(\Z_2^{25}\) and we will denote it as \(\vzero\). Some basic algebra of vector addition in \(\Z_2\) (or mod 2) will help us understand the strategy.%
\par
Start with a configuration \(\vc\). If we press the \(i\)th square, then we obtain the new configuration \(\vc_1 = \vm_i + \vc\) (where each move \(\vm_i\) is also in \(\Z_2^{25}\)).%
\begin{project}{}{x:project:act_LO_1}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}What happens if we press the \(i\)th square twice in a row? Explain in terms of the action and the game and verify using vector addition.%
\item{}Explain why applying move \(\vm_i\) then move \(\vm_j\) is the same as applying move \(\vm_j\), then \(\vm_i\).%
\item{}Explain how the answers to the previous two questions show that to play the game we only need to determine which buttons to press (and only once each) without worrying about the order in which the buttons are pressed.%
\end{enumerate}
\end{project}%
What we have seen is that to play the game we are really looking for scalars \(x_1\), \(x_2\), \(\ldots\), \(x_{25}\) in \(\Z_2\) (in other words, either 0 or 1) so that%
\begin{equation}
x_1\vm_1 + x_2\vm_2 + \cdots + x_{25}\vm_{25} + \vc = \vzero\text{.}\label{x:men:eq_LO1}
\end{equation}
%
\begin{project}{}{g:project:idp14303128}%
Explain why \hyperref[x:men:eq_LO1]{({\xreffont\ref{x:men:eq_LO1}})} has the equivalent matrix equation%
\begin{equation}
M\vx = \vc\text{,}\label{x:men:eq_LO2}
\end{equation}
where \(M = \left[\vm_1 | \vm_2 | \cdots | \vm_{25} \right]\), or%
\begin{equation*}
M =  \left[ \begin{array}{c c c c c c c c c c c c c c c c c c c c c c c c c } 1 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \\ 1 \amp  1 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \\ 0 \amp  1 \amp  1 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \\ 0 \amp  0 \amp  1 \amp  1 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \\ 0 \amp  0 \amp  0 \amp  1 \amp  1 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \\ 1 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \\ 0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  1 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \\ 0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  1 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \\ 0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  1 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \\ 0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  1 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \\ 0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \\ 0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  1 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \\ 0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  1 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \\ 0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  1 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \\ 0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  1 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \\ 0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  0 \\ 0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  1 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \\ 0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  1 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \\ 0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  1 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \\ 0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  1 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \\ 0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  1 \amp  0 \amp  0 \amp  0 \\ 0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  1 \amp  1 \amp  0 \amp  0 \\ 0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  1 \amp  1 \amp  0 \\ 0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  1 \amp  1 \\ 0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  0 \amp  0 \amp  0 \amp  1 \amp  1 \end{array} \right]\text{.}
\end{equation*}
%
\par
Explicitly identify the vector \(\vx\). Also, explain why \(\vc\) is on the right side of this equation.%
\end{project}%
To solve a Lights Out game now, all we need do is determine a solution, if one exists, to the matrix equation \hyperref[x:men:eq_LO2]{({\xreffont\ref{x:men:eq_LO2}})}.%
\begin{project}{}{x:project:act_LO_2}%
For this activity you may use the fact that the reduced row echelon form of the matrix \(M\) (using algebra in \(\Z_2\)) is as shown below.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find a basis for the column space of \(M\).%
\item{}Explain why not every Lights Out puzzle can be solved. That is, explain why there are some initial configurations of lights on and off for which it is not possible to turn out all the lights (without turning off the game). Relate this to the column space of \(M\).%
\end{enumerate}
\end{project}%
The reduced row echelon form of the matrix \(M\) (using algebra in \(\Z_2\)):%
\begin{equation*}
\left[ \begin{array}{ccccccccccccccccccccccccc} 1\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1 \\ 0\amp 1\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0 \\ 0\amp 0\amp 1\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 1 \\ 0\amp 0\amp 0\amp 1\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 1\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 1 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 1 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 1 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 1 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 1 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0\amp 0\amp 0\amp 1\amp 1 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0\amp 0\amp 0\amp 1 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0\amp 1\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\amp 1\amp 1 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
%
\par
To find conditions under which a Lights Out game is not solvable, we will demonstrate that if \(A\) is an \(n \times n\) matrix, then the scalar product of any vector in \(\Nul A^{\tr}\) with any column of \(A\) is \(\vzero\). Let \(A = [a_{ij}]\) be an \(n \times n\) matrix with columns \(\va_1\), \(\va_2\), \(\ldots\), \(\va_n\). Represent the entries in the \(i\)th column as \(\va_i = [a_{1i} \ a_{2i} \ \ldots \ a_{ni}]^{\tr}\) for each \(i\) between 1 and \(n\). Note that \(\va_i\) is also the \(i\)th row of \(A^{\tr}\). Also, let \(\vx = [x_1 \ x_2 \ \ldots \ x_n]^{\tr}\) be a vector in \(\Nul A^{\tr}\). Then \(A^{\tr} \vx = \vzero\). Using the row-column method of multiplying a matrix by a vector, when we multiply the \(i\)th row of \(A^{\tr}\) with \(\vx\) we obtain%
\begin{equation}
a_{1i}x_1 + a_{2i}x_2 + \cdots + a_{ni}x_n = 0\text{.}\label{x:men:eq_LO_dp}
\end{equation}
%
\par
This equation is valid for each \(i\) between 1 and \(n\). Recall that the sum in \hyperref[x:men:eq_LO_dp]{({\xreffont\ref{x:men:eq_LO_dp}})} is the scalar product of \(\va_i\) and \(\vx\) and is denoted \(\va_i \cdot \vx\). That is,%
\begin{equation*}
\va_i \cdot \vx = a_{i1}x_1 + a_{i2}x_2 + \cdots + a_{in}x_n\text{.}
\end{equation*}
%
\par
The fact that \(\vx\) is in \(\Nul A^{\tr}\) means \(\va_i \cdot \vx = \vzero\) for every \(i\) between 1 and \(n\). In other words, the scalar product of any vector in \(\Nul A^{\tr}\) with any column of \(A\) is \(\vzero\). (When the scalar product of two vectors is \(\vzero\), we call the vectors \terminology{orthogonal} \textemdash{} a fancy word for ``perpendicular''.) Since scalar products are linear, we can extend this result to the following.%
\begin{theorem}{}{}{x:theorem:thm_LO_Nul_Col}%
Let \(A\) be an \(n \times n\) matrix. If \(\vx\) is any vector in \(\Col A\) and \(\vy\) is any vector in \(\Nul A^{\tr}\), then \(\vx \cdot \vy = 0\).%
\end{theorem}
With \hyperref[x:theorem:thm_LO_Nul_Col]{Theorem~{\xreffont\ref{x:theorem:thm_LO_Nul_Col}}} in mind we can return to our analysis of the Lights Out game, applying this result to the matrix \(M\).%
\begin{project}{}{g:project:idp14401112}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find a basis for the null space of \(M^{\tr}\). (Recall that \(M^{\tr} = M\), so you can use the reduced row echelon form of \(M\) (using algebra in \(\Z_2\)) given earlier.)%
\item{}Use  \hyperref[x:theorem:thm_LO_Nul_Col]{Theorem~{\xreffont\ref{x:theorem:thm_LO_Nul_Col}}} to show that if \(\vc = [c_1 \  c_2 \  \ldots \  c_{25}]^{\tr}\) is an initial Lights Out configuration that is solvable, then \(\vc\) must be orthogonal to each of the vectors in a basis for \(\Nul M^{\tr}\). Then show that if \(\vc\) is a solvable initial Lights Out configuration, \(\vc\) must satisfy%
\begin{align*}
c_2 + c_3 + c_4 + c_6 + c_8 + c_{10} + c_{11} + c_{12} \amp + c_{14} + c_{15} + c_{16} + c_{18}\\
\amp + c_{20} + c_{22} + c_{23} + c_{24} = 0
\end{align*}
and%
\begin{equation*}
c_1 + c_3 + c_5 + c_6 + c_8 + c_{10} + c_{16} + c_{18} + c_{20} + c_{21} + c_{23} + c_{25} = 0\text{.}
\end{equation*}
Be very specific in your explanation.%
\end{enumerate}
\end{project}%
\begin{project}{}{g:project:idp14413528}%
Now that we know which Lights Out games can be solved, let \(\vc\) be an initial configuration to a solvable Lights Out game. Explain how to find a solution to this game. Will the solution be unique? Explain.%
\end{project}%
Now that we have a strategy for solving the Lights Out game, use it to solve random puzzles at \href{https://www.geogebra.org/m/wcmctahp}{\nolinkurl{geogebra.org/m/wcmctahp}}, or create your own game to solve.%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 14 Eigenspaces of a Matrix}
\typeout{************************************************}
%
\begin{chapterptx}{Eigenspaces of a Matrix}{}{Eigenspaces of a Matrix}{}{}{x:chapter:chap_eigenspaces}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp14412888}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What is an eigenspace of a matrix?%
\item{}How do we find a basis for an eigenspace of a matrix?%
\item{}What is true about any set of eigenvectors for a matrix that correspond to different eigenvalues?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Population Dynamics}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Population Dynamics}{}{Application: Population Dynamics}{}{}{x:section:sec_appl_pop_dynam}
The study of population dynamics \textemdash{} how and why people move from one place to another \textemdash{} is important to economists. The movement of people corresponds to the movement of money, and money makes the economy go. As an example, we might consider a simple model of population migration to and from the state of Michigan.%
\par
According to the   Michigan Department of Technology, Management, and Budget, from 2011 to 2012, approximately 0.05\% of the U.S. population outside of Michigan moved to the state of Michigan, while approximately 2\% of Michigan's population moved out of Michigan. A reasonable question to ask about this situation is, if these numbers don't change, what is the long-term distribution of the US population inside and outside of Michigan (under the assumption that the total US population doesn't change.). The answer to this question involves eigenvalues and eigenvectors of a matrix. More details can be found later in this section.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_egspace_intro}
\begin{exploration}{}{x:exploration:pa_3_c_1}%
Consider the matrix transformation \(T\) from \(\R^2\) to \(\R^2\) defined by \(T(\vx) = A \vx\), where%
\begin{equation*}
A = \left[ \begin{array}{cc} 3\amp 1\\1\amp 3 \end{array}  \right]\text{.}
\end{equation*}
%
\par
We are interested in understanding what this matrix transformation does to vectors in \(\R^2\). The matrix \(A\) has eigenvalues \(\lambda_1 = 2\) and \(\lambda_2 = 4\) with corresponding eigenvectors \(\vv_1 = \left[ \begin{array}{r} -1\\1 \end{array}  \right]\) and \(\vv_2 = \left[ \begin{array}{c} 1\\1 \end{array}  \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why \(\vv_1\) and \(\vv_2\) are linearly independent.%
\item{}Explain why any vector \(\vb\) in \(\R^2\) can be written uniquely as a linear combination of \(\vv_1\) and \(\vv_2\).%
\item{}We now consider the action of the matrix transformation \(T\) on a linear combination of \(\vv_1\) and \(\vv_2\). Explain why%
\begin{equation}
T(c_1\vv_1 + c_2 \vv_2) = 2c_1\vv_1 + 4c_2 \vv_2\text{.}\label{x:men:eq_3_c_1}
\end{equation}
%
\end{enumerate}
\end{exploration}%
Equation \hyperref[x:men:eq_3_c_1]{({\xreffont\ref{x:men:eq_3_c_1}})} illustrates that it would be convenient to view the action of \(T\) in the coordinate system where \(\Span\{\vv_1\}\) serves as the \(x\)-axis and \(\Span\{\vv_2\}\) as the \(y\)-axis. In this case, we can visualize that when we apply the transformation \(T\) to a vector \(\vb = c_1 \vv_1 + c_2 \vv_2\) in \(\R^2\) the result is an output vector is scaled by a factor of \(2\) in the \(\vv_1\) direction and by a factor of \(4\) in the \(\vv_2\) direction. For example, consider the box with vertices at \((0,0)\), \(\vv_1\), \(\vv_2\), and \(\vv_1+\vv_2\) as shown at left in \hyperref[x:figure:F_3_c_1]{Figure~{\xreffont\ref{x:figure:F_3_c_1}}}. The transformation \(T\) stretches this box by a factor of \(2\) in the \(\vv_1\) direction and a factor of \(4\) in the \(\vv_2\) direction as illustrated at right in \hyperref[x:figure:F_3_c_1]{Figure~{\xreffont\ref{x:figure:F_3_c_1}}}. In this situation, the eigenvalues and eigenvectors provide the most convenient perspective through which to visualize the action of the transformation \(T\). Here, \(\Span\{\vv_1\}\) and \(\Span\{\vv_2\}\) are the eigenspaces of the matrix \(A\).%
\begin{figureptx}{A box and a transformed box.}{x:figure:F_3_c_1}{}%
\begin{image}{0.25}{0.5}{0.25}%
\includegraphics[width=\linewidth]{external/3_c_Transformation.pdf}
\end{image}%
\tcblower
\end{figureptx}%
This geometric perspective illustrates how each the span of each eigenvalue of \(A\) tells us something important about \(A\). In this section we explore the idea of eigenvalues and spaces defined by eigenvectors in more detail.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Eigenspaces of Matrix}
\typeout{************************************************}
%
\begin{sectionptx}{Eigenspaces of Matrix}{}{Eigenspaces of Matrix}{}{}{x:section:sec_mtx_egspace}
Recall that the eigenvectors of an \(n \times n\) matrix \(A\) satisfy the equation%
\begin{equation*}
A \vx = \lambda \vx
\end{equation*}
for some scalar \(\lambda\). Equivalently, the eigenvectors of \(A\) with eigenvalue \(\lambda\) satisfy the equation%
\begin{equation*}
(A - \lambda I_n) \vx = \vzero\text{.}
\end{equation*}
%
\par
In other words, the eigenvectors for \(A\) with eigenvalue \(\lambda\) are the non-zero vectors in \(\Nul A-\lambda I_n\). Recall that the null space of an \(n \times n\) matrix is a subspace of \(\R^n\). In \hyperref[x:exploration:pa_3_c_1]{Preview Activity~{\xreffont\ref{x:exploration:pa_3_c_1}}} we say how these subspaces provided a convenient coordinate system through which to view a matrix transformation. These special null spaces are called \terminology{eigenspaces}.%
\begin{definition}{}{g:definition:idp14453720}%
\index{eigenspace}%
Let \(A\) be an \(n \times n\) matrix with eigenvalue \(\lambda\). The \terminology{eigenspace} for \(A\) corresponding to \(\lambda\) is the null space of \(A - \lambda I_n\).%
\end{definition}
\begin{activity}{}{x:activity:act_3_c_1}%
The matrix \(A = \left[ \begin{array}{rrrr} 2\amp 0\amp 1 \\ 0\amp 2\amp -1 \\ 0\amp 0\amp 1 \end{array} \right]\) has two distinct eigenvalues.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find a basis for the eigenspace of \(A\) corresponding to the eigenvalue \(\lambda_1 = 1\). In other words, find a basis for \(\Nul A - I_3\).%
\item{}Find a basis for the eigenspace of \(A\) corresponding to the eigenvalue \(\lambda_2 = 2\).%
\item{}Is it true that if \(\vv_1\) and \(\vv_2\) are two distinct eigenvectors for \(A\), that \(\vv_1\) and \(\vv_2\) are linearly independent? Explain.%
\item{}Is it possible to have two linearly independent eigenvectors corresponding to the same eigenvalue?%
\item{}Is it true that if \(\vv_1\) and \(\vv_2\) are two distinct eigenvectors corresponding to different eigenvalues for \(A\), that \(\vv_1\) and \(\vv_2\) are linearly independent? Explain.%
\end{enumerate}
\end{activity}%
If we know an eigenvalue \(\lambda\) of an \(n \times n\) matrix \(A\), \hyperref[x:activity:act_3_c_1]{Activity~{\xreffont\ref{x:activity:act_3_c_1}}} shows us how to find a basis for the corresponding eigenspace \textemdash{} just row reduce \(A - \lambda I_n\) to find a basis for \(\Nul A-\lambda I_n\). To this point we have always been given eigenvalues for our matrices, and have not seen how to find these eigenvalues. That process will come a bit later. For now, we just want to become more familiar with eigenvalues and eigenvectors. The next activity should help connect eigenvalues to ideas we have discussed earlier.%
\begin{activity}{}{x:activity:act_3_c_2}%
Let \(A\) be an \(n \times n\) matrix with eigenvalue \(\lambda\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}How many solutions does the equation \((A-\lambda I_n) \vx = \vzero\) have? Explain.%
\item{}Can \(A - \lambda I_n\) have a pivot in every column? Why or why not?%
\item{}Can \(A - \lambda I_n\) have a pivot in every row? Why or why not?%
\item{}Can the columns of \(A - \lambda I_n\) be linearly independent? Why or why not?%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Linearly Independent Eigenvectors}
\typeout{************************************************}
%
\begin{sectionptx}{Linearly Independent Eigenvectors}{}{Linearly Independent Eigenvectors}{}{}{x:section:sec_lin_ind_egvec}
An important question we will want to answer about a matrix is how many linearly independent eigenvectors the matrix has. \hyperref[x:activity:act_3_c_1]{Activity~{\xreffont\ref{x:activity:act_3_c_1}}} shows that eigenvectors for the same eigenvalue may be linearly dependent or independent, but all of our examples so far seem to indicate that eigenvectors corresponding to different eigenvalues are linearly independent. This turns out to be universally true as our next theorem demonstrates. The next activity should help prepare us for the proof of this theorem%
\begin{activity}{}{x:activity:act_3_c_3}%
Let \(\lambda_1\) and \(\lambda_2\) be distinct eigenvalues of a matrix \(A\) with corresponding eigenvectors \(\vv_1\) and \(\vv_2\). The goal of this activity is to demonstrate that \(\vv_1\) and \(\vv_2\) are linearly independent. To prove that \(\vv_1\) and \(\vv_2\) are linearly independent, suppose that%
\begin{equation}
x_1\vv_1 + x_2 \vv_2 = \vzero\text{.}\label{x:men:eq_3_c_2}
\end{equation}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Multiply both sides of equation \hyperref[x:men:eq_3_c_2]{({\xreffont\ref{x:men:eq_3_c_2}})} on the left by the matrix \(A\) and show that%
\begin{equation}
x_1\lambda_1\vv_1 + x_2 \lambda_2\vv_2 = \vzero\text{.}\label{x:men:eq_3_c_3}
\end{equation}
%
\item{}Now multiply both sides of equation \hyperref[x:men:eq_3_c_2]{({\xreffont\ref{x:men:eq_3_c_2}})} by the scalar \(\lambda_1\) and show that%
\begin{equation}
x_1\lambda_1\vv_1 + x_2 \lambda_1\vv_2 = \vzero\text{.}\label{x:men:eq_3_c_4}
\end{equation}
%
\item{}Combine equations \hyperref[x:men:eq_3_c_3]{({\xreffont\ref{x:men:eq_3_c_3}})} and \hyperref[x:men:eq_3_c_4]{({\xreffont\ref{x:men:eq_3_c_4}})} to obtain the equation%
\begin{equation}
x_2(\lambda_2-\lambda_1)\vv_2 = \vzero\text{.}\label{x:men:eq_3_c_5}
\end{equation}
%
\item{}Explain how we can conclude that \(x_2 = 0\). Why does it follow that \(x_1 = 0\)? What does this tell us about \(\vv_1\) and \(\vv_2\)?%
\end{enumerate}
\end{activity}%
\hyperref[x:activity:act_3_c_3]{Activity~{\xreffont\ref{x:activity:act_3_c_3}}} contains the basic elements of the proof of the next theorem.%
\begin{theorem}{}{}{x:theorem:thm_4_b_lin_indep_evects}%
Let \(\lambda_1\), \(\lambda_2\), \(\ldots\), \(\lambda_k\) be \(k\) distinct eigenvalues for a matrix \(A\) and for each \(i\) between 1 and \(k\) let \(\vv_i\) be an eigenvector of \(A\) with eigenvalue \(\lambda_i\). Then the vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) are linearly independent.%
\end{theorem}
\begin{proof}{}{g:proof:idp14496728}
Let \(A\) be a matrix with \(k\) distinct eigenvalues \(\lambda_1\), \(\lambda_2\), \(\ldots\), \(\lambda_k\) and corresponding eigenvectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\). To understand why \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) are linearly independent, we will argue by contradiction and suppose that the vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) are linearly dependent. Note that \(\vv_1\) cannot be the zero vector (why?), so the set \(S_1=\{\vv_1\}\) is linearly independent. If we include \(\vv_2\) into this set, the set \(S_2 = \{\vv_1, \vv_2\}\) may be linearly independent or dependent. If \(S_2\) is linearly independent, then the set \(S_3 = \{\vv_1, \vv_2, \vv_3\}\) may be linearly independent or dependent. We can continue adding additional vectors until we reach the set \(S_k = \{\vv_1, \vv_2, \vv_3, \ldots, \vv_k\}\) which we are assuming is linearly dependent. So there must be a smallest integer \(m \geq 2\) such that the set \(S_m\) is linearly dependent while \(S_{m-1}\) is linearly independent. Since \(S_m = \{\vv_1, \vv_2, \vv_3, \ldots, \vv_m\}\) is linearly dependent, there is a linear combination of \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_m\) with weights not all 0 that is the zero vector. Let \(c_1\), \(c_2\), \(\ldots\), \(c_m\) be such weights, not all zero, so that%
\begin{equation}
c_1\vv_1 + c_2\vv_2 + \cdots + c_{m-1} \vv_{m-1} + c_m \vv_m = \vzero\label{x:men:eq_distinct_eigenvalues}
\end{equation}
%
\par
If we multiply both sides of \hyperref[x:men:eq_distinct_eigenvalues]{({\xreffont\ref{x:men:eq_distinct_eigenvalues}})} on the left by the matrix \(A\) we obtain%
\begin{align}
A(c_1\vv_1 + c_{2}\vv_{2} + \cdots + c_m \vv_m) \amp = A\vzero\notag\\
c_1A\vv_1 + c_{2}A\vv_{2} + \cdots + c_m A\vv_m \amp = \vzero\notag\\
c_1 \lambda_1\vv_1 + c_{2}\lambda_{2}\vv_{2} + \cdots + c_m \lambda_m\vv_m \amp = \vzero\text{.}\label{x:mrow:eq_distinct_eigenvalues2}
\end{align}
%
\par
If we multiply both sides of \hyperref[x:men:eq_distinct_eigenvalues]{({\xreffont\ref{x:men:eq_distinct_eigenvalues}})} by \(\lambda_m\) we obtain the equation%
\begin{equation}
c_1\lambda_m\vv_1 + c_{2}\lambda_m\vv_{2} + \cdots + c_m \lambda_m\vv_m = \vzero\text{.}\label{x:men:eq_distinct_eigenvalues3}
\end{equation}
%
\par
Subtracting corresponding sides of equation \hyperref[x:men:eq_distinct_eigenvalues3]{({\xreffont\ref{x:men:eq_distinct_eigenvalues3}})} from \hyperref[x:mrow:eq_distinct_eigenvalues2]{({\xreffont\ref{x:mrow:eq_distinct_eigenvalues2}})} gives us%
\begin{equation}
c_{1}(\lambda_{1}-\lambda_m)\vv_{1} + c_{2}(\lambda_{2}-\lambda_m)\vv_{2} + \cdots + c_{m-1} (\lambda_{m-1}-\lambda_m) \vv_{m-1} = \vzero\text{.}\label{x:men:eq_distinct_eigenvalues4}
\end{equation}
%
\par
Recall that \(S_{m-1}\) is a linearly independent set, so the only way a linear combination of vectors in \(S_{m-1}\) can be \(\vzero\) is if all of the weights are 0. Therefore, we must have%
\begin{equation*}
c_{1}(\lambda_{1}-\lambda_m) = 0, \ \ c_{2}(\lambda_{2}-\lambda_m) = 0, \ \ \ldots, \ \ c_{m-1} (\lambda_{m-1}-\lambda_m) = 0\text{.}
\end{equation*}
%
\par
Since the eigenvalues are all distinct, this can only happen if%
\begin{equation*}
c_1 = c_2 = \cdots = c_{m-1} = 0\text{.}
\end{equation*}
%
\par
But equation \hyperref[x:men:eq_distinct_eigenvalues]{({\xreffont\ref{x:men:eq_distinct_eigenvalues}})} then implies that \(c_m = 0\) and so all of the weights \(c_1\), \(c_2\), \(\ldots\), \(c_m\) are 0. However, when we assumed that the eigenvectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) were linearly dependent, this led to having at least one of the weights \(c_1\), \(c_2\), \(\ldots\), \(c_m\) be nonzero. This cannot happen, so our assumption that the eigenvectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) were linearly dependent must be false and we conclude that the eigenvectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) are linearly independent.%
\end{proof}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_egspace_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp14524632}%
Let \(A = \left[ \begin{array}{rrr} 4\amp -3\amp -3\\-3\amp 4\amp 3\\3\amp -3\amp -2 \end{array} \right]\) and let \(T\) be the matrix transformation defined by \(T(\vx) = A\vx\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(4\) is an eigenvalue for \(A\) and find a basis for the corresponding eigenspace of \(A\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp14536408}{}\quad{}Recall that \(\lambda\) is an eigenvalue of \(A\) if \(A-\lambda I_3\) is not invertible. To show that \(4\) is an eigenvalue for \(A\) we row reduce the matrix%
\begin{equation*}
A - (4)I_3 = \left[ \begin{array}{rrr} 0\amp -3\amp -3\\3\amp 0\amp -3\\3\amp -3\amp -6 \end{array}  \right]
\end{equation*}
to \(\left[ \begin{array}{ccr} 1\amp 0\amp -1\\0\amp 1\amp 1\\0\amp 0\amp 0 \end{array}  \right]\). Since the third column of \(A-4I_3\) is not a pivot column, the matrix \(A-4I_3\) is not invertible. We conclude that \(4\) is an eigenvalue of \(A\). The eigenspace of \(A\) for the eigenvalue \(4\) is \(\Nul (A-4I_3)\). The reduced row echelon form of \(A-4I_3\) shows that if \(\vx = \left[ \begin{array}{c} x_1\\x_2\\x_3 \end{array}  \right]\) and \((A-4I_3) \vx = \vzero\), then \(x_3\) is free, \(x_2 = -x_3\), and \(x_1 = x_3\). Thus,%
\begin{equation*}
\vx = \left[ \begin{array}{c} x_1\\x_2\\x_3 \end{array}  \right] = \left[  \begin{array}{r} x_3\\ -x_3\\x_3 \end{array}  \right] = x_3 \left[ \begin{array}{r} 1\\-1\\1 \end{array}  \right]\text{.}
\end{equation*}
Therefore, \(\left\{\left[ \begin{array}{r} 1\\-1\\1 \end{array}  \right]\right\}\) is a basis for the eigenspace of \(A\) corresponding to the eigenvalue \(4\).%
\item{}Geometrically describe the eigenspace of \(A\) corresponding to the eigenvalue \(4\). Explain what the transformation \(T\) does to this eigenspace.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp14543960}{}\quad{}Since the eigenspace of \(A\) corresponding to the eigenvalue \(4\) is the span of a single nonzero vector \(\vv = \left[ \begin{array}{r} 1\\-1\\1 \end{array}  \right]\), this eigenspace is the line in \(\R^3\) through the origin and the point \((1,-1,1)\). Any vector in this eigenspace has the form \(c \vv\) for some scalar \(c\). Notice that%
\begin{equation*}
T(c\vv) = Ac\vv = cA\vv = 4c\vv\text{,}
\end{equation*}
so \(T\) expands any vector in this eigenspace by a factor of 4.%
\item{}Show that \(1\) is an eigenvalue for \(A\) and find a basis for the corresponding eigenspace of \(A\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp14548056}{}\quad{}To show that \(1\) is an eigenvalue for \(A\) we row reduce the matrix%
\begin{equation*}
A - (1)I_3 = \left[ \begin{array}{crr} 3\amp -3\amp -3\\-3\amp 3\amp 3\\3\amp -3\amp -3 \end{array}  \right]
\end{equation*}
to \(\left[  \begin{array}{crr} 1\amp -1\amp -1\\0\amp 0\amp 0\\0\amp 0\amp 0 \end{array}  \right]\). Since the third column of \(A -I_3\) is not a pivot column, the matrix \(A -I_3\) is not invertible. We conclude that \(1\) is an eigenvalue of \(A\). The eigenspace of \(A\) for the eigenvalue \(1\) is \(\Nul (A-I_3)\). The reduced row echelon form of \(A-I_3\) shows that if \(\vx = \left[ \begin{array}{c} x_1\\x_2\\x_3 \end{array}  \right]\) and \((A-I_3) \vx = \vzero\), then \(x_2\) and \(x_3\) are free, and \(x_1 = x_2+x_3\). Thus,%
\begin{equation*}
\vx = \left[ \begin{array}{c} x_1\\x_2\\x_3 \end{array}  \right] = \left[ \begin{array}{c} x_2+x_3\\ x_2 \\x_3 \end{array}  \right] = x_2 \left[ \begin{array}{c} 1\\1\\0 \end{array}  \right] + x_3 \left[ \begin{array}{c} 1\\0\\1 \end{array}  \right]\text{.}
\end{equation*}
Therefore, \(\left\{\left[ \begin{array}{c} 1\\1\\0 \end{array}  \right], \left[ \begin{array}{c} 1\\0\\1 \end{array}  \right]\right\}\) is a basis for the eigenspace of \(A\) corresponding to the eigenvalue \(1\).%
\item{}Geometrically describe the eigenspace of \(A\) corresponding to the eigenvalue \(1\). Explain what the transformation \(T\) does to this eigenspace.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp14559576}{}\quad{}Since the eigenspace of \(A\) corresponding to the eigenvalue \(1\) is the span of two linearly independent vectors \(\vv_1 = \left[ \begin{array}{r} 1\\1\\0 \end{array}  \right]\) and \(\vv_2 = \left[ \begin{array}{c} 1\\0\\1 \end{array}  \right]\), this eigenspace is the plane in \(\R^3\) through the origin and the points \((1,1,0)\) and \((1,0,1)\). Any vector in this eigenspace has the form \(a \vv_1 + b\vv_2\) for some scalars \(a\) and \(b\). Notice that%
\begin{equation*}
T(a\vv_1+b\vv_2) = A(a\vv_1+b\vv_2) = aA\vv_1+bA\vv_2 = a\vv_1+b\vv_2\text{,}
\end{equation*}
so \(T\) fixes every vector in this plane.%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp14567128}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(A = \left[ \begin{array}{cc} 1\amp 2\\2\amp 1 \end{array} \right]\). Note that the vector \(\vv = \left[ \begin{array}{c}1\\1 \end{array} \right]\) satisfies \(A \vv = 3\vv\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Show that \(\vv\) is an eigenvector of \(A^2\). What is the corresponding eigenvalue?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp14564568}{}\quad{}We use the fact that \(\vv\) is an eigenvector of the matrix \(A\) with eigenvalue \(3\).%
\par
We have that%
\begin{equation*}
A^2 \vv = A(A\vv) = A(3\vv) = 3(A\vv) = 3(3\vv) = 9\vv\text{.}
\end{equation*}
So \(\vv\) is an eigenvector of \(A^2\) with eigenvalue \(9 = 3^2\).%
\item{}Show that \(\vv\) is an eigenvector of \(A^3\). What is the corresponding eigenvalue?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp14578904}{}\quad{}We use the fact that \(\vv\) is an eigenvector of the matrix \(A\) with eigenvalue \(3\).%
\par
We have that%
\begin{equation*}
A^3 \vv = A(A^2\vv) = A(9\vv) = 9(A\vv) = 9(3\vv) = 27\vv\text{.}
\end{equation*}
So \(\vv\) is an eigenvector of \(A^3\) with eigenvalue \(27 = 3^3\).%
\item{}Show that \(\vv\) is an eigenvector of \(A^4\). What is the corresponding eigenvalue?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp14571736}{}\quad{}We use the fact that \(\vv\) is an eigenvector of the matrix \(A\) with eigenvalue \(3\).%
\par
We have that%
\begin{equation*}
A^4 \vv = A(A^3\vv) = A(27\vv) = 27(A\vv) = 27(3\vv) = 81\vv\text{.}
\end{equation*}
So \(\vv\) is an eigenvector of \(A^4\) with eigenvalue \(81 = 3^4\).%
\item{}If \(k\) is a positive integer, do you expect that \(\vv\) is an eigenvector of \(A^k\)? If so, what do you think is the corresponding eigenvalue?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp14583640}{}\quad{}We use the fact that \(\vv\) is an eigenvector of the matrix \(A\) with eigenvalue \(3\).%
\par
The results of the previous parts of this example indicate that \(A^k \vv = 3^k \vv\), or that \(\vv\) is an eigenvector of \(A^k\) with corresponding eigenvalue \(3^k\).%
\end{enumerate}
\item{}The result of part (a) is true in general. Let \(M\) be an \(n \times n\) matrix with eigenvalue \(\lambda\) and corresponding eigenvector \(\vx\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Show that \(\lambda^2\) is an eigenvalue of \(M^2\) with eigenvector \(\vx\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp14593240}{}\quad{}Let \(M\) be an \(n \times n\) matrix with eigenvalue \(\lambda\) and corresponding eigenvector \(\vx\).%
\par
We have that%
\begin{equation*}
M^2 \vx = M(M\vx) = M(\lambda \vx) = \lambda(M\vx) = \lambda(\lambda \vx) = \lambda^2 \vx\text{.}
\end{equation*}
So \(\vx\) is an eigenvector of \(M^2\) with eigenvalue \(\lambda^2\).%
\item{}Show that \(\lambda^3\) is an eigenvalue of \(M^3\) with eigenvector \(\vx\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp14588888}{}\quad{}Let \(M\) be an \(n \times n\) matrix with eigenvalue \(\lambda\) and corresponding eigenvector \(\vx\).%
\par
We have that%
\begin{equation*}
M^3 \vx = M(M^2\vx) = M(\lambda^2 \vx) = \lambda^2(M\vx) = \lambda^2(\lambda \vx) = \lambda^3 \vx\text{.}
\end{equation*}
So \(\vx\) is an eigenvector of \(M^3\) with eigenvalue \(\lambda^3\).%
\item{}Suppose that \(\lambda^k\) is an eigenvalue of \(M^k\) with eigenvector \(\vx\) for some integer \(k \geq 1\). Show then that \(\lambda^{k+1}\) is an eigenvalue of \(M^{k+1}\) with eigenvector \(\vx\). This argument shows that \(\lambda^k\) is an eigenvalue of \(M^k\) with eigenvector \(\vx\) for any positive integer \(k\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp14596952}{}\quad{}Let \(M\) be an \(n \times n\) matrix with eigenvalue \(\lambda\) and corresponding eigenvector \(\vx\).%
\par
Assume that \(M^k \vx = \lambda^k \vx\). Then%
\begin{equation*}
M^{k+1} \vx = M(M^k\vx) = M(\lambda^k \vx) = \lambda^k(M\vx) = 2\lambda^k(\lambda \vx) = \lambda^{k+1} \vx\text{.}
\end{equation*}
So \(\vx\) is an eigenvector of \(M^{k+1}\) with eigenvalue \(\lambda^{k+1}\).%
\end{enumerate}
\item{}We now investigate the eigenvalues of a special type of matrix.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Let \(B = \left[ \begin{array}{ccc} 0\amp 1\amp 0 \\ 0\amp 0\amp 1 \\ 0\amp 0\amp 0 \end{array} \right]\). Show that \(B^3 = 0\). (A square matrix \(M\) is \terminology{nilpotent} ) if \(M^k = 0\) for some positive integer \(k\), so \(B\) is an example of a nilpotent matrix.) What are the eigenvalues of \(B\)? Explain.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp14611160}{}\quad{}Now we investigate a special type of matrix.%
\par
Straightforward calculations show that \(B^3 = 0\). Since \(B\) is an upper triangular matrix, the eigenvalues of \(B\) are the entries on the diagonal. That is, the only eigenvalue of \(B\) is \(0\).%
\item{}Show that the only eigenvalue of a nilpotent matrix is \(0\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp14615384}{}\quad{}Assume that \(M\) is a nilpotent matrix. Suppose that \(\lambda\) is an eigenvalue of \(M\) with corresponding eigenvector \(\vv\). Since \(M\) is a nilpotent matrix, there is a positive integer \(k\) such that \(M^k = 0\). But \(\lambda^k\) is an eigenvalue of \(M^k\) with eigenvector \(\vv\). The only eigenvalue of the zero matrix is \(0\), so \(\lambda^k = 0\). This implies that \(\lambda = 0\). We conclude that the only eigenvalue of a nilpotent matrix is \(0\).%
\end{enumerate}
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_egspace_summ}
%
\begin{itemize}[label=\textbullet]
\item{}An eigenspace of an \(n \times n\) matrix \(A\) corresponding to an eigenvalue \(\lambda\) of \(A\) is the null space of \(A - \lambda I_n\).%
\item{}To find a basis for an eigenspace of a matrix \(A\) corresponding to an eigenvalue \(\lambda\), we row reduce \(A - \lambda I_n\) and find a basis for \(\Nul A - \lambda I_n\).%
\item{}Eigenvectors corresponding to different eigenvalues are always linearly independent.%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_egspace_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp14623448}%
For each of the following, find a basis for the eigenspace of the indicated matrix corresponding to the given eigenvalue.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\left[ \begin{array}{rr} 10\amp 7 \\ -14\amp -11 \end{array} \right]\) with eigenvalue 3%
\item{}\(\left[ \begin{array}{rr} 11\amp 18 \\ -3\amp -4 \end{array} \right]\) with eigenvalue 2%
\item{}\(\left[ \begin{array}{rc} 2\amp 1 \\ -1\amp 0 \end{array} \right]\) with eigenvalue 1%
\item{}\(\left[ \begin{array}{ccc} 1\amp 0\amp 0 \\ 0\amp 0\amp 2 \\ 1\amp 0\amp 2 \end{array} \right]\) with eigenvalue 2%
\item{}\(\left[ \begin{array}{ccc} 1\amp 0\amp 0 \\ 0\amp 0\amp 2 \\ 1\amp 0\amp 2 \end{array} \right]\) with eigenvalue 1%
\item{}\(\left[ \begin{array}{ccc} 2\amp 2\amp 4 \\ 1\amp 1\amp 2 \\ 3\amp 3\amp 6 \end{array} \right]\) with eigenvalue 0%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp14637272}%
Suppose \(A\) is an invertible matrix.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Use the definition of an eigenvalue and an eigenvector to algebraically explain why if \(\lambda\) is an eigenvalue of \(A\), then \(\lambda^{-1}\) is an eigenvalue of \(A^{-1}\).%
\item{}To provide an alternative explanation to the result in the previous part, let \(\vv\) be an eigenvector of \(A\) corresponding to \(\lambda\). Consider the matrix transformation \(T_A\) corresponding to \(A\) and \(T_{A^{-1}}\) corresponding to \(A^{-1}\). Considering what happens to \(\vv\) if \(T_A\) and then \(T_{A^{-1}}\) are applied, describe why this justifies \(\vv\) is also an eigenvector of \(A^{-1}\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp14376152}%
If \(A=\left[ \begin{array}{cc} 0\amp 1\\a\amp b \end{array} \right]\) has two eigenvalues 4 and 6, what are the values of \(a\) and \(b\)?%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp14931464}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}What are the eigenvalues of the identity matrix \(I_2\)? Describe each eigenspace.%
\item{}Now let \(n > 2\) be a positive integer. What are the eigenvalues of the identity matrix \(I_n\)? Describe each eigenspace.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp14937352}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}What are the eigenvalues of the \(2 \times 2\) zero matrix (the matrix all of whose entries are 0)? Describe each eigenspace.%
\item{}Now let \(n > 2\) be a positive integer. What are the eigenvalues of the \(n \times n\) zero matrix? Describe each eigenspace.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp14945672}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
If \(A\vv = \lambda \vv\), then \(\lambda\) is an eigenvalue of \(A\) with eigenvector \(\vv\).%
\item{}\lititle{True\slash{}False.}\par%
The scalar \(\lambda\) is an eigenvalue of a square matrix \(A\) if and only if the equation \((A - \lambda I_n) \vx = \vzero\) has a nontrivial solution.%
\item{}\lititle{True\slash{}False.}\par%
If \(\lambda\) is an eigenvalue of a matrix \(A\), then there is only one nonzero vector \(\vv\) with \(A \vv = \lambda \vv\).%
\item{}\lititle{True\slash{}False.}\par%
The eigenspace of an eigenvalue of an \(n \times n\) matrix \(A\) is the same as \(\Nul (A - \lambda I_n)\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\vv_1\) and \(\vv_2\) are eigenvectors of a matrix \(A\) corresponding to the same eigenvalue \(\lambda\), then \(\vv_1 + \vv_2\) is also an eigenvector of \(A\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\vv_1\) and \(\vv_2\) are eigenvectors of a matrix \(A\), then \(\vv_1 + \vv_2\) is also an eigenvector of \(A\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\vv\) is an eigenvector of an invertible matrix \(A\), then \(\vv\) is also an eigenvector of \(A^{-1}\).%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Modeling Population Migration}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Modeling Population Migration}{}{Project: Modeling Population Migration}{}{}{x:section:sec_proj_migration}
As introduced earlier, data from the Michigan Department of Technology, Management, and Budget shows that from 2011 to 2012, approximately 0.05\% of the U.S. population outside of Michigan moved to the state of Michigan, while approximately 2\% of Michigan's population moved out of Michigan. We are interested in determining the long-term distribution of population in Michigan.%
\par
Let \(\vx_n = \left[ \begin{array}{c} m_n \\ u_n \end{array} \right]\) be the \(2 \times 1\) vector where \(m_n\) is the population of Michigan and \(u_n\) is the U.S. population outside of Michigan in year \(n\). Assume that we start our analysis at generation 0 and \(\vx_0 = \left[ \begin{array}{c} m_0 \\ u_0 \end{array} \right]\).%
\begin{project}{}{x:project:act_Mi_pop_1}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain how the data above shows that%
\begin{align*}
m_1 \amp = 0.98m_0 + 0.0005u_0\\
u_1 \amp = 0.02m_0 + 0.9995u_0
\end{align*}
%
\item{}Identify the matrix \(A\) such that \(\vx_1 = A \vx_{0}\).%
\end{enumerate}
\end{project}%
One we have the equation \(\vx_1 = A\vx_0\), we can extend it to subsequent years:%
\begin{equation*}
\vx_2 = A\vx_1, \ \ \ \ \vx_3 = A \vx_2, \ \ \ \ , ..., \ \ \ \ \vx_{n+1} = A \vx_n
\end{equation*}
for each \(n \geq 0\).%
\par
This example illustrates the general nature of what is called a \emph{Markov process} (see \hyperref[x:definition:def_Markov]{Definition~{\xreffont\ref{x:definition:def_Markov}}}). Recall that the matrix \(A\) that provides the link from one generation to the next is called the transition matrix.%
\par
In situations like these, we are interested in determining if there is a steady-state vector, that is a vector that satisfies%
\begin{equation}
\vx = A \vx\text{.}\label{x:men:eq_Mi_pop_2}
\end{equation}
%
\par
Such a vector would show us the long-term population of Michigan provided the population dynamics do not change.%
\begin{project}{}{g:project:idp14967432}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why a steady-state solution to \hyperref[x:men:eq_Mi_pop_2]{({\xreffont\ref{x:men:eq_Mi_pop_2}})} is an eigenvector of \(A\). What is the corresponding eigenvalue?%
\item{}Consider again the transition matrix \(A\) from \hyperref[x:project:act_Mi_pop_1]{Project Activity~{\xreffont\ref{x:project:act_Mi_pop_1}}}. Recall that the solutions to equation \hyperref[x:men:eq_Mi_pop_2]{({\xreffont\ref{x:men:eq_Mi_pop_2}})} are all the vectors in \(\Nul (A-I_2)\). In other words, the eigenvectors of \(A\) for this eigenvalue are the nonzero vectors in \(\Nul (A-I_2)\). Find a basis for the eigenspace of \(A\) corresponding to this eigenvalue. Use whatever technology is appropriate.%
\item{}Once we know a basis for the eigenspace of the transition matrix \(A\), we can use it to estimate the steady-state population of Michigan (assuming the stated migration trends are valid long-term). According to the \href{https://www.census.gov/data/tables/time-series/demo/popest/2010s-national-total.html}{US Census Bureau}\footnotemark{}, the resident US population on December 1, 2019 was 330,073,471. Assuming no population growth in the U.S., what would the long-term population of Michigan be? How realistic do you think this is?%
\end{enumerate}
\end{project}%
\footnotetext[29]{\nolinkurl{census.gov/data/tables/time-series/demo/popest/2010s-national-total.html}\label{g:fn:idp14974856}}%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 15 Bases and Dimension}
\typeout{************************************************}
%
\begin{chapterptx}{Bases and Dimension}{}{Bases and Dimension}{}{}{x:chapter:chap_bases_dimension}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp14978952}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section%
%
\begin{itemize}[label=\textbullet]
\item{}What is the dimension of a subspace of \(\R^n\)? What property of bases makes the dimension a well-defined number?%
\item{}If \(W\) is a subspace of \(\R^n\) with dimension \(k\), what must be true about any linearly independent subset \(S\) of \(W\) that contains exactly \(k\) vectors?%
\item{}If \(W\) is a subspace of \(\R^n\) with dimension \(k\), what must be true about any subset \(S\) of \(W\) that contains exactly \(k\) vectors and spans \(W\)?%
\item{}What is the rank of a matrix?%
\item{}What does the Rank-Nullity Theorem say?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Lattice Based Cryptography}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Lattice Based Cryptography}{}{Application: Lattice Based Cryptography}{}{}{x:section:sec_appl_latt_crypt}
When you use your credit card, you expect that the information that is transmitted is protected so others can't use your card. Similarly, when you create a password for your computer or other devices, you do so with the intention that it will be difficult for others to decipher.%
\par
Cryptology is the study of methods to maintain secure communication in the presence of other parties (cryptography), along with the study of breaking codes (cryptanalysis). In essence, cryptology is the art of keeping and breaking secrets. The creation of secure codes (cryptography) can provide confidentiality (ensure that information is available only to the intended recipients), data integrity (prevent data from being altered between the sender and recipient), and authentication (making sure that the information is from the correct source).%
\par
Modern cryptology uses mathematical theory that can be implemented with computer hardware and algorithms. The security of public key systems is largely based on mathematical problems that are very difficult to solve. For example, the security of the RSA system relies on the fact that it is computationally difficult to find prime factors of very large numbers, and elliptic curve cryptography relies on the difficulty of the discrete logarithm problem for elliptic curves. However, the continual increase in the power of computers threatens the security of these systems, and so cryptographic systems have to keep adapting to the newest technology. For example, Shor's Algorithm (which could run on a quantum computer) can solve the public key cryptographic systems that rely on the integer factorization problem or the discrete logarithm problem. So if a working quantum computer was ever developed, it would threaten the existing cryptographic systems. Lattice-based cryptography is a potential source of systems that may be secure even in such an environment. The security of these systems is dependent on the fact that the average case of the difficulty of certain problems in lattice theory is higher than the worst case problems that underpin current cryptosystems. As we will see later in this section, lattices are built on bases for subspace of \(\R^n\).%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_base_dim_intro}
A basis provides a system in which we can uniquely represent every vector in the space we are considering. More specifically, every vector in the space can be expressed as a linear combination of the vectors in the basis in a unique way. In order to be able to cover every point in the space, the basis has to span the space. In order to be able to provide a unique coordinate for each point, there should not be any extra vectors in the basis, which is achieved by linear independence of the vectors. For practical reasons, a basis simplifies many problems because we only need to solve the problem for each of the basis vectors. Solutions of the other cases usually follow because every vector in the space can be expressed as a unique linear combination of the basis vectors.%
\par
Recall that a basis for a subspace \(W\) of \(\R^n\) is a set of vectors which are linearly independent and which span \(W\).%
\begin{exploration}{}{x:exploration:pa_3_d}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}For each of the following sets of vectors, determine whether the vectors form a basis of \(\R^3\). Use any appropriate technology for your computations.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}\(\left\{\left[ \begin{array}{c} 1\\0\\0 \end{array} \right],\left[ \begin{array}{c} 1\\1\\ 0 \end{array} \right], \left[ \begin{array}{c} 1\\1\\1 \end{array} \right]\right\}\)%
\item{}\(\left\{\left[ \begin{array}{c} 1\\0\\1 \end{array} \right],\left[ \begin{array}{c} 1\\1\\ 0 \end{array} \right], \left[ \begin{array}{c} 2\\3\\1 \end{array} \right]\right\}\)%
\item{}\(\left\{\left[ \begin{array}{c} 1\\0\\1 \end{array} \right],\left[ \begin{array}{c} 1\\1\\ 1 \end{array} \right], \left[ \begin{array}{c} 0\\3\\3 \end{array} \right], \left[ \begin{array}{r} -1\\2\\1 \end{array} \right]\right\}\)%
\item{}\(\left\{\left[ \begin{array}{c} 1\\0\\0 \end{array} \right],\left[ \begin{array}{c} 0\\1\\ 0 \end{array} \right]\right\}\)%
\end{enumerate}
\item{}In problem (1) we should have noticed that a space can have more than one basis, but that any two bases contain the same number of elements. This is a critically important idea that we investigate in more detail in this problem in one specific case. Assume that \(W\) is a subspace of \(\R^n\) that has a basis \(\B = \{\vv_1, \vv_2\}\) with two basis vectors. We want to see if any other basis for \(W\) can have a different number of elements. Let us now consider a set \(U = \{\vu_1, \vu_2, \vu_3\}\) of three vectors in \(W\). Our goal is to determine if \(U\) can be a basis for \(W\). Since \(\B\) is a basis for \(W\), any vector in \(W\) can be written as a linear combination of the vectors in \(\B\). So we can write%
\begin{align}
\vu_1 \amp = a_{11}\vv_1 + a_{21}\vv_2\label{x:mrow:eq_PA2_9_1}\\
\vu_2 \amp = a_{12}\vv_1 + a_{22}\vv_2\label{x:mrow:eq_PA2_9_2}\\
\vu_3 \amp = a_{13}\vv_1 + a_{23}\vv_2\label{x:mrow:eq_PA2_9_3}
\end{align}
for some scalars \(a_{ij}\). If \(U\) were to be a basis for \(W\), then \(U\) would have to be a linearly independent set. To determine the independence or dependence of \(U\) we consider the vector equation%
\begin{equation}
x_1 \vu_1 + x_2 \vu_2 + x_3 \vu_3 = \vzero\label{x:men:eq_PA2_9_4}
\end{equation}
for scalars \(x_1\), \(x_2\), and \(x_3\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Substitute for \(\vu_1\), \(\vu_2\), and \(\vu_3\) from \hyperref[x:mrow:eq_PA2_9_1]{({\xreffont\ref{x:mrow:eq_PA2_9_1}})}, \hyperref[x:mrow:eq_PA2_9_2]{({\xreffont\ref{x:mrow:eq_PA2_9_2}})}, and \hyperref[x:mrow:eq_PA2_9_3]{({\xreffont\ref{x:mrow:eq_PA2_9_3}})} into \hyperref[x:men:eq_PA2_9_4]{({\xreffont\ref{x:men:eq_PA2_9_4}})} and perform some vector algebra to show that%
\begin{equation}
\vzero = \left(a_{11}x_1+a_{12}x_2 + a_{13}x_3\right) \vv_1 + \left(a_{21}x_1+a_{22}x_2 + a_{23}x_3\right) \vv_2\text{.}\label{x:men:eq_PA2_9_5}
\end{equation}
%
\item{}Recall that \(\B = \{\vv_1, \vv_2\}\) is a basis. What does that tell us about the weights in the linear combination \hyperref[x:men:eq_PA2_9_5]{({\xreffont\ref{x:men:eq_PA2_9_5}})}? Explain why \(A \vx = \vzero\), where \(A = [a_{ij}]\) and \(\vx = [x_1 \ x_2 \ x_3]^{\tr}\).%
\item{}With \(A\) as in part (b), how many solutions does the system \(A \vx = \vzero\) have? Explain. What does this tell us about the independence or dependence of the set \(U\)? Why?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp15017864}{}\quad{}Consider the number of rows and columns of \(A\).%
\item{}Can \(U\) be a basis for \(W\)? Explain.%
\end{enumerate}
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Dimension of a Subspace of \(\R^n\)}
\typeout{************************************************}
%
\begin{sectionptx}{The Dimension of a Subspace of \(\R^n\)}{}{The Dimension of a Subspace of \(\R^n\)}{}{}{x:section:sec_dim_sub_rn}
In \hyperref[x:exploration:pa_3_d]{Preview Activity~{\xreffont\ref{x:exploration:pa_3_d}}} we saw that a subspace of \(\R^n\) can have more than one basis. This prompts the question of how, if at all, are any two bases for a given space related. More specifically, is it possible to have two bases for a given subspace of \(\R^n\) that contain different numbers of vectors? As we will see the answer is no, which will lead us to the concept of \terminology{dimension}.%
\par
Let \(W\) be a subspace of \(\R^n\) that has a basis \(\B = \{\vv_1, \vv_2, \ldots, \vv_k\}\) of \(k\) vectors. Since we have been calling bases minimal spanning sets, we should expect that any two bases for the same subspace have the same number of elements (otherwise one of the two bases would not be minimal). Our goal in this section is to prove that result \textemdash{} that any other basis of \(W\) contains exactly \(k\) vectors. The approach will be the same as was used in \hyperref[x:exploration:pa_3_d]{Preview Activity~{\xreffont\ref{x:exploration:pa_3_d}}}. We will let \(U = \{\vu_1, \vu_2, \ldots, \vu_m\}\) be a set of vectors in \(W\) with \(m > k\) and demonstrate that \(U\) is a linearly dependent set. To argue linear dependence, let \(x_1\), \(x_2\), \(\ldots\), \(x_m\) be scalars so that%
\begin{equation}
x_1 \vu_1 + x_2 \vu_2 + \cdots + x_m \vu_m = \vzero\text{.}\label{x:men:eq_3_d_1}
\end{equation}
%
\par
For each \(i\) there exist scalars \(a_{ij}\) so that%
\begin{equation*}
\vu_i = a_{1i}\vv_1 + a_{2i}\vv_2 + \cdots + a_{ki}\vv_k\text{.}
\end{equation*}
%
\par
Substituting into \hyperref[x:men:eq_3_d_1]{({\xreffont\ref{x:men:eq_3_d_1}})} yields%
\begin{align}
\vzero \amp = x_1 \vu_1 + x_2 \vu_2 + \cdots + x_m \vu_m\notag\\
\amp = x_1( a_{11}\vv_1 + a_{21}\vv_2 + \cdots + a_{k1}\vv_k ) + x_2 ( a_{12}\vv_1 + a_{22}\vv_2\notag\\
\amp \qquad + \cdots  + a_{k2}\vv_k ) + \cdots + x_m ( a_{1m}\vv_1 + a_{2m}\vv_2 + \cdots + a_{km}\vv_k )\notag\\
\amp = (x_1a_{11}+x_2a_{12}+x_3a_{13} + \cdots + x_ma_{1m})\vv_1\notag\\
\amp \qquad + (x_1a_{21}+x_2a_{22}+x_3a_{23} + \cdots + x_ma_{2m})\vv_2\notag\\
\amp \qquad + \cdots  + (x_1a_{k1}+x_2a_{k2}+x_3a_{k3} + \cdots + x_ma_{km})\vv_k\text{.}\label{x:mrow:eq_3_d_2}
\end{align}
%
\par
Since \(\B\) is a basis, the vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) are linearly independent. So each coefficient in \hyperref[x:mrow:eq_3_d_2]{({\xreffont\ref{x:mrow:eq_3_d_2}})} is 0 and \(\vx~=~[x_1 \ x_2 \ \cdots \ x_m]^{\tr}\) is a solution to the homogeneous system \(A \vx = \vzero\), where \(A = [a_{ij}]\). Now \(A\) is a \(k \times m\) matrix with \(m > k\), so not every column of \(A\) is a pivot column. This means that \(A \vx = \vzero\) has a nontrivial solution. It follows that the vector equation \hyperref[x:men:eq_3_d_1]{({\xreffont\ref{x:men:eq_3_d_1}})} has a nontrivial solution and so the \(m\) vectors \(\vu_1\), \(\vu_2\), \(\ldots\), \(\vu_m\) are linearly dependent. We summarize this in the following theorem.%
\begin{theorem}{}{}{x:theorem:thm_3_d_1}%
Let \(W\) be a subspace of \(\R^n\) containing a basis with \(k\) vectors. If \(m > k\), then any set of \(m\) vectors in \(W\) is linearly dependent.%
\end{theorem}
One consequence of \hyperref[x:theorem:thm_3_d_1]{Theorem~{\xreffont\ref{x:theorem:thm_3_d_1}}} is that, in addition to being a minimal spanning set, a basis is also a maximal linearly independent set.%
\begin{activity}{}{x:activity:act_3_d_1}%
Now let's return to the question of the number of elements in a basis for a subspace of \(\R^n\). Recall that we are assuming that \(W\) has a basis \(\B = \{\vv_1, \vv_2, \ldots, \vv_k\}\) of \(k\) vectors in \(\R^n\). Suppose that \(\B'\) is another basis for \(W\) containing \(m\) vectors.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Given the fact that \(\B\) is a basis for \(W\), what does \hyperref[x:theorem:thm_3_d_1]{Theorem~{\xreffont\ref{x:theorem:thm_3_d_1}}} tell us about the relationship between \(m\) and \(k\)?%
\item{}Given the fact that \(\B'\) is a basis for \(W\), what does \hyperref[x:theorem:thm_3_d_1]{Theorem~{\xreffont\ref{x:theorem:thm_3_d_1}}} tell us about the relationship between \(m\) and \(k\)?%
\item{}What do the results of (a) and (b) tell us about the relationship between \(m\) and \(k\)? What can we conclude about any basis for \(W\)?%
\end{enumerate}
\end{activity}%
The result of \hyperref[x:activity:act_3_d_1]{Activity~{\xreffont\ref{x:activity:act_3_d_1}}} is summarized in the following theorem. Recall that the trivial space is the single element set \(\{\vzero\}\).%
\begin{theorem}{}{}{x:theorem:thm_3_d_basis}%
If a nontrivial subspace \(W\) of \(\R^n\) has a basis of \(k\) vectors, then every basis of \(W\) contains exactly \(k\) vectors.%
\end{theorem}
This last theorem states that the number of vectors in a basis for a subspace space is a well-defined number. In other words, the number of vectors in a basis is an \terminology{invariant} of the subspace. This important number is given a name.%
\begin{definition}{}{g:definition:idp15057544}%
\index{dimension!subspace of \(\R^n\)}%
The \terminology{dimension} of a subspace \(W\) of \(\R^n\) is the number of vectors in a basis for \(W\). The dimension of the trivial subspace \(\{\vzero\}\) of \(\R^n\) is defined to be 0.%
\end{definition}
We denote the dimension of a subspace \(W\) of \(\R^n\) by \(\dim(W)\). As we will see later, any two vector spaces of the same dimension are basically the same vector space. So the dimension of a vector space is an important number that essentially tells us all we need to know about the structure of the space.%
\begin{activity}{}{x:activity:act_3_d_2}%
Find the dimensions of each of the indicated subspaces of \(\R^n\) for the appropriate \(n\). Explain your method.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\Span\left\{ \left[ \begin{array}{c} 1 \\ 0 \\ 0 \end{array} \right], \left[ \begin{array}{c} 1 \\ 1 \\ 0 \end{array} \right], \left[ \begin{array}{c} 2 \\ 3 \\ 0 \end{array} \right] \right\}\)%
\item{}\(xy\)-plane in \(\R^3\)%
\item{}\(\R^3\)%
\item{}\(\R^n\)%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Conditions for a Basis of a Subspace of \(\R^n\)}
\typeout{************************************************}
%
\begin{sectionptx}{Conditions for a Basis of a Subspace of \(\R^n\)}{}{Conditions for a Basis of a Subspace of \(\R^n\)}{}{}{x:section:sec_cond_basis_subspace}
There are two items we need to confirm before we can state that a subset \(\B\) of a subspace \(W\) of \(\R^n\) is a basis for \(W\): the set \(\B\) must be linearly independent and span \(W\). However, if we have the right number (namely, the dimension) of vectors in our set \(\B\), then either one of these conditions will imply the other.%
\begin{activity}{}{x:activity:act_3_d_3}%
Let \(W\) be a subspace of \(\R^n\) with \(\dim(W) = k\). We know that every basis of \(W\) contains exactly \(k\) vectors.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Suppose that \(S\) is a subset of \(W\) that contains \(k\) vectors and is linearly independent. In this part of the activity we will show that \(S\) must span \(W\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Suppose that \(S\) does not span \(W\). Explain why this implies that \(W\) contains a set of \(k+1\) linearly independent vectors.%
\item{}Explain why the result of part i. tells us that \(S\) is a basis for \(W\).%
\end{enumerate}
\item{}Now suppose that \(S\) is a subset of \(W\) with \(k\) vectors that spans \(W\). In this part of the activity we will show that \(S\) must be linearly independent.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Suppose that \(S\) is not linearly independent. Explain why we can then find a proper subset of \(S\) that is linearly independent but has the same span as \(S\).%
\item{}Explain why the result of part i. tells us that \(S\) is a basis for \(W\).%
\end{enumerate}
\end{enumerate}
\end{activity}%
The result of \hyperref[x:activity:act_3_d_3]{Activity~{\xreffont\ref{x:activity:act_3_d_3}}} is the following important theorem.%
\begin{theorem}{}{}{x:theorem:thm_3_d_basis_properties}%
Let \(W\) be a subspace of \(\R^n\) of dimension \(k\) and let \(S\) be a subset of \(W\) containing exactly \(k\) vectors.%
\begin{enumerate}
\item{}If \(S\) is linearly independent, then \(S\) is a basis for \(W\).%
\item{}If \(S\) spans \(W\), then \(S\) is a basis for \(W\).%
\end{enumerate}
%
\end{theorem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Finding a Basis for a Subspace}
\typeout{************************************************}
%
\begin{sectionptx}{Finding a Basis for a Subspace}{}{Finding a Basis for a Subspace}{}{}{x:section:sec_find_basis_subspace}
Since every vector in a subspace of \(\R^n\) can be written uniquely as a linear combination of vectors in a basis for the subspace, a basis provides us with the most efficient and convenient way to represent vectors in the subspace. Until now we have been given a set of vectors and have been asked to find a basis from that set, so an important question to address is how we can find a basis for a subspace \(W\) of \(\R^n\) starting from scratch. Here is one way. If \(W = \{\vzero\}\), then the dimension of \(W\) is 0 and \(W\) has no basis. So suppose \(\dim(W) > 0\). Start by choosing any nonzero vector \(\vw_1\) in \(W\). Let \(\B_1 = \{\vw_1\}\). If \(\B_1\) spans \(W\), then \(\B_1\) is a basis for \(W\). If not, there is a vector \(\vw_2\) in \(W\) that is not in \(\Span(\B_1)\). Then \(\B_2 = \{\vw_1, \vw_2\}\) is a linearly independent set. If \(\Span(\B_2) = W\), then \(\B_2\) is a basis for \(W\) and we are done. If not, repeat the process. Since any basis for \(W\) can contain at most \(n=\dim(\R^n)\) vectors, we know the process must stop at some point. This process also allows us to construct a basis for a vector space that contains a given nonzero vector.%
\begin{activity}{}{x:activity:act_3_d_4}%
Find a basis for \(\R^3\) that contains the vector \(\left[ \begin{array}{r} 1 \\ 2 \\ -1 \end{array} \right]\). When constructing your basis, how do you know when to stop?%
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Rank of a Matrix}
\typeout{************************************************}
%
\begin{sectionptx}{Rank of a Matrix}{}{Rank of a Matrix}{}{}{x:section:sec_mtx_rank}
In this section, we define the rank of a matrix and review conditions to add to our Invertible Matrix Theorem.%
\begin{activity}{}{x:activity:act_3_d_5}%
Let \(A = \left[ \begin{array}{rrrrr} 1\amp 2\amp -1\amp 0\amp 0 \\ 0\amp 0\amp 1\amp 0\amp -1 \\ 0\amp 0\amp 0\amp 1\amp 1 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Without performing any calculations, find \(\dim(\Nul A)\). Explain.%
\item{}Without performing any calculations, find \(\dim(\Col A)\). Explain.%
\item{}There is a connection between \(\dim(\Nul A)\), \(\dim(\Col A)\) and the size of \(A\). Find this connection and explain it.%
\end{enumerate}
\end{activity}%
\index{rank of a matrix}\index{nullity of a matrix} As \hyperref[x:activity:act_3_d_5]{Activity~{\xreffont\ref{x:activity:act_3_d_5}}} illustrates, the number of vectors in a basis for \(\Nul A\) is the number of non-pivot columns in \(A\) and the number of vectors in a basis for \(\Col A\) is the number of pivot columns of \(A\). We define the \terminology{rank} of a matrix \(A\) (denoted rank(\(A\))) to be the dimension of \(\Col A\) and the \terminology{nullity} of \(A\) to be dimension of \(\Nul A\). The dimension of the null space of \(A\) is also called the \terminology{nullity} of \(A\) (denoted nullity(\(A\))) Using this terminology we have the Rank-Nullity Theorem.%
\begin{theorem}{The Rank-Nullity Theorem.}{}{x:theorem:thm_3_d_rank_nullity}%
Let \(A\) be an \(m \times n\) matrix. Then%
\begin{equation*}
\rank(A) + \nullity(A) = n\text{.}
\end{equation*}
%
\end{theorem}
There is also a row space of a matrix \(A\), which we define to be the span of the rows of \(A\). We can find the row space of \(A\) by finding the column space of \(A^{\tr}\), so the row space is really nothing new. As it turns out, the dimension of the row space of \(A\) is always equal to the dimension of the column space of \(A\), and justification for this statement is in the exercises.%
\par
The Rank-Nullity Theorem allows us to add extra conditions to the Invertible Matrix Theorem.%
\begin{theorem}{The Invertible Matrix Theorem.}{}{x:theorem:thm_3_d_IMT}%
Let \(A\) be an \(n \times n\) matrix. The following statements are equivalent.%
\begin{enumerate}[label=\alph*]
\item{}The matrix \(A\) is an invertible matrix.%
\item{}The matrix equation \(A \vx = \vzero\) has only the trivial solution.%
\item{}The matrix \(A\) has \(n\) pivot columns.%
\item{}Every row of \(A\) contains a pivot.%
\item{}The columns of \(A\) span \(\R^n\).%
\item{}The matrix \(A\) is row equivalent to the identity matrix \(I_n\).%
\item{}The columns of \(A\) are linearly independent.%
\item{}The columns of \(A\) form a basis for \(\R^n\).%
\item{}The matrix transformation \(T\) from \(\R^n\) to \(\R^n\) defined by \(T(\vx) = A\vx\) is one-to-one.%
\item{}The matrix equation \(A \vx = \vb\) has exactly one solution for each vector \(\vb\) in \(\R^n\).%
\item{}The matrix transformation \(T\) from \(\R^n\) to \(\R^n\) defined by \(T(\vx) = A\vx\) is onto.%
\item{}There is an \(n \times n\) matrix \(C\) so that \(AC = I_n\).%
\item{}There is an \(n \times n\) matrix \(D\) so that \(DA = I_n\).%
\item{}The scalar 0 is not an eigenvalue of \(A\).%
\item{}The matrix \(A^{\tr}\) is invertible.%
\item{}\(\Nul A = \{\vzero\}\).%
\item{}\(\Col A = \R^n\).%
\item{}\(\displaystyle \dim(\Col A) = n\)%
\item{}\(\displaystyle \dim(\Nul A) = 0\)%
\item{}\(\displaystyle \rank(A) = n\)%
\end{enumerate}
%
\end{theorem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_base_dim_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp15154440}%
Let \(W = \left\{ \left[ \begin{array}{c} r+s+u \\ r+3s+2t-u\\ -s-t+u \\ s+t-u \end{array} \right] : r,s,t,u \in \R \right\}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why \(W\) is a subspace of \(\R^4\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp15167880}{}\quad{}We can write any vector in \(W\) in the form%
\begin{equation*}
\left[ \begin{array}{c} r+s+u \\ r+3s+2t-u\\ -s-t+u \\ s+t-u \end{array}  \right] = r\left[ \begin{array}{c} 1 \\ 1 \\ 0 \\ 0 \end{array}  \right] + s\left[ \begin{array}{r} 1 \\ 3\\ -1 \\ 1 \end{array}  \right] + t\left[ \begin{array}{r} 0 \\ 2 \\ -1 \\ 1 \end{array}  \right] + u\left[ \begin{array}{r} 1 \\ -1 \\ 1 \\ -1 \end{array}  \right]\text{,}
\end{equation*}
so%
\begin{equation*}
W = \Span\left\{ \left[ \begin{array}{c} 1 \\ 1 \\ 0 \\ 0 \end{array}  \right], \left[ \begin{array}{r} 1 \\ 3\\ -1 \\ 1 \end{array}  \right] , \left[ \begin{array}{r} 0 \\ 2 \\ -1 \\ 1 \end{array}  \right], \left[ \begin{array}{r} 1 \\ -1 \\ 1 \\ -1 \end{array}  \right]\right\}\text{.}
\end{equation*}
As a span of a set of vectors in \(\R^4\), \(W\) is a subspace of \(\R^4\).%
\item{}Find a basis for \(W\) and determine the dimension of \(W\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp15164680}{}\quad{}Let \(A = \left[ \begin{array}{crrr} 1\amp 1\amp 0\amp 1\\1\amp 3\amp 2\amp -1\\0\amp -1\amp -1\amp 1\\0\amp 1\amp 1\amp -1 \end{array} \right]\). To find a basis for \(W\), we note that the reduced row echelon form of \(A\) is \(\left[ \begin{array}{ccrr} 1\amp 0\amp -1\amp 2\\0\amp 1\amp 1\amp -1\\0\amp 0\amp 0\amp 0\\0\amp 0\amp 0\amp 0 \end{array} \right]\). Since the pivot columns of \(A\) form a basis for \(\Col A = W\), we conclude that%
\begin{equation*}
\left\{ \left[ \begin{array}{c} 1\\1\\0\\0 \end{array} \right], \left[ \begin{array}{r} 1\\3\\-1\\1 \end{array} \right] \right\}
\end{equation*}
is a basis for \(W\). Therefore, \(\dim(W) = 2\).%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp15169544}%
Find a basis and the dimension of the solution set to the system%
\begin{alignat*}{5}
{}r  \amp {}+{}   \amp {}s     \amp {}-{}  \amp {}t  \amp {}+{}  \amp {2}u \amp = 0\amp {}\\
{3}r  \amp {}-{}   \amp {}s     \amp {}+{}  \amp {2}t  \amp {}-{}  \amp {}u \amp = 0\amp {}\\
{}r  \amp {}-{}   \amp {3}s   \amp {}+{}  \amp {4}t  \amp {}-{}  \amp {5}u \amp = 0\amp {}\\
r  \amp {}-{}   \amp {3}s   \amp {}+{}  \amp {5}t  \amp {}-{}  \amp {4}u \amp = 0\amp {.}\text{.}
\end{alignat*}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp15169800}{}\quad{}The coefficient matrix of this system is%
\begin{equation*}
A = \left[ \begin{array}{crrr} 1\amp 1\amp -1\amp 2\\3\amp -1\amp 2\amp -1\\1\amp -3\amp 4\amp -5\\5\amp -3\amp 5\amp -4 \end{array}  \right]\text{,}
\end{equation*}
and the solution set to the system is \(\Nul A\). To find a basis for \(\Nul A\) we row reduce \(A\) to%
\begin{equation*}
\left[  \begin{array}{ccrc} 1\amp 0\amp \frac{1}{4}\amp \frac{1}{4} \\0\amp 1\amp -\frac{5}{4}\amp \frac{7}{4} \\ 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
%
\par
The general solution to the system has the form%
\begin{equation*}
\left[ \begin{array}{c}r\\s\\t\\u \end{array}  \right] = \left[   \begin{array}{c}-\frac{1}{4}t-\frac{1}{4}u\\\frac{5}{4}t-\frac{7}{4}u\\t\\u \end{array}  \right] =  t\left[   \begin{array}{r}-\frac{1}{4}\\\frac{5}{4}\\1\\0 \end{array}  \right] + u \left[   \begin{array}{r}-\frac{1}{4}\\-\frac{7}{4}\\0\\1 \end{array}  \right]\text{,}
\end{equation*}
so \(\left\{\left[   \begin{array}{r}-\frac{1}{4}\\\frac{5}{4}\\1\\0 \end{array}  \right], \left[   \begin{array}{r}-\frac{1}{4}\\-\frac{7}{4}\\0\\1 \end{array}  \right] \right\}\) is a basis for \(\Nul A\) and \(\dim(\Nul A) = 2\).%
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_base_dim_summ}
The key idea in this section is the \terminology{dimension} of a vector space.%
\begin{itemize}[label=\textbullet]
\item{}Any two bases for a vector space \emph{must} contain the same number of vectors. Therefore, we can define the \terminology{dimension} of a vector space \(W\) to be the number of vectors in any basis for \(W\).%
\item{}If \(W\) is a subspace of \(\R^n\) with dimension \(k\) and \(S\) is any linearly independent subset of \(W\) with \(k\) vectors, then \(S\) is a basis for \(W\).%
\item{}If \(W\) is a subspace of \(\R^n\) with dimension \(k\) and \(S\) is any subset of \(W\) with \(k\) vectors that spans \(W\), then \(S\) is a basis for \(W\).%
\item{}The rank of a matrix is the dimension of its column space.%
\item{}The Rank-Nullity Theorem states that if \(A\) is an \(m \times n\) matrix, then \(\rank(A) + \nullity(A) = n\).%
\end{itemize}
%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_base_dim_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp14924936}%
Let \(A = \left[ \begin{array}{ccccr} 1\amp 3\amp 1\amp 2\amp 0\\ 0\amp 0\amp 0\amp 0\amp 1 \\ 2\amp 6\amp 0\amp 0\amp 1\\ 1\amp 3\amp 2\amp 4\amp 1 \\ 3\amp 9\amp 1\amp 2\amp -1\\ 3\amp 9\amp 3\amp 6\amp 1 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find a basis for \(\Col A\). What is the dimension of \(\Col A\)? What, then, is the dimension of \(\Nul A\)?%
\item{}Find a basis for \(\Nul A\) and verify the dimension you found in part (a).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp14928392}%
Let \(A = \left[ \begin{array}{crc} 2\amp -1\amp 1\\ 1\amp 0\amp 1 \\ 1\amp -1\amp 2 \end{array} \right]\). The eigenvalues of \(A\) are \(1\) and \(2\). Find the dimension of each eigenspace of \(A\).%
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp15290248}%
Let \(A = \left[ \begin{array}{rrrr} 1\amp 2\amp -1\amp -1 \\ -2\amp -4\amp 2\amp 2 \\ 1\amp 2\amp -1\amp -1 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find a basis for \(\Col A\). What is the rank of \(A\)?%
\item{}Find a basis for \(\Nul A\). What is the nullity of \(A\).%
\item{}Verify the Rank-Nullity Theorem for \(A\).%
\item{}The row space of \(A\) is the span of the rows of \(A\). Find a basis for the row space of \(A\) and the dimension of the row space of \(A\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp15296136}%
Let \(A\) be an \(m \times n\) matrix with \(r\) pivots, where \(r\) is less than or equal to both \(m, n\). Fill in the blanks.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}The null space of \(A\) is a subspace of \fillintext{10}.%
\item{}The column space of \(A\) is a subspace of \fillintext{10}.%
\item{}Suppose \(r=m\). Then there is a pivot in every \fillintext{10} and \(\Col A =\) \fillintext{10}.%
\item{}Suppose \(r=n\). Then there is a pivot in every \fillintext{10} and \(\Nul A=\)\fillintext{10}.%
\item{}If \(A\) has 3 pivots, then the rank of \(A\) is \fillintext{10}.%
\item{}If \(A\) has 3 pivots, then the number of free variables in the system \(A \vx = \vzero\) is \fillintext{10}.%
\item{}The dimension of \(\Col A\) is equal to the number of \fillintext{10}, i.e. \(\dim \Col A =\)\fillintext{10}.%
\item{}The dimension of \(\Nul A\) is equal to the number of \fillintext{10}, i.e. \(\dim \Nul A =\)\fillintext{10}.%
\item{}\(\dim(\Nul A) + \dim(\Col A) =\) \fillintext{10}.%
\item{}Suppose the columns of \(A\) span \(\R^m\). Then rank \(A\) is \fillintext{10}.%
\item{}Suppose the columns of \(A\) are linearly independent. Then \(r=\) \fillintext{10} and the dimension of \(\Nul A\) is \fillintext{10}.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp15320200}%
Prove the remaining parts of the Invertible Matrix Theorem (\hyperref[x:theorem:thm_3_d_IMT]{Theorem~{\xreffont\ref{x:theorem:thm_3_d_IMT}}}). Let \(A\) be an \(n \times n\) matrix.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Prove that \(A\) is invertible if and only if \(\dim(\Nul A)= 0\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp15322376}{}\quad{}What are the solutions to \(A \vx = \vzero\)?%
\item{}Prove that \(A\) is invertible if and only if \(\dim(\Col A) = n\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp15320840}{}\quad{}Use part (a) and the Rank-Nullity Theorem.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp15320456}%
We can convert the language of the Rank-Nullity Theorem to matrix transformation language, as we show in this exercise. Let \(T\) be the matrix transformation defined by the matrix \(A\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}How is the kernel of \(T\) related to \(A\)?%
\item{}How is the range of \(T\) related to \(A\)?%
\item{}How is the domain of \(T\) related to \(A\)?%
\item{}Explain why the Rank-Nullity Theorem says that \(\dim(\Ker(T)) + \dim(\Range(T)) = \dim(\Domain(T))\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp15327752}%
Let \(W\) be a subspace of \(\R^4\). What are possible values for the dimension of \(W\)? Explain. What are the geometric descriptions of \(W\) in each case?%
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp15340168}%
Is it possible to find two subspaces \(W_1\) and \(W_2\) in \(\R^3\) such that \(W_1 \cap W_2=\{ \vzero \}\) and \(\dim W_1=\dim W_2=2\)? If possible, give an example and justify that they satisfy the conditions. If not possible, explain why not.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp15334280}{}\quad{}Dimension two leads to two linearly independent vectors in each of \(W_i\).%
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp15338120}%
Determine the dimensions of the column space and null space of \(\left[ \begin{array}{ccccc} 1\amp 2\amp 4\amp 3\amp 2 \\ 1\amp 0\amp 2\amp 1\amp 4 \\ 1\amp 1\amp 3\amp 1\amp 2 \\ 1\amp 0\amp 2\amp 2\amp 5 \end{array} \right]\).%
\end{divisionexercise}%
\begin{divisionexercise}{10}{}{}{g:exercise:idp15348360}%
If possible, find a \(3\times 4\) matrix whose column space has dimension 3 and null space has dimension 1. Explain how you \emph{found} the matrix in addition to explaining why your answer works. If not possible, explain why it is not possible to find such a matrix.%
\end{divisionexercise}%
\begin{divisionexercise}{11}{}{}{g:exercise:idp15348744}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}If possible, find a \(5\times 5\) matrix whose column space has the same dimension as its null space. Explain how you \emph{found} the matrix in addition to explaining why your answer works. If not possible, explain why it is not possible to find such a matrix.%
\item{}If possible, find a matrix \(A\) so that \(\Col A = \Nul A\). Explain how you \emph{found} the matrix in addition to explaining why your answer works. If not possible, explain why it is not possible to find such a matrix.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{12}{}{}{g:exercise:idp15342600}%
In this exercise we examine why the dimension of a row space of a matrix is the same as the dimension of the column space of the matrix. Let \(A\) be an \(m \times n\) matrix.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why row operations do not change the row space of a matrix. Then explain why if \(R\) is the reduced row echelon form of \(A\), then \(\Row R = \Row A\), where \(\Row M\) is the row space of the matrix \(M\).%
\item{}Explain why the rows of \(R\) that contain pivots form a basis for \(\Row R\), and also of \(\Row A\).%
\item{}Explain why \(\rank(A)\) is the number of pivots in the matrix \(A\). Then explain why \(\dim(\Row A) = \dim(\Col A)\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{13}{}{}{g:exercise:idp15350024}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
The dimension of the column space of a \(3\times 2\) matrix can be three.%
\item{}\lititle{True\slash{}False.}\par%
There exists a \(3\times 3\) matrix whose column space has equal dimension as the null space.%
\item{}\lititle{True\slash{}False.}\par%
If a set of vectors spans a subspace, then that set is a basis of this subspace.%
\item{}\lititle{True\slash{}False.}\par%
If a linearly independent set of vectors spans a subspace, then that set is a basis of this subspace.%
\item{}\lititle{True\slash{}False.}\par%
The dimension of a space is the minimum number of vectors needed to span that space.%
\item{}\lititle{True\slash{}False.}\par%
The dimension of the null space of a \(3\times 2\) matrix can at most be 2.%
\item{}\lititle{True\slash{}False.}\par%
Any basis of \(\R^4\) contains 4 vectors.%
\item{}\lititle{True\slash{}False.}\par%
If \(n\) vectors span \(\R^n\), then these vectors form a basis of \(\R^n\).%
\item{}\lititle{True\slash{}False.}\par%
Every line in \(\R^n\) is a one-dimensional subspace of \(\R^n\).%
\item{}\lititle{True\slash{}False.}\par%
Every plane through origin in \(\R^n\) is a two-dimensional subspace of \(\R^n\).%
\item{}\lititle{True\slash{}False.}\par%
In \(\R^n\) any \(n\) linearly independent vectors form a basis.%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: The GGH Cryptosystem}
\typeout{************************************************}
%
\begin{sectionptx}{Project: The GGH Cryptosystem}{}{Project: The GGH Cryptosystem}{}{}{x:section:sec_proj_ggh_crypto}
A cryptographic system (or cryptosystem) allows for secure communication between two or more parties. These systems take messages (called \terminology{plaintext}) and encrypt them in some way to produce what is called \terminology{ciphertext}. This is the scrambled information that is transmitted to the receiver, from which it should not be possible for someone who does not have the proper key to recover the original message. When the message is received by the intended recipient, it must be unscrambled or \terminology{decrypted}. Decryption is the process of converting ciphertext back to plaintext.%
\par
The Goldreich-Goldwasser-Halevi (GGH) public key cryptosystem\footnote{Published in 1997 by Oded Goldreich, Shafi Goldwasser, and Shai Halevi.\label{g:fn:idp15378824}} uses lattices to encrypt plaintext. The security of the system depends on the fact that the Closest Vector Problem (CVP) is, in general, a very hard problem. To begin to understand these cryptosystems, we begin with lattices.%
\par
Lattices are closely related to spans of sets of vectors in \(\R^n\). If we start with a linearly independent set \(S = \{\vb_1, \vb_2, \ldots, \vb_m\}\) in \(\R^n\), we can create the span of \(S\) \textemdash{} the set of all linear combinations%
\begin{equation*}
c_1 \vb_1 + c_2 \vb_2 + \cdots + c_m \vb_m\text{,}
\end{equation*}
where \(c_1\), \(c_2\), \(\ldots\), \(c_m\) are real numbers. This span creates a subspace of \(\R^n\). If we restrict the set from which we choose the coefficients, we can create different types of structures. An important one is a lattice. The \emph{lattice} \(\mathcal{L}(S)\) defined by the linearly independent set \(S = \{\vb_1, \vb_2, \ldots, \vb_m\}\) is the set of linear combinations%
\begin{equation*}
c_1 \vb_1 + c_2 \vb_2 + \cdots + c_m \vb_m\text{,}
\end{equation*}
where \(c_1\), \(c_2\), \(\ldots\), \(c_m\) are integers. If the vectors in \(S\) have integer components, then every point in \(\mathcal{L}(S)\) will have integer entries. In these cases, \(\mathcal{L}(S)\) is a subset of \(\Z^n\), as illustrated in \hyperref[x:figure:F_Lattice_2]{Figure~{\xreffont\ref{x:figure:F_Lattice_2}}}. Also, if \(m = n\) we say that the lattice \(\mathcal{L}(S)\) is \terminology{full-rank}. We will restrict ourselves to full-rank lattices in this project. A \terminology{basis} for a lattice is any set of linearly independent vectors that generates the lattice. There is a little special notation that is often used with lattices. If \(\CB =  \{\vb_1, \vb_2, \ldots, \vb_n\}\) is a basis for \(\R^n\), we associate to \(\CB\) the matrix \(B = [\vb_1 \ \vb_2 \ \vb_3 \ \cdots \ \vb_n]\). We then use the notation \(\mathcal{L}(B)\) to also refer to the lattice defined by \(\CB\).%
\begin{project}{}{x:project:act_lattices_1}%
We explore lattices in more detail in this activity.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(S_1 = \{[1 \ 1]^{\tr}, [-1 \ 1]^{\tr}\}\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Find five distinct vectors in \(\mathcal{L}(S_1)\).%
\item{}Is the vector \([1 \ 0]^{\tr}\) in \(\mathcal{L}(S_1)\)? Justify your answer.%
\item{}We can draw pictures of lattices by plotting the terminal points of the lattice vectors. Draw all of the lattice points in \(\mathcal{L}(S_1)\) on the square with vertices \((-4,-4)\), \((4,-4)\), \((4,4)\), and \((-4,4)\).%
\end{enumerate}
\item{}Now let \(S_2 = \{[3 \ 5]^{\tr}, [1 \ 2]^{\tr}\}\). A picture of \(\mathcal{L}(S_2)\) is shown in \hyperref[x:figure:F_Lattice_2]{Figure~{\xreffont\ref{x:figure:F_Lattice_2}}} with the basis vectors highlighted. As we have seen, \(\mathcal{L}(S_1)\) is not the entire space \(\Z^2\). Is \(\mathcal{L}(S_2) = \Z^2\)? Justify your answer. \begin{figureptx}{The lattice \(\mathcal{L}(S_2)\).}{x:figure:F_Lattice_2}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/Lattice_2.pdf}
\end{image}%
\tcblower
\end{figureptx}%
%
\end{enumerate}
\end{project}%
\hyperref[x:project:act_lattices_1]{Project Activity~{\xreffont\ref{x:project:act_lattices_1}}} shows that even if \(\CB\) is a basis for \(\R^n\), it does not follow that \(\mathcal{L}(\CB)\) is all of \(\Z^n\). So latices can be complicated, and problems in lattice theory can be very difficult.%
\par
The GGH cryptosystem relies on the fact that we can convert ``good'' bases for lattices into ``bad'' bases. We will not delve into the details of what separates a ``good'' basis from a ``bad'' one, but suffice it to say that a good basis is one in which the basis vectors are close to being perpendicular\footnote{This is also a good property in vector spaces. We will see in a later section that perpendicular basis vectors make calculations in vector spaces relatively easy. A similar thing is true in lattices, where we are able to solve certain variants of closest vector problem very efficiently.\label{g:fn:idp15405960}} and are all short (that is, they have small norms), while any other basis is a bad basis. An example of a good basis is the basis \(S_1\) for \(\R^2\) in \hyperref[x:project:act_lattices_1]{Project Activity~{\xreffont\ref{x:project:act_lattices_1}}}, and we will see later that \(\{[-2 \ 8]^{\tr}, [-1 \ 3]^{\tr}\}\) is a bad basis for the same lattice. You should draw a picture of the vectors \([-2 \ 8]^{\tr}\) and \([-1 \ 3]^{\tr}\) to convince yourself that this is a bad basis for its lattice.%
\par
The GGH cryptosystem works with two \terminology{keys} \textemdash{} a public key and a private key. The keys are based on lattices. The general process of the GGH cryptosystem is as follows. Begin with a good basis \(\CB = \{\vb_1 \ \vb_2 \ \ldots \ \vb_n\}\) of \(\R^n\) of vectors with integer components. Let \(B = [\vb_1 \ \vb_2 \ \cdots \ \vb_n]\) be the matrix associated with \(\CB\). Let \(\CB' = \{\vb'_1, \vb'_2, \ldots, \vb'_n\}\) be a bad basis for which \(\mathcal{L}(\CB') = \mathcal{L}(\CB)\). Let \(B' = [\vb'_1 \ \vb'_2 \ \cdots \ \vb'_n]\) be the matrix associated to the basis \(\CB'\). The bad basis can be shared with anyone (the public key), but the good basis is kept secret (the private key). Start with a message \(\vm = [m_1 \ m_2 \ \cdots \ m_n]^{\tr}\) with integer entries to send.%
\par
First we encrypt the message, which can be done by anyone who has the public key \(\CB'\).%
\begin{itemize}[label=\textbullet]
\item{}Create the message vector%
\begin{equation*}
\vm' = m_1 \vb'_1 +  m_2 \vb'_2 + \cdots +  m_n \vb'_n = B' \vm
\end{equation*}
that is in the lattice using the bad basis \(\CB'\).%
\item{}Choose a small error \(\ve\) to add to \(\vm'\) to move \(\vm'\) off the lattice (small enough so that \(\vm'\) does not pass by another lattice point). This is an important step that will make the message difficult to decrypt without the key. Let \(\vc = \vm' + \ve = B'\vm + \ve\). The vector \(\vc\) is the ciphertext that is to be transmitted to the receiver.%
\end{itemize}
%
\par
Only someone who knows the basis \(\CB\) can decode the ciphertext. This is done as follows.%
\begin{itemize}[label=\textbullet]
\item{}Find the vector \(\va = a_1 \vb_1 + a_2 \vb_2 + \cdots + a_n \vb_n\) in the good basis \(\CB\) that is closest to \(\vc\).%
\item{}We interpret the vector \([a_1 \ a_2 \ \ldots \ a_n]^{\tr}\) as being the encoded vector without the error. So to recreate the original message vector we need to undo the encrypting using the bad basis \(\CB'\). That is, we need to find the weights \(y_1\), \(y_2\), \(\ldots\), \(y_n\) such that%
\begin{equation*}
[a_1 \ a_2 \ \ldots \ a_n]^{\tr} = y_1 \vb'_1 + y_2 \vb'_2 + \cdots + y_n \vb'_n = B' [y_1 \ y_2 \ \cdots \ y_n]^{\tr}\text{.}
\end{equation*}
We can do this by as \([y_1 \ y_2 \ \cdots \ y_n]^{\tr} = B'^{-1} [a_1 \ a_2 \ \ldots \ a_n]^{\tr}\).%
\end{itemize}
%
\par
There are several items to address before we can implement this algorithm. One is how we create a bad basis \(\CB'\) from \(\CB\) that produces the same lattice. Another is how we find the vector in \(\mathcal{\CB}\) closest to a given vector. The latter problem is called the Closest Vector Problem (CVP) and is, in general, a very difficult problem. This is what makes lattice-based cryptosystems secure. We address the first of these items in the next activity, and the second a bit later.%
\begin{project}{}{x:project:act_lattices_2}%
Consider again the basis \(S_1 = \{[1 \ 1]^{\tr}, [-1 \ 1]^{\tr}\}\) from \hyperref[x:project:act_lattices_1]{Project Activity~{\xreffont\ref{x:project:act_lattices_1}}}, and let \(B = \left[ \begin{array}{cr} 1\amp -1\\1\amp 1 \end{array} \right]\) be the matrix whose columns are the vectors in \(S_1\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(T\) be the triangle with vertices \((0,0)\), \((1,1)\), and \((-1,1)\). Show that this triangle is a right triangle, and conclude that the vectors \(\vv_1 = [1 \ 1]^{\tr}\), and \(\vv_2 = [-1 \ 1]^{\tr}\) are perpendicular.%
\item{}Let \(U = \left[ \begin{array}{cc} 3\amp 1\\5\amp 2 \end{array} \right]\). Let \(S_3\) be the set whose vectors are the columns of the matrix \(B_1U\). Show that \(\mathcal{L}(S_1) = \mathcal{L}(S_3)\).%
\end{enumerate}
\end{project}%
The two bases \(S_1 = \{[1 \ 1]^{\tr}, [-1 \ 1]^{\tr}\}\) and \(S_3 =\{[-2 \ 8]^{\tr}, [-1 \ 3]^{\tr}\}\) from \hyperref[x:project:act_lattices_2]{Project Activity~{\xreffont\ref{x:project:act_lattices_2}}} are shown in \hyperref[x:figure:F_Lattice_3]{Figure~{\xreffont\ref{x:figure:F_Lattice_3}}}. This figure illustrates how the matrix \(U\) transforms the basis \(S_1\), in which the vectors are perpendicular and short, to one in which the vectors are nearly parallel and significantly longer. So the matrix \(U\) converts the ``good'' basis \(S_1\) into a ``bad'' basis \(S_2\), keeping the lattice intact. This is a key idea in the GGH cryptosystem. What makes this work is the fact that both \(U\) and \(U^{-1}\) have integer entries. The reason for this is that, for a \(2 \times 2\) matrix \(U = \left[ \begin{array}{cc} a\amp b\\c\amp d \end{array} \right]\), we know that \(U^{-1} = \frac{1}{ad-bc} \left[ \begin{array}{rr} d\amp -b\\-c\amp a \end{array} \right]\). If \(U\) has integer entries and \(ad-bc = \pm 1\), then \(U^{-1}\) will also have integer entries. The number \(ad-bc\) is called the \emph{determinant} of \(U\), and matrices with determinant of \(1\) or \(-1\) are called \emph{unimodular}. That what happened in \hyperref[x:project:act_lattices_2]{Project Activity~{\xreffont\ref{x:project:act_lattices_2}}} happens in the general case is the focus of the next activity.%
\begin{figureptx}{The lattices \(\mathcal{L}(S_1)\) and \(\mathcal{L}(S_3)\).}{x:figure:F_Lattice_3}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/Lattice_3.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\begin{project}{}{x:project:act_lattices_3}%
We will restrict ourselves to \(2 \times 2\) matrices in this activity, but the results generalize to \(n \times n\) matrices. Let \(\CB = \{\vb_1, \vb_2\}\) and \(\CB' = \{\vb'_1, \vb'_2\}\) be bases for \(\R^2\) with integer entries, and let \(B = [\vb_1 \ \vb_2]\) and \(B' = [\vb'_1 \ \vb'_2]\) be the matrices associated to these bases. Show that if \(B' = BU\) for some unimodular matrix \(U\) with integer entries, then \(\mathcal{L}(B) = \mathcal{L}(B')\).%
\end{project}%
\hyperref[x:project:act_lattices_3]{Project Activity~{\xreffont\ref{x:project:act_lattices_3}}} is the part we need for our lattice-based cryptosysystem. Although we won't show it here, the converse of the statement in \hyperref[x:project:act_lattices_3]{Project Activity~{\xreffont\ref{x:project:act_lattices_3}}} is also true. That is, if \(\CB\) and \(\CB'\) generate the same lattice, then \(B' = BU\) for some unimodular matrix \(U\) with integer entries.%
\par
There is one more item to address before we implement the GGH cryptosystem. That item is how to solve the Closest Vector Problem. There are some algorithms for approximating the closest vector in a basis. One is Babai's Closest Vector algorithm. This algorithm works in the following way. Consider a lattice with basis \(\{\vb_1, \vb_2, \ldots, \vb_n\}\). To approximate the closest vector in the lattice to a vector \(\vw\), find the weights \(c_1\), \(c_2\), \(\ldots\), \(c_n\) in \(\R\) such that \(\vw = c_1\vb_1 + c_2 \vb_2 + \cdots + c_n \vb_n\). Then round the coefficients to the nearest integer. This algorithm works well for a good basis, but is unlikely to return a lattice point that is close to \(\vw\) if the basis is a bad one.%
\par
Now we put this all together to illustrate the GGH algorithm.%
\begin{figureptx}{Decrypting an encrypted message.}{x:figure:F_lattices_GGH_ex}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/GGH_example.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\begin{project}{}{x:project:act_lattices_GGH}%
Let \(\CB = \{[5 \ 0]^{\tr}, [0 \ 3]^{\tr}\}\) be the private key, and let \(B = \left[ \begin{array}{cc}5\amp 0\\0\amp 3 \end{array} \right]\) be the matrix whose columns are the vectors in \(\CB\). Let \(U\) be the unimodular matrix \(U = \left[ \begin{array}{cc} 2\amp 3\\3\amp 5 \end{array} \right]\). Let \(\vm = [3 \ -2]^{\tr}\) be our message and let \(\ve = [1 \ -1]^{\tr}\) be our error vector.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Use the unimodular matrix \(U\) to create the bad basis \(\CB'\).%
\item{}Determine the ciphertext message \(\vc\).%
\item{}A picture of the message vector \(\vm\) and the ciphertext vector \(\vc\) are shown in \hyperref[x:figure:F_lattices_GGH_ex]{Figure~{\xreffont\ref{x:figure:F_lattices_GGH_ex}}}. Although the closest vector in the lattice to \(\vc\) can be determined by the figure, actual messages are constructed in high dimensional spaces where a visual approach is not practical. Use Babai's algorithm to find the vector in \(\mathcal{L}(\CB)\) that is closest to \(\vc\) and compare to \hyperref[x:figure:F_lattices_GGH_ex]{Figure~{\xreffont\ref{x:figure:F_lattices_GGH_ex}}}.%
\item{}The final step in the GGH scheme is to recover the original message. Complete the GGH algorithm to find this message.%
\item{}The GGH cryptosystem works because the CVP can be reasonable solved using a good basis. That is, Babai's algorithm works if our basis is a good basis. To illustrate that a bad basis will not allow us to reproduce the original message vector, show that Babai's algorithm does not return the closest vector to \(\vc\) using the bad basis \(\CB'\).%
\end{enumerate}
\end{project}%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 16 Coordinate Vectors and Change of Basis}
\typeout{************************************************}
%
\begin{chapterptx}{Coordinate Vectors and Change of Basis}{}{Coordinate Vectors and Change of Basis}{}{}{x:chapter:chap_coordinate_vectors}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp15470088}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}How do we find the coordinate vector of a vector \(\vx\) with respect to a basis \(\CB = \{\vv_1, \vv_2, \ldots, \vv_n\}\)?%
\item{}What is a change of basis matrix?%
\item{}Why is a change of basis useful?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Describing Orbits of Planets}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Describing Orbits of Planets}{}{Application: Describing Orbits of Planets}{}{}{x:section:sec_appl_orbits}
Consider a planet orbiting the sun (or an object like a satellite orbiting the Earth). According to Kepler's Laws, we assume an elliptical orbit. There are many different ways to describe this orbit, and which description we use depends on our perspective and the application. One important perspective is to make the description of the orbit as simple as possible for earth-based observations. Two problems arise. One is that the earth's orbit and the orbit of the planet do not lie in the same plane. A second problem is that it is complicated to describe the orbit of a planet using the perspective of the plane of the earth's orbit. A reasonable approach, then, is to establish two different coordinate systems, one for the earth's orbit and one for the planet's orbit. We can then use a change of basis to move back and forth from these two perspectives.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_cob_intro}
In this section we will investigate how a basis in \(\R^n\) provides a coordinate system in which each vector in \(\R^n\) has a unique set of coordinates. In this way, each basis will provide us with a new perspective to visualize \(\R^n\). Then we will see how coordinate vectors can allow us to find change of basis matrices that we can use to easily switch between coordinate systems. We begin our analysis of coordinate systems by looking at how a basis in \(\R^2\) gives us a different view of \(\R^2\).%
\begin{exploration}{}{x:exploration:pa_3_e}%
Two vectors \(\vv_1\) and \(\vv_2\) are shown in \hyperref[x:figure:F_PA_5d_1]{Figure~{\xreffont\ref{x:figure:F_PA_5d_1}}}.%
\begin{figureptx}{Vectors \(\vv_1\) and \(\vv_2\).}{x:figure:F_PA_5d_1}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/5_d_PA_Coords.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why \(\CB = \{\vv_1, \vv_2\}\) is a basis for \(\R^2\).%
\item{}Draw the vector \(\vb = -3\vv_1 + 2\vv_2\) in \hyperref[x:figure:F_PA_5d_1]{Figure~{\xreffont\ref{x:figure:F_PA_5d_1}}}. Explain your process.%
\item{}In part (2) we were given the weights (\(-3\) and 2) of the linear combination of \(\vv_1\) and \(\vv_2\) that produced \(\vb\). We call the vector \(\left[ \begin{array}{r} -3 \\2 \end{array} \right]\) the \terminology{coordinate vector} of \(\vb\) with respect to the basis \(\{\vv_1, \vv_2\}\). Explain why any vector \(\vb\) in \(\R^2\) can be written as a linear combination of vectors \(\vv_1, \vv_2\). This shows that each vector in \(\R^2\) has a coordinate vector in the coordinate system defined by \(\vv_1\) and \(\vv_2\).%
\item{}Since \(\{\vv_1, \vv_2\}\) is a basis for \(\R^2\), any vector \(\vb\) in \(\R^2\) has a coordinate vector in the coordinate system defined by \(\vv_1\) and \(\vv_2\). But we also need to make sure that each vector has a unique coordinate vector. Explain why there is no vector in \(\R^2\) which has two different coordinate vectors.%
\item{}We can think of the vectors \(\vv_1\) and \(\vv_2\) as defining a coordinate system, with \(\Span\{\vv_1\}\) as the ``\(x\)'' -axis and \(\Span\{\vv_2\}\) as the ``\(y\)'' -axis. Any vector \(\vb\) in \(\R^2\) can be written uniquely in the form%
\begin{equation*}
\vb = x_1 \vv_1 + x_2 \vv_2
\end{equation*}
and the weights serve as the coordinates of \(\vb\) in the \(\vv_1\), \(\vv_2\) coordinate system. In this case the coordinate vector \(\left[ \begin{array}{c} x_1 \\ x_2 \end{array}  \right]\) of \(\vb\) with respect to the basis \(\CB = \{\vv_1, \vv_2\}\) is written as \([\vb]_{\CB}\). Let \(\CB = \left\{ \left[ \begin{array}{r} -1 \\ 4 \end{array}  \right], \left[ \begin{array}{c} 2 \\ 0 \end{array}  \right]\right\}\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Show that \(\CB\) is a basis for \(\R^2\).%
\item{}Find \([\vb]_{\CB}\) if \(\vb = \left[ \begin{array}{c} 0 \\ 6 \end{array} \right]\). Draw a picture to illustrate how \(\vb\) is expressed as a linear combination of the vectors in \(\CB\).%
\end{enumerate}
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Bases as Coordinate Systems in \(\R^n\)}
\typeout{************************************************}
%
\begin{sectionptx}{Bases as Coordinate Systems in \(\R^n\)}{}{Bases as Coordinate Systems in \(\R^n\)}{}{}{x:section:sec_coor_base}
Bases are useful for many reasons. A basis provides us with a unique representation of the elements in \(\R^n\) as linear combinations of the basis vectors in the coordinate system defined by the vectors.%
\par
As we saw in \hyperref[x:exploration:pa_3_e]{Preview Activity~{\xreffont\ref{x:exploration:pa_3_e}}}, we can think of a basis \(\CB\) of \(\R^2\) as determining a coordinate system of \(\R^2\). For example, let \(\CB = \{\vv_1, \vv_2\}\) where \(\vv_1 = \left[ \begin{array}{r} 2 \\ -2 \end{array} \right]\) and \(\vv_2 = \left[ \begin{array}{c} 1 \\ 3 \end{array} \right]\). The vector \(\vb = \left[ \begin{array}{r} -4 \\ 12 \end{array} \right]\) can be written as \(\vb = -3\vv_1 + 2\vv_2\). \hyperref[x:figure:F_3_e_1]{Figure~{\xreffont\ref{x:figure:F_3_e_1}}} shows that if we plot the point that is \(-3\) units in the \(\vv_1\) direction (where a ``unit'' is a copy of \(\vv_1\)) and \(2\) units in the \(\vv_2\) direction, then the result is the vector from the origin to point defined by \(\vb\).%
\begin{figureptx}{A Coordinate System Defined by a Basis.}{x:figure:F_3_e_1}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/5_d_Coords_basis.pdf}
\end{image}%
\tcblower
\end{figureptx}%
As discussed in \hyperref[x:exploration:pa_3_e]{Preview Activity~{\xreffont\ref{x:exploration:pa_3_e}}}, we can think of the vectors \(\vv_1\) and \(\vv_2\) as defining a coordinate system, with \(\Span\{\vv_1\}\) as the ``\(x\)'' -axis and \(\Span\{\vv_2\}\) as the ``\(y\)'' -axis. Since \(\CB\) is a basis, any vector \(\vb\) in \(\R^2\) can be written uniquely in the form%
\begin{equation*}
\vb = x_1 \vv_1 + x_2 \vv_2
\end{equation*}
and the weights serve as the coordinates of \(\vb\) in the \(\vv_1\), \(\vv_2\) coordinate system. We call the vector \(\left[ \begin{array}{c} x_1 \\ x_2 \end{array}  \right]\) the \terminology{coordinate vector} of \(\vb\) with respect to the basis \(\CB\) and write this vector as \([\vb]_{\CB}\).%
\par
This is actually a familiar idea, one we have used for years. The standard coordinates of a vector \(\va = \left[ \begin{array}{c} a_1 \\ a_2 \end{array} \right]\) in \(\R^2\) are just the coordinates of \(\va\) with respect to the standard basis \(\{\ve_1, \ve_2\}\) of \(\R^2\).%
\par
While we can draw pictures in \(\R^2\), there is no reason to restrict this idea to \(\R^2\).%
\begin{definition}{}{g:definition:idp15525256}%
\index{coordinate vector with respect to a basis in \(\R^n\)}%
\index{coordinates with respect to a basis in \(\R^n\)}%
Let \(\CB = \{\vv_1, \vv_2, \ldots, \vv_n\}\) be a basis for \(\R^n\) for some positive integer \(n\). For any vector \(\vx\) in \(\R^n\), the \terminology{coordinate vector} of \(\vx\) with respect to \(\CB\) is the vector%
\begin{equation*}
[\vx]_{\CB} = \left[ \begin{array}{c} x_1 \\ x_2 \\ \vdots \\ x_n \end{array}  \right]\text{,}
\end{equation*}
where%
\begin{equation*}
\vx = x_1 \vv_1 + x_2 \vv_2 + \cdots + x_n \vv_n\text{.}
\end{equation*}
%
\par
The scalars \(x_1\), \(x_2\), \(\ldots\), \(x_n\) are the \terminology{coordinates of the vector \(\vx\) with respect to the basis}.%
\end{definition}
Recall that there is exactly one way to write a vector as a linear combination of basis vectors, so there is only one coordinate vector of a given vector with respect to a basis. Therefore, the coordinate vector of any vector with respect to a basis is well-defined.%
\begin{activity}{}{x:activity:act_3_e_1}%
Let \(\CS = \{[1 \ 0 \ 0]^{\tr},[0 \ 1 \ 0]^{\tr}, [0 \ 0 \ 1]^{\tr}\}\) and \(\CB = \{[1 \ 0 \ 0]^{\tr}, [3 \ 2 \ -1]^{\tr}, [1 \ 1 \ -1]^{\tr}\}\). Assume that \(\CS\) and \(\CB\) are bases for \(\R^3\). Find \(\left[\left[ \begin{array}{c} 3\\ 7 \\ 0 \end{array} \right]\right]_{\CS}\) and \(\left[\left[ \begin{array}{c} 3\\ 7 \\ 0 \end{array} \right]\right]_{\CB}\). Note that the coordinate vector depends on the basis that is used.%
\end{activity}%
\begin{paragraphs}{IMPORTANT NOTE.}{g:paragraphs:idp15537416}%
We have defined the coordinate vector of a vector \(\vx\) in \(\R^n\) with respect to a basis \(\CB = \{\vv_1, \vv_2, \ldots, \vv_n\}\) as the vector \([x_1 \ x_2 \ \ldots \ x_n]^{\tr}\) if%
\begin{equation*}
\vx = x_1 \vv_1 + x_2 \vv_2 + \cdots + x_n \vv_n\text{.}
\end{equation*}
%
\par
Until now we have listed a basis as a set without regard to the order in which the basis elements are written. That is, the set \(\{\vv_1, \vv_2\}\) is the same as the set \(\{\vv_2, \vv_1\}\). Notice, however, that if we change the order of the vectors in our basis, say from \(\{\vv_1, \vv_2, \ldots, \vv_n\}\) to \(\{\vv_2, \vv_1, \vv_3, \ldots, \vv_n\}\), then the coordinate vector of \(\vx\) with respect to \(\CB\) will be different. To avoid this problem, when discussing coordinate vectors we will consider our bases to be \emph{ordered bases}, so that the order in which we write the elements in our basis is fixed. So, for example, the ordered basis \(\{\vv_1, \vv_2, \ldots, \vv_n\}\) is different than the ordered basis \(\{\vv_2, \vv_1, \vv_3, \ldots, \vv_n\}\).%
\end{paragraphs}%
\par
Coordinate vectors behave nicely with respect to addition and multiplication by scalars. The next activity illustrates this in \(\R^2\).%
\begin{activity}{}{x:activity:act_3_e_coor_vect_props}%
Let \(\CB = \{\vb_1, \vb_2\}\) be a basis for \(\R^2\). Let \(\vv\) and \(\vw\) be vectors in \(\R^2\) with \([\vv]_{\CB} = [x_1 \ x_2]^{\tr}\) and \([\vw]_{\CB} = [y_1 \ y_2]^{\tr}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Determine the components of the vector \([ \vv + \vw]_{\CB}\). How is \([ \vv + \vw]_{\CB}\) related to \([ \vv]_{\CB}\) and \([\vw]_{\CB}\)?%
\item{}Let \(c\) be any scalar. Determine the components of the vector \([ c\vv]_{\CB}\). How is \([ c\vv]_{\CB}\) related to \(c\) and \([ \vv]_{\CB}\)%
\end{enumerate}
\end{activity}%
\hyperref[x:activity:act_3_e_coor_vect_props]{Activity~{\xreffont\ref{x:activity:act_3_e_coor_vect_props}}} suggests that the following theorem is true for coordinate vectors. The verification is left to the exercises.%
\begin{theorem}{}{}{x:theorem:thm_3_e_coord_vector}%
Let \(n\) be a positive integer and let \(\CB\) be a basis for \(\R^n\). If \(\vv\) and \(\vw\) are in \(\R^n\) and \(a\) and \(b\) are any scalars, then%
\begin{equation*}
[a\vv+b\vw]_{\CB} = a[\vv]_{\CB} + b[\vw]_{\CB}\text{.}
\end{equation*}
%
\end{theorem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Change of Basis in \(\R^n\)}
\typeout{************************************************}
%
\begin{sectionptx}{Change of Basis in \(\R^n\)}{}{Change of Basis in \(\R^n\)}{}{}{x:section:sec_cob_rn}
In calculus we change coordinates, from rectangular to polar, for example, to make certain calculations easier. In order for us to be able to work effectively in different coordinate systems, and to easily change back and forth as needed, we will want to have a way to effectively transition from one coordinate system to another. In other words, if we have two different bases for \(\R^n\), we want a straightforward way to translate between the coordinate vectors of any given vector in \(\R^n\) with respect to the two bases.%
\begin{activity}{}{x:activity:pa_3_e_2}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(\vb_1 = [4 \ 2]^{\tr}\), \(\vb_2 = [-6 \ 8]^{\tr}\), \(\vc_1 = [1 \ 1 ]^{\tr}\), \(\vc_2 = [1 \ -1]^{\tr}\), and let \(\CB = \{\vb_1, \vb_2\}\) and \(\CC = \{\vc_1, \vc_2\}\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Show that \(\CB\) and \(\CC\) are bases for \(\R^2\).%
\item{}Let \(\vv = 3 \vb_1 + 2 \vb_2\). What is \([\vv]_{\CB}\)?%
\item{}Since \(\CC\) is also a basis for \(\R^2\), there is also a coordinate vector for \(\vv\) with respect to \(\CC\), and it is reasonable to ask how \([\vv]_{\CC}\) is related to \([\vv]_{\CB}\). Recall that coordinate vectors respect linear combinations \textemdash{} that is%
\begin{equation*}
[r\vx + s\vy]_{\CS} = r[\vx]_{\CS} + s[\vy]_{\CS}
\end{equation*}
for any vectors \(\vx\) and \(\vy\) in \(\R^n\)  with basis \(\CS\), and any scalars \(r\) and \(s\). Use the fact that \(\vv = 3 \vb_1 + 2 \vb_2\) and the linearity of the coordinate transformation with respect to the basis \(\CC\) to express \([\vv]_{\CC}\) in terms of \([\vb_1]_{\CC}\) and \([\vb_2]_{\CC}\) (don't actually calculate \([\vb_1]_{\CC}\) and \([\vb_2]_{\CC}\) yet, just leave your result in terms of the symbols \([\vb_1]_{\CC}\) and \([\vb_2]_{\CC}\).)%
\item{}The result of part (c) can be expressed as a matrix-vector product of the form%
\begin{equation*}
[\vv]_{\CC} = P[\vv]_{\CB}\text{.}
\end{equation*}
Describe how the columns of the matrix \(P\) are related to \([\vb_1]_{\CC}\) and \([\vb_2]_{\CC}\).%
\item{}Now calculate \([\vb_1]_{\CC}\), \([\vb_2]_{\CC}\), and \([\vv]_{\CC}\). Determine the entries of the matrix \(P\) and verify in this example that \([\vv]_{\CC} = P[\vv]_{\CB}\).%
\end{enumerate}
\item{}The matrix \(P\) that we constructed in problem (1) allows us to quickly and easily switch from coordinates with respect to a basis \(\CB\) to coordinates with respect to another basis \(\CC\), providing a way to effectively transition from one coordinate system to another as described in the introduction. This matrix \(P\) is called a \terminology{change of basis matrix}. In problem (1) we explained why the change of basis matrix exists, and in this problem we will see another perspective from which to view this matrix. Let \(\CB = \{\vb_1, \vb_2\}\) and \(\CC = \{\vc_1, \vc_2\}\) be two bases for \(\R^2\) (not the specific bases we used earlier in this activity, but any bases). The change of basis matrix \(P\) from \(\CB\) to \(\CC\) has the property that \(P[\vx]_{\CB} = [\vx]_{\CC}\) for every vector \(\vx\) in \(\R^2\). We can determine the entries of \(P\) by applying this formula to specific vectors in \(\R^2\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}What are \([\vb_1]_{\CB}\) and \([\vb_2]_{\CB}\)? Why?%
\item{}If \(A\) is an \(n \times n\) matrix and \(\ve_1\), \(\ve_2\), \(\ldots\), \(\ve_n\) are the standard unit vectors in \(\R^n\) (that is, \(\ve_i\) is the \(i\)th column of the \(n \times n\) identity matrix), then what does the product \(A \ve_i\) tell us about the matrix \(A\)?%
\item{}Combine the results of parts (a) and (b) and the equation \(P[\vx]_{\CB} = [\vx]_{\CC}\) to explain why \(P = [[\vb_1]_{\CC} \ [\vb_2]_{\CC}]\).%
\end{enumerate}
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Change of Basis Matrix in \(\R^n\)}
\typeout{************************************************}
%
\begin{sectionptx}{The Change of Basis Matrix in \(\R^n\)}{}{The Change of Basis Matrix in \(\R^n\)}{}{}{x:section:sec_mtx_cob}
\index{change of basis matrix in \(\R^n\)}%
Suppose we have two different finite bases \(\CB\) and \(\CC\) for \(\R^n\). In \hyperref[x:activity:pa_3_e_2]{Activity~{\xreffont\ref{x:activity:pa_3_e_2}}} we learned how to translate between the two bases in the 2-dimensional case \textemdash{} if \(\CB = \{\vb_1, \vb_2\}\) and \(\CC = \{\vc_1, \vc_2\}\), then the change of basis matrix from \(\CB\) to \(\CC\) is the matrix \([[\vb_1]_{\CC} \ [\vb_2]_{\CC}]\). This result in the 2-dimensional case generalizes to the \(n\)-dimensional case, and we can determine a straightforward method for calculating a change of basis matrix. The essential idea was introduced in \hyperref[x:activity:pa_3_e_2]{Activity~{\xreffont\ref{x:activity:pa_3_e_2}}}.%
\par
Let \(\CB = \{\vb_1, \vb_2, \ldots, \vb_n\}\) and \(\CC = \{\vc_1, \vc_2, \ldots, \vc_n\}\) be two bases for \(\R^n\). If \(\vx\) is in \(\R^n\), we have defined the coordinate vectors \([\vx]_{\CB}\) and \([\vx]_{\CC}\) for \(\vx\) with respect to \(\CB\) and \(\CC\), respectively. Recall that \([\vx]_{\CB} = \left[ \begin{array}{c} x_1 \\ x_2 \\ \vdots \\ x_n \end{array}  \right]\) if%
\begin{equation*}
\vx = x_1 \vb_1 + x_2 \vb_2 + \cdots + x_n \vb_n\text{.}
\end{equation*}
%
\par
To see how to convert from the coordinates of \(\vx\) with respect to \(\CB\) to coordinates of \(\vx\) with respect to \(\CC\), note that%
\begin{align*}
_{\CC} \amp = [x_1 \vb_1 + x_2 \vb_2 + \cdots + x_n \vb_n]_{\CC}\\
\amp = x_1 [\vb_1]_{\CC} + x_2 [\vb_2]_{\CC} + \cdots + x_n [\vb_n]_{\CC}\\
\amp = [[\vb_1]_{\CC} \ [\vb_2]_{\CC}  \  \cdots  \ [\vb_n]_{\CC}]\left[ \begin{array}{c} x_1\\
x_2\\
\vdots\\
x_n \end{array} \right]\\
\amp = [[\vb_1]_{\CC} \ [\vb_2]_{\CC}  \  \cdots  \ [\vb_n]_{\CC}][\vx]_{\CB}\text{.}
\end{align*}
%
\par
So we can convert from coordinates with respect to the basis \(\CB\) to coordinates with respect to the basis \(\CC\) by multiplying \([\vx]_{\CB}\) on the left by the matrix%
\begin{equation*}
[[\vb_1]_{\CC} \ [\vb_2]_{\CC}  \  \cdots  \ [\vb_n]_{\CC}]\text{.}
\end{equation*}
%
\par
This matrix is called the \terminology{change of basis matrix} from \(\CB\) to \(\CC\) and is denoted \(\underset{\CC \leftarrow \CB}{P}\).%
\begin{definition}{}{g:definition:idp15618840}%
\index{change of basis matrix in \(\R^n\)}%
Let \(n\) be a positive integer and let \(\CB = \{\vb_1, \vb_2, \ldots, \vb_n\}\) and \(\CC = \{\vc_1, \vc_2, \ldots, \vc_n\}\) be two bases for \(\R^n\). The \terminology{change of basis matrix} from \(\CB\) to \(\CC\) is the matrix%
\begin{equation*}
\underset{\CC \leftarrow \CB}{P} = [[\vb_1]_{\CC} \ [\vb_2]_{\CC}  \  \cdots  \ [\vb_n]_{\CC}]\text{.}
\end{equation*}
%
\end{definition}
The change of basis matrix allows us to convert from coordinates with respect to one basis to coordinates with respect to another. The result is summarized in the following theorem.%
\begin{theorem}{}{}{x:theorem:thm_3_e_COB}%
Let \(n\) be a positive integer and let \(\CB = \{\vb_1, \vb_2, \ldots, \vb_n\}\) and \(\CC = \{\vc_1, \vc_2, \ldots, \vc_n\}\) be two bases for \(\R^n\). Then%
\begin{equation*}
[\vx]_{\CC} = \underset{\CC \leftarrow \CB}{P}[\vx]_{\CB}
\end{equation*}
for any vector \(\vx\) in \(\R^n\).%
\end{theorem}
One way to find a change of basis matrix is to utilize a basis in which computations are straightforward. The following activity illustrates the process.%
\begin{activity}{}{x:activity:act_3_e_COB_standard_basis}%
Let \(\CS = \{[1 \ 0]^{\tr}, [0 \ 1]^{\tr}\}\) be the standard basis for \(\R^2\). Let \(\CB = \{\vb_1, \vb_2\}\) and \(\CC = \{\vc_1, \vc_2\}\), where \(\vb_1 = [4 \ 1]^{\tr}\), \(\vb_1 =  [2 \ 5]^{\tr}\), \(\vc_1 = [-1 \ 2]^{\tr}\), and \(\vc_2 = [-1 \ -1]^{\tr}\). You may assume that%
\begin{equation*}
\underset{\CC \leftarrow \CB}{P} = \left[ \begin{array}{rr} -1\amp 1 \\ -3\amp -3 \end{array}  \right]\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find \([\vb_1]_{\CS}\), \([\vb_2]_{\CS}\), \([\vc_1]_{\CS}\), and \([\vc_2]_{\CS}\).%
\item{}Row reduce \([[\vc_1]_{\CS} \ [\vc_2]_{\CS} \ | \ [\vb_1]_{\CS} \ [\vb_2]_{\CS}]\). What do you notice?%
\end{enumerate}
\end{activity}%
In general, as \hyperref[x:activity:act_3_e_COB_standard_basis]{Activity~{\xreffont\ref{x:activity:act_3_e_COB_standard_basis}}} suggests, we can use the standard basis to do our work to find a change of basis matrix%
\begin{equation*}
\underset{\CC \leftarrow \CB}{P} = [[\vb_1]_{\CC} \ [\vb_2]_{\CC}  \  \cdots  \ [\vb_n]_{\CC}]
\end{equation*}
from a basis \(\CB = \{\vb_1, \vb_2, \ldots, \vb_n\}\) of \(\R^n\) to a basis \(\CC = \{\vc_1, \vc_2, \ldots, \vc_n\}\) of \(\R^n\). Recall that we need to write \(\vb_i\) as a linear combination of the vectors in \(\CC\). That is, we need to find weights \(x_{1,i}\), \(x_{2,i}\), \(\ldots\), \(x_{n,i}\) so that%
\begin{equation}
\vb_i = x_{1,i}\vc_1 + x_{2,i} \vc_2 + \cdots + x_{n,i} \vc_n\text{.}\label{x:men:eq_3_e_COB}
\end{equation}
%
\par
The weights in equation \hyperref[x:men:eq_3_e_COB]{({\xreffont\ref{x:men:eq_3_e_COB}})} are also the weights that satisfy the equation%
\begin{equation*}
[\vb_i]_{\CS} = x_{1,i}[\vc_1]_{\CS} + x_{2,i} [\vc_2]_{\CS} + \cdots + x_{n,i} [\vc_n]_{\CS}
\end{equation*}
where \(\CS\) is any basis for \(\R^n\). So to find these weights, we choose a convenient basis \(\CS\) (often the standard basis, if one exists, is a good choice) and then row reduce the matrix%
\begin{equation*}
[[\vc_1]_{\CS} \ [\vc_2]_{\CS} \ \cdots \ [\vc_n]_{\CS} \ | \ [\vb_i]_{\CS}]\text{.}
\end{equation*}
%
\par
The row operations we will apply to row reduce the coefficient matrix%
\begin{equation*}
[[\vc_1]_{\CS} \ [\vc_2]_{\CS} \ \cdots \ [\vc_n]_{\CS}]
\end{equation*}
will be the same regardless of the augmented column, so we can solve all of the systems at one time by row reducing the matrix%
\begin{equation*}
[[\vc_1]_{\CS} \ [\vc_2]_{\CS} \ \cdots \ [\vc_n]_{\CS} \ | \ [\vb_1]_{\CS} \ [\vb_2]_{\CS} \ \cdots \ [\vb_n]_{\CS}]\text{.}
\end{equation*}
%
\par
The result of the row reduction will be the matrix%
\begin{equation*}
\left[ I_n \ | \ \underset{\CC \leftarrow \CB}{P} \right]\text{.}
\end{equation*}
%
\par
In particular, if we use the standard basis for \(\R^n\) as our basis \(\CS\), then \([\vv]_{\CS} = \vv\) for any vector \(\vv\). Our change of basis matrix can then be realized by row reducing the matrix%
\begin{equation*}
[\vc_1 \ \vc_2 \ \cdots \ \vc_n \ | \ \vb_1 \ \vb_2 \ \cdots \ \vb_n]\text{.}
\end{equation*}
%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Properties of the Change of Basis Matrix}
\typeout{************************************************}
%
\begin{sectionptx}{Properties of the Change of Basis Matrix}{}{Properties of the Change of Basis Matrix}{}{}{x:section:sec_prop_mtx_cob}
The are many different bases for \(\R^n\), so it is natural to ask how change of bases matrices might be related to one another.%
\begin{activity}{}{x:activity:act_3_e_COB_properties}%
The sets \(\CB = \{[3 \ 0]^{\tr}, [4 \ -1]^{\tr}\}\) and \(\CC = \{[1 \ 2]^{\tr}, [-1 \ 1]^{\tr}\}\) are bases for \(\R^2\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the change of basis matrix \(\underset{\CC \leftarrow \CB}{P}\) from the basis \(\CB\) to the basis \(\CC\).%
\item{}Let \(\vv= [2 \ 4]^{\tr}\). Find \([\vv]_{\CB}\) and \([\vv]_{\CC}\).%
\item{}Verify by matrix multiplication that \([\vv]_{\CC} = \underset{\CC \leftarrow \CB}{P} [\vv]_{\CB}\).%
\item{}Find the change of basis matrix \(\underset{\CB \leftarrow \CC}{P}\) from the basis \(\CC\) to the basis \(\CB\).%
\item{}Verify by matrix multiplication that \([\vv]_{\CB} = \underset{\CB \leftarrow \CC}{P} [\vv]_{\CC}\).%
\item{}How, specifically, are the matrices \(\underset{\CC \leftarrow \CB}{P}\) and \(\underset{\CB \leftarrow \CC}{P}\) related?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp15656216}{}\quad{}If you don't see a relationship right away, what is the product of these two matrices?%
\end{enumerate}
\end{activity}%
\hyperref[x:activity:act_3_e_COB_properties]{Activity~{\xreffont\ref{x:activity:act_3_e_COB_properties}}} seems to indicate that the inverse of a change of basis matrix is also a change of basis matrix, which assumes that a change of basis matrix is always invertible. The following theorem provides some properties about change of basis matrices. The proofs are left for the exercises.%
\begin{theorem}{}{}{x:theorem:thm_3_e_COB_properties}%
Let \(n\) be a positive integer, and let \(\CB\), \(\CC\), and \(\CS\) be bases for \(\R^n\). Then%
\begin{enumerate}
\item{}the change of basis matrix \(\underset{\CC \leftarrow \CB}{P}\) is invertible,%
\item{}\(\underset{\CC \leftarrow \CB}{P}^{-1} = \underset{\CB \leftarrow \CC}{P}\),%
\item{}\(\underset{\CS \leftarrow \CC}{P} \ \underset{\CC \leftarrow \CB}{P} = \underset{\CS \leftarrow \CB}{P}\).%
\end{enumerate}
%
\end{theorem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_cob_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp15668120}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the coordinate vector of \(\vv\) with respect to the ordered basis \(\CB\) in the indicated space.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}\(\CB = \{[1 \ 1]^{\tr}, [2 \ -1]^{\tr}\}\) in \(\R^2\) with \(\vv = [4 \ 1]^{\tr}\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp15668504}{}\quad{}Find the coordinate vector of \(\vv\) with respect to the ordered basis \(\CB\) in the indicated vector space.%
\par
We need to write \(\vv =[4 \ 1]^{\tr}\) as a linear combination of \([1 \ 1]^{\tr}\) and \([2-t]2 \ -1]^{\tr}\). If \([4 \ 1]^{\tr} = c_1([1 \ 1]^{\tr}) + c_2([2 \ -1]^{\tr})\), then equating coefficients of like power terms yields the equations \(4 = c_1 +2c_2\) and \(1 = c_1-c_2\). The solution to this system is \(c_1 = 2\) and \(c_2 = 1\), so \([\vv]_{\CB} = [2 \ 1]^{\tr}\).%
\item{}\(\CB = \left\{[1 \ 0 \ 0 \ 1]^{\tr}, [1 \ 0 \ -1 \ 1]^{\tr}, [1 \ 1 \ 0 \ 1]^{\tr}, [0 \ 0 \ 0 \ 1]^{\tr} \right\}\) in \(\R^4\) with \(\vv = [2 \ 3 \ 1 \ 0]^{\tr}\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp15670808}{}\quad{}Find the coordinate vector of \(\vv\) with respect to the ordered basis \(\CB\) in the indicated vector space.%
\par
We need to write \(\vv\) as a linear combination of the vectors in \(\CB\). If%
\begin{equation*}
\vv = c_1[1 \ 0 \ 0 \ 1]^{\tr} + c_2 [1 \  0 \ -1 \ 1]^{\tr} + c_3 [1 \ 1 \ 0 \ 1]^{\tr} + c_4  [0 \ 0 \ 0  \ 1]^{\tr}\text{,}
\end{equation*}
equating corresponding components produces the system%
\begin{alignat*}{6}
{}c_1   \amp {}+{}  \amp {}c_2  \amp {}+{}  \amp {}c_3  \amp {}{}    \amp {}    \amp {}={}  \amp \ {}\amp 2\amp {}\\
{}    \amp {}{}    \amp {}    \amp {}{}    \amp {}c_3  \amp {}{}    \amp {}    \amp {}={}   \amp \ {}\amp 3\amp {}\\
{}    \amp {}{}    \amp {-}c_2  \amp {}{}    \amp {}    \amp {}{}    \amp {}    \amp {}={}   \amp \ {}\amp 1\amp {}\\
{}c_1    \amp {}+{}  \amp {}c_2  \amp {}+{}  \amp {}c_3  \amp {}+{}  \amp {}c_4  \amp {}={}  \amp  \ {}\amp 0\amp {.}
\end{alignat*}
The solution to this system is \(c_1 = 0\), \(c_2 = -1\), \(c_3 = 3\), and \(c_4 = -2\), so \([\vv]_{\CB} = [0 \ -1 \ 3 \ -2]^{\tr}\).%
\end{enumerate}
\item{}Find the vector \(\vv\) given the basis \(\CB\) and the coordinate vector \([\vv]_{\CB}\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}\(\CB = \{[1\ 0 \ 1]^{\tr}, [1 \ 1 \ 1]^{\tr}, [0 \ 1 \ 1]^{\tr}\}\), \([\vv]_{\CB} = [2 \ 1 \ 3]^{\tr}\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp15678872}{}\quad{}Find the vector \(\vv\) given the basis \(\CB\) of \(V\) and the coordinate vector \([\vv]_{\CB}\).%
\par
Since \([\vv]_{\CB} = [2 \ 1 \ 3]^{\tr}\), it follows that%
\begin{equation*}
\vv =  2[1\ 0 \ 1]^{\tr} + [1 \ 1 \ 1]^{\tr} + 3[0 \ 1 \ 1]^{\tr} = [3 \ 4 \ 6]^{\tr}\text{.}
\end{equation*}
%
\item{}\(\CB = \left\{[0 \ 0 \ 1 \ 1 \ 2]^{\tr}, [-1 \ 3 \ 4 \ 7 \ 0]^{\tr}\right\}\) in \(\Span \ \CB\) with \([\vx]_{\CB} = [2 \ -1]^{\tr}\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp15692440}{}\quad{}Find the vector \(\vv\) given the basis \(\CB\) of \(V\) and the coordinate vector \([\vv]_{\CB}\).%
\par
Since \([\vx]_{\CB} = [2 \ -1]^{\tr}\), it follows that%
\begin{equation*}
\vv = 2[0 \ 0 \ 1 \ 1 \ 2]^{\tr} - [-1 \ 3 \ 4 \ 7 \ 0]^{\tr} = [1 \ -3 \ -6 \ -14 \ 4]^{\tr}\text{.}
\end{equation*}
%
\end{enumerate}
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp15687192}%
Let \(\CB = \{\vb_1, \vb_2, \vb_3\}\) and \(\CC = \{\vc_1, \vc_2, \vc_3\}\), where \(\vb_1 = [1 \ 2 \ 0]^{\tr}\), \(\vb_2 = [0 \ 4 \ -1]^{\tr}\), \(\vb_3 = [3 \ -1 \ 1]^{\tr}\), \(\vc_1 = [2 \ 4 \ 1]^{\tr}\), \(\vc_2 = [1 \ 0 \ -1]^{\tr}\), and \(\vc_3 = [0 \ 1 \ 1]^{\tr}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the change of basis matrix \(\underset{\CC \leftarrow \CB}{P}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp15696792}{}\quad{}To find \(\underset{\CC \leftarrow \CB}{P}\) we row reduce \([\vc_1 \ \vc_2 \ \vc_3 \ | \ \vb_1 \ \vb_2 \ \vb_3]\) and get the matrix%
\begin{equation*}
\left[ \begin{array}{ccc|rrr} 1\amp 0\amp 0\amp 1\amp 5\amp -5\\ 0\amp 1\amp 0\amp -1\amp -10\amp 13\\ 0\amp 0\amp 1\amp -2\amp -16\amp 19 \end{array}  \right]\text{.}
\end{equation*}
So%
\begin{equation*}
\underset{\CC \leftarrow \CB}{P} = \left[ \begin{array}{rrr} 1\amp 5\amp -5\\ -1\amp -10\amp 13\\ -2\amp -16\amp 19 \end{array}  \right]\text{.}
\end{equation*}
%
\item{}Use the change of basis matrix to find \([\vb_1]_{\CC}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp15694744}{}\quad{}Since \([\vb_1]_{\CB} = [1 \ 0 \ 0]^{\tr}\), it follows that%
\begin{equation*}
[\vb_1]_{\CC} =  \underset{\CB \leftarrow \CC}{P} [\vb_1]_{\CB} = [1 \ -1 \ -2]^{\tr}\text{.}
\end{equation*}
A quick check shows that \(\vc_1-\vc_2 - 2\vc_3 = \vb_1\).%
\item{}If \(\vv\) and \(\vw\) are vectors in \(\R^3\) with \([\vv]_{\CB} = [2 \ -2 \ 5]^{\tr}\) and \(\vw = [ 1 \ 3 \ -1]^{\tr}\), find \([2\vv-4\vw]_{\CC}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp15709464}{}\quad{}Properties of the matrix-vector product show that%
\begin{align*}
_{\CC} \amp = \underset{\CB \leftarrow \CC}{P}[2\vv-4\vw]_{\CB}\\
\amp = 2\underset{\CB \leftarrow \CC}{P}[\vv]_{\CB} -4\underset{\CB \leftarrow \CC}{P} [\vw]_{\CB}\\
\amp = 2[ -33 \ 83 \ 123]^{\tr} - 4[21 \ -44 \ -69]^{\tr}\\
\amp = [-150 \ 342 \ 522]^{\tr}\text{.}
\end{align*}
%
\item{}Find the change of basis matrix \(\underset{\CB \leftarrow \CC}{P}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp15709592}{}\quad{}Since \(\underset{\CC \leftarrow \CB}{P} = \underset{\CB \leftarrow \CC}{P}^{-1}\), technology shows that%
\begin{equation*}
\underset{\CC \leftarrow \CB}{P} = \frac{1}{3} \left[ \begin{array}{rrr} 18\amp -15\amp 15\\-7\amp 9\amp -8\\-4\amp 6\amp -5 \end{array}  \right]\text{.}
\end{equation*}
%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_cob_summ}
The key ideas in this section are the coordinate vector with respect to a basis and the change of basis matrix.%
\begin{itemize}[label=\textbullet]
\item{}If \(\CB = \{\vv_1, \vv_2, \vv_3, \ldots, \vv_n\}\) is a basis for \(\R^n\), then the coordinate vector of \(\vx\) with respect to \(\CB\) is the vector%
\begin{equation*}
[\vx]_{\CB} = [x_1 \ x_2 \ \ldots \ x_n]^{\tr}\text{,}
\end{equation*}
where%
\begin{equation*}
\vx = x_1 \vv_1 + x_2 \vv_2 + \cdots + x_n \vv_n\text{.}
\end{equation*}
%
\item{}If \(\CB = \{\vb_1, \vb_2, \ldots, \vb_n\}\) and \(\CC = \{\vc_1, \vc_2, \ldots, \vc_n\}\) are two bases for \(\R^n\), then the change of basis matrix from \(\CB\) to \(\CC\) is the matrix%
\begin{equation*}
\underset{\CC \leftarrow \CB}{P} = [[\vb_1]_{\CC} \ [\vb_2]_{\CC}  \  \cdots  \ [\vb_n]_{\CC}]
\end{equation*}
that satisfies%
\begin{equation*}
[\vx]_{\CC} = \underset{\CC \leftarrow \CB}{P}[\vx]_{\CB}
\end{equation*}
for any vector \(\vx\) in \(\R^n\).%
\item{}Change of basis matrices allow us to effectively and efficiently transition from one coordinate system to another.%
\end{itemize}
%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_cob_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp15712024}%
Let \(\CB=\left\{ \left[ \begin{array}{r} 0\\1\\-1 \end{array} \right], \left[ \begin{array}{c} 1\\2\\0 \end{array} \right] \right\}\) be a basis of the subspace defined by the equation \(y-2x+z=0\). Find the coordinates of the vector \(\vb=\left[ \begin{array}{c} 3\\4\\2 \end{array} \right]\) with respect to the basis \(\CB\).%
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp15723160}%
Let \(\CB=\{[1 \ 1 \ 0]^{\tr}, [2 \ 0 \ 1]^{\tr}, [0 \ 1 \ 1]^{\tr}\}\). Assume that \(\CB\) is a basis of \(\R^3\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}For which vector \(\vv\) is \([\vv]_{\CB} = [ 1 \ -1 \ 3 ]^\tr\)?%
\item{}Determine coordinates of \(\vw = [-1 \ 1 \ 2]^{\tr}\) with respect to the basis \(\CB\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp15725976}%
Find two different bases \(\CB_1\) and \(\CB_2\) of \(\R^2\) so that \([\vb]_{\CB_1} = [\vb]_{\CB_2} = \left[ \begin{array}{c} 2\\1 \end{array} \right]\), where \(\vb=\left[ \begin{array}{c} 5\\3 \end{array} \right]\).%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp15732376}%
If \([\vb_1]_{\CB} = \left[ \begin{array}{c} 1\\2\\1 \end{array} \right]\) and \([\vb_2]_{\CB} = \left[ \begin{array}{c} 2\\1\\2 \end{array} \right]\) with respect to some basis \(\CB\) of \(\R^3\), where \(\vb_1=\left[ \begin{array}{c} 1\\2\\3 \end{array} \right]\) and \(\vb_2=\left[ \begin{array}{c} 2\\1\\3 \end{array} \right]\), what are the coordinates of \(\left[ \begin{array}{r} -2\\3\\1 \end{array} \right]\) with respect to \(\CB\)?%
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp15728920}%
If \([\vb_1]_{\CB} = \left[ \begin{array}{c} 1\\1 \end{array} \right]\) and \([\vb_2]_{\CB} = \left[ \begin{array}{c} 2\\1 \end{array} \right]\) with respect to some basis \(\CB\), where \(\vb_1=\left[ \begin{array}{c} 3\\1\\3 \end{array} \right]\) and \(\vb_2=\left[ \begin{array}{c} 4\\1\\5 \end{array} \right]\), what are the vectors in \(\CB\)?%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp15738904}%
Let \(\CB=\{ \vv_1, \vv_2, \ldots, \vv_n\}\) be a basis for \(\R^n\). Describe how the coordinates of a vector with respect to \(\CB\) will change if \(\vv_1\) is replaced with \(\frac{1}{2}\vv_1\).%
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp15736472}%
Let \(\CB = \{[1 \ 0 \ 0]^{\tr}, [0 \ 1 \ 0]^{\tr}, [1 \ 0 \ 1]^{\tr}\}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(\CB\) is a basis for \(\R^3\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp15741848}{}\quad{}Row reduce an appropriate matrix.%
\item{}Let \(\vv_1 = [1 \ 0 \ 2]^{\tr}\), \(\vv_2=[1 \ 1 \ 2]^{\tr}\), and \(\vv_3 = [2 \ -1 \ 1]^{\tr}\). Find \([\vv_1]_{\CB}\), \([\vv_2]_{\CB}\), and \([\vv_3]_{\CB}\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp15749400}%
Let \(\CB\) be a basis for \(\R^n\), and let \(S = \{\vu_1, \vu_2, \ldots, \vu_k\}\) be a subset of \(\R^n\). Let \(R = \{[\vu_1]_{\CB}, [\vu_2]_{\CB}, \ldots, [\vu_k]_{\CB}\}\) in \(\R^n\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that if \(S\) is linearly independent in \(\R^n\), then \(R\) is linearly independent in \(\R^n\).%
\item{}Is the converse of part (a) true? That is, if \(R\) is linearly independent in \(\R^n\), must \(S\) be linearly independent in \(\R^n\)? Justify your answer.%
\item{}Repeat parts (a) and (b), replacing ``linearly independent'' with ``linearly dependent''.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp15754136}%
Verify \hyperref[x:theorem:thm_3_e_coord_vector]{Theorem~{\xreffont\ref{x:theorem:thm_3_e_coord_vector}}}. That is, let \(n\) be a positive integer and let \(\CB\) be a basis for \(\R^n\). Show that if \(\vv\) and \(\vw\) are in \(\R^n\) and \(a\) and \(b\) are any scalars, then%
\begin{equation*}
[a\vv+b\vw]_{\CB} = a[\vv]_{\CB} + b[\vw]_{\CB}\text{.}
\end{equation*}
%
\end{divisionexercise}%
\begin{divisionexercise}{10}{}{}{g:exercise:idp15760792}%
Calculate the change of basis matrix \(\underset{\CC \leftarrow \CB}{P}\) in each of the following cases.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\CB = \{[1 \ 2 \ -1]^{\tr}, [-1 \ 1 \ 0]^{\tr}, [0 \ 0 \ 1]^{\tr}\}\) and \(\CC = \{[0 \ 1 \ 0]^{\tr}, [1 \ -1 \ 1]^{\tr}, [0 \ 1 \ 1]^{\tr}\}\) in \(\R^3\).%
\item{}\(\CB = \{[1 \ 0 \ 0 \ 1]^{\tr}, [0 \ 1 \ -1 \ 0]^{\tr}, [0 \ 1 \ 0 \ 0]^{\tr}, [0 \ 0 \ 0 \ 1]^{\tr}\}\) and \(\CC = \{[1 \ 0 \ 0 \ 0]^{\tr}, [0 \ 1 \ 0 \ 0]^{\tr}, [1 \ 0 \ 1 \ 0]^{\tr}, [0 \ 1 \ 0 \ 1]^{\tr}\}\) in \(\R^4\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{11}{}{}{g:exercise:idp15765784}%
We can view the matrix transformation that performs a counterclockwise rotation by an angle \(\theta\) around the origin in \(\R^2\) as a change of basis matrix. Let \(\CB = \{\ve_1, \ve_2\}\) be the standard basis for \(\R^2\), and let \(\CC = \{\vv_1, \vv_2\}\), where \(\vv_1 = [\cos(\theta) \ \sin(\theta)]^{\tr}\) and \(\vv_2 = [\cos(\theta+\pi/2) \ \sin(\theta+ \pi/2)]^{\tr}\). Note that \(\vv_1\) is a vector rotated counterclockwise from the positive \(x\)-axis by the angle \(\theta\), and \(\vv_2\) is a vector rotated counterclockwise from the positive \(y\)-axis by the angle \(\theta\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Use necessary trigonometric identities to show that the change of basis matrix from \(\CC\) to \(\CB\) is%
\begin{equation*}
\left[ \begin{array}{cr} \cos(\theta) \amp  - \sin(\theta) \\ \sin(\theta) \amp  \cos(\theta) \end{array}  \right]\text{.}
\end{equation*}
Then find the change of basis matrix from \(\CB\) to \(\CC\).%
\item{}Let \(\vx = \left[ \begin{array}{c} 2\\1 \end{array} \right]\) in \(\R^2\). Find \([\vx]_{\CB}\). Then find \([\vx]_{\CC}\), where \(\CC = \{\vv_1, \vv_2\}\) with \(\theta = 30^{\circ}\). Draw a picture to illustrate how the components of \([\vx]_{\CC}\) determine coordinates of \((2,1)\) in the coordinate system with axes \(\vv_1\) and \(\vv_2\).%
\item{}Let \(\vy\) be the vector such that \([\vy]_{\CC} = [2 \ 3]^{\tr}\). Find \([\vy]_{\CB}\). Draw a picture to illustrate how the components of \([\vy]_{\CB}\) determine coordinates of \(\vy\) in the coordinate system with axes \(\ve_1\) and \(\ve_2\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{12}{}{}{g:exercise:idp15780376}%
A \terminology{permutation matrix} is a change of basis matrix that is obtained when the order of the basis vectors is switched. Let \(\CB = \{\vb_1, \vb_2, \vb_3, \vb_4\}\) and \(\CC = \{\vb_2, \vb_3, \vb_1, \vb_4\}\) be two ordered bases for \(\R^4\). Find \(\underset{\CC \leftarrow \CB}{P}\).%
\end{divisionexercise}%
\begin{divisionexercise}{13}{}{}{g:exercise:idp15784984}%
Let \(\CB = \{[0 \ 1\ 0]^{\tr},[1 \ 1 \ 0]^{\tr}, [0 \ 0 \ 1]^{\tr}\}\) be a basis for \(\R^3\). Suppose \(\CC\) is another basis for \(\R^3\) and%
\begin{equation*}
\underset{\CC \leftarrow \CB}{P} = \left[ \begin{array}{ccr} 1\amp 1\amp 0 \\ 0\amp 2\amp -1 \\ 2\amp 0\amp 2 \end{array}  \right]\text{.}
\end{equation*}
Find the vectors in the basis \(\CC\).%
\end{divisionexercise}%
\begin{divisionexercise}{14}{}{}{g:exercise:idp15786136}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
The coordinates of a non-zero vector cannot be the same in the coordinate systems defined by two different bases.%
\item{}\lititle{True\slash{}False.}\par%
The coordinate vector of the zero vector with respect to any basis is always the zero vector.%
\item{}\lititle{True\slash{}False.}\par%
If \(W\) is a \(k\) dimensional subspace of \(\R^n\), and \(\CB\) is a basis of \(W\), then \([\vw]_{\CB}\) is a vector in \(\R^n\) for any \(\vw\) in \(W\).%
\item{}\lititle{True\slash{}False.}\par%
The order of vectors in a basis do not affect the coordinates of vectors with respect to this basis.%
\item{}\lititle{True\slash{}False.}\par%
If \(\CB\) is a basis for \(\R^n\), then the vector \([\vx]_{\CB}\) is unique to \(\vx\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\CB\) is a basis for \(\R^n\) and \(\vw\) is a vector in \(\R^n\), there is a vector \(\vv\) in \(\R^n\) such that \([\vv]_{\CB} = \vw\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\CB\) is a basis for \(\R^n\) and \(\vv\) is a vector in \(\R^n\), then the additive inverse of \([\vv]_{\CB}\) is the coordinate vector of the additive inverse of \(\vv\).%
\item{}\lititle{True\slash{}False.}\par%
If a coordinate vector of \(\vx\) in \(\R^3\) is \(\left[ \begin{array}{r} 1\\-1\\2 \end{array} \right]\) with respect to some basis, then the coordinate vector of \(2\vx\) is \(\left[ \begin{array}{r} 2\\-2\\4 \end{array} \right]\) with respect to the same basis.%
\item{}\lititle{True\slash{}False.}\par%
If \(\CB\) and \(\CC\) are bases for \(\R^n\), then the columns of \(\underset{\CC \leftarrow \CB}{P}\) span \(\R^n\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\CB\) and \(\CC\) are bases for \(\R^n\), then the rows of \(\underset{\CC \leftarrow \CB}{P}\) span \(\R^n\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\CB\) and \(\CC\) are bases for \(\R^n\), then the columns of \(\underset{\CC \leftarrow \CB}{P}\) are linearly independent.%
\item{}\lititle{True\slash{}False.}\par%
The matrix \([\vb_1 \ \vb_2 \ \cdots \ \vb_n \ | \ \vc_1 \ \vc_2 \ \cdots \ \vc_n]\) row reduces to \([I_n \ | \ \underset{\CC \leftarrow \CB}{P}]\).%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Planetary Orbits and Change of Basis}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Planetary Orbits and Change of Basis}{}{Project: Planetary Orbits and Change of Basis}{}{}{x:section:sec_proj_orbits_cob}
We are interested in determining the orbit of planet that orbits the sun. Finding the equation of such an orbit is not difficult, but just having an equation is not enough. For many purposes, it is important to know where the planet is fro the perspective or earth observation. This is a more complicated question, one we can address through change of bases matrices.\footnote{This project is based on the paper ``Planetary Orbits: Change of Basis in R\(^3\)'', Donald Teets, \pubtitle{Teaching Mathematics and its Applications: An International Journal of the IMA}, Volume 17, Issue 2, 1 June 1998, Pages 66-68.\label{g:fn:idp15968280}}%
\begin{project}{}{x:project:act_orbits_ellipse}%
Since planetary orbits are elliptical, not circular, we need to understand ellipses. An ellipse is a shape like a flattened circle. More specifically, while a circle is the set of points equidistant from a fixed point, and ellipse is a set of points so that the sum of the distances from a point on the ellipse to two fixed points (called foci) is a constant. We can use this definition to derive an equation for an ellipse. We will simplify by rotating and translating an ellipse so that its foci are at points \((-c,0)\) and \((c,0)\), and the constant sum is \(2a\). Let \((x,y)\) be a point on the ellipse as illustrated in \hyperref[x:figure:F_ellipse_cob]{Figure~{\xreffont\ref{x:figure:F_ellipse_cob}}}. Use the fact that the sum of the distances from \((x,y)\) to the foci is \(2a\) to show that \((x,y)\) satisfies the equation%
\begin{equation}
\frac{x^2}{a^2} + \frac{y^2}{b^2} = 1\text{,}\label{x:men:eq_orbit_ellipse}
\end{equation}
where the points \((0,b)\) and \((0,-b)\) are the \(y\) intercepts of the ellipse.%
\begin{figureptx}{An ellipse.}{x:figure:F_ellipse_cob}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/5_e_Ellipse.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\end{project}%
The longer axis of an ellipse is called the major axis and the axis perpendicular to the major axis through the origin is the minor axis. Half of these axes (from the origin) are the semi-major axis and the semi-minor axis. So the parameter \(a\) in \hyperref[x:men:eq_orbit_ellipse]{({\xreffont\ref{x:men:eq_orbit_ellipse}})} is the length of the semi-major axis and the parameter \(b\) is the length of the semi-minor axis. Note that the points \((0,b)\) and \((0,-b)\) are the \(y\) intercepts and the points \((a,0)\) and \((-a,0)\) are the \(x\) intercepts of this ellipse. Note that if \(a\) and \(b\) are equal, then the ellipse is a circle. How far the ellipse deviates from a circle is called the \terminology{eccentricity} (usually denoted as \(e\)) of the ellipse. In other words, the eccentricity is a measure of how flattened en ellipse is, and this is determined by how close \(c\) is to \(a\), or how close the ratio \(\frac{c}{a}\) is to \(1\). Thus, we define the eccentricity of an ellipse by%
\begin{equation*}
e =\frac{c}{a} = \sqrt{1-\frac{b^2}{a^2}}\text{.}
\end{equation*}
%
\par
Now we assume we have a planet (different from the earth) orbiting the sun and we establish how to convert back and forth from the coordinate system of earth's orbit to the coordinate system of the planet's orbit. To do so we need to establish some coordinate systems. We assume the orbit of earth is in the standard \(xy\) plane, with the sun (one of the foci) at the origin. The elliptical orbit of the planet is in some other plane with coordinate axes \(x'\) and \(y'\). The two orbital planes intersect in a line. Let this line be the \(x'\) axis and let \(\alpha\) be the angle the positive \(x'\) axis makes with the positive \(x\) axis. We can represent the elliptical orbit of the planet in the \(x'y'\) plane, but the \(x'\) and \(y'\) axes are not likely to be the best axes for this orbit. So we define a third coordinate system \(x''y''\) in the \(x'y'\) plane so that the origin (the position of the sun) is at one focus of the planet's orbit and the \(x''\) axis is the major axis of the orbit and the \(y''\) axis is the minor axis of the orbit of the planet. The unit vectors \(\vb_1\), \(\vb_2\), and \(\vb_3\) in the positive \(x\), \(y\), and \(z\) directions define a basis \(\CB = \{\vb_1, \vb_2, \vb_3\}\) for \(\R^3\), the unit vectors \(\vb'_1\), \(\vb'_2\), \(\vb'_3\) in the positive \(x'\), \(y'\), and \(z'\) directions define a basis \(\CB' = \{\vb'_1, \vb'_2, \vb'_3\}\) for \(\R^3\), and the unit vectors \(\vb''_1\), \(\vb''_2\), \(\vb''_3\) in the positive \(x''\), \(y''\), and \(z''\) directions define a basis \(\CB'' = \{\vb''_1, \vb''_2, \vb''_3\}\) for \(\R^3\). See \hyperref[x:figure:F_orbits]{Figure~{\xreffont\ref{x:figure:F_orbits}}} for illustrations.%
\begin{figureptx}{Left: The planet's orbit in the \(x''y''\) system. Right: The planes of the planet and earth orbits.}{x:figure:F_orbits}{}%
\begin{sidebyside}{2}{0}{0}{0}%
\begin{sbspanel}{0.5}%
\includegraphics[width=\linewidth]{external/5_e_orbit.pdf}
\end{sbspanel}%
\begin{sbspanel}{0.5}%
\includegraphics[width=\linewidth]{external/5_e_orbit_plane.pdf}
\end{sbspanel}%
\end{sidebyside}%
\tcblower
\end{figureptx}%
Finally, let \(\gamma\) be the angle between the positive \(x'\) axis and the positive \(x''\) axis as shown at left in \hyperref[x:figure:F_orbits]{Figure~{\xreffont\ref{x:figure:F_orbits}}}. Our first step is to find the change of basis matrix from \(\CB''\) to \(\CB'\).%
\begin{project}{}{x:project:act_orbits_COB_1}%
Explain why the change of basis matrix \(\underset{\CB' \leftarrow \CB''}{P}\) is given by%
\begin{equation*}
\underset{\CB' \leftarrow \CB''}{P} = \left[ \begin{array}{ccc} \cos(\gamma)\amp -\sin(\gamma)\amp 0 \\ \sin(\gamma)\amp \cos(\gamma)\amp 0 \\ 0\amp 0\amp 1 \end{array}  \right]\text{.}
\end{equation*}
%
\end{project}%
More complicated is the change of basis matrix from \(\CB'\) to \(\CB\).%
\begin{project}{}{x:project:act_orbits_COB_2}%
Now we look for \(\underset{\CB \leftarrow \CB'}{P} = [[\vb'_1]_{\CB} \ [\vb'_2]_{\CB} \ [\vb'_3]_{\CB}]\). Assume that the plane \(p\) in which the planet's orbit lies has equation \(z = ax+by\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why \(\vb'_1 = \cos(\alpha) \vb_1 + \sin(\alpha) \vb_2\).%
\item{}The \(x'\) axis is the intersection of the plane \(z=ax+by\) with the plane \(z=0\), so the equation of \(x'\) axis in terms of \(x\) and \(y\) is \(ax+by=0\). Now we determine the coordinates of \(\vb'_2\) in terms of the basis \(\CB\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Explain why the vector \([b \ -a \ 0]^{\tr}\) lies on the \(x'\) axis. We take this vector to point in the positive \(x'\) direction. This gives us another representation of \(\vb'_1\) \textemdash{} namely that \(\vb'_1 = \frac{1}{\sqrt{a^2+b^2}} [b \ -a \ 0]^{\tr}\).%
\item{}Explain why a vector in the plane \(z = ax+by\) orthogonal to \(\vb'_1\) is \(\left[a \ b \ a^2+b^2 \right]^{\tr}\).%
\item{}From the previous part we have%
\begin{equation*}
\vb'_2 = \frac{1}{\sqrt{a^2+b^2+(a^2+b^2)^2}} \left[b \ a \ a^2+b^2 \right]^{\tr}\text{.}
\end{equation*}
Let \(G = \left(\frac{b}{\sqrt{a^2+b^2+(a^2+b^2)^2}}, \frac{a}{\sqrt{a^2+b^2+(a^2+b^2)^2}}, 0\right)\) be the terminal point of the projection of \(\vb'_2\) onto the \(xy\) plane. Show that \(\overrightarrow{OG}\) is orthogonal to \(\vb'_1\).%
\item{}Let \(\beta\) be the angle between the plane \(p\) and the \(xy\) plane as illustrated at right in \hyperref[x:figure:F_orbits]{Figure~{\xreffont\ref{x:figure:F_orbits}}}. Explain why \(||\overrightarrow{OG}|| = \cos(\beta)\). Then explain why%
\begin{equation*}
\vb'_2 = [ -\cos(\beta)\sin(\alpha) \ \cos(\beta)\cos(\alpha) \ \sin(\beta)]^{\tr}\text{.}
\end{equation*}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp16025240}{}\quad{}Use the trigonometric identities \(\cos\left(A+\frac{\pi}{2}\right) =  -\sin(A)\) and \(\sin\left(A + \frac{\pi}{2}\right) =\cos(A)\).%
\end{enumerate}
\item{}Finally, we find \([\vb'_3]_{\CB}\). The cross product of \(\vb'_1\) and \(\vb'_2\) is a vector orthogonal to \(\vb'_1\) and \(\vb'_2\), so%
\begin{equation*}
\vb'_3 = \frac{1}{\sqrt{\left(a^2+b^2\right)^2+4a^2b^2}} \left[-a(a^2+b^2) \ -b(a^2+b^2) \ 2ab \right]^{\tr}\text{.}
\end{equation*}
Let \(H\) be the terminal point of the projection of \(\vb'_3\) onto the \(xy\) plane as illustrated at right in \hyperref[x:figure:F_orbits]{Figure~{\xreffont\ref{x:figure:F_orbits}}}.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Explain why the angle between \(\vb'_1\) and \(\overrightarrow{OH}\) is \(\frac{\pi}{2}\).%
\item{}Explain why \(||\overrightarrow{OH}|| = \sin(\beta)\). (Hint: Use the trigonometric identity \(\cos\left(\frac{\pi}{2}-A \right) = \sin(A)\).)%
\item{}Since the angle from \(\vb_1\) to \(\overrightarrow{OH}\) is negative, this angle is \(\alpha-\frac{\pi}{2}\). Use this angle and the previous information to find the coordinates of the point \(H\) and, consequently, explain why%
\begin{equation*}
\vb'_3  = [ \sin(\beta)\sin(\alpha) \ -\sin(\beta)\cos(\alpha) \ \cos(\beta)]^{\tr}\text{.}
\end{equation*}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp16033304}{}\quad{}Use the trigonometric identities \(\cos\left(A - \frac{\pi}{2} \right) = \sin(A)\) and \(\sin\left(A - \frac{\pi}{2}\right) = -\cos(A)\).%
\end{enumerate}
\item{}Explain why the change of basis matrix \(\underset{\CB \leftarrow \CB'}{P}\) from \(\CB'\) to \(\CB\) is%
\begin{equation*}
\underset{\CB \leftarrow \CB'}{P} = \left[ \begin{array}{ccc} \cos(\alpha)\amp -\cos(\beta)\sin(\alpha)\amp \sin(\beta)\sin(\alpha) \\ \sin(\alpha)\amp \cos(\beta)\cos(\alpha)\amp -\sin(\beta)\cos(\alpha) \\ 0\amp \sin(\beta)\amp \cos(\beta) \end{array}  \right]\text{.}
\end{equation*}
%
\end{enumerate}
\end{project}%
\begin{figureptx}{Points on the ellipse in terms of angles.}{x:figure:F_ellipse_polar}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/5_e_ellipse_polar.pdf}
\end{image}%
\tcblower
\end{figureptx}%
With the change of basis matrices we can convert from any one coordinate system to the other. Note that all of the change of basis matrices are written in terms of angles, so it will be convenient to have a way to express points on our ellipses using angles as well. Given any point on an ellipse (or any point in the plane), we can represent the coordinates of that point in terms of the angle \(\theta\) the vector through the origin and the point makes with the positive \(x\)-axis and the distance \(r\) from the origin to the point as shown in \hyperref[x:figure:F_ellipse_polar]{Figure~{\xreffont\ref{x:figure:F_ellipse_polar}}}. In this representation we have \(x = r\cos(\theta)\) and \(y = r \sin(\theta)\).%
\par
So we can start in the \(x''y''\) coordinate system with the coordinate vector of a point \(\left[\overrightarrow{OP}\right]_{\CB''} = [ r\cos(\theta) \ r\sin(\theta) \ 0]^{\tr}\). Then to view this point in the \(xy\) system, we apply the change of basis matrices%
\begin{equation*}
\left[\overrightarrow{OP}\right]_{\CB} = \underset{\CB \leftarrow \CB'}{P} \underset{\CB' \leftarrow \CB''}{P} [ r\cos(\theta) \ r\sin(\theta) \ 0]^{\tr}\text{.}
\end{equation*}
%
\par
Of course we can also covert from \(\CB\) coordinates to \(\CB''\) coordinates by applying the inverses of our change of basis matrices.%
\end{sectionptx}
\end{chapterptx}
\end{partptx}
%
%
\typeout{************************************************}
\typeout{Chapter IV Eigenvalues and Eigenvectors}
\typeout{************************************************}
%
\begin{partptx}{Eigenvalues and Eigenvectors}{}{Eigenvalues and Eigenvectors}{}{}{x:part:part-eigen}
 %
%
\typeout{************************************************}
\typeout{Section 17 The Determinant}
\typeout{************************************************}
%
\begin{chapterptx}{The Determinant}{}{The Determinant}{}{}{x:chapter:chap_determinants}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp16039448}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}How do we calculate the determinant of an \(n \times n\) matrix?%
\item{}What is one important fact the determinant tells us about a matrix?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Area and Volume}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Area and Volume}{}{Application: Area and Volume}{}{}{x:section:sec_appl_area_vol}
Consider the problem of finding the area of a parallelogram determined by two vectors \(\vu\) and \(\vv\), as illustrated at left in \hyperref[x:figure:F_det_area]{Figure~{\xreffont\ref{x:figure:F_det_area}}}.%
\begin{figureptx}{A parallelogram and a parallelepiped.}{x:figure:F_det_area}{}%
\begin{sidebyside}{2}{0}{0}{0}%
\begin{sbspanel}{0.5}%
\includegraphics[width=\linewidth]{external/det_area_2.pdf}
\end{sbspanel}%
\begin{sbspanel}{0.5}%
\includegraphics[width=\linewidth]{external/det_volume_2.pdf}
\end{sbspanel}%
\end{sidebyside}%
\tcblower
\end{figureptx}%
We could calculate this area, for example, by breaking up the parallelogram into two triangles and a rectangle and finding the area of each. Now consider the problem of calculating the volume of the three-dimensional analog (called a \terminology{parallelepiped}) determined by three vectors \(\vu\), \(\vv\), and \(\vw\) as illustrated at right in \hyperref[x:figure:F_det_area]{Figure~{\xreffont\ref{x:figure:F_det_area}}}.%
\par
It is quite a bit more difficult to break this parallelepiped into subregions whose volumes are easy to compute. However, all of these computations can be made quickly by using determinants. The details are later in this section.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_det_intro}
We know that a non-zero vector \(\vx\) is an eigenvector of an \(n \times n\) matrix \(A\) if \(A \vx = \lambda \vx\) for some scalar \(\lambda\). Note that this equation can be written as \((A-\lambda I_n)\vx=\vzero\). Until now, we were given eigenvalues of matrices and have used the eigenvalues to find the eigenvectors. In this section we will learn an algebraic technique to find the eigenvalues ourselves. We will also be able to justify why an \(n\times n\) matrix has at most \(n\) eigenvalues.%
\par
A scalar \(\lambda\) is an eigenvalue of \(A\) if \((A - \lambda I_n)\vx=\vzero\) has a non-trivial solution \(\vx\), which happens if and only if \(A-\lambda I_n\) is not invertible. In this section we will find a scalar whose value will tell us when a matrix is invertible and when it is not, and use this scalar to find the eigenvalues of a matrix.%
\begin{exploration}{}{x:exploration:pa_4_a}%
In this activity, we will focus on \(2\times 2\) matrices. Let \(A = \left[ \begin{array}{cc} a\amp b \\ c\amp d \end{array}  \right]\) be a \(2\times 2\) matrix. To see if \(A\) is invertible, we row reduce \(A\) by replacing row 2 with \(a\cdot\)(row 2) \(- c \cdot\)(row 1):%
\begin{equation*}
\left[ \begin{array}{cc} a\amp b \\ 0\amp ad-bc \end{array}  \right]\text{.}
\end{equation*}
%
\par
So the only way \(A\) can be reduced \(I_2\) is if \(ad - bc \neq 0\). We call this quantity \(ad-bc\) the \terminology{determinant} of \(A\), and denote the determinant of \(A\) as \(\det(A)\) or \(|A|\). When \(\det(A)\neq 0\), we know that%
\begin{equation*}
A^{-1} = \frac{1}{ad-bc} \left[ \begin{array}{rr} d\amp -b \\ -c\amp a \end{array}  \right]\text{.}
\end{equation*}
%
\par
We now consider how we can use the determinant to find eigenvalues and other information about the invertibility of a matrix.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(A = \left[ \begin{array}{cc} 1\amp 2 \\ 2\amp 4 \end{array} \right]\). Find \(\det(A)\) by hand. What does this mean about the matrix \(A\)? Can you confirm this with other methods?%
\item{}One of the eigenvalues of \(A=\left[ \begin{array}{cc} 1\amp 3 \\ 2\amp 2 \end{array} \right]\) is \(\lambda=4\). Recall that we can rewrite the matrix equation \(A\vx=4\vx\) in the form \((A-4I_2) \vx = \vzero\). What must be true about \(A-4I_2\) in order for 4 to be an eigenvalue of \(A\)? How does this relate to \(\det(A-4I_2)\)?%
\item{}Another eigenvalue of \(A=\left[ \begin{array}{cc} 1\amp 3 \\ 2\amp 2 \end{array} \right]\) is \(\lambda=-1\). What must be true about \(A+I_2\) in order for \(-1\) to be an eigenvalue of \(A\)? How does this relate to \(\det(A+I_2)\)?%
\item{}To find the eigenvalues of the matrix \(A=\left[ \begin{array}{cc} 3\amp 2\\2\amp 6 \end{array} \right]\), we rewrite the equation \(A \vx = \lambda \vx\) as \((A - \lambda I_2) \vx = \vzero\). The coefficient matrix of this last system has the form \(A-\lambda I_2 = \left[ \begin{array}{cc} 3-\lambda \amp 2 \\ 2\amp 6-\lambda \end{array} \right]\). The determinant of this matrix is a quadratic expression in \(\lambda\). Since the eigenvalues will occur when the determinant is 0, we need to solve a quadratic equation. Find the resulting eigenvalues. (Note: One of the eigenvalues is 2.)%
\item{}Can you explain why a \(2\times 2\) matrix can have at most two eigenvalues?%
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Determinant of a Square Matrix}
\typeout{************************************************}
%
\begin{sectionptx}{The Determinant of a Square Matrix}{}{The Determinant of a Square Matrix}{}{}{x:section:sec_det_square}
Around 1900 or so determinants were deemed much more important than they are today. In fact, determinants were used even before matrices. According to Tucker\footnote{Tucker, Alan. (1993). The Growing Importance of Linear Algebra in Undergraduate Mathematics. \pubtitle{The College Mathematics Journal}, 1, 3-9.\label{g:fn:idp16079256}} determinants (not matrices) developed out of the study of coefficients of systems of linear equations and were used by Leibniz 150 years before the term matrix was coined by J. J. Sylvester in 1848. Even though determinants are not as important as they once were, the determinant of a matrix is still a useful quantity. We saw in \hyperref[x:exploration:pa_4_a]{Preview Activity~{\xreffont\ref{x:exploration:pa_4_a}}} that the determinant of a matrix tells us if the matrix is invertible and how it can help us find eigenvalues. In this section, we will see how to find the determinant of any size matrix and how to use this determinant to find the eigenvalues.%
\par
The determinant of a \(2 \times 2\) matrix \(A = \left[ \begin{array}{cc} a\amp b \\ c\amp d \end{array}  \right]\) is \(\det(A)=ad-bc\). The matrix \(A\) is invertible if and only if \(\det(A) \neq 0\). We will use a recursive approach to find the determinants of larger size matrices building from the \(2\times 2\) determinants. We present the result in the \(3 \times 3\) case here \textemdash{} a more detailed analysis can be found at the end of this section.%
\par
To find the determinant of a \(3 \times 3\) matrix \(A = \left[ \begin{array}{ccc} a_{11}\amp a_{12}\amp a_{13} \\ a_{21}\amp a_{22}\amp a_{23}\\ a_{31}\amp a_{32}\amp a_{33} \end{array}  \right]\), we will use the determinants of three \(2\times 2\) matrices. More specifically, the determinant of \(A\), denoted \(\det(A)\) is the quantity%
\begin{equation}
a_{11} \det\left(\left[ \begin{array}{cc} a_{22}\amp a_{23} \\ a_{32}\amp a_{33} \end{array}  \right] \right)- a_{12} \det \left(\left[ \begin{array}{cc} a_{21}\amp a_{23} \\ a_{31}\amp a_{33} \end{array}  \right] \right) + a_{13} \det \left(\left[ \begin{array}{cc} a_{21}\amp a_{22} \\ a_{31}\amp a_{32} \end{array}  \right] \right)\text{.}\label{x:men:eq_det_3by3}
\end{equation}
%
\par
This sum is called a \terminology{cofactor expansion} of the determinant of \(A\). The smaller matrices in this expansion are obtained by deleting certain rows and columns of the matrix \(A\). In general, when finding the determinant of an \(n\times n\) matrix, we find determinants of \((n-1)\times (n-1)\) matrices, which we can again reduce to smaller matrices to calculate.%
\par
We will use the specific matrix%
\begin{equation*}
A = \left[ \begin{array}{ccc} 1 \amp  2 \amp  0 \\ 1 \amp  4 \amp  3 \\ 2 \amp  2 \amp  1 \end{array}  \right]
\end{equation*}
as an example in illustrating the cofactor expansion method in general.%
\begin{itemize}[label=\textbullet]
\item{}We first pick a row or column of \(A\). We will pick the first row of \(A\) for this example.%
\item{}For each entry in the row (or column) we choose, in this case the first row, we will calculate the determinant of a smaller matrix obtained by removing the row and the column the entry is in. Let \(A_{ij}\) be the smaller matrix found by deleting the \(i\)th row and \(j\)th column of \(A\). For entry \(a_{11}\), we find the matrix \(A_{11}\) obtained by removing first row and first column:%
\begin{equation*}
A_{11} = \left[ \begin{array}{cc} 4 \amp  3 \\ 2 \amp  1 \end{array}  \right]\,\text{.}
\end{equation*}
For entry \(a_{12}\), we find%
\begin{equation*}
A_{12} = \left[ \begin{array}{cc} 1 \amp  3 \\ 2 \amp  1 \end{array}  \right]\,\text{.}
\end{equation*}
Finally, for entry \(a_{13}\), we find%
\begin{equation*}
A_{13} = \left[ \begin{array}{cc} 1 \amp  4 \\ 2 \amp  2 \end{array}  \right] \,\text{.}
\end{equation*}
%
\item{}Notice that in the \(3 \times 3\) determinant formula in \hyperref[x:men:eq_det_3by3]{({\xreffont\ref{x:men:eq_det_3by3}})} above, the middle term had a (-) sign. The signs of the terms in the cofactor expansion alternate within each row and each column. More specifically, the sign of a term in the \(i\)th row and \(j\)th column is \((-1)^{i+j}\). We then obtain the following pattern of the signs within each row and column:%
\begin{equation*}
\left[ \begin{array}{cccc} + \amp  - \amp  + \amp  \cdots \\ - \amp  + \amp  - \amp  \cdots \\ + \amp  - \amp  + \amp  \cdots \\ \vdots \amp  \amp  \amp \end{array}  \right]
\end{equation*}
In particular, the sign factor for \(a_{11}\) is \((-1)^{1+1}=1\), for \(a_{12}\) is \((-1)^{1+2}=-1\), and for \(a_{13}\) is \((-1)^{1+3}=1\).%
\item{}For each entry \(a_{ij}\) in the row (or column) of \(A\) we chose, we multiply the entry \(a_{ij}\) by the determinant of \(A_{ij}\) and the sign \((-1)^{i+j}\). In this case, we obtain the following numbers%
\begin{equation*}
a_{11} (-1)^{1+1} \det(A_{11})  = 1 \det \left[ \begin{array}{cc} 4 \amp  3 \\ 2 \amp  1 \end{array}  \right] = 1(4-6)=-2
\end{equation*}
%
\begin{equation*}
a_{12} (-1)^{1+2} \det(A_{12}) = -2 \det \left[ \begin{array}{cc} 1 \amp  3 \\ 2 \amp  1 \end{array}  \right] = -2(1-6)=10
\end{equation*}
%
\begin{equation*}
a_{13} (-1)^{1+3} \det(A_{13}) = 0
\end{equation*}
Note that in the last calculation, since \(a_{13}=0\), we did not have to evaluate the rest of the terms.%
\item{}Finally, we find the determinant by adding all these values:%
\begin{align*}
\det(A) \amp = a_{11} (-1)^{1+1} \det(A_{11}) + a_{12} (-1)^{1+2} \det(A_{12})\\
\amp \qquad + a_{13} (-1)^{1+3} \det(A_{13})\\
\amp = 8\text{.}
\end{align*}
%
\end{itemize}
%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Cofactors}
\typeout{************************************************}
%
\begin{sectionptx}{Cofactors}{}{Cofactors}{}{}{x:section:sec_cofactors}
\index{matrix!minor}%
\index{cofactor}%
We will now define the determinant of a general \(n \times n\) matrix \(A\) in terms of a cofactor expansion as we did in the \(3 \times 3\) case. To do so, we need some notation and terminology.%
\begin{itemize}[label=\textbullet]
\item{}We let \(A_{ij}\) be the submatrix of \(A = [a_{ij}]\) found by deleting the \(i\)th row and \(j\)th column of \(A\). The determinant of \(A_{ij}\) is called the \(ij\)th \terminology{minor} of \(A\) or the minor corresponding to the entry \(a_{ij}\).%
\item{}Notice that in the \(3 \times 3\) case, we used the opposite of the 1,2 minor in the sum. It will be the case that the terms in the cofactor expansion will alternate in sign. We can make the signs in the sum alternate by taking \(-1\) to an appropriate power. As a result, we define the \(ij\)th \terminology{cofactor} \(C_{ij}\) of \(A\) as%
\begin{equation*}
C_{ij} = (-1)^{i+j} \det\left(A_{ij}\right)\text{.}
\end{equation*}
%
\item{}Finally, we define the determinant of \(A\).%
\end{itemize}
%
\begin{definition}{}{g:definition:idp16125336}%
\index{determinant}%
If \(A=[a_{ij}]\) is an \(n \times n\) matrix, the \terminology{determinant} of \(A\) is the scalar%
\begin{equation*}
\det(A) = a_{11}C_{11} + a_{12}C_{12} + a_{13}C_{13} + \cdots + a_{1n}C_{1n}
\end{equation*}
where \(C_{ij}= (-1)^{i+j} \det(A_{ij})\) is the \(ij\)-cofactor of \(A\) and \(A_{ij}\) is the matrix obtained by removing row \(i\) and column \(j\) of matrix \(A\).%
\end{definition}
This method for computing determinants is called the \terminology{cofactor expansion} or \terminology{Laplace expansion} of \(A\) along the 1st row. The cofactor expansion reduces the computation of the determinant of an \(n \times n\) matrix to \(n\) computations of determinants of \((n-1) \times (n-1)\) matrices. These smaller matrices can be reduced again using cofactor expansions, so it can be a long and grueling process for large matrices. It turns out that we can actually take this expansion along any row or column of the matrix (a proof of this fact is given in \hyperref[x:chapter:chap_det_properties]{Section~{\xreffont\ref{x:chapter:chap_det_properties}}}). For example, the cofactor expansion along the 2nd row is%
\begin{equation*}
\det(A) = a_{21}C_{21} + a_{22}C_{22} + \cdots + a_{2n}C_{2n}
\end{equation*}
and along the 3rd column the formula is%
\begin{equation*}
\det(A) = a_{13}C_{13} + a_{23}C_{23} + \cdots + a_{n3}C_{n3}\text{.}
\end{equation*}
%
\par
Note that when finding a cofactor expansion, choosing a row or column with many zeros makes calculations easier.%
\begin{activity}{}{x:activity:act_4_a_1}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(A = \left[ \begin{array}{rcr} 1\amp 2\amp -1 \\ -2\amp 0\amp 4 \\ 6\amp 3\amp 0 \end{array} \right]\). Use the cofactor expansion along the first row to calculate the determinant of \(A\) by hand.%
\item{}Calculate \(\det(A)\) by using a cofactor expansion along the second row where \(A = \left[ \begin{array}{ccc} 1 \amp 4 \amp 2 \\ 0 \amp 2 \amp 0 \\ 2 \amp 5 \amp 3 \end{array} \right]\).%
\item{}Calculate the determinant of \(\left[ \begin{array}{crr} 1\amp -2\amp 3 \\ 0\amp 4\amp -3 \\ 0\amp 0\amp 8 \end{array} \right]\).%
\item{}Which determinant property can be used to calculate the determinant in part (c)? Explain how. (Determinant properties are included below for easy reference.)%
\item{}Consider the matrix \(A=\left[ \begin{array}{ccc} 1 \amp 1 \amp 2 \\ 0 \amp 2 \amp 1 \\ 1 \amp 2 \amp 2 \end{array} \right]\). Let \(B\) be the matrix which results when \(c\) times row 1 is added to row 2 of \(A\). Evaluate the determinant of \(B\) by hand to check that it is equal to the determinant of \(A\), which verifies one other determinant property (in a specific case).%
\end{enumerate}
\end{activity}%
As with any new idea, like the determinant, we must ask what properties are satisfied. We state the following theorem without proof for the time being. For the interested reader, the proof of many of these properties is given in \hyperref[x:chapter:chap_det_properties]{Section~{\xreffont\ref{x:chapter:chap_det_properties}}} and others in the exercises.%
\begin{theorem}{}{}{x:theorem:thm_determinant_properties}%
Given \(n\times n\) matrices \(A, B\), the following hold:%
\begin{enumerate}
\item{}\(\det (AB) = \det (A) \cdot \det (B)\), and in particular \(\det (A^k) = (\det A)^k\) for any positive integer \(k\).%
\item{}\(\det (A^{\tr}) = \det (A)\).%
\item{}\(A\) is invertible if and only if \(\det (A) \neq 0\).%
\item{}If \(A\) is invertible, then \(\det (A^{-1}) = (\det A)^{-1}\).%
\item{}For a \(2\times 2\) matrix \(A=\begin{bmatrix}a \amp b \\ c \amp d \end{bmatrix}\), \(\det (A) = ad-bc\).%
\item{}If \(A\) is upper\slash{}lower triangular, then \(\det (A)\) is the product of the entries on the diagonal.%
\item{}The determinant of a matrix is the product of the eigenvalues, with each eigenvalue repeated as many times as its multiplicity.%
\item{}Effect of row operations:%
\begin{itemize}[label=\textbullet]
\item{}Adding a multiple of a row to another does NOT change the determinant of the matrix.%
\item{}Multiplying a row by a constant multiplies the determinant by the same constant.%
\item{}Row swapping multiplies the determinant by \((-1)\).%
\end{itemize}
%
\item{}If the row echelon form \(U\) of \(A\) is obtained by adding multiples of one row to another, and row swapping, then \(\det (A)\) is equal to \(\det (U)\) multiplied by \((-1)^r\) where \(r\) is the number of row swappings done during the row reduction.%
\end{enumerate}
%
\end{theorem}
Note that if we were to find the determinant of a \(4\times 4\) matrix using the cofactor method, we will calculate determinants of 4 matrices of size \(3\times 3\), each of which will require 3 determinant calculations again. So, we will need a total of 12 calculations of determinants of \(2\times 2\) matrices. That is a lot of calculations. There are other, more efficient, methods for calculating determinants. For example, we can row reduce the matrix, keeping track of the effect that each row operation has on the determinant.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Determinant of a \(3 \times 3\) Matrix}
\typeout{************************************************}
%
\begin{sectionptx}{The Determinant of a \(3 \times 3\) Matrix}{}{The Determinant of a \(3 \times 3\) Matrix}{}{}{x:section:sec_det_3by3}
Earlier we defined the determinant of a \(3 \times 3\) matrix. In this section we endeavor to understand the motivation behind that definition.%
\par
We will repeat the process we went through in the \(2 \times 2\) case to see how to define the determinant of a \(3 \times 3\) matrix. Let%
\begin{equation*}
A =  \left[ \begin{array}{ccc} a_{11}\amp a_{12}\amp a_{13} \\ a_{21}\amp a_{22}\amp a_{23}\\ a_{31}\amp a_{32}\amp a_{33} \end{array}  \right]\text{.}
\end{equation*}
%
\par
To find the inverse of \(A\) we augment \(A\) by the \(3 \times 3\) identity matrix%
\begin{equation*}
[A \ | \ I_3] = \left[ \begin{array}{cccccc} a_{11}\amp a_{12}\amp a_{13}\amp 1\amp 0\amp 0 \\ a_{21}\amp a_{22}\amp a_{23}\amp 0\amp 1\amp 0 \\ a_{31}\amp a_{32}\amp a_{33}\amp 0\amp 0\amp 1 \end{array}  \right]
\end{equation*}
and row reduce the matrix (using appropriate technology) to obtain%
\begin{equation*}
\left[ \begin{array}{rrrrrr} 1\amp 0\amp 0\amp \ds \frac{a_{33}a_{22}-a_{32}a_{23}}{d}\amp  \ds -\frac{a_{33}a_{12}-a_{32}a_{13}}{d}\amp  \ds \frac{-a_{13}a_{22}+a_{12}a_{23}}{d} \\ 0\amp 1\amp 0\amp  \ds -\frac{a_{33}a_{21}-a_{31}a_{23}}{d}\amp  \ds \frac{a_{33}a_{11}-a_{31}a_{13}}{d}\amp  \ds -\frac{a_{23}a_{11}-a_{21}a_{13}}{d} \\ 0\amp 0\amp 1\amp  \ds \frac{-a_{31}a_{22}+a_{32}a_{21}}{d}\amp  \ds -\frac{a_{32}a_{11}-a_{31}a_{12}}{d}\amp  \ds \frac{a_{22}a_{11}-a_{21}a_{12}}{d} \end{array}  \right]\text{,}
\end{equation*}
where%
\begin{equation}
\begin{aligned}d = a_{33}a_{11}a_{22}\amp -a_{33}a_{21}a_{12}-a_{31}a_{13}a_{22} \\ \amp -a_{32}a_{11}a_{23}+a_{32}a_{21}a_{13}+a_{31}a_{12}a_{23}. \end{aligned}\label{x:men:eq_4_a_3by3_det}
\end{equation}
%
\par
In this case, we can see that the inverse of the \(3 \times 3\) matrix \(A\) will be defined if and only if \(d \neq 0\). So, in the \(3 \times 3\) case the determinant of \(A\) will be given by the value of \(d\) in Equation \hyperref[x:men:eq_4_a_3by3_det]{({\xreffont\ref{x:men:eq_4_a_3by3_det}})}. What remains is for us to see how this is related to determinants of \(2 \times 2\) sub-matrices of \(A\).%
\par
To start, we collect all terms involving \(a_{11}\) in \(d\). A little algebra shows that%
\begin{equation*}
\det(A) = a_{11} \left( a_{33} a_{22} - a_{32} a_{23} \right) - a_{33} a_{21} a_{12} - a_{31} a_{13} a_{22} + a_{32}a_{21}a_{13} + a_{31} a_{12} a_{23}\text{.}
\end{equation*}
%
\par
Now let's collect the remaining terms involving \(a_{12}\):%
\begin{equation*}
\det(A) = a_{11} \left( a_{33} a_{22} - a_{32} a_{23} \right) - a_{12} \left(a_{33} a_{21} - a_{31} a_{23} \right)  - a_{31} a_{13} a_{22} + a_{32}a_{21}a_{13}\text{.}
\end{equation*}
%
\par
Finally, we collect the terms involving \(a_{13}\):%
\begin{equation*}
\det(A) = a_{11} \left( a_{33} a_{22} - a_{32} a_{23} \right) - a_{12} \left(a_{33} a_{21} - a_{31} a_{23} \right) + a_{13} \left(a_{32} a_{21} - a_{31} a_{22} \right)\text{.}
\end{equation*}
%
\par
Now we can connect the determinant of \(A\) to determinants of \(2 \times 2\) sub-matrices of \(A\).%
\begin{itemize}[label=\textbullet]
\item{}Notice that%
\begin{equation*}
a_{33} a_{22} - a_{32} a_{23}
\end{equation*}
is the determinant of the \(2 \times 2\) matrix \(\left[ \begin{array}{cc} a_{22}\amp a_{23} \\ a_{32}\amp a_{33} \end{array}  \right]\) obtained from \(A\) by deleting the first row and first column.%
\item{}Similarly, the expression%
\begin{equation*}
a_{33} a_{21} - a_{31} a_{23}
\end{equation*}
is the determinant of the \(2 \times 2\) matrix \(\left[ \begin{array}{cc} a_{21}\amp a_{23} \\ a_{31}\amp a_{33} \end{array}  \right]\) obtained from \(A\) by deleting the first row and second column.%
\item{}Finally, the expression%
\begin{equation*}
a_{32} a_{21} - a_{31} a_{22}
\end{equation*}
is the determinant of the \(2 \times 2\) matrix \(\left[ \begin{array}{cc} a_{21}\amp a_{22} \\ a_{31}\amp a_{32} \end{array}  \right]\) obtained from \(A\) by deleting the first row and third column.%
\end{itemize}
%
\par
Putting this all together gives us formula \hyperref[x:men:eq_det_3by3]{({\xreffont\ref{x:men:eq_det_3by3}})} for the determinant of a \(3 \times 3\) matrix as we defined earlier.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Two Devices for Remembering Determinants}
\typeout{************************************************}
%
\begin{sectionptx}{Two Devices for Remembering Determinants}{}{Two Devices for Remembering Determinants}{}{}{x:section:sec_det_remember}
There are useful ways to remember how to calculate the formulas for determinants of \(2 \times 2\) and \(3 \times 3\) matrices. In the \(2 \times 2\) case of \(A = \left[ \begin{array}{cc} a_{11}\amp a_{12} \\ a_{21}\amp a_{22} \end{array}  \right]\), we saw that%
\begin{equation*}
|A| = a_{11}a_{22} - a_{21}a_{22}\text{.}
\end{equation*}
%
\par
This makes \(|A|\) the product of the diagonal elements \(a_{11}\) and \(a_{22}\) minus the product of the off-diagonal elements \(a_{12}\) and \(a_{21}\). We can visualize this in an array by drawing arrows across the diagonal and off-diagonal, with a plus sign on the diagonal arrow indicting that we add the product of the diagonal elements and a minus sign on the off-diagonal arrow indicating that we subtract the product of the off-diagonal elements as shown in \hyperref[x:figure:F_2by2_determinant]{Figure~{\xreffont\ref{x:figure:F_2by2_determinant}}}.%
\begin{figureptx}{A diagram to remember the \(2 \times 2\) determinant.}{x:figure:F_2by2_determinant}{}%
\begin{image}{0.375}{0.25}{0.375}%
\includegraphics[width=\linewidth]{external/det_remem_2by2.pdf}
\end{image}%
\tcblower
\end{figureptx}%
We can do a similar thing for the determinant of a \(3 \times 3\) matrix. In this case, we extend the \(3 \times 3\) array to a \(3 \times 5\) array by adjoining the first two columns onto the matrix. We then add the products along the diagonals going from left to right and subtract the products along the diagonals going from right to left as indicated in \hyperref[x:figure:F_3by3_determinant]{Figure~{\xreffont\ref{x:figure:F_3by3_determinant}}}.%
\begin{figureptx}{A diagram to remember the \(3 \times 3\) determinant.}{x:figure:F_3by3_determinant}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/det_remem_3by3.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_det_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp16186904}%
For each of the following%
\begin{itemize}[label=\textbullet]
\item{}Identify the sub-matrices \(A_{1,j}\)%
\item{}Determine the cofactors \(C_{1,j}\).%
\item{}Use the cofactor expansion to calculate the determinant.%
\end{itemize}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(A = \left[ \begin{array}{ccr} 3\amp 6\amp 2 \\ 0\amp 4\amp -1 \\ 5\amp 0\amp 1 \end{array} \right]\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp16195736}{}\quad{}With a \(3 \times 3\) matrix, we will find the sub-matrices \(A_{11}\), \(A_{12}\), and \(A_{13}\). Recall that \(A_{ij}\) is the sub-matrix of \(A\) obtained by deleting the \(i\)th row and \(j\)th column of \(A\). Thus,%
\begin{equation*}
A_{11} =  \left[ \begin{array}{cr} 4\amp -1 \\ 0\amp 1 \end{array}  \right] \ A_{12} =  \left[ \begin{array}{cr} 0\amp -1 \\ 5\amp 1 \end{array}  \right] \ \text{ and } A_{13} =  \left[ \begin{array}{cc} 0\amp 4 \\ 5\amp 0 \end{array}  \right]\text{.}
\end{equation*}
The \(ij\)th cofactor is \(C_{ij} = (-1)^{i+j}\det(A_{ij})\), so%
\begin{align*}
C_{11} \amp = (-1)^2 \left[ \begin{array}{cr} 4\amp -1\\
0\amp 1  \end{array} \right] = 4\\
C_{12} \amp = (-1)^3 \left[ \begin{array}{cr} 0\amp -1\\
5\amp 1  \end{array} \right] = -5\\
C_{13} \amp = (-1)^4 \left[ \begin{array}{cc} 0\amp 4\\
5\amp 0  \end{array} \right] = -20\text{.}
\end{align*}
Then%
\begin{equation*}
\det(A) = a_{11}C_{11} + a_{12}C_{12} + a_{13}C_{13} = (3)(4) +(6)(-5) +(2)(-20) = -58\text{.}
\end{equation*}
%
\item{}\(A = \left[ \begin{array}{rrcr} 3\amp 0\amp 1\amp 1 \\ 2\amp 1\amp 2\amp 1 \\ 1\amp -2\amp 2\amp -1 \\ -3\amp 2\amp 3\amp 1 \end{array} \right]\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp16206488}{}\quad{}With a \(4 \times 4\) matrix, we will find the sub-matrices \(A_{11}\), \(A_{12}\), \(A_{13}\), and \(A_{14}\). We see that%
\begin{align*}
A_{11} \amp = \left[ \begin{array}{rcr}  1\amp 2\amp 1\\
-2\amp 2\amp -1\\
2\amp 3\amp 1  \end{array} \right]\\
A_{12} \amp = \left[ \begin{array}{rcr}  2\amp 2\amp 1\\
1\amp 2\amp -1\\
-3\amp 3\amp 1  \end{array} \right]\\
A_{13} \amp = \left[ \begin{array}{rrr}  2\amp 1\amp 1\\
1\amp -2\amp -1\\
-3\amp 2\amp 1  \end{array} \right]\\
A_{14} \amp = \left[ \begin{array}{rrc}  2\amp 1\amp 2\\
1\amp -2\amp 2\\
-3\amp 2\amp 3  \end{array} \right]\text{.}
\end{align*}
To calculate the \(ij\)th cofactor \(C_{ij} = (-1)^{i+j}\det(A_{ij})\), we need to calculate the determinants of the \(A_{1j}\). Using the device for calculating the determinant of a \(3 \times 3\) matrix we have that%
\begin{align*}
\det(A_{11}) \amp =\det\left( \left[ \begin{array}{rrr}  1\amp 2\amp 1\\
-2\amp 2\amp -1\\
2\amp 3\amp 1  \end{array} \right] \right)\\
\amp = (1)(2)(1)+(2)(-1)(2)+(1)(-2)(3)\\
\amp \qquad - (1)(2)(2)-(1)(-1)(3)-(2)(-2)(1)\\
\amp = -5\text{,}
\end{align*}
%
\begin{align*}
\det(A_{12}) \amp = \det\left(\left[ \begin{array}{rcr}  2\amp 2\amp 1\\
1\amp 2\amp -1\\
-3\amp 3\amp 1  \end{array} \right] \right)\\
\amp = (2)(2)(1)+(2)(-1)(-3)+(1)(1)(3)\\
\amp \qquad - (1)(2)(-3)-(2)(-1)(3)-(2)(1)(1)\\
\amp = 23\text{,}
\end{align*}
%
\begin{align*}
\det(A_{13}) \amp =  \det\left(\left[ \begin{array}{rrr}  2\amp 1\amp 1\\
1\amp -2\amp -1\\
-3\amp 2\amp 1  \end{array} \right] \right)\\
\amp = (2)(-2)(1)+(1)(-1)(-3)+(1)(1)(2)\\
\amp \qquad - (1)(-2)(-3)-(2)(-1)(2)-(1)(1)(1)\\
\amp = -2\text{,}
\end{align*}
and%
\begin{align*}
\det(A_{14}) \amp =  \det\left(\left[ \begin{array}{rrc}  2\amp 1\amp 2\\
1\amp -2\amp 2\\
-3\amp 2\amp 3  \end{array} \right] \right)\\
\amp = (2)(-2)(3)+(1)(2)(-3)+(2)(1)(2)\\
\amp \qquad - (2)(-2)(-3)-(2)(2)(2)-(1)(1)(3)\\
\amp = -37\text{.}
\end{align*}
Then%
\begin{align*}
C_{11} \amp = (-1)^2 \det(A_{11}) = -5\\
C_{12} \amp = (-1)^3 \det(A_{12})= -23\\
C_{13} \amp = (-1)^4 \det(A_{13}) = -2\\
C_{14} \amp = (-1)^5 \det(A_{13}) = 37
\end{align*}
and so%
\begin{align*}
\det(B) \amp = b_{11}C_{11} + b_{12}C_{12} + b_{13}C_{13} + b_{14}C_{14}\\
\amp = (3)(-5) +(0)(-23) +(1)(-2)+ (1)(37)\\
\amp = 20\text{.}
\end{align*}
%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp16221592}%
Show that for any \(2 \times 2\) matrices \(A\) and \(B\),%
\begin{equation*}
\det(AB) = \det(A) \det(B)\text{.}
\end{equation*}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp16222232}{}\quad{}Let \(A = \left[ \begin{array}{cc} a_{11}\amp a_{12} \\ a_{21}\amp a_{22} \end{array}  \right]\) and \(B = \left[ \begin{array}{cc} b_{11}\amp b_{12} \\ b_{21}\amp b_{22} \end{array}  \right]\). Then%
\begin{equation*}
AB = \left[ \begin{array}{cc} a_{11}b_{11}+a_{12}b_{21}\amp a_{11}b_{12}+a_{12}b_{22} \\ a_{21}b_{11}+a_{22}b_{21}\amp a_{21}b_{12}+a_{22}b_{22} \end{array}  \right]\text{.}
\end{equation*}
%
\par
So%
\begin{align*}
\det(AB) \amp = (a_{11}b_{11}+a_{12}b_{21})(a_{21}b_{12}+a_{22}b_{22})\\
\amp  \qquad   - (a_{11}b_{12}+a_{12}b_{22})(a_{21}b_{11}+a_{22}b_{21})\\
\amp = (a_{11}b_{11}a_{21}b_{12} + a_{11}b_{11}a_{22}b_{22} + a_{12}b_{21}a_{21}b_{12} + a_{12}b_{21}a_{22}b_{22})\\
\amp  \qquad - (a_{11}b_{12}a_{21}b_{11} + a_{11}b_{12}a_{22}b_{21} +  a_{12}b_{22}a_{21}b_{11} + a_{12}b_{22}a_{22}b_{21})\\
\amp = a_{11}b_{11}a_{22}b_{22} + a_{12}b_{21}a_{21}b_{12} - a_{11}b_{12}a_{22}b_{21} - a_{12}b_{22}a_{21}b_{11}\text{.}
\end{align*}
%
\par
Also,%
\begin{align*}
\det(A) \det(B) \amp = (a_{11}a_{22}-a_{12}a_{21})(b_{11}b_{22}-b_{12}b_{21})\\
\amp = a_{11}a_{22}b_{11}b_{22} - a_{11}a_{22}b_{12}b_{21} - a_{12}a_{21}b_{11}b_{22} + a_{12}a_{21}b_{12}b_{21}\text{.}
\end{align*}
%
\par
We conclude that \(\det(AB) = \det(A) \det(B)\) if \(A\) and \(B\) are \(2 \times 2\) matrices.%
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_det_summ}
%
\begin{itemize}[label=\textbullet]
\item{}The determinant of an \(n \times n\) matrix \(A = [a_{ij}]\) is found by taking the cofactor expansion of \(A\) along the first row. That is%
\begin{equation*}
\det(A) = a_{11}C_{11} + a_{12}C_{12} + a_{13}C_{13} + \cdots + a_{1n}C_{1n}\text{,}
\end{equation*}
where%
\begin{itemize}[label=$\circ$]
\item{}\(A_{ij}\) is the sub-matrix of \(A\) found by deleting the \(i\)th row and \(j\)th column of \(A\).%
\item{}\(C_{ij} = (-1)^{i+j} \det\left(A_{ij}\right)\) is the \(ij\)th \terminology{cofactor} of \(A\).%
\end{itemize}
%
\item{}The matrix \(A\) is invertible if and only if \(\det(A) \neq 0\).%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_det_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp16321304}%
Use the cofactor expansion to explain why multiplying each of the entries of a \(3\times 3\) matrix \(A\) by 2 multiplies the determinant of \(A\) by 8.%
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp16317336}%
Use the determinant criterion to determine for which \(c\) the matrix \(A=\left[ \begin{array}{crc} 1\amp 1\amp 2\\ 1\amp 0\amp c\\ 2\amp -1\amp 2 \end{array} \right]\) is invertible.%
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp16318360}%
Let \(A\) be a square matrix.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why \(\det(A^2) = [\det(A)]^2\)%
\item{}Expand on the argument from (a) to explain why \(\det(A^k) = [\det(A)]^k\) for any positive integer \(k\).%
\item{}Suppose that \(A\) is an invertible matrix and \(k\) is a positive integer. Must \(A^k\) be an invertible matrix? Why or why not?%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp16325272}%
Let \(A\) be an invertible matrix. Explain why \(\det(A^{-1})= \dfrac{1}{\det(A)}\) using determinant properties.%
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp16331032}%
Simplify the following determinant expression using determinant properties:%
\begin{equation*}
\det(PA^4P^{-1}A^T(A^{-1})^3)
\end{equation*}
%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp16335512}%
Find the eigenvalues of the following matrices. Find a basis for and the dimension of each eigenspace.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(A=\left[ \begin{array}{ccc} 1\amp 1\amp 1\\1\amp 1\amp 1\\1\amp 1\amp 1 \end{array} \right]\)%
\item{}\(A=\left[ \begin{array}{ccc} 2\amp 0\amp 3\\0\amp 1\amp 0\\0\amp 1\amp 2 \end{array} \right]\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp16333208}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
For any two \(n\times n\) matrices \(A\) and \(B\), \(\det (A+B) = \det A + \det B\).%
\item{}\lititle{True\slash{}False.}\par%
For any square matrix \(A\), \(\det(-A)= -\det(A)\).%
\item{}\lititle{True\slash{}False.}\par%
For any square matrix \(A\), \(\det(-A)= \det(A)\).%
\item{}\lititle{True\slash{}False.}\par%
The determinant of a square matrix with all non-zero entries is non-zero.%
\item{}\lititle{True\slash{}False.}\par%
If the determinant of \(A\) is non-zero, then so is the determinant of \(A^2\).%
\item{}\lititle{True\slash{}False.}\par%
If the determinant of a matrix \(A\) is 0, then one of the rows of \(A\) is a linear combination of the other rows.%
\item{}\lititle{True\slash{}False.}\par%
For any square matrix \(A\), \(\det(A^2)>\det(A)\).%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) and \(B\) are \(n \times n\) matrices and \(AB\) is invertible, then \(A\) and \(B\) are invertible.%
\item{}\lititle{True\slash{}False.}\par%
If \(A^2\) is the zero matrix, then the only eigenvalue of \(A\) is 0.%
\item{}\lititle{True\slash{}False.}\par%
If 0 is an eigenvalue of \(A\), then 0 is an eigenvalue of \(AB\) for any \(B\) of the same size as \(A\).%
\item{}\lititle{True\slash{}False.}\par%
Suppose \(A\) is a \(3 \times 3\) matrix. Then any three eigenvectors of \(A\) will form a basis of \(\R^3\).%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Area and Volume Using Determinants}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Area and Volume Using Determinants}{}{Project: Area and Volume Using Determinants}{}{}{x:section:sec_proj_det_area_vol}
The approach we will take to connecting area (volume) to the determinant will help shed light on properties of the determinant that we will discuss from an algebraic perspective in a later section. First, we mention some basic properties of area (we focus on area for now, but these same properties are valid for volumes as well). volume). As a shorthand, we denote the area of a region \(R\) by \(\Area(R)\).%
\begin{itemize}[label=\textbullet]
\item{}Area cannot be negative.%
\item{}If two regions \(R_1\) and \(R_2\) don't overlap, then the area of the union of the regions is equal to the sum of the areas of the regions. That is, if \(R_1 \cap R_2 = \emptyset\), then \(\Area(R_1 \cup R_2) = \Area(R_1) + \Area(R_2)\).%
\item{}Area is invariant under translation. That is, if we move a geometric region by the same amount uniformly in a given direction, the area of the original region and the area of the transformed region are the same. A translation of a region is done by just adding a fixed vector to each vector in the region. That is, a translation by a vector \(\vv\) is a function \(T_{\vv}\) such that the image \(T_{\vv}(R)\) of a region \(R\) is defined as%
\begin{equation*}
T_{\vv}(R) = \{\vr+\vv : \vr \in R\}\text{.}
\end{equation*}
Since area is translation invariant, \(\Area(T_{\vv}(R)) = \Area(R)\).%
\item{}The area of a one-dimensional object like a line segment is \(0\).%
\end{itemize}
%
\par
Now we turn our attention to areas of parallelograms. Let \(\vu\) and \(\vv\) be vectors in \(\R^2\). The parallelogram \(P(\vu,\vv)\) defined by \(\vu\) and \(\vv\) with point \(Q\) as basepoint is the set%
\begin{equation*}
P(\vu, \vv) = \{\overrightarrow{OQ}+r \vu + s \vv : 0 \leq r, s \leq 1\}\text{.}
\end{equation*}
%
\par
An illustration of such a parallelogram is shown at left in \hyperref[x:figure:F_parallelograms]{Figure~{\xreffont\ref{x:figure:F_parallelograms}}}.%
\begin{figureptx}{A parallelogram and a translated, rotated parallelogram.}{x:figure:F_parallelograms}{}%
\begin{image}{0.2}{0.6}{0.2}%
\includegraphics[width=\linewidth]{external/4_a_parallelogram_rotated.pdf}
\end{image}%
\tcblower
\end{figureptx}%
If \(\vu = [u_1 \ u_2]^{\tr}\) and \(\vv = [v_1 \ v_2]^{\tr}\), then we will also represent \(P(\vu,\vv)\) as \(P\left( \left[ \begin{array}{cc}u_1\amp u_2 \\ v_1\amp v_2 \end{array} \right] \right)\).%
\par
Since area is translation and rotation invariant, we can translate our parallelogram by \(-\overrightarrow{OQ}\) to place its basepoint at the origin, then rotate by an angle \(\theta\) (as shown at left in \hyperref[x:figure:F_parallelograms]{Figure~{\xreffont\ref{x:figure:F_parallelograms}}}. This transforms the vector \(\vv\) to a vector \(\vv'\) and the vector \(\vu\) to a vector \(\vu'\) as shown at right in \hyperref[x:figure:F_parallelograms]{Figure~{\xreffont\ref{x:figure:F_parallelograms}}}. With this in mind we can always assume that our parallelograms have one vertex at the origin, with \(\vu\) along the \(x\)-axis, and \(\vv\) in standard position. Now we can investigate how to calculate the area of a parallelogram.%
\begin{project}{}{x:project:act_parallelogram_area}%
There are two situations to consider when we want to find the area of a parallelogram determined by vectors \(\vu\) and \(\vv\), both shown in \hyperref[x:figure:F_parallelogram_area]{Figure~{\xreffont\ref{x:figure:F_parallelogram_area}}}. The parallelogram will be determined by the lengths of these vectors.%
\begin{figureptx}{Parallelograms formed by \(\vu\) and \(\vv\)}{x:figure:F_parallelogram_area}{}%
\begin{image}{0.2}{0.6}{0.2}%
\includegraphics[width=\linewidth]{external/4_a_parallelogram_area.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}In the situation depicted at left in \hyperref[x:figure:F_parallelogram_area]{Figure~{\xreffont\ref{x:figure:F_parallelogram_area}}}, use geometry to explain why \(\Area(P(\vu,\vv)) = h |\vu|\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp16387352}{}\quad{}What can we say about the triangles \(ODB\) and \(EAC\)?%
\item{}In the situation depicted at right in \hyperref[x:figure:F_parallelogram_area]{Figure~{\xreffont\ref{x:figure:F_parallelogram_area}}}, use geometry to again explain why \(\Area(P(\vu,\vv)) = h |\vu|\). (Hint: What can we say about \(\Area(AEC)\) and \(\Area(ODB)\)?)%
\end{enumerate}
\end{project}%
The result of \hyperref[x:project:act_parallelogram_area]{Project Activity~{\xreffont\ref{x:project:act_parallelogram_area}}} is that the area of \(P(\vu,\vv)\) is given by \(h |\vu|\), where \(h\) is the height of the parallelogram determined by dropping a perpendicular from the terminal point of \(\vv\) to the line determined by the vector \(\vu\).%
\par
Now we turn to the question of how the determinant is related to area of a parallelogram. Our approach will use some properties of the area of \(P(\vu, \vv)\).%
\begin{project}{}{x:project:act_P_area_properties}%
Let \(\vu\) and \(\vv\) be vectors that determine a parallelogram in \(\R^2\).%
\begin{figureptx}{Parallelograms formed by \(k\vu\) and \(\vv\) and by \(\vu\) and \(\vv+k\vu\).}{x:figure:F_area_propertities}{}%
\begin{image}{0.2}{0.6}{0.2}%
\includegraphics[width=\linewidth]{external/4_a_parallelogram_shear.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why%
\begin{equation}
\Area(P(\vu,\vv)) = \Area(P(\vv,\vu))\label{x:men:eq_P_area_properties_1}
\end{equation}
%
\item{}If \(k\) is any scalar, then \(k\vu\) either stretches or compresses \(\vu\). Use this idea, and the result of \hyperref[x:project:act_parallelogram_area]{Project Activity~{\xreffont\ref{x:project:act_parallelogram_area}}}, to explain why%
\begin{equation}
\Area(P(k\vu,\vv)) = \Area(P(\vu,k\vv)) = |k| \Area(P(\vu,\vv))\label{x:men:eq_P_area_properties_2}
\end{equation}
for any real number \(k\). A representative picture of this situation is shown at left in \hyperref[x:figure:F_parallelogram_area]{Figure~{\xreffont\ref{x:figure:F_parallelogram_area}}} for a value of \(k > 1\). You will also need to consider what happens when \(k \lt  0\).%
\item{}Finally, use the result of \hyperref[x:project:act_parallelogram_area]{Project Activity~{\xreffont\ref{x:project:act_parallelogram_area}}} to explain why%
\begin{equation}
\Area(P(\vu+k\vv,\vv)) = \Area(P(\vu,\vv+k\vu)) = \Area(P(\vu,\vv))\label{x:men:eq_P_area_properties_3}
\end{equation}
for any real number \(k\). A representative picture is shown at right in \hyperref[x:figure:F_area_propertities]{Figure~{\xreffont\ref{x:figure:F_area_propertities}}}.%
\end{enumerate}
\end{project}%
Properties \hyperref[x:men:eq_P_area_properties_2]{({\xreffont\ref{x:men:eq_P_area_properties_2}})} and \hyperref[x:men:eq_P_area_properties_3]{({\xreffont\ref{x:men:eq_P_area_properties_3}})} will allow us to calculate the area of the parallelogram determined by vectors \(\vu\) and \(\vv\).%
\begin{project}{}{x:project:act_det_area}%
Let \(\vu = [u_1 \ u_2]^{\tr}\) and \(\vv = [v_1 \ v_2]^{\tr}\). We will now demonstrate that%
\begin{equation*}
\Area(P(\vu,\vv)) = \left|\det\left(\left| \begin{array}{cc} u_1\amp u_2\\v_1\amp v_2 \end{array}  \right] \right)\right|\text{.}
\end{equation*}
%
\par
Before we begin, note that if both \(u_1\) and \(v_1\) are \(0\), then \(\vu\) and \(\vv\) are parallel. This makes \(P(\vu, \vv)\) a line segment and so \(\Area(P(\vu,\vv)) = 0\). But if \(u_1 = v_1 = 0\), it is also the case that%
\begin{equation*}
\det\left(\left| \begin{array}{cc} u_1\amp u_2\\v_1\amp v_2 \end{array}  \right] \right) = u_1v_2-u_2v_1 = 0
\end{equation*}
as well. So we can assume that at least one of \(u_1\), \(v_1\) is not \(0\). Since \(P(\vu, \vv) = P(\vv, \vu)\), we can assume without loss of generality that \(u_1 \neq 0\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain using properties \hyperref[x:men:eq_P_area_properties_2]{({\xreffont\ref{x:men:eq_P_area_properties_2}})} and  \hyperref[x:men:eq_P_area_properties_3]{({\xreffont\ref{x:men:eq_P_area_properties_3}})} as appropriate why%
\begin{equation*}
\Area(P(\vu,\vv)) =\Area\left(P\left(\vu, \left[0 \ v_2-\frac{v_1}{u_1}u_2\right] \right) \right)\text{.}
\end{equation*}
%
\item{}Let \(\vv_1 = \left[0 \ v_2-\frac{v_1}{u_1}u_2\right]^{\tr}\). Recall that our alternate representation of \(P(\vu,\vv))\) allows us to write%
\begin{equation*}
\Area(P(\vu, \vv_1)) = \Area\left( P\left(  \left[ \begin{array}{cc} u_1\amp u_2 \\ 0\amp v_2-\frac{v_1}{u_1}u_2 \end{array}  \right] \right) \right)\text{.}
\end{equation*}
This should seem very suggestive. We are essentially applying the process of Gaussian elimination to our parallelogram matrix to reduce it to a diagonal matrix. From there, we can calculate the area. The matrix form should indicate the next step \textemdash{}  applying an operation to eliminate the entry in the first row and second column. To do this, we need to consider what happens if \(v_2-\frac{v_1}{u_1}u_2 = 0\) and if \(v_2-\frac{v_1}{u_1}u_2 \neq 0\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Assume that \(v_2-\frac{v_1}{u_1}u_2 = 0\). Explain why \(\Area(P(\vu,\vv)) = 0\). Then explain why \(\Area(P(\vu,\vv)) = 0 = \det\left(\left[ \begin{array}{cc} u_1\amp u_2\\v_1\amp v_2 \end{array} \right]\right)\).%
\item{}Now we consider the case when \(v_2-\frac{v_1}{u_1}u_2 \neq 0\). Complete the process as in part (a), using properties \hyperref[x:men:eq_P_area_properties_2]{({\xreffont\ref{x:men:eq_P_area_properties_2}})} and  \hyperref[x:men:eq_P_area_properties_3]{({\xreffont\ref{x:men:eq_P_area_properties_3}})} (compare to Gaussian elimination) to continue to reduce the problem of calculating \(\Area(P(\vu,\vv))\) to one of calculating \(\Area(P(\ve_1, \ve_2))\). Use this process to conclude that%
\begin{equation*}
\Area(P(\vu,\vv)) = \left| \det\left(\left[ \begin{array}{cc} u_1\amp u_2\\v_1\amp v_2 \end{array}  \right]\right)\right|\text{.}
\end{equation*}
%
\end{enumerate}
\end{enumerate}
\end{project}%
We can apply the same arguments as above using rotations, translations, shearings, and scalings to show that the properties of area given above work in any dimension. Given vectors \(\vu_1\), \(\vu_2\), \(\ldots\), \(\vu_n\) in \(\R^n\), we let%
\begin{equation*}
P(\vu_1, \vu_2, \ldots, \vu_n) = \{\overrightarrow{OQ}+x_1 \vu_1 + x_2 \vu_2 + \cdots + x_n \vu_n : 0 \leq x_i \leq 1 \text{ for each }  i\}\text{.}
\end{equation*}
%
\par
If \(n = 2\), then \(P(\vu_1,\vu_2)\) is the parallelogram determined by \(\vu_1\) and \(\vu_2\) with basepoint \(Q\). If \(n = 3\), then \(P(\vu_1, \vu_2, \vu_3)\) is the parallelepiped with basepoint \(Q\) determined by \(\vu_1\), \(\vu_2\), and \(\vu_3\). In higher dimensions the sets \(P(\vu_1, \vu_2, \ldots,\vu_n)\) are called parallelotopes, and we use the notation \(\Vol(P(\vu_1, \vu_2, \ldots,\vu_n))\) for their volume. The \(n\)-dimensional volumes of these paralleotopes satisfy the following properties:%
\begin{align}
\Vol\amp (P(\vu_1, \vu_2, \ldots, \vu_{i-1}, \vu_i, \vu_{i+1}, \ldots, \vu_{j-1}, \vu_j, \vu_{j+1}, \ldots,\vu_n))\notag\\
\amp = \Vol(P(\vu_1, \vu_2, \ldots, \vu_{i-1}, \vu_j, \vu_{i+1}, \ldots, \vu_{j-1}, \vu_i, \vu_{j+1}, \ldots,\vu_n))\label{x:mrow:eq_vol_property_1}
\end{align}
for any \(i\) and \(j\).%
\begin{equation}
\Vol(P(\vu_1,\vu_2, \ldots, \vu_{i-1}, k\vu_i, \vu_{i+1}, \ldots, \vu_n)) = |k| \Vol(P(\vu_1,\vu_2, \ldots, \vu_n))\label{x:men:eq_vol_property_2}
\end{equation}
for any real number \(k\) and any \(i\).%
\begin{equation}
\Vol(P(\vu_1,\vu_2, \ldots, \vu_{i-1}, \vu_{i}+k\vu_j, \vu_{i+1}, \ldots, \vu_n)) = \Vol(P(\vu_1,\vu_2, \ldots, \vu_n))\label{x:men:eq_vol_property_3}
\end{equation}
for any real number \(k\) and any distinct \(i\) and \(j\).%
\begin{project}{}{x:project:act_det_vol}%
We now show that \(\Vol(P(\vu_1, \vu_2, \vu_3))\) is the absolute value of the determinant of \(\left[ \begin{array}{c} \vu_1 \\ \vu_2 \\ \vu_3 \end{array}  \right]\). For easier notation, let \(\vu = [u_1 \ u_2 \ u_3]^{\tr}\), \(\vv = [v_1 \ v_2 \ v_3]^{\tr}\), and \(\vw = [w_1 \ w_2 \ w_3]^{\tr}\). As we argued in the 2-dimensional case, we can assume that all terms that we need to be nonzero are nonzero, and we can do so without verification.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain how property \hyperref[x:men:eq_vol_property_2]{({\xreffont\ref{x:men:eq_vol_property_2}})} shows that \(\Vol(P(\vu, \vv, \vw))\) is equal to%
\begin{equation*}
\Vol\left(P\left( \left[ \begin{array}{ccc} u_{1}\amp u_{2}\amp u_{3} \\ 0\amp \frac{1}{u_1}(v_{2}u_1-v_{1}u_{2})\amp \frac{1}{u_1}(v_{3}u_1-v_{1}u_{3})\\ 0\amp \frac{1}{u_1}(w_{2}u_1-w_{1}u_{2})\amp \frac{1}{u_1}(w_{3}u_1-w_{1}u_{3}) \end{array}  \right] \right)\right)\text{.}
\end{equation*}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp16442776}{}\quad{}Think about how these properties are related to row operations.%
\item{}Now let%
\begin{equation*}
\vv_1 = \left[ 0 \ \frac{1}{u_1}(v_{2}u_1-v_{1}u_{2}) \ \frac{1}{u_1}(v_{3}u_1-v_{1}u_{3})\right]^{\tr}
\end{equation*}
and%
\begin{equation*}
\vw_1 = \left[ 0 \ \frac{1}{u_1}(w_{2}u_1-w_{1}u_{2}) \ \frac{1}{u_1}(w_{3}u_1-w_{1}u_{3})\right]^{\tr}\text{.}
\end{equation*}
Explain how property \hyperref[x:men:eq_vol_property_2]{({\xreffont\ref{x:men:eq_vol_property_2}})} shows that \(\Vol(P(\vu, \vv, \vw))\) is equal to%
\begin{equation*}
\Vol\left(P\left( \left[ \begin{array}{ccc} u_{1}\amp u_{2}\amp u_{3} \\ 0\amp \frac{1}{u_1}(v_{2}u_1-v_{1}u_{2})\amp \frac{1}{u_1}(v_{3}u_1-v_{1}u_{3})\\ 0\amp 0\amp d \end{array}  \right] \right)\right)\text{,}
\end{equation*}
where%
\begin{equation*}
d = \frac{1}{u_1v_2-u_2v_1}(u_1(v_2w_3-v_3w_2)-u_2(v_1w_3-v_{3}w_1)+u_3(v_1w_2-v_2w_1))\text{.}
\end{equation*}
%
\item{}Just as we saw in the 2-dimensional case, we can proceed to use the diagonal entries to eliminate the entries above the diagonal without changing the volume to see that%
\begin{equation*}
\Vol(P(\vu, \vv, \vw)) = \Vol\left(P\left( \left[ \begin{array}{ccc} u_{1}\amp 0\amp 0 \\ 0\amp \frac{1}{u_1}(v_{2}u_1-v_{1}u_{2})\amp 0\\ 0\amp 0\amp d \end{array}  \right] \right)\right)\text{.}
\end{equation*}
Complete the process, applying appropriate properties to explain why%
\begin{equation*}
\Vol(P(\vu, \vv, \vw)) = x \Vol(P(\ve_1, \ve_2, \ve_3))
\end{equation*}
for some constant \(x\). Find the constant and, as a result, find a specific expression for \(\Vol(P(\vu, \vv, \vw))\) involving a determinant.%
\end{enumerate}
\end{project}%
Properties \hyperref[x:mrow:eq_vol_property_1]{({\xreffont\ref{x:mrow:eq_vol_property_1}})}, \hyperref[x:men:eq_vol_property_2]{({\xreffont\ref{x:men:eq_vol_property_2}})}, and \hyperref[x:men:eq_vol_property_3]{({\xreffont\ref{x:men:eq_vol_property_3}})} involve the analogs of row operations on matrices, and we will prove algebraically that the determinant exhibits the same properties. In fact, the determinant can be uniquely defined by these properties. So in a sense, the determinant is an area or volume function.%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 18 The Characteristic Equation}
\typeout{************************************************}
%
\begin{chapterptx}{The Characteristic Equation}{}{The Characteristic Equation}{}{}{x:chapter:chap_characteristic_equation}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp16450712}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What is the characteristic polynomial of a matrix?%
\item{}What is the characteristic equation of a matrix?%
\item{}How and why is the characteristic equation of a matrix useful?%
\item{}How many different eigenvalues can an \(n \times n\) matrix have?%
\item{}How large can the dimension of the eigenspace corresponding to an eigenvalue be?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Modeling the Second Law of Thermodynamics}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Modeling the Second Law of Thermodynamics}{}{Application: Modeling the Second Law of Thermodynamics}{}{}{x:section:sec_appl_thermo}
Pour cream into your cup of coffee and the cream spreads out; straighten up your room and it soon becomes messy again; when gasoline is mixed with air in a car's cylinders, it explodes if a spark is introduced. In each of these cases a transition from a low energy state (your room is straightened up) to a higher energy state (a messy, disorganized room) occurs. This can be described by entropy \textemdash{} a measure of the energy in a system. Low energy is organized (like ice cubes) and higher energy is not (like water vapor). It is a fundamental property of energy (as described by the second law of thermodynamics) that the entropy of a system cannot decrease. In other words, in the absence of any external intervention, things never become more organized.%
\par
The Ehrenfest model\footnote{named after Paul and Tatiana Ehrenfest who introduced it in ``Über zwei bekannte Einwände gegen das Boltzmannsche H-Theorem,'' \pubtitle{Physikalishce Zeitschrift}, vol. 8 (1907), pp. 311-314)\label{g:fn:idp16463256}} is a Markov process proposed to explain the statistical interpretation of the second law of thermodynamics using the diffusion of gas molecules. This process can be modeled as a problem of balls and bins, as we will do later in this section. The characteristic polynomial of the transition matrix will help us find the eigenvalues and allow us to analyze our model.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_chareq_intro}
We have seen that the eigenvalues of an \(n \times n\) matrix \(A\) are the scalars \(\lambda\) so that \(A - \lambda I_n\) has a nontrivial null space. Since a matrix has a nontrivial null space if and only if the matrix is not invertible, we can also say that \(\lambda\) is an eigenvalue of \(A\) if%
\begin{equation}
\det(A - \lambda I_n) = 0\text{.}\label{x:men:eq_PA5_2_1}
\end{equation}
%
\par
This equation is called the \terminology{characteristic equation} of \(A\). It provides us an \terminology{algebraic} way to find eigenvalues, which can then be used in finding eigenvectors corresponding to each eigenvalue. Suppose we want to find the eigenvalues of \(A=\left[ \begin{array}{cc} 1 \amp  1 \\ 1\amp  3 \end{array}  \right]\). Note that%
\begin{equation*}
A- \lambda I_2 = \left[ \begin{array}{cc} 1-\lambda \amp  1 \\ 1\amp  3-\lambda \end{array}  \right]\,\text{,}
\end{equation*}
with determinant \((1-\lambda)(3-\lambda)-1=\lambda^2-4\lambda+2\). Hence, the eigenvalues \(\lambda_1, \lambda_2\) are the solutions of the characteristic equation \(\lambda^2-4\lambda+2=0\). Using quadratic formula, we find that \(\lambda_1=2+\sqrt{2}\) and \(\lambda_2=2-\sqrt{2}\) are the eigenvalues.%
\par
In this activity, our goal will be to use the characteristic equation to obtain information about eigenvalues and eigenvectors of a matrix with real entries.%
\begin{exploration}{}{x:exploration:pa_4_b}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}For each of the following parts, use the characteristic equation to determine the eigenvalues of \(A\). Then, for each eigenvalue \(\lambda\), find a basis of the corresponding eigenspace, i.e., \(\Nul(A-\lambda I)\). You might want to recall how to find a basis for the null space of a matrix from \hyperref[x:chapter:chap_null_space]{Section~{\xreffont\ref{x:chapter:chap_null_space}}}. Also, make sure that your eigenvalue candidate \(\lambda\) yields nonzero eigenvectors in \(\Nul(A-\lambda I)\) for otherwise \(\lambda\) will not be an eigenvalue.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}\(A=\left[ \begin{array}{cr} 2 \amp 0 \\ 0 \amp -3 \end{array} \right]\)%
\item{}\(A=\left[ \begin{array}{cc} 1 \amp 2 \\ 0 \amp 1 \end{array} \right]\)%
\item{}\(A=\left[ \begin{array}{cc} 1 \amp 4 \\2 \amp 3 \end{array} \right]\)%
\end{enumerate}
\item{}Use your eigenvalue and eigenvector calculations of the above problem as a guidance to answer the following questions about a matrix with real entries.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}At most how many eigenvalues can a \(2\times 2\) matrix have? Is it possible to have no eigenvalues? Is it possible to have only one eigenvalue? Explain.%
\item{}If a matrix is an upper-triangular matrix (i.e., all entries below the diagonal are 0's, as in the first two matrices of the previous problem), what can you say about its eigenvalues? Explain.%
\item{}How many linearly independent eigenvectors can be found for a \(2\times 2\) matrix? Is it possible to have a matrix without 2 linearly independent eigenvectors? Explain.%
\end{enumerate}
\item{}Using the characteristic equation, determine which matrices have 0 as an eigenvalue.%
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Characteristic Equation}
\typeout{************************************************}
%
\begin{sectionptx}{The Characteristic Equation}{}{The Characteristic Equation}{}{}{x:section:sec_chareq}
Until now, we have been given eigenvalues or eigenvectors of a matrix and determined eigenvectors and eigenvalues from the known information. In this section we use determinants to find (or approximate) the eigenvalues of a matrix. From there we can find (or approximate) the corresponding eigenvectors. The tool we will use is a polynomial equation, the characteristic equation, of a square matrix whose roots are the eigenvalues of the matrix. The characteristic equation will then provide us with an algebraic way of finding the eigenvalues of a square matrix.%
\par
We have seen that the eigenvalues of a square matrix \(A\) are the scalars \(\lambda\) so that \(A - \lambda I\) has a nontrivial null space. Since a matrix has a nontrivial null space if and only if the matrix is not invertible, we can also say that \(\lambda\) is an eigenvalue of \(A\) if%
\begin{equation}
\det(A - \lambda I) = 0\text{.}\label{x:men:eq_4_b_1}
\end{equation}
%
\par
Note that if \(A\) is an \(n \times n\) matrix, then \(\det(A - \lambda I)\) is a polynomial of degree \(n\). Furthermore, if \(A\) has real entries, the polynomial has real coefficients. This polynomial, and the equation \hyperref[x:men:eq_4_b_1]{({\xreffont\ref{x:men:eq_4_b_1}})} are given special names.%
\begin{definition}{}{g:definition:idp16482200}%
\index{characteristic polynomial}%
\index{characteristic equation}%
Let \(A\) be an \(n \times n\) matrix. The \terminology{characteristic polynomial} of \(A\) is the polynomial%
\begin{equation*}
\det(A-\lambda I_n)\text{,}
\end{equation*}
where \(I_n\) is the \(n \times n\) identity matrix. The \terminology{characteristic equation} of \(A\) is the equation%
\begin{equation*}
\det(A-\lambda I_n) = 0\text{.}
\end{equation*}
%
\end{definition}
So the characteristic equation of \(A\) gives us an algebraic way of finding the eigenvalues of \(A\).%
\begin{activity}{}{x:activity:act_4_b_1}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the characteristic polynomial of the matrix \(A = \left[ \begin{array}{crc} 3\amp -2\amp 5 \\ 1\amp 0\amp 7 \\ 0\amp 0\amp 1 \end{array} \right]\), and use the characteristic polynomial to find all of the eigenvalues of \(A\).%
\item{}Verify that 1 and 2 are the only eigenvalues of the matrix \(\left[ \begin{array}{cccc} 1\amp 0\amp 0\amp 1\\ 1\amp 2\amp 0\amp 0 \\ 0\amp 0\amp 1\amp 0 \\ 0\amp 0\amp 0\amp 1 \end{array} \right]\).%
\end{enumerate}
\end{activity}%
As we argued in \hyperref[x:exploration:pa_4_b]{Preview Activity~{\xreffont\ref{x:exploration:pa_4_b}}}, a \(2 \times 2\) matrix can have at most 2 eigenvalues. For an \(n \times n\) matrix, the characteristic polynomial will be a degree \(n\) polynomial, and we know from algebra that a degree \(n\) polynomial can have at most \(n\) roots. Since an eigenvalue of a matrix is a root of the characteristic polynomial of that matrix, we can conclude that an \(n \times n\) matrix can have at most \(n\) distinct eigenvalues. \hyperref[x:activity:act_4_b_1]{Activity~{\xreffont\ref{x:activity:act_4_b_1}}} (b) shows that a \(4 \times 4\) matrix may have fewer than \(4\) eigenvalues, however. Note that one of these eigenvalues, the eigenvalue 1, appears three times as a root of the characteristic polynomial of the matrix. The number of times an eigenvalue appears as a root of the characteristic polynomial is called the \terminology{(algebraic) multiplicity} of the eigenvalue. More formally:%
\begin{definition}{}{g:definition:idp16499224}%
\index{multiplicity!algebraic}%
The \terminology{(algebraic) multiplicity} of an eigenvalue \(\lambda\) of a matrix \(A\) is the largest integer \(m\) so that \((x-\lambda)^m\) divides the characteristic polynomial of \(A\).%
\end{definition}
Thus, in \hyperref[x:activity:act_4_b_1]{Activity~{\xreffont\ref{x:activity:act_4_b_1}}} (b) the eigenvalue 1 has multiplicity 3 and the eigenvalue 2 has multiplicity 1. Notice that if we count the eigenvalues of an \(n \times n\) matrix with their multiplicities, the total will always be \(n\).%
\par
If \(A\) is a matrix with real entries, then the characteristic polynomial will have real coefficients. It is possible that the characteristic polynomial can have complex roots, and that the matrix \(A\) has complex eigenvalues. The Fundamental Theorem of Algebra shows us that if a real matrix has complex eigenvalues, then those eigenvalues will appear in conjugate pairs, i.e., if \(\lambda_1=a+ib\) is an eigenvalue of \(A\), then \(\lambda_2=a-ib\) is another eigenvalue of \(A\). Furthermore, for an odd degree polynomial, since the complex eigenvalues will come in conjugate pairs, we will be able to find at least one real eigenvalue.%
\par
We now summarize the information we have so far about eigenvalues of an \(n\times n\) real matrix:%
\begin{theorem}{}{}{g:theorem:idp16504984}%
Let \(A\) be an \(n\times n\) matrix with real entries. Then%
\begin{enumerate}
\item{}There are at most \(n\) eigenvalues of \(A\). If each eigenvalue (including complex eigenvalues) is counted with its multiplicity, there are exactly \(n\) eigenvalues.%
\item{}If \(A\) has a complex eigenvalue \(\lambda\), the complex conjugate of \(\lambda\) is also an eigenvalue of \(A\).%
\item{}If \(n\) is odd, \(A\) has at least one real eigenvalue.%
\item{}If \(A\) is upper or lower-triangular, the eigenvalues are the entries on the diagonal.%
\end{enumerate}
%
\end{theorem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Eigenspaces, A Geometric Example}
\typeout{************************************************}
%
\begin{sectionptx}{Eigenspaces, A Geometric Example}{}{Eigenspaces, A Geometric Example}{}{}{x:section:sec_egspace_geom}
Recall that for each eigenvalue \(\lambda\) of an \(n \times n\) matrix \(A\), the eigenspace of \(A\) corresponding to the eigenvalue \(\lambda\) is \(\Nul (A - \lambda I_n)\). These eigenspaces can tell us important information about the matrix transformation defined by \(A\). For example, consider the matrix transformation \(T\) from \(\R^3\) to \(\R^3\) defined by \(T(\vx) = A \vx\), where%
\begin{equation*}
A = \left[ \begin{array}{ccc} 1\amp 0\amp 1\\0\amp 1\amp 1\\0\amp 0\amp 2 \end{array}  \right]\text{.}
\end{equation*}
%
\par
We are interested in understanding what this matrix transformation does to vectors in \(\R^3\). First we note that \(A\) has eigenvalues \(\lambda_1 = 1\) and \(\lambda_2 = 2\), with \(\lambda_1\) having multiplicity \(2\). There is a pair \(\vv_1 = \left[ \begin{array}{c} 1\\0\\0 \end{array}  \right]\) and \(\vv_2 = \left[ \begin{array}{c} 0\\1\\0 \end{array}  \right]\) of linearly independent eigenvectors for \(A\) corresponding to the eigenvalue \(\lambda_1\) and an eigenvector \(\vv_3=\left[ \begin{array}{c} 1\\1\\1 \end{array}  \right]\) for \(A\) corresponding to the eigenvalue \(\lambda_2\). Note that the vectors \(\vv_1\), \(\vv_2\), and \(\vv_3\) are linearly independent (recall from Theorem that eigenvectors corresponding to different eigenvalues are always linearly independent). So any vector \(\vb\) in \(\R^3\) can be written uniquely as a linear combination of \(\vv_1\), \(\vv_2\), and \(\vv_3\). Let's now consider the action of the matrix transformation \(T\) on a linear combination of \(\vv_1\), \(\vv_2\), and \(\vv_2\). Note that%
\begin{align}
T(c_1\vv_1 + c_2 \vv_2 + c_3 \vv_3) \amp = c_1T(\vv_1) + c_2T(\vv_2) + c_3 T(\vv_3)\notag\\
\amp = c_1 \lambda_1 \vv_1 + c_2 \lambda_1 \vv_2 + c_3 \lambda_2 \vv_3\notag\\
\amp = (1)(c_1\vv_1 + c_2 \vv_2) + (2)c_3 \vv_3\text{.}\label{x:mrow:eq_4_b_2}
\end{align}
%
\par
Equation \hyperref[x:mrow:eq_4_b_2]{({\xreffont\ref{x:mrow:eq_4_b_2}})} illustrates that it is most convenient to view the action of \(T\) in the coordinate system where \(\Span \{\vv_1\}\) serves as the \(x\)-axis, \(\Span \{\vv_2\}\) serves as the \(y\)-axis, and \(\Span \{\vv_3\}\) as the \(z\)-axis. In this case, we can visualize that when we apply the transformation \(T\) to a vector \(\vb = c_1 \vv_1 + c_2 \vv_2 + c_3 \vv_3\) in \(\R^3\) the result is an output vector that is unchanged in the \(\vv_1\)-\(\vv_2\) plane and scaled by a factor of \(2\) in the \(\vv_3\) direction. For example, consider the box whose sides are determined by the vectors \(\vv_1\), \(\vv_2\), and \(\vv_3\) as shown in \hyperref[x:figure:F_4_b_1]{Figure~{\xreffont\ref{x:figure:F_4_b_1}}}. The transformation \(T\) stretches this box by a factor of \(2\) in the \(\vv_3\) direction and leaves everything else alone, as illustrated in \hyperref[x:figure:F_4_b_1]{Figure~{\xreffont\ref{x:figure:F_4_b_1}}}. So the entire \(\Span \{\vv_1, \vv_2\})\) is unchanged by \(T\), but \(\Span \{\vv_3\})\) is scaled by \(2\). In this situation, the eigenvalues and eigenvectors provide the most convenient perspective through which to visualize the action of the transformation \(T\).%
\begin{figureptx}{A box and a transformed box.}{x:figure:F_4_b_1}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/4_b_Eigenspaces.pdf}
\end{image}%
\tcblower
\end{figureptx}%
This geometric perspective illustrates how each eigenvalue and the corresponding eigenspace of \(A\) tells us something important about \(A\). So it behooves us to learn a little more about eigenspaces.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Dimensions of Eigenspaces}
\typeout{************************************************}
%
\begin{sectionptx}{Dimensions of Eigenspaces}{}{Dimensions of Eigenspaces}{}{}{x:section:sec_egspace_dims}
There is a connection between the dimension of the eigenspace of a matrix corresponding to an eigenvalue and the multiplicity of that eigenvalue as a root of the characteristic polynomial. Recall that the dimension of a subspace of \(\R^n\) is the number of vectors in a basis for the eigenspace. We investigate the connection between dimension and multiplicity in the next activity.%
\begin{activity}{}{x:activity:act_4_b_3}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the dimension of the eigenspace for each eigenvalue of matrix \(A = \left[ \begin{array}{crc} 3\amp -2\amp 5 \\ 1\amp 0\amp 7 \\ 0\amp 0\amp 1 \end{array} \right]\) from \hyperref[x:activity:act_4_b_1]{Activity~{\xreffont\ref{x:activity:act_4_b_1}}} (a).%
\item{}Find the dimension of the eigenspace for each eigenvalue of matrix \(A=\left[ \begin{array}{cccc} 1\amp 0\amp 0\amp 1\\ 1\amp 2\amp 0\amp 0 \\ 0\amp 0\amp 1\amp 0 \\ 0\amp 0\amp 0\amp 1 \end{array} \right]\) from \hyperref[x:activity:act_4_b_1]{Activity~{\xreffont\ref{x:activity:act_4_b_1}}} (b).%
\item{}Consider now a \(3\times 3\) matrix with 3 distinct eigenvalues \(\lambda_1, \lambda_2, \lambda_3\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Recall that a polynomial of degree can have at most three distinct roots. What does that say about the multiplicities of \(\lambda_1, \lambda_2, \lambda_3\)?%
\item{}Use the fact that eigenvectors corresponding to distinct eigenvalues are linearly independent to find the dimensions of the eigenspaces for \(\lambda_1, \lambda_2, \lambda_3\).%
\end{enumerate}
\end{enumerate}
\end{activity}%
The examples in \hyperref[x:activity:act_4_b_3]{Activity~{\xreffont\ref{x:activity:act_4_b_3}}} all provide instances of the principle that the dimension of an eigenspace corresponding to an eigenvalue \(\lambda\) cannot exceed the multiplicity of \(\lambda\). Specifically:%
\begin{theorem}{}{}{g:theorem:idp16549912}%
If \(\lambda\) is an eigenvalue of \(A\), the dimension of the eigenspace corresponding to \(\lambda\) is less than or equal to the multiplicity of \(\lambda\).%
\end{theorem}
The examples we have seen raise another important point. The matrix \(A = \left[ \begin{array}{ccc} 1\amp 0\amp 1\\0\amp 1\amp 1\\0\amp 0\amp 2 \end{array} \right]\) from our geometric example has two eigenvalues \(1\) and \(2\), with the eigenvalue 1 having multiplicity 2. If we let \(E_{\lambda}\) represent the eigenspace of \(A\) corresponding to the eigenvalue \(\lambda\), then \(\dim(E_1)=2\) and \(\dim(E_2) = 1\). If we change this matrix slightly to the matrix \(B = \left[ \begin{array}{crc} 2\amp 0\amp 1 \\ 0\amp 1\amp 1 \\ 0\amp 0\amp 1 \end{array} \right]\) we see that \(B\) has two eigenvalues \(1\) and \(2\), with the eigenvalue 1 having multiplicity 2. However, in this case we have \(\dim(E_1) = 1\) (like the example in from \hyperref[x:activity:act_4_b_1]{Activity~{\xreffont\ref{x:activity:act_4_b_1}}} (a) and \hyperref[x:activity:act_4_b_3]{Activity~{\xreffont\ref{x:activity:act_4_b_3}}} (a)). In this case the vector \(\vv_1 = [1 \ 0 \ 0]^{\tr}\) forms a basis for \(E_2\) and the vector \(\vv_2 = [0 \ 1 \ 0]^{\tr}\) forms a basis for \(E_1\). We can visualize the action of \(B\) on the square formed by \(\vv_1\) and \(\vv_2\) in the \(xy\)-plane as a scaling by 2 in the \(\vv_1\) direction as shown in \hyperref[x:figure:F_4_b_2]{Figure~{\xreffont\ref{x:figure:F_4_b_2}}}, but since we do not have a third linearly independent eigenvector, the action of \(B\) in the direction of \([0 \ 0 \ 1]^{\tr}\) is not so clear.%
\begin{figureptx}{A box and a transformed box.}{x:figure:F_4_b_2}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/4_b_Eigenspaces_2.pdf}
\end{image}%
\tcblower
\end{figureptx}%
So the action of a matrix transformation can be more easily visualized if the dimension of each eigenspace is equal to the multiplicity of the corresponding eigenvalue. This geometric perspective leads us to define the geometric multiplicity of an eigenvalue.%
\begin{definition}{}{g:definition:idp16303640}%
\index{multiplicity!geometric}%
The \terminology{geometric multiplicity} of an eigenvalue of an \(n \times n\) matrix \(A\) is the dimension of the corresponding eigenspace \(\Nul (A-\lambda I_n)\).%
\end{definition}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_chareq_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp16305688}%
Let \(A = \left[ \begin{array}{rcr} -1\amp 0\amp -2 \\ 2\amp 1\amp 2 \\ 0\amp 0\amp 1 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the characteristic polynomial of \(A\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp16307352}{}\quad{}The characteristic polynomial of \(A\) is%
\begin{align*}
p(\lambda) \amp = \det(A - \lambda I_3)\\
\amp = \det\left( \left[ \begin{array}{ccc} -1-\lambda\amp 0\amp -2\\
2\amp 1-\lambda\amp 2\\
0\amp 0\amp 1-\lambda \end{array}\right] \right)\\
\amp = (-1-\lambda)(1-\lambda)(1-\lambda)\text{.}
\end{align*}
%
\item{}Factor the characteristic polynomial and find the eigenvalues of \(A\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp16599064}{}\quad{}The eigenvalues of \(A\) are the solutions to the characteristic equation. Since%
\begin{equation*}
p(\lambda) = (-1-\lambda)(1-\lambda)(1-\lambda) = 0
\end{equation*}
implies \(\lambda = -1\) or \(\lambda = 1\), the eigenvalues of \(A\) are \(1\) and \(-1\).%
\item{}Find a basis for each eigenspace of \(A\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp16597656}{}\quad{}To find a basis for the eigenspace of \(A\) corresponding to the eigenvalue \(1\), we find a basis for \(\Nul (A-I_3)\). The reduced row echelon form of \(A - I_ 3 = \left[ \begin{array}{rcr} -2\amp 0\amp -2 \\ 2\amp 0\amp 2 \\ 0\amp 0\amp 0 \end{array} \right]\) is \(\left[ \begin{array}{ccc} 1\amp 0\amp 1 \\ 0\amp 0\amp 0 \\ 0\amp 0\amp 0 \end{array} \right]\). If \(\vx = \left[ \begin{array}{c} x_1\\x_2\\x_3 \end{array}  \right]\), then \((A-I_3)\vx = \vzero\) has general solution%
\begin{equation*}
\vx = \left[ \begin{array}{c} x_1\\x_2\\x_3 \end{array}  \right] = \left[ \begin{array}{r} -x_3\\x_2\\x_3 \end{array}  \right] = x_2 \left[ \begin{array}{c} 0\\1\\0 \end{array}  \right] + x_3\left[ \begin{array}{r} -1\\0\\1 \end{array}  \right]\text{.}
\end{equation*}
Therefore, \(\{[0 \ 1 \ 0]^{\tr}, [-1 \ 0 \ 1]^{\tr}\}\) is a basis for the eigenspace of \(A\) corresponding to the eigenvalue \(1\). To find a basis for the eigenspace of \(A\) corresponding to the eigenvalue \(-1\), we find a basis for \(\Nul (A+I_3)\). The reduced row echelon form of \(A + I_ 3 = \left[ \begin{array}{ccr} 0\amp 0\amp -2 \\ 2\amp 2\amp 2 \\ 0\amp 0\amp 2 \end{array} \right]\) is \(\left[ \begin{array}{ccc} 1\amp 1\amp 0 \\ 0\amp 0\amp 1 \\ 0\amp 0\amp 0 \end{array} \right]\). If \(\vx = \left[ \begin{array}{c} x_1\\x_2\\x_3 \end{array}  \right]\), then \((A+I_3)\vx = \vzero\) has general solution%
\begin{equation*}
\vx = \left[ \begin{array}{c} x_1\\x_2\\x_3 \end{array}  \right] = \left[ \begin{array}{r} -x_2\\x_2\\0 \end{array}  \right] = x_2 \left[ \begin{array}{r} -1\\1\\0 \end{array}  \right]\text{.}
\end{equation*}
Therefore, a basis for the eigenspace of \(A\) corresponding to the eigenvalue \(-1\) is \(\{[-1 \ 1 \ 0]^{\tr}\}\).%
\item{}Is it possible to find a basis for \(\R^3\) consisting of eigenvectors of \(A\)? Explain.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp16605976}{}\quad{}Let \(\vv_1 = [0 \ 1 \ 0]^{\tr}, [-1 \ 0 \ 1]^{\tr}\), \(\vv_2 = [-1 \ 0 \ 1]^{\tr}\), and \(\vv_3 = [-1 \ 1 \ 0]^{\tr}\). Since eigenvectors corresponding to different eigenvalues are linearly independent, and since neither \(\vv_1\) nor \(\vv_2\) is a scalar multiple of the other, we can conclude that the set \(\{\vv_1, \vv_2, \vv_3\}\) is a linearly independent set with \(3 = \dim(\R^3)\) vectors. Therefore, \(\{\vv_1, \vv_2, \vv_3\}\) is a basis for \(\R^3\) consisting of eigenvectors of \(A\).%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp16618392}%
Find a \(3 \times 3\) matrix \(A\) that has an eigenvector \(\vv_1 = [1 \ 0 \ 1]^{\tr}\) with corresponding eigenvalue \(\lambda_1 = 2\), an eigenvector \(\vv_2 = [0 \ 2 \ -3]^{\tr}\) with corresponding eigenvalue \(\lambda_2 = -3\), and an eigenvector \(\vv_3 = [-4 \ 0 \ 5]^{\tr}\) with corresponding eigenvalue \(\lambda_3 = 5\). Explain your process.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp16615448}{}\quad{}We are looking for a \(3 \times 3\) matrix \(A\) such that \(A \vv_1 = 2 \vv_1\), \(A \vv_2 = -3 \vv_2\) and \(A \vv_3 = 5 \vv_3\). Since \(\vv_1\), \(\vv_2\), and \(\vv_3\) are eigenvectors corresponding to different eigenvalues, \(\vv_1\), \(\vv_2\), and \(\vv_3\) are linearly independent. So the matrix \([\vv_1 \ \vv_2 \ \vv_3]\) is invertible. It follows that%
\begin{align*}
A[\vv_1 \ \vv_2 \ \vv_3] \amp = [A\vv_1 \ A\vv_2 \ A\vv_3]\\
A \left[\begin{array}{rcr} 1\amp 0\amp -4\\
0\amp 2\amp 0\\
1\amp -3\amp 5 \end{array} \right] \amp = [2\vv_1 \ -3\vv_2 \ 5\vv_3]\\
A \left[\begin{array}{crr} 1\amp 0\amp -4\\
0\amp 2\amp 0\\
1\amp -3\amp 5 \end{array} \right] \amp = \left[ \begin{array}{crr} 2\amp 0\amp -20\\
0\amp -6\amp 0\\
2\amp 9\amp 25 \end{array} \right]\\
A \amp =  \left[ \begin{array}{crr} 2\amp 0\amp -20\\
0\amp -6\amp 0\\
2\amp 9\amp 25 \end{array} \right] \left[\begin{array}{crr} 1\amp 0\amp -4\\
0\amp 2\amp 0\\
1\amp -3\amp 5 \end{array} \right]^{-1}\\
A \amp = \left[ \begin{array}{crr} 2\amp 0\amp -20\\
0\amp -6\amp 0\\
2\amp 9\amp 25 \end{array} \right]  \left[ \begin{array}{rcc} \frac{5}{9}\amp \frac{2}{3}\amp \frac{4}{9}\\
0\amp \frac{1}{2}\amp 0\\
-\frac{1}{9}\amp \frac{1}{6}\amp \frac{1}{9} \end{array} \right]\\
A \amp = \left[ \begin{array}{rrr} \frac{10}{3}\amp -2\amp -\frac{4}{3}\\
0\amp -3\amp 0\\
-\frac{5}{3}\amp 10\amp \frac{11}{3} \end{array} \right]\text{.}
\end{align*}
%
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_chareq_summ}
In this section we studied the characteristic polynomial of a matrix and similar matrices.%
\begin{itemize}[label=\textbullet]
\item{}If \(A\) is an \(n \times n\) matrix, the characteristic polynomial of \(A\) is the polynomial%
\begin{equation*}
\det(A-\lambda I_n)\text{,}
\end{equation*}
where \(I_n\) is the \(n \times n\) identity matrix.%
\item{}If \(A\) is an \(n \times n\) matrix, the characteristic equation of \(A\) is the equation%
\begin{equation*}
\det(A-\lambda I_n) = 0\text{.}
\end{equation*}
%
\item{}The characteristic equation of a square matrix provides us an algebraic method to find the eigenvalues of the matrix.%
\item{}The eigenvalues of an upper or lower-triangular matrix are the entries on the diagonal.%
\item{}There are at most \(n\) eigenvalues of an \(n\times n\) matrix.%
\item{}For a real matrix \(A\), if an eigenvalue \(\lambda\) of \(A\) is complex, then the complex conjugate of \(\lambda\) is also an eigenvalue.%
\item{}The algebraic multiplicity of an eigenvalue \(\lambda\) is the multiplicity of \(\lambda\) as a root of the characteristic equation.%
\item{}The dimension of the eigenspace corresponding to an eigenvalue \(\lambda\) is less than or equal to the algebraic multiplicity of \(\lambda\).%
\end{itemize}
%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_chareq_exer}
\begin{divisionexercise}{1}{}{}{x:exercise:ex_determinant_eigenvalues}%
There is a useful relationship between the determinant and eigenvalues of a matrix \(A\) that we explore in this exercise.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(B = \left[ \begin{array}{cc} 2\amp 3\\8\amp 4 \end{array} \right]\). Find the determinant of \(B\) and the eigenvalues of \(B\), and compare \(\det(B)\) to the eigenvalues of \(B\).%
\item{}Let \(A\) be an \(n \times n\) matrix. In this part of the exercise we argue the general case illustrated in the previous part \textemdash{} that \(\det(A)\) is the product of the eigenvalues of \(A\). Let \(p(\lambda) = \det(A - \lambda I_n)\) be the characteristic polynomial of \(A\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Let \(\lambda_1\), \(\lambda_2\), \(\ldots\), \(\lambda_n\) be the eigenvalues of \(A\) (note that these eigenvalues may not all be distinct). Recall that if \(r\) is a root of a polynomial \(q(x)\), then \((x-r)\) is a factor of \(q(x)\). Use this idea to explain why%
\begin{equation*}
p(\lambda) = (-1)^{n} (\lambda-\lambda_1)(\lambda- \lambda_2) \cdots (\lambda - \lambda_n)\text{.}
\end{equation*}
%
\item{}Explain why \(p(0) = \lambda_1 \lambda_2 \cdots \lambda_n\).%
\item{}Why is \(p(0)\) also equal to \(\det(A)\). Explain how we have shown that \(\det(A)\) is the product of the eigenvalues of \(A\).%
\end{enumerate}
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp16663704}%
Find the eigenvalues of the following matrices. For each eigenvalue, determine its algebraic and geometric multiplicity.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(A=\left[ \begin{array}{ccc} 1\amp 1\amp 1\\1\amp 1\amp 1\\1\amp 1\amp 1 \end{array} \right]\)%
\item{}\(A=\left[ \begin{array}{ccc} 2\amp 0\amp 3\\0\amp 1\amp 0\\0\amp 1\amp 2 \end{array} \right]\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp16662040}%
Let \(A\) be an \(n \times n\) matrix. Use the characteristic equation to explain why \(A\) and \(A^\tr\) have the same eigenvalues.%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp16669592}%
Find three \(3 \times 3\) matrices whose eigenvalues are 2 and 3, and for which the dimensions of the eigenspaces for \(\lambda=2\) and \(\lambda=3\) are different.%
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp16675608}%
Suppose \(A\) is an \(n\times n\) matrix and \(B\) is an invertible \(n\times n\) matrix. Explain why the characteristic polynomial of \(A\) is the same as the characteristic polynomial of \(BAB^{-1}\), and hence, as a result, the eigenvalues of \(A\) and \(BAB^{-1}\) are the same.%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp16684312}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
If the determinant of a \(2 \times 2\) matrix \(A\) is positive, then \(A\) has two distinct real eigenvalues.%
\item{}\lititle{True\slash{}False.}\par%
If two \(2 \times 2\) matrices have the same eigenvalues, then the have the same eigenvectors.%
\item{}\lititle{True\slash{}False.}\par%
The characteristic polynomial of an \(n \times n\) matrix has degree \(n\).%
\item{}\lititle{True\slash{}False.}\par%
If \(R\) is the reduced row echelon form of an \(n \times n\) matrix \(A\), then \(A\) and \(R\) have the same eigenvalues.%
\item{}\lititle{True\slash{}False.}\par%
If \(R\) is the reduced row echelon form of an \(n \times n\) matrix \(A\), and \(\vv\) is an eigenvector of \(A\), then \(\vv\) is an eigenvector of \(R\).%
\item{}\lititle{True\slash{}False.}\par%
Let \(A\) and \(B\) be \(n \times n\) matrices with characteristic polynomials \(p_A(\lambda)\) and \(p_B(\lambda)\), respectively. If \(A \neq B\), then \(p_A(\lambda) \neq p_B(\lambda)\).%
\item{}\lititle{True\slash{}False.}\par%
Every matrix has at least one eigenvalue.%
\item{}\lititle{True\slash{}False.}\par%
Suppose \(A\) is a \(3 \times 3\) matrix with three distinct eigenvalues. Then any three eigenvectors, one for each eigenvalue, will form a basis of \(\R^3\).%
\item{}\lititle{True\slash{}False.}\par%
If an eigenvalue \(\lambda\) is repeated 3 times among the eigenvalues of a matrix, then there are at most 3 linearly independent eigenvectors corresponding to \(\lambda\).%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: The Ehrenfest Model}
\typeout{************************************************}
%
\begin{sectionptx}{Project: The Ehrenfest Model}{}{Project: The Ehrenfest Model}{}{}{x:section:sec_proj_ehrenfest}
To realistically model the diffusion of gas molecules we would need to consider a system with a large number of balls as substitutes for the gas molecules. However, the main idea can be seen in a model with a much smaller number of balls, as we will do now. Suppose we have two bins that contain a total of \(4\) balls between them. Label the bins as Bin 1 and Bin 2. In this case we can think of entropy as the number of different possible ways the balls can be arranged in the system. For example, there is only \(1\) way for all of the balls to be in Bin 1 (low entropy), but there are \(4\) ways that we can have one ball in Bin 1 (choose any one of the four different balls, which can be distinguished from each other) and \(3\) balls in Bin 2 (higher entropy). The highest entropy state has the balls equally distributed between the bins (with \(6\) different ways to do this).%
\par
We assume that there is a way for balls to move from one bin to the other (like having gas molecules pass through a permeable membrane). A way to think about this is that we select a ball (from ball 1 to ball 4, which are different balls) and move that ball from its current bin to the other bin. Consider a ``move'' to be any instance when a ball changes bins. A \terminology{state} is any configuration of balls in the bins at a given time, and the state changes when a ball is chosen at random and moved to the other bin. The possible states are to have 0 balls in Bin 1 and 4 balls in Bin 2 (State 0, entropy 1), 1 ball in Bin 1 and 3 in Bin 2 (State 1, entropy 4), 2 balls in each Bin (State 2, entropy 6), 3 balls in Bin 1 and 1 ball in Bin 2 (State 3, entropy 4), and 4 balls in Bin 1 and 0 balls in Bin 2 (State 4, entropy 1). These states are shown in \hyperref[x:figure:F_Ehrenfest]{Figure~{\xreffont\ref{x:figure:F_Ehrenfest}}}.%
\begin{figureptx}{States}{x:figure:F_Ehrenfest}{}%
\begin{image}{0.25}{0.5}{0.25}%
\includegraphics[width=\linewidth]{external/4_b_states.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\begin{project}{}{x:project:act_Eherenfest_model}%
To model the system of balls in bins we need to understand how the system can transform from one state to another. It suffices to count the number of balls in Bin 1 (since the remaining balls will be in Bin 2). Even though the balls are labeled, our count only cares about how many balls are in each bin. Let \(\vx_0 = [x_0, x_1, x_2, x_3, x_4]^{\tr}\), where \(x_i\) is the probability that Bin 1 contains \(i\) balls, and let \(\vx_1 = \left[ x_0^1, x_1^1, x_2^1, x_3^1, x_4^1 \right]^{\tr}\), where \(x_i^1\) is the probability that Bin 1 contains \(i\) balls after the first move. We will call the vectors \(\vx_0\) and \(\vx_1\) \terminology{probability distributions} of balls in bins. Note that since all four balls have to be placed in some bin, the sum of the entries in our probability distribution vectors must be \(1\). Recall that a move is an instance when a ball changes bins. We want to understand how \(\vx_1\) is obtained from \(\vx_0\). In other words, we want to figure out what the probability that Bin 1 contains 0, 1, 2, 3, or 4 balls after one ball changes bins if our initial probability distribution of balls in bins is \(\vx_0\).%
\par
We begin by analyzing the ways that a state can change. For example,%
\begin{itemize}[label=\textbullet]
\item{}Suppose there are \(0\) balls in Bin 1. (In our probability distribution \(\vx_0\), this happens with probability \(x_0\).) Then there are four balls in Bin 2. The only way for a ball to change bins is if one of the four balls moves from Bin 2 to Bin 1, putting us in State 1. Regardless of which ball moves, we will always be put in State 1, so this happens with a probability of \(1\). In other words, if the probability that Bin 1 contains \(0\) balls is \(x_0\), then there is a probability of \((1)x_0\) that Bin 1 will contain 1 ball after the move.%
\item{}Suppose we have 1 ball in Bin 1. There are four ways this can happen (since there are four balls, and the one in Bin 1 is selected at random from the four balls), so the probability of a given ball being in Bin 1 is \(\frac{1}{4}\).%
%
\begin{itemize}[label=$\circ$]
\item{}If the ball in Bin 1 moves, that move puts us in State \(0\). In other words, if the probability that Bin 1 contains 1 ball is \(x_1\), then there is a probability of \(\frac{1}{4}x_1\) that Bin 1 will contain \(0\) balls after a move.%
\item{}If any of the \(3\) balls in Bin 2 moves (each moves with probability \(\frac{3}{4}\)), that move puts us in State 2. In other words, if the probability that Bin 1 contains 1 ball is \(x_1\), then there is a probability of \(\frac{3}{4}x_1\) that Bin 1 will contain \(2\) balls after a move.%
\end{itemize}
\end{itemize}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Complete this analysis to explain the probabilities if there are \(2\), \(3\), or \(4\) balls in Bin 1.%
\item{}Explain how the results of part (a) show that%
\begin{alignat*}{6}
{}x_0^1  \amp =  \amp {0}x_0   \amp {+}  \amp {\frac{1}{4}}x_1  \amp {+}  \amp {0}x_2  \amp {+} \amp {0}x_3 \amp {+} \amp {0}x_4\\
{}x_1^1  \amp =  \amp {1}x_0   \amp {+}  \amp {0}x_1  \amp {+}  \amp {\frac{1}{2}}x_2  \amp {+} \amp {0}x_3 \amp {+} \amp {0}x_4\\
{}x_2^1  \amp =  \amp {0}x_0   \amp {+}  \amp {\frac{3}{4}}x_1  \amp {+}  \amp {0}x_2  \amp {+} \amp {\frac{3}{4}}x_3 \amp {+} \amp {0}x_4\\
{}x_3^1  \amp =  \amp {0}x_0   \amp {+}  \amp {0}x_1  \amp {+}  \amp {\frac{1}{2}}x_2  \amp {+} \amp {0}x_3 \amp {+} \amp {1}x_4\\
{}x_4^1  \amp =  \amp {0}x_0   \amp {+}  \amp {0}x_1  \amp {+}  \amp {0}x_2  \amp {+} \amp {\frac{1}{4}}x_3 \amp {+} \amp {0}x_4
\end{alignat*}
%
\end{enumerate}
\end{project}%
The system we developed in \hyperref[x:project:act_Eherenfest_model]{Project Activity~{\xreffont\ref{x:project:act_Eherenfest_model}}} has matrix form%
\begin{equation*}
\vx_1 = T \vx_0\text{,}
\end{equation*}
where \(T\) is the \terminology{transition matrix}%
\begin{equation*}
T = \left[  \begin{array}{ccccc} 0 \amp  \frac{1}{4}  \amp  0       \amp  0       \amp  0 \\ 1 \amp  0       \amp  \frac{1}{2}  \amp  0       \amp  0 \\ 0 \amp  \frac{3}{4} \amp  0       \amp  \frac{3}{4}   \amp  0 \\ 0 \amp  0       \amp  \frac{1}{2}   \amp  0       \amp  1 \\ 0 \amp  0       \amp  0       \amp  \frac{1}{4}  \amp  0 \end{array}  \right]\text{.}
\end{equation*}
%
\par
Subsequent moves give probability distribution vectors%
\begin{align*}
\vx_2 \amp = T\vx_1\\
\vx_3 \amp = T\vx_2\\
\vdots \amp   \vdots\\
\vx_k \amp = T\vx_{k-1} \text{.}
\end{align*}
%
\par
This example is an example of a Markov process (see \hyperref[x:definition:def_Markov]{Definition~{\xreffont\ref{x:definition:def_Markov}}}). There are several questions we can ask about this model. For example, what is the long-term behavior of this system, and how does this model relate to entropy? That is, given an initial probability distribution vector \(\vx_0\), the system will have probability distribution vectors \(\vx_1\), \(\vx_2\), \(\ldots\) after subsequent moves. What happens to the vectors \(\vx_k\) as \(k\) goes to infinity, and what does this tell us about entropy? To answer these questions, we will first explore the sequence \(\{\vx_k\}\) numerically, and then use the eigenvalues and eigenvectors of \(T\) to analyze the sequence \(\{\vx_k\}\).%
\begin{project}{}{x:project:act_Ehrenfest_numeric}%
Use appropriate technology to do the following.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Suppose we begin with a probability distribution vector \(\vx_0 = [1 \ 0 \ 0 \ 0 \ 0]^{\tr}\). Calculate vectors \(\vx_k\) for enough values of \(k\) so that you can identify the long term behavior of the sequence. Describe this behavior.%
\item{}Repeat part (a) with%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}\(\vx_0 = \left[0 \ \frac{1}{2} \ \frac{1}{2} \ 0 \ 0\right]^{\tr}\)%
\item{}\(\vx_0 = \left[0 \ \frac{1}{3} \ \frac{1}{3} \ 0 \ \frac{1}{3}\right]^{\tr}\)%
\item{}\(\vx_0 = \left[\frac{1}{5} \ \frac{1}{5} \ \frac{1}{5} \ \frac{1}{5} \ \frac{1}{5}\right]^{\tr}\)%
\end{enumerate}
\end{enumerate}
\end{project}%
In what follows, we investigate the behavior of the sequence \(\{\vx_k\}\) that we uncovered in \hyperref[x:project:act_Ehrenfest_numeric]{Project Activity~{\xreffont\ref{x:project:act_Ehrenfest_numeric}}}.%
\begin{project}{}{x:project:act_Ehrenfest_eigenvalues}%
We use the characteristic polynomial to find the eigenvalues of \(T\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the characteristic polynomial of \(T\). Factor the characteristic polynomial into a product of linear polynomials to show that the eigenvalues of \(T\) are \(0\), \(1\), \(-1\), \(\frac{1}{2}\) and \(-\frac{1}{2}\).%
\item{}As we will see a bit later, certain eigenvectors for \(T\) will describe the end behavior of the sequence \(\{\vx_k\}\). Find eigenvectors for \(T\) corresponding to the eigenvalues \(1\) and \(-1\). Explain how the eigenvector for \(T\) corresponding to the eigenvalue \(1\) explains the behavior of one of the sequences was saw in \hyperref[x:project:act_Ehrenfest_numeric]{Project Activity~{\xreffont\ref{x:project:act_Ehrenfest_numeric}}}. (Any eigenvector of \(T\) with eigenvalue \(1\) is called an \emph{equilibrium} or \emph{steady state} vector.)%
\end{enumerate}
\end{project}%
Now we can analyze the behavior of the sequence \(\{\vx_k\}\).%
\begin{project}{}{x:project:act_Ehrenfest_basis}%
To make the notation easier, we will let \(\vv_1\) be an eigenvector of \(T\) corresponding to the eigenvalue \(0\), \(\vv_2\) an eigenvector of \(T\) corresponding to the eigenvalue \(1\), \(\vv_3\) an eigenvector of \(T\) corresponding to the eigenvalue \(-1\), \(\vv_4\) an eigenvector of \(T\) corresponding to the eigenvalue \(\frac{1}{2}\), and \(\vv_5\) an eigenvector of \(T\) corresponding to the eigenvalue \(-\frac{1}{2}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why \(\{\vv_1, \vv_2, \vv_3, \vv_4, \vv_5\}\) is a basis of \(\R^5\).%
\item{}Let \(\vx_0\) be any initial probability distribution vector. Explain why we can write \(\vx_0\) as%
\begin{equation*}
\vx_0 = a_1 \vv_1 + a_2 \vv_2 + a_3 \vv_3 + a_4 \vv_4 + a_5 \vv_5 = \sum_{i=1}^5 a_i \vv_i
\end{equation*}
for some scalars \(a_1\), \(a_2\), \(a_3\), \(a_4\), and \(a_5\).%
\end{enumerate}
\end{project}%
We can now use the eigenvalues and eigenvectors of \(T\) to write the vectors \(\vx_k\) in a convenient form. Let \(\lambda_1 = 0\), \(\lambda_2=1\), \(\lambda_3=-1\), \(\lambda_4=\frac{1}{2}\), and \(\lambda_5=-\frac{1}{2}\). Notice that%
\begin{align*}
\vx_1 \amp = T \vx_0\\
\amp = T(a_1  \vv_1 + a_2 \vv_2 + a_3  \vv_3 + a_4  \vv_4 + a_5 \vv_5)\\
\amp = a_1  T\vv_1 + a_2 T\vv_2 + a_3  T\vv_3 + a_4 T\vv_4 + a_5 T\vv_5\\
\amp = a_1\lambda_1 \vv_1 + a_2\lambda_2 \vv_2 + a_3 \lambda_3 \vv_3 + a_4 \lambda_4 \vv_4 + a_5 \lambda_5 \vv_5\\
\amp = \sum_{i=1}^5 a_i \lambda_i \vv_i\text{.}
\end{align*}
%
\par
Similarly%
\begin{equation*}
\vx_2 = T \vx_1 = T\left(\sum_{i=1}^5 a_i \lambda_i\vv_i\right) = \sum_{i=1}^5 a_i \lambda_i T\vv_i = \sum_{i=1}^5 a_i\lambda_i^2 \vv_i\text{.}
\end{equation*}
%
\par
We can continue in this manner to ultimately show that for each positive integer \(k\) we have%
\begin{equation}
\vx_k = \sum_{i=1}^5 a_i\lambda_i^k \vv_i\label{x:men:eq_Ehrenfest_sum}
\end{equation}
when \(\vx_0 = \sum_{i=1}^5 a_i \vv_i\).%
\begin{project}{}{x:project:act_Ehrenfest_entropy}%
Recall that we are interested in understanding the behavior of the sequence \(\{\vx_k\}\) as \(k\) goes to infinity.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Equation \hyperref[x:men:eq_Ehrenfest_sum]{({\xreffont\ref{x:men:eq_Ehrenfest_sum}})} shows that we need to know \(\lim_{k \to \infty} \lambda_i^k\) for each \(i\) in order to analyze \(\lim_{k \to \infty} \vx_k\). Calculate or describe these limits.%
\item{}Use the result of part (a), Equation \hyperref[x:men:eq_Ehrenfest_sum]{({\xreffont\ref{x:men:eq_Ehrenfest_sum}})}, and \hyperref[x:project:act_Ehrenfest_eigenvalues]{Project Activity~{\xreffont\ref{x:project:act_Ehrenfest_eigenvalues}}} (b) to explain why the sequence \(\{\vx_k\}\) is either eventually fixed or oscillates between two states. Compare to the results from \hyperref[x:project:act_Ehrenfest_numeric]{Project Activity~{\xreffont\ref{x:project:act_Ehrenfest_numeric}}}. How are these results related to entropy? You may use the facts that%
\begin{itemize}[label=\textbullet]
\item{}\(\vv_1 = [1 \ 0 \ -2 \ 0 \ 1]^{\tr}\) is an eigenvector for \(T\) corresponding to the eigenvalue \(0\),%
\item{}\(\vv_2 = [1 \ 4 \ 6 \ 4 \ 1]^{\tr}\) is an eigenvector for \(T\) corresponding to the eigenvalue \(1\),%
\item{}\(\vv_3 = [1 \ -4 \ 6 \ -4 \ 1]^{\tr}\) is an eigenvector for \(T\) corresponding to the eigenvalue \(-1\),%
\item{}\(\vv_4 = [-1 \ -2 \ 0\ 2 \ 1]^{\tr}\) is an eigenvector for \(T\) corresponding to the eigenvalue \(\frac{1}{2}\),%
\item{}\(\vv_5 = [-1 \ 2 \ 0 \ -2 \ 1]^{\tr}\) is an eigenvector for \(T\) corresponding to the eigenvalue \(-\frac{1}{2}\).%
\end{itemize}
%
\end{enumerate}
\end{project}%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 19 Diagonalization}
\typeout{************************************************}
%
\begin{chapterptx}{Diagonalization}{}{Diagonalization}{}{}{x:chapter:chap_diagonalization}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp16790936}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What is a diagonal matrix?%
\item{}What does it mean to diagonalize a matrix?%
\item{}What does it mean for two matrices to be similar?%
\item{}What important properties do similar matrices share?%
\item{}Under what conditions is a matrix diagonalizable?%
\item{}When a matrix \(A\) is diagonalizable, what is the structure of a matrix \(P\) that diagonalizes \(A\)?%
\item{}Why is diagonalization useful?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: The Fibonacci Numbers}
\typeout{************************************************}
%
\begin{sectionptx}{Application: The Fibonacci Numbers}{}{Application: The Fibonacci Numbers}{}{}{x:section:sec_appl_fib_num}
\index{Fibonacci sequence}%
In 1202 Leonardo of Pisa (better known as Fibonacci) published \pubtitle{Liber Abaci} (roughly translated as \pubtitle{The Book of Calculation}), in which he constructed a mathematical model of the growth of a rabbit population. The problem Fibonacci considered is that of determining the number of pairs of rabbits produced in a given time period beginning with an initial pair of rabbits. Fibonacci made the assumptions that each pair of rabbits more than one month old produces a new pair of rabbits each month, and that no rabbits die. (We ignore any issues about that might arise concerning the gender of the offspring.) If we let \(F_n\) represent the number of rabbits in month \(n\), Fibonacci produced the model%
\begin{equation}
F_{n+2} = F_{n+1} + F_{n}\text{,}\label{x:men:eq_Fibonacci}
\end{equation}
for \(n \geq 0\) where \(F_0 = 0\) and \(F_1 = 1\). The resulting sequence%
\begin{equation*}
1,1,2,3,5,8,13,21, \ldots
\end{equation*}
is a very famous sequence in mathematics and is called the Fibonacci sequence. This sequence is thought to model many natural phenomena such as number of seeds in a sunflower and anything which grows in a spiral form. It is so famous in fact that it has a journal devoted entirely to it. As a note, while Fibonacci's work \pubtitle{Liber Abaci} introduced this sequence to the western world, it had been described earlier Sanskrit texts going back as early as the sixth century.%
\par
By definition, the Fibonacci numbers are calculated by recursion. This is a very ineffective way to determine entries \(F_n\) for large \(n\). Later in this section we will derive a fascinating and unexpected formula for the Fibonacci numbers using the idea of diagonalization.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_diag_intro}
As we have seen when studying Markov processes, each state is dependent on the previous state. If \(\vx_0\) is the initial state and \(A\) is the transition matrix, then the \(n\)th state is found by \(A^n \vx_0\). In these situations, and others, it is valuable to be able to quickly and easily calculate powers of a matrix. We explore a way to do that in this section.%
\begin{exploration}{}{x:exploration:pa_4_c}%
Consider a very simplified weather forecast. Let us assume there are two possible states for the weather: rainy (\(R\)) or sunny(\(S\)). Let us also assume that the weather patterns are stable enough that we can reasonably predict the weather tomorrow based on the weather today. If is is sunny today, then there is a 70\% chance that it will be sunny tomorrow, and if it is rainy today then there is a 40\% chance that it will be rainy tomorrow. If \(\vx_0 = \left[ \begin{array}{c} s \\ r \end{array}  \right]\) is a state vector that indicates a probability \(s\) that it is sunny and probability \(r\) that it is rainy on day \(0\), then%
\begin{equation*}
\vx_1 = \left[ \begin{array}{cc} 0.70\amp 0.40 \\ 0.30\amp 0.60 \end{array}  \right] \vx_0
\end{equation*}
tells us the likelihood of it being sunny or rainy on day 1. Let \(A = \left[ \begin{array}{cc} 0.70\amp 0.40 \\ 0.30\amp 0.60 \end{array}  \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Suppose it is sunny today, that is \(\vx_0 = \left[ \begin{array}{c} 1 \\ 0 \end{array} \right]\). Calculate \(\vx_1 = A \vx_0\) and explain how this matrix-vector product tells us the probability that it will be sunny tomorrow.%
\item{}Calculate \(\vx_2 = A\vx_1\) and interpret the meaning of each component of the product.%
\item{}Explain why \(\vx_2 = A^2 \vx_0\). Then explain in general why \(\vx_n = A^n \vx_0\).%
\item{}The previous result demonstrates that to determine the long-term probability of a sunny or rainy day, we want to be able to easily calculate powers of the matrix \(A\). Use a computer algebra system (e.g., Maple, Mathematica, Wolfram\(|\)Alpha) to calculate the entries of \(\vx_{10}\), \(\vx_{20}\), and \(\vx_{30}\). Based on this data, what do you expect the long term probability of any day being a sunny one?%
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Diagonalization}
\typeout{************************************************}
%
\begin{sectionptx}{Diagonalization}{}{Diagonalization}{}{}{x:section:sec_diag}
In \hyperref[x:exploration:pa_4_c]{Preview Activity~{\xreffont\ref{x:exploration:pa_4_c}}} we saw how if we can powers of a matrix we can make predictions about the long-term behavior of some systems. In general, calculating powers of a matrix can be a very difficult thing, but there are times when the process is straightforward.%
\begin{activity}{}{x:activity:act_4_c_0}%
Let \(D = \left[ \begin{array}{cc} 2\amp 0 \\ 0\amp 3 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(D^2 = \left[ \begin{array}{cc} 2^2\amp 0 \\ 0\amp 3^2 \end{array} \right]\).%
\item{}Show that \(D^3 = \left[ \begin{array}{cc} 2^3\amp 0 \\ 0\amp 3^3 \end{array} \right]\). \par\smallskip%
\noindent\(D^3 = DD^2\).%
%
\item{}Explain in general why \(D^n = \left[ \begin{array}{cc} 2^n\amp 0 \\ 0\amp 3^n \end{array} \right]\) for any positive integer \(n\).%
\end{enumerate}
\end{activity}%
\hyperref[x:activity:act_4_c_0]{Activity~{\xreffont\ref{x:activity:act_4_c_0}}} illustrates that calculating powers of square matrices whose only nonzero entries are along the diagonal is rather simple. In general, if%
\begin{equation*}
D = \left[ \begin{array}{cccccc} d_{11}   \amp 0       \amp 0        \amp \cdots      \amp 0    \amp 0 \\ 0       \amp  d_{22}   \amp 0        \amp \cdots      \amp 0    \amp 0 \\ \vdots  \amp  0          \amp 0        \amp \ddots      \amp       \amp \vdots \\ 0       \amp  0       \amp 0          \amp  \cdots    \amp 0    \amp d_{nn} \end{array}  \right]\text{,}
\end{equation*}
then%
\begin{equation*}
D^k = \left[ \begin{array}{cccccc} d_{11}^k   \amp 0       \amp 0        \amp \cdots      \amp 0    \amp 0 \\ 0       \amp  d_{22}^k   \amp 0        \amp \cdots      \amp 0    \amp 0 \\ \vdots  \amp  0            \amp 0        \amp \ddots      \amp       \amp \vdots \\ 0       \amp  0         \amp 0          \amp  \cdots    \amp 0    \amp d_{nn}^k \end{array}  \right]
\end{equation*}
for any positive integer \(k\). Recall that a diagonal matrix is a matrix whose only nonzero elements are along the diagonal (see \hyperref[x:definition:def_special_matrices]{Definition~{\xreffont\ref{x:definition:def_special_matrices}}}). In this section we will see that matrices that are similar to diagonal matrices have some very nice properties, and that diagonal matrices are useful in calculations of powers of matrices.%
\par
We can utilize the method of calculating powers of diagonal matrices to also easily calculate powers of other types of matrices.%
\begin{activity}{}{x:activity:act_4_c_0_5}%
Let \(D\) be any matrix, \(P\) an invertible matrix, and let \(A = P^{-1}DP\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(A^2 = P^{-1}D^2P\).%
\item{}Show that \(A^3 = P^{-1}D^3P\).%
\item{}Explain in general why \(A^n = P^{-1}D^nP\) for positive integers \(n\).%
\end{enumerate}
\end{activity}%
As \hyperref[x:activity:act_4_c_0_5]{Activity~{\xreffont\ref{x:activity:act_4_c_0_5}}} illustrates, to calculate the powers of a matrix of the form \(P^{-1}DP\) we only need determine the powers of the matrix \(D\). If \(D\) is a diagonal matrix, this is especially straightforward.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Similar Matrices}
\typeout{************************************************}
%
\begin{sectionptx}{Similar Matrices}{}{Similar Matrices}{}{}{x:section:sec_mtx_similar}
Similar matrices play an important role in certain calculations. For example, \hyperref[x:activity:act_4_c_0_5]{Activity~{\xreffont\ref{x:activity:act_4_c_0_5}}} showed that if we can write a square matrix \(A\) in the form \(A = P^{-1}DP\) for some invertible matrix \(P\) and diagonal matrix \(D\), then finding the powers of \(A\) is straightforward. As we will see, the relation \(A = P^{-1}DP\) will imply that the matrices \(A\) and \(D\) share many properties.%
\begin{definition}{}{g:definition:idp16839192}%
\index{similar matrices}%
The \(n \times n\) matrix \(A\) is \terminology{similar} to the \(n \times n\) matrix \(B\) if there is an invertible matrix \(P\) such that \(A = P^{-1}BP\).%
\end{definition}
\begin{activity}{}{x:activity:act_4_c_1}%
Let \(A = \left[ \begin{array}{cc} 1\amp 1\\2\amp 0 \end{array} \right]\) and \(B = \left[ \begin{array}{cr} 2\amp 2\\0\amp -1 \end{array} \right]\). Assume that \(A\) is similar to \(B\) via the matrix \(P = \left[ \begin{array}{cc} 2\amp 1\\2\amp 2 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Calculate \(\det(A)\) and \(\det(B)\). What do you notice?%
\item{}Find the characteristic polynomials of \(A\) and \(B\). What do you notice?%
\item{}What can you say about the eigenvalues of \(A\) and \(B\)? Explain.%
\item{}Explain why \(\vx = \left[ \begin{array}{c} 1\\1 \end{array} \right]\) is an eigenvector for \(A\) with eigenvalue 2. Is \(\vx\) an eigenvector for \(B\) with eigenvalue 2? Why or why not?%
\end{enumerate}
\end{activity}%
\hyperref[x:activity:act_4_c_1]{Activity~{\xreffont\ref{x:activity:act_4_c_1}}} suggests that similar matrices share some, but not all, properties. Note that if \(A = P^{-1}BP\), then \(B = Q^{-1}AQ\) with \(Q = P^{-1}\). So if \(A\) is similar to \(B\), then \(B\) is similar to \(A\). Similarly (no pun intended), since \(A = I^{-1}AI\) (where \(I\) is the identity matrix), then any square matrix is similar to itself. Also, if \(A = P^{-1}BP\) and \(B = M^{-1}CM\), then \(A = (MP)^{-1}C(MP)\). So if \(A\) is similar to \(B\) and \(B\) is similar to \(C\), then \(A\) is similar to \(C\). If you have studied relations, these three properties show that similarity is an equivalence relation on the set of all \(n \times n\) matrices. This is one reason why similar matrices share many important traits, as the next activity highlights.%
\begin{activity}{}{x:activity:act_4_c_2}%
Let \(A\) and \(B\) be similar matrices with \(A = P^{-1}BP\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Use the multiplicative property of the determinant to explain why \(\det(A) = \det(B)\). So similar matrices have the same determinants.%
\item{}Use the fact that \(P^{-1}IP = I\) to show that \(A-\lambda I\) is similar to \(B - \lambda I\).%
\item{}Explain why it follows from (a) and (b) that%
\begin{equation*}
\det(A - \lambda I) = \det(B - \lambda I)\text{.}
\end{equation*}
So similar matrices have the same characteristic polynomial, and the same eigenvalues.%
\end{enumerate}
\end{activity}%
We summarize some properties of similar matrices in the following theorem.%
\begin{theorem}{}{}{g:theorem:idp16588440}%
Let \(A\) and \(B\) be similar \(n \times n\) matrices and \(I\) the \(n \times n\) identity matrix. Then%
\begin{enumerate}
\item{}\(\det(A) = \det(B)\),%
\item{}\(A-\lambda I\) is similar to \(B - \lambda I\),%
\item{}\(A\) and \(B\) have the same characteristic polynomial,%
\item{}\(A\) and \(B\) have the same eigenvalues.%
\end{enumerate}
%
\end{theorem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Similarity and Matrix Transformations}
\typeout{************************************************}
%
\begin{sectionptx}{Similarity and Matrix Transformations}{}{Similarity and Matrix Transformations}{}{}{x:section:sec_sim_mtx_trans}
When a matrix is similar to a diagonal matrix, we can gain insight into the action of the corresponding matrix transformation. As an example, consider the matrix transformation \(T\) from \(\R^2\) to \(\R^2\) defined by \(T(\vx) = A \vx\), where%
\begin{equation}
A = \left[ \begin{array}{cc} 3\amp 1\\1\amp 3 \end{array}  \right]\text{.}\label{x:men:eq_4_c_1}
\end{equation}
%
\par
We are interested in understanding what this matrix transformation does to vectors in \(\R^2\). First we note that \(A\) has eigenvalues \(\lambda_1 = 2\) and \(\lambda_2 = 4\) with corresponding eigenvectors \(\vv_1 = \left[ \begin{array}{r} -1\\1 \end{array}  \right]\) and \(\vv_2 = \left[ \begin{array}{c} 1\\1 \end{array}  \right]\). If we let \(P = [\vv_1 \ \vv_2]\), then you can check that%
\begin{equation*}
P^{-1}AP = D
\end{equation*}
and%
\begin{equation*}
A = PDP^{-1}\text{,}
\end{equation*}
where%
\begin{equation*}
D = \left[ \begin{array}{cc} 2 \amp  0 \\ 0 \amp  4 \end{array}  \right]\text{.}
\end{equation*}
%
\par
Thus,%
\begin{equation*}
T(\vx) = PDP^{-1}\vx\text{.}
\end{equation*}
%
\par
A simple calculation shows that%
\begin{equation*}
P^{-1} = \frac{1}{2}\left[ \begin{array}{rc} -1\amp 1 \\ 1\amp 1 \end{array}  \right]\text{.}
\end{equation*}
%
\par
Let us apply \(T\) to the unit square whose sides are formed by the vectors \(\ve_1 = \left[ \begin{array}{c} 1 \\ 0 \end{array} \right]\) and \(\ve_2 = \left[ \begin{array}{c} 0 \\ 1 \end{array} \right]\) as shown in the first picture in \hyperref[x:figure:F_4_c_1]{Figure~{\xreffont\ref{x:figure:F_4_c_1}}}.%
\par
To apply \(T\) we first multiply \(\ve_1\) and \(\ve_2\) by \(P^{-1}\). This gives us%
\begin{equation*}
P^{-1}\ve_1 = \frac{1}{2}\left[ \begin{array}{r} -1\\1 \end{array}  \right] \text{ and }  \ P^{-1}\ve_2 = \frac{1}{2}\left[ \begin{array}{c} 1\\1 \end{array}  \right]\text{.}
\end{equation*}
%
\par
So \(P^{-1}\) transforms the standard coordinate system into a coordinate system in which columns of \(P^{-1}\) determine the axes, as illustrated in the second picture in \hyperref[x:figure:F_4_c_1]{Figure~{\xreffont\ref{x:figure:F_4_c_1}}}. Applying \(D\) to the output scales by 2 in the first component and by \(4\) in the second component as depicted in the third picture in \hyperref[x:figure:F_4_c_1]{Figure~{\xreffont\ref{x:figure:F_4_c_1}}}. Finally, we apply \(P\) to translate back into the standard \(xy\) coordinate system as shown in the last picture in \hyperref[x:figure:F_4_c_1]{Figure~{\xreffont\ref{x:figure:F_4_c_1}}}. In this case, we can visualize that when we apply the transformation \(T\) to a vector in this system it is just scaled in the \(P^{-1}\ve_1-P^{-1}\ve_2\) system by the matrix \(D\). Then the matrix \(P\) translates everything back to the standard \(xy\) coordinate system.%
\begin{figureptx}{The matrix transformation.}{x:figure:F_4_c_1}{}%
\centering
\begin{sidebyside}{2}{0.0975}{0.0975}{0.195}%
\begin{sbspanel}{0.3}%
\includegraphics[width=\linewidth]{external/4_c_Transformation_1.pdf}
\end{sbspanel}%
\begin{sbspanel}{0.31}%
\includegraphics[width=\linewidth]{external/4_c_Transformation_2.pdf}
\end{sbspanel}%
\end{sidebyside}%
\begin{sidebyside}{2}{0.11375}{0.11375}{0.2275}%
\begin{sbspanel}{0.3}%
\includegraphics[width=\linewidth]{external/4_c_Transformation_3.pdf}
\end{sbspanel}%
\begin{sbspanel}{0.245}%
\includegraphics[width=\linewidth]{external/4_c_Transformation_4.pdf}
\end{sbspanel}%
\end{sidebyside}%
\tcblower
\end{figureptx}%
This geometric perspective provides another example of how having a matrix similar to a diagonal matrix informs us about the situation. In what follows we determine the conditions that determine when a matrix is similar to a diagonal matrix.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Diagonalization in General}
\typeout{************************************************}
%
\begin{sectionptx}{Diagonalization in General}{}{Diagonalization in General}{}{}{x:section:sec_diag_general}
In \hyperref[x:exploration:pa_4_c]{Preview Activity~{\xreffont\ref{x:exploration:pa_4_c}}} and in the matrix transformation example we found that a matrix \(A\) was similar to a diagonal matrix whose columns were eigenvectors of \(A\). This will work for a general \(n \times n\) matrix \(A\) as long as we can find an invertible matrix \(P\) whose columns are eigenvectors of \(A\). More specifically, suppose \(A\) is an \(n \times n\) matrix with \(n\) linearly independent eigenvectors \(\vv_1\), \(\vv_1\), \(\ldots\), \(\vv_n\) with corresponding eigenvalues \(\lambda_1\), \(\lambda_1\), \(\ldots\), \(\lambda_n\) (not necessarily distinct). Let%
\begin{equation*}
P = [ \vv_1 \  \vv_2  \ \vv_3  \ \cdots  \ \vv_n]\text{.}
\end{equation*}
%
\par
Then%
\begin{align*}
AP \amp = [ A\vv_1  \ A\vv_2  \ A\vv_3  \ \cdots  \ A\vv_n]\\
\amp = [ \lambda_1\vv_1  \ \lambda_2\vv_2  \ \lambda_3\vv_3  \ \cdots  \ \lambda_n\vv_n]\\
\amp = [ \vv_1 \  \vv_2  \ \vv_3  \ \cdots  \ \vv_n]\left[ \begin{array}{cccccc} \lambda_1\amp 0\amp 0\amp \cdots\amp 0\amp 0\\
0\amp \lambda_2\amp 0\amp \cdots\amp 0\amp 0\\
\vdots\amp \vdots\amp \amp  \cdots\amp \vdots\amp \vdots\\
0\amp 0\amp 0\amp \cdots\amp \lambda_{n-1}\amp 0\\
0\amp 0\amp 0\amp \cdots\amp 0\amp \lambda_{n} \end{array} \right]\\
\amp = P D\text{.}
\end{align*}
where%
\begin{equation*}
D = \left[ \begin{array}{cccccc} \lambda_1\amp 0\amp 0\amp \cdots\amp 0\amp 0 \\ 0\amp \lambda_2\amp 0\amp \cdots\amp 0\amp 0 \\ \vdots\amp \vdots\amp \amp  \cdots\amp \vdots\amp \vdots \\ 0\amp 0\amp 0\amp \cdots\amp \lambda_{n-1}\amp 0 \\  0\amp 0\amp 0\amp \cdots\amp 0\amp \lambda_{n} \end{array}  \right]\text{.}
\end{equation*}
%
\par
Since the columns of \(P\) are linearly independent, we know \(P\) is invertible, and so%
\begin{equation*}
P^{-1}AP = D\text{.}
\end{equation*}
%
\begin{definition}{}{g:definition:idp17048200}%
\index{matrix!diagonalizable}%
An \(n \times n\) matrix \(A\) is \terminology{diagonalizable} if there is an invertible \(n \times n\) matrix \(P\) so that \(P^{-1}AP\) is a diagonal matrix.%
\end{definition}
In other words, a matrix \(A\) is diagonalizable if \(A\) is similar to a diagonal matrix.%
\begin{paragraphs}{IMPORTANT NOTE.}{g:paragraphs:idp17057544}%
The key notion to the process described above is that in order to diagonalize an \(n\times n\) matrix \(A\), we have to find \(n\) linearly independent eigenvectors for \(A\). When \(A\) is diagonalizable, a matrix \(P\) so that \(P^{-1}AP\) is diagonal is said to \terminology{diagonalize} \(A\).%
\end{paragraphs}%
\begin{activity}{}{x:activity:act_4_c_3}%
Find an invertible matrix \(P\) that diagonalizes \(A\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(A = \left[ \begin{array}{cc} 1\amp 1 \\ 0\amp 2 \end{array} \right]\)%
\item{}\(A = \left[ \begin{array}{ccc} 3\amp 2\amp 4 \\ 2\amp 0\amp 2 \\ 4\amp 2\amp 3 \end{array} \right]\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp17067272}{}\quad{}The eigenvalues of \(A\) are 8 and \(-1\).%
\end{enumerate}
\end{activity}%
It should be noted that there are square matrices that are not diagonalizable. For example, the matrix \(A = \left[ \begin{array}{cc} 1\amp 1 \\ 0\amp 1 \end{array} \right]\) has 1 as its only eigenvalue and the dimension of the eigenspace of \(A\) corresponding to the eigenvalue is one. Therefore, it will be impossible to find two linearly independent eigenvectors for \(A\).%
\par
We showed previously that eigenvectors corresponding to distinct eigenvalue are always linearly independent, so if an \(n \times n\) matrix \(A\) has \(n\) distinct eigenvalues then \(A\) is diagonalizable. \hyperref[x:activity:act_4_c_3]{Activity~{\xreffont\ref{x:activity:act_4_c_3}}} (b) shows that it is possible to diagonalize an \(n \times n\) matrix even if the matrix does not have \(n\) distinct eigenvalues. In general, we can diagonalize a matrix as long as the dimension of each eigenspace is equal to the multiplicity of the corresponding eigenvalue. In other words, a matrix is diagonalizable if the geometric multiplicity is the same is the algebraic multiplicity for each eigenvalue.%
\par
At this point we might ask one final question. We argued that if an \(n \times n\) matrix \(A\) has \(n\) linearly independent eigenvectors, then \(A\) is diagonalizable. It is reasonable to wonder if the converse is true \textemdash{} that is, if \(A\) is diagonalizable, must \(A\) have \(n\) linearly independent eigenvectors? The answer is yes, and you are asked to show this in \hyperlink{x:exercise:ex_4_c_diagonal_converse}{Exercise~{\xreffont 6}}. We summarize the result in the following theorem.%
\begin{theorem}{The Diagonalization Theorem.}{}{g:theorem:idp17071880}%
An \(n \times n\) matrix \(A\) is diagonalizable if and only if \(A\) has \(n\) linearly independent eigenvectors. If \(A\) is diagonalizable and has linearly independent eigenvectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_n\) with \(A\vv_i = \lambda_i \vv_i\) for each \(i\), then \(n \times n\) matrix \(P [ \vv_1 \ \vv_2 \ \cdots \ \vv_n]\) whose columns are linearly independent eigenvectors of \(A\) satisfies \(P^{-1}AP = D\), where \(D = [d_{ij}]\) is the diagonal matrix with diagonal entries \(d_{ii} = \lambda_i\) for each \(i\).%
\end{theorem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_diag_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp17085576}%
Let \(A = \left[ \begin{array}{crr} 1\amp -2\amp 1 \\ 0\amp 3\amp -1 \\ 0\amp -2\amp 2 \end{array} \right]\) and \(B = \left[ \begin{array}{ccc} 1\amp 2\amp 0 \\ 0\amp 1\amp 0 \\ 0\amp 0\amp 4 \end{array} \right]\). You should use appropriate technology to calculate determinants, perform any row reductions, or solve any polynomial equations.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Determine if \(A\) is diagonalizable. If diagonalizable, find a matrix \(P\) that diagonalizes \(A\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp17090824}{}\quad{}Technology shows that the characteristic polynomial of \(A\) is%
\begin{equation*}
p(\lambda) = \det(A - \lambda I_3) = (4-\lambda)(1-\lambda)^2\text{.}
\end{equation*}
The eigenvalues of \(A\) are the solutions to the characteristic equation \(p(\lambda) = 0\). Thus, the eigenvalues of \(A\) are \(1\) and \(4\). To find a basis for the eigenspace of \(A\) corresponding to the eigenvalue \(1\), we find the general solution to the homogeneous system \((A - I_3)\vx = \vzero\). Using technology we see that the reduced row echelon form of \(A - I_3 =  \left[ \begin{array}{crr} 0\amp -2\amp 1 \\ 0\amp 2\amp -1 \\ 0\amp -2\amp 1 \end{array}  \right]\) is \(\left[  \begin{array}{ccr} 0\amp 1\amp -\frac{1}{2} \\ 0\amp 0\amp 0 \\ 0\amp 0\amp 0 \end{array}  \right]\). So if \(\vx = \left[ \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array}  \right]\), then the general solution to \((A-I_3)\vx = \vzero\) is%
\begin{align*}
\vx \amp = \left[   \begin{array}{c} x_1\\
\frac{1}{2}x_3\\
x_3 \end{array} \right]\\
\amp = x_1 \left[\begin{array}{c}  1\\
0\\
0 \end{array} \right] + x_3 \left[   \begin{array}{c} 0\\
\frac{1}{2}\\
1 \end{array} \right]\text{.}
\end{align*}
So a basis for the eigenspace of \(A\) corresponding to the eigenvalue \(1\) is%
\begin{equation*}
\left\{ \left[ 1 \ 0 \ 0 \right]^{\tr}, \left[  0 \ 1 \ 2 \right]^{\tr} \right\}\text{.}
\end{equation*}
To find a basis for the eigenspace of \(A\) corresponding to the eigenvalue \(4\), we find the general solution to the homogeneous system \((A - 4I_3)\vx = \vzero\). Using technology we see that the reduced row echelon form of \(A - 4I_3 =  \left[ \begin{array}{rrr} -3\amp -2\amp 1 \\ 0\amp -1\amp -1 \\ 0\amp -2\amp -2 \end{array}  \right]\) is \(\left[ \begin{array}{ccr} 0\amp 1\amp -1 \\ 0\amp 1\amp 1 \\ 0\amp 0\amp 0 \end{array}  \right]\). So if \(\vx = \left[ \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array}  \right]\), then the general solution to \((A-4I_3)\vx = \vzero\) is%
\begin{align*}
\vx \amp = \left[ x_1 \ x_2 \ x_3 \right]^{\tr}\\
\amp = \left[ x_3 \ -x_3 \ x_3 \right]^{\tr}\\
\amp = x_3 \left[  1\ -1 \ 1 \right]^{\tr}\text{.}
\end{align*}
So a basis for the eigenspace of \(A\) corresponding to the eigenvalue \(4\) is%
\begin{equation*}
\left\{ \left[ 1 \ -1 \ 0 \right]^{\tr}\right\}\text{.}
\end{equation*}
Eigenvectors corresponding to different eigenvalues are linearly independent, so the set%
\begin{equation*}
\left\{ \left[ 1 \ 0 \ 0 \right]^{\tr}, \left[  0 \ 1 \ 2  \right]^{\tr},  \left[ 1 \ -1 \ 0 \right]^{\tr} \right\}
\end{equation*}
is a basis for \(\R^3\). Since we can find a basis for \(\R^3\) consisting of eigenvectors of \(A\), we conclude that \(A\) is diagonalizable. Letting%
\begin{equation*}
P =  \left[ \begin{array}{ccr} 1\amp 0\amp 1 \\ 0\amp 1\amp -1 \\ 0\amp 2\amp 1 \end{array}  \right]
\end{equation*}
gives us%
\begin{equation*}
P^{-1}AP = \left[ \begin{array}{ccc} 1\amp 0\amp 0 \\ 0\amp 1\amp 0 \\ 0\amp 0\amp 4 \end{array}  \right]\text{.}
\end{equation*}
%
\item{}Determine if \(B\) is diagonalizable. If diagonalizable, find a matrix \(Q\) that diagonalizes \(B\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp17106440}{}\quad{}Technology shows that the characteristic polynomial of \(B\) is%
\begin{equation*}
p(\lambda) = \det(B - \lambda I_3) = (4-\lambda)(1-\lambda)^2\text{.}
\end{equation*}
The eigenvalues of \(B\) are the solutions to the characteristic equation \(p(\lambda) = 0\). Thus, the eigenvalues of \(B\) are \(1\) and \(4\). To find a basis for the eigenspace of \(B\) corresponding to the eigenvalue \(1\), we find the general solution to the homogeneous system \((B - I_3)\vx = \vzero\). Using technology we see that the reduced row echelon form of \(B - I_3 =  \left[ \begin{array}{ccc} 0\amp 2\amp 0 \\ 0\amp 0\amp 0 \\ 0\amp 0\amp 3 \end{array}  \right]\) is \(\left[ \begin{array}{ccc} 0\amp 1\amp 0 \\ 0\amp 0\amp 1 \\ 0\amp 0\amp 0 \end{array}  \right]\). So if \(\vx = \left[ \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array}  \right]\), then the general solution to \((B-I_3)\vx = \vzero\) is%
\begin{align*}
\vx \amp = \left[ x_1 \ x_2 \ x_3\right]^{\tr}\\
\amp = \left[  x_1 \ 0 \ 0  \right]^{\tr}\\
\amp = x_1 \left[ 1 \ 0 \ 0 \right]^{\tr}\text{.}
\end{align*}
So a basis for the eigenspace of \(B\) corresponding to the eigenvalue \(1\) is%
\begin{equation*}
\left\{ \left[ 1 \ 0 \ 0 \right]^{\tr}\right\}\text{.}
\end{equation*}
To find a basis for the eigenspace of \(B\) corresponding to the eigenvalue \(4\), we find the general solution to the homogeneous system \((B - 4I_3)\vx = \vzero\). Using technology we see that the reduced row echelon form of \(B - 4I_3 =  \left[ \begin{array}{rrc} -3\amp 2\amp 0 \\ 0\amp -3\amp 0 \\ 0\amp 0\amp 0 \end{array}  \right]\) is \(\left[ \begin{array}{ccc} 1\amp 0\amp 0 \\ 0\amp 1\amp 0 \\ 0\amp 0\amp 0 \end{array}  \right]\). So if \(\vx = \left[ \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array}  \right]\), then the general solution to \((B-4I_3)\vx = \vzero\) is%
\begin{align*}
\vx \amp = \left[ x_1 \ x_2 \ x_3  \right]^{\tr}\\
\amp = \left[  0 \ 0 \ x_3  \right]^{\tr}\\
\amp = x_3 \left[  0\ 0 \ 1  \right]^{\tr}\text{.}
\end{align*}
So a basis for the eigenspace of \(B\) corresponding to the eigenvalue \(4\) is%
\begin{equation*}
\left\{ \left[ 0 \ 0 \ 1  \right]^{\tr}\right\}\text{.}
\end{equation*}
Since each eigenspace is one-dimensional, we cannot find a basis for \(\R^3\) consisting of eigenvectors of \(B\). We conclude that \(B\) is not diagonalizable.%
\item{}Is it possible for two matrices \(R\) and \(S\) to have the same eigenvalues with the same algebraic multiplicities, but one matrix is diagonalizable and the other is not? Explain.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp17120904}{}\quad{}Yes it is possible for two matrices \(R\) and \(S\) to have the same eigenvalues with the same multiplicities, but one matrix is diagonalizable and the other is not. An example is given by the matrices \(A\) and \(B\) in this problem.%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp17125000}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Is it possible to find diagonalizable matrices \(A\) and \(B\) such that \(AB\) is not diagonalizable? If yes, provide an example. If no, explain why.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp17119496}{}\quad{}Let \(A = \left[ \begin{array}{cc} 1\amp 1\\0\amp 2 \end{array} \right]\) and \(B = \left[ \begin{array}{cr} 2\amp -2 \\ 0\amp 1 \end{array} \right]\). Since \(A\) and \(B\) are both diagonal matrices, their eigenvalues are their diagonal entries. With \(2\) distinct eigenvalues, both \(A\) and \(B\) are diagonalizable. In this case we have \(AB = \left[ \begin{array}{cr} 2\amp -1\\0\amp 2 \end{array} \right]\), whose only eigenvector is \(2\). The reduced row echelon form of \(AB - 2I_2\) is \(\left[ \begin{array}{cr} 0\amp 1\\0\amp 0 \end{array} \right]\). So a basis for the eigenspace of \(AB\) is \(\{[1 \ 0]^{\tr}\}\). Since there is no basis for \(\R^2\) consisting of eigenvectors of \(AB\), we conclude that \(AB\) is not diagonalizable.%
\item{}Is it possible to find diagonalizable matrices \(A\) and \(B\) such that \(A+B\) is not diagonalizable? If yes, provide an example. If no, explain why.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp17133704}{}\quad{}Let \(A = \left[ \begin{array}{cc} 1\amp 3\\0\amp 2 \end{array} \right]\) and \(B = \left[ \begin{array}{rr} 2\amp 0 \\ 0\amp 1 \end{array} \right]\). Since \(A\) and \(B\) are both diagonal matrices, their eigenvalues are their diagonal entries. With \(2\) distinct eigenvalues, both \(A\) and \(B\) are diagonalizable. In this case we have \(A+B = \left[ \begin{array}{cc} 3\amp 3\\0\amp 3 \end{array} \right]\), whose only eigenvector is \(3\). The reduced row echelon form of \((A+B) - 3I_2\) is \(\left[ \begin{array}{cc} 0\amp 1\\0\amp 0 \end{array} \right]\). So a basis for the eigenspace of \(A+B\) is \(\{[1 \ 0]^{\tr}\}\). Since there is no basis for \(\R^2\) consisting of eigenvectors of \(A+B\), we conclude that \(A+B\) is not diagonalizable.%
\item{}Is it possible to find a diagonalizable matrix \(A\) such that \(A^{\tr}\) is not diagonalizable? If yes, provide an example. If no, explain why.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp17141384}{}\quad{}It is not possible to find a diagonalizable matrix \(A\) such that \(A^{\tr}\) is not diagonalizable. To see why, suppose that matrix \(A\) is diagonalizable. That is, there exists a matrix \(P\) such that \(P^{-1}AP = D\), where \(D\) is a diagonal matrix. Recall that \(\left(P^{-1}\right)^{\tr} = \left(P^{\tr}\right)^{-1}\). So%
\begin{align*}
D \amp = D^{\tr}\\
\amp = \left(P^{-1}AP\right)^{\tr}\\
\amp = P^{\tr}A^{\tr}\left(P^{-1}\right)^{\tr}\\
\amp = P^{\tr}A^{\tr}\left(P^{\tr}\right)^{-1}\text{.}
\end{align*}
Letting \(A = \left(P^{\tr}\right)^{-1}\), we conclude that%
\begin{equation*}
Q^{-1}A^{\tr}Q = D\text{.}
\end{equation*}
Therefore, \(Q\) diagonalizes \(A^{\tr}\).%
\item{}Is it possible to find an invertible diagonalizable matrix \(A\) such that \(A^{-1}\) is not diagonalizable? If yes, provide an example. If no, explain why.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp17149064}{}\quad{}It is not possible to find an invertible diagonalizable matrix \(A\) such that \(A^{-1}\) is not diagonalizable. To see why, suppose that matrix \(A\) is diagonalizable. That is, there exists a matrix \(P\) such that \(P^{-1}AP = D\), where \(D\) is a diagonal matrix. Thus, \(A = PDP^{-1}\). Since \(A\) is invertible, \(\det(A) \neq 0\). It follows that \(\det(D) \neq 0\). So none of the diagonal entries of \(D\) can be \(0\). Thus, \(D\) is invertible and \(D^{-1}\) is a diagonal matrix. Then%
\begin{equation*}
D^{-1} = \left(P^{-1}AP\right)^{-1} = PA^{-1}P^{-1}
\end{equation*}
and so \(P^{-1}\) diagonalizes \(A^{-1}\).%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_diag_summ}
%
\begin{itemize}[label=\textbullet]
\item{}A matrix \(D = [d_{ij}]\) is a diagonal matrix if \(d_{ij} = 0\) whenever \(i \neq j\).%
\item{}A matrix \(A\) is diagonalizable if there is an invertible matrix \(P\) so that \(P^{-1}AP\) is a diagonal matrix.%
\item{}Two matrices \(A\) and \(B\) are similar if there is an invertible matrix \(P\) so that%
\begin{equation*}
B = P^{-1}AP\text{.}
\end{equation*}
%
\item{}Similar matrices have the same determinants, same characteristic polynomials, and same eigenvalues. Note that similar matrices do not necessarily have the same eigenvectors corresponding to the same eigenvalues.%
\item{}An \(n \times n\) matrix \(A\) is diagonalizable if and only if \(A\) has \(n\) linearly independent eigenvectors.%
\item{}When an \(n \times n\) matrix \(A\) is diagonalizable, then \(P = [ \vv_1 \ \vv_2 \ \vv_3 \ \cdots \ \vv_n]\) is invertible and \(P^{-1}AP\) is diagonal, where \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_n\) are \(n\) linearly independent eigenvectors for \(A\).%
\item{}One use for diagonalization is that once we have diagonalized a matrix \(A\) we can quickly and easily compute powers of \(A\). Diagonalization can also help us understand the actions of matrix transformations.%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_diag_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp17175432}%
Determine if each of the following matrices is diagonalizable or not. For diagonalizable matrices, clearly identify a matrix \(P\) which diagonalizes the matrix, and what the resulting diagonal matrix is.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(A=\left[ \begin{array}{cr} 2\amp -1\\ 1\amp 4 \end{array} \right]\)%
\item{}\(A=\left[ \begin{array}{rcr} -1 \amp 4 \amp -2 \\ -3 \amp 4 \amp 0 \\ -3 \amp 1 \amp 3 \end{array} \right]\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp17170696}%
The \(3\times 3\) matrix \(A\) has two eigenvalues \(\lambda_1=2\) and \(\lambda_2=3\). The vectors \(\left[ \begin{array}{c} 1\\2\\1 \end{array} \right]\), \(\left[ \begin{array}{r} 1\\-1\\2 \end{array} \right]\), and \(\left[ \begin{array}{c} 2\\4\\2 \end{array} \right]\) are eigenvectors for \(\lambda_1=2\), while the vectors \(\left[ \begin{array}{c} 1\\1\\1 \end{array} \right], \left[ \begin{array}{c} 2\\2\\2 \end{array} \right]\) are eigenvectors for \(\lambda_2=3\). Find the matrix \(A\).%
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp17178760}%
Find a \(2\times 2\) non-diagonal matrix \(A\) and two different pairs of \(P\) and \(D\) matrices for which \(A=PDP^{-1}\).%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp17188360}%
Find a \(2\times 2\) non-diagonal matrix \(A\) and two different \(P\) matrices for which \(A=PDP^{-1}\) with the same \(D\).%
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp17190408}%
Suppose a \(4\times 4\) matrix \(A\) has eigenvalues 2, 3 and 5 and the eigenspace for the eigenvalue 3 has dimension 2. Do we have enough information to determine if \(A\) is diagonalizable? Explain.%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{x:exercise:ex_4_c_diagonal_converse}%
Let \(A\) be a diagonalizable \(n \times n\) matrix. Show that \(A\) has \(n\) linearly independent eigenvectors.%
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp17194504}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(A = \left[ \begin{array}{cc} 1\amp 1\\0\amp 1 \end{array} \right]\) and \(B = \left[ \begin{array}{cc} 1\amp 2\\0\amp 1 \end{array} \right]\). Find the eigenvalues and eigenvectors of \(A\) and \(B\). Conclude that it is possible for two different \(n \times n\) matrices \(A\) and \(B\) to have exactly the same eigenvectors and corresponding eigenvalues.%
\item{}A natural question to ask is if there are any conditions under which \(n \times n\) matrices that have exactly the same eigenvectors and corresponding eigenvalues must be equal. Determine the answer to this question if \(A\) and \(B\) are both diagonalizable.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp17197704}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that if \(D\) and \(D'\) are \(n \times n\) diagonal matrices, then \(DD' = D'D\).%
\item{}Show that if \(A\) and \(B\) are \(n \times n\) matrices and \(P\) is an invertible \(n \times n\) matrix such that \(P^{-1}AP = D\) and \(P^{-1}BP = D'\) with \(D\) and \(D'\) diagonal matrices, then \(AB = BA\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{x:exercise:ex_trace_eigenvalues}%
\hyperlink{x:exercise:ex_determinant_eigenvalues}{Exercise~{\xreffont 1}} in \hyperref[x:chapter:chap_characteristic_equation]{Section~{\xreffont\ref{x:chapter:chap_characteristic_equation}}} shows that the determinant of a matrix is the product of its eigenvalues. In this exercise we show that the trace of a diagonalizable matrix is the sum of its eigenvalues.\footnotemark{} First we define the trace of a matrix.%
\begin{definition}{}{x:definition:def_trace}%
\index{trace}%
The \terminology{trace} of an \(n \times n\) matrix \(A = [a_{ij}]\) is the sum of the diagonal entries of \(A\). That is,%
\begin{equation*}
\trace(A) = a_{11} + a_{22} + \cdots + a_{nn} = \sum_{i=1}^n a_{ii}\text{.}
\end{equation*}
%
\end{definition}
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that if \(R = [r_{ij}]\) and \(S = [s_{ij}]\) are \(n \times n\) matrices, then \(\trace(RS) = \trace(SR)\).%
\item{}Let \(A\) be a diagonalizable \(n \times n\) matrix, and let \(p(\lambda) = \det(A - \lambda I_n)\) be the characteristic polynomial of \(A\). Let \(P\) be an invertible matrix such that \(P^{-1}AP = D\), where \(D\) is the diagonal matrix whose diagonal entries are \(\lambda_1\), \(\lambda_2\), \(\ldots\), \(\lambda_n\), the eigenvalues of \(A\) (note that these eigenvalues may not all be distinct).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Explain why \(\trace(A) = \trace(D)\).%
\item{}Show that the trace of an \(n \times n\) diagonalizable matrix is the sum of the eigenvalues of the matrix.%
\end{enumerate}
\end{enumerate}
\end{divisionexercise}%
\footnotetext[35]{This result is true for any matrix, but the argument is more complicated.\label{g:fn:idp17202312}}%
\begin{divisionexercise}{10}{}{}{x:exercise:ex_4_c_matrix_exponential}%
In this exercise we generalize the result of \hyperlink{x:exercise:ex_2_a_matrix_exponential}{Exercise~{\xreffont 12}} in \hyperref[x:chapter:chap_matrix_operations]{Section~{\xreffont\ref{x:chapter:chap_matrix_operations}}} to arbitrary diagonalizable matrices.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that if%
\begin{equation*}
D=\left[ \begin{array}{ccccc} \lambda_1\amp  0 \amp 0\amp  \cdots \amp 0 \\ 0 \amp  \lambda_2\amp  0 \amp \cdots \amp  0 \\ \vdots \amp \vdots \amp \vdots \amp  \ddots \amp  \vdots \\ 0 \amp  0\amp  0 \amp \cdots \amp  \lambda_n \end{array}  \right]\text{,}
\end{equation*}
then%
\begin{equation*}
e^D =  \left[ \begin{array}{ccccc} e^{\lambda_1}\amp  0 \amp 0\amp  \cdots \amp 0 \\ 0 \amp  e^{\lambda_2}\amp  0 \amp \cdots \amp  0 \\ \vdots \amp \vdots \amp \vdots \amp  \ddots \amp  \vdots \\ 0 \amp  0\amp  0 \amp \cdots \amp  e^{\lambda_n} \end{array}  \right]\text{.}
\end{equation*}
%
\item{}Now suppose that an \(n \times n\) matrix \(A\) is diagonalizable, with \(P^{-1}AP\) equal to a diagonal matrix \(D\). Show that \(e^A = Pe^DP^{-1}\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{11}{}{}{x:exercise:ex_4_c_matrix_exponential_examples}%
Let \(A = \left[ \begin{array}{cc} 1\amp 1\\0\amp 0 \end{array} \right]\) and let \(B = \left[ \begin{array}{cr} 0\amp -1 \\ 0\amp 0 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Use the result of \hyperlink{x:exercise:ex_4_c_matrix_exponential}{Exercise~{\xreffont 10}} to calculate \(e^A\).%
\item{}Calculate \(e^B\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp17239560}{}\quad{}Explain why \(B\) is not diagonalizable.%
\item{}Use the result of \hyperlink{x:exercise:ex_4_c_matrix_exponential}{Exercise~{\xreffont 10}} to calculate \(e^{A+B}\).%
\item{}The real exponential function satisfies some familiar properties. For example, \(e^xe^y = e^ye^x\) and \(e^{x+y} = e^x e^y\) for any real numbers \(x\) and \(y\). Does the matrix exponential satisfy the corresponding properties. That is, if \(X\) and \(Y\) are \(n \times n\) matrices, must \(e^Xe^Y = e^Ye^X\) and \(e^{X+Y} = e^X e^Y\)? Explain.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{12}{}{}{g:exercise:idp17249032}%
In \hyperlink{x:exercise:ex_4_c_matrix_exponential_examples}{Exercise~{\xreffont 11}} we see that we cannot conclude that \(e^{X+Y} = e^X e^Y\) for \(n \times n\) matrices \(X\) and \(Y\). However, a more limited property is true.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\textgreater{} Follow the steps indicated to show that if \(A\) is an \(n \times n\) matrix and \(s\) and \(t\) are any scalars, then \(e^{As} e^{At} = e^{A(s+t)}\). (Although we will not use it, you may assume that the series for \(e^A\) converges for any square matrix \(A\).)%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Use the definition to show that%
\begin{equation*}
e^{As}e^{At} = \sum_{k \geq 0} \sum_{m \geq 0} \frac{s^kt^m}{k!}m! A^{k+m}\text{.}
\end{equation*}
%
\item{}Relabel and reorder terms with \(n = k+m\) to show that%
\begin{equation*}
e^{As}e^{At} = sum_{n \geq 0} \frac{1}{n!} A^n \sum_{m = 0}^n \frac{n!}{(n-m)!m!} s^{n-m}t^m\text{.}
\end{equation*}
%
\item{}Complete the problem using the Binomial Theorem that says%
\begin{equation*}
(s+t)^n =  \sum_{m = 0}^n \frac{n!} {(n-m)!m!} s^{n-m}t^m\text{.}
\end{equation*}
%
\end{enumerate}
\item{}Use the result of part (a) to show that \(e^A\) is an invertible matrix for any \(n \times n\) matrix \(A\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{13}{}{}{g:exercise:idp17255304}%
There is an interesting connection between the determinant of a matrix exponential and the trace of the matrix. Let \(A\) be a diagonalizable \(n \times n\) matrix with real entries. Let \(D = P^{-1}AP\) for some invertible matrix \(P\), where \(D\) is the diagonal matrix with entries \(\lambda_1\), \(\lambda_2\), \(\ldots\), \(\lambda_n\) the eigenvalues of \(A\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(e^A = Pe^DP^{-1}\).%
\item{}Use \hyperlink{x:exercise:ex_trace_eigenvalues}{Exercise~{\xreffont 9}} to show that%
\begin{equation*}
\det\left(e^A\right) = e^{\trace(A)}\text{.}
\end{equation*}
%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{14}{}{}{x:exercise:ex_Cayley-Hamilton}%
There is interesting relationship between a matrix and its characteristic equation that we explore in this exercise.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}We first illustrate with an example. Let \(B = \left[ \begin{array}{cr} 1\amp 2\\ 1\amp -2 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Show that \(\lambda^2 + \lambda - 4\) is the characteristic polynomial for \(B\).%
\item{}Calculate \(B^2\). Then compute \(B^2 + B - 4I_2\). What do you get?%
\end{enumerate}
\item{}The first part of this exercise presents an example of a matrix that satisfies its own characteristic equation. Show that if \(A\) is an \(n \times n\) \emph{diagonalizable} matrix with characteristic polynomial \(p(x)\), then \(p(A) = 0\).\footnotemark{} That is, if \(p(x) = a_nx^n+a_{n-1}x^{n-1} + \cdots + a_1x + a_0\), then \(p(A) = a_nA^n+a_{n-1}A^{n-1} + \cdots + a_1A + a_0 = 0\). (Hint: If \(A = PDP^{-1}\) for some diagonal matrix \(D\), show that \(p(A) = Pp(D)P^{-1}\). Then determine \(p(D)\).)%
\end{enumerate}
\end{divisionexercise}%
\footnotetext[36]{This result is known as the Cayley-Hamilton Theorem and is one of the fascinating results in linear algebra. This result is true for any square matrix.\label{g:fn:idp17010184}}%
\begin{divisionexercise}{15}{}{}{g:exercise:idp17011464}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
If matrix \(A\) is diagonalizable, then so is \(A^T\).%
\item{}\lititle{True\slash{}False.}\par%
If matrix \(A\) is diagonalizable, then \(A\) is invertible.%
\item{}\lititle{True\slash{}False.}\par%
If an \(n\times n\) matrix \(A\) is diagonalizable, then \(A\) has \(n\) distinct eigenvalues.%
\item{}\lititle{True\slash{}False.}\par%
If matrix \(A\) is invertible and diagonalizable, then so is \(A^{-1}\).%
\item{}\lititle{True\slash{}False.}\par%
If an \(n \times n\) matrix \(C\) is diagonalizable, then there exists a basis of \(\R^n\) consisting of the eigenvectors of \(C\).%
\item{}\lititle{True\slash{}False.}\par%
An \(n\times n\) matrix with \(n\) distinct eigenvalues is diagonalizable.%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is an \(n\times n\) diagonalizable matrix, then there is a unique diagonal matrix such that \(P^{-1}AP = D\) for some invertible matrix \(P\).%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is an \(n\times n\) matrix with eigenvalue \(\lambda\), then the dimension of the eigenspace of \(A\) corresponding to the eigenvalue \(\lambda\) is \(n - \rank(A - \lambda I_n)\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\lambda\) is an eigenvalue of an \(n \times n\) matrix \(A\), then \(e^\lambda\) is an eigenvalue of \(e^A\). (See \hyperlink{x:exercise:ex_2_a_matrix_exponential}{Exercise~{\xreffont 12}} in \hyperref[x:chapter:chap_matrix_operations]{Section~{\xreffont\ref{x:chapter:chap_matrix_operations}}} for information on the matrix exponential.)%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Binet's Formula for the Fibonacci Numbers}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Binet's Formula for the Fibonacci Numbers}{}{Project: Binet's Formula for the Fibonacci Numbers}{}{}{x:section:sec_proj_binet_fibo}
We return to the Fibonacci sequence \(F_n\) where \(F_{n+2} = F_{n+1} + F_{n}\), for \(n \geq 0\), \(F_0 = 0\), and \(F_1=1\). Since \(F_{n+2}\) is determined by previous values \(F_{n+1}\) and \(F_n\), the relation \(F_{n+2} = F_{n+1} + F_{n}\) is called a \terminology{recurrence relation}. The recurrence relation \(F_{n+2} = F_{n+1} + F_{n}\) is very time consuming to use to compute \(F_n\) for large values of \(n\). It turns out that there is a fascinating formula that gives the \(n\)th term of the Fibonacci sequence directly, without using the relation \(F_{n+2} = F_{n+1} + F_{n}\).%
\begin{project}{}{g:project:idp17300632}%
The recurrence relation\(F_{n+2} = F_{n+1} + F_{n}\) gives the equations%
\begin{align}
F_{n+1} \amp = F_{n} + F_{n-1}\label{x:mrow:eq_Fib_1}\\
F_{n} \amp = F_{n}\text{.}\label{x:mrow:eq_Fib_2}
\end{align}
%
\par
Let \(\vx_{n} = \left[ \begin{array}{c} F_{n+1} \\ F_{n} \end{array}  \right]\) for \(n \geq 0\). Explain how the equations \hyperref[x:mrow:eq_Fib_1]{({\xreffont\ref{x:mrow:eq_Fib_1}})} and \hyperref[x:mrow:eq_Fib_1]{({\xreffont\ref{x:mrow:eq_Fib_1}})} can be described with the matrix equation%
\begin{equation}
\vx_n = A \vx_{n-1}\text{,}\label{x:men:eq_Fib_matrix}
\end{equation}
where \(A = \left[ \begin{array}{cc} 1\amp 1 \\ 1\amp 0 \end{array}  \right]\).%
\end{project}%
The matrix equation \hyperref[x:men:eq_Fib_matrix]{({\xreffont\ref{x:men:eq_Fib_matrix}})} shows us how to find the vectors \(\vx_n\) using powers of the matrix \(A\):%
\begin{align*}
\vx_1 \amp = A\vx_0\\
\vx_2 \amp = A\vx_1 = A(A\vx_0) = A^2\vx_0\\
\vx_3 \amp = A\vx_2 = A(A^2\vx_0) = A^3\vx_0\\
\ \vdots \amp  \qquad \vdots\\
\vx_n \amp = A^n\vx_0\text{.}
\end{align*}
%
\par
So if we can somehow easily find the powers of the matrix \(A\), then we can find a convenient formula for \(F_n\). As we have seen, we know how to do this if \(A\) is diagonalizable%
\begin{project}{}{g:project:idp17308824}%
Let \(A = \left[ \begin{array}{cc} 1\amp 1 \\ 1\amp 0 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that the eigenvalues of \(A\) are \(\varphi = \frac{1 + \sqrt{5}}{2}\) and \(\overline{\varphi} = \frac{1 - \sqrt{5}}{2}\).%
\item{}Find bases for each eigenspace of \(A\).%
\end{enumerate}
\end{project}%
Now that we have the eigenvalues and know corresponding eigenvectors for \(A\), we can return to the problem of diagonalizing \(A\).%
\begin{project}{}{g:project:idp17320088}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Why do we know that \(A\) is diagonalizable?%
\item{}Find a matrix \(P\) such that \(P^{-1}AP\) is a diagonal matrix. What is the diagonal matrix?%
\end{enumerate}
\end{project}%
Now we can find a formula for the \(n\)th Fibonacci number.%
\begin{project}{}{g:project:idp17321880}%
Since \(P^{-1}AP = D\), where \(D\) is a diagonal matrix, we also have \(A = PDP^{-1}\). Recall that when \(A = PDP^{-1}\), it follows that \(A^n = PD^nP^{-1}\). Use the equation \(A^n = PD^nP^{-1}\) to show that%
\begin{equation}
F_n = \frac{\varphi^n - \overline{\varphi}^n}{\sqrt{5}}\text{.}\label{x:men:eq_Binet}
\end{equation}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp17317400}{}\quad{}We just need to calculate the second component of \(A^n \vx_0\).%
\end{project}%
\index{Binet's formula} Formula \hyperref[x:men:eq_Binet]{({\xreffont\ref{x:men:eq_Binet}})} is called \emph{Binet's formula}. It is a very surprising formula in the fact that the expression on the right hand side of \hyperref[x:men:eq_Binet]{({\xreffont\ref{x:men:eq_Binet}})} is an integer for each positive integer \(n\). Note that with Binet's formula we can quickly compute \(F_n\) for very large values of \(n\). For example,%
\begin{equation*}
F_{150} = 9969216677189303386214405760200\text{.}
\end{equation*}
%
\par
The number \(\varphi = \frac{1+\sqrt{5}}{2}\), called the \terminology{golden mean} or \terminology{golden ratio} is intimately related to the Fibonacci sequence. Binet's formula provides a fascinating relationship between the Fibonacci numbers and the golden ratio. The golden ratio also occurs often in other areas of mathematics. It was an important number to the ancient Greek mathematicians who felt that the most aesthetically pleasing rectangles had sides in the ratio of \(\varphi:1\).%
\begin{project}{}{g:project:idp17330712}%
You might wonder what happens if we use negative integer exponents in Binet's formula. In other words, are there negatively indexed Fibonacci numbers? For any integer \(n\), including negative integers, let%
\begin{equation*}
F_n = \frac{\varphi^n - \overline{\varphi}^n}{\sqrt{5}}
\end{equation*}
%
\par
There is a specific relationship between \(F_{-n}\) and \(F_n\). Find it and verify it.%
\end{project}%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 20 Approximating Eigenvalues and Eigenvectors}
\typeout{************************************************}
%
\begin{chapterptx}{Approximating Eigenvalues and Eigenvectors}{}{Approximating Eigenvalues and Eigenvectors}{}{}{x:chapter:chap_approx_eigenvalues}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp17334424}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What is the power method for?%
\item{}How does the power method work?%
\item{}How can we use the inverse power method to approximate any eigenvalue\slash{}eigenvector pair?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Leslie Matrices and Population Modeling}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Leslie Matrices and Population Modeling}{}{Application: Leslie Matrices and Population Modeling}{}{}{x:section:sec_appl_leslie_mtx}
The Leslie Matrix (also called the Leslie Model) is a powerful model for describing an age distributed growth of a population that is closed to migration. In a Leslie model, it is usually the case that only one gender (most often female) is considered. As an example, we will later consider a population of sheep that is being grown commercially. A natural question that we will address is how we can harvest the population to build a sustainable environment.%
\par
When working with populations, the matrices we use are often large. For large matrices, using the characteristic polynomial to calculate eigenvalues is too time and resource consuming to be practical, and we generally cannot find the exact values of the eigenvalues. As a result, approximation techniques are very important. In this section we will explore a method for approximating eigenvalues. The eigenvalues of a Leslie matrix are important because they describe the limiting or steady-state behavior of a population. The matrix and model were introduced by Patrick H. Leslie in ``On the Use of Matrices in Certain Population Mathematics'', Leslie, P.H., \pubtitle{Biometrika}, Volume XXXIII, November 1945, pp. 183-212.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_app_eigen_intro}
We have used the characteristic polynomial to find the eigenvalues of a matrix, and for each eigenvalue row reduced a corresponding matrix to find the eigenvectors This method is only practical for small matrices \textemdash{} for more realistic applications approximation techniques are used. We investigate one such technique in this section, the \terminology{power method}.%
\begin{exploration}{}{x:exploration:pa_4_d}%
Let \(A = \left[ \begin{array}{cc} 2\amp 6 \\ 5\amp 3 \end{array} \right]\). Our goal is to find a scalar \(\lambda\) and a nonzero vector \(\vv\) so that \(A \vv = \lambda \vv\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}If we have no prior knowledge of the eigenvalues and eigenvectors of this matrix, we might just begin with a guess. Let \(\vx_0 = [1 \ 0]^{\tr}\) be such a guess for an eigenvector. Calculate \(A \vx_0\). Is \(\vx_0\) an eigenvector of \(A\)? Explain.%
\item{}If \(\vx_0\) is not a good approximation to an eigenvector of \(A\), then we need to make a better guess. We have little to work with other than just random guessing, but we can use \(\vx_1 = A\vx_0\) as another guess. We calculated \(\vx_1\) in part 1. Is \(\vx_1\) an eigenvector for \(A\)? Explain.%
\item{}In parts (a) and (b) you might have noticed that in some sense \(\vx_1\) is closer to being an eigenvector of \(A\) than \(\vx_0\) was. So maybe continuing this process will get us closer to an eigenvector of \(A\). In other words, for each positive integer \(k\) we define \(\vx_k\) as \(A \vx_{k-1}\). Before we proceed, however, we should note that as we calculate the vectors \(\vx_1\), \(\vx_2\), \(\vx_3\), \(\ldots\), the entries in the vectors get large very quickly. So it will be useful to scale the entries so that they stay at a reasonable size, which makes it easier to interpret the output. One way to do this is to divide each vector \(\vx_i\) by its largest component in absolute value so that all of the entries stay between \(-1\) and \(1\).\footnotemark{} So in our example we have \(\vx_0 = [1 \ 0]^{\tr}\), \(\vx_1 = [2/5 \ 1]^{\tr}\), and \(\vx_2 = [1 \ 25/34]^{\tr}\). Explain why scaling our vectors will not affect our search for an eigenvector.%
\item{}Use an appropriate technological tool to find the vectors \(\vx_k\) up to \(k=10\). What do you think the limiting vector \(\lim_{k \to \infty} \vx_k\) is? Is this limiting vector an eigenvector of \(A\)? If so, what is the corresponding eigenvalue?%
\end{enumerate}
\end{exploration}%
\footnotetext[37]{There are several other ways to scale, but we won't consider them here.\label{g:fn:idp17351448}}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Power Method}
\typeout{************************************************}
%
\begin{sectionptx}{The Power Method}{}{The Power Method}{}{}{x:section:sec_power_method}
While the examples we present in this text are small in order to highlight the concepts, matrices that appear in real life applications are often enormous. For example, in Google's PageRank algorithm that is used to determine relative rankings of the importance of web pages, matrices of staggering size are used (most entries in the matrices are zero, but the size of the matrices is still huge). Finding eigenvalues of such large matrices through the characteristic polynomial is impractical. In fact, finding the roots of all but the smallest degree characteristic polynomials is a very difficult problem. As a result, using the characteristic polynomial to find eigenvalues and then finding eigenvectors is not very practical in general, and it is often a better option to use a numeric approximation method. We will consider one such method in this section, the \terminology{power method}.%
\par
In \hyperref[x:exploration:pa_4_d]{Preview Activity~{\xreffont\ref{x:exploration:pa_4_d}}}, we saw an example of a matrix \(A = \left[ \begin{array}{cc} 2\amp 6 \\ 5\amp 3 \end{array} \right]\) so that the sequence \(\{\vx_k\}\), where \(\vx_k = A \vx_{k-1}\), converged to a dominant eigenvector of \(A\) for an initial guess vector \(\vx_0 = [1 \ 0]^{\tr}\). The vectors \(\vx_i\) for \(i\) from \(1\) to \(6\) (with scaling) are approximately%
\begin{align*}
\vx_1 \amp = \left[ \begin{array}{c} 0.4 \\ 1
\end{array}  \right] \amp \vx_2 \amp = \left[ \begin{array}{c} 1 \\ 0.7353
\end{array}  \right] \amp \vx_3 \amp = \left[ \begin{array}{c} 0.8898 \\ 1
\end{array}  \right]\\
\vx_4 \amp = \left[ \begin{array}{c} 1 \\ 0.9575
\end{array}  \right] \amp \vx_5 \amp = \left[ \begin{array}{c} 0.9838 \\ 1
\end{array}  \right] \amp \vx_6 \amp = \left[ \begin{array}{c} 1 \\ 0.9939
\end{array}  \right]\text{.}
\end{align*}
%
\par
Numerically we can see that the sequence \(\{\vx_k\}\) approaches the vector \([1 \ 1]^{\tr}\), and \hyperref[x:figure:F_4_e_1]{Figure~{\xreffont\ref{x:figure:F_4_e_1}}} illustrates this geometrically as well.%
\begin{figureptx}{The power method.}{x:figure:F_4_e_1}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/4_e_Power_Method_1.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\index{power method} This method of successive approximations \(\vx_k = A \vx_{k-1}\) is called the \terminology{power method} (since we could write \(\vx_k\) as \(A^k \vx_0\)). Our task now is to show that this method works in general. In the next activity we restrict our argument to the \(2 \times 2\) case, and then discuss the general case afterwards.%
\par
\index{dominant eigenvalue}\index{dominant eigenvector} Let \(A\) be an arbitrary \(2 \times 2\) matrix with two linearly independent eigenvectors \(\vv_1\) and \(\vv_2\) and corresponding eigenvalues \(\lambda_1\) and \(\lambda_2\), respectively. We will also assume \(|\lambda_1| > |\lambda_2|\). An eigenvalue whose absolute value is larger than that of any other eigenvalue is called a \terminology{dominant eigenvalue}. Any eigenvector for a dominant eigenvalue is called a \terminology{dominant eigenvector}. Before we show that our method can be used to approximate a dominant eigenvector, we recall that since \(\vv_1\) and \(\vv_2\) are eigenvectors corresponding to distinct eigenvalues, then \(\vv_1\) and \(\vv_2\) are linearly independent. So there exist scalars \(a_1\) and \(a_2\) such that%
\begin{equation*}
\vx_0 = a_1 \vv_1 + a_2 \vv_2\text{.}
\end{equation*}
%
\par
We have seen that for each positive integer \(k\) we can write \(\vx_n\) as%
\begin{equation}
\vx_k = a_1 \lambda_1^k \vv_1 + a_2 \lambda_2^k \vv_2\text{.}\label{x:men:eq_4_e_1}
\end{equation}
%
\par
With this representation of \(\vx_0\) we can now see why the power method approximates a dominant eigenvector of \(A\).%
\begin{activity}{}{x:activity:act_4_e_1}%
Assume as above that \(A\) is an arbitrary \(2 \times 2\) matrix with two linearly independent eigenvectors \(\vv_1\) and \(\vv_2\) and corresponding eigenvalues \(\lambda_1\) and \(\lambda_2\), respectively. (We are assuming that we don't know these eigenvectors, but we can assume that they exist.) Assume that \(\lambda_1\) is the dominant eigenvalue for \(A\), \(\vx_0\) is some initial guess to an eigenvector for \(A\), that \(\vx_0 = a_1 \vv_1 + a_2 \vv_2\), and that \(\vx_k = A\vx_{k-1}\) for \(k \geq 1\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}We divide both sides of equation \hyperref[x:men:eq_4_e_1]{({\xreffont\ref{x:men:eq_4_e_1}})} by \(\lambda_1^k\) (since \(\lambda_1\) is the dominant eigenvalue, we know that \(\lambda_1\) is not \(0\)) to obtain%
\begin{equation}
\frac{1}{\lambda_1^k}\vx_k = a_1 \vv_1 + a_2 \left(\frac{\lambda_2}{\lambda_1}\right)^k \vv_2\text{.}\label{x:men:eq_4_e_2}
\end{equation}
Recall that \(\lambda_1\) is the dominant eigenvalue for \(A\). What happens to \(\left( \frac{\lambda_2}{\lambda_1} \right)^k\) as \(k \to \infty\)? Explain what happens to the right hand side of equation \hyperref[x:men:eq_4_e_2]{({\xreffont\ref{x:men:eq_4_e_2}})} as \(k \to \infty\).%
\item{}Explain why the previous result tells us that the vectors \(\vx_k\) are approaching a vector in the \emph{direction} of \(\vv_1\) or \(-\vv_1\) as \(k \to \infty\), assuming \(a_1 \neq 0\). (Why do we need \(a_1 \neq 0\)? What happens if \(a_1=0\)?)%
\item{}What does all of this tell us about the sequence \(\{\vx_k\}\) as \(k \to \infty\)?%
\end{enumerate}
\end{activity}%
The power method is straightforward to implement, but it is not without its drawbacks. We began by assuming that we had a basis of eigenvectors of a matrix \(A\). So we are also assuming that \(A\) is diagonalizable. We also assumed that \(A\) had a dominant eigenvalue \(\lambda_1\). That is, if \(A\) is \(n \times n\) we assume that \(A\) has eigenvalues \(\lambda_1\), \(\lambda_2\), \(\ldots\), \(\lambda_n\), not necessarily distinct, with%
\begin{equation*}
|\lambda_1| > |\lambda_2| \geq |\lambda_3| \geq \cdots \geq |\lambda_n|
\end{equation*}
and with \(\vv_i\) an eigenvector of \(A\) with eigenvalue \(\lambda_i\). We could then write any initial guess \(\vx_0\) in the form%
\begin{equation*}
\vx_0 = a_1 \vv_1 +  a_2\vv_2 + \cdots + a_n \vv_n\text{.}
\end{equation*}
The initial guess is also called a \terminology{seed}.%
\par
Then%
\begin{equation*}
\vx_k = a_1 \lambda_1^k \vv_1 + a_2 \lambda_2^k \vv_2 + \cdots + a_n \lambda_n^k \vv_n
\end{equation*}
and%
\begin{equation}
\frac{1}{\lambda_1^k} \vx_k = a_1 \vv_1 + a_2 \left(\frac{\lambda_2}{\lambda_1}\right)^k \vv_2 + \cdots + a_n \left(\frac{\lambda_n}{\lambda_1}\right)^k \vv_n\text{.}\label{x:men:eq_4_e_3}
\end{equation}
%
\par
Notice that we are not actually calculating the vectors \(\vx_k\) here \textemdash{} this is a theoretical argument and we don't know \(\lambda_1\) and are not performing any scaling like we did in \hyperref[x:exploration:pa_4_d]{Preview Activity~{\xreffont\ref{x:exploration:pa_4_d}}}. We are assuming that \(\lambda_1\) is the dominant eigenvalue of \(A\), though, so for each \(i\) the terms \(\left(\frac{\lambda_i}{\lambda_1}\right)^k\) converge to \(0\) as \(k\) goes to infinity. Thus,%
\begin{equation*}
\vx_k \approx \lambda_1^k a_1 \vv_1
\end{equation*}
for large values of \(k\), which makes the sequence \(\{\vx_k\}\) converge to a vector in the direction of a dominant eigenvector \(\vv_1\) provided \(a_1 \neq 0\). So we need to be careful enough to choose a seed that has a nonzero component in the direction of \(\vv_1\). Of course, we generally don't know that our matrix is diagonalizable before we make these calculations, but for many matrices the sequence \(\{\vx_k\}\) will approach a dominant eigenvector.%
\par
The power method approximates a dominant eigenvector, and there are ways that we can approximate the dominant eigenvalue. \hyperlink{x:exercise:ex_4_d_dominant_eigenvalue}{Exercise~{\xreffont 8}} presents one way \textemdash{} by keeping track of the components of the \(\vx_k\) that have the largest absolute values, and the next activity shows another.%
\begin{activity}{}{x:activity:act_4_d_Rayleigh}%
Let \(A\) be an \(n \times n\) matrix with eigenvalue \(\lambda\) and corresponding eigenvector \(\vv\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why \(\lambda = \frac{\lambda (\vv \cdot \vv)}{\vv \cdot \vv}\).%
\item{}Use the result of part (a) to explain why \(\lambda = \frac{(A\vv) \cdot \vv}{\vv \cdot \vv}\).%
\end{enumerate}
\end{activity}%
\index{Rayleigh quotients} The result of \hyperref[x:activity:act_4_d_Rayleigh]{Activity~{\xreffont\ref{x:activity:act_4_d_Rayleigh}}} is that, when the vectors in the sequence \(\{\vx_k\}\) approximate a dominant eigenvector of a matrix \(A\), the quotients%
\begin{equation}
\frac{(A\vx_k) \cdot \vx_k}{\vx_k \cdot \vx_k} = \frac{\vx_k^{\tr}A\vx_k}{\vx_k^{\tr}\vx_k}\label{x:men:eq_Rayleigh}
\end{equation}
approximate the dominant eigenvalue of \(A\). The quotients in \hyperref[x:men:eq_Rayleigh]{({\xreffont\ref{x:men:eq_Rayleigh}})} are called \terminology{Rayleigh quotients}.%
\par
To summarize, the procedure for applying the power method for approximating a dominant eigenvector and dominant eigenvalue of a matrix \(A\) is as follows.%
\begin{descriptionlist}
\begin{dlimedium}{Step 1}{g:li:idp17421336}%
Select an arbitrary nonzero vector \(\vx_0\) as an initial guess to a dominant eigenvector.%
\end{dlimedium}%
\begin{dlimedium}{Step 2}{g:li:idp17427224}%
Let \(\vx_1 = A\vx_0\). Let \(k = 1\).%
\end{dlimedium}%
\begin{dlimedium}{Step 3}{g:li:idp17426584}%
To avoid having the magnitudes of successive approximations become excessively large, scale this approximation \(\vx_k\). That is, find the entry \(\alpha_k\) of \(\vx_k\) that is largest in absolute value. Then replace \(\vx_k\) by \(\frac{1}{\alpha_k} \vx_k\).%
\end{dlimedium}%
\begin{dlimedium}{Step 4}{g:li:idp17423384}%
Calculate the Rayleigh quotient \(r_k = \frac{(A\vx_{k}) \cdot \vx_k}{\vx_k \cdot \vx_k}\).%
\end{dlimedium}%
\begin{dlimedium}{Step 5}{g:li:idp17424024}%
Let let \(\vx_{k+1} = A \vx_k\). Increase \(k\) by \(1\) and repeat Steps 3 through 5.%
\end{dlimedium}%
\end{descriptionlist}
%
\par
If the sequence \(\{\vx_k\}\) converges to a dominant eigenvector of \(A\), then the sequence \(\{r_k\}\) converges to the dominant eigenvalue of \(A\).%
\par
\index{matrix!sparse} The power method can be useful for approximating a dominant eigenvector as long as the successive multiplications by \(A\) are fairly simple \textemdash{} for example, if many entries of \(A\) are zero.\footnote{A matrix in which most entries are zero is called a \terminology{sparse} matrix.\label{g:fn:idp17434008}} The rate of convergence of the sequence \(\{\vx_k\}\) depends on the ratio \(\frac{\lambda_2}{\lambda_1}\). If this ratio is close to \(1\), then it can take many iterations before the power \(\left(\frac{\lambda_2}{\lambda_1}\right)^k\) makes the \(\vv_2\) term negligible. There are other methods for approximating eigenvalues and eigenvectors, e.g., the QR factorization, that we will not discuss at this point.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Inverse Power Method}
\typeout{************************************************}
%
\begin{sectionptx}{The Inverse Power Method}{}{The Inverse Power Method}{}{}{x:section:sec_power_method_inv}
The power method only allows us to approximate the dominant eigenvalue and a dominant eigenvector for a matrix \(A\). It is possible to modify this method to approximate other eigenvectors and eigenvalues under certain conditions. We consider an example in the next activity to motivate the general situation.%
\begin{activity}{}{x:activity:act_4_e_2}%
Let \(A = \left[ \begin{array}{cc} 2\amp 6 \\ 5\amp 3 \end{array} \right]\) be the matrix from \hyperref[x:exploration:pa_4_d]{Preview Activity~{\xreffont\ref{x:exploration:pa_4_d}}}. Recall that \(8\) is an eigenvalue for \(A\), and a quick calculation can show that \(-3\) is the other eigenvalue of \(A\). Consider the matrix \(B = (A - (-2)I_2)^{-1} = \frac{1}{10}\left[ \begin{array}{rr} -5\amp 6 \\ 5\amp -4 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(\frac{1}{8-(-2)}\) and \(\frac{1}{-3-(-2)}\) are the eigenvalues of \(B\).%
\item{}Recall that \(\vv_1 = [1 \ 1]^{\tr}\) is an eigenvector of \(A\) corresponding to the eigenvalue \(8\) and assume that \(\vv_2 = [-6 \ 5]^{\tr}\) is an eigenvector for \(A\) corresponding to the eigenvalue \(-3\). Calculate the products \(B \vv_1\) and \(B \vv_2\). How do the products relate to the results of part (a)?%
\end{enumerate}
\end{activity}%
\hyperref[x:activity:act_4_e_2]{Activity~{\xreffont\ref{x:activity:act_4_e_2}}} provides evidence that we can translate the matrix \(A\) having a dominant eigenvalue to a different matrix \(B\) with the same eigenvectors as \(A\) and with a dominant eigenvalue of our choosing. To see why, let \(A\) be an \(n \times n\) matrix with eigenvalues \(\lambda_1\), \(\lambda_2\), \(\ldots\), \(\lambda_n\), and let \(\alpha\) be any real number distinct from the eigenvalues. Let \(B = (A - \alpha I_n)^{-1}\). In our example in \hyperref[x:activity:act_4_e_2]{Activity~{\xreffont\ref{x:activity:act_4_e_2}}} the numbers%
\begin{equation*}
\frac{1}{\lambda_1 - \alpha}, \  \frac{1}{\lambda_2 - \alpha}, \ \frac{1}{\lambda_3 - \alpha}, \ \ldots, \frac{1}{\lambda_n - \alpha}
\end{equation*}
were the eigenvalues of \(B\), and that if \(\vv_i\) is an eigenvector for \(A\) corresponding to the eigenvalue \(\lambda_i\), then \(\vv_i\) is an eigenvector of \(B\) corresponding to the eigenvalue \(\frac{1}{\lambda_i - \alpha}\). To see why, let \(\lambda\) be an eigenvalue of an \(n \times n\) matrix \(A\) with corresponding eigenvector \(\vv\). Let \(\alpha\) be a scalar that is not an eigenvalue of \(A\), and let \(B = (A - \alpha I_n)^{-1}\). Now%
\begin{align*}
A \vv \amp = \lambda \vv\\
A \vv - \alpha \vv \amp = \lambda \vv - \alpha \vv\\
(A-\alpha I_n) \vv \amp = (\lambda - \alpha) \vv\\
\frac{1}{\lambda - \alpha} \vv \amp = (A-\alpha I_n)^{-1} \vv\text{.}
\end{align*}
%
\par
So \(\frac{1}{\lambda - \alpha}\) is an eigenvalue of \(B\) with eigenvector \(\vv\).%
\par
Now suppose that \(A\) is an \(n \times n\) matrix with eigenvalues \(\lambda_1\), \(\lambda_2\), \(\ldots\), \(\lambda_n\), and that we want to approximate an eigenvector and corresponding eigenvalue \(\lambda_i\) of \(A\). If we can somehow find a value of \(\alpha\) so that \(|\lambda_i - \alpha| \lt |\lambda_j - \alpha|\) for all \(j \neq i\), then \(\left| \frac{1}{\lambda_i - \alpha} \right| > \left| \frac{1}{\lambda_j - \alpha} \right|\) for any \(j \neq i\). Thus, the matrix \(B = (A - \alpha I_n)^{-1}\) has \(\frac{1}{\lambda_i - \alpha}\) as its dominant eigenvalue and we can use the power method to approximate an eigenvector and the Rayleigh quotient to approximate the eigenvalue \(\frac{1}{\lambda_i - \alpha}\), and hence approximate \(\lambda_i\).%
\begin{activity}{}{x:activity:act_4_e_3}%
Let \(A = \frac{1}{8}\left[ \begin{array}{crr} 7\amp 3\amp 3 \\ 30\amp 22\amp -10 \\ 15\amp -21\amp 11 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Apply the power method to the matrix \(B = (A - I_3)^{-1}\) with initial vector \(\vx_0 = [1 \ 0 \ 0]^{\tr}\) to fill in \hyperref[x:table:T_4_e_2]{Table~{\xreffont\ref{x:table:T_4_e_2}}} (to four decimal places). Use this information to estimate an eigenvalue for \(A\) and a corresponding eigenvector. \begin{tableptx}{\textbf{Applying the power method to \((A - I_3)^{-1}\)}}{x:table:T_4_e_2}{}%
\centering%
{\tabularfont%
\begin{tabular}{cccc}
\(k\)&\(10\)&\(15\)&\(20\)\tabularnewline\hrulethin
\(\vx_k\)&&&\tabularnewline\hrulethin
\(\frac{\vx_k^{\tr}A\vx_k}{\vx_k^{\tr}\vx_k}\)&&&\tabularnewline\hrulethin
\end{tabular}
}%
\end{tableptx}%
%
\item{}Applying the power method to the matrix \(B = (A - 0I_3)^{-1}\) with initial vector \(\vx_0 = [1 \ 0 \ 0]^{\tr}\) yields the information in \hyperref[x:table:T_4_e_3]{Table~{\xreffont\ref{x:table:T_4_e_3}}} (to four decimal places). Use this information to estimate an eigenvalue for \(A\) and a corresponding eigenvector. \begin{tableptx}{\textbf{Applying the power method to \((A - 0I_3)^{-1}\)}}{x:table:T_4_e_3}{}%
\centering%
{\tabularfont%
\begin{tabular}{cccc}\hrulethin
\(k\)&\(10\)&\(15\)&\(20\)\tabularnewline\hrulethin
\(\vx_k\)&\(\left[ \begin{array}{r} -0.3344 \\ 0.6677 \\ 1.0000 \end{array} \right]\)&\(\left[\begin{array}{r} -0.3333 \\ 0.6666 \\ 1.0000 \end{array} \right]\)&\(\left[ \begin{array}{r} -0.3333 \\ 0.6666 \\ 1.0000 \end{array} \right]\)\tabularnewline\hrulethin
\(\frac{\vx_k^{\tr}A\vx_k}{\vx_k^{\tr}\vx_k}\)&\(-1.0014\)&\(-1.0000\)&\(-1.0000\)\tabularnewline\hrulethin
\end{tabular}
}%
\end{tableptx}%
%
\item{}Applying the power method to the matrix \(B = (A - 5I_3)^{-1}\) with initial vector \(\vx_0 = [1 \ 0 \ 0]^{\tr}\) yields the information in \hyperref[x:table:T_4_e_4]{Table~{\xreffont\ref{x:table:T_4_e_4}}} (to four decimal places). Use this information to estimate an eigenvalue for \(A\) and a corresponding eigenvector. \begin{tableptx}{\textbf{Applying the power method to \((A - 5I_3)^{-1}\)}}{x:table:T_4_e_4}{}%
\centering%
{\tabularfont%
\begin{tabular}{cccc}\hrulethin
\(k\)&\(10\)&\(15\)&\(20\)\tabularnewline\hrulethin
\(\vx_k\)&\(\left[ \begin{array}{r} 0.0000 \\ 1.0000 \\ -1.0000 \end{array} \right]\)&\(\left[ \begin{array}{r} 0.0000 \\ 1.0000 \\ -1.0000 \end{array} \right]\)&\(\left[ \begin{array}{r} 0.0000 \\ 1.0000 \\ -1.0000 \end{array} \right]\)\tabularnewline\hrulethin
\(\frac{\vx_k^{\tr}A\vx_k}{\vx_k^{\tr}\vx_k}\)&\(-1.0000\)&\(-1.0000\)&\(-1.0000\)\tabularnewline\hrulethin
\end{tabular}
}%
\end{tableptx}%
%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_app_eigen_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp17499032}%
Let \(A = \left[ \begin{array}{ccc} 1\amp 2\amp 3\\4\amp 5\amp 6\\7\amp 8\amp 9 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Approximate the dominant eigenvalue of \(A\) accurate to two decimal places using the power method. Use technology as appropriate.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp17495704}{}\quad{}We use technology to calculate the scaled vectors \(A^k \vx_0\) for values of \(k\) until the components don't change in the second decimal place. We start with the seed \(\vx_0 = [1 \ 1 \ 1]^{\tr}\). For example, to two decimal places we have \(\vx_k = [0.28 \ 0.64 \ 1.00]^{\tr}\) for \(k  \geq 20\). So we suspect that \(\left[ 0.28 \ 0.64 \ 1.00 \right]^{\tr}\) is close to a dominant eigenvector for \(A\). For the dominant eigenvalue, we can calculate the Rayleigh quotients \(\frac{(A\vx_k) \cdot \vx_k}{\vx_k \cdot \vx_k}\) until they do not change to two decimal places. For \(k \geq 4\), our Rayleigh quotients are all (to two decimal places) equal to \(16.12\). So we expect that the dominant eigenvalue of \(A\) is close to \(16.12\). Notice that%
\begin{equation*}
A [0.28 \ 0.64 \ 1.00]^{\tr} =  [4.56 \ 10.32 \ 16.08]^{\tr}\text{,}
\end{equation*}
which is not far off from \(16.12 [0.28 \ 0.64 \ 1.00]^{\tr}\).%
\item{}Find the characteristic polynomial \(p(\lambda)\) of \(A\). Then find the the root of \(p(\lambda)\) farthest from the origin. Compare to the result of part (a). Use technology as appropriate.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp17504664}{}\quad{}The characteristic polynomial of \(A\) is%
\begin{equation*}
p(\lambda) =-\lambda^3 + 15 \lambda^2 + 18 \lambda = -\lambda(\lambda^2-15\lambda-18)\text{.}
\end{equation*}
The quadratic formula gives the nonzero roots of \(p(\lambda)\) as%
\begin{equation*}
\frac{15 \pm \sqrt{15^2 + 4(18)}}{2} = \frac{15 \pm 3\sqrt{33}}{2}\text{.}
\end{equation*}
The roots farthest from the origin is approximately \(16.12\), as was also calculated in part (a).%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp17508504}%
Let \(A = \left[ \begin{array}{ccc} 2\amp 1\amp 0 \\ 1\amp 3\amp 1 \\ 0\amp 1\amp 2 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Use the power method to approximate the dominant eigenvalue and a corresponding eigenvector (using scaling) accurate to two decimal places. Use \(\vx_0 = [1 \ 1 \ 1]^{\tr}\) as the seed.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp17509272}{}\quad{}We use technology to calculate the scaled vectors \(A^k \vx_0\) for values of \(k\) until the components don't change in the second decimal place. For example, to two decimal places we have \(\vx_k = [0.50 \ 1.00 \ 0.50]^{\tr}\) for \(k \geq 4\). So we suspect that \(\left[ \frac{1}{2} \ 1 \ \frac{1}{2} \right]^{\tr}\) is a dominant eigenvector for \(A\). For the dominant eigenvalue, we can calculate the Rayleigh quotients \(\frac{(A\vx_k) \cdot \vx_k}{\vx_k \cdot \vx_k}\) until they do not change to two decimal places. For \(k \geq 2\), our Rayleigh quotients are all (to two decimal places) equal to \(4\). So we expect that the dominant eigenvalue of \(A\) is \(4\). We could also use the fact that%
\begin{equation*}
A \left[ \frac{1}{2} \ 1 \ \frac{1}{2} \right]^{\tr} = [2 \ 4 \ 2]^{\tr} = 4\left[ \frac{1}{2} \ 1 \ \frac{1}{2} \right]^{\tr}
\end{equation*}
to see that \(\left[ \frac{1}{2} \ 1 \ \frac{1}{2} \right]^{\tr}\) is a dominant eigenvector for \(A\) with eigenvalue \(4\).%
\item{}Determine the exact value of the dominant eigenvalue of \(A\) and compare to your result from part (a).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp17518744}{}\quad{}Technology shows that the characteristic polynomial of \(A - \lambda I_3\) is%
\begin{equation*}
p(\lambda) = -\lambda^3 + 7\lambda^2 - 14 \lambda + 8 = -(\lambda-1)(\lambda-2)(\lambda-4)\text{.}
\end{equation*}
We can see from the characteristic polynomial that \(4\) is the dominant eigenvalue of \(A\).%
\item{}Approximate the remaining eigenvalues of \(A\) using the inverse power method.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp17524120}{}\quad{}Try \(\alpha = 0.5\) and \(\alpha = 1.8\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp17523352}{}\quad{}Applying the power method to \(B = (A-0.5I_3)^{-1}\) with seed \(\vx_0 = [1 \ 1 \ 1]^{\tr}\) gives \(\vx_k \approx [ 0.50 \ 1.00 \ 0.50]^{\tr}\) for \(k \geq 5\), with Rayleigh quotients of \(2\) (to several decimal places). So \(2\) is the dominant eigenvalue of \(B\). But \(\frac{1}{\lambda-0.5}\) is also the dominant eigenvalue of \(B\), where \(\lambda\) is the corresponding eigenvalue of \(A\). . So to find \(\lambda\), we note that \(\frac{1}{\lambda-0.5} = 2\) implies that \(\lambda = 1\) is an eigenvalue of \(A\). Now applying the power method to \(B = (A-1.8I_3)^{-1}\) with seed \(\vx_0 = [1 \ 1 \ 1]^{\tr}\) gives \(\vx_k \approx [ 1.00 \ -1.00 \ 1.00]^{\tr}\) for large enough \(k\), with Rayleigh quotients of \(5\) (to several decimal places). To find the corresponding eigenvalue \(\lambda\) for \(A\), we note that \(\frac{1}{\lambda-1.8} = 5\), or \(\lambda = 2\) is an eigenvalue of \(A\). Admittedly, this method is very limited. Finding good choices for \(\alpha\) often depends on having some information about the eigenvalues of \(A\). Choosing \(\alpha\) close to an eigenvalue provides the best chance of obtaining that eigenvalue.%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_app_eigen_summ}
%
\begin{itemize}[label=\textbullet]
\item{}The power method is an iterative method that can be used to approximate the dominant eigenvalue of an \(n \times n\) matrix \(A\) that has \(n\) linearly independent eigenvectors and a dominant eigenvalue.%
\item{}To use the power method we start with a seed \(\vx_0\) and then calculate the sequence \(\{\vx_k\}\) of vectors, where \(\vx_k = A \vx_{k-1}\). If \(\vx_0\) is chosen well, then the sequence \(\{\vx_k\}\) converges to a dominant eigenvector of \(A\).%
\item{}If \(A\) is an \(n \times n\) matrix with eigenvalues \(\lambda_1\), \(\lambda_2\), \(\ldots\), \(\lambda_n\), to approximate an eigenvector of \(A\) corresponding to the eigenvalue \(\lambda_i\), we apply the power method to the matrix \(B = (A - \alpha I_n)^{-1}\), where \(\alpha\) is not a eigenvalue of \(A\) and \(\left| \frac{1}{\lambda_i - \alpha} \right| > \left| \frac{1}{\lambda_j - \alpha} \right|\) for any \(j \neq i\).%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_app_eigen_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp17704744}%
Let \(A = \left[ \begin{array}{cc} 1\amp 2\\2\amp 1 \end{array} \right]\). Let \(\vx_0 = [1 \ 0]^{\tr}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the eigenvalues and corresponding eigenvectors for \(A\).%
\item{}Use appropriate technology to calculate \(\vx_k = A^k \vx_0\) for \(k\) up to 10. Compare to a dominant eigenvector for \(A\).%
\item{}Use the eigenvectors from part (b) to approximate the dominant eigenvalue for \(A\). Compare to the exact value of the dominant eigenvalue of \(A\).%
\item{}Assume that the other eigenvalue for \(A\) is close to \(0\). Apply the inverse power method and compare the results to the remaining eigenvalue and eigenvectors for \(A\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp17722024}%
Let \(A = \left[ \begin{array}{rcc} 1\amp 2\amp 0\\-2\amp 1\amp 2 \\ 1\amp 3\amp 1 \end{array} \right]\). Use the power method to approximate a dominant eigenvector for \(A\). Use \(\vx_0 = [1 \ 1 \ 1]^{\tr}\) as the seed. Then approximate the dominant eigenvalue of \(A\).%
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp17729448}%
Let \(A = \left[ \begin{array}{rr} 3\amp -1\\-1\amp 3 \end{array} \right]\). Use the power method starting with \(\vx_0 = [1 \ 1]^{\tr}\). Explain why the method fails in this case to approximate a dominant eigenvector, and how you could adjust the seed to make the process work.%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp17728808}%
Let \(A = \left[ \begin{array}{cc} 0\amp 1\\1\amp 0 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the eigenvalues and an eigenvector for each eigenvalue.%
\item{}Apply the power method with an initial starting vector \(\vx_0 = [0 \ 1]^{\tr}\). What is the resulting sequence?%
\item{}Use equation \hyperref[x:men:eq_4_e_3]{({\xreffont\ref{x:men:eq_4_e_3}})} to explain the sequence you found in part (b).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp17732776}%
Let \(A = \left[ \begin{array}{cc} 2\amp 6 \\ 5\amp 3 \end{array} \right]\). Fill in the entries in \hyperref[x:table:T_4_e_1]{Table~{\xreffont\ref{x:table:T_4_e_1}}}, where \(\vx_k\) is the \(k\)th approximation to a dominant eigenvector using the power method, starting with the seed \(\vx_0 = [1 \ 0]^{\tr}\). Compare the results of this table to the eigenvalues of \(A\) and \(\lim_{k \to \infty} \frac{\vx_{k+1}\cdot \vx_k}{\vx_k \cdot \vx_k}\). What do you notice? \begin{tableptx}{\textbf{Values of the Rayleigh quotient}}{x:table:T_4_e_1}{}%
\centering%
{\tabularfont%
\begin{tabular}{lllllll}
\(\vv\)&\(\vx_0\)&\(\vx_1\)&\(\vx_2\)&\(\vx_3\)&\(\vx_4\)&\(\vx_5\)\tabularnewline[0pt]
\(\frac{\vv^{\tr}A\vv}{\vv^{\tr}\vv}\)&~&~&~&~&~&~\tabularnewline\hrulethick
\(\vv\)&\(\vx_6\)&\(\vx_7\)&\(\vx_8\)&\(\vx_9\)&\(\vx_{10}\)&\(\vx_{11}\)\tabularnewline[0pt]
\(\frac{\vv^{\tr}A\vv}{\vv^{\tr}\vv}\)&~&~&~&~&~&~
\end{tabular}
}%
\end{tableptx}%
%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp17750312}%
Let \(A = \left[ \begin{array}{cr} 4\amp -5 \\ 2\amp 15 \end{array} \right]\). The power method will approximate the dominant eigenvalue \(\lambda = 14\). In this exercise we explore what happens if we apply the power method to \(A^{-1}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Apply the power method to \(A^{-1}\) to approximate the dominant eigenvalue of \(A^{-1}\). Use \([1 \ 1]^{\tr}\) as the seed. How is this eigenvalue related to an eigenvalue of \(A\)?%
\item{}Explain in general why applying the power method to the inverse of an invertible matrix \(B\) might give an approximation to an eigenvalue of \(B\) of smallest magnitude. When might this not work?%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp17754536}%
There are other algebraic methods that do not rely on the determinant of a matrix that can be used to find eigenvalues of a matrix. We examine one such method in this exercise. Let \(A\) be any \(n \times n\) matrix, and let \(\vv\) be any vector in \(\R^n\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why the vectors%
\begin{equation*}
\vv, \ A\vv, \ A^2\vv, \ \ldots, \ A^n\vv
\end{equation*}
are linearly dependent.%
\item{}Let \(c_0\), \(c_1\), \(\ldots\), \(c_n\) be scalars, not all 0, so that%
\begin{equation*}
c_0 \vv + c_1 A\vv + c_2 A^2 \vv + \cdots + c_n A^n \vv = \vzero\text{.}
\end{equation*}
Explain why there must be a smallest positive integer \(k\) so that there are scalars \(a_0\), \(a_1\), \(\ldots\), \(a_k\) with \(a_k \neq 0\). such that%
\begin{equation*}
a_0 \vv + a_1 A\vv + a_2 A^2 \vv + \cdots + a_k A^k \vv = \vzero\text{.}
\end{equation*}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp17762216}{}\quad{}Proceed down the list \(c_{n-1}\), \(c_{n-2}\), etc., until you reach a weight that is non-zero.%
\item{}Let%
\begin{equation*}
q(t) = a_0 + a_1t + a_2t^2 + \cdots + a_kt^k\text{.}
\end{equation*}
Then%
\begin{equation*}
q(A) = a_0 + a_1A + a_2A^2 + \cdots + a_kA^k
\end{equation*}
and%
\begin{align*}
q(A)\vv \amp = (a_0 + a_1A + a_2A^2 + \cdots + a_kA^k)\vv\\
\amp = a_0 \vv + a_1 A\vv + a_2 A^2 \vv + \cdots + a_k A^k \vv\\
\amp = \vzero\text{.}
\end{align*}
Suppose the polynomial \(q(t)\) has a linear factor, say \(q(t) = (t-\lambda)Q(t)\) for some degree \(k-1\) polynomial \(Q(t)\). Explain why, if \(Q(A) \vv\) is non-zero, \(\lambda\) is an eigenvalue of \(A\) with eigenvector \(Q(A) \vv\).%
\item{}This method allows us to find certain eigenvalues and eigenvectors, the roots of the polynomial \(q(t)\). Any other eigenvector must lie outside the eigenspaces we have already found, so repeating the process with a vector \(\vv\) not in any of the known eigenspaces will produce different eigenvalues and eigenvectors. Let \(A = \left[ \begin{array}{ccr} 2\amp 2\amp -1 \\ 2\amp 2\amp 2 \\ 0\amp 0\amp 6 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Find the polynomial \(q(t)\). Use \(\vv = [1 \ 1 \ 1]^{\tr}\).%
\item{}Find all of the roots of \(q(t)\).%
\item{}For each root \(\lambda\) of \(q(t)\), find the polynomial \(Q(t)\) and use this polynomial to determine an eigenvector of \(A\). Verify your work.%
\end{enumerate}
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{x:exercise:ex_4_d_dominant_eigenvalue}%
We have seen that the Rayleigh quotients approximate the dominant eigenvalue of a matrix \(A\). As an alternative to using Rayleigh quotients, we can keep track of the scaling factors. Recall that the scaling in the power method can be used to make the magnitudes of the successive approximations smaller and easier to work with. Let \(A\) be an \(n \times n\) matrix and begin with a non-zero seed \(\vv_0\). We now want to keep track of the scaling factors, so let \(\alpha_0\) be the component of \(\vv_0\) with largest absolute value and let \(\vx_0 = \frac{1}{\alpha_0}\vv_0\). For \(k \geq 0\), let \(\vv_k = A\vx_{k-1}\), let \(\alpha_k\) be the component of \(\vv_k\) with largest absolute value and let \(\vx_k = \frac{1}{\alpha_k}\vv_k\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(A = \left[ \begin{array}{rc} 0\amp 1 \\ -8\amp 6 \end{array} \right]\). Use \(\vx_0 = [1 \ 1]^{\tr}\) as the seed and calculate \(\alpha_k\) for \(k\) from \(1\) to \(10\). Compare to the dominant eigenvalue of \(A\).%
\item{}Assume that for large \(k\) the vectors \(\vx_k\) approach a dominant eigenvector with dominant eigenvalue \(\lambda\). Show now in general that the sequence of scaling factors \(\alpha_k\) approaches \(\lambda\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp17796520}%
Let \(A\) be an \(n \times n\) matrix and let \(\alpha\) be a scalar that is not an eigenvalue of \(A\). Suppose that \(\vx\) is an eigenvector of \(B = (A-\alpha I_n)^{-1}\) with eigenvalue \(\beta\). Find an eigenvalue of \(A\) in terms of \(\beta\) and \(\alpha\) with corresponding eigenvector \(\vx\).%
\end{divisionexercise}%
\begin{divisionexercise}{10}{}{}{g:exercise:idp17800232}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
The largest eigenvalue of a matrix is a dominant eigenvalue.%
\item{}\lititle{True\slash{}False.}\par%
If an \(n \times n\) matrix \(A\) has \(n\) linearly independent eigenvectors and a dominant eigenvalue, then the sequence \(\{A^k \vx_0\}\) converges to a dominant eigenvector of \(A\) for any initial vector \(\vx_0\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\lambda\) is an eigenvalue of an \(n \times n\) matrix \(A\) and \(\alpha\) is not an eigenvalue of \(A\), then \(\lambda - \alpha\) is an eigenvalue of \(A - \alpha I_n\).%
\item{}\lititle{True\slash{}False.}\par%
Every square matrix has a dominant eigenvalue.%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Managing a Sheep Herd}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Managing a Sheep Herd}{}{Project: Managing a Sheep Herd}{}{}{x:section:sec_proj_sheep_herd}
Sheep farming is a significant industry in New Zealand. New Zealand is reported to have the highest density of sheep in the world. Sheep can begin to reproduce after one year, and give birth only once per year. \hyperref[x:table:T_Sheep]{Table~{\xreffont\ref{x:table:T_Sheep}}} gives Birth and Survival Rates for Female New Zealand Sheep (from G. Caughley, ``Parameters for Seasonally Breeding Populations,'' \pubtitle{Ecology}, \emph{48}, (1967), 834-839). Since sheep hardly ever live past 12 years, we will only consider the population through 12 years.%
\begin{tableptx}{\textbf{New Zealand female sheep data by age group}}{x:table:T_Sheep}{}%
\centering%
{\tabularfont%
\begin{tabular}{Accc}\hrulethin
\multicolumn{1}{AcA}{Age (years)}&\multicolumn{1}{cA}{Birth Rate}&\multicolumn{1}{cA}{Survival Rate}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{0-1}&\multicolumn{1}{cA}{0.000}&\multicolumn{1}{cA}{0.845}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{1-2}&\multicolumn{1}{cA}{0.045}&\multicolumn{1}{cA}{0.975}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{2-3}&\multicolumn{1}{cA}{0.391}&\multicolumn{1}{cA}{0.965}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{3-4}&\multicolumn{1}{cA}{0.472}&\multicolumn{1}{cA}{0.950}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{4-5}&\multicolumn{1}{cA}{0.484}&\multicolumn{1}{cA}{0.926}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{5-6}&\multicolumn{1}{cA}{0.546}&\multicolumn{1}{cA}{0.895}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{6-7}&\multicolumn{1}{cA}{0.543}&\multicolumn{1}{cA}{0.850}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{7-8}&\multicolumn{1}{cA}{0.502}&\multicolumn{1}{cA}{0.786}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{8-9}&\multicolumn{1}{cA}{0.468}&\multicolumn{1}{cA}{0.691}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{9-10}&\multicolumn{1}{cA}{0.459}&\multicolumn{1}{cA}{0.561}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{10-11}&\multicolumn{1}{cA}{0.433}&\multicolumn{1}{cA}{0.370}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{11-12}&\multicolumn{1}{cA}{0.421}&\multicolumn{1}{cA}{0.000}\tabularnewline\hrulethin
\end{tabular}
}%
\end{tableptx}%
As sheep reproduce, they add to the 0-1 sheep (lamb) population. The potential to produce offspring is called \terminology{fecundity} (derived from the word \terminology{fecund} which generally refers to reproductive ability) and determines how many lamb are added to the population. Let \(F_k\) (the fecundity rate) be the rate at which females in age class \(k\) give birth to female offspring. Not all members of a given age group survive to the next age groups, so let \(s_k\) be the fraction of individuals that survives from age group \(k\) to age group \(k+1\). With these ideas in mind, we can create a life cycle chart as in \hyperref[x:figure:F_Life_cycle]{Figure~{\xreffont\ref{x:figure:F_Life_cycle}}} that illustrates how the population of sheep changes on a farm (for the sake of space, we illustrate with four age classes).%
\begin{figureptx}{Life cycle with four age classes.}{x:figure:F_Life_cycle}{}%
\begin{image}{0.25}{0.5}{0.25}%
\includegraphics[width=\linewidth]{external/4_d_Cycles.pdf}
\end{image}%
\tcblower
\end{figureptx}%
To model the sheep population, we need a few variables. Let \(n_1^{(0)}\) be the number of sheep in age group 0-1, \(n_2^{(0)}\) the number in age group 1-2, \(n_3\) the number in age group 2-3 and, in general, \(n_k^{(0)}\) the number of sheep in age group \((k-1)\)-\(k\) at some initial time (time \(0\)), and let%
\begin{equation*}
\vx_0 = \left[ n_1^{(0)} \ n_2^{(0)} \ n_3^{(0)} \ \cdots \ n_{12}^{(0)} \right]^{\tr}\text{.}
\end{equation*}
%
\par
We wish to determine the populations in the different groups after one year. Let%
\begin{equation*}
\vx_1 = \left[ n_1^{(1)} \ n_2^{(1)} \ n_3^{(1)}  \ \cdots \ n_{12}^{(1)} \right]^{\tr}\text{,}
\end{equation*}
where \(n_1^{(1)}\) denotes the number of sheep in age group 0-1, \(n_2^{(1)}\) the number of sheep in age group 1-2 and, in general, \(n_{k}^{(1)}\) the number of tilapia in age group \((k-1)\)-\(k\) after one year.%
\begin{project}{}{x:project:Leslie_1}%
\hyperref[x:table:T_Sheep]{Table~{\xreffont\ref{x:table:T_Sheep}}} shows that, on average, each female in age group 1-2 produces \(0.045\) female offspring in a year. Since there are \(n_2\) females in age group 1-2, the lamb population increases by \(0.045n_2\) in a year.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Continue this analysis to explain why%
\begin{align*}
n_1^{(1)} \amp = 0.045n_2+0.391 n_3+0.472n_4+0.484 n_5+0.546n_6+0.543n_7\\
\amp \qquad+0.502 n_8+0.468 n_9+0.459n_{10}+0.433 n_{11}+0.421 n_{12}\text{.}
\end{align*}
%
\item{}Explain why \(n_2^{(1)} = 0.845n_1\).%
\item{}Now explain why%
\begin{equation}
\vx_1 = L\vx_0\text{,}\label{x:men:eq_Leslie_1}
\end{equation}
where \(L\) is the matrix%
\begin{equation}
{\scriptsize  \left[  \begin{array}{cccccccccccc} 0       \amp  0.045 \amp  0.391 \amp  0.472 \amp  0.484 \amp  0.546 \amp  0.543 \amp  0.502 \amp  0.468 \amp  0.459 \amp  0.433 \amp  0.421 \\ 0.845   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0.975 \amp  0    \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0.965  \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0.950  \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0.926 \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0.895  \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0    \amp  0.850  \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0    \amp  0    \amp  0.786  \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0    \amp  0    \amp  0    \amp  0.691  \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0    \amp  0    \amp  0    \amp  0    \amp  0.561  \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0    \amp  0    \amp  0    \amp  0    \amp  0    \amp  0.370  \amp  0 \end{array}  \right]}\text{.}\label{x:men:eq_Leslie_2}
\end{equation}
%
\end{enumerate}
\end{project}%
\index{Leslie matrix} Notice that our matrix \(L\) has the form%
\begin{equation*}
\left[ \begin{array}{cccccc} F_1\amp F_2\amp F_3\amp \cdots\amp F_{n-1}\amp F_n \\ s_1\amp 0\amp 0\amp \cdots\amp 0\amp 0 \\ 0\amp s_2\amp 0\amp \cdots\amp 0\amp 0 \\  0\amp 0\amp s_3\amp \cdots\amp 0\amp 0 \\ \amp \amp \amp \ddots\amp \amp  \\ 0\amp 0\amp 0\amp \cdots\amp s_{n-1}\amp 0 \end{array}  \right]\text{.}
\end{equation*}
Such a matrix is called a \terminology{Leslie matrix}.%
\par
Leslie matrices have certain useful properties, and one eigenvalue of a Leslie matrix can tell us a lot about the long-term behavior of the situation being modeled. You can take these properties as fact unless otherwise directed.%
\begin{enumerate}
\item{}A Leslie matrix \(L\) has a unique positive eigenvalue \(\lambda_1\) with a corresponding eigenvector \(\vv_1\) whose entries are all positive.%
\item{}If \(\lambda_i\) (\(i > 1\)) is any other eigenvalue (real or complex) of \(L\), then \(|\lambda_i| \leq \lambda_1\). If \(\lambda_1\) is the largest magnitude eigenvalue of a matrix \(L\), we call \(\lambda_1\) a \terminology{dominant eigenvalue} of \(L\).%
\item{}If any two successive entries in the first row of \(L\) are both positive, then \(|\lambda_i| \lt \lambda_1\) for every \(i > 1\). In this case we say that \(\lambda_1\) is a \terminology{strictly dominant eigenvalue} of \(L\). In a Leslie model, this happens when the females in two successive age classes are fertile, which is almost always the case.%
\item{}If \(\lambda_1\) is a strictly dominant eigenvalue, then \(\vx_k\) is approximately a scalar multiple of \(\vv_1\) for large values of \(k\), regardless of the initial state \(\vx_0\). In other words, large state vectors are close to eigenvectors for \(\lambda_1\).%
\end{enumerate}
%
\par
We can use these properties to determine the long-term behavior of the sheep herd.%
\begin{project}{}{x:project:act_Leslie_2}%
Assume that \(L\) is defined by \hyperref[x:men:eq_Leslie_2]{({\xreffont\ref{x:men:eq_Leslie_2}})}, and let%
\begin{equation*}
\vx_m = \left[ n_1^{(m)} \ n_2^{(m)} \ n_3^{(m)} \ \cdots \ n_{12}^{(m)} \right]^{\tr}\text{,}
\end{equation*}
where \(n_1^{(m)}\) denotes the number of sheep in age group 0-1, \(n_2^{(m)}\) the number of sheep in age group 1-2 and, in general, \(n_k^{(m)}\) the number of sheep in age group \((k-1)\)-\(k\) after \(k\) years.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Assume that \(\vx_0 = \left[100 \ 100 \ 100 \ \cdots \ 100 \right]^{\tr}\). Use appropriate technology to calculate \(\vx_{22}\), \(\vx_{23}\), \(\vx_{24}\), and \(\vx_{25}\). Round to the nearest whole number. What do you notice about the sheep population? You may use the GeoGebra applet at \href{https://www.geogebra.org/m/yqss88xq}{\nolinkurl{geogebra.org/m/yqss88xq}}.%
We can use the third and fourth properties of Leslie matrices to better understand the long-term behavior of the sheep population. Since successive entries in the first row of the Leslie matrix in \hyperref[x:men:eq_Leslie_2]{({\xreffont\ref{x:men:eq_Leslie_2}})} are positive, our Leslie matrix has a strictly dominant eigenvalue \(\lambda_1\). Given the dimensions of our Leslie matrix, finding this dominant eigenvalue through algebraic means is not feasible. Use the power method to approximate the dominant eigenvalue \(\lambda_1\) of the Leslie matrix in \hyperref[x:men:eq_Leslie_2]{({\xreffont\ref{x:men:eq_Leslie_2}})} to five decimal places. Explain your process. Then explain how this dominant eigenvalue tells us that, unchecked, the sheep population grows at a rate that is roughly exponential. What is the growth rate of this exponential growth? You may use the GeoGebra applet at \href{https://www.geogebra.org/m/yqss88xq}{\nolinkurl{geogebra.org/m/yqss88xq}}.%
\end{enumerate}
\end{project}%
\hyperref[x:project:act_Leslie_2]{Project Activity~{\xreffont\ref{x:project:act_Leslie_2}}} indicates that, unchecked, the sheep population will grow without bound, roughly exponentially with ratio equal to the dominant eigenvalue of our Leslie matrix \(L\). Of course, a sheep farmer cannot provide the physical environment or the resources to support an unlimited population of sheep. In addition, most sheep farmers cannot support themselves only by shearing sheep for the wool. Consequently, some harvesting of the sheep population each year for meat and skin is necessary. A sustainable harvesting policy allows for the regular harvesting of some sheep while maintaining the population at a stable level. It is necessary for the farmer to find an optimal harvesting rate to attain this stable population and the following activity leads us through an analysis of how such a harvesting rate can be determined.%
\begin{project}{}{x:project:act_Leslie_harvest}%
The Leslie model can be modified to consider harvesting. It is possible to harvest different age groups at different rates, and to harvest only some age groups and not others. In the case of sheep, it might make sense to only harvest from the youngest population since lamb is more desirable than mutton and the lamb population grows the fastest. Assume that this is our harvesting strategy and that we harvest our sheep from only the youngest age group at the start of each year. Let \(h\) be the fraction of sheep we harvest from the youngest age group each year after considering growth.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}If we begin with an initial population \(\vx_0\), then the state vector after births and expected deaths is \(L\vx_0\). Now we harvest. Explain why if we harvest a fraction \(h\) from the youngest age group after considering growth, then the state vector after 1 year will be%
\begin{equation*}
\vx_1 = L\vx_0 - HL\vx_0\text{,}
\end{equation*}
where%
\begin{equation*}
H =  \left[ \begin{array}{c c c c c c c c c c c c} h      \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0     \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0    \amp  0   \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0    \amp  0    \amp  0   \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0    \amp  0    \amp  0    \amp  0   \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0    \amp  0    \amp  0    \amp  0    \amp  0   \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0    \amp  0    \amp  0    \amp  0    \amp  0    \amp  0   \amp  0 \\ 0     \amp  0   \amp  0    \amp  0    \amp  0   \amp  0    \amp  0    \amp  0    \amp  0    \amp  0    \amp  0    \amp  0 \end{array}  \right]\text{.}
\end{equation*}
%
\item{}Our goal is to find a harvesting rate that will lead to a steady state in which the sheep population remains the same each year. In other words, we want to find a value of \(h\), if one exists, that satisfies%
\begin{equation}
\vx = L\vx - HL\vx\text{.}\label{x:men:eq_Harvest}
\end{equation}
Show that \hyperref[x:men:eq_Harvest]{({\xreffont\ref{x:men:eq_Harvest}})} is equivalent to the matrix equation%
\begin{equation}
\vx = (I_{12}-H)L\vx\text{.}\label{x:men:eq_Harvest2}
\end{equation}
%
\item{}Use appropriate technology to experiment numerically with different values of \(h\) to find the value you think gives the best uniform harvest rate. Explain your reasoning. You may use the GeoGebra applet at \href{https://www.geogebra.org/m/yqss88xq}{\nolinkurl{geogebra.org/m/yqss88xq}}.%
\item{}Now we will use some algebra to find an equation that explicitly gives us the harvest rate in the general setting. This will take a bit of work, but none of it is too difficult. To simplify our work but yet illustrate the overall idea, let us consider the general \(4 \times 4\) case with arbitrary Leslie matrix%
\begin{equation*}
L = \left[ \begin{array}{cccc} F_1 \amp  F_2   \amp  F_3   \amp  F_4 \\ s_1 \amp  0   \amp  0   \amp  0  \\ 0   \amp  s_2   \amp  0   \amp  0 \\ 0   \amp  0   \amp  s_3  \amp  0 \end{array}  \right]\text{.}
\end{equation*}
Recall that we want to find a value of \(h\) that satisfies \hyperref[x:men:eq_Harvest2]{({\xreffont\ref{x:men:eq_Harvest2}})} with \(H = \left[ \begin{array}{cccc} h   \amp  0   \amp  0   \amp  0 \\ 0   \amp  0   \amp  0   \amp  0  \\ 0   \amp  0   \amp  0   \amp  0 \\ 0   \amp  0   \amp  0      \amp  0 \end{array}  \right]\). Let \(\vx = [x_1 \  x_2 \ x_3 \ x_4]^{\tr}\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Calculate the matrix product \((I_{4}-H)L\). Explain why this product is again a Leslie matrix and why \((I_{4}-H)L\) will have a dominant eigenvalue of 1.%
\item{}Now calculate \((I_{4}-H)L\vx\) and set it equal to \(\vx\). Write down the resulting system of 4 equations that must be satisfied. Be sure that your first equation is%
\begin{equation}
x_1 = (1-h)F_1x_1 + (1-h)F_2x_2 + (1-h)F_3x_3 + (1-h)F_4x_4\text{.}\label{x:men:eq_harvestval1}
\end{equation}
%
\item{}Equation \hyperref[x:men:eq_harvestval1]{({\xreffont\ref{x:men:eq_harvestval1}})} as written depends on the entries of the vector \(\vx\), but we should be able to arrive at a result that is independent of \(\vx\). To see how we do this, we assume the population of the youngest group is never 0, so we can divide both sides of \hyperref[x:men:eq_harvestval1]{({\xreffont\ref{x:men:eq_harvestval1}})} by \(x_1\) to obtain%
\begin{equation}
1 = (1-h)F_1 + (1-h)F_2 \frac{x_2}{x_1} + (1-h)F_3 \frac{x_3}{x_1} + (1-h)F_4 \frac{x_4}{x_1}\text{.}\label{x:men:eq_harvestval2}
\end{equation}
Now we need to write the fractions \(\frac{x_2}{x_1}\),   \(\frac{x_3}{x_1}\), and \(\frac{x_4}{x_1}\) so that they do not involve the \(x_i\). Use the remaining equations in your system to show that%
\begin{align*}
\frac{x_2}{x_1} \amp = s_1\\
\frac{x_3}{x_1} \amp = s_1s_2\\
\frac{x_4}{x_1} \amp = s_1s_2s_3\text{.}
\end{align*}
%
\item{}Now conclude that the harvesting value \(h\) must satisfy the equation%
\begin{equation}
1 = (1-h) [F_1 + F_2 s_1 + F_3 s_1s_2 + F_4 s_1s_2s_3]\text{.}\label{x:men:eq_harvestval3}
\end{equation}
The value \(R = F_1 + F_2 s_1 + F_3 s_1s_2 + F_4 s_1s_2s_3\) is called the \terminology{net reproduction rate of the population} and turns out to be the average number of daughters born to a female in her expected lifetime.%
\end{enumerate}
\item{}Extend \hyperref[x:men:eq_harvestval3]{({\xreffont\ref{x:men:eq_harvestval3}})} to the 12 age group case of the sheep herd. Calculate the value of \(R\) for this sheep herd and then find the value of \(h\). Compare this \(h\) to the value you obtained through experimentation earlier. Find the fraction of the lambs that should be harvested each year and explain what the stable population state vector \(\vx\) tells us about the sheep population for this harvesting policy.%
\end{enumerate}
\end{project}%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 21 Complex Eigenvalues}
\typeout{************************************************}
%
\begin{chapterptx}{Complex Eigenvalues}{}{Complex Eigenvalues}{}{}{x:chapter:chap_complex_eigenvalues}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp17900840}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What properties do complex eigenvalues of a real matrix satisfy?%
\item{}What properties do complex eigenvectors of a real matrix satisfy?%
\item{}What is a rotation-scaling matrix?%
\item{}How do we find a rotation-scaling matrix within a matrix with complex eigenvalues?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: The Gershgorin Disk Theorem}
\typeout{************************************************}
%
\begin{sectionptx}{Application: The Gershgorin Disk Theorem}{}{Application: The Gershgorin Disk Theorem}{}{}{x:section:sec_appl_gershgorin}
We have now seen different methods for calculating\slash{}approximating eigenvalues of a matrix. The algebraic method using the characteristic polynomial can provide exact values, but only in cases where the size of the matrix is small. Methods like the power method allow us to approximate eigenvalues in many, but not all, cases. These approximation techniques can be made more efficient if we have some idea of where the eigenvalues are. The Gershgorin Disc Theorem is a useful tool that can quickly provide bounds on the location of eigenvalues using elementary calculations. For example, using the Gershsgorin Disk Theorem we can quickly tell that the real parts of the eigenvalues of the matrix%
\begin{equation*}
\left[ \begin{array}{ccr} 3\amp 1\amp -1 \\ 0\amp -1+i\amp i \\ 2\amp 1\amp -2i \end{array}  \right]
\end{equation*}
lie between \(-4\) and 5 and the imaginary parts lie between \(-5\) and 2. Even more, we can say that the eigenvalues lie in the disks (called \terminology{Gershgorin disks}) shown in \hyperref[x:figure:F_Gershgorin_1]{Figure~{\xreffont\ref{x:figure:F_Gershgorin_1}}}.%
\begin{figureptx}{Gershgorin disks.}{x:figure:F_Gershgorin_1}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/Gershgorin_1.pdf}
\end{image}%
\tcblower
\end{figureptx}%
We will learn more details about the Gershgorin Disk Theorem at the end of this section.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_comp_eigen_intro}
So far we have worked with real matrices whose eigenvalues are all real. However, the characteristic polynomial of a matrix with real entries can have complex roots. In this section we investigate the properties of these complex roots and their corresponding eigenvectors, how these complex eigenvectors are found, and the geometric interpretation of the transformations defined by matrices with complex eigenvalues. Although we can consider matrices that have complex numbers as entries, we will restrict ourselves to matrices with real entries.%
\begin{exploration}{}{x:exploration:pa_4_e}%
Let \(A = \left[ \begin{array}{rc} 2\amp 4 \\ -2\amp 2 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the characteristic polynomial of \(A\).%
\item{}Find the eigenvalues of \(A\). You should get two complex numbers. How are these complex numbers related?%
\item{}Find an eigenvector corresponding to each eigenvalue of \(A\). You should obtain vectors with complex entries.%
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Complex Eigenvalues}
\typeout{************************************************}
%
\begin{sectionptx}{Complex Eigenvalues}{}{Complex Eigenvalues}{}{}{x:section:sec_comp_eigen}
As you noticed in \hyperref[x:exploration:pa_4_e]{Preview Activity~{\xreffont\ref{x:exploration:pa_4_e}}}, the complex roots of the characteristic equation of a real matrix \(A\) come in complex conjugate pairs. This should come as no surprise since we know through our use of the quadratic formula that complex roots of (real) quadratic polynomials come in complex conjugate pairs. More generally, if \(p(x)=a_0+a_1x+a_2x^2 + \cdots + a_nx^n\) is a polynomial with real coefficients and \(z\) is a root of this polynomial, meaning \(p(z)=0\), then%
\begin{equation*}
0=\overline{p(z)} = \overline{a_0+a_1z+a_2z^2 + \cdots + a_nz^n} = a_0 + a_1 \overline{z} + a_2 \overline{z}^2 + \cdots + a_n \overline{z}^n = p(\overline{z})\,\text{.}
\end{equation*}
Therefore, \(\overline{z}\) is also a root of \(p(x)\).%
\begin{activity}{}{x:activity:act_4e_1}%
Let \(A=\left[ \begin{array}{cr} 0\amp -1 \\ 1\amp 0 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}The matrix transformation \(T:\R^2 \to \R^2\) defined by \(T(\vx)=A\vx\) is a rotation transformation. What is the angle of rotation?%
\item{}Find the eigenvalues of \(A\). For each eigenvalue, find an eigenvector.%
\end{enumerate}
\end{activity}%
In \hyperref[x:exploration:pa_4_e]{Preview Activity~{\xreffont\ref{x:exploration:pa_4_e}}} and in \hyperref[x:activity:act_4e_1]{Activity~{\xreffont\ref{x:activity:act_4e_1}}}, you found that if \(\vv\) is an eigenvector of \(A\) corresponding to \(\lambda\), then \(\overline{\vv}\) obtained by taking the complex conjugate of each entry in \(\vv\) is an eigenvector of \(A\) corresponding to \(\overline{\lambda}\). Specifically, if \(\vv=\vu+i\vw\) where both \(\vu\) and \(\vw\) are real vectors is an eigenvector of \(A\), then so is \(\overline{\vv} = \vu-i\vw\). We can justify this property using matrix algebra as follows:%
\begin{equation*}
A\overline{\vv} = \overline{A}\overline{\vv} = \overline{A\vv} = \overline{\lambda \vv} = \overline{\lambda} \overline{\vv} \,\text{.}
\end{equation*}
%
\par
In the first equality, we used the fact that \(A\) is a real matrix, so \(\overline{A}=A\). In all the other equalities, we used the properties of the conjugation operation in complex numbers.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Rotation and Scaling Matrices}
\typeout{************************************************}
%
\begin{sectionptx}{Rotation and Scaling Matrices}{}{Rotation and Scaling Matrices}{}{}{x:section:sec_mtx_rotate_scale}
Recall that a rotation matrix is of the form%
\begin{equation*}
R_\theta= \left[ \begin{array}{cr} \cos(\theta)\amp -\sin(\theta) \\ \sin(\theta)\amp \cos(\theta) \end{array}  \right]
\end{equation*}
where the rotation is counterclockwise about the origin by an angle of \(\theta\) radians. In \hyperref[x:activity:act_4e_1]{Activity~{\xreffont\ref{x:activity:act_4e_1}}}, we considered the rotation matrix with angle \(\pi/2\) in counterclockwise direction. We will soon see that rotation matrices play an important role in the geometry of a matrix transformation for a matrix that has complex eigenvalues. In this activity, we will restrict ourselves to the \(2 \times 2\) case, but similar arguments can be made in higher dimensions.%
\begin{activity}{}{x:activity:act_4e_2}%
Let \(A=\left[ \begin{array}{rc} 1\amp 1 \\ -1\amp 1 \end{array}  \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why \(A\) is not a rotation matrix.%
\item{}Although \(A\) is not a rotation matrix, there is a rotation matrix \(B\) inside \(A\). To find the matrix \(B\), factor out \(\sqrt{2}\) from all entries of \(A\). In other words, write \(A\) as a product of two matrices in the form%
\begin{equation*}
A = \left[ \begin{array}{cc} \sqrt{2} \amp 0 \\ 0\amp \sqrt{2} \end{array}  \right] B \;\text{.}
\end{equation*}
%
\item{}The \(B\) matrix is a rotation matrix with an appropriate \(\theta\). Find this \(\theta\).%
\item{}If we think about the product of two matrices as applying one transformation after another, describe the effect of the matrix transformation defined by \(A\) geometrically.%
\end{enumerate}
\end{activity}%
More generally, if we have a matrix \(A\) of the form \(A=\left[ \begin{array}{cr} a\amp -b \\ b\amp a \end{array}  \right]\), then%
\begin{equation*}
A = \left[ \begin{array}{cc} \sqrt{a^2+b^2} \amp 0 \\ 0\amp \sqrt{a^2+b^2} \end{array}  \right] \left[ \begin{array}{cc} \frac{a}{\sqrt{a^2+b^2}}\amp \frac{-b}{\sqrt{a^2+b^2}} \\ \frac{b}{\sqrt{a^2+b^2}}\amp \frac{a}{\sqrt{a^2+b^2}} \end{array}  \right] \,\text{.}
\end{equation*}
%
\par
\index{rotation-scaling matrices} The first matrix in the decomposition is a scaling matrix with a scaling factor of \(s=\sqrt{a^2+b^2}\). So if \(s>1\), the transformation stretches vectors, and if \(s\lt 1\), the transformation shrinks vectors. The second matrix in the decomposition is a rotation matrix with angle \(\theta\) such that \(\cos(\theta)=\frac{a}{\sqrt{a^2+b^2}}\) and \(\sin(\theta)=\frac{b}{\sqrt{a^2+b^2}}\). This angle is also the angle between the positive \(x\)-axis and the vector \(\vv=\left[ \begin{array}{c} a\\ b \end{array} \right]\). We will refer to the matrices of the form \(\left[ \begin{array}{cr} a\amp -b \\ b\amp a \end{array} \right]\) as \terminology{rotation-scaling matrices}.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Matrices with Complex Eigenvalues}
\typeout{************************************************}
%
\begin{sectionptx}{Matrices with Complex Eigenvalues}{}{Matrices with Complex Eigenvalues}{}{}{x:section:sec_mtx_comp_eigen}
Now we will investigate how a general \(2\times 2\) matrix with complex eigenvalues can be seen to be similar (both in a linear algebra and a colloquial meaning) to a rotation-scaling matrix.%
\begin{activity}{}{x:activity:act_4e_3}%
Let \(B=\left[ \begin{array}{cr} 1\amp -5\\2\amp 3 \end{array} \right]\). The eigenvalues of \(B\) are \(2\pm 3i\). An eigenvector for the eigenvalue \(2-3i\) is \(\vv=\left[ \begin{array}{c} -5 \\ 1-3i \end{array} \right]\). We will use this eigenvector to show that \(B\) is similar to a rotation-scaling matrix.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Any complex vector \(\vv\) can be written as \(\vv=\vu+i\vw\) where both \(\vu\) and \(\vw\) are real vectors. What are these real vectors \(\vu\) and \(\vw\) for the eigenvector \(\vv\) above?%
\item{}Let \(P=[ \vu \ \vw ]\) be the matrix whose first column is the real part of \(\vv\) and whose second column is the imaginary part of \(\vv\) (without the \(i\)). Find \(R=P^{-1}BP\).%
\item{}Express \(R\) as a product of a rotation and a scaling matrix. What is the factor of scaling? What is the rotation angle?%
\end{enumerate}
\end{activity}%
In \hyperref[x:activity:act_4e_3]{Activity~{\xreffont\ref{x:activity:act_4e_3}}}, we saw that the matrix \(B\) with complex eigenvalues \(2\pm 3i\) is similar to a rotation-scaling matrix. Specifically \(R=P^{-1}BP\), where the columns of \(P\) are the real and imaginary parts of an eigenvector of \(B\), is the rotation-scaling matrix with a factor of scaling by \(\sqrt{2^2+3^2}\) and a rotation by angle \(\theta=\arccos(\frac{2}{\sqrt{2^2+3^2}})\).%
\par
Does a similar decomposition result hold for a general \(2\times 2\) matrix with complex eigenvalues? We investigate this question in the next activity.%
\begin{activity}{}{x:activity:act_4e_4}%
Let \(A\) be a \(2\times 2\) matrix with complex eigenvalue \(\lambda=a-bi\), \(b\neq 0\), and corresponding complex eigenvector \(\vv=\vu+i\vw\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why \(A\vv = A\vu+iA\vw\).%
\item{}Explain why \(\lambda\vv = (a\vu+b\vw)+i (a\vw-b\vu)\).%
\item{}Use the previous two results to explain why%
%
\begin{itemize}[label=\textbullet]
\item{}\(A\vu=a\vu+b\vw\) and%
\item{}\(A\vw = a\vw - b\vu\).%
\end{itemize}
\item{}Let \(P=[ \vu \ \vw ]\). We will now show that \(AP=PR\) where \(R=\left[ \begin{array}{cr} a\amp -b \\ b\amp a \end{array}  \right]\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Without any calculation, explain why%
\begin{equation*}
AP = [ A\vu \  A\vw ]\text{.}
\end{equation*}
%
\item{}Recall that if \(M\) is an \(m \times n\) matrix and \(\vx\) is an \(n \times 1\) vector, then the matrix product \(M\vx\) is a linear combination of the columns of \(M\) with weights the corresponding entries of the vector \(\vx\). Use this idea to show that%
\begin{equation*}
PR = [ a\vu + b\vw \  -b\vu + a\vw ]\text{.}
\end{equation*}
%
\item{}Now explain why \(AP = PR\).%
\item{}Assume for the moment that \(P\) is an invertible matrix. Show that \(A = PRP^{-1}\).%
\end{enumerate}
\end{enumerate}
\end{activity}%
Your work in \hyperref[x:activity:act_4e_4]{Activity~{\xreffont\ref{x:activity:act_4e_4}}} shows that any \(2\times 2\) matrix is similar to a rotation-scaling matrix with a factor of scaling by \(\sqrt{a^2+b^2}\) and a rotation by angle \(\theta=\arccos(\frac{a}{\sqrt{a^2+b^2}})\) if \(b\geq 0\), and \(\theta=-\arccos(\frac{a}{\sqrt{a^2+b^2}})\) if \(b\lt 0\). Geometrically, this means that every \(2 \times 2\) real matrix with complex eigenvalues is just a scaled rotation (\(R\)) with respect to the basis \(\B\) formed by \(\vu\) and \(\vw\) from the complex eigenvector \(\vv\). Multiplying by \(P^{-1}\) and \(P\) simply provides the change of basis from the standard basis to the basis \(\B\), as we will see in detail when we learn about linear transformations.%
\begin{theorem}{}{}{g:theorem:idp18019544}%
Let \(A\) be a real \(2\times 2\) matrix with complex eigenvalue \(a-bi\) and corresponding eigenvector \(\vv=\vu+i\vw\). Then%
\begin{equation*}
A=PRP^{-1} \; , \text{ where }  P=[ \vu \ \vw ] \; \text{ and }  R= \left[ \begin{array}{cr} a\amp -b \\ b\amp a \end{array}  \right] \,\text{.}
\end{equation*}
%
\end{theorem}
The one fact that we have not yet addressed is why the matrix \(P = [ \vu \ \vw ]\) is invertible. We do that now to complete the argument.%
\par
Let \(A\) be a real \(2 \times 2\) matrix with \(A \vv = \lambda \vv\), where \(\lambda = a-bi\), \(b \neq 0\) and \(\vv=\vu+i\vw\) (where \(\vu\) and \(\vv\) are in \(\R^2\)) with \(\vw \neq \vzero\). From \hyperref[x:activity:act_4e_4]{Activity~{\xreffont\ref{x:activity:act_4e_4}}} we know that%
\begin{equation*}
A\vu = a\vu + b\vw \ \text{ and }  \ A\vw = a\vw - b \vu\text{.}
\end{equation*}
%
\par
To show that \(\vu\) and \(\vw\) are linearly independent, we need to show that no nontrivial linear combination of \(\vu\) and \(\vw\) can be the zero vector. Suppose%
\begin{equation*}
x_1\vu + x_2\vw = \vzero
\end{equation*}
for some scalars \(x_1\) and \(x_2\). We will show that \(x_1 = x_2 = 0\). Assume to the contrary that one of \(x_1, x_2\) is not zero. First, assume \(x_1 \neq 0\). Then \(\vu = -\frac{x_2}{x_1} \vw\). Let \(c = -\frac{x_2}{x_1}\). From this we have%
\begin{align*}
A \vu \amp = A(c\vw)\\
A \vu \amp = cA \vw\\
a\vu + b\vw \amp = c(a \vw - b \vu)\\
(a+cb)\vu \amp = (ca-b)\vw\\
(a+cb)(c\vw) \amp = (ca-b)\vw\text{.}
\end{align*}
%
\par
Since \(\vw \neq \vzero\) we must have \((a+cb)c = ca-b\). A little algebra shows that \((c^2+1)b = 0\). Since \(b \neq 0\), we conclude that \(c^2+1=0\), which is impossible for a real constant \(c\). Therefore, we cannot have \(x_1 \neq 0\). A similar argument (left to the reader) shows that \(x_2 = 0\). Thus we can conclude that \(\vu\) and \(\vw\) are linearly independent.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_comp_eigen_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp18042840}%
Let \(A = \left[ \begin{array}{rcr} 0\amp 1\amp 0 \\ -1\amp 0\amp -1 \\ 1\amp 1\amp 1 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Without doing any computations, explain why not all of the eigenvalues of \(A\) can be complex.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp18044760}{}\quad{}Since complex eigenvalues occur in conjugate pairs, the complex eigenvalues with nonzero imaginary parts occur in pairs. Since \(A\) can have at most 3 different eigenvalues, at most two of them can have nonzero imaginary parts. So at least one eigenvalue of \(A\) is real.%
\item{}Find all of the eigenvalues of \(A\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp18048472}{}\quad{}For this matrix \(A\) we have \(A - \lambda I_3 = \left[ \begin{array}{rrc} -\lambda\amp 1\amp 0 \\ -1\amp -\lambda\amp -1 \\ 1\amp 1\amp -\lambda+1 \end{array}  \right]\). Using a cofactor expansion along the first row gives us%
\begin{align*}
\det(A - \lambda I_3) \amp = (-\lambda)\left((-\lambda)(1-\lambda)+1\right) - \left((-1)(1-\lambda)+1\right)\\
\amp = -\lambda^3 + \lambda^2 - \lambda +1 - \lambda -1\\
\amp = \lambda^3 + \lambda^2 - 2\lambda\\
\amp = -\lambda(\lambda^2 -\lambda + 2)\text{.}
\end{align*}
The roots of the characteristic polynomial are \(\lambda = 0\) and%
\begin{equation*}
\lambda = \frac{1 \pm \sqrt{1-4(2)}}{2} = \frac{1}{2}(1 \pm \sqrt{7}i)\text{.}
\end{equation*}
%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp18055256}%
Let \(A = \left[ \begin{array}{rc} 1\amp 2\\-1\amp 3 \end{array} \right]\). Find a rotation scaling matrix \(R\) that is similar to \(A\). Identify the rotation and scaling factor.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp18050776}{}\quad{}The eigenvalues of \(A\) are the roots of the characteristic polynomial%
\begin{align*}
p(\lambda) \amp = \det(A - \lambda I_2)\\
\amp = \det\left(\left[ \begin{array}{rc} 1-\lambda\amp 2\\
-1\amp 3-\lambda \end{array} \right]\right)\\
\amp = (1-\lambda)(3-\lambda) + 2\\
\amp = \lambda^2 -4\lambda + 5\text{.}
\end{align*}
%
\par
The quadratic formula shows that the roots of \(p(\lambda)\) are%
\begin{equation*}
\frac{4 \pm \sqrt{-4}}{2} = 2 \pm i\text{.}
\end{equation*}
%
\par
To find an eigenvector for \(A\) with eigenvalue \(2-i\), we row reduce%
\begin{equation*}
A - (2-i) I_3 =   \left[ \begin{array}{cc} -1+i\amp 2\\-1\amp 1+i \end{array}  \right]
\end{equation*}
to%
\begin{equation*}
\left[ \begin{array}{cc} 1\amp -i-1\\0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
%
\par
An eigenvector for \(A\) with eigenvalue \(2-i\) is then%
\begin{equation*}
[ 1+i \  \ 1]^{\tr} = [1 \ 1]^{\tr} + i[1 \ 0]^{\tr}\text{.}
\end{equation*}
%
\par
Letting \(P = \left[  \begin{array}{cc} 1\amp 1\\1\amp 0 \end{array}  \right]\), we have%
\begin{equation*}
R = P^{-1}AP = \left[ \begin{array}{cr} 2\amp -1\\1\amp 2 \end{array}  \right]\text{.}
\end{equation*}
%
\par
The scaling is determined by the determinant of \(R\) which is \(5\), and the angle \(\theta\) of rotation satisfies \(\sin(\theta) = \frac{1}{5}\). This makes \(\theta \approx 0.2014\) radians or approximately \(11.5370^{\circ}\) counterclockwise.%
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_comp_eigen_summ}
%
\begin{itemize}[label=\textbullet]
\item{}For a real matrix, complex eigenvalues appear in conjugate pairs. Specifically, if \(\lambda=a+ib\) is an eigenvalue of a real matrix \(A\), then \(\overline{\lambda}=a-ib\) is also an eigenvalue of \(A\).%
\item{}For a real matrix, if a \(\vv\) is an eigenvector corresponding to \(\lambda\), then the vector \(\overline{\vv}\) obtained by taking the complex conjugate of each entry in \(\vv\) is an eigenvector corresponding to \(\overline{\lambda}\).%
\item{}The rotation-scaling matrix \(A=\left[ \begin{array}{cr} a\amp -b \\ b\amp a \end{array}  \right]\) can be written as%
\begin{equation*}
\left[ \begin{array}{cc} \sqrt{a^2+b^2} \amp 0 \\ 0\amp \sqrt{a^2+b^2} \end{array}  \right] \left[ \begin{array}{cc} \frac{a}{\sqrt{a^2+b^2}}\amp \frac{-b}{\sqrt{a^2+b^2}} \\ \frac{b}{\sqrt{a^2+b^2}}\amp \frac{a}{\sqrt{a^2+b^2}} \end{array}  \right] \,\text{.}
\end{equation*}
This decomposition geometrically means that the transformation corresponding to \(A\) can be viewed as a rotation by angle \(\theta=\arccos\left(\frac{a}{\sqrt{a^2+b^2}}\right)\) if \(b\geq 0\), or \(\theta=-\arccos\left(\frac{a}{\sqrt{a^2+b^2}}\right)\) if \(b\lt 0\), followed by a scaling by factor \(\sqrt{a^2+b^2}\).%
\item{}If \(A\) is a real \(2\times 2\) matrix with complex eigenvalue \(a-bi\) and corresponding eigenvector \(\vv=\vu+i\vw\), then \(A\) is similar to the rotation-scaling matrix \(R=\left[ \begin{array}{cr} a\amp -b \\ b\amp a \end{array}  \right]\). More specifically,%
\begin{equation*}
A=PRP^{-1} \; , \text{ where }  P=[ \vu \ \vw ]\,\text{.}
\end{equation*}
%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_comp_eigen_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp18074584}%
Find eigenvalues and eigenvectors of each of the following matrices.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\left[ \begin{array}{rc} 2\amp 4 \\ -2\amp 2 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{rc} 3\amp 2 \\ -1\amp 1 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{cr} 1\amp -2 \\ 4\amp -3 \end{array} \right]\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp18088280}%
Find a rotation-scaling matrix where the rotation angle is \(\theta=3\pi/4\) and scaling factor is less than 1.%
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp18084184}%
Determine which rotation-scaling matrices have determinant equal to 1. Be as specific as possible.%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp18083416}%
Determine the rotation-scaling matrix inside the matrix \(\left[ \begin{array}{rc} 2\amp 4 \\ -2\amp 2 \end{array} \right]\).%
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp18084568}%
Find a real \(2\times 2\) matrix with eigenvalue \(1+2i\).%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp18097240}%
Find a real \(2\times 2\) matrix which is not a rotation-scaling matrix with eigenvalue \(-1+2i\).%
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp18093784}%
We have seen how to find the characteristic polynomial of an \(n \times n\) matrix. In this exercise we consider the reverse question. That is, given a polynomial \(p(\lambda)\) of degree \(n\), can we find an \(n \times n\) matrix whose characteristic polynomial is \(p(\lambda)\)?%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the characteristic polynomial of the \(2 \times 2\) matrix \(C = \left[ \begin{array}{cc} 0\amp -a_0\\1\amp -a_1 \end{array} \right]\). Use this result to find a real valued matrix whose eigenvalues are \(1+i\) and \(1-i\).%
\item{}Repeat part (a) by showing that \(p(\lambda) = -\left(\lambda^3+a_2\lambda^2+a_1\lambda+a_0\right)\) is the characteristic polynomial of the \(3 \times 3\) matrix \(C = \left[ \begin{array}{ccc} 0\amp 0\amp -a_0\\1\amp 0\amp -a_1\\0\amp 1\amp -a_2 \end{array} \right]\).%
\item{}We can generalize this argument. Prove, using mathematical induction, that the polynomial%
\begin{equation*}
p(\lambda) =(-1)^n\left( \lambda^n + a_{n-1}\lambda^{n-1} + a_{n-2}\lambda^{n-2} + \cdots + a_1 \lambda + a_0\right)
\end{equation*}
is the characteristic polynomial of the matrix%
\begin{equation*}
C = \left[ \begin{array}{cccccc} 0\amp 0\amp 0\amp \cdots\amp 0\amp -a_{0}\\ 1\amp 0\amp 0\amp \cdots\amp 0\amp -a_{1} \\ 0\amp 1\amp 0\amp \cdots\amp 0\amp -a_2 \\ \vdots \amp  \vdots \amp \vdots\amp  \ddots \amp  \vdots   \amp  \vdots   \\ 0 \amp  0 \amp  0\amp  \cdots \amp   1 \amp  -a_{n-1} \end{array}  \right]\text{.}
\end{equation*}
The matrix \(C\) is called the \terminology{companion matrix} for \(p(\lambda)\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp18105944}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
If \(3-4i\) is an eigenvalue of a real matrix, then so is \(3+4i\).%
\item{}\lititle{True\slash{}False.}\par%
If \(2+3i\) is an eigenvalue of a \(3\times 3\) real matrix \(A\), then \(A\) has three distinct eigenvalues.%
\item{}\lititle{True\slash{}False.}\par%
Every \(2\times 2\) real matrix with complex eigenvalues is a rotation-scaling matrix.%
\item{}\lititle{True\slash{}False.}\par%
Every square matrix with real entries has real number eigenvalues.%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is a \(2\times 2\) matrix with complex eigenvalues similar to a rotation-scaling matrix \(R\), the eigenvalues of \(A\) and \(R\) are the same.%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is a real matrix with complex eigenvalues, all eigenvectors of \(A\) must be non-real.%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Understanding the Gershgorin Disk Theorem}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Understanding the Gershgorin Disk Theorem}{}{Project: Understanding the Gershgorin Disk Theorem}{}{}{x:section:sec_proj_gershgorin}
To understand the Gershgorin Disk Theorem, we need to recall how to visualize a complex number in the plane. Recall that a complex number \(z\) is a number of the form \(z = a+bi\) where \(a\) and \(b\) are real numbers and \(i^2 = -1\). The number \(a\) is the real part of \(z\), denoted as \(\text{ Re } (z)\), and \(b\) is the imaginary part of \(z\), denoted \(\text{ Im } (z)\). The set of all complex numbers is denoted \(\C\). We define addition and multiplication on \(\C\) as follows. For \(a+bi, c+di \in \C\),%
\begin{equation*}
(a+bi) + (c+di) = (a+c) + (b+d)i \ \ \ \ \ \text{ and }  \ \ \ \ \ (a+bi)(c+di) = (ac-bd) + (ad+bc)i\text{.}
\end{equation*}
%
\par
Note that the product is what we would expect if we ``expanded'' the product in the normal way and used the fact that \(i^2=-1\). The set of complex numbers forms a field \textemdash{} that is, \(\C\) satisfies all of the same properties as \(\R\) as stated in \hyperref[x:theorem:thm_1_d_reals]{Theorem~{\xreffont\ref{x:theorem:thm_1_d_reals}}}.%
\par
We can visualize the complex number \(a+bi\) in the plane as the point \((a,b)\). Here we are viewing the horizontal axis as the real axis and the vertical axis as the imaginary axis. The length (or magnitude) of the complex number \(z = a+bi\), which we denote as \(|z|\), is the distance from the origin to \(z\). So by the Pythagorean Theorem we have \(|a+bi| = \sqrt{a^2+b^2}\). Note that the magnitude of \(z = a+bi\) can be written as a complex product%
\begin{equation*}
|z| = \sqrt{(a+bi)(a-bi)}\text{.}
\end{equation*}
%
\par
The complex number \(a-bi\) is called the \terminology{complex conjugate} of \(z=a+bi\) and is denoted as \(\overline{z}\). A few important properties of real numbers and their conjugates are the following. Let \(z = a+bi\) and \(w = c+di\) be complex numbers. Then%
\begin{itemize}[label=\textbullet]
\item{}\(\overline{z+w} = \overline{(a+c) + (b+d)i} = (a+c)-(b+d)i = (a-bi) + (c-di) = \overline{z} + \overline{w}\),%
\item{}\(\overline{zw} = \overline{(ac-bd) + (ad+bc)i} = (ac-bd)-(ad+bc)i = (a-bi)(c-di) = \overline{z} \overline{w}\),%
\item{}\(\overline{\overline{z}} = z\),%
\item{}\(|z| = \sqrt{a^2+b^2} \geq \sqrt{a^2} = |a| = |\text{ Re } (z)|\),%
\item{}\(|z| = \sqrt{a^2+b^2} \geq \sqrt{b^2} = |b| = |\text{ Im } (z)|\),%
\item{}\(\left| \overline{z} \right| = |z|\),%
\item{}\(|z| = 0\) if and only if \(z = 0\),%
\item{}If \(p(x)\) is a polynomial with real coefficients and the complex number \(z\) satisfies \(p(z) = 0\), then \(p\left(\overline{z}\right) = 0\) as well.%
\end{itemize}
%
\par
Using these facts we can show that the triangle inequality is true for complex numbers. That is,%
\begin{equation*}
|z+w| \leq |z| + |w|\text{.}
\end{equation*}
%
\par
To see why, notice that%
\begin{align*}
|z+w|^2 \amp = (z+w)(\overline{z+w})\\
\amp = (z+w)(\overline{z} + \overline{w})\\
\amp = z\overline{z} + z\overline{w} + w \overline{z} + w \overline{w}\\
\amp = z\overline{z} + z\overline{w} + \overline{zw\overline{w}} + w \overline{w}\\
\amp = |z|^2 + 2\text{ Re } (z\overline{w}) + |w|^2\\
\amp \leq |z|^2 + 2|zw| + |w|^2\\
\amp = |z|^2 + 2|z| |w| + |w|^2\\
\amp = (|z|+|w|)^2\text{.}
\end{align*}
%
\par
Since \(|z+w|\), \(|z|\), and \(|w|\) are all non-negative, taking square roots of both sides gives us \(|z+w| \leq |z| + |w|\) as desired. We can extend this triangle inequality to any number of complex numbers. That is, if \(z_1\), \(z_2\), \(\ldots\), \(z_k\) are complex numbers, then%
\begin{equation}
|z_1+z_2+ \cdots + z_k| \leq |z_1| + |z_2| + \cdots + |z_k|\text{.}\label{x:men:eq_general_triangle_inequaltity}
\end{equation}
%
\par
We can prove Equation \hyperref[x:men:eq_general_triangle_inequaltity]{({\xreffont\ref{x:men:eq_general_triangle_inequaltity}})} by mathematical induction. We have already done the \(k=2\) case and so we assume that Equation \hyperref[x:men:eq_general_triangle_inequaltity]{({\xreffont\ref{x:men:eq_general_triangle_inequaltity}})} is true for any sum of \(k\) complex numbers. Now suppose that \(z_1\), \(z_2\), \(\ldots\), \(z_k\), \(z_{k+1}\) are complex numbers. Then%
\begin{align*}
|z_1+z_2+ \cdots + z_k + z_{k+1}| \amp = |(z_1+z_2+ \cdots + z_k) + z_{k+1}|\\
\amp \leq |z_1+z_2+ \cdots + z_k| + |z_{k+1}|\\
\amp \leq (|z_1| + |z_2| + \cdots + |z_k|) + |z_{k+1}|\\
\amp = |z_1| + |z_2| + \cdots + |z_k| + |z_{k+1}|\text{.}
\end{align*}
%
\par
To prove the Gershgorin Disk Theorem, we will use the Levy-Desplanques Theorem, which gives conditions that guarantee that a matrix is invertible. We illustrate with an example in the following activity.%
\begin{project}{}{x:project:act_Gershgorin_1}%
Let \(A = \left[ \begin{array}{rc} 3\amp 2 \\ -1\amp 4 \end{array}  \right]\). Since \(\det(A) \neq 0\), we know that \(A\) is an invertible matrix. Let us assume for a moment that we don't know that \(A\) is invertible and try to determine if 0 is an eigenvalue of \(A\). In other words, we want to know if there is a nonzero vector \(\vv\) so that \(A \vv = \vzero\). Assuming the existence of such a vector \(\vv = [v_1 \ v_2]^{\tr}\), for \(A \vv\) to be \(\vzero\) it must be the case that%
\begin{equation*}
3v_1 + 2v_2 = 0\ \text{ and }  \ -v_1 + 4v_2 = 0\text{.}
\end{equation*}
%
\par
Since the vector \(\vv\) is not the zero vector, at least one of \(v_1\), \(v_2\) is not zero. Note that if one of \(v_1\), \(v_2\) is zero, the so is the other. So we can assume that \(v_1\) and \(v_2\) are nonzero.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Use the fact that \(3v_1+2v_2 = 0\) to show that \(|v_2| > |v_1|\).%
\item{}Use the fact that \(-v_1 + 4v_2 = 0\) to show that \(|v_1| > |v_2|\). What conclusion can we draw about whether 0 is an eigenvalue of \(A\)? Why does this mean that \(A\) is invertible?%
\end{enumerate}
\end{project}%
What makes the arguments work in \hyperref[x:project:act_Gershgorin_1]{Project Activity~{\xreffont\ref{x:project:act_Gershgorin_1}}} is that \(|3| > |2|\) and \(|4| > |-1|\). This argument can be extended to larger matrices, as described in the following theorem.%
\begin{theorem}{Levy-Desplanques Theorem.}{}{g:theorem:idp18166744}%
Any square matrix \(A = [a_{ij}]\) satisfying \(|a_{ii}| > \sum_{j \neq i} |a_{ij}|\) for all \(i\) is invertible.%
\end{theorem}
\begin{proof}{}{g:proof:idp18170200}
Let \(A = [a_{ij}]\) be an \(n \times n\) matrix satisfying \(|a_{ii}| > \sum_{j \neq i} |a_{ij}|\) for all \(i\). Let us assume that \(A\) is not invertible, that is that there is a vector \(\vv \neq \vzero\) such that \(A \vv = \vzero\). Let \(\vv = [v_1 \ v_2 \ \cdots \ v_n]\) and \(t\) be between 1 and \(n\) so that \(|v_t| \geq |v_i|\) for all \(i\). That is, choose \(v_t\) to be the component of \(\vv\) with the largest absolute value.%
\par
Expanding the product \(A\vv\) using the row-column product along the \(t\)th row shows that%
\begin{equation*}
a_{t1}v_1 + a_{t2}v_2 + \cdots a_{tn}v_n = 0\text{.}
\end{equation*}
%
\par
Solving for the \(a_{tt}\) term gives us%
\begin{equation*}
a_{tt}v_t = -(a_{t1}v_1 + a_{t2}v_2 + \cdots a_{t(t-1)}v_{t-1}+a_{t(t+1)}v_{t+1} + \cdots + a_{tn}v_n)\text{.}
\end{equation*}
%
\par
Then%
\begin{align*}
|a_{tt}| |v_t| \amp = |-(a_{t1}v_1 + a_{t2}v_2 + \cdots a_{t(t-1)}v_{t-1}+a_{t(t+1)}v_{t+1} + \cdots + a_{tn}v_n|\\
\amp = |a_{t1}v_1 + a_{t2}v_2 + \cdots a_{t(t-1)}v_{t-1}+a_{t(t+1)}v_{t+1} + \cdots + a_{tn}v_n|\\
\amp \leq |a_{t1}| |v_1| + |a_{t2}| |v_2| + \cdots |a_{t(t-1)}| |v_{t-1}| + |a_{t(t+1)}| |v_{t+1}| + \cdots + |a_{tn}| |v_n|\\
\amp \leq |a_{t1}| |v_t| + |a_{t2}| |v_t| + \cdots |a_{t(t-1)}| |v_{t}| + |a_{t(t+1)}| |v_{t}| + \cdots + |a_{tn}| |v_t|\\
\amp =  (|a_{t1}| + |a_{t2}| + \cdots |a_{t(t-1)}|  + |a_{t(t+1)}| + \cdots + |a_{tn}|) |v_t|\text{.}
\end{align*}
%
\par
Since \(|v_t| \neq 0\), we cancel the \(|v_t|\) term to conclude that%
\begin{equation*}
|a_{tt}| \leq  |a_{t1}| + |a_{t2}| + \cdots |a_{t(t-1)}|  + |a_{t(t+1)}| + \cdots + |a_{tn}|\text{.}
\end{equation*}
%
\par
But this contradicts the condition that \(|a_{ii}| > \sum_{j \neq i} |a_{ij}|\) for all \(i\). We conclude that 0 is not an eigenvalue for \(A\) and \(A\) is invertible.%
\end{proof}
Any matrix \(A = [a_{ij}]\) satisfying the condition of the Levy-Desplanques Theorem is given a special name.%
\begin{definition}{}{g:definition:idp18184792}%
\index{strictly diagonally dominant matrix}%
A square matrix \(A = [a_{ij}]\) is \terminology{strictly diagonally dominant} if \(|a_{ii}| > \sum_{j \neq i} |a_{ij}|\) for all \(i\).%
\end{definition}
So any strictly diagonally dominant matrix is invertible. A quick glance can show that a matrix is strictly diagonally dominant. For example, since \(|3| > |1| + |-1|\), \(|12| > |5| +|6|\), and \(|-8| > |-2| + |4|\), the matrix%
\begin{equation*}
A = \left[ \begin{array}{rcr} 3\amp 1\amp -1 \\ 5\amp 12\amp 6 \\ -2\amp 4\amp -8 \end{array}  \right]
\end{equation*}
is strictly diagonally dominant and therefore invertible. However, just because a matrix is not strictly diagonally dominant, it does not follow that the matrix is non-invertible. For example, the matrix \(B = \left[ \begin{array}{cc} 1\amp 2 \\ 0\amp 1 \end{array}  \right]\) is invertible, but not strictly diagonally dominant.%
\par
Now we can address the Gershgorin Disk Theorem.%
\begin{activity}{}{x:activity:act_Gershgorin_2}%
\index{Gershgorin Disk Theorem}%
Let \(A\) be an arbitrary \(n \times n\) matrix and assume that \(\lambda\) is an eigenvalue of \(A\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why the matrix \(A - \lambda I\) is singular.%
\item{}What does the Levy-Desplanques Theorem tell us about the matrix \(A - \lambda I\)?%
\item{}Explain how we can conclude the Gershgorin Disk Theorem. \begin{theorem}{Gershgorin Disk Theorem.}{}{x:theorem:thm_Gershgorin}%
Let \(A=[a_{ij}]\) be an \(n \times n\) matrix with complex entries. Then every eigenvalue of \(A\) lies in one of the Gershgorin discs%
\begin{equation*}
\{z \in \C : |z-a_{ii}| \leq r_i\}\text{,}
\end{equation*}
where \(r_i = \sum_{j \neq i} |a_{ij}|\).%
\end{theorem}
 Based on this theorem, we define a Gershgorin disk to be \(D(a_{ii}, r_i)\), where \(r_i =  \sum_{j \neq i} |a_{ij}|\).%
\item{}Use the Gershgorin Disk Theorem to give estimates on the locations of the eigenvalues of the matrix \(A = \left[ \begin{array}{rr} -1\amp 2 \\ -3\amp 2 \end{array} \right]\).%
\end{enumerate}
\end{activity}%
The Gershgorin Disk Theorem has a consequence that gives additional information about the eigenvalues if some of the Gershgorin disks do not overlap.%
\begin{theorem}{}{}{x:theorem:thm_Gersgorin_consequence}%
If \(S\) is a union of \(m\) Gershgorin disks of a matrix \(A\) such that \(S\) does not intersect any other Gershgorin disk, then \(S\) contains exactly \(m\) eigenvalues (counting multiplicities) of \(A\).%
\end{theorem}
\begin{proof}{}{g:proof:idp18203480}
Most proofs of this theorem require some results from topology. For that reason, we will not present a completely rigorous proof but rather give the highlights. Let \(A = [a_{ij}]\) be an \(n \times n\) matrix. Let \(D_i\) be a collection of Gershgorin disks of \(A\) for \(1 \leq i \leq m\) such that \(S = \cup_{1 \leq i \leq m} D_i\) does not intersect any other Gershgorin disk of \(A\), and let \(S'\) be the union of the Gershgorin disks of \(A\) that are different from the \(D_i\). Note that \(S \cap S' = \emptyset\). Let \(C\) be the matrix whose \(i\)th column is \(a_{ii}\ve_i\), that is \(C\) is the diagonal matrix whose diagonal entries are the corresponding diagonal entries of \(A\). Note that the eigenvalues of \(C\) are \(a_{ii}\) and the Gershgorin disks of \(C\) are just the points \(a_{ii}\). So our theorem is true for this matrix \(C\). To prove the result, we build a continuum of matrices from \(C\) to \(A\) as follows: let \(B = A-C\) (so that \(B\) is the matrix whose off-diagonal entries are those of \(A\) and whose diagonal entries are 0), and let \(A(t) = tB + C\) for \(t\) in the interval \([0,1]\). Note that \(A(1) = A\). Since the diagonal entries of \(A(t)\) are the same as those of \(A\), the Gershgorin disks of \(A(t)\) have the same centers as the corresponding Gershgorin disks of \(A\), while the radii of the Gershgorin disks of \(A(t)\) are those of \(A\) but scaled by \(t\). So the Gershgorin disks of \(A(t)\) increase from points (the \(a_{ii}\)) to the Gershgorin disks of \(A\) as \(t\) increases from 0 to 1. While the centers of the disks all remain fixed, it is important to recognize that the eigenvalues of \(A(t)\) move as \(t\) changes. An illustration of this is shown in \hyperref[x:figure:F_Gershgorin_2]{Figure~{\xreffont\ref{x:figure:F_Gershgorin_2}}} with the eigenvalues as the black points and the changing Gershgorin disks dashed in magenta, using the matrix \(\left[ \begin{array}{cc} i\amp \frac{1}{2} \\ 1\amp -2+i \end{array} \right]\). We can learn about how the eigenvalues move with the characteristic polynomial.%
\begin{figureptx}{How eigenvalues move.}{x:figure:F_Gershgorin_2}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/Gershgorin_2.pdf}
\end{image}%
\tcblower
\end{figureptx}%
Let \(p(t,x)\) be the characteristic polynomial of \(A(t)\). Note that these characteristic polynomials are functions of both \(t\) and \(x\). Since polynomials are continuous functions, their roots (the eigenvalues of \(A(t)\)) are continuous for \(t \in [0,1]\) as well. Let \(\lambda(t)\) be an eigenvalue of \(A(t)\). Note that \(\lambda(1)\) is an eigenvalue of \(A\), and \(\lambda(0)\) is one of the \(a_{ii}\) and is therefore in \(S\). We will argue that \(\lambda(t)\) is in \(S\) for every value of \(t\) in \([0,1]\). Let \(r_i\) be the radius of \(D_i\) and let \(D(t)_i\) be the Gershgorin disk of \(A(t)\) with the same center as \(D_i\) and radius \(r(t)_i = tr_i\). Let \(S(t) = \cup_{1 \leq i \leq m} D(s)_i\). Since \(r(s)_i \leq r_i\), it follows that \(D(s)_i \subseteq D_i\) and so \(S(t) \cap S' = \emptyset\) as well. From topology, we know that since the disks \(D_i\) are closed, the union \(S\) of these disks is also closed. Similarly, \(S(t)\) and \(S'\) are closed. Thus, \(\lambda(t)\) is continuous in a closed set and so does not leave the set. Thus, \(\lambda(t)\) is in \(S\) for every value of \(t\) in \([0,1]\).%
\end{proof}
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 22 Properties of Determinants}
\typeout{************************************************}
%
\begin{chapterptx}{Properties of Determinants}{}{Properties of Determinants}{}{}{x:chapter:chap_det_properties}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp18229848}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}How do elementary row operations change the determinant?%
\item{}How can we represent elementary row operations via matrix multiplication?%
\item{}How can we use elementary row operations to calculate the determinant more efficiently?%
\item{}What is the Cramer's rule for the explicit formula for the inverse of a matrix?%
\item{}How can we interpret determinants from a geometric perspective?%
\item{}What is an \(LU\) factorization of a matrix and why is such a factorization useful?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_det_prop_intro}
This section is different than others in that it contains mainly proofs of previously stated results and only a little new material. Consequently, there is no application attached to this section.%
\par
We have seen that an important property of the determinant is that it provides an easy criteria for the invertibility of a matrix. As a result, we obtained an algebraic method for finding the eigenvalues of a matrix, using the characteristic equation. In this section, we will investigate other properties of the determinant related to how elementary row operations change the determinant. These properties of the determinant will help us evaluate the determinant in a more efficient way compared to using the cofactor expansion method, which is computationally intensive for large \(n\) values due to it being a recursive method. Finally, we will derive a geometrical interpretation of the determinant.%
\begin{exploration}{}{x:exploration:pa_4_f}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}We first consider how the determinant changes if we multiply a row of the matrix by a constant.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Let \(A = \left[ \begin{array}{cc} 2\amp 3 \\ 1\amp 4 \end{array} \right]\). Pick a few different values for the constant \(k\) and compare the determinant of \(A\) and that of \(\left[ \begin{array}{cc} 2k\amp 3k \\ 1\amp 4 \end{array} \right]\). What do you conjecture that the effect of multiplying a row by a constant on the determinant is?%
\item{}If we want to make sure our conjecture is valid for any \(2\times 2\) matrix, we need to show that for \(A = \left[ \begin{array}{cc} a\amp b\\c\amp d \end{array} \right]\), the relationship between \(\det(A)\) and the determinant of \(\left[ \begin{array}{cc} a\cdot k\amp b\cdot k\\c\amp d \end{array} \right]\) follows our conjecture. We should also check that the relationship between \(\det(A)\) and the determinant of \(\left[ \begin{array}{cc} a\amp b\\c\cdot k\amp d\cdot k \end{array} \right]\) follows our conjecture. Verify this.%
\item{}Make a similar conjecture for what happens to the determinant when a row of a \(3\times 3\) matrix \(A\) is multiplied by a constant \(k\), and explain why your conjecture is true using the cofactor expansion definition of the determinant.%
\end{enumerate}
\item{}The second type of elementary row operation we consider is row swapping.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Take a general \(2\times 2\) matrix \(A = \left[ \begin{array}{cc} a\amp b\\c\amp d \end{array} \right]\) and determine how row swapping effects the determinant.%
\item{}Now choose a few different \(3\times 3\) matrices and see how row swapping changes the determinant in these matrices by evaluating the determinant with a calculator or any other appropriate technology.%
\item{}Based on your results so far, conjecture how row swapping changes the determinant in general.%
\end{enumerate}
\item{}The last type of elementary row operation is adding a multiple of a row to another. Determine the effect of this operation on a \(2\times 2\) matrix by evaluating the determinant of a general \(2\times 2\) matrix after a multiple of one row is added to the other row.%
\item{}All of the elementary row operations we discussed above can be achieved by matrix multiplication with \terminology{elementary matrices}. For each of the following elementary matrices, determine what elementary operation it corresponds to by calculating the product \(EA\), where \(A = \left[ \begin{array}{ccc} a_{11}\amp a_{12}\amp a_{13}\\a_{21}\amp a_{22}\amp a_{23}\\a_{31}\amp a_{32}\amp a_{33} \end{array} \right]\) is a general \(3\times 3\) matrix.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}\(E = \left[ \begin{array}{ccc} 0\amp 1\amp 0\\ 1\amp 0\amp 0 \\ 0\amp 0\amp 1 \end{array} \right]\)%
\item{}\(E = \left[ \begin{array}{ccc} 1\amp 0\amp 0\\ 0\amp 3\amp 0 \\ 0\amp 0\amp 1 \end{array} \right]\)%
\item{}\(E = \left[ \begin{array}{ccc} 1\amp 0\amp 0\\ 0\amp 1\amp 2 \\ 0\amp 0\amp 1 \end{array} \right]\)%
\end{enumerate}
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Elementary Row Operations and Their Effects on the Determinant}
\typeout{************************************************}
%
\begin{sectionptx}{Elementary Row Operations and Their Effects on the Determinant}{}{Elementary Row Operations and Their Effects on the Determinant}{}{}{x:section:sec_det_row_ops}
In \hyperref[x:exploration:pa_4_f]{Preview Activity~{\xreffont\ref{x:exploration:pa_4_f}}}, we conjectured how elementary row operations affect the determinant of a matrix. In the following activity, we prove how the determinant changes when a row is multiplied by a constant using the cofactor expansion definition of the determinant.%
\begin{activity}{}{x:activity:act_4_f_1}%
In this activity, assume that the determinant of \(A\) can be determined by a cofactor expansion along any row or column. (We will prove this result independently later in this section.) Consider an arbitrary \(n \times n\) matrix \(A = [a_{ij}]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Write the expression for \(\det(A)\) using the cofactor expansion along the second row.%
\item{}Let \(B\) be obtained by multiplying the second row of \(A\) by \(k\). Write the expression for \(\det(B)\) if the cofactor expansion along the second row is used.%
\item{}Use the expressions you found above, to express \(\det(B)\) in terms of \(\det(A)\).%
\item{}Explain how this method generalizes to prove the relationship between the determinant of a matrix \(A\) and that of the matrix obtained by multiplying a row by a constant \(k\).%
\end{enumerate}
\end{activity}%
Your work in \hyperref[x:activity:act_4_f_1]{Activity~{\xreffont\ref{x:activity:act_4_f_1}}} proves the first part of the following theorem on how elementary row operations change the determinant of a matrix.%
\begin{theorem}{}{}{x:theorem:thm_4_f_1}%
Let \(A\) be a square matrix.%
\begin{enumerate}
\item{}If \(B\) is obtained by multiplying a row of \(A\) by a constant \(k\), then \(\det(B)=k\det(A)\).%
\item{}If \(B\) is obtained by swapping two rows of \(A\), then \(\det(B)=-\det(A)\).%
\item{}If \(B\) is obtained by adding a multiple of a row of \(A\) to another, then \(\det(B)=\det(A)\).%
\end{enumerate}
%
\end{theorem}
In the next section, we will use elementary matrices to prove the last two properties of \hyperref[x:theorem:thm_4_f_1]{Theorem~{\xreffont\ref{x:theorem:thm_4_f_1}}}.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Elementary Matrices}
\typeout{************************************************}
%
\begin{sectionptx}{Elementary Matrices}{}{Elementary Matrices}{}{}{x:section:sec_mtx_elem}
As we saw in \hyperref[x:exploration:pa_4_f]{Preview Activity~{\xreffont\ref{x:exploration:pa_4_f}}}, elementary row operations can be achieved by multiplication by \terminology{elementary matrices}.%
\begin{definition}{}{g:definition:idp18301592}%
\index{matrix!elementary}%
An \terminology{elementary matrix} is a matrix obtained by performing a single elementary row operation on an identity matrix.%
\end{definition}
The following elementary matrices correspond, respectively, to an elementary row operation which swaps rows 2 and 4; an elementary row operation which multiplies the third row by 5; and an elementary row operation which adds four times the third row to the first row on any \(4\times 4\) matrix:%
\begin{equation*}
E_1 = \left[ \begin{array}{cccc} 1\amp 0\amp 0\amp 0\\0\amp 0\amp 0\amp 1\\0\amp 0\amp 1\amp 0\\ 0\amp 1\amp 0\amp 0 \end{array}  \right], \ \ E_2 = \left[ \begin{array}{cccc} 1\amp 0\amp 0\amp 0\\0\amp 1\amp 0\amp 0\\0\amp 0\amp 5\amp 0 \\ 0\amp 0\amp 0\amp 1 \end{array}  \right], \ \ \text{ and }  \ \ E_3 = \left[ \begin{array}{cccc} 1\amp 0\amp 4\amp 0\\0\amp 1\amp 0\amp 0\\0\amp 0\amp 1\amp 0\\0\amp 0\amp 0\amp 1 \end{array}  \right]\,\text{.}
\end{equation*}
%
\par
To obtain an elementary matrix corresponding an elementary row operation, we simply perform the elementary row operation on the identity matrix. For example, \(E_1\) above is obtained by swapping rows 2 and 4 of the identity matrix.%
\par
With the use of elementary matrices, we can now prove the result about how the determinant is affected by elementary row operations. We first rewrite \hyperref[x:theorem:thm_4_f_1]{Theorem~{\xreffont\ref{x:theorem:thm_4_f_1}}} in terms of elementary matrices:%
\begin{theorem}{}{}{x:theorem:thm_4_f_2}%
Let \(A\) be an \(n\times n\) matrix. If \(E\) is an \(n\times n\) elementary matrix, then \(\det(EA)=\det(E)\det(A)\) where%
\begin{equation*}
\det(E) = \left\{ \begin{array}{rl} r \amp  \text{ if \(E\) corresponds to multiplying a row by \(r\) }  \\ -1\amp  \text{ if \(E\) corresponds to swapping two rows }  \\ 1 \amp  \text{ if \(E\) corresponds to adding a multiple of a row to another. } \end{array}  \right.
\end{equation*}
%
\end{theorem}
\begin{paragraphs}{Notes on Theorem~{\xreffont\ref*{x:theorem:thm_4_f_2}}.}{g:paragraphs:idp18295960}%
An elementary matrix \(E\) obtained by multiplying a row by \(r\) is a diagonal matrix with one \(r\) along the diagonal and the rest 1s, so \(\det(E) = r\). Similarly, an elementary matrix \(E\) obtained by adding a multiple of a row to another is a triangular matrix with 1s along the diagonal, so \(\det(E) = 1\). The fact that the the determinant of an elementary matrix obtained by swapping two rows is \(-1\) is a bit more complicated and is verified independently later in this section. Also, the proof of \hyperref[x:theorem:thm_4_f_2]{Theorem~{\xreffont\ref{x:theorem:thm_4_f_2}}} depends on the fact that the cofactor expansion of a matrix is the same along any two rows. A proof of this can also be found later in this section.%
\end{paragraphs}%
\begin{proof}{Proof of Theorem~{\xreffont\ref*{x:theorem:thm_4_f_2}}.}{g:proof:idp18304024}
We will prove the result by induction on \(n\), the size of the matrix \(A\). We verified these results in \hyperref[x:exploration:pa_4_f]{Preview Activity~{\xreffont\ref{x:exploration:pa_4_f}}} for \(n=2\) using elementary row operations. The elementary matrix versions follow immediately.%
\par
Now assume the theorem is true for \(k\times k\) matrices with \(k\geq 2\) and consider an \(n\times n\) matrix \(A\) where \(n=k+1\). If \(E\) is an \(n\times n\) elementary matrix, we want to show that \(\det(EA)=\det(E)\det(A)\). Let \(EA=B\). (Although it is an abuse of language, we will refer to both the elementary matrix and the elementary row operation corresponding to it by \(E\).)%
\par
When finding \(\det(B)=\det(EA)\) we will use a cofactor expansion along a row which is not affected by the elementary row operation \(E\). Since \(E\) affects at most two rows and \(A\) has \(n\geq 3\) rows, it is possible to find such a row, say row \(i\). The cofactor expansion along row \(i\) of \(B\) is%
\begin{equation}
b_{i1} (-1)^{i+1} \det(B_{i1}) + b_{i2} (-1)^{i+2} \det(B_{i2}) + \cdots + b_{in} (-1)^{i+n} \det(B_{in}) \,\text{.}\label{x:men:eq_4_f_1}
\end{equation}
%
\par
Since we chose a row of \(A\) which was not affected by the elementary row operation, it follows that \(b_{ij}=a_{ij}\) for \(1\leq j\leq n\). Also, the matrix \(B_{ij}\) obtained by removing row \(i\) and column \(j\) from matrix \(B=EA\) can be obtained from \(A_{ij}\) by an elementary row operation of the same type as \(E\). Hence there is an elementary matrix \(E_k\) of the same type as \(E\) with \(B_{ij}=E_k A_{ij}\). Therefore, by induction, \(\det(B_{ij})=\det(E_k)\det(A_{ij})\) and \(\det(E_k)\) is equal to 1, -1 or \(r\) depending on the type of elementary row operation. If we substitute this information into equation \hyperref[x:men:eq_4_f_1]{({\xreffont\ref{x:men:eq_4_f_1}})}, we obtain%
\begin{equation*}
\begin{split} \det(B)\amp = a_{i1} (-1)^{i+1} \det(E_k) \det(A_{i1}) + a_{i2} (-1)^{i+2} \det(E_k) \det(A_{i2}) \\ \amp \qquad + \cdots + a_{in} (-1)^{i+n} \det(E_k) \det(A_{in})\\ \amp = \det(E_k) \det(A) \, . \end{split}
\end{equation*}
%
\par
This equation proves \(\det(EA)=\det(E_k)\det(A)\) for any \(n\times n\) matrix \(A\) where \(E_k\) is the corresponding elementary row operation on the \(k\times k\) matrices obtained in the cofactor expansion.%
\par
The proof of the inductive step will be finished if we show that \(\det(E_k)=\det(E)\). This equality follows if we let \(A=I_n\) in \(\det(EA)=\det(E_k)\det(A)\). Therefore, \(\det(E)\) is equal to \(r\), or 1, or \(-1\), depending on the type of the elementary row operation \(E\) since the same is true of \(\det(E_k)\) by inductive hypothesis.%
\par
Therefore, by the principle of induction, the claim is true for every \(n\geq 2\).%
\end{proof}
As a corollary of this theorem, we can prove the multiplicativity of determinants:%
\begin{theorem}{}{}{x:theorem:thm_determinant_product}%
Let \(A\) and \(B\) be \(n\times n\) matrices. Then%
\begin{equation*}
\det(AB)=\det(A)\det(B) \,\text{.}
\end{equation*}
%
\end{theorem}
\begin{proof}{}{g:proof:idp18335128}
If \(A\) is non-invertible, then \(AB\) is also non-invertible and both \(\det(A)\) and \(\det(AB)\) are 0, proving the equality in this case.%
\par
Suppose now that \(A\) is invertible. By the Invertible Matrix Theorem, we know that \(A\) is row equivalent to \(I_n\). Expressed in terms of elementary matrices, this means that there are elementary matrices \(E_1, E_2, \ldots, E_\ell\) such that%
\begin{equation}
A= E_1 E_2 \cdots E_\ell I_n = E_1 E_2 \cdots E_\ell \,\text{.}\label{x:men:eq_4_f_2}
\end{equation}
%
\par
Therefore, repeatedly applying \hyperref[x:theorem:thm_4_f_2]{Theorem~{\xreffont\ref{x:theorem:thm_4_f_2}}}, we find that%
\begin{equation}
\det(A) = \det(E_1) \det(E_2) \cdots \det(E_\ell) \,\text{.}\label{x:men:eq_4_f_3}
\end{equation}
%
\par
If we multiply equation \hyperref[x:men:eq_4_f_2]{({\xreffont\ref{x:men:eq_4_f_2}})} by \(B\) on the right, we obtain%
\begin{equation*}
AB = E_1 E_2 \cdots E_\ell B \,\text{.}
\end{equation*}
%
\par
Again, by repeatedly applying \hyperref[x:theorem:thm_4_f_2]{Theorem~{\xreffont\ref{x:theorem:thm_4_f_2}}} with this product of matrices, we find%
\begin{equation*}
\det(AB) = \det(E_1 E_2 \cdots E_\ell B ) = \det(E_1) \det(E_2) \cdots \det(E_\ell) \det(B) \,\text{.}
\end{equation*}
%
\par
From equation \hyperref[x:men:eq_4_f_3]{({\xreffont\ref{x:men:eq_4_f_3}})}, the product of \(\det(E_i)\)'s equals \(\det(A)\), so%
\begin{equation*}
\det(AB) = \det(A) \det(B)
\end{equation*}
which finishes the proof of the theorem.%
\end{proof}
We can use the multiplicative property of the determinant and the determinants of elementary matrices to calculate the determinant of a matrix in a more efficient way than using the cofactor expansion. The next activity provides an example.%
\begin{activity}{}{x:activity:act_4_f_2}%
Let \(A=\left[ \begin{array}{rcc} 1\amp 1\amp 2\\ 2\amp 2\amp 6\\ -1\amp 2\amp 1 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Use elementary row operations to reduce \(A\) to a row echelon form. Keep track of the elementary row operation you use.%
\item{}Taking into account how elementary row operations affect the determinant, use the row echelon form of \(A\) to calculate \(\det(A)\).%
\end{enumerate}
\end{activity}%
Your work in \hyperref[x:activity:act_4_f_2]{Activity~{\xreffont\ref{x:activity:act_4_f_2}}} provides an efficient method for calculating the determinant. If \(A\) is a square matrix, we use row operations given by elementary matrices \(E_1\), \(E_2\), \(\ldots\), \(E_k\) to row reduce \(A\) to row echelon form \(R\). That is%
\begin{equation*}
R = E_kE_{k-1} \cdots E_2E_1A\text{.}
\end{equation*}
%
\par
We know \(\det(E_i)\) for each \(i\), and since \(R\) is a triangular matrix we can find its determinant. Then%
\begin{equation*}
\det(A) = \det(E_1)^{-1}\det(E_2)^{-1} \cdots \det(E_2)^{-1}\det(R)\text{.}
\end{equation*}
%
\par
In other words, if we keep track of how the row operations affect the determinant, we can calculate the determinant of a matrix \(A\) using row operations.%
\begin{activity}{}{x:activity:act_4_f_2_b}%
\hyperref[x:theorem:thm_4_f_2]{Theorem~{\xreffont\ref{x:theorem:thm_4_f_2}}} and \hyperref[x:theorem:thm_determinant_product]{Theorem~{\xreffont\ref{x:theorem:thm_determinant_product}}} can be used to prove the following (part c of \hyperref[x:theorem:thm_determinant_properties]{Theorem~{\xreffont\ref{x:theorem:thm_determinant_properties}}}) that \(A\) is invertible if and only if \(\det(A) \neq 0\). We see how in this activity. Let \(A\) be an \(n \times n\) matrix. We can row reduce \(A\) to its reduced row echelon form \(R\) by elementary matrices \(E_1\), \(E_2\), \(\ldots\), \(E_k\) so that%
\begin{equation*}
R = E_1E_2 \cdots E_kA\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Suppose \(A\) is invertible. What, then, is \(R\)? What is \(\det(R)\)? Can the determinant of an elementary matrix ever be \(0\)? How do we conclude that \(\det(A) \neq 0\)?%
\item{}Now suppose that \(\det(A) \neq 0\). What can we conclude about \(\det(R)\)? What, then, must \(R\) be? How do we conclude that \(A\) is invertible?%
\end{enumerate}
\end{activity}%
\begin{paragraphs}{Summary.}{g:paragraphs:idp18366744}%
Let \(A\) be an \(n\times n\) matrix. Suppose we swap rows \(s\) times and divide rows by constants \(k_1, k_2, \ldots,
k_r\) while computing a row echelon form \(\text{ REF } (A)\) of \(A\). Then \(\det(A)=(-1)^s k_1 k_2\cdots k_r \det(\text{ REF } (A))\).%
\end{paragraphs}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Geometric Interpretation of the Determinant}
\typeout{************************************************}
%
\begin{sectionptx}{Geometric Interpretation of the Determinant}{}{Geometric Interpretation of the Determinant}{}{}{x:section:sec_det_geom}
Determinants have interesting and useful applications from a geometric perspective. To understand the geometric interpretation of the determinant of an \(n\times n\) matrix \(A\), we consider the image of the unit square under the transformation \(T(\vx)=A\vx\) and see how its area changes based on \(A\).%
\begin{activity}{}{x:activity:act_4_f_3}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(A = \left[ \begin{array}{cc} 2\amp 0\\0\amp 3 \end{array} \right]\). Start with the unit square in \(\R^2\) with corners at the origin and at \((1,1)\). In other words, the unit square we are considering consists of all vectors \(\vv=\left[ \begin{array}{c} x\\y \end{array} \right]\) where \(0\leq x\leq 1\) and \(0\leq y\leq 1\), visualized as points in the plane.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Consider the collection of image vectors \(A\vv\) obtained by multiplying \(\vv\)'s by \(A\). Sketch the rectangle formed by these image vectors.%
\item{}Explain how the area of this image rectangle and the unit square is related via \(\det(A)\).%
\item{}Does the relationship you found above generalize to an arbitrary \(A = \left[ \begin{array}{cc} a\amp 0\\0\amp b \end{array} \right]\)? If not, modify the relationship to hold for all diagonal matrices.%
\end{enumerate}
\item{}Let \(A=\left[ \begin{array}{cc} 2\amp 1\\ 0\amp 3 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Sketch the image of the unit square under the transformation \(T(\vv)=A\vv\). To make the sketching easier, find the images of the vectors \([0 \ 0]^{\tr}, [1 \ 0]^{\tr}, [0 \ 1]^{\tr}, [1 \ 1]^{\tr}\) as points first and then connect these images to find the image of the unit square.%
\item{}Check that the area of the parallelogram you obtained in the above part is equal to \(\det(A)\).%
\item{}Does the relationship between the area and \(\det(A)\) still hold if \(A=\left[ \begin{array}{rc} -2\amp 1\\ 0\amp 3 \end{array} \right]\)? If not, how will you modify the relationship?%
\end{enumerate}
\end{enumerate}
\end{activity}%
It can be shown that for all \(2\times 2\) matrices a similar relationship holds.%
\begin{theorem}{}{}{g:theorem:idp18378008}%
For a \(2\times 2\) matrix \(A\), the area of the image of the unit square under the transformation \(T(\vx)=A\vx\) is equal to \(|\det(A)|\). This is equivalent to saying that \(|\det(A)|\) is equal to the area of the parallelogram defined by the columns of \(A\). The area of the parallelogram is also equal to the lengths of the column vectors of \(A\) multiplied by \(|\sin(\theta)|\) where \(\theta\) is the angle between the two column vectors.%
\end{theorem}
There is a similar geometric interpretation of the determinant of a \(3\times 3\) matrix in terms of volume.%
\begin{theorem}{}{}{g:theorem:idp18389400}%
For a \(3\times 3\) matrix \(A\), the volume of the image of the unit cube under the transformation \(T(\vx)=A\vx\) is equal to \(|\det(A)|\). This is equivalent to saying that \(|\det(A)|\) is equal to the volume of the parallelepiped defined by the columns of \(A\).%
\end{theorem}
The sign of \(\det(A)\) can be interpreted in terms of the orientation of the column vectors of \(A\). See the project in \hyperref[x:chapter:chap_determinants]{Section~{\xreffont\ref{x:chapter:chap_determinants}}} for details.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  An Explicit Formula for the Inverse and Cramer's Rule}
\typeout{************************************************}
%
\begin{sectionptx}{An Explicit Formula for the Inverse and Cramer's Rule}{}{An Explicit Formula for the Inverse and Cramer's Rule}{}{}{x:section:sec_inv_cramers}
In \hyperref[x:chapter:chap_matrix_inverse]{Section~{\xreffont\ref{x:chapter:chap_matrix_inverse}}} we found the inverse \(A^{-1}\) using row reduction of the matrix obtained by augmenting \(A\) with \(I_n\). However, in theoretical applications, having an explicit formula for \(A^{-1}\) can be handy. Such an explicit formula provides us with an algebraic expression for \(A^{-1}\) in terms of the entries of \(A\). A consequence of the formula we develop is Cramer's Rule, which can be used to provide formulas that give solutions to certain linear systems.%
\par
We begin with an interesting connection between a square matrix and the matrix of its cofactors that we explore in the next activity.%
\begin{activity}{}{x:activity:act_4_f_4}%
Let \(A = \left[ \begin{array}{crc} 2\amp 1\amp 3 \\ 1\amp 4\amp 5 \\ 2\amp -1\amp 2 \end{array}  \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Calculate the \((1,1)\), \((1,2)\), and \((1,3)\) cofactors of \(A\).%
\item{}If \(C_{ij}\) represents the \((i,j)\) cofactor of \(A\), then the cofactor matrix \(C\) is the matrix \(C = [C_{ij}]\). The \terminology{adjugate} matrix of \(A\) is the transpose of the cofactor matrix. In our example, the adjugate matrix of \(A\) is%
\begin{equation*}
\adj(A) = \left[ \begin{array}{rrr} 13\amp -5\amp -7 \\ 8\amp -2\amp -7 \\ -9\amp 4\amp 7 \end{array}  \right]\text{.}
\end{equation*}
Check the entries of this adjugate matrix with your calculations from part (a). Then calculate the matrix product%
\begin{equation*}
A \ \adj(A)\text{.}
\end{equation*}
%
\item{}What do you notice about the product \(A \ \adj(A)\)? How is this product related to \(\det(A)\)?%
\end{enumerate}
\end{activity}%
The result of \hyperref[x:activity:act_4_f_4]{Activity~{\xreffont\ref{x:activity:act_4_f_4}}} is rather surprising, but it is valid in general. That is, if \(A = [a_{ij}]\) is an invertible \(n \times n\) matrix and \(C_{ij}\) is the \((i,j)\) cofactor of \(A\), then \(A \ \adj(A)=\det(A) I_n\). In other words, \(A \left(\frac{\adj(A)}{\det(A)}\right) = I_n\) and so%
\begin{equation*}
A^{-1} = \frac{1}{\det(A)} \adj(A)\text{.}
\end{equation*}
%
\par
This gives us another formulation of the inverse of a matrix. To see why \(A \ \adj(A) = \det(A) I_n\), we use the row-column version of the matrix product to find the \(ij\)th entry of \(A \ \adj(A)\) as indicated by the shaded row and column%
\begin{equation*}
\left[ \begin{array}{cccc} a_{11}     \amp  a_{12}   \amp  \cdots   \amp  a_{1n} \\ a_{21}     \amp a_{22}    \amp  \cdots  \amp  a_{2n} \\ \vdots     \amp  \vdots      \amp            \amp \vdots  \\ \cellcolor[gray]{.9}  a_{i1}  \amp \cellcolor[gray]{.9} a_{i2}     \amp \cellcolor[gray]{.9} \cdots   \amp \cellcolor[gray]{.9} a_{in} \\ \vdots     \amp  \vdots      \amp            \amp \vdots  \\ a_{n1}     \amp  a_{n2}   \amp  \cdots   \amp a_{nn} \end{array}  \right] \left[ \begin{array}{cccccc} C_{11}   \amp  C_{21} \amp  \cdots    \amp \cellcolor[gray]{.9} C_{j1} \amp \cdots \amp  C_{n1} \\ C_{12}   \amp  C_{22} \amp  \cdots    \amp \cellcolor[gray]{.9} C_{j2} \amp \cdots \amp  C_{n2} \\ \vdots   \amp   \vdots  \amp          \amp            \amp        \amp  \vdots \\ C_{1n}   \amp  C_{2n} \amp  \cdots  \amp \cellcolor[gray]{.9} C_{jn} \amp \cdots \amp  C_{nn} \end{array}  \right]\text{.}
\end{equation*}
%
\par
Thus the \(ij\)th entry of \(A \ \adj(A)\) is%
\begin{equation}
a_{i1}C_{j1} + a_{i2}C_{j2} + \cdots + a_{in}C_{jn}\text{.}\label{x:men:eq_4_f_4}
\end{equation}
%
\par
Notice that if \(i=j\), then expression \hyperref[x:men:eq_4_f_4]{({\xreffont\ref{x:men:eq_4_f_4}})} is the cofactor expansion of \(A\) along the \(i\)th row. So the \(ii\)th entry of \(A \ \adj(A)\) is \(\det(A)\). It remains to show that the \(ij\)th entry of \(A \ \adj(A)\) is 0 when \(i \neq j\).%
\par
When \(i \neq j\), the expression \hyperref[x:men:eq_4_f_4]{({\xreffont\ref{x:men:eq_4_f_4}})} is the cofactor expansion of the matrix%
\begin{equation*}
\left[ \begin{array}{cccc} a_{11}     \amp  a_{12}   \amp  \cdots   \amp  a_{1n} \\ a_{21}     \amp a_{22}    \amp  \cdots  \amp  a_{2n} \\ \vdots     \amp  \vdots      \amp            \amp \vdots  \\ a_{i1}      \amp a_{i2}     \amp  \cdots   \amp  a_{in} \\ \vdots     \amp  \vdots      \amp            \amp \vdots  \\ a_{j-11}      \amp a_{j-12}     \amp  \cdots   \amp  a_{j-1n} \\ \cellcolor[gray]{.9} a_{i1} \amp \cellcolor[gray]{.9}  a_{i2} \amp \cellcolor[gray]{.9} \cdots   \amp \cellcolor[gray]{.9} a_{in} \\ a_{j+11}      \amp a_{i+12}     \amp  \cdots   \amp  a_{j+1n} \\ \vdots     \amp  \vdots      \amp            \amp \vdots  \\ a_{n1}     \amp  a_{n2}   \amp  \cdots   \amp a_{nn} \end{array}  \right]
\end{equation*}
along the \(j\)th row. This matrix is the one obtained by replacing the \(j\)th row of \(A\) with the \(i\)th row of \(A\). Since this matrix has two identical rows, it is not row equivalent to the identity matrix and is therefore not invertible. Thus, when \(i \neq j\) expression \hyperref[x:men:eq_4_f_4]{({\xreffont\ref{x:men:eq_4_f_4}})} is 0. This makes \(A \ \adj(A) = \det(A) I_n\).%
\par
One consequence of the formula \(A^{-1} = \frac{1}{\det(A)} \adj(A)\) is Cramer's rule, which describes the solution to the equation \(A \vx = \vb\).%
\begin{activity}{}{x:activity:act_4_f_5}%
Let \(A = \left[ \begin{array}{cc} 3\amp 1 \\ 4\amp 2 \end{array} \right]\), and let \(\vb = \left[ \begin{array}{c}2\\6 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Solve the equation \(A \vx = \vb\) using the inverse of \(A\).%
\item{}Let \(A_1 = \left[ \begin{array}{cc} 2\amp 1 \\ 6\amp 2 \end{array} \right]\), the matrix obtained by replacing the first column of \(A\) with \(\vb\). Calculate \(\frac{\det(A_1)}{\det(A)}\) and compare to your solution from part (a). What do you notice?%
\item{}Now let \(A_2 = \left[ \begin{array}{cc} 3\amp 2 \\ 4\amp 6 \end{array} \right]\), the matrix obtained by replacing the second column of \(A\) with \(\vb\). Calculate \(\frac{\det(A_2)}{\det(A)}\) and compare to your solution from part (a). What do you notice?%
\end{enumerate}
\end{activity}%
The result from \hyperref[x:activity:act_4_f_5]{Activity~{\xreffont\ref{x:activity:act_4_f_5}}} may seem a bit strange, but turns out to be true in general. The result is called \emph{Cramer's Rule}.%
\begin{theorem}{Cramer's Rule.}{}{g:theorem:idp18427928}%
Let \(A\) be an \(n\times n\) invertible matrix. For any \(\vb\) in \(\R^n\), the solution \(\vx\) of \(A\vx=\vb\) has entries%
\begin{equation*}
x_i =\frac{\det(A_i)}{\det(A)}
\end{equation*}
where \(A_i\) represents the matrix formed by replacing \(i\)th column of \(A\) with \(\vb\). \index{Cramer's Rule}%
\end{theorem}
To see why Cramer's Rule works in general, let \(A\) be an \(n \times n\) invertible matrix and \(\vb = [b_1 \ b_2 \ \cdots \ b_n]^{\tr}\). The solution to \(A \vx = \vb\) is%
\begin{equation*}
\vx = A^{-1} \vb = \frac{1}{\det(A)} \adj(A) \vb = \frac{1}{\det(A)}\left[ \begin{array}{cccc} C_{11}   \amp  C_{21} \amp \cdots \amp  C_{n1} \\ C_{12}   \amp  C_{22} \amp \cdots \amp  C_{n2} \\ \vdots   \amp   \vdots  \amp        \amp  \vdots \\ C_{1n}   \amp  C_{2n} \amp \cdots \amp  C_{nn} \end{array}  \right] \left[ \begin{array}{c} b_1 \\ b_2 \\ \vdots \\ b_n \end{array}  \right]\text{.}
\end{equation*}
%
\par
Expanding the product gives us%
\begin{equation*}
\vx = \frac{1}{\det(A)}\left[ \begin{array}{c} b_1C_{11} + b_2C_{21} + \cdots + b_nC_{n1}  \\ b_1C_{12} + b_2C_{22} + \cdots + b_nC_{n2}   \\ \vdots \\ b_1C_{1n} + b_2C_{2n} + \cdots + b_nC_{nn} \end{array}  \right]\text{.}
\end{equation*}
%
\par
The expression%
\begin{equation*}
b_1C_{1j} + b_2C_{2j} + \cdots + b_nC_{nj}
\end{equation*}
is the cofactor expansion of the matrix%
\begin{equation*}
A_j = \left[ \begin{array}{cccccccc} a_{11}   \amp  a_{12} \amp  \cdots    \amp a_{1j-1} \amp \cellcolor[gray]{.9} b_1  \amp a_{1j+1}    \amp \cdots   \amp  a_{1n} \\ a_{21}   \amp  a_{22} \amp  \cdots    \amp a_{2j-1} \amp \cellcolor[gray]{.9} b_2   \amp a_{2j+1}  \amp \cdots  \amp  a_{2n} \\ \vdots   \amp   \vdots  \amp          \amp              \amp     \amp         \amp        \amp  \vdots \\ a_{n1}   \amp  a_{n2} \amp  \cdots    \amp a_{nj-1} \amp \cellcolor[gray]{.9} b_n   \amp a_{nj+1}  \amp \cdots  \amp  a_{nn} \end{array}  \right]
\end{equation*}
along the \(j\)th column, giving us the formula in Cramer's Rule.%
\par
Cramer's Rule is not a computationally efficient method. To find a solution to a linear system of \(n\) equations in \(n\) unknowns using Cramer's Rule requires calculating \(n+1\) determinants of \(n \times n\) matrices \textemdash{} quite inefficient when \(n\) is 3 or greater. Our standard method of solving systems using Gaussian elimination is much more efficient. However, Cramer's Rule does provide a formula for the solution to \(A \vx = \vb\) as long as \(A\) is invertible.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Determinant of the Transpose}
\typeout{************************************************}
%
\begin{sectionptx}{The Determinant of the Transpose}{}{The Determinant of the Transpose}{}{}{x:section:sec_det_transpose}
In this section we establish the fact that the determinant of a square matrix is the same as the determinant of its transpose.%
\par
The result is easily verified for \(2 \times 2\) matrices, so we will proceed by induction and assume that the determinant of the transpose of any \((n-1) \times (n-1)\) matrix is the same as the determinant of its transpose. Suppose \(A = [a_{ij}]\) is an \(n \times n\) matrix. By definition,%
\begin{equation*}
\det(A) = a_{11}C_{11} + a_{12}C_{12} + a_{13}C_{13} + \cdots + a_{1n}C_{1n}
\end{equation*}
and%
\begin{equation*}
\det(A^{\tr}) = a_{11}C_{11} + a_{21}C_{21} + a_{31}C_{31} + \cdots + a_{n1}C_{n1}\text{.}
\end{equation*}
%
\par
Note that the only terms in either determinant that contains \(a_{11}\) is \(a_{11}C_{11}\). This term is the same in both determinants, so we proceed to examine other elements. Let us consider all terms in the cofactor expansion for \(\det(A^{\tr})\) that contain \(a_{i1}a_{1j}\). The only summand that contains \(a_{i1}\) is \(a_{i1}C_{i1}\). Letting \(A_{ij}\) be the sub-matrix of \(A\) obtained by deleting the \(i\)th row and \(j\)th column, we see that \(a_{i1}C_{i1} = (-1)^{i+1}a_{i1}\det(A_{i1})\). Now let's examine the sub-matrix \(A_{i1}\):%
\begin{equation*}
\left[ \begin{array}{ccccccc} \cellcolor[gray]{.9} a_{12} \amp \cellcolor[gray]{.9} a_{13} \amp  \cellcolor[gray]{.9} \cdots    \amp \cellcolor[gray]{.9} a_{1j} \amp \cellcolor[gray]{.9} \cdots \amp \cellcolor[gray]{.9} a_{1n-1} \amp \cellcolor[gray]{.9} a_{1n} \\ a_{22} \amp  a_{23} \amp  \cdots    \amp \cellcolor[gray]{.9} a_{2j} \amp \cdots \amp  a_{2n-1} \amp  a_{2n} \\ \vdots \amp        \amp  \ddots    \amp \cellcolor[gray]{.9} \vdots          \amp \ddots \amp  \amp  \\ a_{i-12} \amp  a_{i-13} \amp  \cdots   \amp  \cellcolor[gray]{.9} a_{i-1j} \amp \cdots \amp  a_{i-1n-1} \amp  a_{i-1n} \\ a_{i+12} \amp  a_{i+13} \amp  \cdots    \amp \cellcolor[gray]{.9} a_{i+1j} \amp \cdots \amp  a_{i+1n-1} \amp  a_{i+1n} \\ a_{n2} \amp  a_{n3} \amp  \cdots    \amp \cellcolor[gray]{.9} a_{nj} \amp \cdots \amp  a_{nn-1} \amp  a_{nn} \end{array}  \right]
\end{equation*}
%
\par
When we expand along the first row to calculate \(\det(A_{i1})\), the only term that will involve \(a_{1j}\) is%
\begin{equation*}
(-1)^{j-1+1}a_{1j}\det(A_{i1, 1j})\text{,}
\end{equation*}
where \(A_{ik,jm}\) denotes the sub-matrix of \(A\) obtained by deleting rows \(i\) and \(k\) and columns \(j\) and \(m\) from \(A\). So the term that contains \(a_{i1}a_{1j}\) in the cofactor expansion for \(\det(A^{\tr})\) is%
\begin{equation}
(-1){i+1}a_{i1}(-1)^{j}a_{1j}\det(A_{i1_{1j}}) = (-1)^{i+j+1} a_{i1}a_{1j}\det(A_{i1, 1j})\text{.}\label{x:men:eq_det_transpose_1}
\end{equation}
%
\par
Now we examine the cofactor expansion for \(\det(A)\) to find the terms that contain \(a_{i1}a_{1j}\). The quantity \(a_{1j}\) only appears in the cofactor expansion as%
\begin{equation*}
a_{1j}C_{1j} = (-1)^{1+j}a_{1j}\det(A_{1j})\text{.}
\end{equation*}
%
\par
Now let's examine the sub-matrix \(A_{1j}\):%
\begin{equation*}
\left[ \begin{array}{ccccccc} \cellcolor[gray]{.9} a_{21}   \amp  a_{22}   \amp  \cdots    \amp a_{2j-1} \amp a_{2j+1} \amp \cdots \amp  a_{2n} \\ \cellcolor[gray]{.9} a_{31}   \amp  a_{32}   \amp  \cdots    \amp a_{3j-1} \amp a_{3j+1} \amp \cdots \amp  a_{3n} \\ \cellcolor[gray]{.9} \vdots   \amp            \amp  \ddots    \amp  \vdots   \amp \ddots    \amp       \amp  \\  \cellcolor[gray]{.9} a_{i1}   \amp \cellcolor[gray]{.9} a_{i2}   \amp \cellcolor[gray]{.9} \cdots     \amp \cellcolor[gray]{.9} a_{ij-1} \amp \cellcolor[gray]{.9} a_{ij+1}   \amp \cellcolor[gray]{.9} \cdots \amp \cellcolor[gray]{.9} a_{in} \\ \cellcolor[gray]{.9} \vdots   \amp            \amp             \amp  \vdots   \amp          \amp \vdots \amp \\ \cellcolor[gray]{.9} a_{n1}   \amp  a_{n2}   \amp  \cdots    \amp a_{nj-1} \amp a_{nj+1}  \amp \cdots \amp  a_{nn} \end{array}  \right]
\end{equation*}
%
\par
Here is where we use the induction hypothesis. Since \(A_{1j}\) is an \((n-1) \times (n-1)\) matrix, its determinant can be found with a cofactor expansion down the first column. The only term in this cofactor expansion that will involve \(a_{i1}\) is%
\begin{equation*}
(-1)^{i-1+1}a_{i1} \det(A_{1i, j1})\text{.}
\end{equation*}
%
\par
So the term that contains \(a_{i1}a_{1j}\) in the cofactor expansion for \(\det(A)\) is%
\begin{equation}
(-1)^{1+j}a_{1j}(-1)^{i-1+1}a_{i1} \det(A_{1j_{i1}}) = (-1)^{i+j+1} a_{i1}a_{1j}\det(A_{1i, j1})\text{.}\label{x:men:eq_det_transpose_2}
\end{equation}
%
\par
Since the quantities in \hyperref[x:men:eq_det_transpose_1]{({\xreffont\ref{x:men:eq_det_transpose_1}})} and \hyperref[x:men:eq_det_transpose_2]{({\xreffont\ref{x:men:eq_det_transpose_2}})} are equal, we conclude that the terms in the two cofactor expansions are the same and%
\begin{equation*}
\det(A^{\tr}) = \det(A)\text{.}
\end{equation*}
%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Row Swaps and Determinants}
\typeout{************************************************}
%
\begin{sectionptx}{Row Swaps and Determinants}{}{Row Swaps and Determinants}{}{}{x:section:sec_det_row_swap}
In this section we determine the effect of row swaps to the determinant. Let \(E_{rs}\) be the elementary matrix that swaps rows \(r\) and \(s\) in the \(n \times n\) matrix \(A=[a_{ij}]\). Applying \(E_{12}\) to a \(2 \times 2\) matrix \(A = \left[ \begin{array}{cc} a \amp  b \\ c \amp  d \end{array}  \right]\), we see that%
\begin{equation*}
\det(A) = ad - bc = -(ad-bc) = \det\left(\left[ \begin{array}{cc} c \amp  d \\ a \amp  b \end{array}  \right]\right) = \det(E_{12}A)\text{.}
\end{equation*}
%
\par
So swapping rows in a \(2 \times 2\) matrix multiplies the determinant by \(-1\). Suppose that row swapping on any \((n-1) \times (n-1)\) matrix multiplies the determinant by \(-1\) (in other words, we are proving our statement by mathematical induction). Now suppose \(A\) is an \(n \times n\) matrix and let \(B = [b_{ij}] = E_{rs}A\). We first consider the case that \(s = r+1\) \textemdash{} that we swap adjacent rows. We consider two cases, \(r > 1\) and \(r = 1\). First let us suppose that \(r > 1\). Let \(C_{ij}\) be the \((i,j)\) cofactor of \(A\) and \(C'_{ij}\) the \((i,j)\) cofactor of \(B\). We have%
\begin{equation*}
\det(A) = a_{11}C_{11} + a_{12}C_{12} + \cdots + a_{1n}C_{1n}
\end{equation*}
and%
\begin{equation*}
\det(B) = b_{11}C'_{11} + b_{12}C'_{12} + \cdots + b_{1n}C'_{1n}\text{.}
\end{equation*}
%
\par
Since \(r > 1\),it follows that \(a_{1j} = b_{1j}\) for every \(j\). For each \(j\) the sub-matrix \(B_{1j}\) obtained from \(B\) by deleting the \(i\)th row and \(j\)th column is the same matrix as obtained from \(A_{ij}\) by swapping rows \(r\) and \(s\). So by our induction hypothesis, we have \(C'_{1j} = -C_{1j}\) for each \(j\). Then%
\begin{align*}
\det(B) \amp = b_{11}C'_{11} + b_{12}C'_{12} + \cdots + b_{1n}C'_{1n}\\
\amp = a_{11}(-C_{11}) + a_{12}(-C_{12}) + \cdots + a_{1n}(-C_{1n})\\
\amp = -(a_{11}C_{11} + a_{12}C_{12} + \cdots + a_{1n}C_{1n})\\
\amp = -\det(A)\text{.}
\end{align*}
%
\par
Now we consider the case where \(r=1\), where \(B\) is the matrix obtained from \(A\) by swapping the first and second rows. Here we will use the fact that \(\det(A) = \det(A^{\tr})\) which allows us to calculate \(\det(A)\) and \(\det(B)\) with the cofactor expansions down the first column. In this case we have%
\begin{equation*}
\det(A) = a_{11}C_{11} + a_{21}C_{21} + \cdots + a_{n1}C_{n1}
\end{equation*}
and%
\begin{align*}
\det(B) \amp = b_{11}C'_{11} + b_{21}C'_{21} + \cdots + b_{n1}C'_{n1}\\
\amp = a_{21}C'_{11} + a_{11}C'_{21} + a_{31}C'_{31} + \cdots + a_{n1}C'_{n1}\text{.}
\end{align*}
%
\par
For each \(i \geq 3\), the sub-matrix \(B_{i1}\) is just \(A_{i1}\) with rows 1 and 2 swapped. So we have \(C'_{i1} = -C_{i1}\) by our induction hypothesis. Since we swapped rows 1 and 2, we have \(B_{21} = A_{11}\) and \(B_{11} = A_{21}\). Thus,%
\begin{equation*}
b_{11}C'_{11} = (-1)^{1+1}b_{11}\det(A_{21}) = a_{21}\det(A_{21}) = -a_{21}C_{21}
\end{equation*}
and%
\begin{equation*}
b_{21}C'_{21} = (-1)^{2+1}a_{11}\det(A_{11}) = -a_{11}\det(A_{11}) = -a_{11}C_{11}\text{.}
\end{equation*}
%
\par
Putting this all together gives us%
\begin{align*}
\det(B) \amp = b_{11}C'_{11} + b_{21}C'_{21} + \cdots + b_{n1}C'_{n1}\\
\amp = -a_{21}C_{21} - a_{11}C_{11} + a_{31}(-C_{31}) + \cdots + a_{n1}(-C_{n1})\\
\amp = -\left(a_{11}C_{11} + a_{21}C_{21} + \cdots + a_{n1}C_{n1}\right)\\
\amp = - \det(A)\text{.}
\end{align*}
%
\par
So we have shown that if \(B\) is obtained from \(A\) by interchanging two adjacent rows, then \(\det(B) = -\det(A)\). Now we consider the general case. Suppose \(B\) is obtained from \(A\) by interchanging rows \(r\) and \(s\), with \(r \lt  s\). We can perform this single row interchange through a sequence of adjacent row interchanges. First we swap rows \(r\) and \(r+1\), then rows \(r+1\) and \(r+2\), and continue until we swap rows \(s-1\) and \(s\). This places the original row \(r\) into the row \(s\) position, and the process involved \(s-r\) adjacent row interchanges. Each of these interchanges multiplies the determinant by a factor of \(-1\). At the end of this sequence of row swaps, the original row \(s\) is now row \(s-1\). So it will take one fewer adjacent row interchanges to move this row to be row \(r\). This sequence of \((s-r)+(s-r-1) = 2(s-r-1)-1\) row interchanges produces the matrix \(B\). Thus,%
\begin{equation*}
\det(B) = (-1)^{2(s-r)-1}\det(A) = -\det(A)\text{,}
\end{equation*}
and interchanging any two rows multiplies the determinant by \(-1\).%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Cofactor Expansions}
\typeout{************************************************}
%
\begin{sectionptx}{Cofactor Expansions}{}{Cofactor Expansions}{}{}{x:section:sec_cofactor_expand}
We have stated that the determinant of a matrix can be calculated by using a cofactor expansion along any row or column. We use the result that swapping rows introduces a factor of \(-1\) in the determinant to verify that result in this section. Note that in proving that \(\det(A^{\tr}) = \det(A)\), we have already shown that the cofactor expansion along the first column is the same as the cofactor expansion along the first row. If we can prove that the cofactor expansion along any row is the same, then the fact that \(\det(A^{\tr}) = \det(A)\) will imply that the cofactor expansion along any column is the same as well.%
\par
Now we demonstrate that the cofactor expansions along the first row and the \(i\)th row are the same. Let \(A = [a_{ij}]\) be an \(n \times n\) matrix. The cofactor expansion of \(A\) along the first row is%
\begin{equation*}
a_{11}C_{11} + a_{12}C_{12} + \cdots + a_{1n}C_{1n}
\end{equation*}
and the cofactor expansion along the \(i\)th row is%
\begin{equation*}
a_{i1}C_{i1} + a_{i2}C_{i2} + \cdots + a_{in}C_{in}\text{.}
\end{equation*}
%
\par
Let \(B\) be the matrix obtained by swapping row \(i\) with previous rows so that row \(i\) becomes the first row and the order of the remaining rows is preserved.%
\begin{equation*}
B = \left[ \begin{array}{cccccc} \cellcolor[gray]{.9} a_{i1}    \amp \cellcolor[gray]{.9} a_{i2}   \amp \cellcolor[gray]{.9} \cdots  \amp \cellcolor[gray]{.9} a_{ij}   \amp \cellcolor[gray]{.9} \cdots \amp \cellcolor[gray]{.9} a_{in} \\ \cellcolor[gray]{.9} a_{11}     \amp  a_{12}   \amp  \cdots    \amp a_{1j}   \amp \cdots \amp  a_{1n} \\ \cellcolor[gray]{.9} a_{21}     \amp  a_{22}   \amp  \cdots    \amp a_{2j}   \amp \cdots \amp  a_{2n} \\ \cellcolor[gray]{.9} a_{i-11}   \amp  a_{i-12}   \amp  \cdots    \amp a_{i-1j}   \amp \cdots \amp  a_{i-1n} \\ \cellcolor[gray]{.9} a_{i+11}   \amp  a_{i+12}   \amp  \cdots    \amp a_{i+1j}   \amp \cdots \amp  a_{i+1n} \\ \cellcolor[gray]{.9} \vdots     \amp  \ddots    \amp  \vdots     \amp \ddots    \amp       \amp  \\ \cellcolor[gray]{.9} a_{n1}     \amp  a_{n2}   \amp  \cdots    \amp a_{nj}    \amp \cdots \amp  a_{nn} \end{array}  \right]
\end{equation*}
%
\par
Then%
\begin{equation*}
\det(B) = (-1)^{i-1} \det(A)\text{.}
\end{equation*}
%
\par
So, letting \(C'_{ij}\) be the \((i,j)\) cofactor of \(B\) we have%
\begin{equation*}
\det(A) = (-1)^{i-1} \det(B) = (-1)^{i-1}\left(a_{i1}C'_{11} + a_{i2}C'_{12} + \cdots + a_{in}C'_{1n}\right)\text{.}
\end{equation*}
%
\par
Notice that for each \(j\) we have \(B_{1j} = A_{ij}\). So%
\begin{align*}
\det(A) \amp = (-1)^{i-1}\big(a_{i1}C'_{11} + a_{i2}C'_{12} + \cdots + a_{in}C'_{1n}\big)\\
\amp = (-1)^{i-1}\Big(a_{i1}(-1)^(1+1)\det(B_{11}) + a_{i2}(-1)^{1+2}\det(B_{12})\\
\amp \qquad + \cdots + a_{in}(-1)^{1+n}\det(B_{1n})\Big)\\
\amp = (-1)^{i-1}\Big(a_{i1}(-1)^(1+1)\det(A_{i1}) + a_{i2}(-1)^{1+2}\det(A_{i2})\\
\amp \qquad + \cdots + a_{in}(-1)^{1+n}\det(A_{in})\Big)\\
\amp = a_{i1}(-1)^(i+1)\det(A_{i1}) + a_{i2}(-1)^{i+2}\det(A_{i2})\\
\amp \qquad + \cdots + a_{in}(-1)^{i+n}\det(A_{in})\\
\amp = a_{i1}C_{i1} + a_{i2}C_{i2} + \cdots + a_{in}C_{in}\text{.}
\end{align*}
%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The LU Factorization of a Matrix}
\typeout{************************************************}
%
\begin{sectionptx}{The LU Factorization of a Matrix}{}{The LU Factorization of a Matrix}{}{}{x:section:sec_mtx_lu_factor}
There are many instances where we have a number of systems to solve of the form \(A \vx = \vb\), all with the same coefficient matrix. The system may evolve over time so that we do not know the constant vectors \(\vb\) in the system all at once, but only determine them as time progresses. Each time we obtain a new vector \(\vb\), we have to apply the same row operations to reduce the coefficient matrix to solve the new system. This is time repetitive and time consuming. Instead, we can keep track of the row operations in one row reduction and save ourselves a significant amount of time. One way of doing this is the \(LU\)-factorization (or decomposition).%
\par
To illustrate, suppose we can write the matrix \(A\) as a product \(A = LU\), where%
\begin{equation*}
L = \left[ \begin{array}{rccc} 1\amp 0\amp 0\amp 0\\-1\amp 1\amp 0\amp 0 \\0\amp 1\amp 1\amp 0\\1\amp 0\amp 0\amp 1 \end{array}   \right] \ \ \text{ and }  \ \ U = \left[ \begin{array}{ccrr} 1\amp 0\amp 1\amp 0\\0\amp 1\amp 3\amp -2 \\0\amp 0\amp 0\amp 3\\0\amp 0\amp 0\amp 0 \end{array}   \right]\text{.}
\end{equation*}
%
\par
Let \(\vb = [3 \ 1 \ 1 \ 3]^{\tr}\) and \(\vx = [x_1 \ x_2 \ x_3 \ x_4]^{\tr}\), and consider the linear system \(A \vx = \vb\). If \(A \vx = \vb\), then \(LU \vx = \vb\). We can solve this system without applying row operations as follows. Let \(U\vx = \vz\), where \(\vz = [z_1 \ z_2 \ z_3 \ z_4]^{\tr}\). We can solve \(L\vz = \vb\) by using forward substitution.%
\par
The equation \(L \vz = \vb\) is equivalent to the system%
\begin{align*}
{}z_1   \  \amp {}  \  \amp {}      \  \amp {} \  \amp {}     \  \amp {} \   \amp {}       \amp = 3\\
{-}z_1  \  \amp {+} \  \amp {}z_2   \  \amp {} \   \amp {}     \  \amp {}  \   \amp {}       \amp = 1\\
{}      \  \amp {}  \  \amp {}z_2   \  \amp {+} \   \amp {}z_3  \   \amp {}  \   \amp {}       \amp = 1\\
{}      \  \amp {}  \  \amp {}      \  \amp {}  \   \amp {}     \   \amp {}  \   \amp {}z_4    \amp = 3\text{.}
\end{align*}
%
\par
The first equation shows that \(z_1=3\). Substituting into the second equation gives us \(z_2 = 4\). Using this information in the third equation yields \(z_3 = -3\), and then the fourth equation shows that \(z_4 = 0\). To return to the original system, since \(U\vx = \vz\), we now solve this system to find the solution vector \(\vx\). In this case, since \(U\) is upper triangular, we use back substitution. The equation \(U\vx = \vz\) is equivalent to the system%
\begin{align*}
{}x_1   \  \amp {}  \  \amp {}      \  \amp {+} \  \amp {}x_3   \  \amp {}    \   \amp {}       \amp = \amp {}\amp 3\\
{}      \  \amp {} \  \amp {}x_2   \  \amp {+} \   \amp {3}x_3  \  \amp {-}  \  \amp {2}x_4   \amp = \amp {}\amp 4\\
{}      \  \amp {}  \  \amp {}      \  \amp {}  \   \amp {}      \   \amp {}  \  \amp {3}x_4   \amp = \amp {-}\amp 3\text{.}
\end{align*}
%
\par
\index{\(LU\) factorization} Note that the third column of \(U\) is not a pivot column, so \(x_3\) is a free variable. The last equation shows that \(x_4=-1\). Substituting into the second equation and solving for \(x_2\) yields \(x_2 = 2-3x_3\). The first equation then gives us \(x_1 = 3-x_3\). So the general solution%
\begin{equation*}
\vx = \left[ \begin{array}{r} 3\\2\\0\\-1 \end{array}  \right] +  \left[ \begin{array}{r} -1\\-3\\1\\0 \end{array}  \right]x_3
\end{equation*}
to \(A \vx = \vb\) can be found through \(L\) and \(U\) via forward and backward substitution. If we can find a factorization of a matrix \(A\) into a lower triangular matrix \(L\) and an upper triangular matrix \(U\), then \(A= LU\) is called an \terminology{\(LU\)-factorization} or \terminology{\(LU\)-decomposition}.%
\par
We can use elementary matrices to obtain a factorization of certain matrices into products of lower triangular (the ``L'' in LU) and upper triangular (the ``U'' in LU) matrices. We illustrate with an example. Let%
\begin{equation*}
A = \left[ \begin{array}{rccr} 1\amp 0\amp 1\amp 0\\-1\amp 1\amp 2\amp -2 \\0\amp 1\amp 3\amp 1\\1\amp 0\amp 1\amp 0 \end{array}   \right]\text{.}
\end{equation*}
%
\par
Our goal is to find an upper triangular matrix \(U\) and a lower triangular matrix \(L\) so that \(A = LU\). We begin by row reducing \(A\) to an upper triangular matrix, keeping track of the elementary matrices used to perform the row operations. We start by replacing the entries below the \((1,1)\) entry in \(A\) with zeros. The elementary matrices that perform these operations are%
\begin{equation*}
E_1 = \left[ \begin{array}{cccc} 1\amp 0\amp 0\amp 0\\1\amp 1\amp 0\amp 0 \\0\amp 0\amp 1\amp 0\\0\amp 0\amp 0\amp 1 \end{array}   \right] \ \ \text{ and }  \ \ E_2 = \left[ \begin{array}{rccc} 1\amp 0\amp 0\amp 0\\0\amp 1\amp 0\amp 0 \\0\amp 0\amp 1\amp 0\\-1\amp 0\amp 0\amp 1 \end{array}   \right]\text{,}
\end{equation*}
and%
\begin{equation*}
E_2E_1A = \left[ \begin{array}{ccrr} 1\amp 0\amp 1\amp 0\\0\amp 1\amp 3\amp -2 \\0\amp 1\amp 3\amp 1\\0\amp 0\amp 0\amp 0 \end{array}   \right]\text{.}
\end{equation*}
%
\par
We next zero out the entries below the \((2,2)\) entry as%
\begin{equation*}
E_3E_2E_1A = \left[ \begin{array}{ccrr} 1\amp 0\amp 1\amp 0\\0\amp 1\amp 3\amp -2 \\0\amp 0\amp 0\amp 3\\0\amp 0\amp 0\amp 0 \end{array}   \right]\text{,}
\end{equation*}
where%
\begin{equation*}
E_3 = \left[ \begin{array}{crcc} 1\amp 0\amp 0\amp 0\\0\amp 1\amp 0\amp 0 \\0\amp -1\amp 1\amp 0\\0\amp 0\amp 0\amp 1 \end{array}   \right]\text{.}
\end{equation*}
%
\par
The product \(E_3E_2E_1A\) is an upper triangular matrix \(U\). So we have%
\begin{equation*}
E_3E_2E_1A = U
\end{equation*}
and%
\begin{equation*}
A = E_1^{-1}E_2^{-1}E_3^{-1}U\text{,}
\end{equation*}
where%
\begin{equation*}
E_1^{-1}E_2^{-1}E_3^{-1} = \left[ \begin{array}{rccc} 1\amp 0\amp 0\amp 0\\-1\amp 1\amp 0\amp 0 \\0\amp 1\amp 1\amp 0\\1\amp 0\amp 0\amp 1 \end{array}   \right]
\end{equation*}
is a lower triangular matrix \(L\). So we have decomposed the matrix \(A\) into a product \(A = LU\), where \(L\) is lower triangular and \(U\) is upper triangular. Since every matrix is row equivalent to a matrix in row echelon form, we can always find an upper triangular matrix \(U\) in this way. However, we may not always obtain a corresponding lower triangular matrix, as the next example illustrates.%
\par
Suppose we change the problem slightly and consider the matrix%
\begin{equation*}
B = \left[ \begin{array}{rccr} 1\amp 0\amp 1\amp 0\\-1\amp 1\amp 2\amp -2 \\0\amp 1\amp 3\amp 1\\1\amp 0\amp 0\amp 1 \end{array}   \right]\text{.}
\end{equation*}
%
\par
Using the same elementary matrices \(E_1\), \(E_2\), and \(E_3\) as earlier, we have%
\begin{equation*}
E_3E_2E_1B = \left[ \begin{array}{ccrr} 1\amp 0\amp 1\amp 0\\0\amp 1\amp 3\amp -2 \\0\amp 0\amp 0\amp 3\\0\amp 0\amp -1\amp 1 \end{array}   \right]\text{.}
\end{equation*}
%
\par
To reduce \(B\) to row-echelon form now requires a row interchange. Letting%
\begin{equation*}
E_4 = \left[ \begin{array}{crcc} 1\amp 0\amp 0\amp 0\\0\amp 1\amp 0\amp 0 \\0\amp 0\amp 0\amp 1\\0\amp 0\amp 1\amp 0 \end{array}   \right]
\end{equation*}
brings us to%
\begin{equation*}
E_4E_3E_2E_1B = \left[ \begin{array}{ccrr} 1\amp 0\amp 1\amp 0\\0\amp 1\amp 3\amp -2 \\0\amp 0\amp -1\amp 1\\0\amp 0\amp 0\amp 3 \end{array}   \right]\text{.}
\end{equation*}
%
\par
So in this case we have \(U = E_4E_3E_2E_1B\), but%
\begin{equation*}
E_1^{-1}E_2^{-1}E_3^{-1}E_4^{-1} = \left[ \begin{array}{rccc} 1\amp 0\amp 0\amp 0\\-1\amp 1\amp 0\amp 0 \\0\amp 1\amp 0\amp 1\\1\amp 0\amp 1\amp 0 \end{array}   \right]
\end{equation*}
is not lower triangular. The difference in this latter example is that we needed a row swap to obtain the upper triangular form.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_det_prop_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp18662824}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}If \(A\), \(B\) are \(n\times n\) matrices with \(\det(A)=3\) and \(\det(B)=2\), evaluate the following determinant values. Briefly justify.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}\(\det(A^{-1})\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp18660520}{}\quad{}Assume that \(\det(A)=3\) and \(\det(B)=2\).%
\par
Since \(\det(A) \neq 0\), we know that \(A\) is invertible. Since \(1 = \det(I_n) = \det(AA^{-1}) = \det(A) \det(A^{-1})\), it follows that \(\det(A^{-1}) = \frac{1}{\det(A)} = \frac{1}{3}\).%
\item{}\(\det(ABA^{\mathsf{T}})\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp18666920}{}\quad{}Assume that \(\det(A)=3\) and \(\det(B)=2\).%
\par
We know that \(\det(A^{\tr}) = \det(A)\), so%
\begin{align*}
\det(ABA^\tr) \amp = \det(A) \det(B) \det(A^{\tr})\\
\amp = \det(A) \det(B) \det(A)\\
\amp = (3)(2)(3)\\
\amp = 18\text{.}
\end{align*}
%
\item{}\(\det(A^3(BA)^{-1}(AB)^2)\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp18664360}{}\quad{}Assume that \(\det(A)=3\) and \(\det(B)=2\).%
\par
Using properties of determinants gives us%
\begin{align*}
\det(A^3(BA)^{-1}(AB)^2) \amp = \det(A^3)\det((BA)^{-1}) \det((AB)^2)\\
\amp = (\det(A))^3 \left(\frac{1}{\det(AB)}\right) (\det(AB))^2\\
\amp = 27 \left(\frac{1}{\det(A) \det(B)} \right) (\det(A)\det(B))^2\\
\amp = \frac{(27)(6^2)}{6}\\
\amp = 162\text{.}
\end{align*}
%
\end{enumerate}
\item{}If the determinant of \(\left[ \begin{array}{ccc} a\amp b\amp c\\d\amp e\amp f\\g\amp h\amp i \end{array} \right]\) is \(m\), find the determinant of each of the following matrices.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}\(\left[ \begin{array}{ccc} a\amp b\amp c\\2d\amp 2e\amp 2f\\g\amp h\amp i \end{array} \right]\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp18672808}{}\quad{}Assume that \(\det\left(\left[ \begin{array}{ccc} a\amp b\amp c\\d\amp e\amp f\\g\amp h\amp i \end{array}  \right] \right) = m\).%
\par
Multiplying a row by a scalar multiples the determinant by that scalar, so%
\begin{equation*}
\det\left( \left[ \begin{array}{ccc} a\amp b\amp c\\2d\amp 2e\amp 2f\\g\amp h\amp i \end{array}  \right] \right) = 2m\text{.}
\end{equation*}
%
\item{}\(\left[ \begin{array}{ccc} d\amp e\amp f\\g\amp h\amp i\\a\amp b\amp c \end{array} \right]\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp18676264}{}\quad{}Assume that \(\det\left(\left[ \begin{array}{ccc} a\amp b\amp c\\d\amp e\amp f\\g\amp h\amp i \end{array}  \right] \right) = m\).%
\par
Interchanging two rows multiples the determinant by \(-1\). It takes two row swaps in the original matrix to obtain this one, so%
\begin{equation*}
\det\left( \left[ \begin{array}{ccc} d\amp e\amp f\\g\amp h\amp i\\a\amp b\amp c \end{array}  \right] \right) = (-1)^2m = m\text{.}
\end{equation*}
%
\item{}\(\left[ \begin{array}{ccc} a\amp b\amp c\\g-2d\amp h-2e\amp i-2f\\a+d\amp b+e\amp c+f \end{array} \right]\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp18675112}{}\quad{}Assume that \(\det\left(\left[ \begin{array}{ccc} a\amp b\amp c\\d\amp e\amp f\\g\amp h\amp i \end{array}  \right] \right) = m\).%
\par
Adding a multiple of a row to another does not change the determinant of the matrix. Since there is a row swap needed to get this matrix from the original we have%
\begin{equation*}
\det\left(\left[ \begin{array}{ccc} a\amp b\amp c\\g-2d\amp h-2e\amp i-2f\\a+d\amp b+e\amp c+f \end{array}  \right]\right) = -m\text{.}
\end{equation*}
%
\end{enumerate}
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp18680616}%
Let \(A = \left[ \begin{array}{ccr} 2\amp 8\amp 0\\2\amp 2\amp -3\\1\amp 2\amp 7 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find an LU factorization for \(A\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp18683560}{}\quad{}We row reduce \(A\) to an upper triangular matrix by applying elementary matrices. First notice that if \(E_1 = \left[ \begin{array}{rcc} 1\amp 0\amp 0 \\-1\amp 1\amp 0 \\ 0\amp 0\amp 1 \end{array}  \right]\), then%
\begin{equation*}
E_1 A = \left[ \begin{array}{crr} 2\amp 8\amp 0 \\ 0\amp -6\amp -3 \\ 1\amp 2\amp 7 \end{array}  \right]\text{.}
\end{equation*}
Letting \(E_2 =  \left[ \begin{array}{rcc} 1\amp 0\amp 0 \\0\amp 1\amp 0 \\ -\frac{1}{2}\amp 0\amp 1 \end{array}  \right]\) gives us%
\begin{equation*}
E_2E_1A = \left[ \begin{array}{crr} 2\amp 8\amp 0 \\ 0\amp -6\amp -3 \\ 0\amp -2\amp 7 \end{array}  \right]\text{.}
\end{equation*}
Finally, when \(E_3 = \left[ \begin{array}{crc} 1\amp 0\amp 0 \\0\amp 1\amp 0 \\ 0\amp -\frac{1}{3}\amp 1 \end{array}  \right]\) we have%
\begin{equation*}
U=E_3E_2E_1A = \left[ \begin{array}{crr} 2\amp 8\amp 0 \\ 0\amp -6\amp -3 \\ 0\amp 0\amp 8 \end{array}  \right]\text{.}
\end{equation*}
This gives us \(E_3E_2E_1A = U\), so we can take%
\begin{equation*}
L = E_1^{-1}E_2^{-1}E_3^{-1} =  \left[ \begin{array}{ccc} 1\amp 0\amp 0 \\1\amp 1\amp 0 \\ \frac{1}{2}\amp \frac{1}{3}\amp 1 \end{array}  \right]\text{.}
\end{equation*}
%
\item{}Use the LU factorization with forward substitution and back substitution to solve the system \(A \vx = [18 \ 3 \ 12]^{\tr}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp18685224}{}\quad{}To solve the system \(A \vx = \vb\), where \(\vb = [18 \ 3 \ 12]^{\tr}\), we use the LU factorization of \(A\) and solve \(LU \vx = \vb\). Let \(\vx = [x_1 \ x_2 \ x_3]^{\tr}\) and let \(\vz = [z_1 \ z_2 \ z_3]^{\tr}\) with \(U \vx = \vz\) so that \(L \vz = L(U\vx) = A\vx = \vb\). First we solve \(L \vz = [18 \ 3 \ 12]^{\tr}\) to find \(\vz\) using forward substitution. The first row of \(L\) shows that \(z_1 = 18\) and the second row that \(z_1 + z_2 = 3\). So \(z_2 = -15\). The third row of \(L\) gives us \(\frac{1}{2}z_1 + \frac{1}{3}z_2 + z_3 = 12\), so \(z_3 = 12 - 9 + 5 = 8\). Now to find \(\vx\) we solve \(U \vx = \vz\) using back substitution. The third row of \(U\) tells us that \(8x_3 = 8\) or that \(x_3 = 1\). The second row of \(U\) shows that \(-6x_2-3x_3 = -15\) or \(x_2 =2\). Finally, the first row of \(U\) gives us \(2x_1+8x_2 = 18\), or \(x_1 = 1\). So the solution to \(A \vx = \vb\) is \(\vx = [1 \ 2 \ 1]^{\tr}\).%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_det_prop_summ}
%
\begin{itemize}[label=\textbullet]
\item{}The elementary row operations have the following effects on the determinant:%
\begin{itemize}[label=$\circ$]
\item{}If we multiply a row of a matrix by a constant \(k\), then the determinant is multiplied by \(k\).%
\item{}If we swap two rows of a matrix, then the determinant changes sign.%
\item{}If we add a multiple of a row of a matrix to another, the determinant does not change.%
\end{itemize}
%
\item{}Each of the elementary row operations can be achieved by multiplication by elementary matrices. To obtain the elementary matrix corresponding to an elementary row operation, we perform the operation on the identity matrix.%
\item{}Let \(A\) be an \(n\times n\) invertible matrix. For any \(\vb\) in \(\R^n\), the solution \(\vx\) of \(A\vx=\vb\) has entries%
\begin{equation*}
x_i =\frac{\det(A_i(\vb))}{\det(A)}
\end{equation*}
where \(A_i(\vb)\) represents the matrix formed by replacing \(i\)th column of \(A\) with \(\vb\).%
\item{}Let \(A\) be an invertible \(n\times n\) matrix. Then%
\begin{equation*}
A^{-1} = \frac{1}{\det(A)} \text{ adj }  A
\end{equation*}
where the \(\text{ adj }  A\) matrix, the \terminology{adjugate of \(A\)}, is defined as the matrix whose \(ij\)-th entry is \(C_{ji}\), the \(ji\)-th cofactor of \(A\).%
\item{}For a \(2\times 2\) matrix \(A\), the area of the image of the unit square under the transformation \(T(\vx)=A\vx\) is equal to \(|\det(A)|\), which is also equal to the area of the parallelogram defined by the columns of \(A\).%
\item{}For a \(3\times 3\) matrix \(A\), the volume of the image of the unit cube under the transformation \(T(\vx)=A\vx\) is equal to \(|\det(A)|\), which is also equal to the volume of the parallelepiped defined by the columns of \(A\).%
\item{}An \(LU\) factorization of a square matrix \(A\) consists of a lower triangular matrix \(L\) and an upper triangular matrix \(U\) so that \(A = LU\).%
\item{}A square matrix \(A\) has an \(LU\) factorization if we can use row operations without row interchanges to row reduce \(A\) to an upper triangular matrix \(U\). In this situation the elementary matrices that perform the row operations produce a lower triangular matrix \(L\) so that \(A = LU\). If \(A\) cannot be reduced to an upper triangular matrix \(U\) without row interchanges, then we can factor \(A\) in the form \(PLU\), where \(L\) is a lower triangular matrix, \(U\) is an upper triangular matrix, and \(P\) is obtained from the identity matrix by appropriate row interchanges.%
\item{}There are many instances where we have a number of systems to solve of the form \(A \vx = \vb\), all with the same coefficient matrix but where the vectors \(\vb\) can change. With an \(LU\) factorization, we can keep track of the row operations in one row reduction and save ourselves a significant amount of time when solving these systems.%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_det_prop_exer}
\begin{divisionexercise}{1}{}{}{x:exercise:ex_4_f_det_multiple}%
Find a formula for \(\det(rA)\) in terms of \(r\) and \(\det(A)\), where \(A\) is an \(n\times n\) matrix and \(r\) is a scalar. Explain why your formula is valid.%
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp18733352}%
Find \(\det(A)\) by hand using elementary row operations where%
\begin{equation*}
A= \left[ \begin{array}{rrrr} 1\amp 2\amp -1\amp 3\\ -1\amp -2\amp 3\amp -1\\ -2\amp -1\amp 2\amp -3\\ 1\amp 8\amp -3\amp 8 \end{array}  \right] \,\text{.}
\end{equation*}
%
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp18736552}%
Consider the matrix \(A= \left[ \begin{array}{rrrr} 4\amp -1\amp -1\amp -1 \\ -1\amp 4\amp -1\amp -1\\ -1\amp -1\amp 4\amp -1\\ -1\amp -1\amp -1\amp 4 \end{array}  \right]\). We will find \(\det(A)\) using elementary row operations. (This matrix arises in graph theory, and its determinant gives the number of spanning trees in the complete graph with 5 vertices. This number is also equal to the number of labeled trees with 5 vertices.)%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Add rows \(R_2\), \(R_3\) and \(R_4\) to the first row in that order.%
\item{}Then add the new \(R_1\) to rows \(R_2\), \(R_3\) and \(R_4\) to get a triangular matrix \(B\).%
\item{}Find the determinant of \(B\). Then use \(\det(B)\) and properties of how elementary row operations affect determinants to find \(\det(A)\).%
\item{}Generalize your work to find the determinant of the \(n\times n\) matrix%
\begin{equation*}
A= \left[ \begin{array}{rrrrrr} n\amp -1\amp -1\amp \cdots \amp -1\amp -1 \\ -1\amp n\amp -1\amp \cdots \amp -1\amp -1\\ \vdots \amp \vdots \amp \vdots \amp \cdots \amp  \vdots \amp  \vdots\\ -1\amp -1\amp -1\amp  \cdots\amp -1\amp n \end{array}  \right] \,\text{.}
\end{equation*}
%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp18750632}%
For which matrices \(A\), if any, is \(\det(A)=-\det(-A)\)? Justify your answer.%
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp18745768}%
Find the inverse \(A^{-1}\) of \(A=\left[ \begin{array}{ccc} 1\amp 0\amp 1 \\ 0\amp 1\amp 0\\2\amp 0\amp 1 \end{array} \right]\) using the adjugate matrix.%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp18744872}%
For an invertible \(n\times n\) matrix \(A\), what is the relationship between \(\det(A)\) and \(\det(\text{ adj } A)\)? Justify your result.%
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp18755752}%
Let \(A = \left[ \begin{array}{ccc} a\amp b\amp 1\\c\amp d\amp 2\\e\amp f\amp 3 \end{array} \right]\), and assume that \(\det(A) = 2\). Determine the determinants of each of the following.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(B= \left[ \begin{array}{ccc} a\amp b\amp 1\\3c\amp 3d\amp 6\\e+a\amp f+b\amp 4 \end{array} \right]\)%
\item{}\(C = \left[ \begin{array}{ccr} 2e\amp 2f\amp 6 \\2c-2e\amp 2d-2f\amp -2\\2a\amp 2b\amp 2 \end{array} \right]\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp18755240}%
Find the area of the parallelogram with one vertex at the origin and adjacent vertices at \((1,2)\) and \((a,b)\). For which \((a,b)\) is the area 0? When does this happen geometrically?%
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp18758696}%
Find the volume of the parallelepiped with one vertex at the origin and three adjacent vertices at \((3,2,0)\), \((1,1,1)\) and \((1,3,c)\) where \(c\) is unknown. For which \(c\), is the volume 0? When does this happen geometrically?%
\end{divisionexercise}%
\begin{divisionexercise}{10}{}{}{g:exercise:idp18762792}%
Find an \(LU\) factorization of each of the following matrices \(A\). Use the \(LU\) factorization to solve the system \(A \vx = \vb\) for the given vector \(\vb\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(A = \left[ \begin{array}{rcc} 2\amp 1\amp 3 \\ -1\amp 0\amp 1 \\ 2\amp 1\amp 5 \end{array} \right]\), \(\vb = \left[ \begin{array}{c} 1 \\ 2 \\ 1 \end{array} \right]\)%
\item{}\(A = \left[ \begin{array}{rcc} 1\amp 1\amp 0 \\ -1\amp 1\amp 1 \\ 0\amp 2\amp 1 \end{array} \right]\), \(\vb = \left[ \begin{array}{c} 1 \\ 2 \\ 1 \end{array} \right]\)%
\item{}\(A = \left[ \begin{array}{rcrc} 1\amp 1\amp 1\amp 0 \\ 1\amp 0\amp 1\amp 1 \\ -1\amp 1\amp 0\amp 0 \\ 0\amp 1\amp -1\amp 0 \end{array} \right]\), \(\vb = \left[ \begin{array}{c} 3 \\ 0 \\ 2 \\ 0 \end{array} \right]\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{11}{}{}{g:exercise:idp18771752}%
Let \(A = \left[ \begin{array}{cc} 1\amp 2\\2\amp 1 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find an \(LU\) decomposition of \(A\).%
\item{}Find a different factorization of \(A\) into a product \(L'U'\) where \(L'\) is a lower triangular matrix different from \(L\) and \(U'\) is an upper triangular matrix different from \(U\). Conclude that the \(LU\) decomposition of a matrix is not unique.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{12}{}{}{g:exercise:idp18771624}%
Let \(A = \left[ \begin{array}{rcc} 1\amp 2\amp 3\\-1\amp 2\amp 0 \\ 2\amp 0\amp 0 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find an \(LU\) decomposition of \(A\).%
\item{}Find an \(LU\) decomposition of \(A\) in which the diagonal entries of \(D\) are all 1.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp18781352}{}\quad{}Continue row reducing.%
\item{}Find an upper triangular matrix \(L\) whose diagonal entries are all 1, a lower triangular matrix \(U\) whose diagonal entries are all 1, and a diagonal matrix \(D\) such that \(A = LDU\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{13}{}{}{g:exercise:idp18778536}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
If two rows are equal in \(A\), then \(\det(A)=0\).%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is a square matrix and \(R\) is a row echelon form of \(A\), then \(\det(A) = \det(R)\).%
\item{}\lititle{True\slash{}False.}\par%
If a matrix \(A\) is invertible, then 0 is not an eigenvalue of \(A\).%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is a \(2\times 2\) matrix for which the image of the unit square under the transformation \(T(\vx)=A\vx\) has zero area, then \(A\) is non-invertible.%
\item{}\lititle{True\slash{}False.}\par%
Row operations do not change the determinant of a square matrix.%
\item{}\lititle{True\slash{}False.}\par%
If \(A_{ij}\) is the matrix obtained from a square matrix \(A = [a_{ij}]\) by deleting the \(i\)th row and \(jth\) column of \(A\), then%
\begin{align*}
a_{i1}(-1)^{i+1}\amp \det(A_{i1}) + a_{i2}(-1)^{i+2}\det(A_{i2}) + \cdots\\
\amp \qquad + a_{in}(-1)^{i+n}\det(A_{in})\\
\amp =  a_{1j}(-1)^{j+1}\det(A_{1j}) + a_{2i}(-1)^{j+2}\det(A_{2i}) + \cdots\\
\amp \qquad + a_{nj}(-1)^{j+n}\det(A_{nj})
\end{align*}
for any \(i\) and \(j\) between \(1\) and \(n\).%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is an invertible matrix, then \(\det\left(A^{\tr}A\right) > 0\).%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
\end{chapterptx}
\end{partptx}
%
%
\typeout{************************************************}
\typeout{Chapter V Orthogonality}
\typeout{************************************************}
%
\begin{partptx}{Orthogonality}{}{Orthogonality}{}{}{x:part:part-orthog}
 %
%
\typeout{************************************************}
\typeout{Section 23 The Dot Product in \(\R^n\)}
\typeout{************************************************}
%
\begin{chapterptx}{The Dot Product in \(\R^n\)}{}{The Dot Product in \(\R^n\)}{}{}{x:chapter:chap_dot_product}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp18808488}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What is the dot product of two vectors? Under what conditions is the dot product defined?%
\item{}How do we find the angle between two nonzero vectors in \(\R^n\)?%
\item{}How does the dot product tell us if two vectors are orthogonal?%
\item{}How do we define the length of a vector in any dimension and how can the dot product be used to calculate the length?%
\item{}How do we define the distance between two vectors?%
\item{}What is the orthogonal projection of a vector \(\vu\) in the direction of the vector \(\vv\) and how do we find it?%
\item{}What is the orthogonal complement of a subspace \(W\) of \(\R^n\)?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Hidden Figures in Computer Graphics}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Hidden Figures in Computer Graphics}{}{Application: Hidden Figures in Computer Graphics}{}{}{x:section:sec_appl_figs_computer}
In video games, the speed at which a computer can render changing graphics views is vitally important. To increase a computer's ability to render a scene, programs often try to identify those parts of the images a viewer could see and those parts the viewer could not see. For example, in a scene involving buildings, the viewer could not see any images blocked by a solid building. In the mathematical world, this can be visualized by graphing surfaces. In \hyperref[x:figure:F_House]{Figure~{\xreffont\ref{x:figure:F_House}}} we see a crude image of a house made up of small polygons (this is how programs generally represent surfaces). On the left in \hyperref[x:figure:F_House]{Figure~{\xreffont\ref{x:figure:F_House}}} we see all of the polygons that are needed to construct the entire surface, even those polygons that lie behind others which we could not see if the surface was solid. On the right in \hyperref[x:figure:F_House]{Figure~{\xreffont\ref{x:figure:F_House}}} we have hidden the parts of the polygons that we cannot see from our view.%
\begin{figureptx}{Images of a house.}{x:figure:F_House}{}%
\begin{image}{0.25}{0.5}{0.25}%
\includegraphics[width=\linewidth]{external/6_a_House.pdf}
\end{image}%
\tcblower
\end{figureptx}%
We also see this idea in mathematics when we graph surfaces. \hyperref[x:figure:F_Surface]{Figure~{\xreffont\ref{x:figure:F_Surface}}} shows the graph of the surface defined by \(f(x,y) = \sqrt{4-x^2}\) that is made up of polygons. At left we see all of the polygons and at right only those parts that would be visible from our viewing perspective.%
\begin{figureptx}{Graphs of \(f(x,y) = \sqrt{4-x^2}\).}{x:figure:F_Surface}{}%
\begin{sidebyside}{2}{0.1}{0.1}{0.2}%
\begin{sbspanel}{0.3}%
\includegraphics[width=\linewidth]{external/6_a_Surface_1.pdf}
\end{sbspanel}%
\begin{sbspanel}{0.3}%
\includegraphics[width=\linewidth]{external/6_a_Surface_2.pdf}
\end{sbspanel}%
\end{sidebyside}%
\tcblower
\end{figureptx}%
By eliminating the parts of the polygons we cannot see from our viewing perspective, the computer program can more quickly render the viewing image. Later in this section we will explore one method for how programs remove the hidden portions of images. This process involves the dot product of vectors.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_dot_prod_intro}
Orthogonality, a concept which generalizes the idea of perpendicularity, is an important concept in linear algebra. We use the dot product to define orthogonality and more generally angles between vectors in \(\R^n\) for any dimension \(n\). The dot product has many applications, e.g., finding components of forces acting in different directions in physics and engineering. The dot product is also an example of a larger concept, \terminology{inner products}, that we will discuss later. We introduce and investigate dot products in this section.%
\par
We will illustrate the dot product in \(\R^2\), but the process we go through will translate to any dimension. Recall that we can represent the vector \(\vv = \left[ \begin{array}{c} v_1 \\ v_2 \end{array}  \right]\) as the directed line segment (or arrow) from the origin to the point \((v_1, v_2)\) in \(\R^2\), as illustrated in \hyperref[x:figure:F_6_a_Vector_norm]{Figure~{\xreffont\ref{x:figure:F_6_a_Vector_norm}}}. Using the Pythagorean Theorem we can then define the length (or magnitude or norm) of the vector \(\vv\) in \(\R^2\) as%
\begin{equation*}
|| \vv || = \sqrt{v_1^2 + v_2^2}\text{.}
\end{equation*}
%
\par
We can also write this norm as%
\begin{equation*}
\sqrt{v_1v_1 + v_2v_2}\text{.}
\end{equation*}
%
\par
The expression under the square root is an important one and we extend it and give it a special name.%
\begin{figureptx}{A vector in \(\R^2\) from the origin to a point.}{x:figure:F_6_a_Vector_norm}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/6_a_Vector_norm.pdf}
\end{image}%
\tcblower
\end{figureptx}%
If \(\vu = [ u_1 \ u_2 ]^{\tr}\) and \(\vv = [ v_1 \ v_2]^{\tr}\) are vectors in \(\R^2\), then we call the expression \(u_1v_1+u_2v_2\) the \terminology{dot product} of \(\vu\) and \(\vv\), and denote it as \(\vu \cdot \vv\). With this idea in mind, we can rewrite the norm of the vector \(\vv\) as%
\begin{equation*}
|| \vv || = \sqrt{\vv \cdot \vv}\text{.}
\end{equation*}
%
\par
The definition of the dot product translates naturally to \(\R^n\) (see \hyperlink{x:exercise:ex_1_e_scalar_product}{Exercise~{\xreffont 5}} in \hyperref[x:chapter:chap_matrix_vector]{Section~{\xreffont\ref{x:chapter:chap_matrix_vector}}}).%
\begin{definition}{}{x:definition:def_6_a_dot_product}%
\index{dot product}%
\index{scalar product}%
Let \(\vu = [u_1 \ u_2 \ \cdots \ u_n]\) and \(\vv = [ v_1 \ v_2 \ \cdots \ v_n ]\) be vectors in \(\R^n\). The \terminology{dot product} (or \terminology{scalar product} ) of \(\vu\) and \(\vv\) is the scalar%
\begin{equation*}
\vu \cdot \vv = u_1v_1 + u_2v_2 + \cdots + u_nv_n = \displaystyle \sum_{i=1}^n u_iv_i\text{.}
\end{equation*}
%
\end{definition}
The dot product then allows us to define the norm (or magnitude or length) of any vector in \(\R^n\).%
\begin{definition}{}{x:definition:def_6_a_length_Rn}%
\index{vector!norm in \(\R^n\)}%
The \terminology{norm} \(||\vv||\) of the vector \(\vv \in \R^n\) is%
\begin{equation*}
||\vv|| = \sqrt{\vv \cdot \vv}\text{.}
\end{equation*}
%
\end{definition}
\index{vector!magnitude in \(\R^n\)}\index{vector!length in \(\R^n\)} We also use the words \terminology{magnitude} or \terminology{length} as alternatives for the word norm. We can equivalently write the norm of the vector \(\vv = [ v_1 \ v_2 \ \cdots \ v_n ]^{\tr}\) as%
\begin{equation*}
||\vv|| = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}\text{.}
\end{equation*}
%
\par
We can also realize the dot product as a matrix product. If \(\vu = [ u_1 \ u_2 \ \cdots \ u_n ]^{\tr}\) and \(\vv = [  v_1 \ v_2 \ \cdots \ v_n ]^{\tr}\), then%
\begin{equation*}
\vu \cdot \vv = \vu^{\tr}\vv
\end{equation*}
\footnote{Technically, \(\vu^{\tr}\vv\) is a \(1 \times 1\) matrix and not a scalar, but we usually think of \(1 \times 1\) matrices as scalars.\label{g:fn:idp18844712}}.%
\begin{paragraphs}{IMPORTANT NOTE.}{g:paragraphs:idp18848680}%
The dot product is only defined between two vectors with the \emph{same number of components}.%
\end{paragraphs}%
\begin{exploration}{}{x:exploration:pa_6_a}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find \(\vu \cdot \vv\) if \(\vu = [2 \ 3 \ -1 \ 4]^{\tr}\) and \(\vv = [4 \ 6 \ 7 \ -5]^{\tr}\) in \(\R^4\).%
\item{}The dot product satisfies some useful properties as given in the next theorem.%
\begin{theorem}{}{}{x:theorem:thm_6_a_dot_product}%
Let \(\vu\), \(\vv\), and \(\vw\) be vectors in \(\R^n\), and let \(c\) be a scalar. Then%
\begin{itemize}[label=\textbullet]
\item{}\(\vu \cdot \vv = \vv \cdot \vu\) (the dot product is \terminology{commutative}),%
\item{}\((\vu + \vv) \cdot \vw = (\vu \cdot \vw) + (\vv \cdot \vw)\) (the dot product \terminology{distributes over vector addition}),%
\item{}\((c\vu) \cdot \vv = \vu \cdot (c\vv) = c(\vu \cdot \vv)\),%
\item{}\(\vu \cdot \vu \geq 0\) with equality if and only if \(\vu = \vzero\),%
\item{}\(||c \vu || = |c| ||\vu||\).%
\end{itemize}
%
\end{theorem}
Verification of some of these properties is left to the exercises. Let \(\vu\) and \(\vv\) be vectors in \(\R^5\) with \(\vu \cdot \vv = -1\), \(|| \vu || = 2\) and \(|| \vv || = 3\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Use property (c) of \hyperref[x:theorem:thm_6_a_dot_product]{Theorem~{\xreffont\ref{x:theorem:thm_6_a_dot_product}}} to determine the value of \(\vu \cdot 2\vv\).%
\item{}Use property (b) of \hyperref[x:theorem:thm_6_a_dot_product]{Theorem~{\xreffont\ref{x:theorem:thm_6_a_dot_product}}} to determine the value of \((\vu + \vv) \cdot \vv\).%
\item{}Use whatever properties of \hyperref[x:theorem:thm_6_a_dot_product]{Theorem~{\xreffont\ref{x:theorem:thm_6_a_dot_product}}} that are needed to determine the value of \((2\vu+4\vv) \cdot (\vu - 7\vv)\).%
\end{enumerate}
\item{}At times we will want to find vectors in the direction of a given vector that have a certain magnitude. Let \(\vu = [2 \ 2 \ 1]^{\tr}\) in \(\R^3\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}What is \(|| \vu ||\)?%
\item{}Show that \(\left| \left| \frac{1}{||\vu||} \vu \right| \right| = 1\).%
\item{}Vectors with magnitude 1 are important and are given a special name. \begin{definition}{}{x:definition:def_6_a_unit_vector}%
\index{unit vector}A vector \(\vv\) in \(\R^n\) is a \terminology{unit vector} if \(|| \vv || = 1\).%
\end{definition}
%
\par
We can use unit vectors to find vectors of a given length in the direction of a given vector. Let \(c\) be a positive scalar and \(\vv\) a vector in \(\R^n\). Use properties from \hyperref[x:theorem:thm_6_a_dot_product]{Theorem~{\xreffont\ref{x:theorem:thm_6_a_dot_product}}} to show that the magnitude of the vector \(c \frac{\vv}{||\vv||}\) is \(c\).%
\end{enumerate}
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Distance Between Vectors}
\typeout{************************************************}
%
\begin{sectionptx}{The Distance Between Vectors}{}{The Distance Between Vectors}{}{}{x:section:sec_dist_vec}
Finding optimal solutions to systems is an important problem in applied mathematics. It is often the case that we cannot find an exact solution that satisfies certain constraints, so we look instead for the ``best'' solution that satisfies the constraints. An example of this is fitting a least squares line to a set of data. As we will see, the dot product will allow us to find ``best'' solutions to certain types of problems, where we measure accuracy using the notion of a distance between vectors. Geometrically, we can represent a vector \(\vu\) as a directed line segment from the origin to the point defined by \(\vu\). If we have two vectors \(\vu\) and \(\vv\), we can think of the length of the difference \(\vu - \vv\) as a measure of how far apart the two vectors are from each other. It is natural, then to define the distance between vectors as follows.%
\begin{definition}{}{x:definition:def_6_a_distance}%
\index{distance between vectors}%
Let \(\vu\) and \(\vv\) be vectors in \(\R^n\). The \terminology{distance} between \(\vu\) and \(\vv\) is the length of the difference \(\vu - \vv\) or%
\begin{equation*}
|| \vu - \vv ||\text{.}
\end{equation*}
%
\end{definition}
As \hyperref[x:figure:F_6_a_vector_difference]{Figure~{\xreffont\ref{x:figure:F_6_a_vector_difference}}} illustrates, if vectors \(\vu\) and \(\vv\) emanate from the same initial point, and \(P\) and \(Q\) are the terminal points of \(\vu\) and \(\vv\), respectively, then the difference \(|| \vu - \vv||\) is the standard Euclidean distance between the points \(P\) and \(Q\).%
\begin{figureptx}{\(|| \vu - \vv||\).}{x:figure:F_6_a_vector_difference}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/6_a_vector_difference.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Angle Between Two Vectors}
\typeout{************************************************}
%
\begin{sectionptx}{The Angle Between Two Vectors}{}{The Angle Between Two Vectors}{}{}{x:section:sec_angle_vec}
Determining a ``best'' solution to a problem often involves finding a solution that minimizes a distance. We generally accomplish a minimization through orthogonality \textemdash{} which depends on the angle between vectors. Given two vectors \(\vu\) and \(\vv\) in \(\R^n\), we position the vectors so that they emanate from the same initial point. If the vectors are nonzero, then they determine a plane in \(\R^n\). In that plane there are two angles that these vectors create. We will define the angle between the vectors to be the smaller of these two angles. The dot product will tell us how to find the angle between vectors. Let \(\vu\) and \(\vv\) be vectors in \(\R^n\) and \(\theta\) the angle between them as illustrated in \hyperref[x:figure:F_Angle]{Figure~{\xreffont\ref{x:figure:F_Angle}}}.%
\begin{figureptx}{The angle between \(\vu\) and \(\vv\)}{x:figure:F_Angle}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/6_a_Angle.pdf}
\end{image}%
\tcblower
\end{figureptx}%
Using the Law of Cosines, we have%
\begin{equation*}
||\vu-\vv||^2=||\vu||^2+||\vv||^2 - 2||\vu|| \ ||\vv|| \ \cos(\theta) \,\text{.}
\end{equation*}
%
\par
Rearranging, we obtain%
\begin{align*}
||\vu|| \ ||\vv|| \ \cos(\theta) \amp = \frac{1}{2} \left( ||\vu||^2+||\vv||^2 - ||\vu-\vv||^2 \right)\\
\amp = \frac{1}{2} (||\vu||^2+||\vv||^2 - (\vu-\vv)\cdot (\vu-\vv) )\\
\amp = \frac{1}{2} (||\vu||^2+||\vv||^2 - \vu\cdot \vu +2\vu \cdot \vv - \vv\cdot \vv )\\
\amp = \vu \cdot \vv  \, \text{.}
\end{align*}
%
\par
\index{angle between vectors} So the angle \(\theta\) between two nonzero vectors \(\vu\) and \(\vv\) in \(\R^n\) satisfies the equation%
\begin{equation}
\cos(\theta) = \frac{\vu \cdot \vv}{||\vu|| \ ||\vv||}\text{.}\label{x:men:eq_6_a_angle_between}
\end{equation}
%
\par
Of particular interest to us will be the situation where vectors \(\vu\) and \(\vv\) are \terminology{orthogonal} (perpendicular).\footnote{We use the term orthogonal instead of perpendicular because we will be able to extend this idea to situations where we normally don't think of objects as being perpendicular.\label{g:fn:idp18902952}} Intuitively, we think of two vectors as orthogonal if the angle between them is \(90^{\circ}\).%
\begin{activity}{}{x:activity:act_6_a_orthogonality}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}The vectors \(\ve_1 = [ 1 \ 0]^{\tr}\) and \(\ve_2 = [0 \ 1]^{\tr}\) are perpendicular in \(\R^2\). What is \(\ve_1 \cdot \ve_2\)?%
\item{}Now let \(\vu\) and \(\vv\) be any vectors in \(\R^n\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Suppose the angle between nonzero vectors \(\vu\) and \(\vv\) is \(90^{\circ}\). What does Equation \hyperref[x:men:eq_6_a_angle_between]{({\xreffont\ref{x:men:eq_6_a_angle_between}})} tell us about \(\vu \cdot \vv\)?%
\item{}Now suppose that \(\vu \cdot \vv = 0\). What does Equation \hyperref[x:men:eq_6_a_angle_between]{({\xreffont\ref{x:men:eq_6_a_angle_between}})} tell us about the angle between \(\vu\) and \(\vv\)? Why?%
\item{}Explain why the following definition makes sense. \begin{definition}{}{x:definition:def_6_a_orthogonal_dot_product}%
\index{orthogonal}Two vectors \(\vu\) and \(\vv\) in \(\R^n\) are \terminology{orthogonal} if \(\vu \cdot \vv = 0\).%
\end{definition}
%
\item{}According to \hyperref[x:definition:def_6_a_orthogonal_dot_product]{Definition~{\xreffont\ref{x:definition:def_6_a_orthogonal_dot_product}}}, to which vectors is \(\vzero\) orthogonal? Does this make sense to you intuitively? Explain.%
\end{enumerate}
\end{enumerate}
\end{activity}%
\begin{activity}{}{g:activity:idp18915336}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the angle between the two vectors \(\vu = [1 \ 3 \ -2 \ 5]^{\tr}\) and \(\vv = [5 \ 2 \ 3 \ -1]^{\tr}\).%
\item{}Find, if possible, two non-parallel vectors orthogonal to \(\vu = \left[ \begin{array}{r} 0\\3\\-2\\1 \end{array} \right]\).%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Orthogonal Projections}
\typeout{************************************************}
%
\begin{sectionptx}{Orthogonal Projections}{}{Orthogonal Projections}{}{}{x:section:sec_orthog_proj}
When running a sprint, the racers may be aided or slowed by the wind. The wind assistance is a measure of the wind speed that is helping push the runners down the track. It is much easier to run a very fast race if the wind is blowing hard in the direction of the race. So that world records aren't dependent on the weather conditions, times are only recorded as record times if the wind aiding the runners is less than or equal to 2 meters per second. Wind speed for a race is recorded by a wind gauge that is set up close to the track. It is important to note, however, that weather is not always as cooperative as we might like. The wind does not always blow exactly in the direction of the track, so the gauge must account for the angle the wind makes with the track. If the wind is blowing in the direction of the vector \(\vu\) in \hyperref[x:figure:F_Projection]{Figure~{\xreffont\ref{x:figure:F_Projection}}} and the track is in the direction of the vector \(\vv\) in \hyperref[x:figure:F_Projection]{Figure~{\xreffont\ref{x:figure:F_Projection}}}, then only part of the total wind vector is actually working to help the runners. This part is called the orthogonal projection of the vector \(\vu\) onto the vector \(\vv\) and is denoted \(\proj_{\vv} \vu\). The next activity shows how to find this projection.%
\begin{figureptx}{The orthogonal projection of \(\vu\) onto \(\vv\).}{x:figure:F_Projection}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/6_a_Projection.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\begin{activity}{}{g:activity:idp18920072}%
Since the orthogonal projection \(\proj_{\vv} \vu\) is in the direction of \(\vv\), there exists a constant \(c\) such that \(\proj_{\vv} \vu = c \vv\). If we determine the value of \(c\), we can find \(\proj_{\vv} \vu\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}The wind component that acts perpendicular to the direction of \(\vv\) is called the projection of \(\vu\) orthogonal to \(\vv\) and is denoted \(\proj_{\perp \vv} \vu\) as shown in \hyperref[x:figure:F_Projection]{Figure~{\xreffont\ref{x:figure:F_Projection}}}. Write an equation that involves \(\proj_{\vv} \vu\), \(\proj_{\perp \vv} \vu\), and \(\vu\). Then solve that equation for \(\proj_{\perp \vv} \vu\).%
\item{}Given that \(\vv\) and \(\proj_{\perp \vv} \vu\) are orthogonal, what does that tell us about \(\vv \cdot \proj_{\perp \vv} \vu\)? Combine this fact with the result of part (a) and that \(\proj_{\vv} \vu = c \vv\) to obtain an equation involving \(\vv\), \(\vu\), and \(c\).%
\item{}Solve for \(c\) using the equation you found in the previous step.%
\item{}Use your value of \(c\) to identify \(\proj_{\vv} \vu\).%
\end{enumerate}
\end{activity}%
To summarize:%
\begin{definition}{}{g:definition:idp18940936}%
\index{orthogonal projection!in the direction of a vector}%
\index{projection!orthogonal to a vector}%
Let \(\vu\) and \(\vv\) be vectors in \(\R^n\) with \(\vv \neq \vzero\).%
\begin{enumerate}
\item{}The \terminology{orthogonal projection} of \(\vu\) onto \(\vv\) is the vector%
\begin{equation}
\proj_{\vv} \vu = \frac{\vu \cdot \vv}{||\vv||^2} \vv\text{.}\label{x:men:eq_6_a_projection}
\end{equation}
%
\item{}The \terminology{projection} of \(\vu\) orthogonal to \(\vv\) is the vector%
\begin{equation*}
\proj_{\perp \vv} \vu = \vu - \proj_{\vv} \vu\text{.}
\end{equation*}
%
\end{enumerate}
%
\end{definition}
\begin{activity}{}{g:activity:idp18943496}%
Let \(\vu = \left[ \begin{array}{c} 5\\8 \end{array} \right]\) and \(\vv = \left[ \begin{array}{r} 6\\-10 \end{array} \right]\). Find \(\proj_{\vv} \vu\) and \(\proj_{\perp \vv} \vu\) and draw a picture to illustrate.%
\end{activity}%
The orthogonal projection of a vector \(\vu\) onto a vector \(\vv\) is really a projection of the vector \(\vu\) onto the space \(\Span\{\vv\}\). The vector \(\proj_{\vv} \vu\) is the best approximation to \(\vu\) of all the vectors in \(\Span\{\vv\}\) in the sense that \(\proj_{\vv} \vu\) is the closest to \(\vu\) among all vectors in \(\Span\{\vv\}\), as we will prove later.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Orthogonal Complements}
\typeout{************************************************}
%
\begin{sectionptx}{Orthogonal Complements}{}{Orthogonal Complements}{}{}{x:section:sec_orthog_comp}
\index{normal vector} In \hyperref[x:activity:act_6_a_orthogonality]{Activity~{\xreffont\ref{x:activity:act_6_a_orthogonality}}} we defined two vectors \(\vu\) and \(\vv\) in \(\R^n\) to be orthogonal (or perpendicular) if \(\vu \cdot \vv = 0\). A use of orthogonality in geometry is to define a plane. A plane through the origin in \(\R^3\) is a two dimensional subspace of \(\R^3\), and a plane is defined to be the set of all vectors in \(\R^3\) that are orthogonal to a given vector (called a \terminology{normal} vector). For example, to find the equation of the plane through the origin in \(\R^3\) orthogonal to the normal vector \(\vn = [1 \ 2 \ -1]^{\tr}\), we seek all the vectors \(\vv = [x \  y \ z]^{\tr}\) such that%
\begin{equation*}
\vv \cdot \vn = \vzero\text{.}
\end{equation*}
%
\par
This gives us the equation%
\begin{equation*}
x+2y-z = 0
\end{equation*}
as the equation of this plane. The collection of all vectors that are orthogonal to a given subspace of vectors is called the \terminology{orthogonal complement} of that subspace in \(\R^n\).%
\begin{definition}{}{x:definition:def_6_a_orth_complement}%
\index{orthogonal complement}%
Let \(W\) be a subspace of \(\R^n\) for some \(n \geq 1\). The \terminology{orthogonal complement} of \(W\) is the set%
\begin{equation*}
W^{\perp} = \{\vx \in \R^n : \vx \cdot \vw = 0 \text{ for all }  \vw \in W\}\text{.}
\end{equation*}
%
\end{definition}
\begin{exploration}{}{x:exploration:pa_6_a_2}%
Let \(W = \Span\{[1 \ -1]^{\tr}\}\) in \(\R^2\). Completely describe all vectors in \(W^{\perp}\) both algebraically and geometrically.%
\end{exploration}%
There is a more general idea here as defined in \hyperref[x:exploration:pa_6_a_2]{Preview Activity~{\xreffont\ref{x:exploration:pa_6_a_2}}}. If we have a set \(S\) of vectors in \(\R^n\), we let \(S^{\perp}\) (read as ``\(S\) perp'', called the \terminology{orthogonal complement} of \(S\)) be the set of all vectors in \(\R^n\) that are orthogonal to every vector in \(S\). In our plane example, the set \(S\) is \(\{\vn\}\) and \(S^{\perp}\) is the plane with equation \(x+2y-z=0\).%
\begin{activity}{}{g:activity:idp18970248}%
We have seen another example of orthogonal complements. Let \(A\) be an \(m \times n\) matrix with rows \(\vr_1\), \(\vr_2\) , \(\ldots\), \(\vr_m\) in order. Consider the three spaces \(\Nul A\), \(\Row A\), and \(\Col A\) related to \(A\), where \(\Row A = \Span\{\vr_1, \vr_2, \ldots, \vr_m\}\) (that is, \(\Row A\) is the span of the rows of \(A\)). Let \(\vx\) be a vector in \(\Row A\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}What does it mean for \(\vx\) to be in \(\Row A\)?%
\item{}Now let \(\vy\) be a vector in \(\Nul A\). Use the result of part (a) and the fact that \(A \vy = \vzero\) to explain why \(\vx \cdot \vy = 0\). Explain how this verifies \((\Row A)^{\perp} = \Nul A\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp18977928}{}\quad{}Calculate \(A \vy\) using scalar products of rows of \(A\) with \(\vy\).%
\item{}Use \(A^\tr\) in place of \(A\) in the result of the previous part to show \((\Col A)^{\perp} = \Nul A^{\tr}\).%
\end{enumerate}
\end{activity}%
The activity proves the following theorem:%
\begin{theorem}{}{}{x:theorem:thm_6_a_orthogonal_subspaces}%
Let \(A\) be an \(m \times n\) matrix. Then%
\begin{equation*}
(\Row A)^{\perp} = \Nul A \text{ and }  (\Col A)^{\perp} = \Nul A^{\tr}\text{.}
\end{equation*}
%
\end{theorem}
To show that a vector is in the orthogonal complement of a subspace, it is not necessary to demonstrate that the vector is orthogonal to every vector in the subspace. If we have a basis for the subspace, it suffices to show that the vector is orthogonal to every vector in that basis for the subspace, as the next theorem demonstrates.%
\begin{theorem}{}{}{x:theorem:thm_6_a_dot_pd_orth_complement_basis}%
Let \(\CB = \{\vw_1, \vw_2, \ldots, \vw_m\}\) be a basis for a subspace \(W\) of \(\R^n\). A vector \(\vv\) in \(\R^n\) is orthogonal to every vector in \(W\) if and only if \(\vv\) is orthogonal to every vector in \(\CB\).%
\end{theorem}
\begin{proof}{}{g:proof:idp18982920}
Let \(\CB = \{\vw_1, \vw_2, \ldots, \vw_m\}\) be a basis for a subspace \(W\) of \(\R^n\) and let \(\vv\) be a vector in \(\R^n\). Our theorem is a biconditional, so we need to prove both implications. Since \(\CB \subset W\), it follows that if \(\vv\) is orthogonal to every vector in \(W\), then \(\vv\) is orthogonal to every vector in \(\CB\). This proves the forward implication. Now we assume that \(\vv\) is orthogonal to every vector in \(\CB\) and show that \(\vv\) is orthogonal to every vector in \(W\). Let \(\vx\) be a vector in \(W\). Then%
\begin{equation*}
\vx = x_1\vw_1 + x_2\vw_2 + \cdots + x_m\vw_m
\end{equation*}
for some scalars \(x_1\), \(x_2\), \(\ldots\), \(x_m\). Then%
\begin{align*}
\vv \cdot \vx \amp = \vv \cdot (x_1\vw_1 + x_2\vw_2 + \cdots + x_m\vw_m)\\
\amp = x_1(\vv \cdot \vw_1) + x_2(\vv \cdot \vw_2) + \cdots + x_m(\vv \cdot \vw_m)\\
\amp = 0\text{.}
\end{align*}
%
\par
Thus, \(\vv\) is orthogonal to \(\vx\) and \(\vv\) is orthogonal to every vector in \(W\).%
\end{proof}
\begin{activity}{}{g:activity:idp19006216}%
Let \(W = \Span\left\{ \left[ \begin{array}{c} 1 \\ 1 \\ 0 \end{array} \right], \left[ \begin{array}{c} 0 \\ 0 \\ 1 \end{array} \right] \right\}\). Find all vectors in \(W^{\perp}\).%
\end{activity}%
We will work more closely with projections and orthogonal complements in later sections.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_dot_prod_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp18999944}%
Let \(\ell\) be the line defined by the equation \(ax+by+c=0\) with in \(\R^2\) and let \(P = (x_0,y_0)\) be a point in the plane. In this example we will learn how to find the distance from \(P\) to \(\ell\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(\vn = [a \ b]^{\tr}\) is orthogonal to the line \(\ell\). That is, \(\vn\) is orthogonal to any vector on the line \(\ell\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp19012488}{}\quad{}Any vector on the line \(\ell\) is a  vector between two points on the line. Let \(Q = (x_1,y_1)\) and \(R = (x_2,y_2)\) be points on the line \(\ell\). Then \(\vu = \overrightarrow{QR} = [x_2-x_1 \ y_2-y_1]^{\tr}\) is a vector on line \(\ell\). Since \(Q\) and \(R\) are on the line, we know that \(ax_1+by_1+c=0\) and \(ax_2+by_2+c = 0\). So \(-c = ax_1+by_1 = ax_2+by_2\) and%
\begin{equation*}
0 = a(x_2-x_1) + b(y_2-y_1) = [a \ b]^{\tr} \vu\text{.}
\end{equation*}
Thus, \(\vn = [a \ b]^{\tr}\) is orthogonal to every vector on the line \(\ell\).%
\item{}Let \(Q = (x_1, y_1)\) be any point on line \(\ell\). Draw a representative picture of \(P\), \(\vn\) with its initial point at \(P\), along with \(Q\) and \(\ell\). Explain how to use a projection to determine the distance from \(P\) to \(\ell\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp19023240}{}\quad{}A picture of the situation is shown in \hyperref[x:figure:F_dist_point_line]{Figure~{\xreffont\ref{x:figure:F_dist_point_line}}}. If \(\vv = \overrightarrow{PQ}\), then the distance from point \(P\) to line \(\ell\) is given by \(||\proj_{\vn} \vv||\). \begin{figureptx}{Distance from a point to a line.}{x:figure:F_dist_point_line}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/6_a_distance_line.pdf}
\end{image}%
\tcblower
\end{figureptx}%
%
\item{}Use the idea from part (b) to show that the distance \(d\) from \(P\) to \(\ell\) satisfies%
\begin{equation}
d = \frac{|ax_0+by_0+c|}{\sqrt{a^2+b^2}}\text{.}\label{x:men:eq_dist_point_line}
\end{equation}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp19022600}{}\quad{}Recall that \(\vn = [a \ b]^{\tr}\) and \(\vv = \overrightarrow{PQ} = [x_1-x_0 \ y_1-y_0]^{\tr}\). Since \(ax_1+by_1+c = 0\), we have%
\begin{align*}
\proj_{\vn} \vv \amp = \frac{\vv \cdot \vv}{||\vn||^2} \vn\\
\amp = \frac{a(x_1-x_0)+ b(y_1-y_0)}{a^2+b^2} [a \ b]^{\tr}\\
\amp = \frac{ax_1+by_1-ax_0-by_0}{a^2+b^2} [a \ b]^{\tr}\\
\amp = \frac{-c-ax_0-by_0}{a^2+b^2} [a \ b]^{\tr}\text{.}
\end{align*}
So%
\begin{equation*}
||\proj_{\vn} \vv|| = \frac{|ax_0+by_0+c|}{a^2+b^2} \sqrt{a^2+b^2} = \frac{|ax_0+by_0+c|}{\sqrt{a^2+b^2}}\text{.}
\end{equation*}
%
\item{}Use Equation \hyperref[x:men:eq_dist_point_line]{({\xreffont\ref{x:men:eq_dist_point_line}})} to find the distance from the point \((3,4)\) to the line \(y = 2x+1\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp19023880}{}\quad{}Here we have \(P = (3,4)\), and the equation of our line is \(2x-y+1=0\). So \(a=2\), \(b=-1\), and \(c=-1\). Thus, the distance from \(P\) to the line is%
\begin{equation*}
\frac{|ax_0+by_0+c|}{\sqrt{a^2+b^2}} = \frac{|2(3)-(4)+1|}{\sqrt{4+1}} = \frac{3}{\sqrt{5}}\text{.}
\end{equation*}
%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp19025928}%
Let \(a\), \(b\), and \(c\) be scalars with \(a \neq 0\), and let%
\begin{equation*}
W = \{ax+by+cz=0 : x,y,z \in \R\} \,\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find two vectors that span \(W\), showing that \(W\) is a subspace of \(\R^3\). (In fact, \(W\) is a plane through the origin in \(\R^3\).)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp19038856}{}\quad{}The coefficient matrix for the system \(ax+by+cz = 0\) is \([a \ b \ c]^{\tr}\). The first column is a pivot column and the others are not. So \(y\) and \(z\) are free variables and%
\begin{equation*}
[x \ y \ z]^{\tr} = \left[-\frac{b}{a}y - \frac{c}{a}z, y, z\right]^{\tr} = y\left[ -\frac{b}{a} \ 1 \ 0\right]^{\tr} + z\left[ -\frac{c}{a} \ 0 \ 1\right]^{\tr}\text{.}
\end{equation*}
So \(W = \Span\left\{\left[ -\frac{b}{a} \ 1 \ 0\right]^{\tr}, \left[ -\frac{c}{a} \ 0 \ 1\right]^{\tr}\right\}\).%
\item{}Find a vector \(\vn\) that is orthogonal to the two vectors you found in part (a).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp19034504}{}\quad{}If we let \(\vn = [a \ b \ c]^{\tr}\), then%
\begin{align*}
\vn \cdot \left[ -\frac{b}{a} \ 1 \ 0\right]^{\tr} \amp = -b+b = 0\\
\vn \cdot \left[ -\frac{c}{a} \ 0 \ 1\right]^{\tr}\ \amp = -c+c = 0\text{.}
\end{align*}
Thus, \([a \ b \ c]^{\tr}\) is orthogonal to both \(\left[ -\frac{b}{a} \ 1 \ 0\right]^{\tr}\) and \(\left[ -\frac{c}{a} \ 0 \ 1\right]^{\tr}\).%
\item{}Explain why \(\{\vn\}\) is a basis for \(W^{\perp}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp19043592}{}\quad{}Let \(\vu = \left[ -\frac{b}{a} \ 1 \ 0\right]^{\tr}\) and \(\vv = \left[ -\frac{c}{a} \ 0 \ 1\right]^{\tr}\). Every vector in \(W\) has the form \(x\vu + y\vv\) for some scalars \(x\) and \(y\), and%
\begin{equation*}
\vn \cdot (x\vu + y\vv) = x(\vn \cdot \vu) + y (\vn \cdot \vv) = 0\text{.}
\end{equation*}
So \(\vn \in W^{\perp}\). Now we need to verify that \(\{\vn\}\) spans \(W^{\perp}\). Let \(\vw = [w_1 \ w_2 \ w_3]^{\tr}\) be in \(W^{\perp}\). Then \(\vw \cdot \vz = 0\) for every \(\vz \in W\). In particular, \(\vw \cdot \vu = 0\) or \(-\frac{b}{a}w_1 + w_2 = 0\), and \(\vw \cdot \vv = 0\) or \(-\frac{c}{a}w_1 + w_3 = 0\). Equivalently, we have \(w_2 = \frac{b}{a}w_1\) and \(w_3 = \frac{c}{a}w_1\). So%
\begin{align*}
\vw \amp =  [w_1 \ w_2 \ w_3]^{\tr}\\
\amp = \left[ w_1 \ \frac{b}{a}w_1 \ \frac{c}{a}w_1\right]^{\tr}\\
\amp = \frac{1}{a}[a \ b \ c]^{\tr}w_1\\
\amp = \frac{w_1}{a} \vn\text{.}
\end{align*}
So every vector in \(W^{\perp}\) is a multiple of \(\vn\), and \(\{\vn\}\) spans \(W^{\perp}\). We conclude that \(\{\vn\}\) is a basis for \(W^{\perp}\). Thus, the vector \([a \ b \ c]^{\tr}\) is a normal vector to the plane \(ax+by+cz=0\) if \(a \neq 0\). The same reasoning works if at least one of \(a\), \(b\),or \(c\) is nonzero, so we can say in every case that \([a \ b \ c]^{\tr}\) is a normal vector to the plane \(ax+by+cz=0\).%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_dot_prod_summ}
%
\begin{itemize}[label=\textbullet]
\item{}The dot product of vectors \(\vu = [u_1 \ u_2 \  \cdots  \ u_n ]^{\tr}\) and \(\vv = [ v_1 \ v_2 \ \cdots \ v_n ]^{\tr}\) in \(\R^n\) is the scalar%
\begin{equation*}
\vu \cdot \vv = u_1v_1 + u_2v_2 + \cdots + u_nv_n = \displaystyle \sum_{i=1}^n u_iv_i\text{.}
\end{equation*}
%
\item{}The angle \(\theta\) between two nonzero vectors \(\vu\) and \(\vv\) in \(\R^n\) satisfies the equation%
\begin{equation*}
\cos(\theta) = \frac{\vu \cdot \vv}{||\vu|| \ ||\vv||}
\end{equation*}
and \(0\leq \theta \leq 180\).%
\item{}Two vectors \(\vu\) and \(\vv\) are orthogonal if \(\vu \cdot \vv = 0\).%
\item{}The length, or norm, of the vector \(\vu\) can be found as \(\ds || \vu || = \sqrt{\vu \cdot \vu}\).%
\item{}The distance between the vectors \(\vu\) and \(\vv\) in \(\R^n\) is \(||\vu - \vv ||\), which is the length of the difference \(\vu - \vv\).%
\item{}Let \(\vu\) and \(\vv\) be vectors in \(\R^n\).%
\begin{itemize}[label=$\circ$]
\item{}The orthogonal projection of \(\vu\) onto \(\vv\) is the vector%
\begin{equation*}
\proj_{\vv} \vu = \frac{\vu \cdot \vv}{||\vv||^2} \vv\text{.}
\end{equation*}
%
\item{}The projection of \(\vu\) perpendicular to \(\vv\) is the vector%
\begin{equation*}
\proj_{\perp \vv} \vu = \vu - \proj_{\vv} \vu\text{.}
\end{equation*}
%
\end{itemize}
%
\item{}The orthogonal complement of the subspace \(W\) of \(\R^n\) is the set%
\begin{equation*}
W^{\perp} = \{\vx \in \R^n : \vx \cdot \vw = 0 \text{ for all }  \vw \in W\}\text{.}
\end{equation*}
%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_dot_prod_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp19075080}%
For each of the following pairs of vectors, find \(\vu \cdot \vv\), calculate the angle between \(\vu\) and \(\vv\), determine if \(\vu\) and \(\vv\) are orthogonal, find \(||\vu||\) and \(||\vv||\), calculate the distance between \(\vu\) and \(\vv\), and determine the orthogonal projection of \(\vu\) onto \(\vv\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\vu = [1 \ 2]^{\tr}\), \(\vv = [-2 \ 1]^{\tr}\)%
\item{}\(\vu = [2 \ -2]^{\tr}\), \(\vv = [1 \ -1]^{\tr}\)%
\item{}\(\vu = [2 \ -1]^{\tr}\), \(\vv = [1 \ 3]^{\tr}\)%
\item{}\(\vu = [1 \ 2 \ 0]^{\tr}\), \(\vv = [-2 \ 1 \ 1]^{\tr}\)%
\item{}\(\vu = [0 \ 0 \ 1]^{\tr}\), \(\vv = [1 \ 1 \ 1]^{\tr}\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp19106184}%
Given \(\vu=[2 \ 1\ 2]^\tr\), find a vector \(\vv\) so that the angle between \(\vu\) and \(\vv\) is \(60^\circ\) and the orthogonal projection of \(\vv\) onto \(\vu\) has length 2.%
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp19105288}%
For which value(s) of \(h\) is the angle between \([1\ 1\ h]^\tr\) and \([1\ 2\ 1]^\tr\) equal to \(60^\circ\)?%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp19105672}%
Let \(A = [a_{ij}]\) be a \(k \times m\) matrix with rows \(\vr_1\), \(\vr_2\), \(\ldots\), \(\vr_k\), and let \(B = [\vb_1 \ \vb_2 \ \cdots \ \vb_n]\) be an \(m \times n\) matrix with columns \(\vb_1\), \(\vb_2\), \(\ldots\), \(\vb_n\). Show that we can write the matrix product \(AB\) in a shorthand way as \(AB = [\vr_i \cdot \vb_j]\).%
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp19115912}%
Let \(A\) be an \(m \times n\), \(\vu\) a vector in \(\R^n\) and \(\vv\) a vector in \(\R^m\). Show that%
\begin{equation*}
A\vu \cdot \vv = \vu \cdot A^{\tr} \vv\text{.}
\end{equation*}
%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp19127176}%
Let \(\vu\), \(\vv\), and \(\vw\) be vectors in \(\R^n\). Show that%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\((\vu + \vv) \cdot \vw = (\vu \cdot \vw) + (\vv \cdot \vw)\) (the dot product \terminology{distributes over vector addition})%
\item{}If \(c\) is an arbitrary constant, then \((c\vu) \cdot \vv = \vu \cdot (c\vv) = c(\vu \cdot \vv)\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{x:exercise:ex_Pyth_Thm}%
The Pythagorean Theorem states that if \(a\) and \(b\) are the lengths of the legs of a right triangle whose hypotenuse has length \(c\), then \(a^2+b^2=c^2\). If we think of the legs as defining vectors \(\vu\) and \(\vv\), then the hypotenuse is the vector \(\vu+\vv\) and we can restate the Pythagorean Theorem\index{Pythagorean Theorem in \(\R^n\)} as%
\begin{equation*}
||\vu+\vv||^2 = ||\vu||^2+||\vv||^2\text{.}
\end{equation*}
In this exercise we show that this result holds in any dimension.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(\vu\) and \(\vv\) be orthogonal vectors in \(\R^n\). Show that \(||\vu+\vv||^2 = ||\vu||^2+||\vv||^2\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19132424}{}\quad{}Rewrite \(||\vu+\vv||^2\) using the dot product.%
\item{}Must it be true that if \(\vu\) and \(\vv\) are vectors in \(\R^n\) with \(||\vu+\vv||^2 = ||\vu||^2+||\vv||^2\), then \(\vu\) and \(\vv\) are orthogonal? If not, provide a counterexample. If true, verify the statement.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{x:exercise:ex_Cauchy_Schwarz}%
\index{Cauchy-Schwarz inequality!in \(\R^n\)}%
The Cauchy-Schwarz inequality,%
\begin{equation}
|\vu \cdot \vv| \leq ||\vu|| \ \||\vv||\label{x:men:eq_6_a_Cauchy_Schwartz}
\end{equation}
for any vectors \(\vu\) and \(\vv\) in \(\R^n\), is considered one of the most important inequalities in mathematics. We verify the Cauchy-Schwarz inequality in this exercise. Let \(\vu\) and \(\vv\) be vectors in \(\R^n\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why the inequality \hyperref[x:men:eq_6_a_Cauchy_Schwartz]{({\xreffont\ref{x:men:eq_6_a_Cauchy_Schwartz}})} is true if either \(\vu\) or \(\vv\) is the zero vector. As a consequence, we assume that \(\vu\) and \(\vv\) are nonzero vectors for the remainder of this exercise.%
\item{}Let \(\vw = \proj_{\vv} \vu = \frac{\vu \cdot \vv}{||\vv||^2} \vv\) and let \(\vz = \vu - \vw\). We know that \(\vw \cdot \vz = 0\). Use \hyperlink{x:exercise:ex_Pyth_Thm}{Exercise~{\xreffont 7}} of this section to show that%
\begin{equation*}
||\vu||^2 \geq ||\vw||^2\text{.}
\end{equation*}
%
\item{}Now show that \(||\vw||^2 = \frac{|\vu \cdot \vv|^2}{||\vv||^2}\).%
\item{}Combine parts (b) and (c) to explain why equation \hyperref[x:men:eq_6_a_Cauchy_Schwartz]{({\xreffont\ref{x:men:eq_6_a_Cauchy_Schwartz}})} is true.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp19147784}%
\index{triangle inequality in \(\R^n\)}%
Let \(\vu\) and \(\vv\) be vectors in \(\R^n\). Then \(\vu\), \(\vv\) and \(\vu+\vv\) form a triangle. We should then expect that the length of any one side of the triangle is smaller than the sum of the lengths of the other sides (since the straight line distance is the shortest distance between two points). In other words, we expect that%
\begin{equation}
||\vu + \vv|| \leq ||\vu|| + ||\vv||\text{.}\label{x:men:eq_6_a_triangle_inequality}
\end{equation}
Equation \hyperref[x:men:eq_6_a_triangle_inequality]{({\xreffont\ref{x:men:eq_6_a_triangle_inequality}})} is called the \terminology{Triangle Inequality}. Use the Cauchy-Schwarz inequality (\hyperlink{x:exercise:ex_Cauchy_Schwarz}{Exercise~{\xreffont 8}}) to prove the triangle inequality.%
\end{divisionexercise}%
\begin{divisionexercise}{10}{}{}{g:exercise:idp19159816}%
Let \(W\) be a subspace of \(\R^n\) for some \(n\). Show that \(W^{\perp}\) is also a subspace of \(\R^n\).%
\end{divisionexercise}%
\begin{divisionexercise}{11}{}{}{g:exercise:idp19160968}%
Let \(W\) be a subspace of \(\R^n\). Show that \(W\) is a subspace of \((W^\perp)^\perp\).%
\end{divisionexercise}%
\begin{divisionexercise}{12}{}{}{g:exercise:idp19163528}%
If \(W\) is a subspace of \(\R^n\) for some \(n\), what is \(W \cap W^{\perp}\)? Verify your answer.%
\end{divisionexercise}%
\begin{divisionexercise}{13}{}{}{g:exercise:idp19166344}%
Suppose \(W_1\subseteq W_2\) are two subspaces of \(\R^n\). Show that \(W_2^\perp \subseteq W_1^\perp\).%
\end{divisionexercise}%
\begin{divisionexercise}{14}{}{}{g:exercise:idp19164296}%
What are \(\left(\R^n\right)^{\perp}\) and \(\{\vzero\}^{\perp}\) in \(\R^n\)? Justify your answers.%
\end{divisionexercise}%
\begin{divisionexercise}{15}{}{}{g:exercise:idp19164936}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
The dot product is defined between any two vectors.%
\item{}\lititle{True\slash{}False.}\par%
If \(\vu\) and \(\vv\) are vectors in \(\R^n\), then \(\vu \cdot \vv\) is another vector in \(\R^n\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\vu\) and \(\vv\) are vectors in \(\R^n\), then \(\vu \cdot \vv\) is always non-negative.%
\item{}\lititle{True\slash{}False.}\par%
If \(\vv\) is a vector in \(\R^n\), then \(\vv \cdot \vv\) is never negative.%
\item{}\lititle{True\slash{}False.}\par%
If \(\vu\) and \(\vv\) are vectors in \(\R^n\) and \(\vu \cdot \vv = 0\), then \(\vu = \vv = \vzero\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\vv\) is a vector in \(\R^n\) and \(\vv \cdot \vv = 0\), then \(\vv = \vzero\).%
\item{}\lititle{True\slash{}False.}\par%
The norm of the sum of vectors is the sum of the norms of the vectors.%
\item{}\lititle{True\slash{}False.}\par%
If \(\vu\) and \(\vv\) are vectors in \(\R^n\), then \(\proj_{\vv} \vu\) is a vector in the same direction as \(\vu\).%
\item{}\lititle{True\slash{}False.}\par%
The only subspace \(W\) of \(\R^n\) for which \(W^\perp=\{\vzero\}\) is \(W=\R^n\).%
\item{}\lititle{True\slash{}False.}\par%
If a vector \(\vu\) is orthogonal to \(\vv_1\) and \(\vv_2\), then \(\vu\) is also orthogonal to \(\vv_1+\vv_2\).%
\item{}\lititle{True\slash{}False.}\par%
If a vector \(\vu\) is orthogonal to \(\vv_1\) and \(\vv_2\), then \(\vu\) is also orthogonal to all linear combinations of \(\vv_1\) and \(\vv_2\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\vu\neq \vzero\) and \(\vv\) are parallel, then the orthogonal projection of \(\vv\) onto \(\vu\) equals \(\vv\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\vu\neq \vzero\) and \(\vv\) are orthogonal, then the orthogonal projection of \(\vv\) onto \(\vu\) equals \(\vv\).%
\item{}\lititle{True\slash{}False.}\par%
For any vector \(\vv\) and \(\vu\neq \vzero\), \(||\proj_\vu \vv|| \leq ||\vv||\).%
\item{}\lititle{True\slash{}False.}\par%
Given an \(m\times n\) matrix, \(\dim(\Row A)+\dim(\Row A)^\perp = n\).%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is a square matrix, then the columns of \(A\) are orthogonal to the vectors in \(\Nul A\).%
\item{}\lititle{True\slash{}False.}\par%
The vectors in the null space of an \(m \times n\) matrix are orthogonal to vectors in the row space of \(A\).%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Back-Face Culling}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Back-Face Culling}{}{Project: Back-Face Culling}{}{}{x:section:sec_proj_back_face}
\index{back-face culling}%
To identify hidden polygons in a surface, we will utilize a technique called \terminology{back face culling}. This involves identifying which polygons are back facing and which are front facing relative to the viewer's perspective. The first step is to assign a direction to each polygon in a surface.%
\begin{project}{}{x:project:act_bf_normal}%
\index{cross product}%
Consider the polygon \(ABCD\) in \hyperref[x:figure:F_Normal_vector]{Figure~{\xreffont\ref{x:figure:F_Normal_vector}}}. Since a polygon is flat, every vector in the polygon is perpendicular to a fixed vector (which we call a \terminology{normal vector} to the polygon). A normal vector \(\vn\) for the polygon \(ABCD\) in \hyperref[x:figure:F_Normal_vector]{Figure~{\xreffont\ref{x:figure:F_Normal_vector}}} is shown. In this activity we learn how to find a normal vector to a polygon.%
\par
Let \(\vx = [x_1 \ x_2 \ x_3]^{\tr}\) and \(\vy = [y_1 \ y_2 \ y_3]^{\tr}\) be two vectors in \(\R^3\). If \(\vx\) and \(\vy\) are linearly independent, then \(\vx\) and \(\vy\) determine a polygon as shown in \hyperref[x:figure:F_Normal_vector]{Figure~{\xreffont\ref{x:figure:F_Normal_vector}}}. Our goal is to find a vector \(\vn\) that is orthogonal to both \(\vx\) and \(\vy\). Let \(\vw = [w_1 \ w_2 \ w_3]^{\tr}\) be another vector in \(\R^3\) and let \(C = \left[ \begin{array}{c} \vw^{\tr} \\ \vx^{\tr} \\ \vy^{\tr} \end{array}  \right]\) be the matrix whose rows are \(\vw\), \(\vx\), and \(\vy\). Let \(C_{ij}\) be the \(ij\)th cofactor of \(C\), that is \(C_{ij}\) is \((-1)^{i+j}\) times the determinant of the submatrix of \(C\) obtained by deleting the \(i\)th row and \(j\)th column of \(C\). Now define the vector \(\vx \times \vy\) as follows:%
\begin{equation*}
\vx \times \vy = C_{11}\ve_1 + C_{12}\ve_2 + C_{13} \ve_3\text{.}
\end{equation*}
%
\par
The vector \(\vx \times \vy\) is called the \terminology{cross product} of the vectors \(\vx\) and \(\vy\). (Note that the cross product is only defined for vectors in \(\R^3\).) We will show that \(\vx \times \vy\) is orthogonal to both \(\vx\) and \(\vy\), making \(\vx \times \vy\) a normal vector to the polygon defined by \(\vx\) and \(\vy\).%
\begin{figureptx}{Normal vector to a polygon.}{x:figure:F_Normal_vector}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/6_a_normal.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that%
\begin{equation*}
\vx \times \vy = \left[ \begin{array}{c} x_2y_3-x_3y_2 \\ x_3y_1-x_1y_3 \\ x_1y_2-x_2y_1 \end{array}  \right]\text{.}
\end{equation*}
%
\item{}Use a cofactor expansion of \(C\) along the first row and properties of the dot product to show that%
\begin{equation*}
\det(C) = \vw \cdot (\vx \times \vy)\text{.}
\end{equation*}
%
\item{}Use the result of part (b) and properties of the determinant to calculate \(\vx \cdot (\vx \times \vy)\) and \(\vy \cdot (\vx \times \vy)\). Explain why \(\vx \times \vy\) is orthogonal to both \(\vx\) and \(\vy\) and is therefore a normal vector to the polygon determined by \(\vx\) and \(\vy\).%
\end{enumerate}
\end{project}%
\hyperref[x:project:act_bf_normal]{Project Activity~{\xreffont\ref{x:project:act_bf_normal}}} shows how we can find a normal vector to a parallelogram \textemdash{} take two vectors \(\vx\) and \(\vy\) between the vertices of the parallelogram and calculate their cross products. Such a normal vector can define a direction for the parallelogram. There is still a problem, however.%
\begin{project}{}{x:project:act_bf_normal_2}%
Let \(\vx = [x_1 \ x_2 \ x_3]^{\tr}\) and \(\vy = [y_1 \ y_2 \ y_3]^{\tr}\) be any vectors in \(\R^3\). There is a relationship between \(\vx \times \vy\) and \(\vy \times \vx\). Find and verify this relationship.%
\end{project}%
\hyperref[x:project:act_bf_normal_2]{Project Activity~{\xreffont\ref{x:project:act_bf_normal_2}}} shows that the cross product is anticommutative, so we get different directions if we switch the order in which we calculate the cross product. To fix a direction, we establish the convention that we always label the vertices of our parallelogram in the counterclockwise direction as shown in \hyperref[x:figure:F_Normal_vector]{Figure~{\xreffont\ref{x:figure:F_Normal_vector}}}. This way we always use \(\vx\) as the vector from vertex \(A\) to vertex \(B\) rather than the reverse. With this convention established, we can now define the direction of a parallelogram as the direction of its normal vector.%
\par
Once we have a normal vector established for each polygon, we can now determine which polygons are back-face and which are front-face. \hyperref[x:figure:F_Hidden]{Figure~{\xreffont\ref{x:figure:F_Hidden}}} at left provides the gist of the idea, where we represent the polygons with line segments to illustrate. If the viewer's eye is at point \(P\) and views the figures, the normal vectors of the visible polygons point in a direction toward the viewer (front-face) and the normal vectors of the polygons hidden from the viewer point away from the viewer (back-face). What remains is to determine an effective computational way to identify the front and back facing polygons.%
\begin{figureptx}{Left: Hidden faces. Right: Back face culling.}{x:figure:F_Hidden}{}%
\begin{sidebyside}{2}{0}{0}{0}%
\begin{sbspanel}{0.5}%
\includegraphics[width=\linewidth]{external/6_a_hidden.pdf}
\end{sbspanel}%
\begin{sbspanel}{0.5}%
\includegraphics[width=\linewidth]{external/6_a_cull.pdf}
\end{sbspanel}%
\end{sidebyside}%
\tcblower
\end{figureptx}%
\begin{project}{}{x:project:act_bf_dot_product}%
Consider the situation as depicted at right in \hyperref[x:figure:F_Hidden]{Figure~{\xreffont\ref{x:figure:F_Hidden}}}. Assume that \(AB\) and \(RS\) are polygons (rendered one dimensionally here) with normal vectors \(\vn\) at their centers as shown. The viewer's eye is at point \(P\) and the viewer's line of vision to the centers \(C_{AB}\) and \(C_{RS}\) are indicated by the vectors \(\vv\). Each vector \(\vv\) makes an angle \(\theta\) with the normal to the polygon.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}What can be said about the angle \(\theta\) for a front-facing polygon? What must be true about \(\vv \cdot \vn\) for a front-facing polygon? Why?%
\item{}What can be said about the angle \(\theta\) for a back-facing polygon? What must be true about \(\vv \cdot \vn\) for a back-facing polygon? Why?%
\item{}The dot product then provides us with a simple computational tool for identifying back-facing polygons (assuming we have already calculated all of the normal vectors). We can then create an algorithm to cull the back-facing polygons. Assuming that we the viewpoint \(P\) and the coordinates of the polygons of the surface, complete the pseudo-code for a back-face culling algorithm:%
\begin{program}{none}{0}{1}{0}
for all polygons on the surface do
  calculate the normal vector n using the ______ product for the current polygon
  calculate the center C of the current polygon
  calculate the viewing vector ______
    if ______ then
      render the current polygon
  end if
end for
\end{program}
\end{enumerate}
\end{project}%
As a final comment, back-face culling generally reduces the number of polygons to be rendered by half. This algorithm is not perfect and does not always do what we want it to do (e.g., it may not remove all parts of a polygon that we don't see), so there are other algorithms to use in concert with back-face culling to correctly render objects.%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 24 Orthogonal and Orthonormal Bases in \(\R^n\)}
\typeout{************************************************}
%
\begin{chapterptx}{Orthogonal and Orthonormal Bases in \(\R^n\)}{}{Orthogonal and Orthonormal Bases in \(\R^n\)}{}{}{x:chapter:chap_orthogonal_basis}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp19281192}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What is an orthogonal set in \(\R^n\)? What is one useful fact about orthogonal subsets of \(\R^n\)?%
\item{}What is an orthogonal basis for a subspace of \(\R^n\)? What is an orthonormal basis?%
\item{}How does orthogonality help us find the weights to write a vector as a linear combination of vectors in an orthogonal basis?%
\item{}What is an orthogonal matrix and why are orthogonal matrices useful?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Rotations in 3D}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Rotations in 3D}{}{Application: Rotations in 3D}{}{}{x:section:sec_appl_3d_rotate}
An aircraft in flight, like a plane or the space shuttle, can perform three independent rotations: \terminology{roll}, \terminology{pitch}, and \terminology{yaw}. Roll is a rotation about the axis through the nose and tail of the aircraft, pitch is rotation moving the nose of the aircraft up or down through the axis from wingtip to wingtip, and yaw is the rotation when the nose of the aircraft turns left or right about the axis though the plane from top to bottom. These rotations take place in \(3\)-space and the axes of the rotations change as the aircraft travels through space. To understand how aircraft maneuver, it is important to know about general rotations in space. These are more complicated than rotations in \(2\)-space, and, as we will see later in this section, involve orthogonal sets.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_orthog_set_intro}
If \(\CB = \{\vv_1, \vv_2, \ldots, \vv_m\}\) is a basis for a subspace \(W\) of \(\R^n\), we know that any vector \(\vw\) in \(W\) can be written uniquely as a linear combination of the vectors in \(\CB\). In the past, the way we have found the coordinates of \(\vx\) with respect to \(\CB\), i.e. the weights needed to write a vector \(\vx\) as a linear combination of the elements in \(\CB\), has been to row reduce the matrix \([\vv_1 \ \vv_2 \ \cdots \ \vv_m \ | \ \vx]\) to solve the corresponding system. This can be a cumbersome process, especially if we need to do it many times. This process also forces us to determine all of the weights at once. For certain types of bases, namely the \terminology{orthogonal} and \terminology{orthonormal} bases, there is a much easier way to find individual weights for this linear combination.%
\par
Recall that two nonzero vectors \(\vu\) and \(\vv\) in \(\R^n\) are orthogonal if \(\vu \cdot \vv = 0\). We can extend this idea to an entire set. For example, the standard basis \(\CS = \{\ve_1, \ve_2, \ve_3\}\) for \(\R^3\) has the property that any two distinct vectors in \(\CS\) are orthogonal to each other. The basis vectors in \(\CS\) make a very nice coordinate system for \(\R^3\), where the basis vectors provide the directions for the coordinate axes. We could rotate this standard basis, or multiply any of the vectors in the basis by a nonzero constant, and retain a basis in which all distinct vectors are orthogonal to each other (e.g., \(\{[2 \ 0 \ 0]^{\tr}, [0 \ 3 \ 0]^{\tr}, [0 \ 0 \ 1]^{\tr}\}\)). We define this idea of having all vectors be orthogonal to each other for sets, and then for bases.%
\begin{definition}{}{x:definition:def_6_b_orthogonal_set_Rn}%
A non-empty subset \(S\) of \(\R^n\) is \terminology{orthogonal}\index{orthogonal set in \(\R^n\)} if \(\vu \cdot \vv = 0\) for every pair of distinct vectors \(\vu\) and \(\vv\) in \(S\).%
\end{definition}
\begin{exploration}{}{x:exploration:pa_6_b}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Determine if the set \(S = \{[1 \ 2 \ 1]^{\tr}, [2 \ -1 \ 0]^{\tr}\}\) is an orthogonal set.%
\item{}Orthogonal bases are especially important. \begin{definition}{}{x:definition:def_6_b_orthogonal_basis_Rn}%
An \terminology{orthogonal basis}\index{orthogonal basis in \(\R^n\)} \(\CB\) for a subspace \(W\) of \(\R^n\) is a basis of \(W\) that is also an orthogonal set.%
\end{definition}
 Let \(\CB = \{\vv_1, \vv_2, \vv_3\}\), where \(\vv_1 = [1 \ 2 \ 1]^{\tr}\), \(\vv_2 = [2 \ -1 \ 0]^{\tr}\), and \(\vv_3 = [1 \ 2 \ -5]^{\tr}\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Explain why \(\CB\) is an orthogonal basis for \(\R^3\).%
\item{}Suppose \(\vx\) has coordinates \(x_1, x_2, x_3\) with respect to the basis \(\CB\), i.e.%
\begin{equation*}
\vx = x_1 \vv_1 + x_2 \vv_2 + x_3 \vv_3 \,\text{.}
\end{equation*}
Substitute this expression for \(\vx\) in \(\vx \cdot \vv_1\) and use the orthogonality property of the basis \(\CB\) to show that \(x_1 = \frac{\vx \cdot \vv_1}{\vv_1 \cdot \vv_1}\). Then determine \(x_2\) and \(x_3\) similarly. Finally, calculate the values of \(x_1\), \(x_2\), and \(x_3\) if \(\vx = [1 \ 1 \ 1]^{\tr}\).%
\item{}Find components of \(\vx = [1 \ 1 \ 1]^{\tr}\) by reducing the augmented matrix \([\vv_1 \ \vv_2 \ \vv_3 \ | \ \vx]\). Does this result agree with your work from the previous part?%
\end{enumerate}
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Orthogonal Sets}
\typeout{************************************************}
%
\begin{sectionptx}{Orthogonal Sets}{}{Orthogonal Sets}{}{}{x:section:sec_orthog_sets}
We defined orthogonal sets in \(\R^n\) and bases of subspaces of \(\R^n\) in \hyperref[x:definition:def_6_b_orthogonal_set_Rn]{Definitions~{\xreffont\ref{x:definition:def_6_b_orthogonal_set_Rn}}} and \hyperref[x:definition:def_6_b_orthogonal_basis_Rn]{Definition~{\xreffont\ref{x:definition:def_6_b_orthogonal_basis_Rn}}}. We saw that the standard basis in \(\R^3\) is an orthogonal set and an orthogonal basis of \(\R^3\) \textemdash{} there are many other examples as well.%
\begin{activity}{}{x:activity:act_6_b_orthog}%
Let \(\vw_1 = \left[ \begin{array}{r} -2 \\ 1 \\ -1 \end{array} \right]\), \(\vw_2 = \left[ \begin{array}{r} 0 \\ 1 \\ 1 \end{array} \right]\), and \(\vw_3 = \left[ \begin{array}{r} 1 \\ 1 \\ -1 \end{array} \right]\). In the same manner as in \hyperref[x:exploration:pa_6_b]{Preview Activity~{\xreffont\ref{x:exploration:pa_6_b}}}, we can show that the set \(S_1 = \left\{ \vw_1, \vw_2, \vw_3 \right\}\) is an orthogonal subset of \(\R^3\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Is the set \(S_2 = \left\{ \left[ \begin{array}{r} -2 \\ 1 \\ -1 \end{array} \right], \left[ \begin{array}{r} 0 \\ 1 \\ 1 \end{array} \right], \left[ \begin{array}{r} 1 \\ 1 \\ -1 \end{array} \right], \left[ \begin{array}{r} 1 \\ 2 \\ 0 \end{array} \right] \right\}\) an orthogonal subset of \(\R^3\)?%
\item{}Suppose a vector \(\vv\) is a vector so that \(S_1 \cup \{\vv\}\) is an orthogonal subset of \(\R^3\). Then \(\vw_i \cdot \vv = 0\) for each \(i\). Explain why this implies that \(\vv\) is in \(\Nul A\), where \(A = \left[ \begin{array}{rcr} -2\amp 1\amp -1 \\ 0\amp 1\amp 1 \\ 1\amp 1\amp -1 \end{array} \right]\).%
\item{}Assuming that the reduced row echelon form of the matrix \(A\) is \(I_3\), explain why it is not possible to find a nonzero vector \(\vv\) so that \(S_1 \cup \{\vv\}\) is an orthogonal subset of \(\R^3\).%
\end{enumerate}
\end{activity}%
The example from \hyperref[x:activity:act_6_b_orthog]{Activity~{\xreffont\ref{x:activity:act_6_b_orthog}}} suggests that we can have three orthogonal nonzero vectors in \(\R^3\), but no more. Orthogonal vectors are, in a sense, as far apart as they can be. So we might expect that there is no linear relationship between orthogonal vectors. The following theorem makes this clear.%
\begin{theorem}{}{}{x:theorem:thm_6_b_Orth_li}%
Let \(\{\vv_1, \vv_2, \ldots, \vv_m\}\) be a set of nonzero orthogonal vectors in \(\R^n\). Then the vectors \(\vv_1, \vv_2, \ldots, \vv_m\) are linearly independent.%
\end{theorem}
\begin{proof}{}{g:proof:idp19340712}
Let \(S = \{\vv_1, \vv_2, \ldots, \vv_m\}\) be a set of nonzero orthogonal vectors in \(\R^n\). To show that \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_m\) are linearly independent, assume that%
\begin{equation}
x_1\vv_1 + x_2\vv_2 + \cdots + x_m \vv_m = \vzero\label{x:men:eq_6_b_orthog}
\end{equation}
for some scalars \(x_1\), \(x_2\), \(\ldots\), \(x_m\). We will show that \(x_i = 0\) for each \(i\) from 1 to \(m\). Since the vectors in \(S\) are orthogonal to each other, we know that \(\vv_i \cdot \vv_j = 0\) whenever \(i \neq j\). Fix an index \(k\) between 1 and \(m\). We evaluate the dot product of both sides of \hyperref[x:men:eq_6_b_orthog]{({\xreffont\ref{x:men:eq_6_b_orthog}})} with \(\vv_k\) and simplify using the dot product properties:%
\begin{align}
\vv_k \cdot (x_1\vv_1 + x_2\vv_2 + \cdots + x_m \vv_m) \amp = \vv_k \cdot \vzero\notag\\
(\vv_k \cdot x_1\vv_1) + (\vv_k \cdot x_2\vv_2) + \cdots + (\vv_k \cdot x_m \vv_m) \amp = 0\notag\\
x_1(\vv_k \cdot \vv_1) + x_2(\vv_k \cdot \vv_2) + \cdots + x_m(\vv_k \cdot  \vv_m) \amp = 0\text{.}\label{x:mrow:eq_6_b_orthog2}
\end{align}
%
\par
Now all of the dot products on the left are 0 except for \(\vv_k \cdot \vv_k\), so \hyperref[x:mrow:eq_6_b_orthog2]{({\xreffont\ref{x:mrow:eq_6_b_orthog2}})} becomes%
\begin{equation*}
x_k (\vv_k \cdot \vv_k) = 0\text{.}
\end{equation*}
%
\par
We assumed that \(\vv_k \neq \vzero\) and since \(\vv_k \cdot \vv_k = || \vv_k ||^2 \neq 0\), we conclude that \(x_k = 0\). We chose \(k\) arbitrarily, so we have shown that \(x_k =0\) for each \(k\) between 1 and \(m\). Therefore, the only solution to equation \hyperref[x:men:eq_6_b_orthog]{({\xreffont\ref{x:men:eq_6_b_orthog}})} is the trivial solution with \(x_1 = x_2 = \cdots = x_m = 0\) and the set \(S\) is linearly independent.%
\end{proof}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Properties of Orthogonal Bases}
\typeout{************************************************}
%
\begin{sectionptx}{Properties of Orthogonal Bases}{}{Properties of Orthogonal Bases}{}{}{x:section:sec_orthog_bases_prop}
Orthogonality is a useful and important property for a basis to have. In \hyperref[x:exploration:pa_6_b]{Preview Activity~{\xreffont\ref{x:exploration:pa_6_b}}} we saw that if a vector \(\vx\) in the span of an orthogonal basis \(\{\vv_1, \vv_2, \vv_3\}\) could be written as a linear combination of the basis vectors as \(\vx = x_1 \vv_1 + x_2 \vv_2 + x_3 \vv_3\), then \(x_1 = \frac{\vx \cdot \vv_1}{\vv_1 \cdot \vv_1}\). If we continued that same argument we could show that%
\begin{equation*}
\vx = \left(\frac{\vx \cdot \vv_1}{\vv_1 \cdot \vv_1}\right) \vv_1 + \left(\frac{\vx \cdot \vv_2}{\vv_2 \cdot \vv_2}\right) \vv_2 + \left(\frac{\vx \cdot \vv_3}{\vv_3 \cdot \vv_3}\right) \vv_3\text{.}
\end{equation*}
%
\par
We can apply this idea in general to see how the orthogonality of an orthogonal basis allows us to quickly and easily determine the weights to write a given vector as a linear combination of orthogonal basis vectors. To see why, let \(\CB = \{\vv_1, \vv_2, \ldots, \vv_m\}\) be an orthogonal basis for a subspace \(W\) of \(\R^n\) and let \(\vx\) be any vector in \(W\). We know that%
\begin{equation*}
\vx = x_1\vv_1 + x_2\vv_2 + \cdots + x_m \vv_m
\end{equation*}
for some scalars \(x_1\), \(x_2\), \(\ldots\), \(x_m\). Let \(1\leq k\leq m\). Then, using orthogonality of vectors \(\vv_1, \vv_2, \ldots, \vv_m\), we have%
\begin{equation*}
\vv_k \cdot \vx = x_1(\vv_k \cdot \vv_1) + x_2(\vv_k \cdot \vv_2) + \cdots + x_m(\vv_k \cdot \vv_m) = x_k \vv_k \cdot \vv_k\text{.}
\end{equation*}
%
\par
So%
\begin{equation*}
x_k = \ds \frac{\vx \cdot \vv_k}{\vv_k \cdot \vv_k}\text{.}
\end{equation*}
%
\par
Thus, we can calculate each weight individually with two simple dot products. We summarize this discussion in the next theorem.%
\begin{theorem}{}{}{x:theorem:thm_6_b_orth_dcomp}%
Let \(\CB = \{\vv_1, \vv_2, \ldots, \vv_m\}\) be an orthogonal basis for a subspace of \(\R^n\). Let \(\vx\) be a vector in \(W\). Then%
\begin{equation}
\vx = \ds \frac{\vx \cdot \vv_1}{\vv_1 \cdot \vv_1} \vv_1 +  \frac{\vx \cdot \vv_2}{\vv_2 \cdot \vv_2} \vv_2 + \cdots + \frac{\vx \cdot \vv_m}{\vv_m \cdot \vv_m} \vv_m\text{.}\label{x:men:eq_6_b_orth_ddecomp}
\end{equation}
%
\end{theorem}
\begin{activity}{}{g:activity:idp19362344}%
Let \(\vv_1 = [1 \ 0 \ 1]^{\tr}\), \(\vv_2 = [0 \ 1 \ 0]^{\tr}\), and \(\vv_3 = [0 \ 0 \ 1]^{\tr}\). The set \(\CB = \{\vv_1, \vv_2, \vv_3\}\) is a basis for \(\R^3\). Let \(\vx = [1 \ 0 \ 0]^{\tr}\). Calculate%
\begin{equation*}
\frac{\vx \cdot \vv_1}{\vv_1 \cdot \vv_1}\vv_1 + \frac{\vx \cdot \vv_2}{\vv_2 \cdot \vv_2}\vv_2 + \frac{\vx \cdot \vv_3}{\vv_3 \cdot \vv_3}\vv_3\text{.}
\end{equation*}
%
\par
Compare to \(\vx\). Does this violate \hyperref[x:theorem:thm_6_b_orth_dcomp]{Theorem~{\xreffont\ref{x:theorem:thm_6_b_orth_dcomp}}}? Explain.%
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Orthonormal Bases}
\typeout{************************************************}
%
\begin{sectionptx}{Orthonormal Bases}{}{Orthonormal Bases}{}{}{x:section:sec_orthon_bases}
The decomposition \hyperref[x:men:eq_6_b_orth_ddecomp]{({\xreffont\ref{x:men:eq_6_b_orth_ddecomp}})} is even simpler if \(\vv_k \cdot \vv_k = 1\) for each \(k\), that is, if \(\vv_k\) is a unit vector for each \(k\). In this case, the denominators are all 1 and we don't even need to consider them. We have a familiar example of such a basis for \(\R^n\), namely the standard basis \(\CS = \{\ve_1, \ve_2, \ldots, \ve_n\}\).%
\par
Recall that%
\begin{equation*}
\vv \cdot \vv = || \vv ||^2\text{,}
\end{equation*}
so the condition \(\vv \cdot \vv = 1\) implies that the vector \(\vv\) has norm 1. An orthogonal basis with this additional condition is a very nice basis and is given a special name.%
\begin{definition}{}{x:definition:def_6_b_orthonormal_basis}%
\index{orthonormal basis in \(\R^n\)}%
An \terminology{orthonormal basis} \(\CB = \{\vu_1, \vu_2, \ldots, \vu_m\}\) for a subspace \(W\) of \(\R^n\) is an orthogonal basis such that \(|| \vu_k || = 1\) for \(1\leq k\leq m\).%
\end{definition}
In other words, an orthonormal basis is an orthogonal basis in which every basis vector is a unit vector. A good question to ask here is how we can construct an orthonormal basis from an orthogonal basis.%
\begin{activity}{}{g:activity:idp19381032}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(\vv_1\) and \(\vv_2\) be orthogonal vectors. Explain how we can obtain unit vectors \(\vu_1\) in the direction of \(\vv_1\) and \(\vu_2\) in the direction of \(\vv_2\).%
\item{}Show that \(\vu_1\) and \(\vu_2\) from the previous part are orthogonal vectors.%
\item{}Use the ideas from this problem to construct an orthonormal basis for \(\R^3\) from the orthogonal basis \(S = \left\{ \left[ \begin{array}{r} -2 \\ 1 \\ -1 \end{array} \right], \left[ \begin{array}{r} 0 \\ 1 \\ 1 \end{array} \right], \left[ \begin{array}{r} 1 \\ 1 \\ -1 \end{array} \right] \right\}\).%
\end{enumerate}
\end{activity}%
In general, we can construct an orthonormal basis \(\{\vu_1, \vu_2, \ldots, \vu_m\}\) from an orthogonal basis \(\CB = \{\vv_1, \vv_2, \ldots, \vv_m\}\) by normalizing each vector in \(\CB\) (that is, dividing each vector by its norm).%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Orthogonal Matrices}
\typeout{************************************************}
%
\begin{sectionptx}{Orthogonal Matrices}{}{Orthogonal Matrices}{}{}{x:section:sec_orthog_mtx}
We have seen in the diagonalization process that we diagonalize a matrix \(A\) with a matrix \(P\) whose columns are linearly independent eigenvectors of \(A\). In general, calculating the inverse of the matrix whose columns are eigenvectors of \(A\) in the diagonalization process can be time consuming, but if the columns form an orthonormal set, then the calculation is very straightforward.%
\begin{activity}{}{x:activity:act_6_b_orthogonal_inverse}%
Let \(\vu_1 = \frac{1}{3}[2 \ 1 \ 2]^{\tr}\), \(\vu_2 = \frac{1}{3}[-2 \ 2 \ 1]^{\tr}\), and \(\vu_3 = \frac{1}{3}[1 \ 2 \ -2]^{\tr}\). It is not difficult to see that the set \(\{\vu_1, \vu_2, \vu_3\}\) is an orthonormal basis for \(\R^3\). Let%
\begin{equation*}
A = [\vu_1 \ \vu_2 \ \vu_3] = \frac{1}{3} \left[ \begin{array}{crr} 2\amp -2\amp 1\\1\amp 2\amp 2\\2\amp 1\amp -2 \end{array}  \right]\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Use the definition of the matrix-matrix product to find the entries of the second row of the matrix product \(A^{\tr}A\). Why should you have expected the result?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19385512}{}\quad{}How are the rows of \(A^{\tr}\) related to the columns of \(A\)?%
\item{}With the result of part (a) in mind, what is the matrix product \(A^{\tr}A\)? What does this tell us about the relationship between \(A^{\tr}\) and \(A^{-1}\)? Use technology to calculate \(A^{-1}\) and confirm your answer.%
\item{}Suppose \(P\) is an \(n \times n\) matrix whose columns form an orthonormal basis for \(\R^n\). Explain why \(P^{\tr}P = I_n\).%
\end{enumerate}
\end{activity}%
The result of \hyperref[x:activity:act_6_b_orthogonal_inverse]{Activity~{\xreffont\ref{x:activity:act_6_b_orthogonal_inverse}}} is that if the columns of a square matrix \(P\) form an orthonormal set, then \(P^{-1} = P^{\tr}\). This makes calculating \(P^{-1}\) very easy. Note, however, that this only works if the columns of \(P\) form an orthonormal basis for \(\Col P\). You should also note that if \(P\) is an \(n \times n\) matrix satisfying \(P^{\tr}P = I_n\), then the columns of \(P\) must form an orthonormal set. Matrices like this appear quite often and are given a special name.%
\begin{definition}{}{g:definition:idp19407144}%
\index{matrix!orthogonal}%
An \terminology{orthogonal} matrix is an \(n \times n\) matrix \(P\) such that \(P^{\tr}P = I_n\).\footnotemark{}%
\end{definition}
\footnotetext[41]{It isn't clear why such matrices are called orthogonal since the columns are actually orthonormal, but that is the standard terminology in mathematics.\label{g:fn:idp19402664}}%
\begin{activity}{}{g:activity:idp19403816}%
As a special case, we apply the result of \hyperref[x:activity:act_6_b_orthogonal_inverse]{Activity~{\xreffont\ref{x:activity:act_6_b_orthogonal_inverse}}} to a \(2 \times 2\) rotation matrix \(P = \left[ \begin{array}{cr} \cos(\theta) \amp -\sin(\theta) \\ \sin(\theta) \amp \cos(\theta) \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that the columns of \(P\) form an orthonormal set.%
\item{}Use the fact that \(P^{-1} = P^{\tr}\) to find \(P^{-1}\). Explain how this shows that the inverse of a rotation matrix by an angle \(\theta\) is just another rotation matrix but by the angle \(-\theta\).%
\end{enumerate}
\end{activity}%
Orthogonal matrices are useful because they satisfy some special properties. For example, if \(P\) is an orthogonal \(n \times n\) matrix and \(\vx, \vy \in \R^n\), then%
\begin{equation*}
(P\vx) \cdot (P\vy) = (P\vx)^{\tr}(P\vy) = \vx^{\tr}P^{\tr}P\vy = \vx^{\tr}\vy = \vx \cdot \vy\text{.}
\end{equation*}
%
\par
\index{isometry} This property tells us that the matrix transformation \(T\) defined by \(T(\vx) = P\vx\) preserves dot products and, hence, orthogonality. In addition,%
\begin{equation*}
||P\vx||^2 = P\vx \cdot P\vx = \vx \cdot \vx = ||\vx||^2\text{,}
\end{equation*}
so \(||P\vx|| = ||\vx||\). This means that \(T\) preserves length. Such a transformation is called an \terminology{isometry} and it is convenient to work with functions that don't expand or contract things. Moreover, if \(\vx\) and \(\vy\) are nonzero vectors, then%
\begin{equation*}
\frac{P\vx \cdot P\vy}{||P\vx|| \ ||P\vy||} = \frac{\vx \cdot \vy}{||\vx|| \ ||\vy||}\text{.}
\end{equation*}
%
\par
Thus \(T\) also preserves angles. Transformations defined by orthogonal matrices are very well behaved transformations. To summarize,%
\begin{theorem}{}{}{g:theorem:idp19413800}%
Let \(P\) be an \(n \times n\) orthogonal matrix and let \(\vx, \vy \in \R^n\). Then%
\begin{enumerate}
\item{}\((P\vx) \cdot (P\vy) = \vx \cdot \vy\),%
\item{}\(||P\vx|| = ||\vx||\), and%
\item{}\(\frac{P\vx \cdot P\vy}{||P\vx|| \ ||P\vy||} = \frac{\vx \cdot \vy}{||\vx|| \ ||\vy||}\) if \(\vx\) and \(\vy\) are nonzero.%
\end{enumerate}
%
\end{theorem}
We have discussed orthogonal and orthonormal bases for subspaces of \(\R^n\) in this section. There are reasonable questions that follow, such as%
\begin{itemize}[label=\textbullet]
\item{}Can we always find an orthogonal (or orthonormal) basis for any subspace of \(\R^n\)?%
\item{}Given a vector \(\vv\) in \(W\), can we find an orthogonal basis of \(W\) that contain \(\vv\)?%
\end{itemize}
We will answer these questions in subsequent sections.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_orthog_set_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp19429672}%
Let \(S = \{\vv_1, \vv_2, \vv_3\}\), where \(\vv_1 = [1 \ 1 \ -4]^{\tr}\), \(\vv_2 = [2 \ 2 \ 1]^{\tr}\), and \(\vv_3 = [1 \ -1 \ 0]^{\tr}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(S\) is an orthogonal set.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp19430824}{}\quad{}Using the dot product formula, we see that \(\vv_1 \cdot \vv_2 = 0\), \(\vv_1 \cdot \vv_3 = 0\), and \(\vv_2 \cdot \vv_3 = 0\). Thus, the set \(S\) is an orthogonal set.%
\item{}Create an orthonormal set \(S' = \{\vu_1, \vu_2, \vu_3\}\) from the vectors in \(S\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp19439528}{}\quad{}To make an orthonormal set \(S' = \{\vu_1, \vu_2, \vu_3\}\) from \(S\), we divide each vector in \(S\) by its magnitude. This gives us%
\begin{equation*}
\vu_1 = \frac{1}{\sqrt{18}}[1 \ 1 \ -4]^{\tr}, \ \vu_2 = \frac{1}{3}[2 \ 2 \ 1]^{\tr}, \ \text{ and }  \ \vu_3 = \frac{1}{\sqrt{2}} [1 \ -1 \ 0]^{\tr}\text{.}
\end{equation*}
%
\item{}Just by calculating dot products, write the vector \(\vw = [2 \ 1 \ -1]^{\tr}\) as a linear combination of the vectors in \(S'\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp19440296}{}\quad{}Since \(S'\) is an orthonormal basis for \(\R^3\), we know that%
\begin{align*}
\vw \amp = (\vw \cdot \vu_1) \vu_1 + (\vw \cdot \vu_2) \vu_2 + (\vw \cdot \vu_3) \vu_3\\
\amp = \frac{7}{\sqrt{18}}[1 \ 1 \ -4]^{\tr} +  \frac{5}{3}[2 \ 2 \ 1]^{\tr} + \frac{1}{\sqrt{2}}[1 \ -1 \ 0]^{\tr}\text{.}
\end{align*}
%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp19440168}%
Let \(\vu_1 = \frac{1}{\sqrt{3}}[1 \ 1 \ 1]^{\tr}\), \(\vu_2 = \frac{1}{\sqrt{2}}[1 \ -1 \ 0]^{\tr}\), and \(\vu_3 = \frac{1}{\sqrt{6}}[1 \ 1 \ -2]^{\tr}\). Let \(\CB = \{\vu_1, \vu_2, \vu_3\}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(\CB\) is an orthonormal basis for \(\R^3\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp19444648}{}\quad{}Using the dot product formula, we see that \(\vu_i \cdot \vu_j = 0\) if \(i \neq j\) and that \(\vu_i \cdot \vu_i = 1\) for each \(i\). Since orthogonal vectors are linearly independent, the set \(\CB\) is a linearly independent set with \(3\) vectors in a \(3\)-dimensional space. It follows that \(\CB\) is an orthonormal basis for \(\R^3\).%
\item{}Let \(\vw = [1 \ 2 \ 1]^{\tr}\). Find \([\vw]_{\CB}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp19447208}{}\quad{}Since \(\CB\) is an orthonormal basis for \(\R^3\), we know that%
\begin{equation*}
\vw = (\vw \cdot \vu_1) \vu_1 + (\vw \cdot \vu_2) \vu_2 + (\vw \cdot \vu_3) \vu_3\text{.}
\end{equation*}
Therefore,%
\begin{equation*}
[\vw]_{\CB} = [(\vw \cdot \vu_1) \ (\vw \cdot \vu_2) \ (\vw \cdot \vu_3)]^{\tr} = \left[ \frac{4}{\sqrt{3}} \ -\frac{1}{\sqrt{2}} \ \frac{1}{\sqrt{6}} \right]^{\tr}\text{.}
\end{equation*}
%
\item{}Calculate \(||\vw||\) and \(\left|\left|[\vw]_{\CB}\right|\right|\). What do you notice?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp19453608}{}\quad{}Using the definition of the norm of a vector we have%
\begin{align*}
||\vw|| \amp = \sqrt{1^2+2^2+1^2} = \sqrt{6}\\
\left|\left|[\vw]_{|CB}\right|\right| \amp = \sqrt{\left( \frac{4}{\sqrt{3}}\right)^2+\left(-\frac{1}{\sqrt{2}}\right)^2 + \left(\frac{1}{\sqrt{6}}\right)^2} = \sqrt{6}\text{.}
\end{align*}
So in this case we have \(||\vw|| = \left|\left|[\vw]_{\CB}\right|\right|\).%
\item{}Show that the result of part (c) is true in general. That is, if \(\CS = \{\vv_1, \vv_2, \ldots, \vv_n\}\) is an orthonormal basis for \(\R^n\), and if \(\vz = c_1\vv_1 + c_2 \vv_2 + \cdots + c_n \vv_n\), then%
\begin{equation*}
||\vz|| =\sqrt{c_1^2+c_2^2 + \cdots + c_n^2}\text{.}
\end{equation*}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp19457064}{}\quad{}Let \(\CS = \{\vv_1, \vv_2, \ldots, \vv_n\}\) be an orthonormal basis for \(\R^n\), and suppose that \(\vz = c_1\vv_1 + c_2 \vv_2 + \cdots + c_n \vv_n\). Then%
\begin{align}
|| \vz || \amp = \sqrt{\vz \cdot \vz}\notag\\
\amp = \sqrt{ (c_1\vv_1 + c_2 \vv_2 + \cdots + c_n \vv_n) \cdot (c_1\vv_1 + c_2 \vv_2 + \cdots + c_n \vv_n)}\text{.}\label{x:mrow:eq_6_b_ex_2}
\end{align}
Since \(\CS\) is an orthonormal basis for \(\R^n\), it follows that \(\vv_i \cdot \vv_j = 0\) if \(i \neq j\) and \(\vv_ \cdot \vv_i = 1\). Expanding the dot product in \hyperref[x:mrow:eq_6_b_ex_2]{({\xreffont\ref{x:mrow:eq_6_b_ex_2}})}, the only terms that won't be zero are the ones that involve \(\vv_i \cdot \vv_i\). This leaves us with%
\begin{align*}
|| \vz || \amp =  \sqrt{ (c_1\vv_1 + \cdots + c_n \vv_n) \cdot (c_1\vv_1 + \cdots + c_n \vv_n)}\\
\amp = \sqrt{c_1c_1 (\vv_1 \cdot \vv_1) + c_2c_2 (\vv_2 \cdot \vv_2) + \cdots + c_nc_n (\vv_n \cdot \vv_n)}\\
\amp = \sqrt{c_1^2+c_2^2 + \cdots + c_n^2}\text{.}
\end{align*}
%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_orthog_set_summ}
%
\begin{itemize}[label=\textbullet]
\item{}A subset \(S\) of \(\R^n\) is an orthogonal set if \(\vu \cdot \vv = 0\) for every pair of distinct vector \(\vu\) and \(\vv\) in \(S\).%
\item{}Any orthogonal set of nonzero vectors is linearly independent.%
\item{}A basis \(\CB\) for a subspace \(W\) of \(\R^n\) is an orthogonal basis if \(\CB\) is also an orthogonal set.%
\item{}An orthogonal basis \(\CB\) for a subspace \(W\) of \(\R^n\) is an orthonormal basis if each vector in \(\CB\) has unit length.%
\item{}If \(\CB = \{\vv_1, \vv_2, \ldots, \vv_m\}\) is an orthogonal basis for a subspace \(W\) of \(\R^n\) and \(\vx\) is any vector in \(W\), then%
\begin{equation*}
\vx = \sum_{i=1}^m c_i \vv_i
\end{equation*}
where \(c_i = \frac{\vx \cdot \vv_i}{\vv_i \cdot \vv_i}\).%
\item{}An \(n \times n\) matrix \(P\) is an orthogonal matrix if \(P^{\tr}P = I_n\). Orthogonal matrices are important, in part, because the matrix transformations they define are isometries.%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_orthog_set_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp19491992}%
Find an orthogonal basis for the subspace \(W = \{ [x \ y \ z] : 4x-3z = 0\}\) of \(\R^3\).%
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp19505432}%
Let \(\{\vv_1, \vv_2, \ldots, \vv_n\}\) be an orthogonal basis for \(\R^n\) and, for some \(k\) between 1 and \(n\), let \(W = \Span\{\vv_1,\vv_2, \ldots, \vv_k\}\). Show that \(\{\vv_{k+1}, \vv_{k+2}, \ldots, \vv_n\}\) is a basis for \(W^{\perp}\).%
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{x:exercise:problem_orthog_decomp}%
Let \(W\) be a subspace of \(\R^n\) for some \(n\), and let \(\{\vw_1, \vw_2, \ldots, \vw_k\}\) be an orthogonal basis for \(W\). Let \(\vx\) be a vector in \(\R^n\). and define \(\vw\) as%
\begin{equation*}
\vw = \frac{\vx \cdot \vw_1}{\vw_1 \cdot \vw_1} \vw_1 + \frac{\vx \cdot \vw_2}{\vw_2 \cdot \vw_2} \vw_2 + \cdots + \frac{\vx \cdot \vw_k}{\vw_k \cdot \vw_k} \vw_k\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why \(\vw\) is in \(W\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19511064}{}\quad{}Where are \(\vw_1\), \(\vw_2\), \(\ldots\), \(\vw_k\)?%
\item{}Let \(\vz = \vx - \vw\). Show that \(\vz\) is in \(W^{\perp}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19507096}{}\quad{}Take the dot product of \(\vz\) with \(\vw_i\).%
\item{}Explain why \(\vx\) can be written as a sum of vectors, one in \(W\) and one in \(W^{\perp}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19517336}{}\quad{}Use parts (a) and (b).%
\item{}Suppose \(\vx = \vw+\vw_1\) and \(\vx = \vu+\vu_1\), where \(\vw\) and \(\vu\) are in \(W\) and \(\vw_1\) and \(\vu_1\) are in \(W^{\perp}\). Show that \(\vw=\vu\) and \(\vw_1 = \vu_1\), so that the representation of \(\vx\) as a sum of a vector in \(W\) and a vector in \(W^{\perp}\) is unique.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19522584}{}\quad{}Collect terms in \(W\) and in \(W^{\perp}\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp19515416}%
Use the result of problem \hyperlink{x:exercise:problem_orthog_decomp}{Exercise~{\xreffont 3}} above and that \(W\cap W^\perp=\{\vzero\}\) to show that \(\dim(W)+\dim(W^\perp)=n\) for a subspace \(W\) of \(\R^n\). (See \hyperlink{x:exercise:ex_3_a_sum}{Exercise~{\xreffont 13}} in \hyperref[x:chapter:chap_R_n]{Section~{\xreffont\ref{x:chapter:chap_R_n}}} for the definition of the sum of subspaces.)%
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp19526680}%
Let \(P\) be an \(n \times n\) matrix. We showed that if \(P\) is an orthogonal matrix, then \((P\vx) \cdot (P\vy) = \vx \cdot \vy\) for any vectors \(\vx\) and \(\vy\) in \(\R^n\). Now we ask if the converse of this statement is true. That is, determine the validity of the following statement: if \((P\vx) \cdot (P\vy) = \vx \cdot \vy\) for any vectors \(\vx\) and \(\vy\) in \(\R^n\), then \(P\) is an orthogonal matrix? Verify your answer.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19524760}{}\quad{}Consider \((P \ve_i) \cdot (P \ve_j)\) where \(\ve_t\) is the \(t\)th standard basis vector for \(\R^n\).%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{x:exercise:ex_Reflection_matrices}%
In this exercise we examine reflection matrices. In the following exercise we will show that the reflection and rotation matrices are the only \(2 \times 2\) orthogonal matrices. We will determine how to represent the reflection across a line through the origin in \(\R^2\) as a matrix transformation. The setup is as follows. Let \(L(\theta)\) be the line through the origin in \(\R^2\) that makes an angle \(\theta\) with the positive \(x\)-axis as illustrated in \hyperref[x:figure:F_Reflection]{Figure~{\xreffont\ref{x:figure:F_Reflection}}}.%
\begin{figureptx}{Reflecting across a line \(L(\theta)\).}{x:figure:F_Reflection}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/Reflection.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find a unit vector \(\vu\) in the direction of the line \(L(\theta)\).%
\item{}Let \(\vv = \left[ \begin{array}{c} a\\b \end{array} \right]\) be an arbitrary vector in \(\R^2\) as represented in \hyperref[x:figure:F_Reflection]{Figure~{\xreffont\ref{x:figure:F_Reflection}}}. Determine the components of the vectors \(\proj_{\vu} \vv\) and \(\proj_{\perp \vu} \vv\). Reproduce \hyperref[x:figure:F_Reflection]{Figure~{\xreffont\ref{x:figure:F_Reflection}}} and draw the vectors \(\proj_{\vu} \vv\) and \(\proj_{\perp \vu} \vv\) in your figure.%
\item{}The vector labeled \(\vw\) is the reflection of the vector \(\vv\) across the line \(L(\theta)\). Write \(\vw\) in terms of \(\vv\) and \(\proj_{\vu} \vv\). Clearly explain your method.%
\item{}Finally, show that the matrix \(A\) such that \(A \vv = \vw\) is given by%
\begin{equation*}
A = \left[ \begin{array}{cr} \cos(2\theta)\amp \sin(2\theta) \\ \sin(2\theta)\amp  -\cos(2\theta) \end{array}  \right]\text{.}
\end{equation*}
The matrix \(A\) is the \terminology{reflection} matrix across the line \(L(\theta)\). (You will want to look up some appropriate trigonometric identities.)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp19555096}%
In this exercise we will show that the only orthogonal \(2 \times 2\) matrices are the rotation matrices \(\left[ \begin{array}{cr} \cos(\theta)\amp -\sin(\theta) \\ \sin(\theta)\amp \cos(\theta) \end{array} \right]\) and the reflection matrices \(\left[ \begin{array}{cr} \cos(\theta)\amp \sin(\theta) \\ \sin(\theta) \amp -\cos(\theta) \end{array} \right]\) (see \hyperlink{x:exercise:ex_Reflection_matrices}{Exercise~{\xreffont 6}}). Throughout this exercise let \(a\), \(b\), \(c\), and \(d\) be real numbers such that \(M = \left[ \begin{array}{cc} a\amp b \\ c\amp d \end{array} \right]\) is an orthogonal \(2 \times 2\) matrix. Let \(\vv_1= \left[ \begin{array}{c} a \\ c \end{array} \right]\) and \(\vv_2= \left[ \begin{array}{c} b \\ d \end{array} \right]\) be the columns of \(M\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why the terminal point of \(\vv_1\) in standard position lies on the unit circle. Then explain why there is an angle \(\theta\) such that \(a = \cos(\theta)\) and \(c = \sin(\theta)\). What angle, specifically, is \(\theta\)? Draw a picture to illustrate.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19559320}{}\quad{}Think polar coordinates.%
\item{}A similar argument to (b) shows that there is an angle \(\alpha\) such that \(\vv_2 = \left[ \begin{array}{c} b\\d \end{array} \right] = \left[ \begin{array}{c} \cos(\alpha)\\\sin(\alpha) \end{array} \right]\). Given that \(M\) is an orthogonal matrix, how must \(\alpha\) be related to \(\theta\)? Use this result to find the two possibilities for \(\vv_2\) as a vector in terms of \(\cos(\theta)\) and \(\sin(\theta)\). (You will likely want to look up some trigonometric identities for this part of the problem.)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19561752}{}\quad{}What properties do the columns of an orthogonal matrix have?%
\item{}By considering the two possibilities from part (c), show that \(M\) is either a rotation matrix or a reflection matrix. Conclude that the only \(2 \times 2\) orthogonal matrices are the reflection and rotation matrices.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp19562136}%
Suppose \(A, B\) are orthogonal matrices of the same size.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(AB\) is also an orthogonal matrix.%
\item{}Show that \(A^\tr\) is also an orthogonal matrix.%
\item{}Show that \(A^{-1}\) is also an orthogonal matrix.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp19568408}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
Any orthogonal subset of \(\R^n\) is linearly independent.%
\item{}\lititle{True\slash{}False.}\par%
Every single vector set is an orthogonal set.%
\item{}\lititle{True\slash{}False.}\par%
If \(S\) is an orthogonal set in \(\R^n\) with exactly \(n\) nonzero vectors, then \(S\) is a basis for \(\R^n\).%
\item{}\lititle{True\slash{}False.}\par%
Every set of three linearly independent vectors in \(\R^3\) is an orthogonal set.%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) and \(B\) are \(n \times n\) orthogonal matrices, then \(A+B\) must also be an orthogonal matrix.%
\item{}\lititle{True\slash{}False.}\par%
If the set \(S=\{\vv_1, \vv_2, \ldots, \vv_n\}\) is an orthogonal set in \(\R^n\), then so is the set \(\{c_1\vv_1, c_2\vv_2, \ldots,
c_n\vv_n\}\) for any scalars \(c_1\), \(c_2\), \(\ldots\), \(c_n\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\CB=\{\vv_1, \vv_2, \ldots, \vv_n\}\) is an orthogonal basis of \(\R^n\), then so is \(\{c_1\vv_1\), \(c_2\vv_2\), \(\ldots\), \(c_n\vv_n\}\) for any nonzero scalars \(c_1\), \(c_2\), \(\ldots\), \(c_n\).%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is an \(n\times n\) orthogonal matrix, the rows of \(A\) form an orthonormal basis of \(\R^n\).%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is an orthogonal matrix, any matrix obtained by interchanging columns of \(A\) is also an orthogonal matrix.%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Understanding Rotations in 3-Space}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Understanding Rotations in 3-Space}{}{Project: Understanding Rotations in 3-Space}{}{}{x:section:sec_proj_3d_rotate}
Recall that a counterclockwise rotation of \(2\)-space around the origin by an angle \(\theta\) is accomplished by left multiplication by the matrix \(\left[ \begin{array}{cr} \cos(\theta)\amp -\sin(\theta) \\ \sin(\theta)\amp \cos(\theta) \end{array} \right]\). Notice that the columns of this rotation matrix are orthonormal, so this rotation matrix is an orthogonal matrix. As the next activity shows, rotation matrices in 3D are also orthogonal matrices.%
\begin{project}{}{x:project:act_rot_det}%
Let \(R\) be a rotation matrix in 3D. A rotation does not change lengths of vectors, nor does it change angles between vectors. Let \(\ve_1 = [ 1 \ 0 \ 0]^{\tr}\), \(\ve_2 = [0 \ 1 \ 0]^{\tr}\), and \(\ve_3 = [0 \ 0 \ 1]^{\tr}\) be the standard unit vectors in \(\R^3\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why the columns of \(R\) form an orthonormal set.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19592472}{}\quad{}How are \(R \ve_1\), \(R\ve_2\), and \(R\ve_3\) related to the columns of \(R\)?%
\item{}Explain why \(R\) is an orthogonal matrix. What must be true about \(\det(R)\)?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19600408}{}\quad{}What is \(R^{\tr}\) and what is \(\det(R^{\tr}R)\)?%
\end{enumerate}
\end{project}%
By \hyperref[x:project:act_rot_det]{Project Activity~{\xreffont\ref{x:project:act_rot_det}}} we know that the determinant of any rotation matrix is either \(1\) or \(-1\). Having a determinant of \(1\) preserves orientation, and we will identify these rotations as being counterclockwise, and we will identify the others with determinant of \(-1\) as being clockwise. We will set the convention that a rotation is always measured counterclockwise (as we did in \(\R^2\)), and so every rotation matrix will have determinant \(1\).%
\par
Returning to the counterclockwise rotation of \(2\)-space around the origin by an angle \(\theta\) determined by left multiplication by the matrix \(\left[ \begin{array}{cr} \cos(\theta)\amp -\sin(\theta) \\ \sin(\theta)\amp \cos(\theta) \end{array}  \right]\), we can think of this rotation in \(3\)-space as the rotation that keeps points in the \(xy\) plane in the \(xy\) plane, but rotates these points counterclockwise around the \(z\) axis. In other words, in the standard \(xyz\) coordinate system, with standard basis \(\ve_1\), \(\ve_2\), \(\ve_3\), our rotation matrix \(R\) has the property that \(R \ve_3 = \ve_3\). Now \(R \ve_3\) is the third column of \(R\), so the third column of \(R\) is \(\ve_3\). Similarly, \(R \ve_1\) is the first column of \(R\) and \(R \ve_2\) is the second column of \(R\). Since \(R\) is a counterclockwise rotation of the \(xy\) plane space around the origin by an angle \(\theta\) it follows that this rotation is given by the matrix%
\begin{equation}
R_{\ve_3}(\theta) = \left[ \begin{array}{ccc} \cos(\theta)\amp -\sin(\theta)\amp 0 \\ \sin(\theta)\amp \cos(\theta)\amp 0 \\ 0\amp 0\amp 1 \end{array}  \right]\text{.}\label{x:men:eq_rot_z}
\end{equation}
%
\par
In this notation in \hyperref[x:men:eq_rot_z]{({\xreffont\ref{x:men:eq_rot_z}})}, the subscript gives the direction of the line fixed by the rotation and the angle provides the counterclockwise rotation in the plane perpendicular to this vector. This vector is called a \terminology{normal} vector for the rotation. Note also that the columns of \(R_{\ve_3}(\theta)\) form an orthogonal set such that each column vector has norm \(1\).%
\par
This idea describes a general rotation matrix \(R_{\vn}(\theta)\) in 3D by specifying a normal vector \(\vn\) and an angle \(\theta\). For example, with roll, a normal vector points from the tail of the aircraft to its tip. It is our goal to understand how we can determine an arbitrary rotation matrix of the form \(R_{\vn}(\theta)\). We can accomplish this by using the rotation around the \(z\) axis and change of basis matrices to find rotation matrices around other axes. Let \(\CS = \{\ve_1, \ve_2, \ve_3\}\) be the standard basis for \(\R^3\)%
\begin{project}{}{x:project:act_rot_x}%
In this activity we see how to determine the rotation matrix around the \(x\) axis using the matrix \(R_{\ve_3}(\theta)\) and a change of basis.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Define a new ordered basis \(\CB\) so that our axis of rotation is the third vector. So in this case the third vector in \(\CB\) will be \(\ve_1\). The other two vectors need to make \(\CB\) an orthonormal set. So we have plenty of choices. For example, we could set \(\CB = \{\ve_2, \ve_3, \ve_1\}\). Find the change of basis matrix \(\underset{\CS \leftarrow \CB}{P}\) from \(\CB\) to \(\CS\).%
\item{}Use the change of basis matrix from part (a) to find the change of basis matrix \(\underset{\CB \leftarrow \CS}{P}\) from \(\CS\) to \(\CB\).%
\item{}To find our rotation matrix around the \(x\) axis, we can first change basis from \(\CS\) to \(\CB\), then perform a rotation around the new \(z\) axis using \hyperref[x:men:eq_rot_z]{({\xreffont\ref{x:men:eq_rot_z}})}, then changing basis back from \(\CB\) to \(\CS\). In other words,%
\begin{equation*}
R_{\ve_1}(\theta) = \underset{\CS \leftarrow \CB}{P} R_{\ve_3}(\theta) \underset{\CB \leftarrow \CS}{P}\text{.}
\end{equation*}
Find the entries of this matrix \(R_{\ve_1}(\theta)\).%
\end{enumerate}
\end{project}%
\begin{paragraphs}{IMPORTANT NOTE.}{g:paragraphs:idp19633176}%
We could have considered using \(\CB_1 = \{\ve_3, \ve_2, \ve_1\}\) in \hyperref[x:project:act_rot_x]{Project Activity~{\xreffont\ref{x:project:act_rot_x}}} instead of \(\CB = \{\ve_2, \ve_3, \ve_1\}\). Then we would have%
\begin{equation*}
\underset{\CS \leftarrow \CB_1}{P} = \left[ \begin{array}{ccc} 0\amp 0\amp 1 \\ 0\amp 1\amp 0 \\ 1\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
%
\end{paragraphs}%
\par
The difference between the two options is that, in the first we have \(\det\left(\underset{\CS \leftarrow \CB_1}{P}\right) = -1\) while \(\det\left(\underset{\CS \leftarrow \CB}{P} \right) = 1\) in the second. Using \(\CB_1\) will give clockwise rotations while \(\CB\) gives counterclockwise rotations (this is the difference between a left hand system and a right hand system). So it is important to ensure that our change of basis matrix is one with determinant \(1\).%
\begin{project}{}{x:project:act_rot_y}%
Repeat \hyperref[x:project:act_rot_y]{Project Activity~{\xreffont\ref{x:project:act_rot_y}}} to find the 3D rotation around the \(y\) axis.%
\end{project}%
We do one more example to illustrate the process before tackling the general case.%
\begin{project}{}{x:project:act_rot_ex}%
In this activity we find the rotation around the axis given by the line \(x=y/2=z\). This line is in the direction of the vector \(\vn = [1 \ 2 \ 1]^{\tr}\). So we start with making a unit vector in the direction of \(\vn\) as the third vector in an ordered basis \(\CB\). The other two vectors need to make \(\CB\) an orthonormal set with \(\det\left(\underset{\CS \leftarrow \CB}{P} \right) = 1\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find a unit vector \(\vw\) in the direction of \(\vn\).%
\item{}Show that \([2 \ -1 \ 0]^{\tr}\) is orthogonal to the vector \(\vw\) from part (a). Then find a unit vector \(\vv\) that is in the same direction as \([2 \ -1 \ 0]^{\tr}\).%
\item{}Let \(\vv\) be as in the previous part. Now the trick is to find a third unit vector \(\vu\) so that \(\CB = \{\vu, \vv, \vw\}\) is an orthonormal set. This can be done with the cross product. If \(\va = [a_1 \ a_2 \ a_3]^{\tr}\) and \(\vb = [b_1 \ b_2 \ b_3]^{\tr}\), then the cross product \(\va \times \vb\) of \(\va\) and \(\vb\) is the vector%
\begin{equation*}
\va \times \vb = \left(a_2b_3-a_3b_2\right) \ve_1 - \left(a_1b_3-a_3b_1\right) \ve_2 + \left(a_1b_2-a_2b_1\right) \ve_3\text{.}
\end{equation*}
(You can check that \(\{\va \times \vb, \va, \vb\}\) is an orthogonal set that gives the correct determinant for the change of basis matrix.) Use the cross product to find a unit vector \(\vu\) so that \(\CB = \{\vu, \vv, \vw\}\) is an orthonormal set.%
\item{}Find the entries of the matrix \(R_{\vw}(\theta)\).%
\end{enumerate}
\end{project}%
In the next activity we summarize the general process to find a 3D rotation matrix \(R_{\vn}(\theta)\) for any normal vector \(\vn\). There is a GeoGebra applet at \href{https://www.geogebra.org/m/n9gbjhfx}{\nolinkurl{geogebra.org/m/n9gbjhfx}} that allows you to visualize rotation matrices in 3D.%
\begin{project}{}{x:project:act_rot_general}%
Let \(\vn = [n_1 \ n_2 \ n_3]^{\tr}\) be a normal vector (nonzero) for our rotation. We need to create an orthonormal basis \(\CB = \{\vu, \vv, \vw\}\) where \(\vw\) is a unit vector in the direction of \(\vn\) so that the change of basis matrix \(\underset{\CS \leftarrow \CB}{P}\) has determinant \(1\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find, by inspection, a vector \(\vy\) that is orthogonal to \(\vn\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19655448}{}\quad{}You may need to consider some cases to ensure that \(\vv\) is not the zero vector.%
\item{}Once we have a normal vector \(\vn\) and a vector \(\vy\) orthogonal to \(\vn\), the vector \(\vz = \vy \times \vn\) gives us an orthogonal set \(\{\vz, \vy, \vn\}\). We then normalize each vector to create our orthonormal basis \(\CB = \{\vu, \vv, \vw\}\). Use this process to find the matrix that produces a \(45^{\circ}\) counterclockwise rotation around the normal vector \([1 \ 0 \ -1]^{\tr}\).%
\end{enumerate}
\end{project}%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 25 Projections onto Subspaces and the Gram-Schmidt Process in \(\R^n\)}
\typeout{************************************************}
%
\begin{chapterptx}{Projections onto Subspaces and the Gram-Schmidt Process in \(\R^n\)}{}{Projections onto Subspaces and the Gram-Schmidt Process in \(\R^n\)}{}{}{x:chapter:chap_gram_schmidt}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp19654552}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What is a projection of a vector onto a subspace and why are such projections important?%
\item{}What is a projection of a vector orthogonal to a subspace and why are such orthogonal projections important?%
\item{}What is the Gram-Schmidt process and why is it useful?%
\item{}What is the QR-factorization of a matrix and why is it useful?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: MIMO Systems}
\typeout{************************************************}
%
\begin{sectionptx}{Application: MIMO Systems}{}{Application: MIMO Systems}{}{}{x:section:sec_mimo}
MIMO (Multiple Input Multiple Output) systems are widely used to increase data transmission rates and improve the quality of wireless communication systems. MIMO is one of several forms of smart antenna technology, and it makes use of multiple antennas at the transmitter to send signals and multiple antennas at the receiver to accept the signals. MIMO systems transmit the same data on multiple streams, which introduces redundancy into the system. MIMO systems can utilize bounced and reflected signals to actually improve signal strength. This is very useful in urban environments in the presence of large buildings and the absence of direct line-of-sight transmission. MIMO systems can transmit several information streams in parallel (known as spatial multiplexing), allowing for increased data transformation rates. The presence of multiple receiver antennas allows for greater reliability in the system.%
\begin{figureptx}{A MIMO system.}{x:figure:F_MIMO}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/MIMO.pdf}
\end{image}%
\tcblower
\end{figureptx}%
In a MIMO system, every transmitter antenna sends a signal to every receiver antenna as illustrated in \hyperref[x:figure:F_MIMO]{Figure~{\xreffont\ref{x:figure:F_MIMO}}}. In each of these transmissions the signal can be disturbed in some way. Two types of disturbances are \terminology{fading} and \terminology{noise}. Fading is due to time variations in the received signal, which can occur because of atmospheric conditions or obstacles over the path which are varying with respect to time. In a MIMO system we have \terminology{multipath fading} for the different paths the signal takes from different antennas to receivers, which causes fluctuations in amplitude, phase and angle of arrival of the received signal. Noise is an unwanted signal which interferes with actual signal. Both fading and noise result in a received signal that is different than the transmitted signal. The problem is how to recover the original signal from the transmitted signal. The QR decomposition is used in this process. To improve the efficiency of MIMO systems, different methods are introduced to determine a QR decomposition. We will discuss MIMO systems in more detail later in this section, and how Householder transformations can be used to find QR decompositions.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_gram_schmidt_intro_noip}
In many situations (least squares approximations, for example) want to find a vector \(\vw\) in a subspace \(W\) of \(\R^n\) that is the ``best'' approximation to a vector \(\vv\) not in the subspace. A natural measure of ``best'' is to find a vector \(\vw\)in \(W\) if one exists, such that the distance from \(\vw\) to \(\vu\) is a minimum. This means we want to find \(\vw\) so that the quantity%
\begin{equation*}
||\vw - \vu ||
\end{equation*}
is as small as possible over all vectors \(\vw\) in \(W\) To do this, we will need to find a suitable projection of the vector \(\vv\) onto the subspace \(W\) We have already done this in the case that \(W = \Span\{\vw_1\}\) is the span of a single vector -{}-{} we can project the vector \(\vv\) in the direction of \(\vw_1\). Our goal is to generalize this to project a vector \(\vv\) onto an entire subspace in a useful way.%
\begin{exploration}{}{x:exploration:pa_6_e_1}%
Let \(\CB = \{\vw_1, \vw_2\}\) be a basis for a subspace \(W\) of \(\R^3\) where \(\vw_1 = [1 \ 0 \ 0]^{\tr}\) and \(\vw_2 = [0 \ 1 \ 0]^{\tr}\) Note that \(\CB\) is an orthonormal basis for \(W\) Let \(\vv = [1 \ 2 \ 1]^{\tr}\) Note that \(W\) is the \(xy\)plane and that \(\vv\) is not in \(W\) as illustrated in \hyperref[x:figure:F_6_d_o_proj]{Figure~{\xreffont\ref{x:figure:F_6_d_o_proj}}}.%
\begin{figureptx}{The space \(W\) and vectors \(\vw_1\), \(\vw_2\), and \(\vv\).}{x:figure:F_6_d_o_proj}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/6_d_O_proj.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the projection \(\vu_1\)of \(\vv\)onto \(W_1 = \Span\{\vw_1\}\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19690904}{}\quad{}See Equation \hyperref[x:men:eq_6_a_projection]{({\xreffont\ref{x:men:eq_6_a_projection}})}.%
\item{}Find the projection \(\vu_2\)of \(\vv\)onto \(W_2 = \Span\{\vw_2\}\)%
\item{}Calculate the distance between \(\vv\)and \(\vu_1\)and \(\vv\)and \(\vu_2\) Which of \(\vu_1\)and \(\vu_2\)is closer to \(\vv\)?%
\item{}Show that the vector \(\frac{3}{4} [1 \ 2 \ 0]^{\tr}\)is in \(W\)and find the distance between \(\vv\)and \(\frac{3}{4} [1 \ 2 \ 0]^{\tr}\)%
\item{}Part (4) shows that neither vector \(\vu_1 = \proj_{\vw_1} \vv\)nor \(\vu_2 = \proj_{\vw_2} \vv\)is the vector in \(W\)that is closest to \(\vv\) We should probably expect this neither projection uses the fact that the other vector might contribute to the closest vector. Let us instead consider the sum \(\vu_3 = \vu_1+\vu_2\) Calculate the components of this vector \(\vu_3\)and determine the distance between \(\vu_3\)and \(\vv\) Which of the three vectors \(\vu_1\) \(\vu_2\) and \(\vu_3\)in \(W\)is closest to \(\vv\)?%
\item{}A picture of \(\vw_1\) \(\vw_2\) \(W\) and \(\vv\)is shown in \hyperref[x:figure:F_6_d_o_proj]{Figure~{\xreffont\ref{x:figure:F_6_d_o_proj}}}. Draw in \(\vu_1\) \(\vu_2\) and \(\vu_3\) Draw a picture of the vector \(\vw\)in \(W\)that appears to be closest to \(\vv\) Does this vector seem to have any relationship to \(\vu_1\) \(\vu_2\) or \(\vu_3\)? If yes, what relationship do you see?%
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Projections onto Subspaces and Orthogonal Projections}
\typeout{************************************************}
%
\begin{sectionptx}{Projections onto Subspaces and Orthogonal Projections}{}{Projections onto Subspaces and Orthogonal Projections}{}{}{x:section:sec_proj_subsp_orth}
\hyperref[x:exploration:pa_6_e_1]{Preview Activity~{\xreffont\ref{x:exploration:pa_6_e_1}}} gives an indication of how we can project a vector \(\vv\) in \(\R^n\) onto a subspace \(W\) of \(\R^n\). If we have an orthogonal basis for \(W\), we can just add the projections of \(\vv\) onto each basis vector. The resulting vector is called the projection of \(\vv\) onto the subspace \(W\). As we did with projections onto vectors, we can also define the projection of \(\vv\) orthogonal to \(W\). Note that to make this all work out properly, we will see that we need an orthogonal basis for \(W\).%
\begin{definition}{}{g:definition:idp19713560}%
\index{projection onto a subspace!in \(\R^n\)}%
\index{projection orthogonal to a subspace}%
Let \(W\) be a subspace of \(\R^n\) and let \(\{\vw_1, \vw_2, \ldots, \vw_m\}\) be an orthogonal basis for \(W\). The \terminology{projection of the vector \(\vv\) in \(V\) onto \(W\)} is the vector%
\begin{equation*}
\proj_W \vv = \frac{\vv \cdot \vw_1}{\vw_1 \cdot \vw_1} \vw_1 + \frac{\vv \cdot \vw_2}{\vw_2 \cdot \vw_2}  \vw_2 + \cdots + \frac{\vv \cdot \vw_m}{\vw_m \cdot \vw_m}  \vw_m\text{.}
\end{equation*}
%
\par
The \terminology{projection of \(\vv\) orthogonal to \(W\)} is the vector%
\begin{equation*}
\proj_{\perp W} \vv = \vv - \proj_W \vv\text{.}
\end{equation*}
%
\end{definition}
The notation \(\proj_{\perp W} \vv\) indicates that we expect this vector to be orthogonal to every vector in \(W\). We address this in the following activity.%
\begin{activity}{}{x:activity:act_6_e_orth_projection}%
Let \(W = \Span \{\vw_1, \vw_2\}\) in \(\R^3\), with \(\vw_1=[1 \ 0 \ 0]^{\tr}\) and \(\vw_2= [0 \ 1 \ 0]^{\tr}\), and \(\vv = [1 \ 2 \ 1]^{\tr}\) as in \hyperref[x:exploration:pa_6_e_1]{Preview Activity~{\xreffont\ref{x:exploration:pa_6_e_1}}}. Recall that \(\proj_W \vv = [1 \ 2 \ 0]^{\tr}\). Find the projection of \(\vv\) orthogonal to \(W\) and show directly that \(\proj_{\perp W} \vv\) is orthogonal to the basis vectors for \(W\) (and hence to every vector in \(W\)).%
\end{activity}%
\hyperref[x:activity:act_6_e_orth_projection]{Activity~{\xreffont\ref{x:activity:act_6_e_orth_projection}}} indicates that the vector \(\proj_{\perp W} \vv\) is in fact orthogonal to every vector in \(W\). To see that this is true in general, let \(\CB = \{\vw_1, \vw_2, \ldots, \vw_m\}\) be an orthogonal basis for a subspace \(W\) of \(\R^n\) and let \(\vv\) be a vector in \(\R^n\). Let%
\begin{equation*}
\vw =  \proj_W \vv = \frac{\vv \cdot \vw_1}{\vw_1 \cdot \vw_1} \vw_1 + \frac{\vv \cdot \vw_2}{\vw_2 \cdot \vw_2}  \vw_2 + \cdots + \frac{\vv \cdot \vw_m}{\vw_m \cdot \vw_m}  \vw_m\text{.}
\end{equation*}
%
\par
Then \(\vv - \vw\) is the projection of \(\vv\) orthogonal to \(W\). We will show that \(\vv - \vw\) is orthogonal to every basis vector for \(W\). Since \(\CB\) is an orthogonal basis for \(W\), we know that \(\vw_i \cdot \vw_j = 0\) for \(i \neq j\). So if \(k\) is between 1 and \(m\) then%
\begin{align*}
\vw_k \cdot (\vv - \vw) \amp = (\vw_k \cdot \vv) - (\vw_k \cdot \vw)\\
\amp = (\vw_k \cdot \vv) - \left[\vw_k \cdot \left(\ds \frac{\vv \cdot \vw_1}{\vw_1 \cdot \vw_1} \vw_1 + \frac{\vv \cdot \vw_2}{\vw_2 \cdot \vw_2}  \vw_2 + \cdots + \frac{\vv \cdot \vw_m}{\vw_m \cdot \vw_m}  \vw_m \right)\right]\\
\amp = (\vw_k \cdot \vv) - \left(\frac{\vv \cdot \vw_k}{\vw_k \cdot \vw_k}\right) (\vw_k \cdot \vw_k)\\
\amp = (\vw_k \cdot \vv) - (\vv \cdot \vw_k)\\
\amp = 0\text{.}
\end{align*}
%
\par
So the vector \(\vv - \vw\) is orthogonal to every basis vector for \(W\), and therefore to every vector in \(W\) (\hyperref[x:theorem:thm_6_a_dot_pd_orth_complement_basis]{Theorem~{\xreffont\ref{x:theorem:thm_6_a_dot_pd_orth_complement_basis}}}). Because \(\CB\) is a basis for \(W\), if \(\vx\) is in \(W\), then%
\begin{equation*}
\vx = c_1 \vw_1 + c_2 \vw_2 + \cdots + c_m \vw_m
\end{equation*}
for some scalars \(c_1\), \(c_2\), \(\ldots\), \(c_m\). So%
\begin{equation*}
(\vv - \vw) \cdot \vx = \sum_{k=1}^m c_k (\vv - \vw) \cdot \vw_k = 0\text{,}
\end{equation*}
and so \(\vv - \vw\) is orthogonal to every vector in \(W\).%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Best Approximations}
\typeout{************************************************}
%
\begin{sectionptx}{Best Approximations}{}{Best Approximations}{}{}{x:section:sec_best_approx}
We have seen that \(\proj_{\perp W} \vv\) is orthogonal to every vector in \(W\), which suggests that \(\proj_W \vv\) is in fact the vector in \(W\) that is closest to \(\vv\). We now verify this fact and conclude that \(\proj_W \vv\) is the vector in \(W\) closest to \(\vv\) and therefore the best approximation of \(\vv\) by a vector in \(W\).%
\begin{theorem}{}{}{g:theorem:idp19767688}%
Let \(\B = \{\vw_1, \vw_2, \ldots, \vw_m\}\) be an orthogonal basis for a subspace \(W\) of \(\R^n\) and let \(\vv\) be a vector in \(\R^n\). Then%
\begin{equation*}
||\vv - \proj_W \vv || \lt  || \vv - \vx ||
\end{equation*}
for every vector \(\vx\) different from \(\proj_W \vv\) in \(W\).%
\end{theorem}
\begin{proof}{}{g:proof:idp19762952}
Let \(\B = \{\vw_1, \vw_2, \ldots, \vw_m\}\) be an orthogonal basis for a subspace \(W\) of \(\R^n\) and let \(\vv\) be a vector in \(\R^n\). Let \(\vx\) be a vector in \(W\). The vector \(\proj_W \vv - \vx\) is in \(W\), so is orthogonal to \(\proj_{\perp W} \vv = \vv - \proj_W \vv\). Thus, the dotted triangle whose vertices are the tips of the vectors \(\vv\), \(\vx\), and \(\proj_W \vv\) in \hyperref[x:figure:F_best_approx]{Figure~{\xreffont\ref{x:figure:F_best_approx}}} is a right triangle.%
\begin{figureptx}{The best approximation to \(\vv\) in \(W\)}{x:figure:F_best_approx}{}%
\begin{image}{0.2}{0.6}{0.2}%
\includegraphics[width=\linewidth]{external/6_d_best_approx.pdf}
\end{image}%
\tcblower
\end{figureptx}%
The Pythagorean theorem then shows that%
\begin{equation*}
|| \vv - \vx ||^2 = || \proj_{\perp W} \vv ||^2 + || \proj_W \vv - \vx ||^2\text{.}
\end{equation*}
%
\par
Now \(\vx \neq \proj_W \vv\), so \(|| \proj_W \vv - \vx ||^2 > 0\). This implies%
\begin{equation*}
|| \vv - \vx ||^2 > || \proj_{\perp W} \vv ||^2
\end{equation*}
and it follows that%
\begin{equation*}
|| \vv - \vx || > || \proj_{\perp W} \vv ||\text{.}\qedhere
\end{equation*}
%
\end{proof}
This theorem shows that the distance from \(\proj_W \vv\) to \(\vv\) is less than the distance from any other vector in \(W\) to \(\vv\). So \(\proj_W \vv\) is the best approximation to \(\vv\) of all the vectors in \(W\).%
\par
If \(\vv = [v_1 \ v_2 \ v_3 \ \ldots \ v_m]^{\tr}\) and \(\proj_W \vv =  [w_1 \ w_2 \ w_3 \ \ldots \ w_m]^{\tr}\), then the square of the error in approximating \(\vv\) by \(\proj_W \vv\) is given by%
\begin{equation*}
|| \vv - \proj_W \vv ||^2 = \sum_{i=1}^m (v_i - w_i)^2\text{.}
\end{equation*}
%
\par
So \(\proj_W \vv\) minimizes this sum of squares over all vectors in \(W\). As a result, we call \(\proj_W \vv\) the \terminology{least squares approximation} to \(\vv\).%
\begin{activity}{}{g:activity:idp19781640}%
Let \(\CB = \left\{\left[ \begin{array}{r} 1 \\ 0 \\ -1 \\ 0 \end{array} \right], \left[ \begin{array}{r} 0 \\ 1 \\ 0 \\ -1 \end{array} \right], \left[ \begin{array}{r} -1 \\ 1 \\ -1 \\ 1 \end{array} \right]\right\}\) and let \(W = \Span(\CB)\) in \(\R^4\). Find the best approximation in \(W\) to the vector \(\vv = \left[ \begin{array}{r} 2 \\ 3 \\ 1 \\ -1 \end{array} \right]\) in \(W\).%
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Gram-Schmidt Process}
\typeout{************************************************}
%
\begin{sectionptx}{The Gram-Schmidt Process}{}{The Gram-Schmidt Process}{}{}{x:section:sec_gram_schmidt_process}
We have seen that orthogonal bases make computations very convenient. However, until now we had no convenient way to construct orthogonal bases. The problem we address in this section is how to create an orthogonal basis from any basis.%
\begin{exploration}{}{x:exploration:pa_6_d_2_no_inner_product}%
Let \(W = \Span \{\vv_1, \vv_2, \vv_3\}\) in \(\R^4\), where \(\vv_1 = [1 \ 1 \ 1 \ 1]^{\tr}\), \(\vv_2 = [-1 \ 4 \ 4 \ -1]^{\tr}\), and \(\vv_3 = [2 \ -1 \ 1 \ 0]^{\tr}\). Our goal in this preview activity is to begin to understand how we can find an orthogonal set \(\CB = \{\vw_1, \vw_2, \vw_3\}\) in \(\R^4\) so that \(\Span \ \CB = W\). To begin, we could start by letting \(\vw_1 = \vv_1\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Now we want to find a vector in \(W\) that is orthogonal to \(\vw_1\). Let \(W_1 = \Span\{\vw_1\}\). Explain why \(\vw_2 = \proj_{\perp W_1} \vv_2\) is in \(W\) and is orthogonal to \(\vw_1\). Then calculate the vector \(\vw_2\).%
\item{}Next we need to find a third vector \(\vw_3\) that is in \(W\) and is orthogonal to both \(\vw_1\) and \(\vw_2\). Let \(W_2 = \Span \{\vw_1, \vw_2\}\). Explain why \(\vw_3 = \proj_{\perp W_2} \vv_3\) is in \(W\) and is orthogonal to both \(\vw_1\) and \(\vw_2\). Then calculate the vector \(\vw_3\).%
\item{}Explain why the set \(\{\vw_1, \vw_2, \vw_3\}\) is an orthogonal basis for \(W\).%
\end{enumerate}
\end{exploration}%
\hyperref[x:exploration:pa_6_d_2_no_inner_product]{Preview Activity~{\xreffont\ref{x:exploration:pa_6_d_2_no_inner_product}}} shows the first steps of the Gram-Schmidt process to construct an orthogonal basis from any basis of a subspace in \(\R^n\). To understand why the process works in general, let \(\{\vv_1, \vv_2, \ldots, \vv_m\}\) be a basis for a subspace \(W\) of \(\R^n\). Let \(\vw_1 = \vv_1\) and let \(W_1 = \Span\{\vw_1\}\). Since \(\vw_1 = \vv_1\) we have that \(W_1 = \Span\{\vw_1\} = \Span\{\vv_1\}\).%
\par
The vectors \(\vv_1 = \vw_1\) and \(\vv_2\) are possibly not orthogonal, but we know the orthogonal projection of \(\vv_2\) onto \(W_1^{\perp}\) is orthogonal to \(\vw_1\). Let%
\begin{equation*}
\vw_2 = \proj_{\perp W_1} \vv_2 = \vv_2 -  \frac{ \vv_2 \cdot \vw_1 }{ \vw_1 \cdot  \vw_1 } \vw_1\text{.}
\end{equation*}
%
\par
Then \(\{\vw_1, \vw_2\}\) is an orthogonal set. Note that \(\vw_1 = \vv_1 \neq \vzero\), and the fact that \(\vv_2 \notin W_1\) implies that \(\vw_2 \neq \vzero\). So the set \(\{\vw_1, \vw_2\}\) is linearly independent, being a set of non-zero orthogonal vectors. Now the question is whether \(\Span\{\vw_1,\vw_2\} = \Span\{\vv_1,\vv_2\}\). Note that \(\vw_2\) is a linear combination of \(\vv_1\) and \(\vv_2\), so \(\vw_2\) is in \(\Span\{\vv_1,\vv_2\}\). Since \(\Span\{\vw_1,\vw_2\}\) is a 2-dimensional subspace of the 2-dimensional space \(\Span\{\vv_1,\vv_2\}\), it must be true that \(\Span\{\vw_1,\vw_2\} = W_2 = \Span\{\vv_1,\vv_2\}\).%
\par
Now we take the next step, adding \(\vv_3\) into the mix. The vector%
\begin{equation*}
\vw_3 = \proj_{\perp W_2} \vv_3 = \vv_3 - \frac{ \vv_3 \cdot  \vw_1 }{ \vw_1 \cdot  \vw_1 } \vw_1 - \frac{ \vv_3 \cdot  \vw_2 }{ \vw_2 \cdot  \vw_2 } \vw_2
\end{equation*}
is orthogonal to both \(\vw_1\) and \(\vw_2\) and, by construction, \(\vw_3\) is a linear combination of \(\vv_1\), \(\vv_2\), and \(\vv_3\). So \(\vw_3\) is in \(W_3 = \Span\{\vv_1, \vv_2, \vv_3\}\). The fact that \(\vv_3 \notin W_2\) implies that \(\vw_3 \neq \vzero\) and \(\{\vw_1, \vw_2, \vw_3\}\) is a linearly independent set. Since \(\Span\{\vw_1, \vw_2, \vw_3\}\) is a 3-dimensional subspace of the 3-dimensional space \(\Span\{\vv_1,\vv_2,\vv_3\}\), we conclude that \(\Span\{\vw_1, \vw_2, \vw_3\} = W_3 = \Span\{\vv_1,\vv_2,\vv_3\}\).%
\par
We continue inductively in this same manner. If we have constructed a set \(\{\vw_1\), \(\vw_2\), \(\vw_3\), \(\ldots\), \(\vw_{k-1}\}\) of \(k-1\) orthogonal vectors such that%
\begin{equation*}
\Span\{\vw_1, \vw_2, \vw_3, \ldots, \vw_{k-1}\} = \Span\{\vv_1,\vv_2,\vv_3, \ldots, \vv_{k-1}\}\text{,}
\end{equation*}
then we let%
\begin{align*}
\vw_{k} \amp = \proj_{\perp W_{k-1}} \vv_k\\
\amp = \vv_k - \ds \frac{ \vv_k \cdot  \vw_1 }{ \vw_1 \cdot  \vw_1 } \vw_1 - \frac{ \vv_k \cdot  \vw_2 }{ \vw_2 \cdot  \vw_2 } \vw_2 - \cdots - \frac{ \vv_k \cdot  \vw_{k-1} }{ \vw_{k-1} \cdot  \vw_{k-1} } \vw_{k-1}\text{,}
\end{align*}
where%
\begin{equation*}
W_{k-1} = \Span\{\vw_1, \vw_2, \vw_3, \ldots, \vw_{k-1}\}\text{.}
\end{equation*}
%
\par
We know that \(\vw_k\) is orthogonal to \(\vw_1\), \(\vw_2\), \(\ldots\), \(\vw_{k-1}\). Since \(\vw_1\), \(\vw_2\), \(\ldots\), \(\vw_{k-1}\), and \(\vv_k\) are all in \(W_k = \Span\{\vv_1, \vv_2, \ldots, \vv_k\}\) we see that \(\vw_k\) is also in \(W_k\). Since \(\vv_k \notin W_{k-1}\) implies that \(\vw_k \neq \vzero\) and \(\{\vw_1, \vw_2, \ldots, \vw_k\}\) is a linearly independent set. Then \(\Span\{\vw_1, \vw_2, \vw_3, \ldots, \vw_{k}\}\) is a \(k\)-dimensional subspace of the \(k\)-dimensional space \(W_k\), so it follows that%
\begin{equation*}
\Span\{\vw_1, \vw_2, \vw_3, \ldots, \vw_{k}\} = W_k = \Span\{\vv_1,\vv_2,\vv_3, \ldots, \vv_{k}\}\text{.}
\end{equation*}
%
\par
This process will end when we have an orthogonal set \(\{\vw_1\), \(\vw_2\), \(\vw_3\), \(\ldots\), \(\vw_{m}\}\) with \(\Span\{\vw_1\), \(\vw_2\), \(\vw_3\), \(\ldots\), \(\vw_{m}\}\) = \(W\).%
\par
We summarize the process in the following theorem.%
\begin{theorem}{The Gram-Schmidt Process.}{}{x:theorem:thm_6_d_Gram_Schmidt_noips}%
\index{Gram-Schmidt Process!in \(\R^n\)}%
Let \(\{\vv_1, \vv_2, \ldots, \vv_m\}\) be a basis for a subspace \(W\) of \(\R^n\). The set \(\{\vw_1, \vw_2, \vw_3, \ldots, \vw_{m}\}\) defined by%
\begin{itemize}[label=\textbullet]
\item{}\(\vw_1 = \vv_1\),%
\item{}\(\vw_2 = \vv_2 - \ds \frac{ \vv_2 \cdot \vw_1 }{ \vw_1 \cdot \vw_1 } \vw_1\),%
\item{}\(\vw_3 = \vv_3 - \ds \frac{ \vv_3 \cdot \vw_1 }{ \vw_1 \cdot \vw_1 } \vw_1 - \frac{ \vv_3 \cdot \vw_2 }{ \vw_2 \cdot \vw_2 } \vw_2\), \(\vdots\)%
\item{}\(\vw_m = \vv_m - \ds \frac{ \vv_m \cdot \vw_1 }{ \vw_1 \cdot \vw_1 } \vw_1 - \frac{ \vv_m \cdot \vw_2 }{ \vw_2 \cdot \vw_2 } \vw_2 - \cdots - \frac{ \vv_m \cdot \vw_{m-1} }{ \vw_{m-1} \cdot \vw_{m-1} } \vw_{m-1}\).%
\end{itemize}
%
\par
is an orthogonal basis for \(W\). Moreover,%
\begin{equation*}
\Span\{\vw_1, \vw_2, \vw_3, \ldots, \vw_{k}\} = \Span\{\vv_1,\vv_2,\vv_3, \ldots, \vv_{k}\}
\end{equation*}
for \(1\leq k\leq m\).%
\end{theorem}
The Gram-Schmidt process builds an orthogonal basis \(\{\vw_1, \vw_2, \vw_3, \ldots, \vw_{m}\}\) for us from a given basis. To make an orthonormal basis \(\{\vu_1, \vu_2, \vu_3, \ldots, \vu_{m}\}\), all we need do is normalize each basis vector: that is, for each \(i\), we let%
\begin{equation*}
\vu_i = \ds \frac{\vw_i}{|| \vw_i ||} \,\text{.}
\end{equation*}
%
\begin{activity}{}{x:activity:act_6_d_gs_examples}%
Let \(W = \Span \left\{\left[ \begin{array}{c} 1\\1\\0\\0 \end{array} \right], \left[ \begin{array}{c} 0\\1\\1\\0 \end{array} \right], \left[ \begin{array}{c} 0\\0\\1\\1 \end{array} \right] \right\}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Use the Gram-Schmidt process to find an orthogonal basis for \(W\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19848328}{}\quad{}Order your vectors carefully to minimize computation.%
\item{}Find the projection of the vector \(\vv = [0 \ 1 \ 1 \ 1]^{\tr}\) onto \(W\).%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The QR Factorization of a Matrix}
\typeout{************************************************}
%
\begin{sectionptx}{The QR Factorization of a Matrix}{}{The QR Factorization of a Matrix}{}{}{x:section:sec_qr_fact}
There are several different factorizations, or decompositions, of matrices where each matrix is written as a product of certain types of matrices: LU decomposition using lower and upper triangular matrices (see \hyperref[x:chapter:chap_det_properties]{Section~{\xreffont\ref{x:chapter:chap_det_properties}}}), EVD (EigenVector Decomposition) decomposition using eigenvectors and diagonal matrices (see \hyperref[x:chapter:chap_diagonalization]{Section~{\xreffont\ref{x:chapter:chap_diagonalization}}}), and in this section we will introduce the QR decomposition of a matrix. The QR decomposition is one of the most important tools for matrix computations. Jack Dongarra and Francis Sullivan\footnote{Jack Dongarra and Francis Sullivan. Introduction to the top 10 algorithms. Computing in Science and Engineering, 2: 22-23, 2000.\label{g:fn:idp19857672}} nominated the QR decomposition as one of the ``10 algorithms with the greatest influence on the development and practice of science and engineering in the 20th century.'' The QR algorithm's importance stems from the fact that is a ``genuinely new contribution to the field of numerical analysis and not just a refinement of ideas given by Newton, Gauss, Hadamard, or Schur.''\footnote{[3] Beresford N. Parlett. The QR algorithm. Computing in Science and Engineering, 2:38-42, 2000.\label{g:fn:idp19852936}} Most computer algebra systems (e.g., MATLAB) use a variant of the QR decomposition to compute eigenvalues. While there are other methods for approximating eigenvalues, the QR algorithm stands out. For example, the power method is useful when a matrix is large and contains a lot of zero entries (such matrices are called \terminology{sparse}). When most of the entries of a matrix are nonzero, the power method is not recommended. The better, more sophisticated, alternative in these situations is to use the QR decomposition. The QR factorization has applications to solving least squares problems and approximating eigenvalues of matrices.%
\begin{activity}{}{x:activity:act_6_e_QR}%
Let \(A = \left[ \begin{array}{cc} 1\amp 0 \\ 0\amp 0 \\ 0\amp 2 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find an orthonormal basis \(\CB\) for \(\Col A\). Let \(Q\) be the matrix whose columns are these orthonormal basis vectors.%
\item{}Write the columns of \(A\) as linear combinations of the columns of \(Q\). That is, if \(A = [\va_1 \ \va_2]\), find \([\va_1]_{\CB}\) and \([\va_2]_{\CB}\). Let \(R = [[\va_1]_{\CB} \ [\va_2]_{\CB}]\).%
\item{}Find the product \(QR\) and compare to \(A\).%
\end{enumerate}
\end{activity}%
\hyperref[x:activity:act_6_e_QR]{Activity~{\xreffont\ref{x:activity:act_6_e_QR}}} contains the main ideas to find the QR factorization of a matrix. Let%
\begin{equation*}
A = [\va_1  \ \va_2  \ \va_3  \ \cdots  \ \va_n ]
\end{equation*}
be an \(m \times n\) matrix with rank\footnote{Recall that the rank of a matrix \(A\) is the dimension of the column space of \(A\).\label{g:fn:idp19861000}} \(n\). We can use the Gram-Schmidt process to find an orthonormal basis \(\{\vu_1, \vu_2, \ldots, \vu_n\}\) for \(\Col A\). Recall also that \(\Span\{\vu_1, \vu_2, \ldots, \vu_k\}  = \Span\{\va_1, \va_2, \ldots, \va_k\}\) for any \(k\) between 1 and \(n\). Let%
\begin{equation*}
Q = [\vu_1 \   \vu_2 \  \vu_3 \  \cdots \  \vu_n ]\text{.}
\end{equation*}
%
\par
If \(k\) is between 1 and \(n\), then \(\va_k\) is in \(\Span\{\vu_1, \vu_2, \ldots, \vu_k\}\) and%
\begin{equation*}
\va_k = r_{1k}\vu_1 + r_{2k}\vu_2 + \cdots + r_{kk}\vu_k
\end{equation*}
for some scalars \(r_{1k}\), \(r_{2k}\), \(\ldots\), \(r_{kk}\). Then%
\begin{equation*}
Q \left[ \begin{array}{c} r_{1k} \\ r_{2k} \\ \vdots \\ r_{kk} \\ 0 \\ \vdots \\ 0 \end{array}  \right] = \va_k\text{.}
\end{equation*}
%
\par
If we let \(\vr_k = \left[ \begin{array}{c} r_{1k} \\ r_{2k} \\ \vdots \\ r_{kk} \\ 0 \\ \vdots \\ 0 \end{array}  \right]\) for \(k\) from 1 to \(n\), then%
\begin{equation*}
A = [Q\vr_1  \ Q\vr_2  \ Q\vr_3  \ \cdots  \ Q\vr_k]\text{.}
\end{equation*}
%
\par
This is the QR factorization of \(A\) into the product%
\begin{equation*}
A = QR
\end{equation*}
where the columns of \(Q\) form an orthonormal basis for \(\Col A\) and%
\begin{equation*}
R = [\vr_1  \ \vr_2  \ \vr_3  \ \cdots  \ \vr_n ] = \left[ \begin{array}{cccccc} r_{11}\amp r_{12}\amp r_{13}\amp  \cdots \amp r_{1n-1}\amp r_{1n} \\ 0\amp r_{22}\amp r_{23}\amp  \cdots \amp r_{2n-1}\amp r_{2n} \\ 0\amp 0\amp r_{33}\amp  \cdots \amp r_{3n-1}\amp r_{3n} \\ \vdots\amp \vdots \amp \vdots \amp \vdots \amp \vdots \amp \vdots \\ 0\amp 0\amp 0\amp  \cdots \amp 0\amp r_{nn} \end{array}  \right]
\end{equation*}
is an upper triangular matrix. Note that \(Q\) is an \(m \times n\) matrix and \(R = [r_{ij}]\) is an \(n \times n\) matrix with \(r_{ii} \neq 0\) for each \(i\).%
\begin{activity}{}{g:activity:idp19879560}%
The set \(\{\vu_1, \vu_2\}\), where \(\vu_1 = [1 \ 1 \ 1 \ 1]^{\tr}\) and \(\vu_2 = [1 \ -1 \ 1 \ -1]^{\tr}\) is an orthogonal basis for the column space of a \(4 \times 2\) matrix \(A = [\va_1 \ \va_2]\). Moreover, \(\va_1 = 2\vu_1\) and \(\va_2 = 3\vu_2+4\vu_2\). What is \(A\)?%
\end{activity}%
The QR factorization provides a widely used algorithm (the QR algorithm) for approximating all of the eigenvalues of a matrix. The computer system MATLAB utilizes four versions of the QR algorithm to approximate the eigenvalues of real symmetric matrices, eigenvalues of real nonsymmetric matrices, eigenvalues of pairs of complex matrices, and singular values of general matrices.%
\par
The algorithm works as follows.%
\begin{itemize}[label=\textbullet]
\item{}Start with an \(n \times n\) matrix \(A\). Let \(A_1 = A\).%
\item{}Find the QR factorization for \(A_1\) and write it as \(A_1 = Q_1R_1\), where \(Q_1\) is orthogonal and \(R_1\) is upper triangular.%
\item{}Let \(A_2 = Q_1^{-1}A_1Q_1 = Q_1^{\tr}AQ_1 = R_1Q_1\). Find the QR factorization of \(A_2\) and write it as \(A_2 = Q_2R_2\).%
\item{}Let \(A_3 = Q_2^{-1}A_2Q_2 = Q_2^{\tr}AQ_2 = R_2Q_2\). Find the QR factorization of \(A_3\) and write it as \(A_3 = Q_3R_3\).%
\item{}Continue in this manner to obtain a sequence \(\{A_k\}\) where \(A_k = Q_kR_k\) and \(A_{k+1} = R_kQ_k\).%
\end{itemize}
%
\par
Note that \(A_{k+1} = Q_k^{-1}A_kQ_k\) and so all of the matrices \(A_k\) are similar to each other and therefore all have the same eigenvalues. We won't go into the details, but it can be shown that if the eigenvalues of \(A\) are real and have distinct absolute values, then the sequence \(\{A_i\}\) converges to an upper triangular matrix with the eigenvalues of \(A\) as the diagonal entries. If some of the eigenvalues of \(A\) are complex, then the sequence \(\{A_i\}\) converges to a block upper triangular matrix, where the diagonal blocks are either \(1 \times 1\) (approximating the real eigenvalues of \(A\)) or \(2 \times 2\) (which provide a pair of conjugate complex eigenvalues of \(A\)).%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_gram_schmidt_examples}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp19894152}%
Let \(W\) be the subspace of \(\R^4\) spanned by \(\vw_1 = [1 \ 0 \ 0 \ 0]^{\tr}\), \(\vw_2 = [1 \ 1 \ 1 \ 0]^{\tr}\), and \(\vw_3 = [1 \ 2 \ 0 \ 1]^{\tr}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Use the Gram-Schmidt process to find an orthonormal basis for the subspace \(W\) of \(\R^4\) spanned by \(\vw_1 = [1 \ 0 \ 0 \ 0]^{\tr}\), \(\vw_2 = [1 \ 1 \ 1 \ 0]^{\tr}\), and \(\vw_3 = [1 \ 2 \ 0 \ 1]^{\tr}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp19900168}{}\quad{}First note that \(\vw_1\), \(\vw_2\), and \(\vw_3\) are linearly independent. We let \(\vv_1 = \vw_1\) and the Gram-Schmidt process gives us%
\begin{align*}
\vv_2 \amp = \vw_2 - \frac{\vw_2 \cdot \vv_1}{\vv_1 \cdot \vv_1} \vv_1\\
\amp = [1 \ 1 \ 1 \ 0]^{\tr} - \frac{1}{1}[1 \ 0 \ 0 \ 0]^{\tr}\\
\amp = [0 \ 1 \ 1 \ 0]^{\tr}
\end{align*}
and%
\begin{align*}
\vv_3 \amp = \vw_3 - \frac{\vw_3 \cdot \vv_1}{\vv_1 \cdot \vv_1} \vv_1 - \frac{\vw_3 \cdot \vv_2}{\vv_2 \cdot \vv_2} \vv_2\\
\amp = [1 \ 2 \ 0 \ 1]^{\tr} - \frac{1}{1}[1 \ 0 \ 0 \ 0]^{\tr} - \frac{2}{2}[0 \ 1 \ 1 \ 0]^{\tr}\\
\amp = [0 \ 1 \ -1 \ 1]^{\tr}\text{.}
\end{align*}
So \(\{\vv_1, \vv_2, \vv_3\}\) is an orthogonal basis for \(W\). An orthonormal basis is found by dividing each vector by its magnitude, so%
\begin{equation*}
\{[1 \ 0 \ 0 \ 0]^{\tr}, \frac{1}{\sqrt{2}} [0 \ 1 \ 1 \ 0]^{\tr}, \frac{1}{\sqrt{3}}[0 \ 1 \ -1 \ 1]^{\tr}\}
\end{equation*}
is an orthonormal basis for \(W\).%
\item{}Find a QR factorization of the matrix \(A = \left[ \begin{array}{cccc} 1\amp 1\amp 1\amp 0 \\ 0\amp 1\amp 2\amp 0 \\ 0\amp 1\amp 0\amp 0 \\ 0\amp 0\amp 1\amp 1 \end{array} \right]\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp19912072}{}\quad{}Technology shows that the reduced row echelon form of \(A\) is \(I_4\), so the columns of \(A\) are linearly independent and \(A\) has rank \(4\). From part (a) we have an orthogonal basis for the span of the first three columns of \(A\). To find a fourth vector to add so that the span is \(\Col \ A\), we apply the Gram-Schmidt process one more time with \(\vw_4 = [0 \ 0 \ 0 \ 1]^{\tr}\):%
\begin{align*}
\vv_4 \amp = \vw_4 - \frac{\vw_4 \cdot \vv_1}{\vv_1 \cdot \vv_1} \vv_1 - \frac{\vw_4 \cdot \vv_2}{\vv_2 \cdot \vv_2} \vv_2 - \frac{\vw_4 \cdot \vv_3}{\vv_3 \cdot \vv_3} \vv_3\\
\amp = [0 \ 0 \ 0 \ 1]^{\tr} - \frac{0}{1}[1 \ 0 \ 0 \ 0]^{\tr} - \frac{0}{2}[0 \ 1 \ 1 \ 0]^{\tr} - \frac{1}{3}[0 \ 1 \ -1 \ 1]^{\tr}\\
\amp = \frac{1}{3} [0 \ -1 \ 1 \ 2]^{\tr}\text{.}
\end{align*}
So \(\{\vu_1, \vu_2, \vu_3, \vu_4\}\) is an orthonormal basis for \(\Col \ A\), where%
\begin{equation*}
\begin{array}{ccc} \vu_1 = [1 \ 0 \ 0 \ 0]^{\tr} \amp  \amp \vu_2 = \frac{\sqrt{2}}{2}[0 \ 1 \ 1 \ 0]^{\tr} \\ \vu_3 = \frac{\sqrt{3}}{3}[0 \ 1 \ -1 \ 1]^{\tr} \amp    \amp \vu_4 = \frac{\sqrt{6}}{6} [0 \ -1 \ 1 \ 2]^{\tr}. \end{array}
\end{equation*}
This makes%
\begin{equation*}
Q = \left[  \begin{array}{ccrr} 1\amp 0\amp 0\amp 0 \\ 0\amp \frac{\sqrt{2}}{2}\amp \frac{\sqrt{3}}{3}\amp -\frac{\sqrt{6}}{6} \\ 0\amp \frac{\sqrt{2}}{2}\amp -\frac{\sqrt{3}}{3}\amp \frac{\sqrt{6}}{6} \\ 0\amp 0\amp \frac{\sqrt{3}}{3}\amp \frac{\sqrt{6}}{3} \end{array}  \right]\text{.}
\end{equation*}
To find the matrix \(R\), we write the columns of \(A\) in terms of the basis vectors \(\vu_1\), \(\vu_2\), \(\vu_3\), and \(\vu_4\). Technology shows that the reduced row echelon form of \([Q  \ | \ A]\) is%
\begin{equation*}
\left[   \begin{array}{cccc|cccc} 1\amp 0\amp 0\amp 0\amp 1\amp 1\amp 1\amp 0 \\ 0\amp 1\amp 0\amp 0\amp 0\amp \sqrt{2}\amp \sqrt{2}\amp 0 \\ 0\amp 0\amp 1\amp 0\amp 0\amp 0\amp \sqrt{3}\amp \frac{\sqrt{3}}{3} \\ 0\amp 0\amp 0\amp 1\amp 0\amp 0\amp 0\amp \frac{\sqrt{6}}{3} \end{array}  \right]\text{.}
\end{equation*}
So%
\begin{equation*}
R = \left[  \begin{array}{cccc} 1\amp 1\amp 1\amp 0 \\ 0\amp \sqrt{2}\amp \sqrt{2}\amp 0 \\ 0\amp 0\amp \sqrt{3}\amp \frac{\sqrt{3}}{3} \\ 0\amp 0\amp 0\amp \frac{\sqrt{6}}{3} \end{array}  \right]\text{.}
\end{equation*}
%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp19923080}%
Let \(W = \Span\{[1 \ 0 \ 1 \ 0]^{\tr}, [0 \ 0 \ 1 \ -1]^{\tr}, [1 \ 0 \ -1 \ 0]^{\tr}\}\). Find the vector in \(W\) that is closest to the vector \([1 \ 1 \ 1 \ 1]^{\tr}\). Provide a numeric measure of the error in approximating \([1 \ 1 \ 1 \ 1]^{\tr}\) by this vector.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp19920520}{}\quad{}Our job is to find \(\proj_{W} [1 \ 1 \ 1 \ 1]^{\tr}\). To do this, we need an orthogonal basis of \(W\). Let \(\vv_1 = [1  \ 0 \ 1 \ 0]^{\tr}\), \(\vv_2 = [0 \ 0 \ 1 \ -1]^{\tr}\), and \(\vv_3 = [1 \ 0 \ -1 \ 0]^{\tr}\). Technology shows that each column of the matrix \([\vv_1 \ \vv_2 \ \vv_3]\) is a pivot column, so the set \(\{\vv_1, \vv_2, \vv_3\}\) is a basis for \(W\). We apply the Gram-Schmidt process to this basis to obtain an orthogonal basis \(\{\vw_1, \vw_2, \vw_3\}\) of \(W\). We start with \(\vw_1= \vv_1\), then%
\begin{align*}
\vw_2 \amp = \vv_2 - \frac{\vv_2 \cdot \vw_1}{ \vw_1 \cdot \vw_1} \vw_1\\
\amp = [0 \ 0 \ 1 \ -1]^{\tr} - \frac{1}{2} [1  \ 0 \ 1 \ 0]^{\tr}\\
\amp = \frac{1}{2}[-1 \ 0 \ 1 \ -2]^{\tr}
\end{align*}
and%
\begin{align*}
\vw_3 \amp = \vv_3 - \frac{\vv_3 \cdot \vw_1}{ \vw_1 \cdot \vw_1} \vw_1 - \frac{\vv_3 \cdot \vw_2}{ \vw_2 \cdot \vw_2} \vw_2\\
\amp = [1 \ 0 \ -1 \ 0]^{\tr} - \frac{0}{2} [1  \ 0 \ 1 \ 0]^{\tr} + \frac{2}{3} \left(\frac{1}{2}[-1 \ 0 \ 1 \ -2]^{\tr}\right)\\
\amp = \frac{1}{3} [2 \ 0 \ -2 \ -2]^{\tr}\text{.}
\end{align*}
Then, letting \(\vz = [1 \ 1 \ 1 \ 1]^{\tr}\) we have%
\begin{align*}
\proj_{W} \vz \amp = \frac{\vz \cdot \vw_1}{\vw_1 \cdot \vw_1} \vw_1 +  \frac{\vz \cdot \vw_2}{\vw_2 \cdot \vw_2} \vw_2 +  \frac{\vz \cdot \vw_3}{\vw_3 \cdot \vw_3} \vw_3\\
\amp = \frac{2}{2}[1  \ 0 \ 1 \ 0]^{\tr} + \frac{-1}{3/2}\frac{1}{2}[-1 \ 0 \ 1 \ -2]^{\tr} + \frac{-2/3}{4/3}  \frac{1}{3} [2 \ 0 \ -2 \ -2]^{\tr}\\
\amp = [1  \ 0 \ 1 \ 0]^{\tr} - \frac{1}{3}[-1 \ 0 \ 1 \ -2]^{\tr} - \frac{1}{6}[2 \ 0 \ -2 \ -2]^{\tr}\\
\amp = [1 \ 0 \ 1 \ 1]^{\tr}\text{.}
\end{align*}
The norm of \(\proj_{W^{\perp}} \vz = \vz - \proj_W \vz\) tells us how well our projection \([1 \ 0 \ 1 \ 1]^{\tr}\) approximates \(\vz = [1 \ 1 \ 1 \ 1]^{\tr}\). Now%
\begin{equation*}
||\proj_{W^{\perp}} \vz || = ||[1 \ 1 \ 1 \ 1]^{\tr} - [1 \ 0 \ 1 \ 1]^{\tr}|| = ||[0 \ 1 \ 0 \ 0]^{\tr}|| = 1\text{,}
\end{equation*}
so \([1 \ 0 \ 1 \ 1]^{\tr}\) is one unit away from \(\vz = [1 \ 1 \ 1 \ 1]^{\tr}\).%
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_gram_schmidt_summ_noips}
%
\begin{itemize}[label=\textbullet]
\item{}The projection of the vector \(\vv\) in \(V\) onto \(W\) is the vector%
\begin{equation*}
\proj_W \vv = \frac{\vv \cdot \vw_1}{\vw_1 \cdot \vw_1} \vw_1 + \frac{\vv \cdot \vw_2}{\vw_2 \cdot \vw_2}  \vw_2 + \cdots + \frac{\vv \cdot \vw_m}{\vw_m \cdot \vw_m}  \vw_m\text{,}
\end{equation*}
where \(W\) is the a subspace of \(\R^n\) with orthogonal basis \(\{\vw_1, \vw_2, \ldots, \vw_m\}\). These projections are important in that \(\proj_W \vv\) is the best approximation of the vector \(\vv\) by a vector in \(W\) in the least squares sense.%
\item{}With \(W\) as in (a), the projection of \(\vv\) orthogonal to \(W\) is the vector%
\begin{equation*}
\proj_{\perp W} \vv = \vv - \proj_W \vv\text{.}
\end{equation*}
The norm of \(\proj_{\perp W} \vv\) provides a measure of how well \(\proj_W \vv\) approximates the vector \(\vv\).%
\item{}The Gram-Schmidt process produces an orthogonal basis from any basis. It works as follows. Let \(\{\vv_1, \vv_2, \ldots, \vv_m\}\) be a basis for a subspace \(W\) of \(\R^n\). The set \(\{\vw_1\), \(\vw_2\), \(\vw_3\), \(\ldots\), \(\vw_{m}\}\) defined by%
\begin{itemize}[label=$\circ$]
\item{}\(\vw_1 = \vv_1\),%
\item{}\(\vw_2 = \vv_2 - \ds \frac{\vv_2 \cdot \vw_1}{\vw_1 \cdot \vw_1} \vw_1\),%
\item{}\(\vw_3 = \vv_3 - \ds \frac{ \vv_3 \cdot \vw_1 }{ \vw_1 \cdot \vw_1 } \vw_1 - \frac{ \vv_3 \cdot \vw_2 }{ \vw_2 \cdot \vw_2 } \vw_2\), \(\vdots\)%
\item{}\(\vw_m = \vv_m - \ds \frac{ \vv_m \cdot \vw_1 }{ \vw_1 \cdot \vw_1 } \vw_1 - \frac{ \vv_m \cdot \vw_2 }{ \vw_2 \cdot \vw_2 } \vw_2 - \cdots - \frac{ \vv_m \cdot \vw_{m-1} }{ \vw_{m-1} \cdot \vw_{m-1} } \vw_{m-1}\).%
\end{itemize}
is an orthogonal basis for \(W\). Moreover,%
\begin{equation*}
\Span\{\vw_1, \vw_2, \vw_3, \ldots, \vw_{k}\} = \Span\{\vv_1,\vv_2,\vv_3, \ldots, \vv_{k}\}
\end{equation*}
for each \(k\) between 1 and \(m\).%
\item{}The QR factorization has applications to solving least squares problems and approximating eigenvalues of matrices. The QR factorization writes an \(m \times n\) matrix with rank \(n\) as a product \(A = QR\), where the columns of \(Q\) form an orthonormal basis for \(\Col A\) and%
\begin{equation*}
R = [\vr_1 \ | \ \vr_2 \ | \ \vr_3 \ | \ \cdots \ | \ \vr_n ] = \left[ \begin{array}{cccccc} r_{11}\amp r_{12}\amp r_{13}\amp  \cdots \amp r_{1n-1}\amp r_{1n} \\ 0\amp r_{22}\amp r_{23}\amp  \cdots \amp r_{2n-1}\amp r_{2n} \\ 0\amp 0\amp r_{33}\amp  \cdots \amp r_{3n-1}\amp r_{3n} \\ \vdots\amp \vdots \amp \vdots \amp \vdots \amp \vdots \amp \vdots \\ 0\amp 0\amp 0\amp  \cdots \amp 0\amp r_{nn} \end{array}  \right]
\end{equation*}
is an upper triangular matrix.%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_gram_schmidt_exercises}
\begin{divisionexercise}{1}{}{}{g:exercise:idp19956104}%
Let \(W = \Span \{\vw_1, \vw_2\}\) in \(\R^3\), with \(\vw_1=[1 \ 2 \ 1]^{\tr}\) and \(\vw_2= [1 \ -1 \ 1]^{\tr}\), and \(\vv = [1 \ 0 \ 0]^{\tr}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find \(\proj_W \vv\) and \(\proj_{\perp W} \vv\)%
\item{}Find the vector in \(W\) that is closest to \(\vv\). How close is this vector to \(\vv\)?%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp19960456}%
Let \(\CB = \left\{\left[ \begin{array}{r} 1 \\ 0 \\ -1 \\ 0 \end{array} \right], \left[ \begin{array}{r} 0 \\ 1 \\ 0 \\ -1 \end{array} \right], \left[ \begin{array}{r} -1 \\ 1 \\ -1 \\ 1 \end{array} \right]\right\}\) and let \(W = \Span(\CB)\) in \(\R^4\). Find the best approximation in \(W\) to the vector \(\vv = \left[ \begin{array}{r} 2 \\ 3 \\ 1 \\ -1 \end{array} \right]\) in \(W\) and give a numeric estimate of how good this approximation is.%
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp19961992}%
In this exercise we determine the least-squares line (the line of best fit in the least squares sense) to a set of \(k\) data points \((x_1,y_1)\), \((x_2,y_2)\), \(\ldots\), \((x_k,y_k)\) in the plane. In this case, we want to fit a line of the form \(f(x) = mx+b\) to the data. If the data were to lie on a line, then we would have a solution to the system%
\begin{align*}
mx_1 + b \amp = y_1\\
mx_2 + b \amp = y_2\\
\vdots\\
mx_k + b \amp = y_k
\end{align*}
This system can be written as%
\begin{equation*}
m \vw_1 + b\vw_2 = \vy\text{,}
\end{equation*}
where \(\vw_1 = [x_1 \ x_2 \ \cdots \ x_k]^{\tr}\), \(\vw_2 = [1 \ 1 \ \cdots \ 1]^{\tr}\), and \(\vy = [y_1 \ y_2 \ \cdots \ y_k]^{\tr}\). If the data does not lie on a line, then the system won't have a solution. Instead, we want to minimize the square of the distance between \(\vy\) and a vector of the form \(m\vw_1 + b \vw_2\). That is, minimize%
\begin{equation}
||\vy - (m \vw_1 + b\vw_2)||^2\text{.}\label{x:men:eq_6_d_LS_line_1}
\end{equation}
Rephrasing this in terms of projections, we are looking for the vector in \(W = \Span\{\vw_1, \vw_2\}\) that is closest to \(\vy\). In other words, the values of \(m\) and \(b\) will occur as the weights when we write \(\proj_{W} \vy\) as a linear combination of \(\vw_1\) and \(\vw_2\). The one wrinkle in this problem is that we need an orthogonal basis for \(W\) to find this projection. Find the least squares line for the data points \((1,2)\), \((2,4)\) and \((3,5)\) in \(\R^2\).%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp19976072}%
Each set \(S\) is linearly independent. Use the Gram-Schmidt process to create an orthogonal set of vectors with the same span as \(S\). Then find an orthonormal basis for the same span.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(S = \left\{[1 \ 1 \ 1]^{\tr}, [5 \ -1 \ 2]^{\tr}\right\}\) in \(\R^3\)%
\item{}\(S = \left\{[1 \ 0 \ 2]^{\tr}, [-1 \ 1 \ 1]^{\tr}, [1 \ 1 \ 19]^{\tr} \right\}\) in \(\R^3\)%
\item{}\(S = \left\{[1 \ 0 \ 1 \ 0 \ 1 \ 0 \ 1]^{\tr}, [-1 \ 2 \ 3 \ 0 \ 1 \ 0 \ 1]^{\tr}, [1 \ 0 \ 4 \ -1 \ 2 \ 0 \ 1]^{\tr}, [1 \ 1 \ 1 \ 1 \ 1 \ 1 \ 1]^{\tr} \right\}\) in \(\R^7\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp19984008}%
Let \(S = \{\vv_1, \vv_2, \vv_3\}\) be a set of linearly dependent vectors in \(\R^n\) for some integer \(n \geq 3\). What is the result if the Gram-Schmidt process is applied to the set \(S\)? Explain.%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp19984904}%
A fellow student wants to find a QR factorization for a \(3 \times 4\) matrix. What would you tell this student and why?%
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp19983112}%
Find the QR factorizations of the given matrices.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\left[ \begin{array}{ccc} 1\amp 2\amp 3 \\ 0\amp 4\amp 5 \\ 0\amp 0\amp 6 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{cc} 1\amp 2 \\ 1\amp 3 \\ 1\amp 2 \\ 1\amp 3 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{ccc} 0\amp 1\amp 0\\0\amp 0\amp 0\\3\amp 0\amp 4 \\ 0\amp 0\amp 5 \end{array} \right]\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp19995400}%
Find an orthonormal basis \(\{\vu_1, \vu_2, \vu_3\}\) of \(\R^3\) such that \(\Span\{\vu_1,\vu_2\} = \Span\{[1 \ 2 \ 3]^{\tr}, [1 \ 1 \ 0]^{\tr}\}\).%
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp19996936}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(A = \left[ \begin{array}{cc} 2\amp 1\\2\amp 6\\2\amp 6\\2\amp 1 \end{array} \right] = [\va_1 \ \va_2]\). A QR factorization of \(A\) has \(Q = [\vu_1 \ \vu_2]\) with \(\vu_1 = \frac{1}{2}[1 \ 1 \ 1 \ 1]^{\tr}\) and \(\vu_2 = \frac{1}{2}[-1 \ 1 \ 1 \ -1]^{\tr}\), and \(\vu_3 = \frac{1}{\sqrt{12}}[1 \ -1 \ -1 \ 3]^{\tr}\) and \(R = [r_{ij}] = \left[ \begin{array}{cc} 4\amp 7 \\ 0\amp 5 \end{array} \right]\). Calculate the dot products \(\va_i \cdot \vu_j\) for \(i\) and \(j\) between \(1\) and \(2\). How are these dot products connected to the entries of \(R\)?%
\item{}Explain why the result of part (a) is true in general. That is, if \(A = [\va_1 \ \ \va_2 \ \ldots \ \va_n]\) has QR factorization with \(Q = [\vu_1 \ \vu_2 \ \ldots \ \vu_n]\) and \(R = [r_{ij}]\), then \(r_{ij} = \va_j \cdot \vu_i\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{10}{}{}{x:exercise:ex_6_e_upper_triangular_props}%
Upper triangular matrices play an important role in the QR decomposition. In this exercise we examine two properties of upper triangular matrices.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that if \(R\) and \(S\) are upper triangular \(n \times n\) matrices with positive diagonal entries, then \(RS\) is also an upper triangular \(n \times n\) matrices with positive diagonal entries.%
\item{}Show that if \(R\) is an invertible upper triangular matrix with positive diagonal entries, then \(R^{-1}\) is also an upper triangular matrix with positive diagonal entries.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{11}{}{}{g:exercise:idp20012680}%
In this exercise we demonstrate that the QR decomposition of an \(m \times n\) matrix with linearly independent columns is unique.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Suppose that \(A = QR\), where \(Q\) is an \(m \times n\) matrix with orthogonal columns and \(R\) is an \(n \times n\) upper triangular matrix. Show that \(R\) is invertible.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp20016136}{}\quad{}What can we say about \(\vx\) if \(R \vx = \vzero\)?%
\item{}Show that the only \(n \times n\) orthogonal upper triangular matrix with positive diagonal entries is the identity matrix \(I_n\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19759496}{}\quad{}Let \(A = [\va_1 \ \va_2 \ \ldots \ \va_n] = [a_{ij}]\) be an \(n \times n\) orthogonal upper triangular matrices with positive diagonal entries. What does that tell us about \(\va_i \cdot \va_j\)?%
\item{}Show that if \(Q_1\) and \(Q_2\) are \(m \times n\) matrices with orthogonal columns, and \(S\) is a matrix such that \(Q_1 = Q_2S\), then \(S\) is an orthogonal matrix.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19758600}{}\quad{}Write \(Q_1^{\tr}Q_1\) in terms of \(Q_2\) and \(S\).%
\item{}Suppose that \(Q_1\) and \(Q_2\) are \(m \times n\) matrices with orthogonal columns and \(R_1\) and \(R_2\) are \(n \times n\) upper triangular matrices such that%
\begin{equation*}
A = Q_1R_1 = Q_2R_2\text{.}
\end{equation*}
Show that \(Q_1 = Q_2\) and \(R_1 = R_2\). Conclude that the QR factorization of a matrix is unique.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp20336136}{}\quad{}Use the previous parts of this problem along with the results of \hyperlink{x:exercise:ex_6_e_upper_triangular_props}{Exercise~{\xreffont 10}}.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{12}{}{}{g:exercise:idp20336776}%
Label each of the following statements as True or False. Provide justification for your response. Throughout, let \(V\) be a vector space.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
If \(\{\vw_1, \vw_2\}\) is a basis for a subspace \(W\) of \(\R^3\), then the vector \(\frac{\vv \cdot \vw_1}{\vw_1 \cdot \vw_1} \vw_1 + \frac{\vv \cdot \vw_2}{\vw_2 \cdot \vw_2} \vw_2\) is the vector in \(W\) closest to \(\vv\).%
\item{}\lititle{True\slash{}False.}\par%
If \(W\) is a subspace of \(\R^n\), then the vector \(\vv - \proj_{W} \vv\) is orthogonal to every vector in \(W\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\vu_1\), \(\vu_2\), \(\vu_3\) are vectors in \(\R^n\), then the Gram-Schmidt process constructs an orthogonal set of vectors \(\{\vv_1, \vv_2, \vv_3\}\) with the same span as \(\{\vu_1, \vu_2, \vu_3\}\).%
\item{}\lititle{True\slash{}False.}\par%
Any set \(\{\vu_1, \vu_2, \ldots, \vu_k\}\) of orthogonal vectors in \(\R^n\) can be extended to an orthogonal basis of \(V\).%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is an \(n \times n\) matrix with \(AA^{\tr} = I_n\), then the rows of \(A\) form an orthogonal set.%
\item{}\lititle{True\slash{}False.}\par%
Every nontrivial subspace of \(\R^n\) has an orthogonal basis.%
\item{}\lititle{True\slash{}False.}\par%
If \(W\) is a subspace of \(\R^n\) satisfying \(W^\perp=\{\vzero\}\), then \(W=\R^n\).%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: MIMO Systems and Householder Transformations}
\typeout{************************************************}
%
\begin{sectionptx}{Project: MIMO Systems and Householder Transformations}{}{Project: MIMO Systems and Householder Transformations}{}{}{x:section:sec_project_mimo}
\begin{introduction}{}%
In a simplified model, a MIMO system will have \(k\) transmitting antennas and \(m\) receiving antennas. We record a transmitted symbol in a vector \(\vx = [x_1 \ x_2 \ \ldots \ x_k]^{\tr}\) (one \(x_i\) for each transmitting antenna) and the received symbol as a vector \(\vy = [y_1 \ y_2 \ \ldots \ y_m]^{\tr}\) (one \(y_j\) for each received symbol).%
\par
Between each transmit antenna and each receiver antenna is a fading channel. The MIMO system is the collection of fading channels. If we let \(h_{ij}\) be the fading between the \(j\)th transmitter and \(i\)th receiver, then we can represent this multipath fading as an \(m \times k\) matrix \(H = [h_{ij}]\). We assume that there is also some noise in the received signal that we represent by \(n_j\) (for the \(j\)th receiving antenna). Our MIMO system is then represented as the matrix-vector equation%
\begin{equation*}
\vy = H\vx + \vn\text{,}
\end{equation*}
where \(\vn = [n_1 \ n_2 \ \ldots \ n_m]^{\tr}\). The goal is to reproduce the original signal \(\vx\) from the received signal \(\vy\). This is where the QR decomposition comes into play.%
\begin{project}{}{x:project:act_QR_example_1}%
To see why and how the QR decomposition is used in MIMO systems, we begin with an example. Assume that we have two transmitters and three receivers, and suppose \(H = \left[ \begin{array}{cc} 2\amp 1\\2\amp 1\\1\amp 5 \end{array}  \right]\). From this point, to simplify the situation we assume that noise is negligible and consider the system \(\vy = H\vx\). Assume that the received signal is \(\vy = \left[ \begin{array}{c} 2\\1\\4 \end{array}  \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that the system \(H \vx = \vy\) is inconsistent.%
\item{}When we receive a signal, we need to interpret it, and that is difficult if we cannot find a solution. One reason that the system might not have a solution is that the elements in \(\vx\) have to come from a specified alphabet, and so we are looking for solutions that are in that alphabet, and there may be no direct solution in that alphabet. As a result, we generally have to find a ``best'' solution in the alphabet space. That can be done with the QR decomposition. Assume that \(H\) has a QR decomposition \(H = QR\) with \(Q\) having orthonormal columns and \(R\) a diagonal matrix. Explain how the equation \(\vy = H\vx\) can be transformed into%
\begin{equation}
Q^{\tr} \vy = R\vx\text{.}\label{x:men:eq_QR_system_example}
\end{equation}
%
\item{}Return to our example of \(H = \left[ \begin{array}{cc} 2\amp 1\\2\amp 1\\1\amp 5 \end{array}  \right]\). Assume that%
\begin{equation*}
H = QR =  \left[  \begin{array}{cr} \frac{2}{3}\amp  -\frac{\sqrt{2}}{6} \\ \frac{2}{3}\amp  -\frac{\sqrt{2}}{6} \\ \frac{1}{3}\amp  \frac{2\sqrt{2}}{3} \end{array}  \right] \left[ \begin{array}{cc} 3\amp 3\\0\amp 3\sqrt{2} \end{array}  \right]\text{.}
\end{equation*}
Use \hyperref[x:men:eq_QR_system_example]{({\xreffont\ref{x:men:eq_QR_system_example}})} to find \(\vx\) if \(\vy = \left[ \begin{array}{c} 2\\1\\4 \end{array}  \right]\).%
\end{enumerate}
\end{project}%
In \hyperref[x:project:act_QR_example_1]{Project Activity~{\xreffont\ref{x:project:act_QR_example_1}}} we saw that a QR decomposition allows us to replace the system \(\vy = H\vx\) with an upper triangular system \(Q^{\tr}\vy = R\vx\) that has a solution. This solution is what is called a least squares solution (which we will discuss in more detail in a later section). The key to all of this is finding an efficient way to calculate a QR decomposition. This is the focus of the remainder of this project.%
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Householder Transformations and the QR Decomposition}
\typeout{************************************************}
%
\begin{subsectionptx}{Householder Transformations and the QR Decomposition}{}{Householder Transformations and the QR Decomposition}{}{}{g:subsection:idp20377352}
There are three main ways to compute a QR decomposition.%
\begin{itemize}[label=\textbullet]
\item{}The method discussed in this section uses Gram-Schmidt orthogonalization. This method is not recommended for numerical use as it is inherently numerically unstable. Under certain circumstances, cancellation can destroy the accuracy of the computed vectors and the resulting vectors may not be orthogonal.%
\item{}The method that we will discuss in this project using Householder transformations (developed by Alston S. Householder). In Gaussian elimination, we can zero out entries below the diagonal by multiplying by elementary matrices to perform row operations. Householder transformations do something similar, allowing us to replace columns with columns of mostly zeros, which then allow for more efficient computations.%
\item{}Givens (or Jacobi) rotations (used by W. Givens and originally invented by Jacobi for use with in solving the symmetric eigenvalue problem) is another method that allows us to selectively produce zeros into a matrix. We will focus on Householder transformations in this project.%
\end{itemize}
%
\par
The first step in a QR decomposition using Householder matrices is to create a matrix that is upper triangular. To get to this form, we use a \terminology{Householder transformation}. A Householder transformation (or matrix) is a matrix of the form%
\begin{equation*}
H = I - 2 \frac{\vv \vv^{\tr}}{||\vv||^2}\text{,}
\end{equation*}
where \(\vv\) is a vector in \(\R^n\) and \(I\) is the \(n \times n\) identity matrix.%
\begin{project}{}{g:project:idp20388360}%
We will discover some properties of Householder transformations in this activity.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(\vx\) be any vector and let \(\vy\) be a unit vector. In this part of the exercise we show that, with a suitable choice of \(\vv\), the Householder transformation \(H\) transforms \(\vx\) into a vector of the same length parallel to \(\vy\). That is,%
\begin{equation*}
H \vx = -\sigma \vy
\end{equation*}
for some scalar \(\sigma\). A similar argument shows that \(H (\vx - \sigma \vy) = \sigma \vy\). Letting \(\vy = \ve_1\) gives us the following result. \begin{lemma}{}{}{x:lemma:lem_Householder}%
Let \(\vx\) be any vector, let \(\sigma = ||\vx||\), and let \(\vv = \vx \pm \sigma \ve_1\). Then \(H \vx = \mp \sigma \ve_1\).%
\end{lemma}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp20398088}{}\quad{}Let \(\sigma = ||\vx||\), and let \(\vv = \vx + \sigma \vy\). Apply \(H\) to \(\vx\), factor out \(\vx + \sigma \vy\), and ultimately show that \(H \vx = -\sigma \vy\).%
\item{}The Householder matrix \(H\) has two other important properties. Show that \(H\) is symmetric and orthogonal.%
\end{enumerate}
\end{project}%
There are many advantages to using Householder matrices. One is that instead of having to store all entries of the matrix \(H = I - 2 \frac{\vv \vv^{\tr}}{||\vv||^2}\), all we need to store is the entries of \(\vv\). Another advantage of Householder transformations is that they can be used to efficiently calculate QR decompositions. The next project activity shows how to use \hyperref[x:lemma:lem_Householder]{Lemma~{\xreffont\ref{x:lemma:lem_Householder}}} and Householder transformations to compute a QR decomposition through an example.%
\begin{project}{}{x:project:act_MIMO_Householder}%
Assume that we have five receiving antennas and three transmitting antennas, and that%
\begin{equation*}
A = \left[ \begin{array}{crc} 1.0\amp 0.0\amp 1.0 \\ 7.0\amp 7.0\amp 8.0 \\1.0\amp 2.0\amp 1.0 \\ 7.0\amp 7.0\amp 6.0 \\ 0.0\amp -1.0\amp 0.0 \end{array}  \right]\text{.}
\end{equation*}
%
\par
(We use \(A\) now instead of \(H\) so as to avoid confusion with Householder matrices.) The calculations in this activity can get rather messy, so use appropriate technology and round entries to four decimal places to the right of the decimal.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(\vx_1\) be the first column of \(A\). Identify an appropriate Householder transformation \(H_1\) so that \(H_1 \vx\) is a constant multiple of \([1 \ 0 \ 0 \ 0 \ 0]^{\tr}\). Determine the matrix \(A_1\), where \(A_1 = H_1 A\). (Special note: when deciding which of \(\vx \pm \sigma \ve_1\) to use to create \(H\), it is best to use the one whose sign is the same as \(x_1\) (the first entry of \(\vx\)). We won't go into the details, but this helps prevent problems due to cancellation,)%
\item{}Next we consider just the bottom right \(4 \times 2\) portion \(\hat{A}_2\) of the matrix \(A_1\) found in part (a).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Repeat the process on \(\hat{A}_2\) to find a Householder matrix \(\hat{H}_2\) that will make the first column of \(\hat{A}_2\) have all zeros below its first entry.%
\item{}Let%
\begin{equation*}
H_2 =  \left[ \begin{array}{cc} I_1\amp 0 \\ 0\amp \hat{H}_2 \end{array}  \right]\text{.}
\end{equation*}
Explain what the matrix \(A_2 = H_2H_1A\) is.%
\end{enumerate}
\item{}As a final step, we consider just the bottom right \(3 \times 1\) portion \(\hat{A}_3\) of the matrix \(A_2\) and repeat the process.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Find a matrix \(\hat{H}_3\) that produces zeros below the first entry.%
\item{}Let%
\begin{equation*}
H_3 = \left[ \begin{array}{cc}I_2\amp 0 \\ 0\amp \hat{H}_3 \end{array}  \right]\text{.}
\end{equation*}
What matrix is \(H_3H_2H_1A\)? Why?%
\end{enumerate}
\item{}Explain why \(Q = H_1H_2H_3\) is an orthogonal matrix. Then find an upper triangular matrix \(R\) such that \(A = QR\).%
\end{enumerate}
\end{project}%
\hyperref[x:project:act_MIMO_Householder]{Project Activity~{\xreffont\ref{x:project:act_MIMO_Householder}}} illustrates the general method for finding a QR decomposition of a matrix \(A\) by using Householder transformations to reduce \(A\) to an upper triangular matrix \(R\). The product of the Householder matrices is then the orthogonal matrix \(Q\). One final note: You may have noticed that the matrix \(Q\) you found in this project was a square \(5 \times 5\) matrix and \(R\) was a \(5 \times 3\) matrix, and so they have different sizes than the QR decomposition in this section. We could use the Gram-Schmidt process to obtain these larger matrices by extending the columns of a \(5 \times 3\) matrix \(Q\) to form an orthonormal basis for \(\R^5\). However, if we want a QR decomposition for \(A\) in which \(Q\) is \(5 \times 3\) and \(R\) is \(3 \times 3\) from the work we did in \hyperref[x:project:act_MIMO_Householder]{Project Activity~{\xreffont\ref{x:project:act_MIMO_Householder}}}, we can simply delete the last two columns of the matrix \(Q\) and the last two rows of the matrix \(R\) we calculated. This smaller QR decomposition is often referred to as a \terminology{thin} QR decomposition.%
\end{subsectionptx}
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 26 Least Squares Approximations}
\typeout{************************************************}
%
\begin{chapterptx}{Least Squares Approximations}{}{Least Squares Approximations}{}{}{x:chapter:chap_least_squares}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp20427656}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}How, in general, can we find a least squares approximation to a system \(A \vx = \vb\)?%
\item{}If the columns of \(A\) are linearly independent, how can we find a least squares approximation to \(A \vx = \vb\) using just matrix operations?%
\item{}Why are these approximations called ``least squares'' approximations?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Fitting Functions to Data}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Fitting Functions to Data}{}{Application: Fitting Functions to Data}{}{}{x:section:sec_appl_fit_func}
Data is all around us. Data is collected on almost everything and it is important to be able to use data to make predictions. However, data is rarely well-behaved and so we generally need to use approximation techniques to estimate from the data. One technique for this is least squares approximations. As we will see, we can use linear algebra to fit a variety of different types of curves to data.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_ls_intro}
In this section our focus is on fitting linear and polynomial functions to data sets.%
\begin{exploration}{}{x:exploration:pa_LS_1}%
NBC was awarded the U.S. television broadcast rights to the 2016 and 2020 summer Olympic games. \hyperref[x:table:T_LS_Olympics]{Table~{\xreffont\ref{x:table:T_LS_Olympics}}} lists the amounts paid (in millions of dollars) by NBC sports for the 2008 through 2012 summer Olympics plus the recently concluded bidding for the 2016 and 2020 Olympics, where year 0 is the year 2008. (We will assume a simple model here, ignoring variations such as value of money due to inflation, viewership data which might affect NBC's expected revenue, etc.) \hyperref[x:figure:F_LS_Olympics]{Figure~{\xreffont\ref{x:figure:F_LS_Olympics}}} shows a plot of the data. Our goal in this activity is to find a linear function \(f\) defined by \(f(x) = a_0 + a_1x\) that fits the data well.%
\begin{sidebyside}{2}{0}{0}{0}%
\begin{sbspanel}{0.5}%
\begin{tableptx}{\textbf{Olympics television broadcast rights.}}{x:table:T_LS_Olympics}{}%
\resizebox{\ifdim\width > \linewidth\linewidth\else\width\fi}{!}{%
{\centering%
{\tabularfont%
\begin{tabular}{Acc}\hrulethin
\multicolumn{1}{AcA}{Year}&\multicolumn{1}{cA}{Amount}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{0}&\multicolumn{1}{cA}{894}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{4}&\multicolumn{1}{cA}{1180}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{8}&\multicolumn{1}{cA}{1226}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{12}&\multicolumn{1}{cA}{1418}\tabularnewline\hrulethin
\end{tabular}
}%
\par}
}%
\end{tableptx}%
\end{sbspanel}%
\begin{sbspanel}{0.5}%
\begin{figureptx}{A plot of the data.}{x:figure:F_LS_Olympics}{}%
\includegraphics[width=\linewidth]{external/7_d_pa_3.pdf}
\tcblower
\end{figureptx}%
\end{sbspanel}%
\end{sidebyside}%
\par
If the data were actually linear, then the data would satisfy the system%
\begin{alignat*}{3}
{}a_0 \amp {}+{}  \amp {0}a_1 \amp = \amp 894\amp {}\\
{}a_0 \amp {}+{}  \amp {4}a_1 \amp = \amp 1180\amp {}\\
{}a_0 \amp {}+{}  \amp {8}a_1 \amp = \amp 1226\amp {}\\
{}a_0 \amp {}+{}  \amp {12}a_1 \amp = \amp 1418\amp {.}
\end{alignat*}
%
\par
The vector form of this equation is%
\begin{equation*}
a_0 [1 \ 1 \ 1 \ 1]^{\tr} + a_1[0 \ 4 \ 8 \ 12]^{\tr} = [894 \ 1180 \ 1226 \ 1418]^{\tr}\text{.}
\end{equation*}
%
\par
This equation does not have a solution, so we seek the best approximation to a solution we can find. That is, we want to find \(a_0\) and \(a_1\) so that the line \(f(x) = a_0+a_1x\) provides a best fit to the data.%
\par
Letting \(\vv_1 = [1 \ 1 \ 1 \ 1]^{\tr}\) and \(\vv_2 = [0 \ 4 \ 8 \ 12]^{\tr}\), and \(\vb = [894 \ 1180 \ 1226 \ 1418]^{\tr}\), our vector equation becomes%
\begin{equation*}
a_0 \vv_1 + a_1\vv_2 = \vb\text{.}
\end{equation*}
%
\par
To make a best fit, we will minimize the square of the distance between \(\vb\) and a vector of the form \(a_0\vv_1 + a_1 \vv_2\). That is, minimize%
\begin{equation}
||\vb - (a_0 \vv_1 + a_1\vv_2)||^2\text{.}\label{x:men:eq_LS_line_1}
\end{equation}
%
\par
Rephrasing this in terms of projections, we are looking for the vector in \(W = \Span\{\vv_1, \vv_2\}\) that is closest to \(\vb\). In other words, the values of \(a_0\) and \(a_1\) will occur as the weights when we write \(\proj_{W} \vb\) as a linear combination of \(\vv_1\) and \(\vv_2\). The one wrinkle in this problem is that we need an orthogonal basis for \(W\) to find this projection. Use appropriate technology throughout this activity.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find an orthogonal basis \(\CB = \{\vw_1, \vw_2\}\) for \(W\).%
\item{}Use the basis \(\CB\) to find \(\vy = \proj_{W} \vb\) as illustrated in \hyperref[x:figure:F_6_f_proj]{Figure~{\xreffont\ref{x:figure:F_6_f_proj}}}.%
\begin{figureptx}{Projecting \(\vb\) onto \(W\).}{x:figure:F_6_f_proj}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/6_f_O_proj.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\item{}Find the values of \(a_0\) and \(a_1\) that give our best fit line by writing \(\vy\) as a linear combination of \(\vv_1\) and \(\vv_2\).%
\item{}Draw a picture of your line from the previous part on the axes with the data set. How well do you think your line approximates the data? Explain.%
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Least Squares Approximations}
\typeout{************************************************}
%
\begin{sectionptx}{Least Squares Approximations}{}{Least Squares Approximations}{}{}{x:section:sec_ls_approx}
In \hyperref[x:chapter:chap_gram_schmidt]{Section~{\xreffont\ref{x:chapter:chap_gram_schmidt}}} we saw that the projection of a vector \(\vv\) in \(\R^n\) onto a subspace \(W\) of \(\R^n\) is the best approximation to \(\vv\) of all the vectors in \(W\). In fact, if \(\vv = [v_1 \ v_2 \ \ldots \ v_n]^{\tr}\) and \(\proj_W \vv =  [w_1 \ w_2 \ w_3 \ \ldots \ w_m]^{\tr}\), then the error in approximating \(\vv\) by \(\vw\) is given by%
\begin{equation*}
|| \vv - \proj_W \vv ||^2 = \sum_{i=1}^m (v_i - w_i)^2\text{.}
\end{equation*}
In the context of \hyperref[x:exploration:pa_LS_1]{Preview Activity~{\xreffont\ref{x:exploration:pa_LS_1}}}, we projected the vector \(\vb\) onto the span of  the vectors \(\vv_1 = [1 \ 1 \ 1 \ 1]^{\tr}\) and \(\vv_2 = [0 \ 4 \ 8 \ 12]^{\tr}\). The projection   minimizes the distance between the vectors in \(W\) and the vector \(\vb\) (as shown in \hyperref[x:figure:F_6_f_proj]{Figure~{\xreffont\ref{x:figure:F_6_f_proj}}}), and also produces a line which minimizes the sums of the squares of the vertical  distances from the line to the data set as illustrated in \hyperref[x:figure:F_LS_Olympics2]{Figure~{\xreffont\ref{x:figure:F_LS_Olympics2}}} with the olympics data. This is why these approximations are called least squares approximations.%
\begin{figureptx}{Least squares linear approximation}{x:figure:F_LS_Olympics2}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/Regression1.pdf}
\end{image}%
\tcblower
\end{figureptx}%
While we can always solve least squares problems using projections, we can often avoid having to create an orthogonal basis when fitting functions to data. We work in a more general setting, showing how to fit a polynomial of degree \(n\) to a set of data points. Our goal is to fit a polynomial \(p(x) = a_0+a_1x+a_2x^2+ \cdots + 
a_nx^n\) of degree \(n\) to \(m\) data points \((x_1,y_1)\), \((x_2,y_2)\), \(\ldots\), \((x_m,y_m)\), no two of which have the same \(x\) coordinate. In the unlikely event that the polynomial \(p(x)\) actually passes through the \(m\) points, then we would have the \(m\) equations%
\begin{alignat}{7}
y_1 \amp{}={} \amp{a_0} \amp{}+{} \amp{a_1}x_1 \amp{}+{} \amp{a_2}x_1^2 \amp{}+{} \amp\cdots \amp{}+{} \amp{a_{n-1}}x_1^{n-1} \amp{}+{} \amp{a_n}x_1^n\label{g:mrow:idp20477192}\\
y_2 \amp{}={} \amp{a_0} \amp{}+{} \amp{a_1}x_2 \amp{}+{} \amp{a_2}x_2^2 \amp{}+{} \amp\cdots \amp{}+{} \amp{a_{n-1}}x_2^{n-1} \amp{}+{} \amp{a_n}x_2^n\label{g:mrow:idp20479496}\\
y_3 \amp{}={} \amp{a_0} \amp{}+{} \amp{a_1}x_3 \amp{}+{} \amp{a_2}x_3^2 \amp{}+{} \amp\cdots \amp{}+{} \amp{a_{n-1}}x_3^{n-1} \amp{}+{} \amp{a_n}x_3^n\label{g:mrow:idp20477448}\\
{}   \amp{}     \amp{}       \amp{}      \amp{}          \amp{}      \amp{}             \amp{}       \amp\vdots \amp{}     \amp{}                            \amp{}       \amp{}\label{g:mrow:idp20480648}\\
y_m \amp{}={} \amp{a_0} \amp{}+{} \amp{a_1}x_m \amp{}+{} \amp{a_2}x_m^2 \amp{}+{} \amp\cdots \amp{}+{} \amp{a_{n-1}}x_m^{n-1} \amp{}+{} \amp{a_n}x_m^n\label{g:mrow:idp20474632}
\end{alignat}
in the \(n+1\) unknowns \(a_0\), \(a_1\), \(\ldots\), \(a_{n-1}\), and \(a_n\).%
\par
The \(m\) data points are known in this situation and the coefficients \(a_0\), \(a_1\), \(\ldots\), \(a_n\) are the unknowns. To write the system in matrix-vector form, the coefficient matrix \(M\) is%
\begin{equation*}
M = \left[ \begin{array}{cccccc} 1 \amp x_1 \amp x_1^2\amp \cdots  \amp x_1^{n-1} \amp x_1^{n} \\ 1 \amp x_2 \amp x_2^2\amp \cdots  \amp x_2^{n-1} \amp x_2^{n} \\ 1 \amp x_3 \amp x_3^2\amp \cdots  \amp x_3^{n-1} \amp x_3^{n} \\ \vdots \amp \vdots \amp \vdots \amp \cdots \amp\vdots \amp\vdots \\ 1 \amp x_m \amp x_m^2\amp \cdots  \amp x_m^{n-1} \amp x_m^{n} \end{array} \right]\text{,}
\end{equation*}
while the vectors \(\va\) and \(\vy\) are%
\begin{equation*}
\va = \left[ \begin{array}{c} a_{0} \\ a_{1} \\ a_{2} \\ \vdots \\ a_{n-1} \\a_{n} \end{array} \right] \text{ and } \vy = \left[ \begin{array}{c} y_{1} \\ y_{2} \\ y_{3} \\ \vdots \\ y_{m-1} \\ y_{m} \end{array} \right]\text{.}
\end{equation*}
Letting \(\vy = [y_1 \ y_2 \ \ldots \ y_m]^{\tr}\) and \(\vv_i = [x_1^{i-1} \ x_2^{i-1} \ \ldots \ x_m^{i-1}]^{\tr}\) for \(1 \leq i \leq n+1\), the vector form of the system is%
\begin{equation*}
\vy = a_0\vv_1 + a_1 \vv_2 +  \cdots + a_n \vv_{n+1}\text{.}
\end{equation*}
%
\par
Of course, it is unlikely that the \(m\) data points already lie on a polynomial of degree \(n\), so the system will usually have no solution. So instead of attempting to find coefficients \(a_0\), \(a_1\), \(\ldots\), \(a_n\) that give a solution to this system, which may be impossible, we instead look for a vector that is \textasciigrave{}\textasciigrave{}close" to a solution. As we have seen, the vector \(\proj_{W} \vy\), where \(W\) is the span of the columns of \(M\), minimizes the sum of the squares of the differences of the components. That is, our desired approximation to a solution to \(M \vx = \vy\) is the projection of \(\vy\) onto \(\Col M\). Now \(\proj_W \vy\) is a linear combination of the columns of \(M\), so \(\proj_W \vy = M \va^*\) for some vector \(\va^*\). This vector \(\va^*\) then minimizes \(|| \proj_{\perp W} \vy|| = ||\vy - M \va ||\). That is, if we let \((M\va)^{\tr} = [b_1 \ b_2 \ b_3 \ \cdots \ b_m]\), we are minimizing%
\begin{equation}
||\vy - M\va||^2 = (y_1-b_1)^2 + (t_2-b_2)^2 + \cdots + (y_m-b_m)^2\text{.}\label{x:men:eq_ls_approx}
\end{equation}
The expression \(||\vy - M\va||^2\) measures the error in our approximation.%
\par
The question we want to answer is how we can find the vector \(\va^*\) that minimizes \(||\vy -M\va||\) in a way that is more convenient than computing a projection. We answer this question in a general setting in the next activity.%
\begin{activity}{}{x:activity:act_LS_matrices}%
Let \(A\) be an \(m \times n\) matrix and let \(\vb\) be in \(\R^m\). Let \(W = \Col A\). Then \(\proj_W \vb\) is in \(\Col A\), so let \(\vx^*\) be in \(\R^{n}\) such that \(A\vx^* = \proj_W \vb\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why \(\vb - A\vx^*\) is orthogonal to every vector of the form \(A\vx\), for any \(\vx\) in \(\R^{n}\). That is, \(\vb - A\vx^*\) is orthogonal to \(\Col A\).%
\item{}Let \(\va_i\) be the \(i\)th column of \(A\). Explain why \(\va_i \cdot \left(\vb - A\vx^*\right) = 0\). From this, explain why \(A^{\tr}\left(\vb - A\vx^*\right) = 0\).%
\item{}From the previous part, show that \(\vx^*\) satisfies the equation%
\begin{equation*}
A^{\tr}A\vx^* = A^{\tr}\vb\text{.}
\end{equation*}
%
\end{enumerate}
\end{activity}%
The result of \hyperref[x:activity:act_LS_matrices]{Activity~{\xreffont\ref{x:activity:act_LS_matrices}}} is that we can now do least squares polynomial approximations with just matrix operations. We summarize this in the following theorem.%
\begin{theorem}{}{}{x:theorem:thm_6_f_least_squares_1}%
The least squares solutions to the system \(A \vx = \vb\) are the solutions to the corresponding system%
\begin{equation}
A^{\tr}A\vx = A^{\tr}\vb\text{.}\label{x:men:eq_6_f_normal_equations}
\end{equation}
%
\end{theorem}
The equations in the system \hyperref[x:men:eq_6_f_normal_equations]{({\xreffont\ref{x:men:eq_6_f_normal_equations}})} are called the \terminology{normal equations} for \(A \vx = \vb\). To illustrate, with the Olympics data, our data points are \((0, 894)\), \((4, 1180)\), \((8, 1226)\), \((12,1418)\) with \(\vy = [894 \ 1180 \ 1226 \ 1418]^{\tr}\). So \(M = \left[ \begin{array}{cc} 1\amp0 \\ 1\amp4 \\ 1\amp8 \\ 1\amp12 \end{array} \right]\). Notice that \(M^{\tr}M\) is invertible, to find the degree 1 approximation to the data, technology shows that%
\begin{equation*}
\va^* = \left(M^{\tr}M\right)^{-1}M^{\tr}\vy = \left[ \frac{4684}{5} \ \frac{809}{20}\right]
\end{equation*}
just as in \hyperref[x:exploration:pa_LS_1]{Preview Activity~{\xreffont\ref{x:exploration:pa_LS_1}}}.%
\begin{activity}{}{g:activity:idp20511624}%
Now use the least squares method to find the best polynomial approximations (in the least squares sense) of degrees 2 and 3 for the Olympics data set in \hyperref[x:table:T_LS_Olympics]{Table~{\xreffont\ref{x:table:T_LS_Olympics}}}. Which polynomial seems to give the ``best'' fit? Explain why. Include a discussion of the errors in your approximations. Use your ``best'' least squares approximation to estimate how much NBC might pay for the television rights to the 2024 Olympic games. Use technology as appropriate.%
\end{activity}%
The solution with our Olympics data gave us the situation where \(M^{\tr}M\) was invertible. This corresponded to a unique least squares solution \(\left(M^{\tr}M\right)^{-1}M^{\tr}\vy\). It is reasonable to ask when this happens in general. To conclude this section, we will demonstrate that if the columns of a matrix \(A\) are linearly independent, then \(A^{\tr}A\) is invertible.%
\begin{activity}{}{x:activity:act_LS_invertible}%
Let \(A\) be an \(m \times n\) matrix with linearly independent columns.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}What must be the relationship between \(m\) and \(n\)? Explain.%
\item{}We know that an \(n \times n\) matrix \(M\) is invertible if and only if the only solution to the homogeneous system \(M \vx = \vzero\) is \(\vx = \vzero\). Note that \(A^{\tr}A\) is an \(n \times n\) matrix. Suppose that \(A^{\tr}A \vx = \vzero\) for some \(\vx\) in \(\R^n\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Show that \(||A\vx|| = 0\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp20526344}{}\quad{}What is \(\vx^{\tr} (A^{\tr}A \vx)\)?%
\item{}What does \(||A\vx|| = 0\) tell us about \(\vx\) in relation to \(\Nul A\)? Why?%
\item{}What is \(\Nul A\)? Why? What does this tell us about \(\vx\) and then about \(A^{\tr}A\)?%
\end{enumerate}
\end{enumerate}
\end{activity}%
We summarize the result of \hyperref[x:activity:act_LS_invertible]{Activity~{\xreffont\ref{x:activity:act_LS_invertible}}} in the following theorem.%
\begin{theorem}{}{}{x:theorem:thm_6_f_least_squares_2}%
If the columns of \(A\) are linearly independent, then the least squares solution \(\vx^*\) to the system \(A \vx = \vb\) is%
\begin{equation*}
\vx^* = \left(A^{\tr}A\right)^{-1}A^{\tr}\vb\text{.}
\end{equation*}
%
\end{theorem}
If the columns of \(A\) are linearly dependent, we can still solve the normal equations, but will obtain more than one solution. In a later section we will see that we can also use a pseudoinverse in these situations.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_ls_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp20532104}%
According to the \href{https://www.cdc.gov/growthcharts/html_charts/lenageinf.htm}{Centers for Disease Control and Prevention}\footnotemark{}, the average length of a male infant (in centimeters) in the US as it ages (with time in months from 1.5 to 8.5) is given in \hyperref[x:table:T_lengths]{Table~{\xreffont\ref{x:table:T_lengths}}}.%
\begin{tableptx}{\textbf{Average lengths of male infants}}{x:table:T_lengths}{}%
\centering%
{\tabularfont%
\begin{tabular}{Accccccccc}\hrulethin
\multicolumn{1}{AcB}{Age (months)}&\multicolumn{1}{cA}{1.5}&\multicolumn{1}{cA}{2.5}&\multicolumn{1}{cA}{3.5}&\multicolumn{1}{cA}{4.5}&\multicolumn{1}{cA}{5.5}&\multicolumn{1}{cA}{6.5}&\multicolumn{1}{cA}{7.5}&\multicolumn{1}{cA}{8.5}\tabularnewline\hrulethin
\multicolumn{1}{AcB}{Average Length (cm)}&\multicolumn{1}{cA}{56.6}&\multicolumn{1}{cA}{59.6}&\multicolumn{1}{cA}{62.1}&\multicolumn{1}{cA}{64.2}&\multicolumn{1}{cA}{66.1}&\multicolumn{1}{cA}{67.9}&\multicolumn{1}{cA}{69.5}&\multicolumn{1}{cA}{70.9}\tabularnewline\hrulethin
\end{tabular}
}%
\end{tableptx}%
In this problem we will find the line and the quadratic of best fit in the least squares sense to this data. We treat age in months as the independent variable and length in centimeters as the dependent variable.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find a line that is the best fit to the data in the least squares sense. Draw a picture of your least squares solution against a scatterplot of the data.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp20545032}{}\quad{}We assume that a line of the form \(f(x) = a_1x+a_0\) contains all of the data points. The first data point would satisfy \(1.5a_1+a_0 = 56.6\), the second \(2.5a_1+a_0 = 59.6\), and so on, giving us the linear system%
\begin{alignat*}{2}
{3} {1.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 56.6{}\\
{2.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 59.6{}\\
{3.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 62.1{}\\
{4.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 64.2{}\\
{5.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 66.1{}\\
{6.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 67.9{}\\
{7.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 69.5{}\\
{8.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 70.9{.}
\end{alignat*}
Letting%
\begin{equation*}
A = \left[ \begin{array}{cc} 1.5\amp 1 \\ 2.5\amp 1 \\ 3.5\amp 1 \\ 4.5\amp 1 \\ 5.5\amp 1 \\ 6.5\amp 1 \\ 7.5\amp 1 \\ 8.5\amp 1 \end{array}  \right], \ \vx = \left[ \begin{array}{c} a_1\\a_0 \end{array}  \right], \ \text{ and }  \ \vb=\left[ \begin{array}{c} 56.6\\59.6\\62.1\\64.2\\66.1\\67.9\\69.5\\70.9 \end{array}  \right]\text{,}
\end{equation*}
we can write this system in the matrix form  \(A \vx = \vb\). Neither column of \(A\) is a multiple of the other, so the columns of \(A\) are linearly independent. The least squares solution \(\vx^*\) to the system is then found by%
\begin{equation*}
\vx^* = \left(A^{\tr}A\right)^{-1}A^{\tr} \vb\text{.}
\end{equation*}
Technology shows that (with entries rounded to 3 decimal places), \(\left(A^{\tr}A\right)^{-1}A^{\tr}\) is%
\begin{equation*}
\left[ \begin{array}{rrrrcrrr} - 0.083\amp - 0.060\amp - 0.036\amp - 0.012\amp  0.012\amp  0.036\amp  0.060\amp  0.083\\ 0.542\amp  0.423\amp  0.304\amp  0.185\amp  0.065\amp - 0.054\amp - 0.173\amp - 0.292 \end{array}  \right]\text{,}
\end{equation*}
and%
\begin{equation*}
\vx^* \approx \left[ \begin{array}{cc} 2.011 \\ 54.559 \end{array}  \right]\text{.}
\end{equation*}
So the least squares linear function to the data is \(f(x) \approx 2.011x + 54.559\). A graph of \(f\) against the data points is shown at left in \hyperref[x:figure:F_LS_linear]{Figure~{\xreffont\ref{x:figure:F_LS_linear}}}. \begin{figureptx}{Left: Least squares line. Right: Least squares quadratic.}{x:figure:F_LS_linear}{}%
\begin{image}{0.25}{0.5}{0.25}%
\includegraphics[width=\linewidth]{external/7_d_least_squares_line.pdf}
\end{image}%
\tcblower
\end{figureptx}%
%
\item{}Now find the least squares quadratic of the form \(q(x) = a_2x^2+a_1x+a_0\) to the data. Draw a picture of your least squares solution against a scatterplot of the data.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp20558472}{}\quad{}The first data point would satisfy \((1.5^2)a_2+1.5a_1+a_0 = 56.6\), the second \((2.5)^2a_2+2.5a_1+a_0 = 59.6\), and so on, giving us the linear system%
\begin{alignat*}{3}
{1.5^2}a_2  \amp {}+{}  \amp {1.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 56.6{}\\
{2.5^2}a_2  \amp {}+{}  \amp {2.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 59.6{}\\
{3.5^2}a_2  \amp {}+{}  \amp {3.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 62.1{}\\
{4.5^2}a_2  \amp {}+{}  \amp {4.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 64.2{}\\
{5.5^2}a_2  \amp {}+{}  \amp {5.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 66.1{}\\
{6.5^2}a_2  \amp {}+{}  \amp {6.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 67.9{}\\
{7.5^2}a_2  \amp {}+{}  \amp {7.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 69.5{}\\
{8.5^2}a_2  \amp {}+{}  \amp {8.5}a_1  \amp {}+{}  \amp a_0 \amp {}={} 70.9{.}
\end{alignat*}
Letting%
\begin{equation*}
A = \left[ \begin{array}{ccc} 1.5^2\amp 1.5\amp 1 \\ 2.5^2\amp 2.5\amp 1 \\ 3.5^2\amp 3.5\amp 1 \\ 4.5^2\amp 4.5\amp 1 \\ 5.5^2\amp 5.5\amp 1 \\ 6.5^2\amp 6.5\amp 1 \\ 7.5^2\amp 7.5\amp 1 \\ 8.5^2\amp 8.5\amp 1 \end{array}  \right], \ \vx = \left[ \begin{array}{c} a_2\\a_1\\a_0 \end{array}  \right], \ \text{ and }  \ \vb=\left[ \begin{array}{c} 56.6\\59.6\\62.1\\64.2\\66.1\\67.9\\69.5\\70.9 \end{array}  \right]\text{,}
\end{equation*}
we can write this system in the matrix form  \(A \vx = \vb\). Technology shows that every column of the reduced row echelon form of \(A\) contains a pivot, so the columns of \(A\) are linearly independent. The least squares solution \(\vx^*\) to the system is then found by%
\begin{equation*}
\vx^* = \left(A^{\tr}A\right)^{-1}A^{\tr} \vb\text{.}
\end{equation*}
Technology shows that (with entries rounded to 3 decimal places) \(\left(A^{\tr}A\right)^{-1}\) is%
\begin{equation*}
\left[   \begin{array}{rrrrrrrr}  0.042\amp  0.006\amp - 0.018\amp - 0.030\amp - 0.030\amp - 0.018\amp  0.006\amp  0.042\\ - 0.500\amp - 0.119\amp  0.143\amp  0.286\amp  0.310\amp  0.214\amp  0.000\amp -0.333\\  1.365\amp  0.540\amp - 0.049\amp - 0.403\amp -0.522\amp - 0.406\amp - 0.055\amp  0.531 \end{array}  \right]\text{,}
\end{equation*}
and%
\begin{equation*}
\vx^* \approx \left[ \begin{array}{r} -0.118 \\ 3.195 \\ 52.219 \end{array}  \right]\text{.}
\end{equation*}
So the least squares quadratic function to the data is \(q\) defined by \(q(x) \approx -0.118x^2+3.195x + 52.219\). A graph of \(q\) against the data points is shown at right in \hyperref[x:figure:F_LS_linear]{Figure~{\xreffont\ref{x:figure:F_LS_linear}}}.%
\end{enumerate}
\end{example}
\footnotetext[45]{\nolinkurl{cdc.gov/growthcharts/html_charts/lenageinf.htm}\label{g:fn:idp20532488}}%
\begin{example}{}{g:example:idp20569352}%
Least squares solutions can be found through a QR factorization, as we explore in this example. Let \(A\) be an \(m \times n\) matrix with linearly independent columns and QR factorization \(A = QR\). Suppose that \(\vb\) is not in \(\Col A\) so that the system \(A \vx = \vb\) is inconsistent. We know that the least squares solution \(\vx^*\) to \(A \vx = \vb\) is%
\begin{equation}
\vx^* = \left(A^{\tr}A\right)^{-1}A^{\tr}\vb\text{.}\label{x:men:eq_ls_example}
\end{equation}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Replace \(A\) by its QR factorization in \hyperref[x:men:eq_ls_example]{({\xreffont\ref{x:men:eq_ls_example}})} to show that%
\begin{equation*}
\vx^* = R^{-1}Q^{\tr} \vb\text{.}
\end{equation*}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp20572296}{}\quad{}Use the fact that \(Q\) is orthogonal and \(R\) is invertible.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp20578696}{}\quad{}Replacing \(A\) with \(QR\) and using the fact that \(R\) is invertible and \(Q\) is orthogonal to see that%
\begin{align*}
\vx^* \amp = \left(A^{\tr}A\right)^{-1}A^{\tr}\vb\\
\amp = \left((QR)^{\tr}QR\right)^{-1} (QR)^{\tr} \vb\\
\amp = \left(R^{\tr}Q^{\tr}QR\right) R^{\tr}Q^{\tr} \vb\\
\amp = \left(R^{\tr}R\right)^{-1}R^{\tr}Q^{\tr} \vb\\
\amp = R^{-1}\left(R^{\tr}\right)^{-1}R^{\tr} Q^{\tr} \vb\\
\amp = R^{-1}Q^{\tr} \vb\text{.}
\end{align*}
So if \(A = QR\) is a QR factorization of \(A\), then the least squares solution to \(A \vx = \vb\) is \(R^{-1}Q^{\tr} \vb\).%
\item{}Consider the data set in \hyperref[x:table:T_life_exectancy]{Table~{\xreffont\ref{x:table:T_life_exectancy}}}, which shows the average life expectance in years in the US for selected years from 1950 to 2010.%
\begin{tableptx}{\textbf{Life expectancy in the US}}{x:table:T_life_exectancy}{}%
\centering%
{\tabularfont%
\begin{tabular}{llllll}
\multicolumn{1}{lA}{year}&1950&1965&1980&1995&2010\tabularnewline[0pt]
\multicolumn{1}{lA}{age}&68.14&70.21&73.70&75.98&78.49
\end{tabular}
}%
\end{tableptx}%
 (Data from \href{https://www.macrotrends.net/countries/USA/united-states/life-expectancy}{macrotrends}\footnotemark{}.)\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Use \hyperref[x:men:eq_ls_example]{({\xreffont\ref{x:men:eq_ls_example}})} to find the least squares linear fit to the data set.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp20582664}{}\quad{}A linear fit to the data will be provided by the least squares solution to \(A \vx = \vb\), where%
\begin{equation*}
A = \left[ \begin{array}{cc} 1\amp 1950 \\ 1\amp 1965 \\ 1 \amp 1980 \\  1\amp 1995 \\ 1 \amp 2010 \end{array}  \right], \ \vx = \left[ \begin{array}{c} a\\b \end{array}  \right], \ \text{ and }  \ \vb =  \left[ \begin{array}{c} 68.14\\70.21\\73.70\\75.98  \\78.49 \end{array}  \right]\text{.}
\end{equation*}
Technology shows that%
\begin{equation*}
\left(A^{\tr}A\right)^{-1}A^{\tr} \vb \approx [-276.1000 \ 0.1765]^{\tr}\text{.}
\end{equation*}
%
\item{}Use appropriate technology to find the QR factorization of an appropriate matrix \(A\), and use the QR decomposition to find the least squares linear fit to the data. Compare to what you found in part i.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp20588936}{}\quad{}Technology shows that \(A = QR\), where%
\begin{equation*}
Q = \left[  \begin{array}{cr} \frac{1}{\sqrt{5}}\amp -\frac{2}{\sqrt{10}} \\ \frac{1}{\sqrt{5}}\amp -\frac{1}{\sqrt{10}} \\ \frac{1}{\sqrt{5}}\amp 0 \\ \frac{1}{\sqrt{5}}\amp \frac{1}{\sqrt{10}} \\ \frac{1}{\sqrt{5}}\amp \frac{2}{\sqrt{10}} \end{array}  \right] \text{ and }  R = \left[ \begin{array}{cc} \sqrt{5}\amp 1980 \sqrt{5} \\ 0\amp 15 \sqrt{10} \end{array}  \right]\text{.}
\end{equation*}
Then we have that%
\begin{equation*}
R^{-1} Q^{\tr} \vb \approx [-276.1000 \ 0.1765]^{\tr}\text{,}
\end{equation*}
just as in part i.%
\end{enumerate}
\end{enumerate}
\end{example}
\footnotetext[46]{\nolinkurl{macrotrends.net/countries/USA/united-states/life-expectancy}\label{g:fn:idp20585352}}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_ls_summ}
%
\begin{itemize}[label=\textbullet]
\item{}A least squares approximation to \(A \vx = \vb\) is found by orthogonally projecting \(\vb\) onto \(\Col A\).%
\item{}If the columns of \(A\) are linearly independent, then the least squares approximation to \(A \vx = \vb\) is \(\left(A^{\tr}A\right)^{-1}A^{\tr} \vb\).%
\item{}The least squares solution to \(A \vx = \vb\), where \(A\vx = [y_1 \ y_2 \ \ldots \ y_m]\) and \(\vb = [b_1 \ b_2 \ \ldots \ b_ m]^{\tr}\), minimizes the distance \(||A\vx - \vb||\), where%
\begin{equation*}
||A\vx - \vb||^2 = (y_1-b_1)^2 + (y_2-b_2)^2 + \cdots + (y_m-b_m)^2\text{.}
\end{equation*}
So the least squares solution minimizes a sum of squares.%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_ls_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp20596744}%
The University of Denver Infant Study Center investigated whether babies take longer to learn to crawl in cold months, when they are often bundled in clothes that restrict their movement, than in warmer months. The study sought a relationship between babies' first crawling age and the average temperature during the month they first try to crawl (about 6 months after birth). Some of the data from the study is in \hyperref[x:table:T_7_d_infants]{Table~{\xreffont\ref{x:table:T_7_d_infants}}}. Let \(x\) represent the temperature in degrees Fahrenheit and \(C(x)\) the average crawling age in months.%
\begin{tableptx}{\textbf{Crawling age}}{x:table:T_7_d_infants}{}%
\centering%
{\tabularfont%
\begin{tabular}{Alllll}\hrulethin
\multicolumn{1}{AlB}{\(x\)}&\multicolumn{1}{lA}{33}&\multicolumn{1}{lA}{37}&\multicolumn{1}{lA}{48}&\multicolumn{1}{lA}{57}\tabularnewline\hrulethin
\multicolumn{1}{AlB}{\(C(x)\)}&\multicolumn{1}{lA}{33.83}&\multicolumn{1}{lA}{33.35}&\multicolumn{1}{lA}{33.38}&\multicolumn{1}{lA}{32.32}\tabularnewline\hrulethin
\end{tabular}
}%
\end{tableptx}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the least squares line to fit this data. Plot the data and your line on the same set of axes. (We aren't concerned about whether a linear fit is really a good choice outside of this data set, we just fit a line to it to see what happens.)%
\item{}Use your least squares line to predict the average crawling age when the temperature is 65.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp20625304}%
The cost, in cents, of a first class postage stamp in years from 1981 to 1995 is shown in \hyperref[x:table:T_7_d_stamps]{Table~{\xreffont\ref{x:table:T_7_d_stamps}}}.%
\begin{tableptx}{\textbf{Cost of postage}}{x:table:T_7_d_stamps}{}%
\centering%
{\tabularfont%
\begin{tabular}{Acccccc}\hrulethin
\multicolumn{1}{AcB}{Year}&\multicolumn{1}{cA}{1981}&\multicolumn{1}{cA}{1985}&\multicolumn{1}{cA}{1988}&\multicolumn{1}{cA}{1991}&\multicolumn{1}{cA}{1995}\tabularnewline\hrulethin
\multicolumn{1}{AcB}{Cost}&\multicolumn{1}{cA}{20}&\multicolumn{1}{cA}{22}&\multicolumn{1}{cA}{25}&\multicolumn{1}{cA}{29}&\multicolumn{1}{cA}{32}\tabularnewline\hrulethin
\end{tabular}
}%
\end{tableptx}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the least squares line to fit this data. Plot the data and your line on the same set of axes.%
\item{}Now find the least squares quadratic approximation to this data. Plot the quadratic function on same axes as your linear function.%
\item{}Use your least squares line and quadratic to predict the cost of a postage stamp in this year. Look up the cost of a stamp today and determine how accurate your prediction is. Which function gives a better approximation? Provide reasons for any discrepancies.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp20635416}%
According to \pubtitle{The Song of Insects} by G.W. Pierce (Harvard College Press, 1948) the sound of striped ground crickets chirping, in number of chirps per second, is related to the temperature. So the number of chirps per second could be a predictor of temperature. The data Pierce collected is shown in the table and scatterplot below, where \(x\) is the (average) number of chirps per second and \(y\) is the temperature in degrees Fahrenheit.%
\begin{sidebyside}{2}{0}{0}{0}%
\begin{sbspanel}{0.5}%
\resizebox{\ifdim\width > \linewidth\linewidth\else\width\fi}{!}{%
{\centering%
{\tabularfont%
\begin{tabular}{Acc}\hrulethin
\multicolumn{1}{AcA}{\(x\)}&\multicolumn{1}{cA}{\(y\)}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{20.0}&\multicolumn{1}{cA}{88.6}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{16.0}&\multicolumn{1}{cA}{71.6}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{19.8}&\multicolumn{1}{cA}{93.3}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{18.4}&\multicolumn{1}{cA}{84.3}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{17.1}&\multicolumn{1}{cA}{80.6}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{15.5}&\multicolumn{1}{cA}{75.2}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{14.7}&\multicolumn{1}{cA}{69.7}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{17.1}&\multicolumn{1}{cA}{82.0}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{15.4}&\multicolumn{1}{cA}{69.4}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{16.2}&\multicolumn{1}{cA}{83.3}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{15.0}&\multicolumn{1}{cA}{79.6}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{17.2}&\multicolumn{1}{cA}{82.6}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{16.0}&\multicolumn{1}{cA}{80.6}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{17.0}&\multicolumn{1}{cA}{83.5}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{14.4}&\multicolumn{1}{cA}{76.3}\tabularnewline\hrulethin
\end{tabular}
}%
\par}
}%
\end{sbspanel}%
\begin{sbspanel}{0.5}%
\includegraphics[width=\linewidth]{external/10_7_crickets.pdf}
\end{sbspanel}%
\end{sidebyside}%
\par
The relationship between \(x\) and \(y\) is not exactly linear, but looks to have a linear pattern. It could be that the relationship is really linear but experimental error causes the data to be slightly inaccurate. Or perhaps the data is not linear, but only approximately linear. Find the least squares linear approximation to the data.%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp20653592}%
We showed that if the columns of \(A\) are linearly independent, then \(A^{\tr}A\) is invertible. Show that the reverse implication is also true. That is, show that if \(A^{\tr}A\) is invertible, then the columns of \(A\) are linearly independent.%
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{x:exercise:ex_6_f_not_li}%
Consider the small data set of points \(S = \{(2,1), (2,2), (2,3)\}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find a linear system \(A \vx = \vb\) whose solution would define a least squares linear approximation to the data in set \(S\).%
\item{}Explain what happens when we attempt to find the least squares solution \(\vx^*\) using the matrix \(\left(A^{\tr}A\right)^{-1}A^{\tr}\). Why does this happen?%
\item{}Does the system \(A \vx = \vb\) have a least squares solution? If so, how many and what are they? If not, why not?%
\item{}Fit a linear function of the form \(x = a_0 + a_1y\) to the data. Why should you have expected the answer?%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{x:exercise:ex_6_f_ranks}%
Let \(M\) and \(N\) be any matrices such that \(MN\) is defined. In this exercise we investigate relationships between ranks of various matrices.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(\Col MN\) is a subspace of \(\Col M\). Use this result to explain why \(\rank(MN) \leq \rank(M)\).%
\item{}Show that \(\rank\left(M^{\tr}M\right) = \rank(M) = \rank\left(M^{\tr}\right)\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp20676504}{}\quad{}For part, see Exercise 12 in Section 15.%
\item{}Show that \(\rank(MN) \leq \min\{\rank(M), \rank(N)\}\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp20677656}%
We have seen that if the columns of a matrix \(M\) are linearly independent, then%
\begin{equation*}
\va^* = \left(M^{\tr}M\right)^{-1}M^{\tr} \vb
\end{equation*}
is a least squares solution to \(M \va = \vy\). What if the columns of \(M\) are linearly dependent? From Activity 26.1, a least squares solution to \(M \va = \vy\) is a solution to the equation \(\left(M^{\tr}M\right)\va = M^{\tr}\vy\). In this exercise we demonstrate that \(\left(M^{\tr}M\right)\va = M^{\tr}\vy\) always has a solution.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why it is enough to show that the rank of the augmented matrix \([M^{\tr}M \ | \ M^{\tr}\vy]\) is the same as the rank of \(M^{\tr}M\).%
\item{}Explain why \(\rank\left([M^{\tr}M \ | \ M^{\tr}\vy]\right) \geq \rank(M)\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp20679064}{}\quad{}See \hyperlink{x:exercise:ex_6_f_ranks}{Exercise~{\xreffont 6}}.%
\item{}Explain why \([M^{\tr}M \ | \ M^{\tr}\vy] = M^{\tr}[M \ | \ \vy]\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp20685848}{}\quad{}Use the definition of the matrix product.%
\item{}Explain why \(\rank\left([M^{\tr}M \ | \ M^{\tr}\vy]\right) \leq \rank\left(M^{\tr} \right)\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp20683800}{}\quad{}See \hyperlink{x:exercise:ex_6_f_ranks}{Exercise~{\xreffont 6}}.%
\item{}Finally, explain why \(\rank\left([M^{\tr}M \ | \ M^{\tr}\vy]\right) = \rank\left(M^{\tr}M\right)\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp20678552}{}\quad{}Combine parts (b) and (d).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp20679832}%
If \(A\) is an \(m \times n\) matrix with linearly independent columns, the least squares solution \(\vx^* = \left(A^{\tr}A\right)^{-1}A^{\tr} \vb\) to \(A \vx = \vb\) has the property that \(A \vx^* = A\left(A^{\tr}A\right)^{-1}A^{\tr} \vb\) is the vector in \(\Col A\) that is closest to \(\vb\). That is, \(A\left(A^{\tr}A\right)^{-1}A^{\tr} \vb\) is the projection of \(\vb\) onto \(\Col A\). The matrix \(P = A\left(A^{\tr}A\right)^{-1}A^{\tr}\) is called a \terminology{projection matrix}. Projection matrices have special properties.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(P^2 = P = P^{\tr}\).%
\item{}In general, we define projection matrices as follows. \begin{definition}{}{g:definition:idp20690456}%
\index{projection matrix}A square matrix \(E\) is a \terminology{projection matrix} if \(E^2 = E\).%
\end{definition}
 Show that \(E = \left[ \begin{array}{cc} 0\amp 1\\ 0\amp 1 \end{array} \right]\) is a projection matrix. Onto what does \(E\) project?%
\item{}Notice that the projection matrix from part (b) is not an orthogonal matrix. \begin{definition}{}{g:definition:idp20689432}%
A square matrix \(E\) is a \terminology{orthogonal projection matrix} if \(E^2 = E = E^{\tr}\).%
\end{definition}
 Show that \(E = \left[ \begin{array}{cc} \frac{1}{2}\amp \frac{1}{2} \\ \frac{1}{2}\amp \frac{1}{2} \end{array} \right]\) is an orthogonal projection matrix. Onto what does \(E\) project?%
\item{}If \(E\) is an \(n \times n\) orthogonal projection matrix, show that if \(\vb\) is in \(\R^n\), then \(\vb - E\vb\) is orthogonal to every vector in \(\Col E\). (Hence, \(E\) projects orthogonally onto \(\Col E\).)%
\item{}Recall the projection \(\proj_{\vv} \vu\) of a vector \(\vu\) in the direction of a vector \(\vv\) is give by \(\proj_{\vv} \vu = \frac{\vu \cdot \vv}{\vv \cdot \vv} \vv\). Show that \(\proj_{\vv} \vu = E\vu\), where \(E\) is the orthogonal projection matrix \(\frac{1}{\vv \cdot \vv} \left(\vv \vv^{\tr} \right)\). Illustrate with \(\vv = [1 \ 1]^{\tr}\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp20710552}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
If the columns of \(A\) are linearly independent, then there is a unique least squares solution to \(A\vx = \vb\).%
\item{}\lititle{True\slash{}False.}\par%
Let \(\{\vv_1, \vv_2, \ldots, \vv_n\}\) be an orthonormal basis for \(\R^n\). The least squares solution to \([\vv_1 \ \vv_2 \ \cdots \ \vv_{n-1}] \vx = \vv_n\) is \(\vx = \vzero\).%
\item{}\lititle{True\slash{}False.}\par%
The least squares line to the data points \((0,0)\), \((1,0)\), and \((0,1)\) is \(y=x\).%
\item{}\lititle{True\slash{}False.}\par%
If the columns of a matrix \(A\) are not invertible and the vector \(\vb\) is not in \(\Col A\), then there is no least squares solution to \(A \vx = \vb\).%
\item{}\lititle{True\slash{}False.}\par%
Every matrix equation of the form \(A \vx = \vb\) has a least squares solution.%
\item{}\lititle{True\slash{}False.}\par%
If the columns of \(A\) are linearly independent, then the least squares solution to \(A \vx = \vb\) is the orthogonal projection of \(\vb\) onto \(\Col A\).%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Other Least Squares Approximations}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Other Least Squares Approximations}{}{Project: Other Least Squares Approximations}{}{}{x:section:sec_proj_ls_approx_other}
In this section we learned how to fit a polynomial function to a set of data in the least squares sense. But data takes on many forms, so it is important to be able to fit other types of functions to data sets. We investigate three different types of regression problems in this project.%
\begin{project}{}{g:project:idp20716568}%
The length of a species of fish is to be represented as a function of the age and water temperature as shown in the table on the next page.\footnotemark{} The fish are kept in tanks at 25, 27, 29 and 31 degrees Celsius. After birth, a test specimen is chosen at random every 14 days and its length measured. The data include:%
\begin{itemize}[label=\textbullet]
\item{}\(I\), the index;%
\item{}\(x\), the age of the fish in days;%
\item{}\(y\), the water temperature in degrees Celsius;%
\item{}\(z\), the length of the fish.%
\end{itemize}
%
\par
Since there are three variables in the data, we cannot perform a simple linear regression. Instead, we seek a model of the form%
\begin{equation*}
f(x,y) = ax+by+c
\end{equation*}
to fit the data, where \(f(x,y)\) approximates the length. That is, we seek the best fit plane to the data. This is an example of what is called multiple linear regression. A scatterplot of the data, along with the best fit plane, is also shown.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}As we did when we fit polynomials to data, we start by considering what would happen if all of our data points satisfied our model function. In this case our data points have the form \((x_1,y_1,z_1)\), \((x_2,y_2,z_2)\), \(\ldots\), \((x_m,y_m,z_m)\). Explain what system of linear equations would result if the data points actually satisfy our model function \(f(x,y)= ax+by+c\). (You don't need to write 44 different equations, just explain the general form of the system.)%
\item{}Write the system from (a) in the form \(M \va = \vz\), and specifically identify the matrix \(M\) and the vectors \(\va\) and \(\vz\).%
\item{}The same derivation as with the polynomial regression models shows that the vector \(\va^*\) that minimizes \(||\vz - M\va||\) is found by%
\begin{equation*}
\va^* = \left(M^{\tr}M\right)^{-1}M^{\tr}\vz\text{,}
\end{equation*}
Use this to find the least squares fit of the form \(f(x,y) = ax+by+c\) to the data.%
\item{}Provide a numeric measure of how well this model function fits the data. Explain.%
\end{enumerate}
\end{project}%
\footnotetext[47]{ Data from \pubtitle{Mathematical Algorithms for Linear Regression}, Helmut Spaeth, Academic Press, 1991, page 305, ISBN 0-12-656460-4.\label{g:fn:idp20726680}}%
\begin{sidebyside}{2}{0}{0}{0}%
\begin{sbspanel}{0.5}%
\resizebox{\ifdim\width > \linewidth\linewidth\else\width\fi}{!}{%
{\centering%
{\tabularfont%
\begin{tabular}{cccc}
Index&Age&Temp (\(^\circ\)C)&Length\tabularnewline\hrulemedium
1&14&25&620\tabularnewline[0pt]
2&28&25&1315\tabularnewline[0pt]
3&41&25&2120\tabularnewline[0pt]
4&55&25&2600\tabularnewline[0pt]
5&69&25&3110\tabularnewline[0pt]
6&83&25&3535\tabularnewline[0pt]
7&97&25&3935\tabularnewline[0pt]
8&111&25&4465\tabularnewline[0pt]
9&125&25&4530\tabularnewline[0pt]
10&139&25&4570\tabularnewline[0pt]
11&153&25&4600\tabularnewline[0pt]
12&14&27&625\tabularnewline[0pt]
13&28&27&1215\tabularnewline[0pt]
14&41&27&2110\tabularnewline[0pt]
15&55&27&2805\tabularnewline[0pt]
16&69&27&3255\tabularnewline[0pt]
17&83&27&4015\tabularnewline[0pt]
18&97&27&4315\tabularnewline[0pt]
19&111&27&4495\tabularnewline[0pt]
20&125&27&4535\tabularnewline[0pt]
21&139&27&4600\tabularnewline[0pt]
22&153&27&4600\tabularnewline[0pt]
23&14&29&590\tabularnewline[0pt]
24&28&29&1305\tabularnewline[0pt]
25&41&29&2140\tabularnewline[0pt]
26&55&29&2890\tabularnewline[0pt]
27&69&29&3920\tabularnewline[0pt]
28&83&29&3920\tabularnewline[0pt]
29&97&29&4515\tabularnewline[0pt]
30&111&29&4520\tabularnewline[0pt]
31&125&29&4525\tabularnewline[0pt]
32&139&29&4565\tabularnewline[0pt]
33&153&29&4566\tabularnewline[0pt]
34&14&31&590\tabularnewline[0pt]
35&28&31&1205\tabularnewline[0pt]
36&41&31&1915\tabularnewline[0pt]
37&55&31&2140\tabularnewline[0pt]
38&69&31&2710\tabularnewline[0pt]
39&83&31&3020\tabularnewline[0pt]
40&97&31&3030\tabularnewline[0pt]
41&111&31&3040\tabularnewline[0pt]
42&125&31&3180\tabularnewline[0pt]
43&139&31&3257\tabularnewline[0pt]
44&153&31&3214
\end{tabular}
}%
\par}
}%
\end{sbspanel}%
\begin{sbspanel}{0.5}%
\includegraphics[width=\linewidth]{external/Multi_linear.png}
\end{sbspanel}%
\end{sidebyside}%
\begin{project}{}{g:project:idp20791192}%
Population growth is typically not well modeled by polynomial functions. Populations tend to grow at rates proportional to the population, which implies exponential growth. For example, \hyperref[x:table:T_7_d_US_population]{Table~{\xreffont\ref{x:table:T_7_d_US_population}}} shows the approximate population of the United States in years between 1920 and 2000, with the population measured in millions.%
\begin{tableptx}{\textbf{U.S. population}}{x:table:T_7_d_US_population}{}%
\centering%
{\tabularfont%
\begin{tabular}{Alccccccccc}\hrulethin
\multicolumn{1}{AlB}{Year}&\multicolumn{1}{cA}{1920}&\multicolumn{1}{cA}{1930}&\multicolumn{1}{cA}{1940}&\multicolumn{1}{cA}{1950}&\multicolumn{1}{cA}{1960}&\multicolumn{1}{cA}{1970}&\multicolumn{1}{cA}{1980}&\multicolumn{1}{cA}{1990}&\multicolumn{1}{cA}{2000}\tabularnewline\hrulethin
\multicolumn{1}{AlB}{Population}&\multicolumn{1}{cA}{106}&\multicolumn{1}{cA}{123}&\multicolumn{1}{cA}{142}&\multicolumn{1}{cA}{161}&\multicolumn{1}{cA}{189}&\multicolumn{1}{cA}{213}&\multicolumn{1}{cA}{237}&\multicolumn{1}{cA}{259}&\multicolumn{1}{cA}{291}\tabularnewline\hrulethin
\end{tabular}
}%
\end{tableptx}%
If we assume the population grows exponentially, we would want to find the best fit function \(f\) of the form \(f(t) = ae^{kt}\), where \(a\) and \(k\) are constants. However, an exponential function is not linear. So to apply the methods we have developed, we could instead apply the natural logarithm to both sides of \(y = ae^{kt}\) to obtain the equation \(\ln(y) = \ln(a) + kt\). We can then find the best fit line to the data in the form \((t, \ln(y))\) to determine the values of \(\ln(a)\) and \(k\). Use this approach to find the best fit exponential function in the least squares sense to the U.S. population data. Then look up the U.S. population in 2010 (include your source) and compare to the estimate given by your model function. If your prediction is not very close, give some plausible explanations for the difference.%
\end{project}%
\begin{figureptx}{Best fit ellipse.}{x:figure:F_ellipse}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/ellipse_plot.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\begin{project}{}{g:project:idp20806296}%
Carl Friedrich Gauss is often credited with inventing the method of least squares. He used the method to find a best-fit ellipse which allowed him to correctly predict the orbit of the asteroid Ceres as it passed behind the sun in 1801. (Adrien-Marie Legendre appears to be the first to publish the method, though.) Here we examine the problem of fitting an ellipse to data.%
\par
An ellipse is a quadratic equation that can be written in the form%
\begin{equation}
x^2 + By^2 + Cxy + Dx + Ey + F = 0\label{x:men:eq_ellipse}
\end{equation}
for constants \(B\), \(C\), \(D\), \(E\), and \(F\), with \(B > 0\). We will find the best-fit ellipse in the least squares sense through the points%
\begin{equation*}
(0,2), \ (2,1), \ (1, -1), \ (-1, -2), \ (-3,1), \ \text{ and }  \ (-1, -1)\text{.}
\end{equation*}
%
\par
A picture of the best fit ellipse is shown in \hyperref[x:figure:F_ellipse]{Figure~{\xreffont\ref{x:figure:F_ellipse}}}.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the system of linear equations that would result if the ellipse \hyperref[x:men:eq_ellipse]{({\xreffont\ref{x:men:eq_ellipse}})} were to exactly pass through the given points.%
\item{}Write the linear system from part (a) in the form \(A \vx = \vb\), where the vector \(\vx\) contains the unknowns in the system. Clearly identify \(A\), \(\vx\), and \(\vb\).%
\item{}Find the least squares ellipse to this set of points. Make sure your method is clear. (Note that we are really fitting a surface of the form \(f(x,y) = x^2 + By^2 + Cxy + Dx + Ey + F\) to a set of data points in the \(xy\)-plane. So the error is the sum of the vertical distances from the points in the \(xy\)-plane to the surface.)%
\end{enumerate}
\end{project}%
\end{sectionptx}
\end{chapterptx}
\end{partptx}
%
%
\typeout{************************************************}
\typeout{Chapter VI Applications of Orthogonality}
\typeout{************************************************}
%
\begin{partptx}{Applications of Orthogonality}{}{Applications of Orthogonality}{}{}{x:part:part-app-orthog}
 %
%
\typeout{************************************************}
\typeout{Section 27 Orthogonal Diagonalization}
\typeout{************************************************}
%
\begin{chapterptx}{Orthogonal Diagonalization}{}{Orthogonal Diagonalization}{}{}{x:chapter:chap_orthogonal_diagonalization}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp20819224}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What does it mean for a matrix to be orthogonally diagonalizable and why is this concept important?%
\item{}What is a symmetric matrix and what important property related to diagonalization does a symmetric matrix have?%
\item{}What is the spectrum of a matrix?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: The Multivariable Second Derivative Test}
\typeout{************************************************}
%
\begin{sectionptx}{Application: The Multivariable Second Derivative Test}{}{Application: The Multivariable Second Derivative Test}{}{}{x:section:sec_appl_mulit_2nd_deriv}
In single variable calculus, we learn that the second derivative can be used to classify a critical point of the type where the derivative of a function is 0 as a local maximum or minimum.%
\begin{theorem}{The Second Derivative Test for Single-Variable Functions.}{}{g:theorem:idp20824600}%
If \(a\) is a critical number of a function \(f\) so that \(f'(a)=0\) and if \(f''(a)\) exists, then%
\begin{itemize}[label=\textbullet]
\item{}if \(f''(a) \lt 0\), then \(f(a)\) is a local maximum value of \(f\),%
\item{}if \(f''(a) > 0\), then \(f(a)\) is a local minimum value of \(f\), and%
\item{}if \(f''(a) = 0\), this test yields no information.%
\end{itemize}
%
\end{theorem}
In the two-variable case we have an analogous test, which is usually seen in a multivariable calculus course.%
\begin{theorem}{The Second Derivative Test for Functions of Two Variables.}{}{g:theorem:idp20828568}%
Suppose \((a,b)\) is a critical point of the function \(f\) for which \(f_x(a,b) = 0\) and \(f_y(a,b) = 0\). Let \(D\) be the quantity defined by%
\begin{equation*}
D = f_{xx}(a,b) f_{yy}(a,b) - f_{xy}(a,b)^2\text{.}
\end{equation*}
%
\begin{itemize}[label=\textbullet]
\item{}If \(D>0\) and \(f_{xx}(a,b) \lt 0\), then \(f\) has a local maximum at \((a,b)\).%
\item{}If \(D>0\) and \(f_{xx}(a,b) > 0\), then \(f\) has a local minimum at \((a,b)\).%
\item{}If \(D \lt 0\), then \(f\) has a saddle point at \((a,b)\).%
\item{}If \(D = 0\), then this test yields no information about what happens at \((a,b)\).%
\end{itemize}
%
\end{theorem}
A proof of this test for two-variable functions is based on Taylor polynomials, and relies on symmetric matrices, eigenvalues, and quadratic forms. The steps for a proof will be found later in this section.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_orthog_diag_intro}
We have seen how to diagonalize a matrix \textemdash{} if we can find \(n\) linearly independent eigenvectors of an \(n\times n\) matrix \(A\) and let \(P\) be the matrix whose columns are those eigenvectors, then \(P^{-1}AP\) is a diagonal matrix with the eigenvalues down the diagonal in the same order corresponding to the eigenvectors placed in \(P\). We will see that in certain cases we can take this one step further and create an orthogonal matrix with eigenvectors as columns to diagonalize a matrix. This is called orthogonal diagonalization. Orthogonal diagonalizability is useful in that it allows us to find a ``convenient'' coordinate system in which to interpret the results of certain matrix transformations. A set of orthonormal basis vectors for an orthogonally diagonalizable matrix \(A\) is called a set of \terminology{principal axes} for \(A\). Orthogonal diagonalization will also play a crucial role in the singular value decomposition of a matrix, a decomposition that has been described by some as the ``pinnacle'' of linear algebra.%
\begin{definition}{}{x:definition:def_7_a_orthogonal__diagonalization}%
\index{orthogonal diagonalization}%
An \(n \times n\) matrix \(A\) is \terminology{orthogonally diagonalizable} if there is an orthogonal matrix \(P\) such that%
\begin{equation*}
P^{\tr}AP
\end{equation*}
is a diagonal matrix. We say that the matrix \(P\) \terminology{orthogonally diagonalizes} the matrix \(A\).%
\end{definition}
\begin{exploration}{}{x:exploration:pa_7_a}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}For each matrix \(A\) whose eigenvalues and corresponding eigenvectors are given, find a matrix \(P\) such that \(P^{-1}AP\) is a diagonal matrix.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}\(A = \left[ \begin{array}{cc} 1\amp 2 \\ 2\amp 1 \end{array} \right]\) with eigenvalues \(-1\) and 3 and corresponding eigenvectors \(\vv_1 = \left[ \begin{array}{r} -1 \\ 1 \end{array} \right]\) and \(\vv_2 = \left[ \begin{array}{c} 1 \\ 1 \end{array} \right]\).%
\item{}\(A = \left[ \begin{array}{cc} 1\amp 2 \\ 1\amp 2 \end{array} \right]\) with eigenvalues \(0\) and \(3\) and corresponding eigenvectors \(\vv_1 = \left[ \begin{array}{r} -2 \\ 1 \end{array} \right]\) and \(\vv_2 = \left[ \begin{array}{c} 1 \\ 1 \end{array} \right]\).%
\item{}\(A = \left[ \begin{array}{ccc} 1\amp 0\amp 1 \\ 0\amp 1\amp 1 \\ 1\amp 1\amp 2 \end{array} \right]\) with eigenvalues \(0\), \(1\), and \(3\) and corresponding eigenvectors \(\vv_1 = \left[ \begin{array}{r} -1 \\ -1 \\ 1 \end{array} \right]\), \(\vv_2 = \left[ \begin{array}{r} -1 \\ 1 \\ 0 \end{array} \right]\), and \(\vv_3 = \left[ \begin{array}{c} 1 \\ 1 \\ 2 \end{array} \right]\).%
\end{enumerate}
\item{}Which matrices in part 1 seem to satisfy the orthogonal diagonalization requirement? Do you notice any common traits among these matrices?%
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Symmetric Matrices}
\typeout{************************************************}
%
\begin{sectionptx}{Symmetric Matrices}{}{Symmetric Matrices}{}{}{x:section:sec_mtx_symm}
As we saw in \hyperref[x:exploration:pa_7_a]{Preview Activity~{\xreffont\ref{x:exploration:pa_7_a}}}, matrices that are not symmetric need not be orthogonally diagonalizable, but the symmetric matrix examples are orthogonally diagonalizable. We explore that idea in this section.%
\par
If \(P\) is a matrix that orthogonally diagonalizes the matrix \(A\), then \(P^{\tr}AP = D\), where \(D\) is a diagonal matrix. Since \(D^{\tr} = D\) and \(A = PDP^{\tr}\), we have%
\begin{align*}
A \amp = PDP^{\tr}\\
\amp = PD^{\tr}P^{\tr}\\
\amp = \left(P^{\tr}\right)^{\tr}D^{\tr}P^{\tr}\\
\amp = \left(PDP^{\tr}\right)^{\tr}\\
\amp = A^{\tr}\text{.}
\end{align*}
%
\par
Therefore, \(A^{\tr} = A\) and matrices with this property are the only matrices that can be orthogonally diagonalized. Recall that any matrix \(A\) satisfying \(A^{\tr} = A\) is a symmetric matrix.%
\par
While we have just shown that the only matrices that can be orthogonally diagonalized are the symmetric matrices, the amazing thing about symmetric matrices is that \emph{every} symmetric matrix can be orthogonally diagonalized. We will prove this shortly.%
\par
Symmetric matrices have useful properties, a few of which are given in the following activity (we will use some of these properties later in this section).%
\begin{activity}{}{x:activity:act_7_a_symmetric}%
Let \(A\) be a symmetric \(n \times n\) matrix and let \(\vx\) and \(\vy\) be vectors in \(\R^n\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(\vx^{\tr} A \vy = (A\vx)^{\tr} \vy\).%
\item{}Show that \((A\vx) \cdot \vy = \vx \cdot (A\vy)\).%
\item{}Show that the eigenvalues of a \(2 \times 2\) symmetric matrix \(A = \left[ \begin{array}{cc} a\amp b\\b\amp c \end{array} \right]\) are real.%
\end{enumerate}
\end{activity}%
\hyperref[x:activity:act_7_a_symmetric]{Activity~{\xreffont\ref{x:activity:act_7_a_symmetric}}} (c) shows that a \(2 \times 2\) symmetric matrix has real eigenvalues. This is a general result about real symmetric matrices.%
\begin{theorem}{}{}{x:theorem:thm_7_a_symmetric_eigenvalues}%
Let \(A\) be an \(n\times n\) symmetric matrix with real entries. Then the eigenvalues of \(A\) are real.%
\end{theorem}
\begin{proof}{}{g:proof:idp20604696}
Let \(A\) be an \(n\times n\) symmetric matrix with real entries and let \(\lambda\) be an eigenvalue of \(A\) with eigenvector \(\vv\). To show that \(\lambda\) is real, we will show that \(\overline{\lambda} = \lambda\). We know%
\begin{equation}
A \vv = \lambda \vv\text{.}\label{x:men:eq_symm1}
\end{equation}
%
\par
Since \(A\) has real entries, we also know that \(\overline{\lambda}\) is an eigenvalue for \(A\) with eigenvector \(\overline{\vv}\). Multiply both sides of \hyperref[x:men:eq_symm1]{({\xreffont\ref{x:men:eq_symm1}})} on the left by \(\overline{\vv}^{\tr}\) to obtain%
\begin{equation}
\overline{\vv}^{\tr} A \vv = \overline{\vv}^{\tr} \lambda \vv = \lambda \left(\overline{\vv}^{\tr} \vv \right)\text{.}\label{x:men:eq_symm2}
\end{equation}
%
\par
Now%
\begin{equation*}
\overline{\vv}^{\tr} A \vv = (A\overline{\vv})^{\tr} \vv = (\overline{\lambda} \ \overline{\vv})^{\tr} \vv = \overline{\lambda} \left(\overline{\vv}^{\tr} \vv \right)
\end{equation*}
and equation \hyperref[x:men:eq_symm2]{({\xreffont\ref{x:men:eq_symm2}})} becomes%
\begin{equation*}
\overline{\lambda} \left(\overline{\vv}^{\tr} \vv \right) = \lambda \left(\overline{\vv}^{\tr} \vv \right)\text{.}
\end{equation*}
%
\par
Since \(\vv \neq \vzero\), this implies that \(\overline{\lambda} = \lambda\) and \(\lambda\) is real.%
\end{proof}
To orthogonally diagonalize a matrix, it must be the case that eigenvectors corresponding to different eigenvalues are orthogonal. This is an important property and it would be useful to know when it happens.%
\begin{activity}{}{x:activity:act_7_a_symmetric_eigenvalues}%
Let \(A\) be a real symmetric matrix with eigenvalues \(\lambda_1\) and \(\lambda_2\) and corresponding eigenvectors \(\vv_1\) and \(\vv_2\), respectively.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Use \hyperref[x:activity:act_7_a_symmetric]{Activity~{\xreffont\ref{x:activity:act_7_a_symmetric}}} (b) to show that \(\lambda_1 \vv_1\cdot \vv_2 = \lambda_2 \vv_1 \cdot \vv_2\).%
\item{}Explain why the result of part (a) shows that \(\vv_1\) and \(\vv_2\) are orthogonal if \(\lambda_1\neq \lambda_2\).%
\end{enumerate}
\end{activity}%
\hyperref[x:activity:act_7_a_symmetric_eigenvalues]{Activity~{\xreffont\ref{x:activity:act_7_a_symmetric_eigenvalues}}} proves the following theorem.%
\begin{theorem}{}{}{g:theorem:idp20926744}%
If \(A\) is a real symmetric matrix, then eigenvectors corresponding to distinct eigenvalues are orthogonal.%
\end{theorem}
Recall that the only matrices that can be orthogonally diagonalized are the symmetric matrices. Now we show that every real symmetric matrix can be orthogonally diagonalized, which completely characterizes the matrices that are orthogonally diagonalizable. The proof of the following theorem proceeds by induction. A reader who has not yet encountered this technique of proof can safely skip the proof of this theorem without loss of continuity.%
\begin{theorem}{}{}{g:theorem:idp20935704}%
Let \(A\) be a real symmetric matrix. Then \(A\) is orthogonally diagonalizable.%
\end{theorem}
\begin{proof}{}{g:proof:idp20935576}
Let \(A\) be a real \(n \times n\) symmetric matrix. The proof proceeds by induction on \(n\). If \(n = 1\), then \(A\) is diagonal and orthogonally diagonalizable. So assume that any real \((n-1) \times (n-1)\) symmetric matrix is orthogonally diagonalizable. Assume that \(A\) is a real \(n \times n\) symmetric matrix. By Theorem 25.4 (find reference), the eigenvalues of \(A\) are real. Let \(\lambda_1\) be a real eigenvalue of \(A\) with corresponding unit eigenvector \(\vp_1\). We can use the Gram-Schmidt process to extend \(\{\vp_1\}\) to an orthonormal basis \(\{\vp_1, \vp_2, \ldots, \vp_n\}\) for \(\R^n\). Let \(P_1 = [\vp_1 \ \vp_2 \ \ldots \ \vp_n]\). Then \(P_1\) is an orthogonal matrix. Also,%
\begin{align*}
P_1^{-1}AP_1 \amp = P_1^{\tr}AP_1\\
\amp = P_1^{\tr} [A\vp_1 \ A\vp_2 \ \ldots \ A\vp_n]\\
\amp = \left[ \begin{array}{c} \vp_1^{\tr}\\
\vp_2^{\tr}\\
\vdots\\
\vp_n^{\tr} \end{array} \right]  [\lambda \vp_1 \ A\vp_2 \ \ldots \ A\vp_n]\\
\amp = \left[ \begin{array}{ccccc} \vp_1^{\tr} \lambda_1 \vp_1 \amp  \vp_1^{\tr} A\vp_2 \amp  \vp_1 A\vp_3 \amp  \cdots \amp  \vp_1^{\tr} A \vp_n\\
\vp_2^{\tr} \lambda_1 \vp_1 \amp  \vp_2^{\tr} A\vp_2 \amp  \vp_2 A\vp_3 \amp \cdots \amp  \vp_2^{\tr} A \vp_n\\
\amp   \amp  \vdots   \amp  \amp\\
\vp_n^{\tr} \lambda_1 \vp_1 \amp  \vp_n^{\tr} A\vp_2 \amp  \vp_n A\vp_3 \amp \cdots \amp  \vp_n^{\tr} A \vp_n \end{array} \right]\\
\amp = \left[ \begin{array}{ccccc} \lambda_1\amp  \vp_1^{\tr} A\vp_2 \amp  \vp_1 A\vp_3 \amp  \cdots \amp  \vp_1^{\tr} A \vp_n\\
0 \amp  \vp_2^{\tr} A\vp_2 \amp  \vp_2 A\vp_3 \amp \cdots \amp  \vp_2^{\tr} A \vp_n\\
\amp   \amp  \vdots   \amp  \amp\\
0 \amp  \vp_n^{\tr} A\vp_2 \amp  \vp_n A\vp_3 \amp \cdots \amp  \vp_n^{\tr} A \vp_n \end{array} \right]\\
\amp = \left[ \begin{array}{cc} \lambda_1\amp \vx^{\tr}\\
\vzero \amp  A_1 \end{array} \right]
\end{align*}
where \(\vx\) is a \((n-1)\times 1\) vector, \(\vzero\) is the zero vector in \(\R^{n-1}\), and \(A_1\) is an \((n-1) \times (n-1)\) matrix. Letting \(R = P_1^{\tr}AP_1\) we have that%
\begin{equation*}
R^{\tr} = \left(P_1^{\tr}AP_1\right)^{\tr} = P_1^{\tr}A^{\tr}P_1 = P_1^{\tr}AP_1 = R\text{,}
\end{equation*}
so \(R\) is a symmetric matrix. Therefore, \(\vx = \vzero\) and \(A_1\) is a symmetric matrix. By our induction hypothesis, \(A_1\) is orthogonally diagonalizable. That is, there exists an \((n-1) \times (n-1)\) orthogonal matrix \(Q\) such that \(Q^{\tr}A_1Q = D_1\), where \(D_1\) is a diagonal matrix. Now define \(P_2\) by%
\begin{equation*}
P_2 = \left[ \begin{array}{cc} 1\amp \vzero^{\tr} \\ \vzero \amp  Q \end{array}  \right]\text{,}
\end{equation*}
where \(\vzero\) is the zero vector in \(\R^{n-1}\). By construction, the columns of \(P_2\) are orthonormal, so \(P_2\) is an orthogonal matrix. Since \(P_1\) is also an orthogonal matrix,%
\begin{equation*}
P^{\tr} = (P_1P_2)^{\tr} = P_2^{\tr}P_1^{\tr} = P_2^{-1}P_1^{-1} = (P_1P_2)^{-1}  = P^{-1}
\end{equation*}
and \(P\) is an orthogonal matrix. Finally,%
\begin{align*}
P^{\tr}AP \amp = (P_1P_2)^{\tr}A(P_1P_2)\\
\amp = P_2^{\tr}\left(P_1^{\tr}AP_1\right)P_2\\
\amp = \left[ \begin{array}{cc} 1\amp \vzero^{\tr}\\
\vzero \amp  Q \end{array} \right]^{\tr} \left[ \begin{array}{cc} \lambda_1\amp \vx^{\tr}\\
0 \amp  A_1 \end{array} \right] \left[ \begin{array}{cc} 1\amp \vzero^{\tr}\\
\vzero \amp  Q \end{array} \right]\\
\amp = \left[ \begin{array}{cc} 1\amp \vzero^{\tr}\\
\vzero \amp  Q^{\tr} \end{array} \right] \left[ \begin{array}{cc} \lambda_1\amp \vx^{\tr}\\
0 \amp  A_1 \end{array} \right]\left[ \begin{array}{cc} 1\amp \vzero^{\tr}\\
\vzero \amp  Q \end{array} \right]\\
\amp = \left[ \begin{array}{cc} \lambda_1\amp \vzero^{\tr}\\
\vzero \amp  Q^{\tr}A_1Q \end{array} \right]\\
\amp = \left[ \begin{array}{cc} \lambda_1\amp \vzero^{\tr}\\
\vzero \amp  D_1 \end{array} \right]\text{.}
\end{align*}
%
\par
Therefore, \(P^{\tr}AP\) is a diagonal matrix and \(P\) orthogonally diagonalizes \(A\). This completes our proof.%
\end{proof}
\index{spectrum of a matrix} The set of eigenvalues of a matrix \(A\) is called the \terminology{spectrum} of \(A\) and we have just proved the following theorem.%
\begin{theorem}{The Spectral Theorem for Real Symmetric Matrices.}{}{g:theorem:idp20963352}%
Let \(A\) be an \(n \times n\) symmetric matrix with real entries. Then%
\begin{enumerate}
\item{}\(A\) has \(n\) real eigenvalues (counting multiplicities)%
\item{}the dimension of each eigenspace of \(A\) is the multiplicity of the corresponding eigenvalue as a root of the characteristic polynomial%
\item{}eigenvectors corresponding to different eigenvalues are orthogonal%
\item{}\(A\) is orthogonally diagonalizable.%
\end{enumerate}
%
\end{theorem}
So \emph{any} real symmetric matrix is orthogonally diagonalizable. We have seen examples of the orthogonal diagonalization of \(n \times n\) real symmetric matrices with \(n\) distinct eigenvalues, but how do we orthogonally diagonalize a symmetric matrix having eigenvalues of multiplicity greater than 1? The next activity shows us the process.%
\begin{activity}{}{x:activity:act_7_a_3}%
Let \(A = \left[ \begin{array}{ccc} 4\amp 2\amp 2 \\ 2\amp 4\amp 2 \\ 2\amp 2\amp 4 \end{array} \right]\). The eigenvalues of \(A\) are 2 and 8, with eigenspace of dimension 2 and dimension 1, respectively.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why \(A\) can be orthogonally diagonalized.%
\item{}Two linearly independent eigenvectors for \(A\) corresponding to the eigenvalue 2 are \(\vv_1 = \left[ \begin{array}{r} -1 \\ 0 \\ 1 \end{array} \right]\) and \(\vv_2 = \left[ \begin{array}{r} -1 \\ 1 \\ 0 \end{array} \right]\). Note that \(\vv_1, \vv_2\) are not orthogonal, so cannot be in an orthogonal basis of \(\R^3\) consisting of eigenvectors of \(A\). So find a set \(\{\vw_1, \vw_2\}\) of orthogonal eigenvectors of \(A\) so that \(\Span\{\vw_1, \vw_2\} = \Span\{\vv_1, \vv_2\}\).%
\item{}The vector \(\vv_3=\left[ \begin{array}{c} 1 \\ 1 \\ 1 \end{array} \right]\) is an eigenvector for \(A\) corresponding to the eigenvalue 8. What can you say about the orthogonality relationship between \(\vw_i\)'s and \(\vv_3\)?%
\item{}Find a matrix \(P\) that orthogonally diagonalizes \(A\). Verify your work.%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Spectral Decomposition of a Symmetric Matrix \(A\)}
\typeout{************************************************}
%
\begin{sectionptx}{The Spectral Decomposition of a Symmetric Matrix \(A\)}{}{The Spectral Decomposition of a Symmetric Matrix \(A\)}{}{}{x:section:sec_spec_decomp_symm_mtx}
Let \(A\) be an \(n \times n\) symmetric matrix with real entries. The Spectral Theorem tells us we can find an orthonormal basis \(\{\vu_1, \vu_2, \ldots, \vu_n\}\) of eigenvectors of \(A\). Let \(A \vu_i = \lambda_i \vu_i\) for each \(1 \leq i \leq n\). If \(P = [ \vu_1 \  \vu_2 \  \vu_3 \  \cdots \ \vu_n]\), then we know that%
\begin{equation*}
P^{\tr}AP = P^{-1}AP = D\text{,}
\end{equation*}
where \(D\) is the \(n \times n\) diagonal matrix%
\begin{equation*}
\left[ \begin{array}{cccccc} \lambda_1\amp 0\amp 0\amp \cdots\amp 0\amp 0 \\ 0\amp \lambda_2\amp 0\amp \cdots\amp 0\amp 0 \\ \vdots\amp \vdots\amp \vdots\amp \amp \vdots\amp \vdots \\ 0\amp 0\amp 0\amp \cdots\amp 0\amp \lambda_n \end{array}  \right]\text{.}
\end{equation*}
%
\par
\index{spectral decomposition} Since \(A = PDP^{\tr}\) we see that%
\begin{align}
A \amp = [ \vu_1 \ \vu_2 \ \vu_3 \ \cdots \ \vu_n] \left[ \begin{array}{cccccc} \lambda_1\amp 0\amp 0\amp \cdots\amp 0\amp 0\notag\\
0\amp \lambda_2\amp 0\amp \cdots\amp 0\amp 0\notag\\
\vdots\amp \vdots\amp \vdots\amp \amp \vdots\amp \vdots\notag\\
0\amp 0\amp 0\amp \cdots\amp 0\amp \lambda_n \end{array} \right] \left[ \begin{array}{c} \vu_1^{\tr}\notag\\
\vu_2^{\tr}\notag\\
\vu_3^{\tr}\notag\\
\vdots\notag\\
\vu_n^{\tr} \end{array} \right]\notag\\
\amp = [ \lambda_1\vu_1 \ \lambda_2\vu_2 \ \lambda_3\vu_3 \ \cdots \ \lambda_n\vu_n] \left[ \begin{array}{c} \vu_1^{\tr}\notag\\
\vu_2^{\tr}\notag\\
\vu_3^{\tr}\notag\\
\vdots\notag\\
\vu_n^{\tr} \end{array} \right]\notag\\
\amp = \lambda_1 \vu_1\vu_1^{\tr} + \lambda_2 \vu_2\vu_2^{\tr} + \lambda_3 \vu_3\vu_3^{\tr} + \cdots + \lambda_n \vu_n\vu_n^{\tr}\text{,}\label{x:mrow:eq_spec_decomp}
\end{align}
where the last product follows from \hyperlink{x:exercise:ex_7_a_product}{Exercise~{\xreffont 4}}. The expression in \hyperref[x:mrow:eq_spec_decomp]{({\xreffont\ref{x:mrow:eq_spec_decomp}})} is called a \terminology{spectral decomposition} of the matrix \(A\). Let \(P_i = \vu_i\vu_i^{\tr}\) for each \(i\). The matrices \(P_i\) satisfy several special conditions given in the next theorem. The proofs are left to the exercises.%
\begin{theorem}{}{}{x:theorem:thm_7_a_spectral_decomposition}%
Let \(A\) be an \(n \times n\) symmetric matrix with real entries, and let \(\{\vu_1, \vu_2, \ldots, \vu_n\}\) be an orthonormal basis of eigenvectors of \(A\) with \(A \vu_i = \lambda_i \vu_i\) for each \(i\). For each \(i\), let \(P_i = \vu_i\vu_i^{\tr}\). Then%
\begin{enumerate}
\item{}\(A = \lambda_1 P_1 + \lambda_2 P_2 + \cdots + \lambda_n P_n\),%
\item{}\(P_i\) is a symmetric matrix for each \(i\),%
\item{}\(P_i\) is a rank 1 matrix for each \(i\),%
\item{}\(P_i^2 = P_i\) for each \(i\),%
\item{}\(P_i P_j = 0\) if \(i \neq j\),%
\item{}\(P_i \vu_i = \vu_i\) for each \(i\),%
\item{}\(P_i \vu_j = \vzero\) if \(i \neq j\),%
\item{}For any vector \(\vv\) in \(\R^n\), \(P_i \vv = \text{ proj } _{\Span\{\vu_i\}} \vv\).%
\end{enumerate}
%
\end{theorem}
The consequence of \hyperref[x:theorem:thm_7_a_spectral_decomposition]{Theorem~{\xreffont\ref{x:theorem:thm_7_a_spectral_decomposition}}} is that any symmetric matrix can be written as the sum of symmetric, rank 1 matrices. As we will see later, this kind of decomposition contains much information about the matrix product \(A^{\tr}A\) for any matrix \(A\).%
\begin{activity}{}{x:activity:act_7_a_4}%
Let \(A = \left[ \begin{array}{ccc} 4\amp 2\amp 2 \\ 2\amp 4\amp 2 \\ 2\amp 2\amp 4 \end{array} \right]\). Let \(\lambda_1 = 2\), \(\lambda_2 = 2\), and \(\lambda_3 = 8\) be the eigenvalues of \(A\). A basis for the eigenspace \(E_8\) of \(A\) corresponding to the eigenvalue 8 is \(\{[1 \ 1\ 1]^{\tr}\}\) and a basis for the eigenspace \(E_2\) of \(A\) corresponding to the eigenvalue 2 is \(\{[1 \ -1\ 0]^{\tr}, [1 \ 0 \ -1]^{\tr}\}\). (Compare to \hyperref[x:activity:act_7_a_3]{Activity~{\xreffont\ref{x:activity:act_7_a_3}}}.)%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find orthonormal eigenvectors \(\vu_1\), \(\vu_2\), and \(\vu_3\) of \(A\) corresponding to \(\lambda_1\), \(\lambda_2\), and \(\lambda_3\), respectively.%
\item{}Compute \(\lambda_1 \vu_1\vu_1^{\tr}\)%
\item{}Compute \(\lambda_2 \vu_2\vu_2^{\tr}\)%
\item{}Compute \(\lambda_3 \vu_3\vu_3^{\tr}\)%
\item{}Verify that \(A = \lambda_1 \vu_1\vu_1^{\tr} + \lambda_2 \vu_2\vu_2^{\tr} + \lambda_3 \vu_3\vu_3^{\tr}\).%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_orthog_diag_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp21029656}%
For each of the following matrices \(A\), determine if \(A\) is diagonalizable. If \(A\) is not diagonalizable, explain why. If \(A\) is diagonalizable, find a matrix \(P\) so that \(P^{-1}AP\) is a diagonal matrix. If the matrix is diagonalizable, is it orthogonally diagonalizable? If orthogonally diagonalizable, find an orthogonal matrix that diagonalizes \(A\). Use appropriate technology to find eigenvalues and eigenvectors.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(A = \left[ \begin{array}{rrc} 2 \amp 0 \amp 0 \\ -1 \amp 3 \amp 2 \\ 1 \amp -1 \amp 0 \end{array} \right]\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp21030808}{}\quad{}Recall that an \(n \times n\) matrix \(A\) is diagonalizable if and only if \(A\) has \(n\) linearly independent eigenvectors, and \(A\) is orthogonally diagonalizable if and only if \(A\) is symmetric. Since \(A\) is not symmetric, \(A\) is not orthogonally diagonalizable. Technology shows that the eigenvalues of \(A\) are \(2\) and \(1\) and bases for the corresponding eigenspaces are \(\{ [1 \ 1\ 0]^{\tr}, [2 \ 0 \ 1]^{\tr} \}\) and \(\{[0 \ -1 \ 1]^{\tr}\}\). So \(A\) is diagonalizable and if \(P = \left[ \begin{array}{rcr} 1\amp 2\amp 0\\1\amp 0\amp -1\\0\amp 1\amp 1 \end{array}  \right]\), then%
\begin{equation*}
P^{-1}AP = \left[ \begin{array}{ccc} 2\amp 0\amp 0\\0\amp 2\amp 0\\0\amp 0\amp 1 \end{array}  \right]\text{.}
\end{equation*}
%
\item{}\(A = \left[ \begin{array}{ccc} 1 \amp 1 \amp 0 \\ 0 \amp 1 \amp 0 \\ 0 \amp 0 \amp 0 \end{array} \right]\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp21035928}{}\quad{}Since \(A\) is not symmetric, \(A\) is not orthogonally diagonalizable. Technology shows that the eigenvalues of \(A\) are \(0\) and \(1\) and bases for the corresponding eigenspaces are \(\{[0 \ 0 \ 1]^{\tr}\}\) and \(\{ [1 \ 0\ 0]^{\tr} \}\). We cannot create a basis of \(\R^3\) consisting of eigenvectors of \(A\), so \(A\) is not diagonalizable.%
\item{}\(A = \left[ \begin{array}{ccc} 4 \amp 2 \amp 1 \\ 2 \amp 7 \amp 2 \\ 1 \amp 2 \amp 4 \end{array} \right]\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp21044888}{}\quad{}Since \(A\) is symmetric, \(A\) is orthogonally diagonalizable. Technology shows that the eigenvalues of \(A\) are \(3\) and \(9\) and bases for the eigenspaces \(\{[-1 \ 0 \ 1]^{\tr}, [-2 \ 1 \ 0]^{\tr}\}\) and \(\{ [1 \ 2 \ 1]^{\tr} \}\), respectively. To find an orthogonal matrix that diagonalizes \(A\), we must find an orthonormal basis of \(\R^3\) consisting of eigenvectors of \(A\). To do that, we use the Gram-Schmidt process to obtain an orthogonal basis for the eigenspace of \(A\) corresponding to the eigenvalue \(3\). Doing so gives an orthogonal basis \(\{\vv_1, \vv_2\}\), where \(\vv_1 = [-1 \ 0 \ 1]^{\tr}\) and%
\begin{align*}
\vv_2 \amp =  [-2 \ 1 \ 0]^{\tr} - \frac{ [-2 \ 1 \ 0]^{\tr} \cdot [-1 \ 0 \ 1]^{\tr}}{[-1 \ 0 \ 1]^{\tr} \cdot [-1 \ 0 \ 1]^{\tr}} [-1 \ 0 \ 1]^{\tr}\\
\amp =  [-2 \ 1 \ 0]^{\tr} - [-1 \ 0 \ 1]^{\tr}\\
\amp = [ -1 \ 1 \ -1]^{\tr}\text{.}
\end{align*}
So an orthonormal basis for \(\R^3\) of eigenvectors of \(A\) is%
\begin{equation*}
\left\{\frac{1}{\sqrt{2}} [-1 \ 0 \ 1]^{\tr}, \frac{1}{\sqrt{3}}[ -1 \ 1 \ -1]^{\tr}, \frac{1}{\sqrt{6}}[1 \ 1 \ 1]^{\tr} \right\}\text{.}
\end{equation*}
Therefore, \(A\) is orthogonally diagonalizable and if \(P\) is the matrix \(\left[{ \begin{array}{rrc} -\frac{1}{\sqrt{2}}\amp -\frac{1}{\sqrt{3}}\amp \frac{1}{\sqrt{6}}\\0\amp \frac{1}{\sqrt{3}}\amp \frac{2}{\sqrt{6}}\\\frac{1}{\sqrt{2}}\amp -\frac{1}{\sqrt{3}}\amp \frac{1}{\sqrt{6}} \end{array} } \right]\), then%
\begin{equation*}
P^{-1}AP = \left[ \begin{array}{ccc} 3\amp 0\amp 0\\0\amp 3\amp 0\\0\amp 0\amp 9 \end{array}  \right]\text{.}
\end{equation*}
%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp21051288}%
Let \(A = \left[ \begin{array}{cccc} 0\amp 0\amp 0\amp 1 \\ 0\amp 0\amp 1\amp 0 \\ 0\amp 1\amp 0\amp 0 \\ 1\amp 0\amp 0\amp 0 \end{array} \right]\). Find an orthonormal basis for \(\R^4\) consisting of eigenvectors of \(A\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp21054360}{}\quad{}Since \(A\) is symmetric, there is an orthogonal matrix \(P\) such that \(P^{-1}AP\) is diagonal. The columns of \(P\) will form an orthonormal basis for \(\R^4\). Using a cofactor expansion along the first row shows that%
\begin{align*}
\det(A-\lambda I_4) \amp = \det\left(\left[ \begin{array}{rrrr} -\lambda\amp 0\amp 0\amp 1\\
0\amp -\lambda\amp 1\amp 0\\
0\amp 1\amp -\lambda\amp 0\\
1\amp 0\amp 0\amp -\lambda \end{array} \right] \right)\\
\amp = \left(\lambda^2-1\right)^2\\
\amp = (\lambda+1)^2(\lambda-1)^2\text{.}
\end{align*}
%
\par
So the eigenvalues of \(A\) are \(1\) and \(-1\). The reduced row echelon forms of \(A-I_4\) and \(A+I_4\) are, respectively,%
\begin{equation*}
\left[ \begin{array}{ccrr} 1\amp 0\amp 0\amp -1 \\ 0\amp 1\amp -1\amp 0 \\0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0 \end{array}  \right] \ \text{ and }  \ \left[ \begin{array}{cccc} 1\amp 0\amp 0\amp 1 \\ 0\amp 1\amp 1\amp 0 \\0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
%
\par
Thus, a basis for the eigenspace \(E_{1}\) of \(A\) is \(\{[0 \ 1 \ 1 \ 0]^{\tr}, [1 \ 0 \ 0 \ 1]^{\tr}\}\) and a basis for the eigenspace \(E_{-1}\) of \(A\) is \(\{[0 \ 1 \ -1 \ 0]^{\tr}, [1 \ 0 \ 0 \ -1]^{\tr}\}\). The set \(\{[0 \ 1 \ 1 \ 0]^{\tr}, [1 \ 0 \ 0 \ 1]^{\tr}, [0 \ 1 \ -1 \ 0]^{\tr}, [1 \ 0 \ 0 \ -1]^{\tr}\}\) is an orthogonal set, so an orthonormal basis for \(\R^4\) consisting of eigenvectors of \(A\) is%
\begin{equation*}
\left\{\frac{1}{\sqrt{2}} [0 \ 1 \ 1 \ 0]^{\tr}, \frac{1}{\sqrt{2}}[1 \ 0 \ 0 \ 1]^{\tr}, \frac{1}{\sqrt{2}}[0 \ 1 \ -1 \ 0]^{\tr}, \frac{1}{\sqrt{2}}[1 \ 0 \ 0 \ -1]^{\tr}\right\}\text{.}
\end{equation*}
%
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_orthog_diag_summ}
%
\begin{itemize}[label=\textbullet]
\item{}An \(n \times n\) matrix \(A\) is orthogonally diagonalizable if there is an orthogonal matrix \(P\) such that \(P^{\tr}AP\) is a diagonal matrix. Orthogonal diagonalizability is useful in that it allows us to find a ``convenient'' coordinate system in which to interpret the results of certain matrix transformations. Orthogonal diagonalization also a plays a crucial role in the singular value decomposition of a matrix.%
\item{}An \(n \times n\) matrix \(A\) is symmetric if \(A^{\tr} = A\). The symmetric matrices are exactly the matrices that can be orthogonally diagonalized.%
\item{}The spectrum of a matrix is the set of eigenvalues of the matrix.%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_orthog_diag_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp21075352}%
For each of the following matrices, find an orthogonal matrix \(P\) so that \(P^{\tr}AP\) is a diagonal matrix, or explain why no such matrix exists.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(A = \left[ \begin{array}{rr} 3\amp -4 \\ -4\amp -3 \end{array} \right]\)%
\item{}\(A = \left[ \begin{array}{ccc} 4\amp 1\amp 1 \\ 1\amp 1\amp 4 \\ 1\amp 4\amp 1 \end{array} \right]\)%
\item{}\(A = \left[ \begin{array}{cccc} 1\amp 2\amp 0\amp 0 \\ 0\amp 1\amp 2\amp 1 \\ 1\amp 1\amp 1\amp 1 \\ 3\amp 0\amp 5\amp 2 \end{array} \right]\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp21074072}%
For each of the following matrices find an orthonormal basis of eigenvectors of \(A\). Then find a spectral decomposition of \(A\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(A = \left[ \begin{array}{rr} 3\amp -4 \\ -4\amp -3 \end{array} \right]\)%
\item{}\(A = \left[ \begin{array}{ccc} 4\amp 1\amp 1 \\ 1\amp 1\amp 4 \\ 1\amp 4\amp 1 \end{array} \right]\)%
\item{}\(A = \left[ \begin{array}{rrr} -4\amp 0\amp -24 \\ 0\amp -8\amp 0 \\ -24\amp 0\amp 16 \end{array} \right]\)%
\item{}\(A = \left[ \begin{array}{ccr} 1\amp 0\amp 0 \\ 0\amp 0\amp 2 \\ 0\amp 2\amp -3 \end{array} \right]\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp21081880}%
Find a non-diagonal \(4 \times 4\) matrix with eigenvalues 2, 3 and 6 which can be orthogonally diagonalized.%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{x:exercise:ex_7_a_product}%
Let \(A = [a_{ij}] = [ \vc_1 \ \vc_2 \ \cdots \ \vc_m]\) be an \(k \times m\) matrix with columns \(\vc_1\), \(\vc_2\), \(\ldots\), \(\vc_m\), and let \(B = [b_{ij}] = \left[ \begin{array}{c} \vr_1 \\ \vr_2 \\ \vdots \\ \vr_m \end{array}  \right]\) be an \(m \times n\) matrix  with rows \(\vr_1\), \(\vr_2\), \(\ldots\), \(\vr_m\). Show that%
\begin{equation*}
AB = [ \vc_1 \ \vc_2 \ \cdots \ \vc_m]\left[\begin{array}{c} \vr_1 \\ \vr_2 \\ \vdots \\ \vr_m \end{array}  \right] = \vc_1\vr_1 + \vc_2\vr_2 + \cdots + \vc_m \vr_m\text{.}
\end{equation*}
%
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{x:exercise:ex_7_a_spectral_decomposition}%
Let \(A\) be an \(n \times n\) symmetric matrix with real entries and let \(\{\vu_1, \vu_2, \ldots, \vu_n\}\) be an orthonormal basis of eigenvectors of \(A\). For each \(i\), let \(P_i = \vu_i\vu_i^{\tr}\). Prove \hyperref[x:theorem:thm_7_a_spectral_decomposition]{Theorem~{\xreffont\ref{x:theorem:thm_7_a_spectral_decomposition}}} \textemdash{} that is, verify each of the following statements.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}For each \(i\), \(P_i\) is a symmetric matrix.%
\item{}For each \(i\), \(P_i\) is a rank 1 matrix.%
\item{}For each \(i\), \(P_i^2 = P_i\).%
\item{}If \(i \neq j\), then \(P_iP_j = 0\).%
\item{}For each \(i\), \(P_i \vu_i = \vu_i\).%
\item{}If \(i \neq j\), then \(P_i \vu_j = 0\).%
\item{}If \(\vv\) is in \(\R^n\), show that%
\begin{equation*}
P_i \vv = \proj_{\Span\{\vu_i\} } \vv\text{.}
\end{equation*}
For this reason we call \(P_i\) an \terminology{orthogonal projection matrix}.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp21110552}%
Show that if \(M\) is an \(n \times n\) matrix and \((M\vx) \cdot \vy = \vx \cdot (M\vy)\) for every \(\vx, \vy\) in \(\R^n\), then \(M\) is a symmetric matrix.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21119128}{}\quad{}Try \(\vx = \textbf{e}_i\) and \(\vy = \textbf{e}_j\).%
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp21117464}%
Let \(A\) be an \(n \times n\) symmetric matrix and assume that \(A\) has an orthonormal basis \(\{\vu_1\), \(\vu_2\), \(\ldots\), \(\vu_n\}\) of eigenvectors of \(A\) so that \(A \vu_i = \lambda_i \vu_i\) for each \(i\). Let \(P_i = \vu_i\vu_i^{\tr}\) for each \(i\). It is possible that not all of the eigenvalue of \(A\) are distinct. In this case, some of the eigenvalues will be repeated in the spectral decomposition of \(A\). If we want only distinct eigenvalues to appear, we might do the following. Let \(\mu_1\), \(\mu_2\), \(\ldots\), \(\mu_k\) be the distinct eigenvalues of \(A\). For each \(j\) between 1 and \(k\), let \(Q_j\) be the sum of all of the \(P_i\) that have \(\mu_j\) as eigenvalue.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}The eigenvalues for the matrix \(A = \left[ \begin{array}{cccc} 0\amp 2\amp 0\amp 0 \\ 2\amp 3\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 2 \\ 0\amp 0\amp 2\amp 3 \end{array} \right]\) are \(-1\) and \(4\). Find a basis for each eigenspace and determine each \(P_i\). Then find \(k\), \(\mu_1\), \(\ldots\), \(\mu_k\), and each \(Q_j\).%
\item{}Show in general (not just for the specific example in part (a), that the \(Q_j\) satisfy the same properties as the \(P_i\). That is, verify the following.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}\(A = \mu_1 Q_1 + \mu_2 Q_2 + \cdots \mu_k Q_k\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21139352}{}\quad{}Collect matrices with the same eigenvalues.%
\item{}\(Q_j\) is a symmetric matrix for each \(j\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21140504}{}\quad{}Use the fact that each \(P_i\) is a symmetric matrix.%
\item{}\(Q_j^2 = Q_j\) for each \(j\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21138456}{}\quad{}Use Theorem 31.8.%
\item{}\(Q_j Q_{\ell} = 0\) when \(j \neq \ell\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21142552}{}\quad{}Use Theorem 31.8.%
\item{}if \(E_{\mu_j}\) is the eigenspace for \(A\) corresponding to the eigenvalue \(\mu_j\), and if \(\vv\) is in \(\R^n\), then \(Q_j \vv = \proj_{E_{\mu_j}} \vv\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21149592}{}\quad{}Explain why \(\{\vu_{1_j}\), \(\vu_{2_j}\), \(\ldots\), \(\vu_{m_j}\}\) is a orthonormal basis for \(E_{\mu_j}\).%
\end{enumerate}
\item{}What is the rank of \(Q_j\)? Verify your answer.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp21151000}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
Every real symmetric matrix is diagonalizable.%
\item{}\lititle{True\slash{}False.}\par%
If \(P\) is a matrix whose columns are eigenvectors of a symmetric matrix, then the columns of \(P\) are orthogonal.%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is a symmetric matrix, then eigenvectors of \(A\) corresponding to distinct eigenvalues are orthogonal.%
\item{}\lititle{True\slash{}False.}\par%
If \(\vv_1\) and \(\vv_2\) are distinct eigenvectors of a symmetric matrix \(A\), then \(\vv_1\) and \(\vv_2\) are orthogonal.%
\item{}\lititle{True\slash{}False.}\par%
Any symmetric matrix can be written as a sum of symmetric rank 1 matrices.%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is a matrix satisfying \(A^{\tr} = A\), and \(\vu\) and \(\vv\) are vectors satisfying \(A \vu = 2 \vu\) and \(A \vv = -2 \vv\), then \(\vu \cdot \vv = 0\).%
\item{}\lititle{True\slash{}False.}\par%
If an \(n\times n\) matrix \(A\) has \(n\) orthogonal eigenvectors, then \(A\) is a symmetric matrix.%
\item{}\lititle{True\slash{}False.}\par%
If an \(n\times n\) matrix has \(n\) real eigenvalues (counted with multiplicity), then \(A\) is a symmetric matrix.%
\item{}\lititle{True\slash{}False.}\par%
For each eigenvalue of a symmetric matrix, the algebraic multiplicity equals the geometric multiplicity.%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is invertible and orthogonally diagonalizable, then so is \(A^{-1}\).%
\item{}\lititle{True\slash{}False.}\par%
If \(A, B\) are orthogonally diagonalizable \(n\times n\) matrices, then so is \(AB\).%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: The Second Derivative Test for Functions of Two Variables}
\typeout{************************************************}
%
\begin{sectionptx}{Project: The Second Derivative Test for Functions of Two Variables}{}{Project: The Second Derivative Test for Functions of Two Variables}{}{}{x:section:sec_proj_two_var_deriv}
In this project we will verify the Second Derivative Test for functions of two variables.\footnote{Many thanks to Professor Paul Fishback for sharing his activity on this topic. Much of this project comes from his activity.\label{g:fn:idp21176856}} This test will involve Taylor polynomials and linear algebra. As a quick review, recall that the second order Taylor polynomial for a function \(f\) of a single variable \(x\) at \(x = a\) is%
\begin{equation}
P_2(x) = f(a)+f'(a)(x-a)+\frac{f''(a)}{2}(x-a)^2\text{.}\label{x:men:eq_Taylor_2}
\end{equation}
%
\par
As with the linearization of a function, the second order Taylor polynomial is a good approximation to \(f\) around \(a\) \textemdash{} that is \(f(x) \approx P_2(x)\) for \(x\) close to \(a\). If \(a\) is a critical number for \(f\) with \(f'(a) = 0\), then%
\begin{equation*}
P_2(x) = f(a) + \frac{f''(a)}{2}(x-a)^2\text{.}
\end{equation*}
%
\par
In this situation, if \(f''(a) \lt 0\), then \(\frac{f''(a)}{2}(x-a)^2 \leq 0\) for \(x\) close to \(a\), which makes \(P_2(x) \leq f(a)\). This implies that \(f(x) \approx P_2(x) \leq f(a)\) for \(x\) close to \(a\), which makes \(f(a)\) a relative maximum value for \(f\). Similarly, if \(f''(a) > 0\), then \(f(a)\) is a relative minimum.%
\par
We now need a Taylor polynomial for a function of two variables. The complication of the additional independent variable in the two variable case means that the Taylor polynomials will need to contain all of the possible monomials of the indicated degrees. Recall that the linearization (or tangent plane) to a function \(f = f(x,y)\) at a point \((a,b)\) is given by%
\begin{equation*}
P_1(x,y) = f(a,b) + f_x(a,b)(x-a) + f_y(a,b)(y-b)\text{.}
\end{equation*}
%
\par
Note that \(P_1(a,b) = f(a,b)\), \(\frac{\partial P_1}{\partial x}(a,b) = f_x(a,b)\), and \(\frac{\partial P_1}{\partial y}(a,b) = f_y(a,b)\). This makes \(P_1(x,y)\) the best linear approximation to \(f\) near the point \((a,b)\). The polynomial \(P_1(x,y)\) is the first order Taylor polynomial for \(f\) at \((a,b)\).%
\par
Similarly, the second order Taylor polynomial \(P_2(x,y)\) centered at the point \((a,b)\) for the function \(f\) is%
\begin{align*}
P_2(x,y) = f(a,b) \amp + f_x(a,b)(x-a) + f_y(a,b)(y-b) + \frac{f_{xx}(a,b)}{2}(x-a)^2\\
\amp + f_{xy}(a,b)(x-a)(y-b) + \frac{f_{yy}(a,b)}{2}(y-b)^2\text{.}
\end{align*}
%
\begin{project}{}{g:project:idp21196168}%
To see that \(P_2(x,y)\) is the best approximation for \(f\) near \((a,b)\), we need to know that the first and second order partial derivatives of \(P_2\) agree with the corresponding partial derivatives of \(f\) at the point \((a,b)\). Verify that this is true.%
\end{project}%
We can rewrite this second order Taylor polynomial using matrices and vectors so that we can apply techniques from linear algebra to analyze it. Note that%
\begin{align}
P_2(x,y) \amp = f(a,b) + \nabla f(a,b)^{\tr} \left[ \begin{array}{c} x-a\notag\\
y-b \end{array} \right]\notag\\
\amp \qquad + \frac{1}{2}\left[ \begin{array}{c} x-a\notag\\
y-b \end{array} \right]^{\tr} \left[ \begin{array}{cc} f_{xx}(a,b)\amp f_{xy}(a,b)\notag\\
f_{xy}(a,b)\amp  f_{yy}(a,b) \end{array} \right] \left[ \begin{array}{c} x-a\notag\\
y-b \end{array} \right]\text{,}\label{x:mrow:eq_Taylor_2_vector}
\end{align}
where \(\nabla f(x,y) = \left[ \begin{array}{c} f_x(x,y)\\f_y(x,y) \end{array}  \right]\) is the gradient of \(f\) and \(H\) is the \terminology{Hessian} of \(f\), where \(H(x,y) = \left[ \begin{array}{cc} f_{xx}(x,y)\amp f_{xy}(x,y) \\ f_{yx}(x,y)\amp  f_{yy}(x,y) \end{array}  \right]\).\footnote{Note that under reasonable conditions (e.g., that \(f\) has continuous second order mixed partial derivatives in some open neighborhood containing \((x,y)\)) we have that \(f_{xy}(x,y) = f_{yx}(x,y)\) and \(H(x,y) = \left[ \begin{array}{cc} f_{xx}(a,b)\amp f_{xy}(a,b) \\ f_{xy}(a,b)\amp  f_{yy}(a,b) \end{array}  \right]\) is a symmetric matrix. We will only consider functions that satisfy these reasonable conditions.\label{g:fn:idp21203592}}%
\begin{project}{}{x:project:ex_example_1}%
Use Equation \hyperref[x:mrow:eq_Taylor_2_vector]{({\xreffont\ref{x:mrow:eq_Taylor_2_vector}})} to compute \(P_2(x,y)\) for \(f(x,y)=x^4+y^4-4xy+1\) at \((a, b)=(2,3)\).%
\end{project}%
The important idea for us is that if \((a, b)\) is a point at which \(f_x\) and \(f_y\) are zero, then \(\nabla f\) is the zero vector and Equation \hyperref[x:mrow:eq_Taylor_2_vector]{({\xreffont\ref{x:mrow:eq_Taylor_2_vector}})} reduces to%
\begin{equation}
P_2(x,y) = f(a,b) +  \frac{1}{2}\left[ \begin{array}{c} x-a\\y-b \end{array}  \right]^{\tr} \left[ \begin{array}{cc} f_{xx}(a,b)\amp f_{xy}(a,b) \\ f_{xy}(a,b)\amp  f_{yy}(a,b) \end{array}  \right] \left[ \begin{array}{c} x-a\\y-b \end{array}  \right]\text{,}\label{x:men:eq_Taylor_2_vector_2}
\end{equation}
%
\par
To make the connection between the multivariable second derivative test and properties of the Hessian, \(H(a,b)\), at a critical point of a function \(f\) at which \(\nabla f = \vzero\), we will need to connect the eigenvalues of a matrix to the determinant and the trace.%
\par
Let \(A\) be an \(n \times n\) matrix with eigenvalues \(\lambda_1\), \(\lambda_2\), \(\ldots\), \(\lambda_n\) (not necessarily distinct). \hyperlink{x:exercise:ex_determinant_eigenvalues}{Exercise~{\xreffont 1}} in \hyperref[x:chapter:chap_characteristic_equation]{Section~{\xreffont\ref{x:chapter:chap_characteristic_equation}}} shows that%
\begin{equation}
\det(A) = \lambda_1 \lambda_2 \cdots \lambda_n\text{.}\label{x:men:eq_evals_det}
\end{equation}
%
\par
In other words, the determinant of a matrix is equal to the product of the eigenvalues of the matrix. In addition, \hyperlink{x:exercise:ex_trace_eigenvalues}{Exercise~{\xreffont 9}} in \hyperref[x:chapter:chap_diagonalization]{Section~{\xreffont\ref{x:chapter:chap_diagonalization}}} shows that%
\begin{equation}
\trace(A) = \lambda_1 +  \lambda_2 + \cdots + \lambda_n\text{.}\label{x:men:eq_evals_trace}
\end{equation}
for a diagonalizable matrix, where \(\trace(A)\) is the sum of the diagonal entries of \(A\). Equation \hyperref[x:men:eq_evals_trace]{({\xreffont\ref{x:men:eq_evals_trace}})} is true for any square matrix, but we don't need the more general result for this project.%
\par
The fact that the Hessian is a symmetric matrix makes it orthogonally diagonalizable. We denote the eigenvalues of \(H(a,b)\) as \(\lambda_1\) and \(\lambda_2\). Thus there exists an orthogonal matrix \(P\) and a diagonal matrix \(D = \left[ \begin{array}{cc} \lambda_1\amp 0 \\ 0\amp \lambda_2 \end{array}  \right]\) such that \(P^{\tr}H(a,b)P=D\), or \(H(a,b) = PDP^{\tr}\). Equations \hyperref[x:men:eq_evals_det]{({\xreffont\ref{x:men:eq_evals_det}})} and \hyperref[x:men:eq_evals_trace]{({\xreffont\ref{x:men:eq_evals_trace}})} show that%
\begin{equation*}
\lambda_1\lambda_2 = f_{xx}(a,b)f_{yy}(a,b)-f_{xy}(a,b)^2 \ \text{ and  }  \ \lambda_1 + \lambda_2 = f_{xx}(a,b) + f_{yy}(a,b)\text{.}
\end{equation*}
%
\par
Now we have the machinery to verify the Second Derivative Test for Two-Variable Functions. We assume \((a,b)\) is a point in the domain of a function \(f\) so that \(\nabla f(a,b) = \vzero\). First we consider the case where \(f_{xx}(a,b)f_{yy}(a,b)-f_{xy}(a,b)^2\lt 0\).%
\begin{project}{}{g:project:idp21233160}%
Explain why if \(f_{xx}(a,b)f_{yy}(a,b)-f_{xy}(a,b)^2\lt 0\), then%
\begin{equation*}
\left[ \begin{array}{c} x-a \\ y-b \end{array}  \right]^{\tr} H(a,b) \left[ \begin{array}{c} x-a \\ y-b \end{array}  \right]
\end{equation*}
is indefinite. Explain why this implies that \(f\) is ``saddle-shaped'' near \((a,b)\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21229064}{}\quad{}Substitute \(\vw = \left[ \begin{array}{c} w_1\\w_2 \end{array} \right] = P^{\tr}\left[ \begin{array}{c} x-a \\ y-b \end{array} \right]\). What does the graph of \(f\) look like in the \(w_1\) and \(w_2\) directions?%
\end{project}%
Now we examine the situation when \(f_{xx}(a,b)f_{yy}(a,b)-f_{xy}(a,b)^2>0\).%
\begin{project}{}{g:project:idp21228552}%
Assume that \(f_{xx}(a,b)f_{yy}(a,b)-f_{xy}(a,b)^2>0\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why either both \(f_{xx}(a,b)\) and \(f_{yy}(a,b)\) are positive or both are negative.%
\item{}If \(f_{xx}(a,b)>0\) and \(f_{yy}(a,b)>0\), explain why \(\lambda_1\) and \(\lambda_2\) must be positive.%
\item{}Explain why, if \(f_{xx}(a,b)>0\) and \(f_{yy}(a,b)>0\), then \(f(a,b)\) is a local minimum value for \(f\).%
\end{enumerate}
\end{project}%
When \(f_{xx}(a,b)f_{yy}(a,b)-f_{xy}(a,b)^2>0\) and either \(f_{xx}(a,b)\) or \(f_{yy}(a,b)\) is negative, a slight modification of the preceding argument leads to the fact that \(f\) has a local maximum at \((a,b)\) (the details are left to the reader). Therefore, we have proved the Second Derivative Test for functions of two variables!%
\begin{project}{}{g:project:idp21235976}%
Use the Hessian to classify the local maxima, minima, and saddle points of \(f(x,y)=x^4+y^4-4xy+1\). Draw a graph of \(f\) to illustrate.%
\end{project}%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 28 Quadratic Forms and the Principal Axis Theorem}
\typeout{************************************************}
%
\begin{chapterptx}{Quadratic Forms and the Principal Axis Theorem}{}{Quadratic Forms and the Principal Axis Theorem}{}{}{x:chapter:chap_principal_axis_theorem}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp21248904}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What is a quadratic form on \(\R^n\)?%
\item{}What does the Principal Axis Theorem tell us about quadratic forms?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: The Tennis Racket Effect}
\typeout{************************************************}
%
\begin{sectionptx}{Application: The Tennis Racket Effect}{}{Application: The Tennis Racket Effect}{}{}{x:section:sec_appl_tennis}
Try an experiment with a tennis racket (or a squash racket, or a ping pong paddle). Let us define a 3D coordinate system with the center of the racket as the origin with the head of the racket lying in the \(xy\)-plane. We let \(\vu_1\) be the vector in the direction of the handle and \(\vu_2\) the perpendicular direction (still lying in the plane defined by the head) as illustrated in \hyperref[x:figure:F_tennis_racket]{Figure~{\xreffont\ref{x:figure:F_tennis_racket}}}. We then let \(\vu_3\) be a vector perpendicular to the plane of the head. Hold the racket by the handle and spin it to make one rotation around the \(\vu_1\) axis. This is pretty easy. It is also not difficult to throw the racket so that it rotates around the \(\vu_3\). Now toss the racket into the air to make one complete rotation around the axis of the vector \(\vu_2\) and catch the handle. Repeat this several times. You should notice that in most instances, the racket will also have made a half rotation around the \(\vu_1\) axis so that the other face of the racket now points up. This is quite different than the rotations around the \(\vu_1\) and \(\vu_3\) axes. A good video that illustrates this behavior can be seen at \href{https://www.youtube.com/watch?v=4dqCQqI-Gis}{\nolinkurl{youtube.com/watch?v=4dqCQqI-Gis}}.%
\begin{figureptx}{Two principal axes of a tennis racket.}{x:figure:F_tennis_racket}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/7_b_tennis_racket.pdf}
\end{image}%
\tcblower
\end{figureptx}%
This effect is a result in classical mechanics that describes the rotational movement of a rigid body in space, called the \terminology{tennis racket effect} (or the Dzhanibekov effect, after the Russian cosmonaut Vladimir Dzhanibekov who discovered the theorem's consequences while in zero gravity in space \textemdash{} you can see an illustration of this in the video at \href{https://www.youtube.com/watch?v=L2o9eBl_Gzw}{\nolinkurl{youtube.com/watch?v=L2o9eBl_Gzw}}). The result is simple to see in practice, but is difficult to intuitively understand why the behavior is different around the intermediate axis. There is a story of a student who asked the famous physicist Richard Feynman if there is any intuitive way to understand the result; Feynman supposedly went into deep thought for about 10 or 15 seconds and answered, ``no.'' As we will see later in this section, we can understand this effect using the principal axes of a rigid body.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_pat_intro}
We are familiar with quadratic equations in algebra. Examples of quadratic equations include \(x^2=1\), \(x^2+y^2=1\), and \(x^2+xy+y^2=3\). We don't, however, have to restrict ourselves to two variables. A quadratic equation in \(n\) variables is any equation in which the sum of the exponents in any monomial term is 2. So a quadratic equation in the variables \(x_1\), \(x_2\), \(\ldots\), \(x_n\) is an equation in the form%
\begin{align*}
a_{11}x_1^2\amp +a_{22}x_2^2 + a_{33}x_3^2 + \cdots + a_{nn}x_n^2\\
\amp + a_{12}x_1x_2 + a_{13}x_1x_3 + \cdots a_{1n}x_1x_n\\
\amp + a_{23}x_2x_3 + a_{24}x_2x_4 + \cdots a_{2n}x_2x_n + \cdots\\
\amp + a_{n-1n}x_{n-1}x_n\\
\amp = c
\end{align*}
for some constant \(c\). In matrix notation this expression on the left of this equation has the form%
\begin{equation*}
\sum_{i=1}^{n} \sum_{j=1}^n a_{ij}x_ix_j = \vx^{\tr} A \vx
\end{equation*}
where \(\vx = \left[ \begin{array}{c} x_1 \\ x_2 \\ \vdots \\ x_n \end{array}  \right]\) and \(A\) is the \(n \times n\) matrix \(A = [a_{ij}]\). For example, if \(A= \left[ \begin{array}{rcr} 1\amp 3\amp -2 \\ -1\amp 1\amp 2 \\ 0\amp 2\amp -2 \end{array}  \right]\), then we get the quadratic expression \(x_1^2+3x_1x_2-2x_1x_3-x_2x_1+x_2^2+2x_2x_3+2x_3x_2-2x_3^2\). We should note here that the terms involving \(x_ix_j\) and \(x_jx_i\) are repeated in our sum, but%
\begin{equation*}
a_{ij}x_ix_j + a_{ji}x_jx_i = 2\left(\frac{a_{ij} + a_{ji}}{2}\right)x_ix_j
\end{equation*}
and so we could replace \(a_{ij}\) and \(a_{ji}\) both with \(\left(\frac{a_{ij} + a_{ji}}{2}\right)\) without changing the quadratic form. With this alteration in mind, we can then assume that \(A\) is a symmetric matrix. So in the previous example, the symmetric matrix \(A'= \left[ \begin{array}{rcr} 1\amp 1\amp -1 \\ 1\amp 1\amp 2 \\ -1\amp 2\amp -2 \end{array} \right]\) gives the same quadratic expression. This leads to the following definition.%
\begin{definition}{}{g:definition:idp21269256}%
\index{quadratic form}%
A \terminology{quadratic form} on \(\R^n\) is a function \(Q\) defined by%
\begin{equation*}
Q(\vx) = \vx^{\tr} A \vx
\end{equation*}
for some \(n \times n\) symmetric matrix \(A\).%
\end{definition}
As we show in \hyperlink{x:exercise:ex_7_b_unique_A}{Exercise~{\xreffont 7}}, the symmetric matrix \(A\) is unique to the quadratic form, so we call the symmetric matrix \(A\) is the \terminology{matrix of the quadratic form}. It is these quadratic forms that we will study in this section.%
\begin{exploration}{}{x:exploration:pa_7_b}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}To get a little more comfortable with quadratic forms, write the quadratic forms in matrix form, explicitly identifying the vector \(\vx\) and the symmetric matrix \(A\) of the quadratic form.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}\(3x_1^2-2x_2^2 +4x_1x_2+x_2x_3\)%
\item{}\(x_1x_4 + 4x_2x_3 - x_2^2 + 10x_1x_5\)%
\end{enumerate}
\item{}Some quadratic forms form equations in \(\R^2\) that are very familiar: \(x^2+y^2=1\) is an equation of a circle, \(2x^2+3y^2=2\) is an equation of an ellipse, and \(x^2-y^2=1\) is an equation of a hyperbola. Of course, these do not represent all of the quadratic forms in \(\R^2\) \textemdash{} some contain cross-product terms. We can recognize the equations above because they contain no cross-product terms (terms involving \(xy\)). We can more easily recognize the quadratic forms that contain cross-product terms if  we can somehow rewrite the forms in a different format with no cross-product terms. We illustrate how this can be done with the quadratic form \(Q\) defined by \(Q(\vx) = x^2-xy+y^2\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Write \(Q(\vx)\) in the form \(\vx^{\tr} A \vx\), where \(A\) is a \(2 \times 2\) symmetric matrix.%
\item{}Since \(A\) is a symmetric matrix we can orthogonally diagonalize \(A\). Given that the eigenvalues of \(A\) are \(\frac{3}{2}\) and \(\frac{1}{2}\) with corresponding eigenvectors \(\left[ \begin{array}{r} -1 \\ 1 \end{array} \right]\) and \(\left[ \begin{array}{c} 1 \\ 1 \end{array} \right]\), respectively, find a matrix \(P\) that orthogonally diagonalizes \(A\).%
\item{}Define \(\vy = \left[ \begin{array}{c} w \\ z \end{array} \right]\) to satisfy \(\vx = P\vy\). Substitute for \(\vx\) in the quadratic form \(Q(\vx)\) to write the quadratic form in terms of \(w\) and \(z\). What kind of graph does the quadratic equation \(Q(\vx) = 1\) have?%
\end{enumerate}
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Equations Involving Quadratic Forms in \(\R^2\)}
\typeout{************************************************}
%
\begin{sectionptx}{Equations Involving Quadratic Forms in \(\R^2\)}{}{Equations Involving Quadratic Forms in \(\R^2\)}{}{}{x:section:sec_eqs_quad_r2}
When we consider equations of the form \(Q(\vx) = d\), where \(Q\) is a quadratic form in \(\R^2\) and \(d\) is a constant, we wind up with old friends like \(x^2+y^2=1\), \(2x^2+3y^2=2\), or \(x^2-y^2=1\). As we saw in \hyperref[x:exploration:pa_7_b]{Preview Activity~{\xreffont\ref{x:exploration:pa_7_b}}} these equations are relatively easy to recognize. However, when we have cross-product terms, like in \(x^2-xy+y^2=1\), it is not so easy to identify the curve the equation represents. If there was a way to eliminate the cross-product term \(xy\) from this form, we might be more easily able to recognize its graph. The discussion in this section will focus on quadratic forms in \(\R^2\), but we will see later that the arguments work in any number of dimensions. While working in \(\R^2\) we will use the standard variables \(x\) and \(y\) instead of \(x_1\) and \(x_2\).%
\par
In general, the equation of the form \(Q(\vx) = d\), where \(Q\) is a quadratic form in \(\R^2\) defined by a matrix \(A = \left[ \begin{array}{cc} a\amp b/2\\b/2\amp c \end{array}  \right]\) and \(d\) is a constant looks like%
\begin{equation*}
ax^2+bxy+cy^2 = d\text{.}
\end{equation*}
%
\par
The graph of an equation like this is either an ellipse (a circle is a special case of an ellipse), a hyperbola, two non-intersecting lines, a point, or the empty set (see \hyperlink{x:exercise:ex_7_b_QF_characterization}{Exercise~{\xreffont 5}}). The quadratic forms do not involve linear terms, so we don't consider the cases of parabolas. One way to see into which category one of these quadratic form equations falls is to write the equation in standard form.%
\par
The standard forms for quadratic equations in \(\R^2\) are as follows, where \(a\) and \(b\) are nonzero constants and \(h\) and \(k\) are any constants.%
\begin{descriptionlist}
\begin{dlimedium}{Lines:}{g:li:idp21304328}%
\(\ds ax^2 = 1\) or \(\ds ay^2=1\) (\(a > 0\))%
\end{dlimedium}%
\begin{dlimedium}{Ellipse:}{g:li:idp21307784}%
\(\displaystyle \ds \frac{(x-h)^2}{a^2} + \frac{(y-k)^2}{b^2} = 1\)%
\end{dlimedium}%
\begin{dlimedium}{Hyperbola:}{g:li:idp21308168}%
\(\ds \frac{(x-h)^2}{a^2} - \frac{(y-k)^2}{b^2} = 1\) or \(\ds \frac{(y-k)^2}{b^2} - \frac{(x-h)^2}{a^2} = 1\)%
\end{dlimedium}%
\end{descriptionlist}
%
\par
\hyperref[x:exploration:pa_7_b]{Preview Activity~{\xreffont\ref{x:exploration:pa_7_b}}} contains the main tool that we need to convert a quadratic form into one of these standard forms. By this we mean that if we have a quadratic form \(Q\) in the variables \(x_1\), \(x_2\), \(\ldots\), \(x_n\), we want to find variables \(y_1\), \(y_2\), \(\ldots\), \(y_n\) in terms of \(x_1\), \(x_2\), \(\ldots\), \(x_n\) so that when written in terms of the variables \(y_1\), \(y_2\), \(\ldots\), \(y_n\) the quadratic form \(Q\) contains no cross terms. In other words, we want to find a vector \(\vy = \left[ \begin{array}{c} y_1 \\ y_2 \\ \vdots \\ y_n \end{array} \right]\) so that \(Q(\vx) = \vy^{\tr} D \vy\), where \(D\) is a diagonal matrix. Since every real symmetric matrix is orthogonally diagonalizable, we will always be able to find a matrix \(P\) that orthogonally diagonalizes \(A\). The details are as follows.%
\par
Let \(Q\) be the quadratic form defined by \(Q(\vx) = \vx^{\tr} A \vx\), where \(A\) is an \(n \times n\) symmetric matrix. As in \hyperref[x:exploration:pa_7_b]{Preview Activity~{\xreffont\ref{x:exploration:pa_7_b}}}, the fact that \(A\) is symmetric means that we can find an orthogonal matrix \(P = [ \vp_1 \  \vp_2 \  \vp_3 \  \cdots \  \vp_n ]\) whose columns are orthonormal eigenvectors of \(A\) corresponding to eigenvalues \(\lambda_1\), \(\lambda_2\), \(\ldots\), \(\lambda_n\), respectively. Letting \(\vy = P^{\tr} \vx\) give us \(\vx = P\vy\) and%
\begin{equation*}
\vx^{\tr} A \vx = (P\vy)^{\tr}A(P\vy) = \vy^{\tr}(P^{\tr}AP)\vy = \vy^{\tr} D \vy\text{,}
\end{equation*}
where \(D\) is the diagonal matrix whose \(i\)th diagonal entry is \(\lambda_i\).%
\par
Moreover, the set \(\B = \{\vp_1, \vp_2, \cdots, \vp_n\}\) is an orthonormal basis for \(\R^n\) and so defines a coordinate system for \(\R^n\). Note that if \(y = [y_1 \ y_2 \ \cdots \ y_n]^{\tr}\), then%
\begin{equation*}
\vx = P\vy = y_1\vp_1 + y_2 \vp_2 + \cdots + y_n \vp_n\text{.}
\end{equation*}
%
\par
Thus, the coordinate vector of \(\vx\) with respect to \(\B\) is \(\vy\), or \([\vx]_{\B} = \vy\). We summarize in \hyperref[x:theorem:thm_7_b_Principal_Axis_Theorem]{Theorem~{\xreffont\ref{x:theorem:thm_7_b_Principal_Axis_Theorem}}}.%
\begin{theorem}{Principal Axis Theorem.}{}{x:theorem:thm_7_b_Principal_Axis_Theorem}%
Let \(A\) be an \(n \times n\) symmetric matrix. There is an orthogonal change of variables \(\vx = P\vy\) so that the quadratic form \(Q\) defined by \(Q(\vx) = \vx^{\tr} A \vx\) is transformed into the quadratic form \(\vy^{\tr} D \vy\) where \(D\) is a diagonal matrix.%
\end{theorem}
The columns of the orthogonal matrix \(P\) in the Principal Axis Theorem form an orthogonal basis for \(\R^n\) and are called the \emph{principal axes} for the quadratic form \(Q\). Also, the coordinate vector of \(\vx\) with respect to this basis is \(\vy\).%
\begin{activity}{}{g:activity:idp21337864}%
Let \(Q\) be the quadratic form defined by \(Q(\vx) = 2x^2 + 4xy + 5y^2 = \vx^{\tr} A \vx\), where \(\vx = \left[ \begin{array}{c} x \\ y \end{array} \right]\) and \(A = \left[ \begin{array}{cc}2\amp 2\\2\amp 5 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}The eigenvalues of \(A\) are \(\lambda_1 = 6\) and \(\lambda_2=1\) with corresponding eigenvectors \(\vv_1 = [1 \ 2]^{\tr}\) and \(\vv_2 = [-2 \ 1]^{\tr}\), respectively. Find an orthogonal matrix \(P\) with determinant 1 that diagonalizes \(A\). Is \(P\) unique? Explain. Is there a matrix without determinant 1 that orthogonally diagonalizes \(A\)? Explain.%
\item{}Use the matrix \(P\) to write the quadratic form without the cross-product.%
\item{}We can view \(P\) as a change of basis matrix from the coordinate system defined by \(\vy = P^{\tr} \vx\) to the standard coordinate system. In other words, in the standard \(xy\) coordinate system, the quadratic form is written as \(\vx^{\tr}A \vx\), but in the new coordinate system defined by \(\vy\) the quadratic form is written as \((P\vy)^{\tr}A(P\vy)\). As a change of basis matrix, \(P\) performs a rotation. See if you can recall what we learned about rotation matrices and determine the angle of rotation \(P\) defines. Plot the graph of the quadratic equation \(Q(\vx) = 1\) in the new coordinate system and identify this angle on the graph. Interpret the result.%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Classifying Quadratic Forms}
\typeout{************************************************}
%
\begin{sectionptx}{Classifying Quadratic Forms}{}{Classifying Quadratic Forms}{}{}{x:section:sec_class_quad_forms}
If we draw graphs of equations of the type \(z=Q(\vx)\), where \(Q\) is a quadratic form, we can see that a quadratic form whose matrix does not have 0 as an eigenvalue can take on all positive values (except at \(\vx=\vzero\)) as shown at left in \hyperref[x:figure:F_7_b_Paraboloids]{Figure~{\xreffont\ref{x:figure:F_7_b_Paraboloids}}}, all negative values (except at \(\vx=\vzero\)) as shown in the center of \hyperref[x:figure:F_7_b_Paraboloids]{Figure~{\xreffont\ref{x:figure:F_7_b_Paraboloids}}}, or both positive and negative values as depicted at right in \hyperref[x:figure:F_7_b_Paraboloids]{Figure~{\xreffont\ref{x:figure:F_7_b_Paraboloids}}}. We can see when these cases happen by analyzing the eigenvalues of the matrix that defines the quadratic form. Let \(A\) be a \(2 \times 2\) symmetric matrix with eigenvalues \(\lambda_1\) and \(\lambda_2\), and let \(P\) be a matrix that orthogonally diagonalizes \(A\) so that \(P^{\tr}AP = D = \left[ \begin{array}{cc} \lambda_1 \amp  0 \\ 0 \amp  \lambda_2 \end{array}  \right]\). If we let \(\vy = \left[ \begin{array}{c} w\\z \end{array}  \right] = P^{\tr}\vx\), then%
\begin{align*}
Q(\vx) \amp = \vx^{\tr}A\vx\\
\amp = \vy^{\tr} D \vy\\
\amp = \lambda_1 w^2 + \lambda_2 z^2\text{.}
\end{align*}
%
\par
Then \(Q(\vx) \geq 0\) if all of the eigenvalues of \(A\) are positive (with \(Q(\vx) > 0\) when \(\vx \neq \vzero\)) and \(Q(\vx) \leq 0\) if all of the eigenvalues of \(A\) are negative (with \(Q(\vx) \lt 0\) when \(\vx \neq \vzero\)). If one eigenvalue of \(A\) is positive and the other negative, then \(Q(\vx)\) will take on both positive and negative values. As a result, we classify symmetric matrices (and their corresponding quadratic forms) according to these behaviors.%
\begin{figureptx}{Left: Paraboloid \(Q(\vx)=x^2+y^2\). Center: Hyperbolic Paraboloid \(Q(\vx)=x^2-y^2\). Right: Paraboloid \(Q(\vx) = -x^2 - y^2\).}{x:figure:F_7_b_Paraboloids}{}%
\begin{sidebyside}{3}{0}{0}{0}%
\begin{sbspanel}{0.333333333333333}%
\includegraphics[width=\linewidth]{external/7_b_Paraboloid.pdf}
\end{sbspanel}%
\begin{sbspanel}{0.333333333333333}%
\includegraphics[width=\linewidth]{external/7_b_Hyperbolic_Paraboloid.pdf}
\end{sbspanel}%
\begin{sbspanel}{0.333333333333333}%
\includegraphics[width=\linewidth]{external/7_b_Paraboloid2.pdf}
\end{sbspanel}%
\end{sidebyside}%
\tcblower
\end{figureptx}%
\begin{definition}{}{x:definition:def_7_4_definite}%
\index{matrix!positive definite}%
\index{matrix!positive semidefinite}%
\index{matrix!negative definite}%
\index{matrix!negative semidefinite}%
\index{matrix!indefinite}%
A symmetric matrix \(A\) (and its associated quadratic form \(Q\)) is%
\begin{itemize}[label=\textbullet]
\item{}\terminology{positive definite} if \(\vx^{\tr}A\vx > 0\) for all \(\vx \neq \vzero\),%
\item{}\terminology{positive semidefinite} if \(\vx^{\tr}A\vx \geq 0\) for all \(\vx\),%
\item{}\terminology{negative definite} if \(\vx^{\tr}A\vx \lt 0\) for all \(\vx \neq \vzero\),%
\item{}\terminology{negative semidefinite} if \(\vx^{\tr}A\vx \leq 0\) for all \(\vx\),%
\item{}\terminology{indefinite} if \(\vx^{\tr}A\vx\) takes on both positive and negative values.%
\end{itemize}
%
\end{definition}
For example, the quadratic form \(Q(\vx) = x^2+y^2\) at left in \hyperref[x:figure:F_7_b_Paraboloids]{Figure~{\xreffont\ref{x:figure:F_7_b_Paraboloids}}} is positive definite (with repeated eigenvalue 1), the quadratic form \(Q(\vx) = -(x^2+y^2)\) at right in \hyperref[x:figure:F_7_b_Paraboloids]{Figure~{\xreffont\ref{x:figure:F_7_b_Paraboloids}}} is negative definite (repeated eigenvalue \(-1\)), and the hyperbolic paraboloid \(Q(\vx) = x^2-y^2\) in the center of \hyperref[x:figure:F_7_b_Paraboloids]{Figure~{\xreffont\ref{x:figure:F_7_b_Paraboloids}}} is indefinite (eigenvalues \(1\) and \(-1\)).%
\par
So we have argued that a quadratic form \(Q(\vx) = \vx^{\tr} A \vx\) is positive definite if \(A\) has all positive eigenvalues, negative definite if \(A\) has all negative eigenvalues, and indefinite if \(A\) has both positive and negative eigenvalues. Similarly, the quadratic form is positive semidefinite if \(A\) has all nonnegative eigenvalues and negative semidefinite if \(A\) has all nonpositive eigenvalues. Positive definite matrices are important, as we discuss in the next section.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Inner Products}
\typeout{************************************************}
%
\begin{sectionptx}{Inner Products}{}{Inner Products}{}{}{x:section:sec_pat_inner_prod}
We used the dot product to define lengths of vectors, to measure angles between vectors, and to define orthogonality in \(\R^n\). We can generalize the notion of orthogonality by using different types of products called \terminology{inner products} that behave like the dot product.%
\begin{exploration}{}{x:exploration:pa_7_b_inner_product}%
Define a mapping from \(\R^2 \times \R^2\) to \(\R\) by%
\begin{equation*}
\langle \vu, \vv \rangle = u_1v_1 + 2u_2v_2
\end{equation*}
for \(\vu = [u_1 \ u_2]^{\tr}\) and \(\vv = [v_1 \ v_2]^{\tr}\) in \(\R^2\). (The brackets \(\langle \ \rangle\) provide a shorthand way of representing the function.)%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Calculate \(\langle [1 \ 2]^{\tr}, [3,-4]^{\tr} \rangle\).%
\item{}If \(\vu\) and \(\vv\) are in \(\R^2\), is it true that%
\begin{equation*}
\langle \vu , \vv \rangle = \langle \vv , \vu \rangle?
\end{equation*}
Verify your answer.%
\item{}If \(\vu\), \(\vv\), and \(\vw\) are in \(\R^2\), is it true that%
\begin{equation*}
\langle \vu + \vv , \vw \rangle = \langle \vu , \vw \rangle + \langle \vv , \vw \rangle?
\end{equation*}
Verify your answer.%
\item{}If \(\vu\) and \(\vv\) are in \(\R^2\) and \(c\) is a scalar, is it true that%
\begin{equation*}
\langle c\vu , \vv \rangle = c\langle \vu , \vv \rangle?
\end{equation*}
Verify your answer.%
\item{}If \(\vu\) and \(\vv\) are in \(\R^2\), must it be the case that \(\langle \vu , \vu \rangle \geq 0\)? When is \(\langle \vu , \vu \rangle = 0\)?%
\item{}There is a matrix \(A\) such that \(\langle \vu, \vv \rangle = \vu^{\tr} A \vv\). Find this matrix \(A\).%
\end{enumerate}
\end{exploration}%
\hyperref[x:exploration:pa_7_b_inner_product]{Preview Activity~{\xreffont\ref{x:exploration:pa_7_b_inner_product}}} illustrates that there are functions from \(\R^n \times \R^n\) to \(\R\) other than the dot product that satisfy many of the properties in \hyperref[x:theorem:thm_6_a_dot_product]{Theorem~{\xreffont\ref{x:theorem:thm_6_a_dot_product}}}. Such functions allow us to broaden our ideas of what right angles look like. These functions are called \terminology{inner products}.%
\begin{definition}{}{x:definition:def_7_b_inner_product}%
\index{inner product}%
An \terminology{inner product} \(\langle \ , \ \rangle\) on \(\R^n\) is a mapping from \(\R^n \times \R^n \to \R\) satisfying%
\begin{enumerate}
\item{}\(\langle \vu , \vv \rangle = \langle \vv , \vu \rangle\) for all \(\vu\) and \(\vv\) in \(\R^n\),%
\item{}\(\langle \vu + \vv , \vw \rangle = \langle \vu , \vw \rangle + \langle \vv , \vw \rangle\) for all \(\vu\), \(\vv\), and \(\vw\) in \(\R^n\),%
\item{}\(\langle c\vu , \vv \rangle = c\langle \vu , \vv \rangle\) for all \(\vu\), \(\vv\) in \(\R^n\) and all scalars \(c\),%
\item{}\(\langle \vu , \vu \rangle \geq 0\) for all \(\vu\) in \(\R^n\) and \(\langle \vu , \vu \rangle = 0\) if and only if \(\vu = \vzero\).%
\end{enumerate}
%
\end{definition}
The dot product and the example in \hyperref[x:exploration:pa_7_b_inner_product]{Preview Activity~{\xreffont\ref{x:exploration:pa_7_b_inner_product}}} provide two examples of inner products. The examples below provide two other important inner products on \(\R^n\).%
\begin{itemize}[label=\textbullet]
\item{}If \(a_1\), \(a_2\), \(\ldots\), \(a_n\) are positive scalars, then%
\begin{equation*}
\langle [u_1 \ u_2 \ \cdots \ u_n]^{\tr},  [v_1 \ v_2 \ \cdots \ v_n]^{\tr} \rangle = a_1u_1v_1+a_2u_2v_2+ \cdots + a_nu_nv_n
\end{equation*}
defines an inner product on \(\R^n\).%
\item{}Every invertible \(n \times n\) matrix \(A\) defines an inner product on \(\R^n\) by%
\begin{equation*}
\langle \vu, \vv \rangle = (A\vu) \cdot (A\vv)\text{.}
\end{equation*}
%
\end{itemize}
%
\par
As \hyperlink{x:exercise:ex_7_b_PD_inner_product}{Exercise~{\xreffont 8}} will demonstrate, every inner product on \(\R^n\) can be written in the form \(\langle \vu, \vv \rangle = \vu^{\tr} A \vv\) for some special type of matrix \(A\).%
\begin{activity}{}{g:activity:idp21431176}%
Let \(A\) be a symmetric \(n \times n\) matrix, and define \(\langle \ , \ \rangle : \R^n \times \R^n \to \R\) by%
\begin{equation}
\langle \vu, \vv \rangle = \vu^{\tr}A\vv\text{.}\label{x:men:eq_7_b_PD_inner_product}
\end{equation}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why it is necessary for \(A\) to be positive definite in order for \hyperref[x:men:eq_7_b_PD_inner_product]{({\xreffont\ref{x:men:eq_7_b_PD_inner_product}})} to define an inner product on \(\R^n\).%
\item{}Show that \hyperref[x:men:eq_7_b_PD_inner_product]{({\xreffont\ref{x:men:eq_7_b_PD_inner_product}})} defines an inner product on \(\R^n\) if \(A\) is positive definite.%
\item{}Let \(\langle \ , \ \rangle\) be the mapping from \(\R^2\times \R^2\to \R\) defined by%
\begin{equation*}
\left\langle \left[ \begin{array}{c} x_1 \\ x_2 \end{array}  \right], \left[ \begin{array}{c} y_1 \\ y_2 \end{array}  \right] \right\rangle = 2x_1y_1 - x_1y_2 - x_2y_1 + x_2y_2\text{.}
\end{equation*}
Find a matrix \(A\) so that \(\langle \vx, \vy \rangle = \vx^{\tr} A \vy\) and explain why \(\langle \ , \ \rangle\) defines an inner product.%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_pat_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp21438856}%
Write the given quadratic equation in a system in which it has no cross-product terms.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(8x^2-4xy+5y^2 = 1\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp21435784}{}\quad{}We write the quadratic form \(Q(x,y)=8x^2-4xy+5y^2\) as \(\vx^{\tr} A \vx\), where \(\vx = \left[ \begin{array}{c} x\\y \end{array}  \right]\) and \(A = \left[  \begin{array}{rr} 8\amp -2\\-2\amp 5 \end{array}  \right]\). The eigenvalues for \(A\) are \(9\) and \(4\), and bases for the corresponding eigenspaces \(E_9\) and \(E_{4}\) are \(\{[-2 \ 1]^{\tr}\}\) and \(\{[1 \ 2]^{\tr}\}\), respectively. An orthogonal matrix \(P\) that orthogonally diagonalizes \(A\) is%
\begin{equation*}
P = \left[ \begin{array}{rc} -\frac{2}{\sqrt{5}}\amp \frac{1}{\sqrt{5}} \\ \frac{1}{\sqrt{5}}\amp \frac{2}{\sqrt{5}} \end{array}  \right]\text{.}
\end{equation*}
If \(\vy =  [u \ v]^{\tr}\) and we let \(\vx = P\vy\), then we can rewrite the quadratic equation \(8x^2-4xy+5y^2 = 1\) as%
\begin{align*}
8x^2-4xy+5y^2 \amp = 1\\
\vx^{\tr}A\vx \amp = 1\\
(P\vy)^{\tr}A(P\vy) \amp = 1\\
\vy^{\tr} P^{\tr}AP \vy \amp = 1\\
\vy^{\tr} \left[ \begin{array}{cc} 9\amp 0\\
0\amp 4 \end{array} \right] \vy \amp = 1\\
9u^2+4v^2 \amp = 1\text{.}
\end{align*}
So the quadratic equation \(8x^2-4xy+5y^2=1\) is an ellipse.%
\item{}\(x^2+4xy+y^2=1\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp21440648}{}\quad{}We write the quadratic form \(Q(x,y)=x^2+4xy+y^2\) as \(\vx^{\tr} A \vx\), where \(\vx = \left[ \begin{array}{c} x\\y \end{array}  \right]\) and \(A = \left[  \begin{array}{cc} 1\amp 2\\2\amp 1 \end{array}  \right]\). The eigenvalues for \(A\) are \(3\) and \(-1\), and bases for the corresponding eigenspaces \(E_3\) and \(E_{-1}\) are \(\{[1 \ 1]^{\tr}\}\) and \(\{[-1 \ 1]^{\tr}\}\), respectively. An orthogonal matrix \(P\) that orthogonally diagonalizes \(A\) is%
\begin{equation*}
P = \left[ \begin{array}{rc} \frac{1}{\sqrt{2}}\amp \frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}}\amp \frac{1}{\sqrt{2}} \end{array}  \right]\text{.}
\end{equation*}
If \(\vy =  [u \ v]^{\tr}\) and we let \(\vx = P\vy\), then we can rewrite the quadratic equation \(x^2+4xy+y^2 = 1\) as%
\begin{align*}
x^2+4xy+y^2 \amp = 1\\
\vx^{\tr}A\vx \amp = 1\\
(P\vy)^{\tr}A(P\vy) \amp = 1\\
\vy^{\tr} P^{\tr}AP \vy \amp = 1\\
\vy^{\tr} \left[ \begin{array}{cr} 3\amp 0\\
0\amp -1 \end{array} \right] \vy \amp = 1\\
3u^2-v^2 \amp = 1\text{.}
\end{align*}
So the quadratic equation \(x^2+4xy+y^2=1\) is a hyperbola.%
\item{}\(4x^2+4y^2+4z^2+4xy+4xz+4yz-3=0\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp21460504}{}\quad{}We write the quadratic form \(Q(x,y,z)=4x^2+4y^2+4z^2+4xy+4xz+4yz\) as \(\vx^{\tr} A \vx\), where \(\vx = \left[ \begin{array}{c} x\\y\\z \end{array}  \right]\) and \(A = \left[ \begin{array}{ccc} 4\amp 2\amp 2\\2\amp 4\amp 2\\2\amp 2\amp 4 \end{array}  \right]\). The eigenvalues for \(A\) are \(2\) and \(8\), and bases for the corresponding eigenspaces \(E_2\) and \(E_8\) are \(\{[-1 \ 0 \ 1]^{\tr}, [-1 \ 1\ 0]^{\tr}\}\) and \(\{[1 \ 1\ 1]^{\tr}\}\), respectively. Applying the Gram-Schmidt process to the basis for \(E_2\) gives us an orthogonal basis \(\{\vw_1, \vw_2\}\) of \(E_2\), where \(\vw_1 = [-1 \ 0 \ 1]^{\tr}\) and%
\begin{align*}
\vw_2 \amp =  [-1 \ 1 \ ]^{\tr} - \frac{ [-1 \ 0 \ 1]^{\tr} \cdot [-1 \ 1 \ 0]^{\tr}}{[-1 \ 0 \ 1]^{\tr} \cdot [-1 \ 0 \ 1]^{\tr}} [-1 \ 0 \ 1]^{\tr}\\
\amp =  [-1 \ 1 \ 0]^{\tr} - \frac{1}{2}[-1 \ 0 \ 1]^{\tr}\\
\amp = \frac{1}{2}[ -1 \ 2 \ -1]^{\tr}\text{.}
\end{align*}
An orthogonal matrix \(P\) that orthogonally diagonalizes \(A\) is%
\begin{equation*}
P = \left[ \begin{array}{rrc} -\frac{1}{\sqrt{2}}\amp -\frac{1}{\sqrt{6}}\amp \frac{1}{\sqrt{3}} \\ 0\amp \frac{2}{\sqrt{6}}\amp \frac{1}{\sqrt{3}} \\ \frac{1}{\sqrt{2}}\amp -\frac{1}{\sqrt{6}}\amp \frac{1}{\sqrt{3}} \end{array}  \right]\text{.}
\end{equation*}
If \(\vy =  [u \ v \ w]^{\tr}\) and we let \(\vx = P\vy\), then we can rewrite the quadratic equation \(4x^2+4y^2+4z^2+4xy+4xz+4yz = 3\) as%
\begin{align*}
4x^2+4y^2+4z^2+4xy+4xz+4yz \amp = 3\\
\vx^{\tr}A\vx \amp = 3\\
(P\vy)^{\tr}A(P\vy) \amp = 3\\
\vy^{\tr} P^{\tr}AP \vy \amp = 3\\
\vy^{\tr} \left[ \begin{array}{ccc} 2\amp 0\amp 0\\
0\amp 2\amp 0\\
0\amp 0\amp 8 \end{array} \right] \vy \amp = 3\\
2u^2+2v^2+8w^2 \amp = 3\text{.}
\end{align*}
So the quadratic equation \(4x^2+4y^2+4z^2+4xy+4xz+4yz-3 = 0\) is a ellipsoid.%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp21465496}%
Let \(A\) and \(B\) be positive definite matrices, and let \(C = \left[ \begin{array}{rr}5\amp -3\\-3\amp 3 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Must \(A\) be invertible? Justify your answer.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp21476504}{}\quad{}Since \(A\) has all positive eigenvalues and \(\det(A)\) is the product of the eigenvalues of \(A\), then \(\det(A) > 0\). Thus, \(A\) is invertible.%
\item{}Must \(A^{-1}\) be positive definite? Justify your answer.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp21477016}{}\quad{}The fact that \(A\) is positive definite means that \(A\) is also symmetric. Recall that \(\left(A^{-1}\right)^{\tr} = \left(A^{\tr}\right)^{-1}\). Since \(A\) is symmetric, it follows that \(\left(A^{-1}\right)^{\tr} = A^{-1}\) and \(A^{-1}\) is symmetric. The eigenvalues of \(A^{-1}\) are the reciprocals of the eigenvalues of \(A\). Since the eigenvalues of \(A\) are all positive, so are the eigenvalues of \(A^{-1}\). Thus, \(A^{-1}\) is positive definite.%
\item{}Must \(A^{2}\) be positive definite? Justify your answer.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp21481624}{}\quad{}Notice that%
\begin{equation*}
\left(A^2\right)^{\tr} = (AA)^{\tr} = A^{\tr}A^{\tr} = A^2\text{,}
\end{equation*}
so \(A^2\) is symmetric. The eigenvalues of \(A^2\) are the squares of the eigenvalues of \(A\). Since no eigenvalue of \(A\) is \(0\), the eigenvalues of \(A^2\) are all positive and \(A^2\) is positive definite.%
\item{}Must \(A+B\) be positive definite? Justify your answer.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp21483928}{}\quad{}We know that \(B\) is symmetric, and%
\begin{equation*}
(A+B)^{\tr} = A^{\tr} + B^{\tr} = A+B\text{,}
\end{equation*}
so \(A+B\) is symmetric. Also, the fact that \(\vx^{\tr}A\vx > 0\) and \(\vx^{\tr}B\vx > 0\) for all \(\vx\) implies that%
\begin{equation*}
\vx^{\tr}(A+B)\vx = \vx^{\tr}A\vx + \vx^{\tr}B\vx > 0
\end{equation*}
for all \(\vx\). Thus, \(A+B\) is positive definite.%
\item{}Is \(C\) positive definite? Justify your answer.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp21495704}{}\quad{}The matrix \(C\) is symmetric and%
\begin{equation*}
\det(C - \lambda I_2) = (5-\lambda)(3-\lambda) - 9 = \lambda^2-8\lambda+6\text{.}
\end{equation*}
So the eigenvalues of \(C\) are \(4 + \sqrt{10}\) and \(4 - \sqrt{10} \approx 0.8\). Since the eigenvalues of \(C\) are both positive, \(C\) is positive definite.%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_pat_summ}
%
\begin{itemize}[label=\textbullet]
\item{}A quadratic form on \(\R^n\) is a function \(Q\) defined by%
\begin{equation*}
Q(\vx) = \vx^{\tr} A \vx
\end{equation*}
for some \(n \times n\) symmetric matrix \(A\).%
\item{}The Principal Axis Theorem tells us that there is a change of variable \(\vx = P\vy\) that will remove the cross-product terms from a quadratic form and allow us to identify the form and determine the principal axes for the form.%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_pat_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp21502616}%
Find the matrix for each quadratic form.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(x_1^2 - 2x_1x_2 + 4x_2^2\) if \(\vx\) is in \(\R^2\)%
\item{}\(10x_1^2 + 4x_1x_3 + 2x_2x_3 + x_3^2\) if \(\vx\) is in \(\R^3\)%
\item{}\(2x_1x_2 + 2x_1x_3 - x_1x_4 + 5x_2^2 + 4x_3x_4 + 8x_4^2\) if \(\vx\) is in \(\R^4\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp21507096}%
For each quadratic form, identify the matrix \(A\) of the form, find a matrix \(P\) that orthogonally diagonalizes \(A\), and make a change of variable that transforms the quadratic form into one with no cross-product terms.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(x_1^2+2x_1x_2+x_2^2\)%
\item{}\(-2x_1^2+2x_1x_2+4x_1x_3-2x_2^2-4x_2x_3-x_3^2\)%
\item{}\(11x_1^2-12x_1x_2-12x_1x_3-12x_1x_4-x_2^2-2x_3x_4\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{x:exercise:ex_7_b_constrained_optimization}%
One topic in multivariable calculus is constrained optimization. We can use the techniques of this section to solve certain types of constrained optimization problems involving quadratic forms. As an example, we will find the maximum and minimum values of the quadratic form defined by the matrix \(\left[ \begin{array}{cc} 2\amp 1\\1\amp 2 \end{array}  \right]\) on the unit circle.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}First we determine some bounds on the values of a quadratic form. Let \(Q\) be the quadratic form defined by the \(n \times n\) real symmetric matrix \(A\). Let \(\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n\) be the eigenvalues of \(A\), and let \(P\) be a matrix that orthogonally diagonalizes \(A\), with \(P^{\tr}AP = D\) as the matrix with diagonal entries \(\lambda_1\), \(\lambda_2\), \(\ldots\), \(\lambda_n\) in order. Let \(\vy = [y_1 \ y_2 \  \cdots \ y_n]^{\tr} = P^{\tr} [x_1 \ x_2 \ \cdots \ x_n]^{\tr}\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Show that%
\begin{equation*}
Q(\vx) = \lambda_1 y_1^2 + \lambda_2 y_2^2 + \cdots + \lambda_n y_n^2\text{.}
\end{equation*}
%
\item{}Use the fact that \(\lambda_1 \geq \lambda_i\) for each \(i\) and the fact that \(P\) (and \(P^{\tr}\)) is an orthogonal matrix to show that%
\begin{equation*}
Q(\vx) \leq  \lambda_1 ||\vx||\text{.}
\end{equation*}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21524248}{}\quad{}Substitute in part i.%
\item{}Now show that \(Q(\vx) \geq \lambda_n ||\vx||\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21526424}{}\quad{}Make an argument similar to part ii.%
\end{enumerate}
\item{}Use the result of part (a) to find the maximum and minimum values of the quadratic form defined by the matrix \(\left[ \begin{array}{cc} 2\amp 1\\1\amp 2 \end{array} \right]\) on the unit circle.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp21537944}%
In this exercise we characterize the symmetric, positive definite, \(2 \times 2\) matrices with real entries in terms of the entries of the matrices. Let \(A = \left[ \begin{array}{cc} a\amp b/2\\b/2\amp c \end{array} \right]\) for some real numbers \(a\), \(b\), and \(c\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Assume that \(A\) is positive definite.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Show that \(a\) must be positive.%
\item{}Use the fact that the eigenvalues of \(A\) must be positive to show that \(ac > b^2\). We conclude that if \(A\) is positive definite, then \(a> 0\) and \(ac > b^2\).%
\end{enumerate}
\item{}Now show that if \(a > 0\) and \(ac > b^2\), then \(A\) is positive definite. This will complete our classification of positive definite \(2 \times 2\) matrices.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{x:exercise:ex_7_b_QF_characterization}%
In this exercise we determine the form of%
\begin{equation}
\vx^{\tr}A\vx=1\text{,}\label{x:men:eq_7_b_QF_characterization}
\end{equation}
where \(A\) is a symmetric \(2 \times 2\) matrix. Let \(P\) be a matrix that orthogonally diagonalizes \(A\) and let \(\vy = P^{\tr}\vx\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Substitute \(\vy\) for \(P^{\tr}\vx\) in the equation \(\vx^{\tr}A\vx = 1\). What form does the resulting equation have (write this form in terms of the eigenvalues of \(A\))?%
\item{}What kind of graph does the equation \hyperref[x:men:eq_7_b_QF_characterization]{({\xreffont\ref{x:men:eq_7_b_QF_characterization}})} have if \(A\) is positive definite? Why?%
\item{}What kind of graph does the equation \hyperref[x:men:eq_7_b_QF_characterization]{({\xreffont\ref{x:men:eq_7_b_QF_characterization}})} have if \(A\) has both positive and negative eigenvalues? Why?%
\item{}What kind of graph does the equation \hyperref[x:men:eq_7_b_QF_characterization]{({\xreffont\ref{x:men:eq_7_b_QF_characterization}})} have if one eigenvalue of \(A\) is zero and the other non-zero? Why?%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{x:exercise:ex_7_b_aij}%
Let \(A = [a_{ij}]\) be a symmetric \(n \times n\) matrix.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(\ve_i^{\tr}A\ve_j = a_{ij}\), where \(\ve_i\) is the \(i\)th standard unit vector for \(\R^n\). (This result will be useful in \hyperlink{x:exercise:ex_7_b_unique_A}{Exercise~{\xreffont 7}})%
\item{}Let \(\vu\) be a unit eigenvector of \(A\) with eigenvalue \(\lambda\). Find \(\vu^{\tr}A\vu\) in terms of \(\lambda\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{x:exercise:ex_7_b_unique_A}%
Suppose \(A\) and \(B\) are symmetric \(n \times n\) matrices, and let \(Q_A(\vx) = \vx^{\tr}A\vx\) and \(Q_B(\vx) = \vx^{\tr} B\vx\). If \(Q_A(\vx) = Q_B(\vx)\) for all \(\vx\) in \(\R^n\), show that \(A=B\). (Hint: Use \hyperlink{x:exercise:ex_7_b_aij}{Exercise~{\xreffont 6}} (a) to compare \(Q_A(\ve_i)\) and \(Q_B(\ve_i)\), then compare \(Q_A(\ve_i+\ve_j)\) to \(Q_B(\ve_i + \ve_j)\) for \(i \neq j\).) Thus, quadratic forms are uniquely determined by their symmetric matrices.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21570456}{}\quad{}Use the previous exercise to compare \(Q_A(\ve_i)\) and \(Q_B(\ve_i)\), then compare \(Q_A(\ve_i+\ve_j)\) to \(Q_B(\ve_i + \ve_j)\) for \(i \neq j\).%
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{x:exercise:ex_7_b_PD_inner_product}%
In this exercise we analyze all inner products on \(\R^n\). Let \(\langle \ , \ \rangle\) be an inner product on \(\R^n\). Let \(\vx = [x_1 \ x_2 \ \ldots \ x_n]^{\tr}\) and \(\vy = [y_1 \ y_2 \ \ldots \ y_n]^{\tr}\) be arbitrary vectors in \(\R^n\). Then%
\begin{equation*}
\vx = \sum_{i=1}^n x_i \ve_i \ \ \ \text{ and }  \ \ \ \vy = \sum_{j=1}^n y_j \ve_j\text{,}
\end{equation*}
where \(\ve_i\) is the \(i\)th standard vector in \(\R^n\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why%
\begin{equation}
\langle \vx, \vy \rangle = \sum_{i=1}^n \sum_{j=1}^n x_i \langle \ve_i, \ve_j \rangle y_j\text{.}\label{x:men:eq_7_b_PD_inner_product_1}
\end{equation}
%
\item{}Calculate the matrix product%
\begin{equation*}
\vx^{\tr} \left[ \begin{array}{cccc} \langle \ve_1,\ve_1 \rangle \amp  \langle \ve_1,\ve_2 \rangle \amp  \cdots \amp  \langle \ve_1,\ve_n \rangle \\ \langle \ve_2,\ve_1 \rangle \amp  \langle \ve_2,\ve_2 \rangle \amp  \cdots \amp  \langle \ve_2,\ve_n \rangle \\ \vdots                      \amp   \vdots                       \amp        \amp  \vdots \\ \langle \ve_n,\ve_1 \rangle \amp  \langle \ve_n,\ve_2 \rangle \amp  \cdots \amp  \langle \ve_n,\ve_n \rangle \end{array}  \right] \vy
\end{equation*}
and compare to \hyperref[x:men:eq_7_b_PD_inner_product_1]{({\xreffont\ref{x:men:eq_7_b_PD_inner_product_1}})}. What do you notice?%
\item{}Explain why any inner product on \(\R^n\) is of the form \(\vx^{\tr} A \vy\) for some symmetric, positive definite matrix \(A\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{x:exercise:ex_7_b_dot_product}%
\hyperlink{x:exercise:ex_7_b_PD_inner_product}{Exercise~{\xreffont 8}} shows that any inner product \(\langle \, \rangle\) on \(\R^n\) has the form \(\langle \vu, \vv \rangle = \vu^{\tr} A \vv\) for some symmetric, positive definite matrix \(A\). Find the matrix \(A\) for which \(\vu \cdot \vv = \vu^{\tr} A \vv\) for all \(\vu\) and \(\vv\) in \(\R^n\).%
\end{divisionexercise}%
\begin{divisionexercise}{10}{}{}{g:exercise:idp21586200}%
Let \(\langle \ , \ \rangle\) be an inner product on \(\R^n\), let \(\vu\), \(\vv\), and \(\vw\) be vectors \(\R^n\), and let \(c\) be a scalar. Verify the following properties.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\langle \vzero , \vv \rangle = \langle \vv , \vzero \rangle = 0\)%
\item{}\(\langle \vu , c\vv \rangle = c\langle \vu , \vv \rangle\)%
\item{}\(\langle \vv+\vw , \vu \rangle = \langle \vv , \vu \rangle + \langle \vw , \vu \rangle\)%
\item{}\(\langle \vu - \vv, \vw \rangle = \langle \vw, \vu - \vv \rangle= \langle \vu , \vw \rangle - \langle \vv , \vw \rangle= \langle \vw, \vu \rangle - \langle \vw, \vv \rangle\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{11}{}{}{x:exercise:ex_7_b_Pyth_Thm}%
\index{Pythagorean Theorem in \(\R^n\) with respect to an inner product}%
We extend the notions of length and orthogonality in \(\R^n\) with respect to an inner product as follows. If \(\langle \, \rangle\) is an inner product on \(\R^n\), we define the length of a vector \(\vu\) with respect to the inner product to be \(||\vu|| = \sqrt{\langle \vu, \vu \rangle}\). We can also define two vectors \(\vu\) and \(\vv\) to be orthogonal if \(\langle \vu, \vv \rangle = 0\). In this exercise we verify the Pythagorean Theorem with respect to an inner product.%
\par
The Pythagorean Theorem states that if \(a\) and \(b\) are the lengths of the legs of a right triangle whose hypotenuse has length \(c\), then \(a^2+b^2=c^2\). If we think of the legs as defining vectors \(\vu\) and \(\vv\), then the hypotenuse is the vector \(\vu+\vv\) and we can restate the Pythagorean Theorem as%
\begin{equation*}
||\vu+\vv||^2 = ||\vu||^2+||\vv||^2\text{.}
\end{equation*}
In this exercise we show that this result holds in any dimension and for any inner product. Use an arbitrary inner product \(\langle \ , \ \rangle\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(\vu\) and \(\vv\) be orthogonal vectors in \(\R^n\). Show that \(||\vu+\vv||^2 = ||\vu||^2+||\vv||^2\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21601432}{}\quad{}Expand \(||\vu+\vv||^2\) using the dot product.%
\item{}Must it be true that if \(\vu\) and \(\vv\) are vectors in \(\R^n\) with \(||\vu+\vv||^2 = ||\vu||^2+||\vv||^2\), then \(\vu\) and \(\vv\) are orthogonal? If not, provide a counterexample. If true, verify the statement.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21601816}{}\quad{}Expand \(||\vu+\vv||^2\) using the dot product.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{12}{}{}{x:exercise:ex_7_b_Cauchy_Schwarz}%
\index{Cauchy-Schwarz inequality!in \(\R^n\) with respect to an inner product}%
The Cauchy-Schwarz inequality,%
\begin{equation}
|\langle \vu, \vv \rangle| \leq ||\vu|| \ \||\vv||\label{x:men:eq_7_b_Cauchy_Schwartz}
\end{equation}
for any vectors \(\vu\) and \(\vv\) in \(\R^n\), is considered one of the most important inequalities in mathematics. We verify the Cauchy-Schwarz inequality for an arbitrary inner product \(\langle \ , \ \rangle\) in this exercise. Let \(\vu\) and \(\vv\) be vectors in \(\R^n\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why the inequality \hyperref[x:men:eq_7_b_Cauchy_Schwartz]{({\xreffont\ref{x:men:eq_7_b_Cauchy_Schwartz}})} is true if either \(\vu\) or \(\vv\) is the zero vector. As a consequence, we assume that \(\vu\) and \(\vv\) are nonzero vectors for the remainder of this exercise.%
\item{}Let \(\vw = \proj_{\vv} \vu = \frac{\langle \vu, \vv \rangle }{||\vv||^2} \vv\) and let \(\vz = \vu - \vw\). We know that \(\langle \vw, \vz \rangle = 0\). Use \hyperlink{x:exercise:ex_7_b_Pyth_Thm}{Exercise~{\xreffont 11}} of this section to show that%
\begin{equation*}
||\vu||^2 \geq ||\vw||^2\text{.}
\end{equation*}
%
\item{}Now show that \(||\vw||^2 = \frac{|\langle \vu, \vv \rangle^2}{||\vv||^2}\).%
\item{}Combine parts (b) and (c) to explain why equation \hyperref[x:men:eq_7_b_Cauchy_Schwartz]{({\xreffont\ref{x:men:eq_7_b_Cauchy_Schwartz}})} is true.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{13}{}{}{g:exercise:idp21619352}%
\index{triangle inequality in \(\R^n\) with respect to an inner product}%
Let \(\vu\) and \(\vv\) be vectors in \(\R^n\). Then \(\vu\), \(\vv\) and \(\vu+\vv\) form a triangle. We should then expect that the length of any one side of the triangle is smaller than the sum of the lengths of the other sides (since the straight line distance is the shortest distance between two points). In other words, we expect that%
\begin{equation}
||\vu + \vv|| \leq ||\vu|| + ||\vv||\text{.}\label{x:men:eq_7_b_triangle_inequality}
\end{equation}
Equation \hyperref[x:men:eq_7_b_triangle_inequality]{({\xreffont\ref{x:men:eq_7_b_triangle_inequality}})} is called the \terminology{Triangle Inequality}. Use the Cauchy-Schwarz inequality (\hyperlink{x:exercise:ex_7_b_Cauchy_Schwarz}{Exercise~{\xreffont 12}}) to prove the triangle inequality for any inner product on \(\R^n\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21624088}{}\quad{}Expand \(||\vu+\vv||^2\) using the dot product.%
\end{divisionexercise}%
\begin{divisionexercise}{14}{}{}{g:exercise:idp21625368}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
If \(Q\) is a quadratic form, then there is exactly one matrix \(A\) such that \(Q(\vx) = \vx^{\tr}A\vx\).%
\item{}\lititle{True\slash{}False.}\par%
The matrix of a quadratic form is unique.%
\item{}\lititle{True\slash{}False.}\par%
If the matrix of a quadratic form is a diagonal matrix, then the quadratic form has no cross-product terms.%
\item{}\lititle{True\slash{}False.}\par%
The eigenvectors of the symmetric matrix \(A\) form the principal axes of the quadratic form \(\vx^{\tr}A\vx\).%
\item{}\lititle{True\slash{}False.}\par%
The principal axes of a quadratic form are orthogonal.%
\item{}\lititle{True\slash{}False.}\par%
If \(a\) and \(c\) are positive, then the quadratic equation \(ax^2 + bxy + cy^2 = 1\) defines an ellipse.%
\item{}\lititle{True\slash{}False.}\par%
If the entries of a symmetric matrix \(A\) are all positive, then the quadratic form \(\vx^{\tr}A\vx\) is positive definite.%
\item{}\lititle{True\slash{}False.}\par%
If a quadratic form \(\vx^{\tr}A \vx\) defined by a symmetric matrix \(A\) is positive definite, then the entries of \(A\) are all non-negative.%
\item{}\lititle{True\slash{}False.}\par%
If a quadratic form \(Q(\vx)\) on \(\R^2\) is positive definite, then the graph of \(z=Q(\vx)\) is a paraboloid opening upward.%
\item{}\lititle{True\slash{}False.}\par%
If a quadratic form \(Q(\vx)\) on \(\R^2\) is negative definite, then the graph of \(z=Q(\vx)\) is a paraboloid opening downward.%
\item{}\lititle{True\slash{}False.}\par%
If a quadratic form \(Q(\vx)\) on \(\R^2\) is indefinite, then there is a nonzero vector \(\vx\) such that \(Q(\vx) = 0\).%
\item{}\lititle{True\slash{}False.}\par%
If \(Q(\vx)\) is positive definite, then so is the quadratic form \(aQ(\vx)\) for \(a>0\).%
\item{}\lititle{True\slash{}False.}\par%
If If \(Q(\vx)=\vx^\tr A\vx\) is indefinite, then at least one of the eigenvalues of \(A\) is negative and at least one positive.%
\item{}\lititle{True\slash{}False.}\par%
If \(n \times n\) symmetric matrices \(A\) and \(B\) define positive definite quadratic forms, then so does \(A+B\).%
\item{}\lititle{True\slash{}False.}\par%
If an invertible symmetric matrix \(A\) defines a positive definite quadratic form, then so does \(A^{-1}\).%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: The Tennis Racket Theorem}
\typeout{************************************************}
%
\begin{sectionptx}{Project: The Tennis Racket Theorem}{}{Project: The Tennis Racket Theorem}{}{}{x:section:sec_proj_tennis}
\index{moment of inertia}%
If a particle of mass \(m\) and velocity \(v\) is moving in a straight line, its kinetic energy \(KE\) is given by \(KE = \frac{1}{2}mv^2\). If, instead, the particle rotates around an axis with angular velocity \(\omega\) (in radians per unit of time), its linear velocity is \(v = r \omega\), where \(r\) is the radius of the particle's circular path. Substituting into the kinetic energy formula shows that the kinetic energy of the rotating particle is then \(KE = \frac{1}{2}\left(mr^2\right) \omega^2\). The quantity \(mr^2\) is called the \terminology{moment of inertia} of the particle and is denoted by \(I\). So \(KE = \frac{1}{2}I\omega^2\) for a rotating particle. Notice that the larger the value of \(r\), the larger the inertia. You can imagine this with a figure skater. When a skater spins along their major axis with their arms outstretched, the speed at which they rotate is lower than when they bring their arms into their bodies. The moment of inertia for rotational motion plays a role similar to the mass in linear motion. Essentially, the inertia tells us how resistant the particle is to rotation.%
\par
To understand the tennis racket effect, we are interested in rigid bodies as they move through space. Any rigid body in three space has three principal axes about which it likes to spin. These axes are at right angles to each other and pass through the center of mass. Think of enclosing the object in an ellipsoid \textemdash{} the longest axis is the \terminology{primary} axis, the middle axis is the \terminology{intermediate} axis, and the third axis is the third axis. As a rigid body moves through space, it rotates around these axes and there is inertia along each axis. Just like with a tennis racket, if you were to imagine an axle along any of the principal axes and spin the object along that axel, it will either rotate happily with no odd behavior like flipping, or it won't. The former behavior is that of a stable axis and the latter an unstable axis. The Tennis Racket Theorem is a statement about the rotation of the body. Essentially, the Tennis Racket Theorem states that the rotation of a rigid object around its primary and third principal axes is stable, while rotation around its intermediate axis is not. To understand why this is so, we need to return to moments of inertia.%
\par
Assume that we have a rigid body moving through space. Euler's (rotation) equation describes the rotation of a rigid body with respect to the body's principal axes of inertia. Assume that \(I_1\), \(I_2\), and \(I_3\) are the moments of inertia around the primary, intermediate, and third principal axes with \(I_1 > I_2 > I_3\). Also assume that \(\omega_1\), \(\omega_2\), and \(\omega_3\) are the components of the angular velocity along each axis. When there is no torque applied, using a principal orthogonal coordinates, Euler's equation tells us that%
\begin{align}
I_1 \dot{\omega}_1 \amp = (I_2-I_3) \omega_2 \omega_3\label{x:mrow:eq_Euler_1}\\
I_2 \dot{\omega}_2 \amp = (I_3-I_1) \omega_3 \omega_1\label{x:mrow:eq_Euler_2}\\
I_3 \dot{\omega}_3 \amp = (I_1-I_2) \omega_1 \omega_2\text{.}\label{x:mrow:eq_Euler_3}
\end{align}
%
\par
(The dots indicate a derivative with respect to time, which is common notation in physics.) We will use Euler's equations to understand the Tennis Racket Theorem.%
\begin{project}{}{x:project:act_TRT_axes1}%
To start, we consider rotation around the first principal axis. Our goal is to show that rotation around this axis is stable. That is, small perturbations in angular velocity will have only small effects on the rotation of the object. So we assume that \(\omega_2\) and \(\omega_3\) are small. In general, the product of two small quantities will be much smaller, so \hyperref[x:mrow:eq_Euler_1]{({\xreffont\ref{x:mrow:eq_Euler_1}})} implies that \(\dot{\omega}_1\) must be very small. So we can disregard \(\dot{\omega}_1\) in our calculations.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Differentiate \hyperref[x:mrow:eq_Euler_2]{({\xreffont\ref{x:mrow:eq_Euler_2}})} with respect to time to explain why%
\begin{equation*}
I_2 \ddot{\omega}_2 \approx (I_3-I_1)  \dot{\omega}_3 \omega_1\text{.}
\end{equation*}
%
\item{}Substitute for \(\dot{\omega}_3\) from \hyperref[x:mrow:eq_Euler_3]{({\xreffont\ref{x:mrow:eq_Euler_3}})} to show that \(\omega_2\) is an approximate solution to%
\begin{equation}
\ddot{\omega}_2 = -k \omega_2\label{x:men:eq_TRT_de_1}
\end{equation}
for some positive constant \(k\).%
\item{}The equation \hyperref[x:men:eq_TRT_de_1]{({\xreffont\ref{x:men:eq_TRT_de_1}})} is a differential equation because it is an equation that involves derivatives of a function. Show by differentiating twice that, if%
\begin{equation}
\omega_2 = A\cos\left(\sqrt{k}t + B\right)\label{x:men:eq_TRT_de_2}
\end{equation}
(where \(A\) and \(B\) are any scalars), then \(\omega_2\) is a solution to \hyperref[x:men:eq_TRT_de_1]{({\xreffont\ref{x:men:eq_TRT_de_1}})}. (In fact, \(\omega_2\) is the general solution to \hyperref[x:men:eq_TRT_de_1]{({\xreffont\ref{x:men:eq_TRT_de_1}})}, which is verified in just about any course in differential equations.)%
\end{enumerate}
\end{project}%
Equation \hyperref[x:men:eq_TRT_de_2]{({\xreffont\ref{x:men:eq_TRT_de_2}})} shows that \(\omega_2\) is is bounded, so that any slight perturbations in angular velocity have a limited effect on \(\omega_2\). A similar argument can be made for \(\omega_3\). This implies that the rotation around the principal axes is stable \textemdash{} slight changes in angular velocity have limited effects on the rotations around the other axes.%
\par
We can make a similar argument for rotation around the third principal axes.%
\begin{project}{}{x:project:act_TRT_axes3}%
In this activity, repeat the process from Project Activity to show that rotation around the third principal axis is stable. So assume that \(\omega_1\) and \(\omega_3\) are small, which implies by \hyperref[x:mrow:eq_Euler_3]{({\xreffont\ref{x:mrow:eq_Euler_3}})} implies that \(\dot{\omega}_3\) must be very small and can be disregarded in calculations.%
\end{project}%
Now the issue is why is rotation around the second principal axis different.%
\begin{project}{}{x:project:act_TRT_axes2}%
Now assume that \(\omega_1\) and \(\omega_3\) are small. Thus, \(\dot{\omega}_2\) is very small by \hyperref[x:mrow:eq_Euler_2]{({\xreffont\ref{x:mrow:eq_Euler_2}})}, and we consider \(\dot{\omega}_2\) to be negligible.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Differentiate  \hyperref[x:mrow:eq_Euler_1]{({\xreffont\ref{x:mrow:eq_Euler_1}})} to show that%
\begin{equation*}
I_1 \ddot{\omega}_1 \approx (I_2-I_3)  \omega_2 \dot{\omega}_3\text{.}
\end{equation*}
%
\item{}Substitute for \(\dot{\omega}_3\) from \hyperref[x:mrow:eq_Euler_3]{({\xreffont\ref{x:mrow:eq_Euler_3}})} to show that \(\omega_1\) is an approximate solution to%
\begin{equation}
\ddot{\omega}_1 = k \omega_1\label{x:men:eq_TRT_de_3}
\end{equation}
for some positive scalar \(k\).%
\item{}The fact that  the constant multiplier in \hyperref[x:men:eq_TRT_de_3]{({\xreffont\ref{x:men:eq_TRT_de_3}})} is positive instead of negative as in \hyperref[x:men:eq_TRT_de_1]{({\xreffont\ref{x:men:eq_TRT_de_1}})} completely changes the type of solution. Show that%
\begin{equation}
\omega_1 = Ae^{\sqrt{k}t+B}\label{x:men:eq_TRT_de_4}
\end{equation}
(where \(A\) and \(B\) are any scalars) is a solution to \hyperref[x:men:eq_TRT_de_3]{({\xreffont\ref{x:men:eq_TRT_de_3}})} (and, in fact, is the general solution). Explain why this shows that rotation around the second principal axis is not stable.%
\end{enumerate}
\end{project}%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 29 The Singular Value Decomposition}
\typeout{************************************************}
%
\begin{chapterptx}{The Singular Value Decomposition}{}{The Singular Value Decomposition}{}{}{x:chapter:chap_SVD}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp21689880}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What is the operator norm of a matrix and what does it tell us about the matrix?%
\item{}What is a singular value decomposition of a matrix? Why is a singular value decomposition important?%
\item{}How does a singular value decomposition relate fundamental subspaces connected to a matrix?%
\item{}What is an outer product decomposition of a matrix and how is it useful?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Search Engines and Semantics}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Search Engines and Semantics}{}{Application: Search Engines and Semantics}{}{}{x:section:sec_appl_search_engn}
\index{Latent Semantic Indexing}%
Effective search engines search for more than just words. Language is complex and search engines must deal with the fact that there are often many ways to express a given concept (this is called \terminology{synonymy}, that multiple words can have the same meaning), and that a single word can have multiple meanings (\terminology{polysemy}). As a consequence, a search on a word may provide irrelevant matches (e.g., searching for \emph{derivative} could provide pages on mathematics or financial securities) or you might search for articles on \emph{cats} but the paper you really want uses the word \emph{felines}. A better search engine will not necessarily try to match terms, but instead retrieve information based on concept or intent. Latent Semantic Indexing (LSI) (or \terminology{Latent Semantic Analysis}), developed in the late 1980s, helps search engines determine concept and intent in order to provide more accurate and relevant results. LSI essentially works by providing underlying (latent) relationships between words (semantics) that search engines need to provide context and understanding (indexing). LSI provides a mapping of both words and documents into a lower dimensional ``concept'' space, and makes the search in this new space. The mapping is provided by the singular value decomposition.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_svd_intro}
The singular value decomposition (SVD) of a matrix is an important and useful matrix decomposition. Unlike other matrix decompositions, \emph{every} matrix has a singular value decomposition. The SVD is used in a variety of applications including scientific computing, digital signal processing, image compression, principal component analysis, web searching through latent semantic indexing, and seismology. Recall that the eigenvector decomposition of an \(n \times n\) diagonalizable matrix \(M\) has the form \(P^{-1}MP\), where the columns of the matrix \(P\) are \(n\) linearly independent eigenvectors of \(M\) and the diagonal entries of the diagonal matrix \(P^{-1}MP\) are the eigenvalues of \(M\). The singular value decomposition does something similar for any matrix of any size. One of the keys to the SVD is that the matrix \(A^{\tr}A\) is symmetric for any matrix \(A\).%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Operator Norm of a Matrix}
\typeout{************************************************}
%
\begin{sectionptx}{The Operator Norm of a Matrix}{}{The Operator Norm of a Matrix}{}{}{x:section:sec_mtx_op_norm}
Before we introduce the Singular Value Decomposition, let us work through some preliminaries to motivate the idea. The first is to provide an answer to the question ``How `big' is a matrix?'' There are many ways to interpret and answer this question, but a substantial (and useful) answer should involve more than just the dimensions of the matrix. A good measure of the size of a matrix, which we will refer to as the norm of the matrix, should take into account the action of the linear transformation defined by the matrix on vectors. This then will lead to questions about how difficult or easy is it to solve a matrix equation \(A \vx = \vb\).%
\par
If we want to incorporate the action of a matrix \(A\) into a calculation of the norm of \(A\), we might think of measuring how much \(A\) can change a vector \(\vx\). This could lead us to using \(||A\vx||\) as some sort of measure of a norm of \(A\). However, since \(||A (c\vx)|| = |c| \ ||A\vx||\) for any scalar \(c\), scaling \(\vx\) by a large scalar will produce a large norm, so this is not a viable definition of a norm. We could instead measure the \terminology{relative} effect that \(A\) has on a vector \(\vx\) as \(\ds \frac{||A\vx||}{||\vx||}\), since this ratio does not change when \(\vx\) is multiplied by a scalar. The largest of all of these ratios would provide a good sense of how much \(A\) can change vectors. Thus, we define the operator norm of a matrix \(A\) as follows.%
\begin{definition}{}{g:definition:idp21711384}%
\index{operator norm of a matrix}%
The \terminology{operator norm} \footnotemark{} of a matrix \(A\) is%
\begin{equation*}
||A|| = \max_{||\vx|| \neq 0} \left\{\frac{||A\vx||}{||\vx||} \right\}\text{.}
\end{equation*}
%
\end{definition}
\footnotetext[50]{Technically this definition should be in terms of a supremum, but because the equivalent definition restricts the \(\vx\)'s to a compact subset, the sup is achieved and we can use max.\label{g:fn:idp21716760}}%
Due to the linearity of matrix multiplication, we can restrict ourselves to unit vectors for an equivalent definition of the operator norm of the matrix \(A\) as%
\begin{equation*}
||A|| = \ds \max_{||\vx|| = 1}\{||A\vx||\}\text{.}
\end{equation*}
%
\begin{exploration}{}{x:exploration:pa_7_c_1}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Determine \(||A||\) if \(A\) is the zero matrix.%
\item{}Determine \(||I_n||\), where \(I_n\) is the \(n \times n\) identity matrix.%
\item{}Let \(A = \left[ \begin{array}{cc} 1\amp 0\\0\amp 2 \end{array} \right]\). Find \(||A||\). Justify your answer. (Hint: \(x_1^2+4x_2^2 \leq 4(x_1^2 + x_2^2)\).)%
\item{}If \(P\) is an orthogonal matrix, what is \(||P||\)? Why?%
\end{enumerate}
\end{exploration}%
The operator norm of a matrix tells us that how big the action of an \(m \times n\) matrix is can be determined by its action on the unit sphere in \(\R^n\) (the unit sphere is the set of terminal point of unit vectors). Let us consider two examples.%
\begin{example}{}{g:example:idp21944456}%
Let \(A = \left[ \begin{array}{cc} 2\amp 1 \\ 2\amp 5 \end{array} \right]\). We can draw a graph to see the action of \(A\) on the unit circle. A picture of the set \(\{A\vx \ : \ ||\vx|| = 1\}\) is shown in \hyperref[x:figure:F_7_c_Mat_norm1]{Figure~{\xreffont\ref{x:figure:F_7_c_Mat_norm1}}}.%
\begin{figureptx}{The image of the unit circle under the action of \(A\).}{x:figure:F_7_c_Mat_norm1}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/7_c_Mat_norm_a.pdf}
\end{image}%
\tcblower
\end{figureptx}%
It appears that \(A\) transforms the unit circle into an ellipse. To find \(||A||\) we want to maximize \(||A\vx||\) for \(\vx\) on the unit circle. This is the same as maximizing%
\begin{equation*}
||A\vx||^2 = (A\vx)^{\tr}(A\vx) = \vx^{\tr}A^{\tr}A\vx\text{.}
\end{equation*}
%
\par
Now \(A^{\tr}A = \left[ \begin{array}{cc} 8\amp 12 \\ 12\amp 26 \end{array}  \right]\) is a symmetric matrix, so we can orthogonally diagonalize \(A^{\tr}A\). The eigenvalues of \(A^{\tr}A\) are 32 and 2. Let \(P = [\vu_1 \ \vu_2]\), where \(\vu_1=\left[ \frac{\sqrt{5}}{5} \ \frac{2\sqrt{5}}{5} \right]^{\tr}\) is a unit eigenvector of \(A^{\tr}A\) with eigenvalue 32 and \(\vu_2=\left[ -\frac{2\sqrt{5}}{5} \ \frac{\sqrt{5}}{5} \right]^{\tr}\) is a unit eigenvector of \(A^{\tr}A\) with eigenvalue 2. Then \(P\) is an orthogonal matrix such that \(P^{\tr}(A^{\tr}A)P = \left[ \begin{array}{cc} 32\amp 0 \\ 0\amp 2 \end{array}  \right] = D\). It follows that%
\begin{equation*}
\vx^{\tr} (A^{\tr}A)  \vx = \vx^{\tr} PDP^{\tr} \vx = (P^{\tr}\vx)^{\tr} D (P^{\tr}\vx)\text{.}
\end{equation*}
%
\par
Now \(P^{\tr}\) is orthogonal, so \(||P^{\tr}\vx|| = ||\vx||\) and \(P^{\tr}\) maps the unit circle to the unit circle. Moreover, if \(\vx\) is on the unit circle, then \(\vy = P\vx\) is also on the unit circle and \(P^{\tr}\vy = P^{\tr}P\vx = \vx\). So every point \(\vx\) on the unit circle corresponds to a point \(P\vx\) on the unit circle. Thus, the forms \(\vx^{\tr} (A^{\tr}A) \vx\) and \((P^{\tr}\vx)^{\tr} D (P^{\tr}\vx)\) take on exactly the same values over all points on the unit circle. Now we just need to find the maximum value of \((P^{\tr}\vx)^{\tr} D (P^{\tr}\vx)\). This turns out to be relatively easy since \(D\) is a diagonal matrix.%
\par
Let's simplify the notation. Let \(\vy = P^{\tr}\vx\). Then our job is to maximize \(\vy^{\tr}D\vy\). If \(\vy = [y_1 \ y_2]^{\tr}\), then%
\begin{equation*}
\vy^{\tr} D  \vy = 32y_1^2 + 2y_2^2\text{.}
\end{equation*}
%
\par
We want to find the maximum value of this expression for \(\vy\) on the unit circle. Note that \(2y_2^2 \leq 32y_2^2\) and so%
\begin{equation*}
32y_1^2 + 2y_2^2 \leq 32y_1^2 + 32y_2^2 = 32(y_1^2+y_2^2) = 32||\vy||^2 = 32\text{.}
\end{equation*}
%
\par
Since \([1 \ 0]^{\tr}\) is on the unit circle, the expression \(32y_1^2 + 2y_2^2\) attains the value 32 at some point on the unit circle, so 32 is the maximum value of \(\vy^{\tr} D \vy\) over all \(\vy\) on the unit circle. While we are at it, we can similarly find the minimum value of \(\vy^{\tr} D  \vy\) for \(\vy\) on the unit circle. Since \(2y_1^2 \leq 32y_1^2\) we see that%
\begin{equation*}
32y_1^2 + 2y_2^2 \geq 2y_1^2 + 2y_2^2 = 2(y_1^2+y_2^2) = 2||\vy||^2 = 2\text{.}
\end{equation*}
%
\par
Since the expression \(\vy^{\tr} D \vy\) attains the value 2 at \([0 \ 1]^{\tr}\) on the unit circle, we can see that \(\vy^{\tr} D \vy\) attains the minimum value of 2 on the unit circle.%
\par
Now we can return to the expression \(\vx^{\tr} (A^{\tr}A) \vx\). Since \(\vy^{\tr} D \vy\) assumes the same values as \(\vx^{\tr} (A^{\tr}A) \vx\), we can say that the maximum value of \(\vx^{\tr} (A^{\tr}A) \vx\) for \(\vx\) on the unit circle is 32 (and the minimum value is 2). Moreover, the quadratic form \((P^{\tr}\vx)^{\tr} D (P^{\tr}\vx)\) assumes its maximum value when \(P^{\tr}\vx = [1 \ 0]^{\tr}\) or \([-1 \ 0]^{\tr}\). Thus, the form \(\vx^{\tr} (A^{\tr}A) \vx\) assumes its maximum value at the vector \(\vx = P[1 \ 0 ]^{\tr} = \vu_1\) or \(-\vu_1\). Similarly, the quadratic form \(\vx^{\tr} (A^{\tr}A) \vx\) attains its minimum value at \(P[0 \ 1]^{\tr} = \vu_2\) or \(-\vu_2\). We conclude that \(||A|| = \sqrt{32}\).%
\par
\hyperref[x:figure:F_7_c_Mat_norm1_b]{Figure~{\xreffont\ref{x:figure:F_7_c_Mat_norm1_b}}} shows the image of the unit circle under the action of \(A\) and the images of \(A\vu_1\) and \(A\vu_2\) where \(\vu_1, \vu_2\) are the two unit eigenvectors of \(A^{\tr}A\). The image also supports that \(A\vx\) assumes its maximum and minimum values for points on the unit circle at \(\vu_1\) and \(\vu_2\).%
\begin{figureptx}{The image of the unit circle under the action of \(A\), and the vectors \(A\vu_1\) and \(A\vu_2\)}{x:figure:F_7_c_Mat_norm1_b}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/7_c_Mat_norm_b.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\end{example}
\begin{paragraphs}{IMPORTANTE NOTE 1.}{g:paragraphs:idp21975688}%
What we have just argued is that the maximum value of \(||A\vx||\) for \(\vx\) on the unit sphere in \(\R^n\) is the square root of the largest eigenvalue of \(A^{\tr}A\) and occurs at a corresponding unit eigenvector.%
\end{paragraphs}%
\begin{example}{}{g:example:idp21973896}%
This same process works for matrices other than \(2 \times 2\) ones. For example, consider \(A = \left[ \begin{array}{rrr} -2\amp 8\amp 20 \\ 14\amp 19\amp 10 \end{array} \right]\). In this case \(A\) maps \(\R^3\) to \(\R^2\). The image of the unit sphere \(\{\vx \in \R^3 : ||\vx|| = 1\}\) under left multiplication by \(A\) is a filled ellipse as shown in \hyperref[x:figure:F_7_c_Mat_norm_2]{Figure~{\xreffont\ref{x:figure:F_7_c_Mat_norm_2}}}.%
\begin{figureptx}{The image of the unit circle under the action of \(A\), and the vectors \(A\vu_1\) and \(A\vu_2\)}{x:figure:F_7_c_Mat_norm_2}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/7_c_Mat_norm_2.pdf}
\end{image}%
\tcblower
\end{figureptx}%
As with the previous example, the norm of \(A\) is the square root of the maximum value of \(\vx^{\tr} (A^{\tr}A) \vx\) and this maximum value is the dominant eigenvalue of \(A^{\tr}A = \left[ \begin{array}{ccc} 200\amp 250\amp 100\\ 250\amp 425\amp 350\\ 100\amp 350\amp 500 \end{array} \right]\). The eigenvalues of \(A\) are \(\lambda_1 = 900\), \(\lambda_2 = 225\), and \(\lambda_3 = 0\) with corresponding unit eigenvectors \(\vu_1=\left[ \frac{1}{3} \ \frac{2}{3} \ \frac{2}{3} \right]^{\tr}\), \(\vu_1=\left[ -\frac{2}{3} \ -\frac{1}{3} \ \frac{2}{3} \right]^{\tr}\), and \(\vu_3=\left[ \frac{2}{3} \ -\frac{2}{3} \ \frac{1}{3} \right]^{\tr}\). So in this case we have \(||A|| = \sqrt{900} = 30\). The transformation defined by matrix multiplication by \(A\) from \(\R^3\) to \(\R^2\) has a one-dimensional kernel which is spanned by the eigenvector corresponding to \(\lambda_3\). The image of the transformation is 2-dimensional and the image of the unit circle is an ellipse where \(A\vu_1\) gives the major axis of the ellipse and \(A\vu_2\) gives the minor axis. Essentially, the square roots of the eigenvalues of \(A^{\tr}A\) tell us how \(A\) stretches the image space in each direction.%
\end{example}
\begin{paragraphs}{IMPORTANT NOTE 2.}{g:paragraphs:idp21996936}%
We have just argued that the image of the unit \(n\)-sphere under the action of an \(m \times n\) matrix is an ellipsoid in \(\R^m\) stretched the greatest amount, \(\sqrt{\lambda_1}\), in the direction of an eigenvector for the largest eigenvalue (\(\lambda_1\)) of \(A^{\tr}A\); the next greatest amount, \(\sqrt{\lambda_2}\), in the direction of a unit vector for the second largest eigenvalue (\(\lambda_2\)) of \(A^{\tr}A\); and so on.%
\end{paragraphs}%
\begin{activity}{}{g:activity:idp21998344}%
Let \(A = \left[ \begin{array}{rc} 0\amp 5\\ 4\amp 3 \\ -2\amp 1 \end{array} \right]\). Then \(A^{\tr}A = \left[ \begin{array}{cc} 20\amp 10\\ 10\amp 35 \end{array} \right]\). The eigenvalues of \(A^{\tr}A\) are \(\lambda_1 = 40\) and \(\lambda_2 = 15\) with respective eigenvectors \(\vv_1 = \left[ \begin{array}{c} \frac{1}{2} \\ 1 \end{array} \right]\) and \(\vv_2 = \left[ \begin{array}{r} -2 \\ 1 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find \(||A||\).%
\item{}Find a unit vector \(\vx\) at which \(||A\vx||\) assumes its maximum value.%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The SVD}
\typeout{************************************************}
%
\begin{sectionptx}{The SVD}{}{The SVD}{}{}{x:section:sec_svd}
The Singular Value Decomposition (SVD) is essentially a concise statement of what we saw in the previous section that works for \emph{any} matrix. We will uncover the SVD in this section.%
\begin{exploration}{}{x:exploration:pa_7_c_2}%
Let \(A = \left[ \begin{array}{ccc} 1\amp 1\amp 0\\ 0\amp 1\amp 1 \end{array}  \right]\). Since \(A\) is not square, we cannot diagonalize \(A\). However, the matrix%
\begin{equation*}
A^{\tr}A = \left[ \begin{array}{ccc} 1\amp 1\amp 0 \\ 1\amp 2\amp 1 \\ 0\amp 1\amp 1 \end{array}  \right]
\end{equation*}
is a symmetric matrix and can be orthogonally diagonalized. The eigenvalues of \(A^{\tr}A\) are 3, 1, and 0 with corresponding eigenvectors%
\begin{equation*}
\left[ \begin{array}{c} 1\\2\\1 \end{array}  \right], \ \left[ \begin{array}{r} -1\\0\\1 \end{array}  \right], \ \text{ and }  \ \left[ \begin{array}{r} 1\\-1\\1 \end{array}  \right]\text{,}
\end{equation*}
respectively. Use appropriate technology to do the following.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find an orthogonal matrix \(V = [\vv_1 \ \vv_2 \ \vv_3]\) that orthogonally diagonalizes \(A^{\tr}A\), where%
\begin{equation*}
V^{\tr}\left(A^{\tr}A\right)V = \left[ \begin{array}{ccc} 3\amp 0\amp 0\\0\amp 1\amp 0\\0\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
%
\item{}For \(i=1,2\), let \(\vu_i = \frac{A \vv_i}{||A \vv_i||}\). Find each \(\vu_i\). Why don't we define \(\vu_3\) in this way?%
\item{}Let \(U = [\vu_1 \ \vu_2]\). What kind of matrix is \(U\)? Explain.%
\item{}Calculate the matrix product \(U^{\tr}AV\). What do you notice? How is this similar to the eigenvector decomposition of a matrix?%
\end{enumerate}
\end{exploration}%
\hyperref[x:exploration:pa_7_c_2]{Preview Activity~{\xreffont\ref{x:exploration:pa_7_c_2}}} contains the basic ideas behind the Singular Value Decomposition. Let \(A\) be an \(m \times n\) matrix with real entries. Note that \(A^{\tr}A\) is a symmetric \(n \times n\) matrix and, hence, it can be orthogonally diagonalized. Let \(V = [\vv_1 \ \vv_2 \ \vv_3 \  \cdots \  \vv_n ]\) be an \(n \times n\) orthogonal matrix whose columns form an orthonormal set of eigenvectors for \(A^{\tr}A\). For each \(i\), let \((A^{\tr}A)\vv_i = \lambda_i \vv_i\). We know%
\begin{equation*}
V^{\tr} (A^{\tr}A) V = \left[ \begin{array}{ccccc} \lambda_1\amp 0\amp 0\amp \cdots\amp 0 \\ 0\amp  \lambda_2\amp 0\amp \cdots\amp 0 \\ \vdots \amp  \vdots \amp  \vdots \amp  \amp  \vdots  \\ 0\amp 0\amp 0\amp \cdots\amp  \lambda_n \end{array}  \right]\text{.}
\end{equation*}
%
\par
Now notice that for each \(i\) we have%
\begin{equation}
|| A \vv_i ||^2 = (A\vv_i)^{\tr}(A\vv_i) = \vv_i^{\tr}(A^{\tr}A) \vv_i = \vv_i^{\tr} \lambda_i \vv_i = \lambda_i ||\vv_i||^2 = \lambda_i\text{,}\label{x:men:eq_7_c_sing_vals1}
\end{equation}
so \(\lambda_i \geq 0\). Thus, the matrix \(A^{\tr}A\) has no negative eigenvalues. We can always arrange the eigenvectors and eigenvalues of \(A^{\tr}A\) so that%
\begin{equation*}
\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n \geq 0\text{.}
\end{equation*}
%
\par
Also note that%
\begin{equation*}
(A\vv_i) \cdot (A\vv_j) = (A\vv_i)^{\tr} (A\vv_j) = \vv_i^{\tr} (A^{\tr}A) \vv_j = \vv_i^{\tr} \lambda_j\ \vv_j = \lambda_j \vv_i \cdot \vv_j = 0
\end{equation*}
if \(i \neq j\). So the set \(\{A\vv_1, A\vv_2, \ldots, A\vv_n\}\) is an orthogonal set in \(\R^m\). Each of the vectors \(A\vv_i\) is in \(\Col A\), and so \(\{A\vv_1, A\vv_2, \ldots, A\vv_n\}\) is an orthogonal subset of \(\Col A\). It is possible that \(A\vv_i = \vzero\) for some of the \(\vv_i\) (if \(A^{\tr}A\) has 0 as an eigenvalue). Let \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_r\) be the eigenvectors corresponding to the nonzero eigenvalues. Then the set%
\begin{equation*}
\CB=\{A\vv_1, A\vv_2, \ldots, A\vv_r\}
\end{equation*}
is a linearly independent set of nonzero orthogonal vectors in \(\Col A\). Now we will show that \(\CB\) is a basis for \(\Col A\). Let \(\vy\) be a vector in \(\Col A\). Then \(\vy = A\vx\) for some vector \(\vx\) in \(\R^n\). Recall that the vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_n\) form an orthonormal basis of \(\R^n\), so%
\begin{equation*}
\vx = x_1 \vv_1 + x_2 \vv_2 + \cdots + x_n \vv_n
\end{equation*}
for some scalars \(x_1\), \(x_2\), \(\ldots\), \(x_n\). Since \(A\vv_j = \vzero\) for \(r+1 \leq j \leq n\) we have%
\begin{align*}
\vy \amp = A\vx\\
\amp = A(x_1 \vv_1 + x_2 \vv_2 + \cdots + x_n \vv_n)\\
\amp = x_1 A\vv_1 + x_2 A\vv_2 + \cdots + x_r A\vv_r + x_{r+1} A \vv_{r+1} + \cdots + x_n A\vv_n\\
\amp = x_1 A\vv_1 + x_2 A\vv_2 + \cdots + x_r A\vv_r\text{.}
\end{align*}
%
\par
So \(\Span \ \CB = \Col A\) and \(\CB\) is an orthogonal basis for \(\Col A\).%
\par
Now we are ready to find the Singular Value Decomposition of \(A\). First we create an orthonormal basis \(\{\vu_1, \vu_2, \ldots, \vu_r\}\) for \(\Col A\) by normalizing the vectors \(A\vv_i\). So we let%
\begin{equation*}
\vu_i = \frac{A\vv_i}{||A\vv_i||}
\end{equation*}
for \(i\) from 1 to \(r\).%
\par
Remember from \hyperref[x:men:eq_7_c_sing_vals1]{({\xreffont\ref{x:men:eq_7_c_sing_vals1}})} that \(||A\vv_i||^2 = \lambda_i\), so if we let \(\sigma_i = \sqrt{\lambda_i}\), then we have%
\begin{equation*}
\vu_i = \frac{A\vv_i}{\sigma_i} \text{ and }  A \vv_i = \sigma_i \vu_i\text{.}
\end{equation*}
%
\par
We ordered the \(\lambda_i\) so that \(\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n\), so we also have%
\begin{equation*}
\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0\text{.}
\end{equation*}
%
\par
The scalars \(\sigma_1\), \(\sigma_2\), \(\ldots\), \(\sigma_n\) are called the \emph{singular values} of \(A\).%
\begin{definition}{}{g:definition:idp22047368}%
\index{singular values}%
Let \(A\) be an \(m \times n\) matrix. The \terminology{singular values} of \(A\) are the square roots of the eigenvalues of \(A^{\tr}A\).%
\end{definition}
The vectors \(\vu_1\), \(\vu_2\), \(\ldots\), \(\vu_r\) are \(r\) orthonormal vectors in \(\R^m\). We can extend the set \(\{\vu_1\), \(\vu_2\), \(\ldots\), \(\vu_r\}\) to an orthonormal basis \(\CC = \{\vu_1, \vu_2, \ldots, \vu_r, \vu_{r+1} \vu_{r+2}, \ldots, \vu_m\}\) of \(\R^m\). Recall that \(A \vv_i = \sigma_i \vu_i\) for \(1 \leq i \leq r\) and \(A \vv_j = \vzero\) for \(r+1 \leq j \leq n\), so%
\begin{align*}
AV \amp = A [\vv_1 \  \vv_2  \ \cdots  \ \vv_n]\\
\amp = [A\vv_1  \ A\vv_2  \ \cdots  \ A\vv_n]\\
\amp = [\sigma_1 \vu_1  \ \sigma_2 \vu_2  \ \cdots  \ \sigma_r \vu_r  \ \vzero  \ \vzero  \ \cdots  \ \vzero]\text{.}
\end{align*}
%
\par
We can write the matrix \([\sigma_1 \vv_1  \ \sigma_2 \vv_2  \ \cdots  \ \sigma_r \vv_r  \ \vzero  \ \vzero  \ \cdots  \ \vzero]\) in another way. Let \(\Sigma\) be the \(m\times n\) matrix defined as%
\begin{equation*}
\Sigma = \left[ \begin{array}{ccccc|c} \sigma_1\amp \amp \amp \amp 0\amp  \\ \amp  \sigma_2\amp \amp \amp \amp 0 \\ \amp \amp  \sigma_3\amp \amp \amp  \\ \amp   \amp  \amp  \ddots \amp  \amp  \\ 0\amp \amp \amp \amp  \sigma_r \\ \hline \amp \amp 0\amp \amp \amp 0 \end{array}  \right]\text{.}
\end{equation*}
%
\par
Now%
\begin{equation*}
[\vu_1  \ \vu_2  \ \cdots  \ \vu_m] \Sigma = [\sigma_1\vu_1  \ \sigma_1\vu_2  \ \cdots  \ \sigma_r\vu_r \ \vzero \ \vzero \ \cdots \ \vzero] = AV\text{.}
\end{equation*}
%
\par
So if \(U = [\vu_1 \ \vu_2 \ \cdots \ \vu_m]\), then%
\begin{equation*}
U\Sigma = AV\text{.}
\end{equation*}
%
\par
Since \(V\) is an orthogonal matrix, we have that%
\begin{equation*}
U \Sigma V^{\tr} = AV V^{\tr} = A\text{.}
\end{equation*}
This is the Singular Value Decomposition of \(A\).%
\begin{theorem}{The Singular Value Decomposition.}{}{g:theorem:idp22057480}%
Let \(A\) be an \(m \times n\) matrix of rank \(r\). There exist an \(m \times m\) orthogonal matrix \(U\), an \(n \times n\) orthogonal matrix \(V\), and an \(m \times n\) matrix \(\Sigma\) whose first \(r\) diagonal entries are the singular values \(\sigma_1\), \(\sigma_2\), \(\ldots\), \(\sigma_r\) and whose other entries are 0, such that%
\begin{equation*}
A = U \Sigma V^{\tr}\text{.}
\end{equation*}
%
\end{theorem}
\begin{paragraphs}{SVD Summary.}{g:paragraphs:idp22063752}%
\index{singular vectors!right}%
\index{singular vectors!left}%
A Singular Value Decomposition of an \(m \times n\) matrix \(A\) of rank \(r\) can be found as follows.%
\begin{enumerate}
\item{}Find an orthonormal basis \(\{\vv_1, \vv_2, \vv_3, \ldots, \vv_n\}\) of eigenvectors of \(A^{\tr}A\) such that \((A^{\tr}A) \vv_i = \lambda_i \vv_i\) for \(i\) from 1 to \(n\) with \(\lambda_1 \geq \lambda_{2} \geq \cdots \geq \lambda_n \geq 0\) with the first \(r\) eigenvalues being positive. The vectors \(\vv_1\), \(\vv_2\), \(\vv_3\), \(\ldots\), \(\vv_n\) are the \terminology{right singular vectors} of \(A\).%
\item{}Let%
\begin{equation*}
V= [\vv_1 \  \vv_2 \  \vv_3 \  \cdots \  \vv_n ]\text{.}
\end{equation*}
Then \(V\) orthogonally diagonalizes \(A^{\tr}A\).%
\item{}The singular values of \(A\) are the numbers \(\sigma_i\), where \(\sigma_i = \sqrt{\lambda_i} > 0\) for \(i\) from 1 to \(r\). Let \(\Sigma\) be the \(m \times n\) matrix%
\begin{equation*}
\Sigma = \left[ \begin{array}{ccccc|c} \sigma_1\amp \amp \amp \amp 0\amp  \\ \amp  \sigma_2\amp \amp \amp \amp 0 \\ \amp \amp  \sigma_3\amp \amp \amp  \\ \amp   \amp  \amp  \ddots \amp  \amp  \\ 0\amp \amp \amp \amp  \sigma_r \\ \hline \amp \amp 0\amp \amp \amp 0 \end{array}  \right]
\end{equation*}
%
\item{}For \(i\) from 1 to \(r\), let \(\vu_i = \frac{A\vv_i}{||A\vv_i||}\). Then the set \(\{\vu_1, \vu_2, \ldots, \vu_r\}\) forms an orthonormal basis of \(\Col A\).%
\item{}Extend the set \(\{\vu_1, \vu_2, \ldots, \vu_r\}\) to an orthonormal basis%
\begin{equation*}
\{\vu_1, \vu_2, \ldots, \vu_r, \vu_{r+1} \vu_{r+2}, \ldots, \vu_m\}
\end{equation*}
of \(\R^m\). Let%
\begin{equation*}
U = [\vu_1 \ \vu_2 \ \cdots \ \vu_m]\text{.}
\end{equation*}
The vectors \(\vu_1\), \(\vu_2\), \(\ldots\), \(\vu_m\) are the \terminology{left singular vectors} of \(A\).%
\item{}Then \(A = U \Sigma V^{\tr}\) is a singular value decomposition of \(A\).%
\end{enumerate}
%
\end{paragraphs}%
\begin{activity}{}{x:activity:ex_7_c_SVD}%
Let \(A = \left[ \begin{array}{rc} 0\amp 5\\ 4\amp 3 \\ -2\amp 1 \end{array}  \right]\). Then \(A^{\tr}A = \left[ \begin{array}{cc} 20\amp 10\\ 10\amp 35 \end{array} \right]\). The eigenvalues of \(A^{\tr}A\) are \(\lambda_1 = 40\) and \(\lambda_2 = 15\) with respective eigenvectors \(\vw_1 = \left[ \begin{array}{c} 1 \\ 2 \end{array}  \right]\) and \(\vw_2 = \left[ \begin{array}{r} -2 \\ 1 \end{array}  \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find an orthonormal basis \(\{\vv_1, \vv_2, \vv_3, \ldots, \vv_n\}\) of eigenvectors of \(A^{\tr}A\). What is \(n\)? Find the matrix \(V\) in a SVD for \(A\).%
\item{}Find the singular values of \(A\). What is the rank \(r\) of \(A\)? Why?%
\item{}What are the dimensions of the matrix \(\Sigma\) in a SVD of \(A\)? Find \(\Sigma\).%
\item{}Find the vectors \(\vu_1\), \(\vu_2\), \(\ldots\), \(\vu_r\). If necessary, extend this set to an orthonormal basis%
\begin{equation*}
\{\vu_1, \vu_2, \ldots, \vu_r, \vu_{r+1} \vu_{r+2}, \ldots, \vu_m\}
\end{equation*}
of \(\R^m\).%
\item{}Find the matrix \(U\) so that \(A = U \Sigma V^{\tr}\) is a SVD for \(A\).%
\end{enumerate}
\end{activity}%
There is another way we can write this SVD of \(A\). Let the \(m \times n\) matrix \(A\) have a singular value decomposition \(U \Sigma V^{\tr}\), where%
\begin{align*}
U \amp = [\vu_1 \ \vu_2 \ \cdots \ \vu_m],\\
\Sigma \amp = \left[ \begin{array}{ccccc|c} \sigma_1\amp \amp \amp \amp 0\amp\\
\amp  \sigma_2\amp \amp \amp \amp 0\\
\amp \amp  \sigma_3\amp \amp \amp\\
\amp   \amp  \amp  \ddots \amp  \amp\\
0\amp \amp \amp \amp  \sigma_r\\
\hline \amp \amp 0\amp \amp \amp 0 \end{array} \right],  \text{ and }\\
V \amp = [\vv_1 \ \vv_2 \ \vv_3 \   \cdots \  \vv_n ]\text{.}
\end{align*}
%
\par
Since \(A = U \Sigma V^{\tr}\) we see that%
\begin{align}
A \amp = [ \vu_1 \ \vu_2 \ \vu_3 \ \cdots \ \vu_m] \left[ \begin{array}{ccccc|c} \sigma_1\amp \amp \amp \amp 0\amp\notag\\
\amp  \sigma_2\amp \amp \amp \amp 0\notag\\
\amp \amp  \sigma_3\amp \amp \amp\notag\\
\amp   \amp  \amp  \ddots \amp  \amp\notag\\
0\amp \amp \amp \amp  \sigma_r\notag\\
\hline \amp \amp 0\amp \amp \amp 0 \end{array} \right] \left[  \begin{array}{c} \vv_1^{\tr}\notag\\
\vv_2^{\tr}\notag\\
\vv_3^{\tr}\notag\\
\vdots\notag\\
\vv_n^{\tr} \end{array} \right]\notag\\
\amp = [ \sigma_1\vu_1 \ \sigma_2\vu_2 \ \sigma_3\vu_3 \ \cdots \ \sigma_r\vu_r \ \vzero \ \cdots \ \vzero] \left[  \begin{array}{c} \vv_1^{\tr}\notag\\
\vv_2^{\tr}\notag\\
\vv_3^{\tr}\notag\\
\vdots\notag\\
\vv_n^{\tr} \end{array} \right]\notag\\
\amp = \sigma_1 \vu_1\vv_1^{\tr} + \sigma_2 \vu_2\vv_2^{\tr} + \sigma_3 \vu_3\vv_3^{\tr} + \cdots + \sigma_r \vu_r\vv_r^{\tr}\text{.}\label{x:mrow:eq_7_c_SVD}
\end{align}
%
\par
\index{outer product decomposition} This is called an \terminology{outer product decomposition} of \(A\) and tells us everything we learned above about the action of the matrix \(A\) as a linear transformation. Each of the products \(\vu_i\vv_i^{\tr}\) is a rank 1 matrix (see \hyperlink{x:exercise:ex_7_c_rank1}{Exercise~{\xreffont 9}}), and \(||A\vv_1|| = \sigma_1\) is the largest value \(A\) takes on the unit \(n\)-sphere, \(||A\vv_2|| = \sigma_2\) is the next largest dilation of the unit \(n\)-sphere, and so on. An outer product decomposition allows us to approximate \(A\) with smaller rank matrices. For example, the matrix \(\sigma_1 \vu_1\vv_1^{\tr}\) is the best rank 1 approximation to \(A\), \(\sigma_1 \vu_1\vv_1^{\tr} + \sigma_2 \vu_2\vv_2^{\tr}\) is the best rank 2 approximation, and so on. This will be very useful in applications, as we will see in the next section.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  SVD and the Null, Column, and Row Spaces of a Matrix}
\typeout{************************************************}
%
\begin{sectionptx}{SVD and the Null, Column, and Row Spaces of a Matrix}{}{SVD and the Null, Column, and Row Spaces of a Matrix}{}{}{x:section:sec_svd_mtx_spaces}
We conclude this section with a short discussion of how a singular value decomposition relates fundamental subspaces of a matrix. We have seen that the vectors \(\vu_1\), \(\vu_2\), \(\ldots\), \(\vu_r\) in an SVD for an \(m \times n\) matrix \(A\) form a basis for \(\Col A\). Recall also that \(A \vv_j = \vzero\) for \(r+1 \leq j \leq n\). Since \(\dim(\Nul A) + \dim(\Col A) = n\), it follows that the vectors \(\vv_{r+1}\), \(\vv_{r+2}\), \(\ldots\), \(\vv_n\) form a basis for \(\Nul A\). As you will show in the exercises, the set \(\{\vv_1, \vv_2, \ldots, \vv_r\}\) is a basis for \(\Row A\). Thus, an SVD for a matrix \(A\) tells us about three fundamental vector spaces related to \(A\).%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_svd_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp22135688}%
Let \(A = \left[ \begin{array}{cccc} 2 \amp 0 \amp 0 \amp 0 \\ 0 \amp 2 \amp 1 \amp 0 \\ 0\amp 1 \amp 2 \amp 0 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find a singular value decomposition for \(A\). You may use technology to find eigenvalues and eigenvectors of matrices.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp22132872}{}\quad{}With \(A\) as given, we have \(A^{\tr}A = \left[ \begin{array}{cccc} 4  \amp   0  \amp   0  \amp   0 \\  0  \amp   5  \amp   4  \amp   0 \\ 0  \amp   4  \amp   5  \amp   0 \\ 0\amp 0\amp 0\amp 0 \end{array}  \right]\). Technology shows that the eigenvalues of \(A^{\tr}A\) are \(\lambda_1 = 9\), \(\lambda_2 = 4\), \(\lambda_3 = 1\), and \(\lambda_4 = 0\) with corresponding orthonormal eigenvectors \(\vv_1 = \frac{1}{\sqrt{2}}[0 \ 1 \ 1 \ 0]^{\tr}\), \(\vv_2 = [1 \ 0 \ 0 \ 0]^{\tr}\), \(\vv_3 = \frac{1}{\sqrt{2}}[0 \ -1 \ 1 \ 0]^{\tr}\), and \(\vv_4 = [0 \ 0 \ 0 \ 1]^{\tr}\). This makes \(V = [\vv_1 \ \vv_2 \ \vv_3 \ \vv_4]\). The singular values of \(A\) are \(\sigma_1 = \sqrt{9} = 3\), \(\sigma_2 = \sqrt{4}= 2\), \(\sigma_3 = \sqrt{1} = 1\), and \(\sigma_4 = 0\), so \(\Sigma\) is the \(3 \times 4\) matrix with the nonzero singular values along the diagonal and zeros everywhere else. Finally, we define the vectors \(\vu_i\) as \(\vu_i = \frac{1}{||A \vv_i||} A\vv_i\). Again, technology gives us \(\vu_1 = \frac{1}{\sqrt{2}}[0 \ 1 \ 1]^{\tr}\), \(\vu_2 = [1 \ 0 \ 0]^{\tr}\), and \(\vu_3 = \frac{1}{\sqrt{2}}[0 \ -1 \ 1]^{\tr}\). Thus, a singular value decomposition of \(A\) is \(U \Sigma V^{\tr}\), where%
\begin{align*}
U \amp = \left[ \begin{array}{ccr} 0\amp 1\amp 0\\
\frac{1}{\sqrt{2}}\amp 0\amp -\frac{1}{\sqrt{2}}\\
\frac{1}{\sqrt{2}}\amp 0\amp \frac{1}{\sqrt{2}} \end{array} \right],\\
\Sigma \amp = \left[ \begin{array}{cccc} 3\amp 0\amp 0\amp 0\\
0\amp 2\amp 0\amp 0\\
0\amp 0\amp 1\amp 0 \end{array} \right], \ \text{ and }\\
V \amp = \left[ \begin{array}{ccrc} 0\amp 1\amp 0\amp 0\\
\frac{1}{\sqrt{2}}\amp 0\amp -\frac{1}{\sqrt{2}}\amp 0\\
\frac{1}{\sqrt{2}}\amp 0\amp \frac{1}{\sqrt{2}}\amp 0\\
0\amp 0\amp 0\amp 1 \end{array} \right]\text{.}
\end{align*}
%
\item{}Use the singular value decomposition to find a basis for \(\Col A\), \(\Row A\), and \(\Nul A\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp22150280}{}\quad{}Recall that the right singular vectors of an \(m \times n\) matrix \(A\) of rank \(r\) form an orthonormal basis \(\{\vv_1, \vv_2, \vv_3, \ldots, \vv_n\}\) of eigenvectors of \(A^{\tr}A\) such that \((A^{\tr}A) \vv_i = \lambda_i \vv_i\) for \(i\) from 1 to \(n\) with \(\lambda_1 \geq \lambda_{2} \geq \cdots \geq \lambda_n \geq 0\). These vectors are the columns of the matrix \(V = [\vv_1 \ \vv_2 \ \cdots \ \vv_n]\) in a singular value decomposition of \(A\). For \(i\) from 1 to \(r\), we let \(\vu_i = \frac{A\vv_i}{||A\vv_i||}\). Then the set \(\{\vu_1, \vu_2, \ldots, \vu_r\}\) forms an orthonormal basis of \(\Col A\). We extend this set \(\{\vu_1, \vu_2, \ldots, \vu_r\}\) to an orthonormal basis \(\{\vu_1, \vu_2, \ldots, \vu_r, \vu_{r+1} \vu_{r+2}, \ldots, \vu_m\}\) of \(\R^m\). Recall also that \(A \vv_j = \vzero\) for \(r+1 \leq j \leq n\). Since \(\dim(\Nul A) + \dim(\Col A) = n\), it follows that the vectors \(\vv_{r+1}\), \(\vv_{r+2}\), \(\ldots\), \(\vv_n\) form a basis for \(\Nul A\). Finally, the set \(\{\vv_1, \vv_2, \ldots, \vv_r\}\) is a basis for \(\Row A\). So in our example, we have \(m = 3\), \(n = 4\), \(\vv_1 = \frac{1}{\sqrt{2}}[0 \ 1 \ 1 \ 0]^{\tr}\), \(\vv_2 = [1 \ 0 \ 0 \ 0]^{\tr}\), \(\vv_3 = \frac{1}{\sqrt{2}}[0 \ -1 \ 1 \ 0]^{\tr}\), and \(\vv_4 = [0 \ 0 \ 0 \ 1]^{\tr}\). Since the singular values of \(A\) are \(3\), \(2\), \(1\), and \(0\), it follows that \(r = \rank(A) = 3\). We also have \(\vu_1 = \frac{1}{\sqrt{2}}[0 \ 1 \ 1]^{\tr}\), \(\vu_2 = [1 \ 0 \ 0]^{\tr}\), and \(\vu_3 = \frac{1}{\sqrt{2}}[ 0 \ -1 \ 1]^{\tr}\). So%
\begin{equation*}
\left\{\frac{1}{\sqrt{2}}[0 \ 1 \ 1 \ 0]^{\tr}, [1 \ 0 \ 0 \ 0]^{\tr}, \frac{1}{\sqrt{2}}[0 \ -1 \ 1 \ 0]^{\tr}\right\}
\end{equation*}
is a basis for \(\Row A\) and%
\begin{equation*}
\{[0 \ 0 \ 0 \ 1]^{\tr}\}
\end{equation*}
is a basis for \(\Nul A\). Finally, the set%
\begin{equation*}
\left\{ \frac{1}{\sqrt{2}}[0 \ 1 \ 1]^{\tr}, [1 \ 0 \ 0]^{\tr}, \frac{1}{\sqrt{2}}[[ 0 \ -1 \ 1]^{\tr}\right\}
\end{equation*}
is a basis for \(\Col A\).%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp22172936}%
Let%
\begin{equation*}
A = \left[ \begin{array}{ccc} 2\amp 5\amp 4\\6\amp 3\amp 0\\6\amp 3\amp 0\\2\amp 5\amp 4 \end{array}  \right]\text{.}
\end{equation*}
%
\par
The eigenvalues of \(A^{\tr}A\) are \(\lambda_1 = 144\), \(\lambda_2 = 36\), and \(\lambda_3=0\) with corresponding eigenvectors%
\begin{equation*}
\vw_1 = \left[ \begin{array}{c} 2\\2\\1 \end{array}  \right], \ \vw_1 = \left[ \begin{array}{r} -2\\1\\2 \end{array}  \right], \ \text{ and }  \ \vw_1 = \left[ \begin{array}{r} 1\\-2\\2 \end{array}  \right]\text{.}
\end{equation*}
%
\par
In addition,%
\begin{equation*}
A \vw_1 = \left[ \begin{array}{c} 18\\18\\18\\18 \end{array}  \right] \ \text{ and }  A \vw_2 = \left[ \begin{array}{r} 9\\-9\\-9\\9 \end{array}  \right]\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find orthogonal matrices \(U\) and \(V\), and the matrix \(\Sigma\), so that \(U \Sigma V^{\tr}\) is a singular value decomposition of \(A\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp22170888}{}\quad{}Normalizing the eigenvectors \(\vw_1\), \(\vw_2\), and \(\vw_3\) to normal eigenvectors \(\vv_1\), \(\vv_2\), and \(\vv_3\), respectively, gives us an orthogonal matrix%
\begin{equation*}
V = \left[  \begin{array}{crr} \frac{2}{3}\amp -\frac{2}{3}\amp \frac{1}{3}\\ \frac{2}{3}\amp \frac{1}{3}\amp - \frac{2}{3}\\  \frac{1}{3}\amp \frac{2}{3}\amp \frac{2}{3} \end{array}  \right]\text{.}
\end{equation*}
Now \(A \vv_i = A \frac{\vw_i}{||\vw_i||} = \frac{1}{|| \vw_i ||} A \vw_i\), so normalizing the vectors \(A \vv_1\) and \(A \vv_2\) gives us vectors%
\begin{equation*}
\vu_1 = \frac{1}{2} \left[ \begin{array}{c} 1\\1\\1\\1 \end{array}  \right] \ \text{ and }  \ \vu_2 = \frac{1}{2} \left[ \begin{array}{r} 1\\-1\\-1\\1 \end{array}  \right]
\end{equation*}
that are the first two columns of our matrix \(U\). Given that \(U\) is a \(4 \times 4\) matrix, we need to find two other vectors orthogonal to \(\vu_1\) and \(\vu_2\) that will combine with \(\vu_1\) and \(\vu_2\) to form an orthogonal basis for \(\R^4\). Letting \(\vz_1 = [1 \ 1 \ 1 \ 1]^{\tr}\), \(\vz_2 = [1 \ -1 \ -1 \ 1]^{\tr}\), \(\vz_3 = [1 \ 0 \ 0 \ 0]^{\tr}\), and \(\vz_4 = [0 \ 1 \ 0 \ 1]^{\tr}\), a computer algebra system shows that the reduced row echelon form of the matrix \([\vz_1 \ \vz_2 \ \vz_3 \ \vz_4]\) is \(I_4\), so that vectors \(\vz_1\), \(\vz_2\), \(\vz_3\), \(\vz_4\) are linearly independent. Letting \(\vw_1 = \vz_1\) and \(\vw_2 = \vz_2\), the Gram-Schmidt process shows that the set \(\{\vw_1, \vw_2, \vw_3, \vw_4\}\) is an orthogonal basis for \(\R^4\), where%
\begin{align*}
\vw_3 \amp = [1 \ 0 \ 0 \ 0]^{\tr} - \frac{[1 \ 0 \ 0 \ 0]^{\tr} \cdot [1 \ 1 \ 1 \ 1]^{\tr}}{[1 \ 1 \ 1 \ 1]^{\tr} \cdot [1 \ 1 \ 1 \ 1]^{\tr}} [1 \ 1 \ 1 \ 1]^{\tr}\\
\amp \qquad - \frac{[1 \ 0 \ 0 \ 0]^{\tr} \cdot [1 \ -1 \ -1 \ 1]^{\tr}}{[1 \ -1 \ -1 \ 1]^{\tr} \cdot [1 \ -1 \ -1 \ 1]^{\tr}} [1 \ -1 \ -1 \ 1]^{\tr}\\
\amp = [1 \ 0 \ 0 \ 0]^{\tr} - \frac{1}{4}[1 \ 1 \ 1 \ 1]^{\tr} - \frac{1}{4} [1 \ -1 \ -1 \ 1]^{\tr}\\
\amp = \frac{1}{4} [2 \ 0 \ 0 \ -2]^{\tr}
\end{align*}
and (using \([1 \ 0 \ 0 \ -1]^{\tr}\) for \(\vw_3\))%
\begin{align*}
\vw_4 \amp = [0 \ 1 \ 0 \ 0]^{\tr} - \frac{[0 \ 1 \ 0 \ 0]^{\tr} \cdot [1 \ 1 \ 1 \ 1]^{\tr}}{[1 \ 1 \ 1 \ 1]^{\tr} \cdot [1 \ 1 \ 1 \ 1]^{\tr}} [1 \ 1 \ 1 \ 1]^{\tr}\\
\amp \qquad - \frac{[0 \ 1 \ 0 \ 0]^{\tr} \cdot [1 \ -1 \ -1 \ 1]^{\tr}}{[1 \ -1 \ -1 \ 1]^{\tr} \cdot [1 \ -1 \ -1 \ 1]^{\tr}} [1 \ -1 \ -1 \ 1]^{\tr}\\
\amp \qquad - \frac{[0 \ 1 \ 0 \ 0]^{\tr} \cdot [1 \ 0 \ 0 \ -1]^{\tr}}{[1 \ 0 \ 0 \ -1]^{\tr} \cdot [1 \ 0 \ 0 \ -1]^{\tr}} [1 \ 0 \ 0 \ -1]^{\tr}\\
\amp = [0 \ 1 \ 0 \ 0]^{\tr} - \frac{1}{4}[1 \ 1 \ 1 \ 1]^{\tr} + \frac{1}{4} [1 \ -1 \ -1 \ 1]^{\tr} - \vzero\\
\amp = \frac{1}{4} [0 \ 2 \ -2 \ 0]^{\tr}\text{.}
\end{align*}
The set \(\{\vu_1, \vu_2, \vu_3, \vu_4\}\) where \(\vu_1 = \frac{1}{2}[1 \ 1 \ 1 \ 1]^{\tr}\), \(\vu_2 = \frac{1}{2}[1 \ -1 \ -1 \ 1]^{\tr}\), \(\vu_3 = \frac{1}{\sqrt{2}}[1 \ 0 \ 0 \ -1]^{\tr}\) and \(\vu_4 = \frac{1}{\sqrt{2}}[0 \ 1 \ -1 \ 0]^{\tr}\) is an orthonormal basis for \(\R^4\) and we can let%
\begin{equation*}
U = \left[  \begin{array}{crrr} \frac{1}{2} \amp  \frac{1}{2} \amp  \frac{1}{\sqrt{2}} \amp  0 \\ \frac{1}{2} \amp  -\frac{1}{2} \amp  0 \amp  \frac{1}{\sqrt{2}} \\  \frac{1}{2} \amp  -\frac{1}{2} \amp  0 \amp  -\frac{1}{\sqrt{2}} \\ \frac{1}{2} \amp  \frac{1}{2} \amp  -\frac{1}{\sqrt{2}} \amp  0 \end{array}  \right]\text{.}
\end{equation*}
The singular values of \(A\) are \(\sigma_1 = \sqrt{\lambda_1} = 12\) and \(\sigma_2 = \sqrt{\lambda_2} = 6\), and so%
\begin{equation*}
\Sigma = \begin{bmatrix}12\amp 0\amp 0 \\ 0\amp 6\amp 0 \\0\amp 0\amp 0 \\ 0\amp 0\amp 0 \end{bmatrix}\text{.}
\end{equation*}
Therefore, a singular value decomposition of \(A\) is \(U \Sigma V^{\tr}\) of%
\begin{equation*}
\left[ \begin{array}{crrr} \frac{1}{2} \amp  \frac{1}{2} \amp  \frac{1}{\sqrt{2}} \amp  0 \\ \frac{1}{2} \amp  -\frac{1}{2} \amp  0 \amp  \frac{1}{\sqrt{2}} \\  \frac{1}{2} \amp  -\frac{1}{2} \amp  0 \amp  -\frac{1}{\sqrt{2}} \\ \frac{1}{2} \amp  \frac{1}{2} \amp  -\frac{1}{\sqrt{2}} \amp  0 \end{array}  \right] \begin{bmatrix}12\amp 0\amp 0 \\ 0\amp 6\amp 0 \\0\amp 0\amp 0 \\ 0\amp 0\amp 0 \end{bmatrix}  \left[ \begin{array}{rrc} \frac{2}{3}\amp \frac{2}{3}\amp \frac{1}{3}\\ -\frac{2}{3}\amp \frac{1}{3}\amp \frac{2}{3}\\  \frac{1}{3}\amp -\frac{2}{3}\amp \frac{2}{3} \end{array}  \right]\text{.}
\end{equation*}
%
\item{}Determine the best rank 1 approximation to \(A\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp22197256}{}\quad{}Determine the best rank 1 approximation to \(A\). The outer product decomposition of \(A\) is%
\begin{equation*}
A = \sigma_1 \vu_1 \vv_1^{\tr} + \sigma_2 \vu_2 \vv_2^{\tr}\text{.}
\end{equation*}
So the rank one approximation to \(A\) is%
\begin{equation*}
\sigma_1 \vu_1 \vv_1^{\tr} = 12 \left(\frac{1}{2}\right) \left[ \begin{array}{c} 1\\1\\1\\1 \end{array}  \right] \left[ \begin{array}{ccc} \frac{2}{3} \amp  \frac{2}{3} \amp  \frac{1}{3} \end{array}  \right] = \left[ \begin{array}{ccc} 4\amp 4\amp 2\\ 4\amp 4\amp 2 \\ 4\amp 4\amp 2\\ 4\amp 4\amp 2 \end{array}   \right]\text{.}
\end{equation*}
Note that the rows in this rank one approximation are the averages of the two distinct rows in the matrix \(A\), which makes sense considering that this is the closest rank one matrix to \(A\).%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_svd_summ}
We learned about the singular value decomposition of a matrix.%
%
\begin{itemize}[label=\textbullet]
\item{}The operator norm of an \(m \times n\) matrix \(A\) is%
\begin{equation*}
||A|| = \max_{||\vx|| \neq 0} \frac{||A\vx||}{||\vx||} = \max_{||\vx||=1} ||A\vx||\text{.}
\end{equation*}
The operator norm of a matrix tells us that how big the action of an \(m \times n\) matrix is can be determined by its action on the unit sphere in \(\R^n\).%
\item{}A singular value decomposition of an \(m \times n\) matrix is of the form \(A = U \Sigma V^{\tr}\), where%
%
\begin{itemize}[label=$\circ$]
\item{}\(V= [\vv_1 \ \vv_2 \ \vv_3 \ \cdots \ \vv_n ]\) where \(\{\vv_1, \vv_2, \vv_3, \ldots, \vv_n\}\) is an orthonormal basis of eigenvectors of \(A^{\tr}A\) such that \((A^{\tr}A) \vv_i = \lambda_i \vv_i\) for \(i\) from 1 to \(n\) with \(\lambda_1 \geq \lambda_{2} \geq \cdots \geq \lambda_n \geq 0\),%
\item{}\(U = [\vu_1 \ \vu_2 \ \cdots \ \vu_m]\) where \(\vu_i = \frac{A\vv_i}{||A\vv_i||}\) for \(i\) from 1 to \(r\), and this orthonormal basis of \(\Col A\) is extended to an orthonormal basis \(\{\vu_1, \vu_2, \ldots, \vu_r, \vu_{r+1} \vu_{r+2}, \ldots, \vu_m\}\) of \(\R^m\),%
\item{}\(\Sigma = \left[ \begin{array}{ccccc|c} \sigma_1\amp \amp \amp \amp 0\amp \\ \amp \sigma_2\amp \amp \amp \amp 0 \\ \amp \amp \sigma_3\amp \amp \amp \\ \amp \amp \amp \ddots \amp \amp \\ 0\amp \amp \amp \amp \sigma_r \\ \hline \amp \amp 0\amp \amp \amp 0 \end{array} \right]\), where \(\sigma_i = \sqrt{\lambda_i} > 0\) for \(i\) from 1 to \(r\).%
\par
A singular value decomposition is important in that every matrix has a singular value decomposition, and a singular value decomposition has a variety of applications including scientific computing and digital signal processing, image compression, principal component analysis, web searching through latent semantic indexing, and seismology.%
\end{itemize}
\item{}The vectors \(\vu_1\), \(\vu_2\), \(\ldots\), \(\vu_r\) in an SVD for an \(m \times n\) matrix \(A\) form a basis for \(\Col A\) while the vectors \(\vv_{r+1}\), \(\vv_{r+2}\), \(\ldots\), \(\vv_n\) form a basis for \(\Nul A\). Also, the set \(\{\vv_1, \vv_2, \ldots, \vv_r\}\) is a basis for \(\Row A\).%
\item{}Let \(A\) have an SVD as in the second bullet. Decomposing \(A\) as%
\begin{equation*}
A =\sigma_1 \vu_1\vv_1^{\tr} + \sigma_2 \vu_2\vv_2^{\tr} + \sigma_3 \vu_3\vv_3^{\tr} + \cdots + \sigma_r \vu_r\vv_r^{\tr}
\end{equation*}
is an outer product decomposition of \(A\). An outer product decomposition allows us to approximate \(A\) with smaller rank matrices. For example, the matrix \(\sigma_1 \vu_1\vv_1^{\tr}\) is the best rank 1 approximation to \(A\), \(\sigma_1 \vu_1\vv_1^{\tr} + \sigma_2 \vu_2\vv_2^{\tr}\) is the best rank 2 approximation, and so on.%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_svd_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp22237592}%
Find a singular value decomposition of the following matrices.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\left[ \begin{array}{cc} 1\amp 1\\0\amp 0 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{c} 1\\0\\1 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{ccc} 1\amp 1\amp 0\\1\amp 0\amp 1 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{cc} 1\amp 2\\2\amp 1\\3\amp 1\\1\amp 3 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{cccc} 2\amp 0\amp 0\amp 0\\0\amp 2\amp 1\amp 0\\0\amp 1\amp 2\amp 0 \end{array} \right]\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp22256792}%
Let \(A\) be an \(m \times n\) matrix of rank \(r\) with singular value decomposition \(U \Sigma V^{\tr}\), where \(U = [ \vu_1 \ \vu_2 \ \cdots \ \vu_m ]\) and \(V = [\vv_1 \ \vv_2 \ \cdots \ \vv_n]\). We have seen that the set \(\{ \vu_1, \vu_2, \ldots, \vu_r\}\) is a basis for \(\Col A\), and the vectors \(\vv_{r+1}\), \(\vv_{r+2}\), \(\ldots\), \(\vv_n\) form a basis for \(\Nul A\). In this exercise we examine the set \(\{\vv_1, \vv_2, \ldots, \vv_r\}\) and determine what this set tells us about \(\Row A\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find a singular value decomposition for \(A^{\tr}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp22266520}{}\quad{}Use the singular value decomposition \(U \Sigma V^{\tr}\) for \(A\).%
\item{}Explain why the result of (a) shows that the set \(\{\vv_1, \vv_2, \ldots, \vv_r\}\) is a basis for \(\Row A\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp22266648}%
Let \(A = \left[ \begin{array}{cc} 1\amp 1 \\ 2\amp 2 \\ 3\amp 3 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the singular values of \(A\).%
\item{}Find a singular value decomposition of \(A\).%
\item{}Use a singular value decomposition to find orthonormal bases for the following:%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}\(\Nul A\)%
\item{}\(\Col A\)%
\item{}\(\Row A\)%
\end{enumerate}
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp22267800}%
Let \(A\) have the singular value decomposition as in \hyperref[x:mrow:eq_7_c_SVD]{({\xreffont\ref{x:mrow:eq_7_c_SVD}})}.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show, using \hyperref[x:mrow:eq_7_c_SVD]{({\xreffont\ref{x:mrow:eq_7_c_SVD}})}, that \(||A\vv_j|| = \sigma_j\).%
\item{}Explain why \(||A|| = \sigma_1\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp22279960}{}\quad{}The set \(\{\vv_1, \vv_2, \ldots, \vv_n\}\) is an orthonormal basis of \(\R^n\). Use this to show that \(||A \vx||^2 \leq \sigma_1^2\) for any unit vector \(\vx\) in \(\R^n\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp22282520}%
Show that \(A\) and \(A^{\tr}\) have the same nonzero singular values. How are their singular value decompositions related?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp22279704}{}\quad{}Find the transpose of an SVD for \(A\).%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{x:exercise:ex_7_c_AAT}%
The vectors \(\vv_i\) that form the columns of the matrix \(V\) in a singular value decomposition of a matrix \(A\) are eigenvectors of \(A^{\tr}A\). In this exercise we investigate the vectors \(\vu_i\) that make up the columns of the matrix \(U\) in a singular value decomposition of a matrix \(A\) for each \(i\) between 1 and the rank of \(A\), and their connection to the matrix \(AA^{\tr}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\textgreater{} Let \(A = \left[ \begin{array}{ccr} 1\amp 1\amp 0 \\ 0\amp 1\amp -1 \end{array}  \right]\). A singular value decomposition of \(A\) is \(U \Sigma V^{\tr}\), where%
\begin{align*}
U \amp = \frac{1}{\sqrt{2}}\left[ \begin{array}{rr} -1\amp 1\\
-1\amp -1 \end{array} \right],\\
\Sigma \amp = \left[ \begin{array}{ccc} \sqrt{3}\amp 0\amp 0\\
0\amp 1\amp 0 \end{array} \right],\\
V \amp = \left[ \begin{array}{rcc} -\frac{1}{\sqrt{6}}\amp -\frac{1}{3\sqrt{6}}\amp \frac{1}{\sqrt{6}}\\
\frac{1}{\sqrt{2}}\amp 0\amp \frac{1}{\sqrt{2}}\\
-\frac{1}{\sqrt{3}} \amp  \frac{1}{\sqrt{3}} \amp  \frac{1}{\sqrt{3}}\end{array} \right]\text{.}
\end{align*}
%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Determine the rank \(r\) of \(A^{\tr}A\) and identify the vectors \(\vu_1\), \(\vu_2\), \(\ldots\), \(\vu_r\).%
\item{}Calculate \(AA^{\tr} \vu_i\) for each \(i\) between 1 and \(r\). How is \(AA^{\tr} \vu_i\) related to \(\vu_i\)?%
\end{enumerate}
\item{}Now we examine the result of part (a) in general. Let \(A\) be an arbitrary matrix. Calculate \(AA^{\tr} \vu_i\) for \(1 \leq i \leq \rank(A)\) and determine specifically how \(AA^{\tr} \vu_i\) is related to \(\vu_i\). What does this tell us about the vectors \(\vu_i\) and the matrix \(AA^{\tr}\)?%
\item{}Now show in general that the columns of \(U\) are orthonormal eigenvectors for \(AA^{\tr}\). (That is, what can we say about the vectors \(\vu_i\) if \(i > \rank(A)\)?)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp22307224}%
If \(A\) is a symmetric matrix with eigenvalues \(\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n \geq 0\), what is \(||A||\)? Justify your answer.%
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{x:exercise:ex_7_c_Symmetric_SVD}%
Let \(A\) be a \(n \times n\) symmetric matrix.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that if \(\vv\) is an eigenvector of \(A\) with eigenvalue \(\lambda\), then \(\vv\) is an eigenvector for \(A^{\tr}A\). What is the corresponding eigenvalue?%
\item{}Show that if \(\vv\) is an eigenvector of \(A^{\tr}A\) with non-negative eigenvalue \(\lambda\), then \(A\vv\) is an eigenvector of \(A^{\tr}A\). What is the corresponding eigenvalue?%
\item{}Suppose \(U \Sigma V^{\tr}\) is a singular value decomposition of \(A\). Explain why \(V \Sigma V^{\tr}\) is also a singular value decomposition of \(A\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{x:exercise:ex_7_c_rank1}%
Let \(\vu_1\), \(\vu_2\), \(\ldots\), \(\vu_r\) and \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_r\) be the vectors found in a singular value decomposition of a matrix \(A\), where \(r\) is the rank of \(A\). Show that \(\vu_i\vv_i^{\tr}\) is a rank 1 matrix for each \(i\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp22324376}{}\quad{}Mimic \hyperlink{x:exercise:ex_7_a_spectral_decomposition}{Exercise~{\xreffont 5}} in \hyperref[x:chapter:chap_orthogonal_diagonalization]{Section~{\xreffont\ref{x:chapter:chap_orthogonal_diagonalization}}}.%
\end{divisionexercise}%
\begin{divisionexercise}{10}{}{}{g:exercise:idp22319256}%
Is it possible for a matrix \(A\) to have a singular value decomposition \(U \Sigma V^{\tr}\) in which \(U = V\)? If no, explain why. If yes, determine for which matrices we can have \(U = V\).%
\end{divisionexercise}%
\begin{divisionexercise}{11}{}{}{g:exercise:idp22323736}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
If \(\sigma\) is a singular value of a matrix \(A\), then \(\sigma\) is an eigenvalue of \(A^{\tr}A\).%
\item{}\lititle{True\slash{}False.}\par%
A set of right singular vectors of a matrix \(A\) is also a set of left singular vectors of \(A^{\tr}\).%
\item{}\lititle{True\slash{}False.}\par%
The transpose of a singular value decomposition of a matrix \(A\) is a singular value decomposition for \(A^{\tr}\).%
\item{}\lititle{True\slash{}False.}\par%
Similar matrices have the same singular values.%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is an \(n \times n\) matrix, then the singular values of \(A^2\) are the squares of the singular values of \(A\).%
\item{}\lititle{True\slash{}False.}\par%
The \(\Sigma\) matrix in an SVD of \(A\) is unique.%
\item{}\lititle{True\slash{}False.}\par%
The matrices \(U, V\) in an SVD of \(A\) are unique.%
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is a positive definite matrix, then an orthogonal diagonalization of \(A\) is an SVD of \(A\).%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Latent Semantic Indexing}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Latent Semantic Indexing}{}{Project: Latent Semantic Indexing}{}{}{x:section:sec_proj_indexing}
As an elementary example to illustrate the idea behind Latent Semantic Indexing (LSI), consider the problem of creating a program to search a collection of documents for words, or words related to a given word. Document collections are usually very large, but we use a small example for illustrative purposes. A standard example that is given in several publications\footnote{e.g., Deerwester, S., Dumais, S. T., Fumas, G. W., Landauer, T. K. and Harshman, R. Indexing by latent semantic analysis. \pubtitle{Journal of the American Society for Information Science}, 1990, 41: 391\textendash{}407, and Landauer, T. and Dutnais, S. A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge. \pubtitle{Psychological Review}, 1997. Vol. 1M. No. 2, 211-240.\label{g:fn:idp22337432}} is the following. Suppose we have nine documents \(c_1\) through \(c_5\) (titles of documents about human-computer interaction) and \(m_1\) through \(m_4\) (titles of graph theory papers) that make up our library:%
\begin{itemize}[label=\textbullet]
\item{}\(c_1\): \emph{Human} machine \emph{interface} for ABC \emph{computer} applications%
\item{}\(c_2\): A \emph{survey} of \emph{user} opinion of \emph{computer system response time}%
\item{}\(c_3\): The \emph{EPS user interface} management \emph{system}%
\item{}\(c_4\): \emph{System} and \emph{human system} engineering testing of \emph{EPS}%
\item{}\(c_5\): Relation of \emph{user} perceived \emph{response time} to error measurement%
\item{}\(m_1\): The generation of random, binary, ordered \emph{trees}%
\item{}\(m_2\): The intersection \emph{graph} of paths in \emph{trees}%
\item{}\(m_3\): \emph{Graph minors} IV: Widths of \emph{trees} and well-quasi-ordering%
\item{}\(m_4\): \emph{Graph minors}: A \emph{survey}%
\end{itemize}
%
\par
To make a searchable database, one might start by creating a list of key terms that appear in the documents (generally removing common words such as ``a'', ``the'', ``of'', etc., called \terminology{stop words} \textemdash{} these words contribute little, if any, context). In our documents we identify the key words that are shown in italics. (Note that we are just selecting key words to make our example manageable, not necessarily identifying the most important words.) Using the key words we create a \terminology{term-document} matrix. The term-document matrix is the matrix in which the terms form the rows and the documents the columns. If \(A = [a_{ij}]\) is the term-document matrix, then \(a_{ij}\) counts the number of times word \(i\) appears in document \(j\). The term-document matrix \(A\) for our library is \begin{figureptx}{Term-document matrix for our library}{x:figure:fig_LSI_A}{}%
\begin{image}{0.25}{0.5}{0.25}%
\includegraphics[width=\linewidth]{external/term-doc-mtx.pdf}
\end{image}%
\tcblower
\end{figureptx}%
  %
\par
One of our goals is to rate the pages in our library for relevance if we search for a query. For example, suppose we want to rate the pages for the query \emph{survey, computer}. This query can be represented by the vector \(\vq= [0 \ 0 \ 1 \ 0 \ 0 \ 0 \ 0 \ 0 \ 1 \ 0 \ 0 \ 0 ]^{\tr}\).%
\begin{project}{}{x:project:act_LSI_standard}%
In a standard term-matching search with \(m \times n\) term-document matrix \(A\), a query vector \(\vq\) would be matched with the terms to determine the number of matches. The matching counts the number of times each document agrees with the query.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why this matching is accomplished by the matrix-vector product \(A^{\tr} \vq\).%
\item{}Let \(\vy = [y_1 \ y_2 \ \ldots \ y_n]^{\tr} = A^{\tr} \vq\). Explain why \(y_i = \cos(\theta_i) ||\va_i|| ||\vq||\), where \(\va_i\) is the \(i\)th column of \(A\) and \(\theta_i\) is the angle between \(\va_i\) and \(\vq\).%
\item{}We can use the cosine calculation from part (b) to compare matches to our query \textemdash{} the closer the cosine is to \(1\), the better the match (dividing by the product of the norms is essentially converting all vectors to unit vectors for comparison purposes). This is often referred to as the cosine distance. Calculate the cosines of the \(\theta_i\) for our example of the query \(\vq= [0 \ 0 \ 1 \ 0  \ 0 \ 0 \ 0 \ 0 \ 1 \ 0 \ 0 \ 0 ]^{\tr}\). Order the documents from best to worst match for this query.%
\end{enumerate}
\end{project}%
Though we were able to rate the documents in \hyperref[x:project:act_LSI_standard]{Project Activity~{\xreffont\ref{x:project:act_LSI_standard}}} using the cosine distance, the result is less than satisfying. Documents \(c_3\), \(c_4\), and \(c_5\) are all related to computers but do not appear at all in or results. This is a problem with language searches \textemdash{} we don't want to compare just words, but we also need to compare the concepts the words represent. The fact that words can represent different things implies that a random choice of word by different authors can introduce noise into the word-concept relationship. To filter out this noise, we can apply the singular value decomposition to find a smaller set of concepts to better represent the relationships. Before we do so, we examine some useful properties of the term-document matrix.%
\begin{project}{}{x:project:act_LSI_tt_dd}%
Let \(A = [ \va_1 \ \va_2 \ \cdots \ \va_9]\), where \(\va_1\), \(\va_2\), \(\ldots\), \(\va_9\) are the columns of \(A\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}In \hyperref[x:project:act_LSI_standard]{Project Activity~{\xreffont\ref{x:project:act_LSI_standard}}} you should have seen that \(b_{ij} = \va_i^{\tr} \va_j = \va_i \cdot \va_j\). Assume for the moment that all of the entries in \(A\) are either \(0\) or \(1\). Explain why in this case the dot product \(\va_i \cdot \va_j\) tells us how many terms documents \(i\) and \(j\) have in common. Also, the matrix \(A^{\tr}A\) takes dot products of the columns of \(A\), which refer to what's happening in each document and so is looking at document-document interactions. For these reasons, we call \(A^{\tr}A\) the document-document matrix.%
\item{}Use appropriate technology to calculate the entries of the matrix \(C = [c_{ij}] = AA^{\tr}\). This matrix is the term-term matrix. Assume for the moment that all of the entries in \(A\) are either \(0\) or \(1\). Explain why if terms \(i\) and \(j\) occur together in \(k\) documents, then \(c_{ij} =k\).%
\end{enumerate}
\end{project}%
The nature of the term-term and document-document matrices makes it realistic to think about a SVD.%
\begin{project}{}{x:project:act_LSI_ATA_AAT}%
To see why a singular value decomposition might be useful, suppose our term-document matrix \(A\) has singular value decomposition \(A = U \Sigma V^{\tr}\). (Don't actually calculate the SVD yet).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that the document-document matrix \(A^{\tr}A\) satisfies \(A^{\tr}A = \left(V \Sigma^{\tr}\right) \left(V\Sigma^{\tr}\right)^{\tr}\). This means that we can compare document \(i\) and document \(j\) using the dot product of row \(i\) and column \(j\) of the matrix product \(V\Sigma^{\tr}\).%
\item{}Show that the term-term matrix \(AA^{\tr}\) satisfies \(AA^{\tr} = \left(U \Sigma\right) \left(U\Sigma\right)^{\tr}\). Thus we can compare term \(i\) and term \(j\) using the dot product of row \(i\) and column \(j\) of the matrix product \(U\Sigma\). (\hyperlink{x:exercise:ex_7_c_AAT}{Exercise~{\xreffont 6}} shows that the columns of \(U\) are orthogonal eigenvectors of \(AA^{\tr}\).)%
\end{enumerate}
\end{project}%
As we will see, the connection of the matrices \(U\) and \(V\) to documents and terms that we saw in \hyperref[x:project:act_LSI_ATA_AAT]{Project Activity~{\xreffont\ref{x:project:act_LSI_ATA_AAT}}} will be very useful when we use the SVD of the term-document matrix to reduce dimensions to a ``concept'' space. We will be able to interpret the rows of the matrices \(U\) and \(V\) as providing coordinates for terms and documents in this space.%
\begin{project}{}{x:project:act_LSI_SVD}%
The singular value decomposition (SVD) allows us to produce new, improved term-document matrices. For this activity, use the term-document matrix \(A\) in \hyperref[x:figure:fig_LSI_A]{Figure~{\xreffont\ref{x:figure:fig_LSI_A}}}.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Use appropriate technology to find a singular value decomposition of \(A\) so that \(A = U \Sigma V^{\tr}\). Print your entries to two decimal places (but keep as many as possible for computational purposes).%
Each singular value tells us how important its semantic dimension is. If we remove the smaller singular values (the less important dimensions), we retain the important information but eliminate minor details and noise. We produce a new term-document matrix \(A_k\) by keeping the largest \(k\) of the singular values and discarding the rest. This gives us an approximation%
\begin{equation*}
A_k = \sigma_1 \vu_1\vv_1^{\tr} + \sigma_2 \vu_2\vv_2^{\tr} + \cdots + \sigma_k \vu_k\vv_k^{\tr}
\end{equation*}
using the outer product decomposition, where \(\sigma_1\), \(\sigma_2\), \(\ldots\), \(\sigma_k\) are the \(k\) largest singular values of \(A\). Note that if \(A\) is an \(m \times n\) matrix, letting \(U_k = [\vu_1 \ \vu_2 \ \cdots \ \vu_k]\) (an \(m \times k\) matrix), \(\Sigma_k\) the \(k \times k\) matrix with the first \(k\) singular values along the diagonal, and \(V^{\tr} = [\vv_1 \ \vv_2 \ \cdots \ \vv_k ]^{\tr}\) (a \(k \times n\) matrix), then we can also write \(A_k = U_k \Sigma_k V_k^{\tr}\). This is sometimes referred to as a reduced SVD.  Find \(U_2\), \(\Sigma_2\), and \(V_2^{\tr}\), and find the new term-document matrix \(A_2\).%
\end{enumerate}
\end{project}%
Once we have our term-document matrix, there are three basic comparisons to make: comparing terms, comparing documents, and comparing terms and documents. Term-document matrices are usually very large, with dimension being the number of terms. By using a reduced SVD we can create a much smaller approximation. In our example, the matrix \(A_k\) in \hyperref[x:project:act_LSI_SVD]{Project Activity~{\xreffont\ref{x:project:act_LSI_SVD}}} reduces our problem to a \(k\)-dimensional space. Intuitively, we can think of LSI as representing terms as averages of all of the documents in which they appear and documents as averages of all of the terms they contain. Through this process, LSI attempts to combine the surface information in our library into a deeper abstraction (the ``concept'' space) that captures the mutual relationships between terms and documents.%
\par
We now need to understand how we can represent documents and terms in this smaller space where \(A_k = U_k \Sigma_k V_k^{\tr}\). Informally, we can consider the rows of \(U_k\) as representing the coordinates of each term in the lower dimensional concept space and the columns of \(V_k^{\tr}\) as the coordinates of the documents, while the entries of \(\Sigma_k\) tell us how important each semantic dimension is. The dot product of two row vectors of \(A_k\) indicates how terms compare across documents. This product is \(A_kA_k^{\tr}\). Just as in \hyperref[x:project:act_LSI_ATA_AAT]{Project Activity~{\xreffont\ref{x:project:act_LSI_ATA_AAT}}}, we have \(A_kA_k^{\tr} = \left(U_k \Sigma_k\right) \left(U_k\Sigma_k\right)^{\tr}\). In other words, if we consider the rows of \(U_k\Sigma_k\) as coordinates for terms, then the dot products of these rows give us term to term comparisons. (Note that multiplying \(U\) by \(\Sigma\) just stretches the rows of \(U\) by the singular values according to the importance of the concept represented by that singular value.) Similarly, the dot product between columns of \(A\) provide a comparison of documents. This comparison is given by \(A_k^{\tr}A_k^ = \left(V_k \Sigma_k^{\tr}\right) \left(V_k\Sigma_k^{\tr}\right)^{\tr}\) (again by \hyperref[x:project:act_LSI_ATA_AAT]{Project Activity~{\xreffont\ref{x:project:act_LSI_ATA_AAT}}}). So we can consider the rows of \(V\Sigma^{\tr}\) as providing coordinates for documents.%
\begin{project}{}{x:project:act_LSI_term_document}%
We have seen how to compare terms to terms and documents to documents. The matrix \(A_k\) itself compares terms to documents. Show that \(A_k = \left(U_k \Sigma_k^{1/2}\right) \left(V_k\Sigma_k^{1/2}\right)^{\tr}\), where \(\Sigma_k^{1/2}\) is the diagonal matrix of the same size as \(\Sigma_k\) whose diagonal entries are the square roots of the corresponding diagonal entries in \(\Sigma_k\). Thus, all useful comparisons of terms and documents can be made using the rows of the matrices \(U\) and \(V\), scaled in some way by the singular values in \(\Sigma\).%
\end{project}%
To work in this smaller concept space, it is important to be able to find appropriate comparisons to objects that appeared in the original search. For example, to complete the latent structure view of the system, we must also convert the original query to a representation within the new term-document system represented by \(A_k\). This new representation is called a \terminology{pseudo-document}. \begin{figureptx}{Terms in the reduced concept space}{x:figure:fig-term-reduced}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/term-reduced.pdf}
\end{image}%
\tcblower
\end{figureptx}%
 \begin{figureptx}{Documents in the reduced concept space}{x:figure:fig-doc-reduced}{}%
\begin{image}{0.15}{0.7}{0.15}%
\includegraphics[width=\linewidth]{external/doc-reduced.pdf}
\end{image}%
\tcblower
\end{figureptx}%
  %
\begin{project}{}{x:project:act_LSI_concept_space}%
For an original query \(\vq\), we start with its term vector \(\va_{\vq}\) (a vector in the coordinate system determined by the columns of \(A\)) and find a representation \(\vv_{\vq}\) that we can use as a column of \(V^{\tr}\) in the document-document comparison matrix. If this representation was perfect, then it would take a real document in the original system given by \(A\) and produce the corresponding column of \(U\) if we used the full SVD. In other words, we would have \(\va_{\vq} = U \Sigma \vv_{\vq}^{\tr}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Use the fact that \(A_k = U_k \Sigma_k V_k^{\tr}\), to show that \(V_k = A_k^{\tr} U_k \Sigma_k^{-1}\). It follows that \(\vq\) is transformed into the query \(\vq_k = \vq^{\tr}U_k \Sigma_k^{-1}\).%
\item{}In our example, using \(k=2\), the terms can now be represented as \(2\)-dimensional vectors (the rows of \(U_2\), see \hyperref[x:figure:fig-term-reduced]{Figure~{\xreffont\ref{x:figure:fig-term-reduced}}}), or as points in the plane. More specifically, \emph{human} is represented by the vector (to two decimal places) \([-0.22 \ -0.11]^{\tr}\), \emph{interface} by \([- 0.20 \ - 0.07]^{\tr}\), etc. Similarly, the documents are represented by columns of \(V_2\) (see \hyperref[x:figure:fig-doc-reduced]{Figure~{\xreffont\ref{x:figure:fig-doc-reduced}}}), so that the document \(c_1\) is represented by \([-0.20 \ -0.06]^{\tr}\), \(c_2\) by \([-0.61 \ 0.17]^{\tr}\), etc. From this perspective we can visualize these documents in the plane. Plot the documents and the query in the 2-dimensional concept space. Then calculate the cosine distances from the query to the documents in this space. Which documents now give the best three matches to the query? Compare the matches to your plot.%
\end{enumerate}
\end{project}%
As we can see from \hyperref[x:project:act_LSI_concept_space]{Project Activity~{\xreffont\ref{x:project:act_LSI_concept_space}}}, the original query had no match at all with any documents except \(c_1\), \(c_2\), and \(m_4\). In the new concept space, the query now has some connection to every document,. So LSI has made semantic connections between the terms and documents that were not present in the original term-document matrix, which gives us better results for our search.%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 30 Using the Singular Value Decomposition}
\typeout{************************************************}
%
\begin{chapterptx}{Using the Singular Value Decomposition}{}{Using the Singular Value Decomposition}{}{}{x:chapter:chap_pseudoinverses}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp22442392}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What is the condition number of a matrix and what does it tell us about the matrix?%
\item{}What is the pseudoinverse of a matrix?%
\item{}Why are pseudoinverses useful?%
\item{}How does the pseudoinverse of a matrix allow us to find least squares solutions to linear systems?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Global Positioning System}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Global Positioning System}{}{Application: Global Positioning System}{}{}{x:section:sec_appl_gps}
You are probably familiar with the Global Positioning System (GPS). The system allows anyone with the appropriate software to accurately determine their location at any time. The applications are almost endless, including getting real-time driving directions while in your car, guiding missiles, and providing distances on golf courses.%
\par
The GPS is a worldwide radio-navigation system owned by the US government and operated by the US Air Force. GPS is one of four global navigation satellite systems. At least twenty four GPS satellites orbit the Earth at an altitude of approximately 11,000 nautical miles. The satellites are placed so that at any time at least four of them can be accessed by a GPS receiver. Each satellite carries an atomic clock to relay a time stamp along with its position in space. There are five ground stations to coordinate and ensure that the system is working properly.%
\par
The system works by triangulation, but there is also error involved in the measurements that go into determining position. Later in this section we will see how the method of least squares can be used to determine the receiver's position.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_pseudo_intro}
A singular value decomposition has many applications, and in this section we discuss how a singular value decomposition can be used in image compression, to determine how sensitive a matrix can be to rounding errors in the process of row reduction, and to solve least squares problems.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Image Compression}
\typeout{************************************************}
%
\begin{sectionptx}{Image Compression}{}{Image Compression}{}{}{x:section:sec_img_conpress}
The digital age has brought many new opportunities for the collection, analysis, and dissemination of information. Along with these opportunities come new difficulties as well. All of this digital information must be stored in some way and be retrievable in an efficient manner. A singular value decomposition of digitally stored information can be used to compress the information or clean up corrupted information. In this section we will see how a singular value decomposition can be used in image compression. While a singular value decomposition is normally used with very large matrices, we will restrict ourselves to small examples so that we can more clearly see how a singular value decomposition is applied.%
\begin{exploration}{}{x:exploration:pa_7_d_1}%
Let \(A = \frac{1}{4}\left[ \begin{array}{ccrr} 67\amp 29\amp -31\amp -73 \\ 29\amp 67\amp -73\amp -31 \\ 31\amp 73\amp -67\amp -29  \\ 73\amp 31\amp -29\amp -67 \end{array}  \right]\). A singular value decomposition for \(A\) is \(U \Sigma V^{\tr}\), where%
\begin{align*}
U \amp = [\vu_1 \ \vu_2 \ \vu_3 \ \vu_4] = \frac{1}{2} \left[ \begin{array}{crrr} 1\amp -1\amp 1\amp -1\\
1\amp 1\amp 1\amp 1\\
1\amp 1\amp -1\amp -1\\
1\amp -1\amp -1\amp 1 \end{array} \right],\\
\Sigma \amp =  \left[ \begin{array}{cccc} 50\amp 0\amp 0\amp 0\\
0\amp 20\amp 0\amp 0\\
0\amp 0\amp 2\amp 0\\
0\amp 0\amp 0\amp 1 \end{array} \right],\\
V \amp = [\vv_1 \ \vv_2 \ \vv_3 \ \vv_4] = \frac{1}{2} \left[ \begin{array}{rrrr} 1\amp 1\amp -1\amp 1\\
1\amp -1\amp -1\amp -1\\
-1\amp 1\amp -1\amp -1\\
-1\amp -1\amp -1\amp 1 \end{array} \right]\text{.}
\end{align*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Write the summands in the corresponding outer product decomposition of \(A\).%
\item{}The outer product decomposition of \(A\) writes \(A\) as a sum of rank 1 matrices (the summands \(\sigma_i \vu_i \vv_i^{\tr})\). Each summand contains some information about the matrix \(A\). Since \(\sigma_1\) is the largest of the singular values, it is reasonable to expect that the summand \(A_1 = \sigma_1 \vu_1  \vv_1^{\tr}\) contains the most information about \(A\) among all of the summands. To get a measure of how much information \(A_1\) contains of \(A\), we can think of \(A\) as simply a long vector in \(\R^{mn}\) where we have folded the data into a rectangular array (we will see later why taking the norm as the norm of the vector in \(\R^{nm}\) makes sense, but for now, just use this definition). If we are interested in determining the error in approximating an image by a compressed image, it makes sense to use the standard norm in \(\R^{mn}\) to determine length and distance, which is really just the Frobenius norm that comes from the Frobenius inner product defined by%
\begin{equation}
\langle U,V \rangle = \sum u_{ij}v_{ij}\text{,}\label{x:men:eq_7_d_Frobenius_ip}
\end{equation}
where \(U = [u_{ij}]\) and \(V = [v_{ij}]\) are \(m \times n\) matrices. (That \hyperref[x:men:eq_7_d_Frobenius_ip]{({\xreffont\ref{x:men:eq_7_d_Frobenius_ip}})} defines an inner product on the set of all \(n \times n\) matrices is left to discuss in a later section.) So in this section all the norms for matrices will refer to the Frobenius norm. Rather than computing the distance between \(A_1\) and \(A\) to measure the error, we are more interested in the relative error%
\begin{equation*}
\frac{||A-A_1||}{||A||}\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Calculate the relative error in approximating \(A\) by \(A_1\). What does this tell us about how much information \(A_1\) contains about \(A\)?%
\item{}Let \(A_2 = \sum_{k=1}^2 \sigma_k \vu_k \vv_k^{\tr}\). Calculate the relative error in approximating \(A\) by \(A_2\). What does this tell us about how much information \(A_2\) contains about \(A\)?%
\item{}Let \(A_3 = \sum_{k=1}^3 \sigma_k \vu_k \vv_k^{\tr}\). Calculate the relative error in approximating \(A\) by \(A_3\). What does this tell us about how much information \(A_3\) contains about \(A\)?%
\item{}Let \(A_4 = \sum_{k=1}^4 \sigma_k \vu_k \vv_k^{\tr}\). Calculate the relative error in approximating \(A\) by \(A_4\). What does this tell us about how much information \(A_4\) contains about \(A\)? Why?%
\end{enumerate}
\end{enumerate}
\end{exploration}%
The first step in compressing an image is to digitize the image. There are many ways to do this and we will consider one of the simplest ways and only work with gray-scale images, with the scale from 0 (black) to 255 (white). A digital image can be created by taking a small grid of squares (called pixels) and coloring each pixel with some shade of gray. The resolution of this grid is a measure of how many pixels are used per square inch. As an example, consider the 16 by 16 pixel picture of a flower shown in \hyperref[x:figure:Fig_7_c_Flower]{Figure~{\xreffont\ref{x:figure:Fig_7_c_Flower}}}.%
\begin{figureptx}{A 16 by 16 pixel image}{x:figure:Fig_7_c_Flower}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/7_c_Flower.pdf}
\end{image}%
\tcblower
\end{figureptx}%
To store this image pixel by pixel would require \(16 \times 16 = 256\) units of storage space (1 for each pixel). If we let \(M\) be the matrix whose \(i,j\)th entry is the scale of the \(i,j\)th pixel, then \(M\) is the matrix%
\begin{equation*}
\tiny{ \left[ \begin{array}{cccccccccccccccc} 240\amp 240\amp 240\amp 240\amp 130\amp 130\amp 240\amp 130\amp 130\amp 240\amp 240\amp 240\amp 240\amp 240\amp 240\amp 240\\ 240\amp 240\amp 240\amp 130\amp 175\amp 175\amp 130\amp 175\amp 175\amp 130\amp 240\amp 240\amp 240\amp 240\amp 240\amp 240 \\ 240\amp 240\amp 130\amp 130\amp 175\amp 175\amp 130\amp 175\amp 175\amp 130\amp 130\amp 240\amp 240\amp 240\amp 240\amp 240\\ 240\amp 130\amp 175\amp 175\amp 130\amp 175\amp 175\amp 175\amp 130\amp 175\amp 175\amp 130\amp 240\amp 240\amp 240\amp 240\\ 240\amp 240\amp 130\amp 175\amp 175\amp 130\amp 175\amp 130\amp 175\amp 175\amp 130\amp 240\amp 240\amp 240\amp 240\amp 240\\ 255\amp 240\amp 240\amp 130\amp 130\amp 175\amp 175\amp 175\amp 130\amp 130\amp 240\amp 240\amp 225\amp 240\amp 240\amp 240 \\ 240\amp 240\amp 130\amp 175\amp 175\amp 130\amp 130\amp 130\amp 175\amp 175\amp 130\amp 240\amp 225\amp 255\amp 240\amp 240\\ 240\amp 240\amp 130\amp 175\amp 130\amp 240\amp 130\amp 240\amp 130\amp 175\amp 130\amp 240\amp 255\amp 255\amp 255\amp 240\\ 240\amp 240\amp 240\amp 130\amp 240\amp 240\amp 75\amp 240\amp 240\amp 130\amp 240\amp 255\amp 255\amp 255\amp 255\amp 255\\ 240 \amp 240\amp 240\amp 240\amp 240\amp 240\amp 75\amp 240\amp 240\amp 240\amp 240\amp 240\amp 240\amp 240\amp 240\amp 240 \\ 240\amp 240\amp 240\amp 75\amp 75\amp 240\amp 75\amp 240\amp 75\amp 75\amp 240\amp 240\amp 240\amp 240\amp 240\amp 240\\ 50\amp 240\amp 240\amp 240\amp 75\amp 240\amp 75\amp 240\amp 75\amp 240\amp 240\amp 240\amp 240\amp 50\amp 240\amp 240\\ 240\amp 75\amp 240\amp 240\amp 240\amp 75\amp 75\amp 75 \amp 240\amp 240\amp 50\amp 240\amp 50\amp 240\amp 240\amp 50\\ 240\amp 240\amp 75\amp 240\amp 240\amp 240\amp 75\amp 240\amp 240\amp 50\amp 240\amp 50\amp 240\amp 240\amp 50\amp 240\\ 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\\ 75\amp 75\amp 75\amp 75 \amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75 \end{array}  \right]}\text{.}
\end{equation*}
%
\par
Recall that if \(U \Sigma V^{\tr}\) is a singular value decomposition for \(M\), then we can also write \(M\) in the form%
\begin{equation*}
M=\sigma_1 \vu_1\vv_1^{\tr} + \sigma_2 \vu_2\vv_2^{\tr} + \sigma_3 \vu_3\vv_3^{\tr} + \cdots + \sigma_{16} \vu_{16}\vv_{16}^{\tr}\text{.}
\end{equation*}
given in \hyperref[x:mrow:eq_7_c_SVD]{({\xreffont\ref{x:mrow:eq_7_c_SVD}})}. For this \(M\), the singular values are approximately%
\begin{equation}
\left[ \begin{array}{c}  3006.770088367795 \\ 439.13109000200205 \\ 382.1756550649652 \\ 312.1181752764884 \\ 254.45105800344953 \\ 203.36470770057494 \\ 152.8696215072527 \\ 101.29084240890717 \\ 63.80803769229468 \\ 39.6189181773536 \\ 17.091891798245463 \\ 12.304589419140656 \\ 4.729898943556077 \\ 2.828719409809012 \\ 6.94442317024232 \times 10^{-15} \\2.19689952047833 \times 10^{-15} \end{array}  \right]\text{.}\label{x:men:eq_7_c_Flower_sing_values}
\end{equation}
%
\par
Notice that some of these singular values are very small compared to others. As in \hyperref[x:exploration:pa_7_d_1]{Preview Activity~{\xreffont\ref{x:exploration:pa_7_d_1}}}, the terms with the largest singular values contain most of the information about the matrix. Thus, we shouldn't lose much information if we eliminate the small singular values. In this particular example, the last 4 singular values are significantly smaller than the rest. If we let%
\begin{equation*}
M_{12} = \sigma_1 \vu_1\vv_1^{\tr} + \sigma_2 \vu_2\vv_2^{\tr} + \sigma_3 \vu_3\vv_3^{\tr} + \cdots + \sigma_{12} \vu_{12}\vv_{12}^{\tr}\text{,}
\end{equation*}
then we should expect the image determined by \(M_{12}\) to be close to the image made by \(M\). The two images are presented side by side in \hyperref[x:figure:F_7_c_Compress]{Figure~{\xreffont\ref{x:figure:F_7_c_Compress}}}.%
\begin{figureptx}{A 16 by 16 pixel image and a compressed image using a singular value decomposition.}{x:figure:F_7_c_Compress}{}%
\begin{sidebyside}{2}{0.1}{0.1}{0.2}%
\begin{sbspanel}{0.3}%
\includegraphics[width=\linewidth]{external/7_c_Flower.pdf}
\end{sbspanel}%
\begin{sbspanel}{0.3}%
\includegraphics[width=\linewidth]{external/7_c_CFlower.pdf}
\end{sbspanel}%
\end{sidebyside}%
\tcblower
\end{figureptx}%
This small example illustrates the general idea. Suppose we had a satellite image that was \(1000 \times 1000\) pixels and we let \(M\) represent this image. If we have a singular value decomposition of this image \(M\), say%
\begin{equation*}
M = \sigma_1 \vu_1\vv_1^{\tr} + \sigma_2 \vu_2\vv_2^{\tr} + \sigma_3 \vu_3\vv_3^{\tr} + \cdots + \sigma_{r} \vu_{r}\vv_{r}^{\tr}\text{,}
\end{equation*}
if the rank of \(M\) is large, it is likely that many of the singular values will be very small. If we only keep \(s\) of the singular values, we can approximate \(M\) by%
\begin{equation*}
M_s = \sigma_1 \vu_1\vv_1^{\tr} + \sigma_2 \vu_2\vv_2^{\tr} + \sigma_3 \vu_3\vv_3^{\tr} + \cdots + \sigma_{s} \vu_{s}\vv_{s}^{\tr}
\end{equation*}
and store the image with only the vectors \(\sigma_1 \vu_1\), \(\sigma_2 \vu_2\), \(\ldots\), \(\sigma_{s}\vu_s\), \(\vv_1\), \(\vv_1\), \(\ldots\), \(\vv_s\). For example, if we only need 10 of the singular values of a satellite image (\(s = 10\)), then we can store the satellite image with only 20 vectors in \(\R^{1000}\) or with \(20 \times 1000 = 20,000\) numbers instead of \(1000 \times 1000 = 1,000,000\) numbers.%
\par
A similar process can be used to denoise data.\footnote{For example, as stated in \href{http://www2.imm.dtu.dk/\~pch/Projekter/tsvd.html}{\nolinkurl{http://www2.imm.dtu.dk/\~pch/Projekter/tsvd.html}}, ``The SVD [singular value decomposition] has also applications in digital signal processing, e.g., as a method for noise reduction. The central idea is to let a matrix \(A\) represent the noisy signal, compute the SVD, and then discard small singular values of \(A\). It can be shown that the small singular values mainly represent the noise, and thus the rank-\(k\) matrix \(A_k\) represents a filtered signal with less noise.''\label{g:fn:idp22517656}}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Calculating the Error in Approximating an Image}
\typeout{************************************************}
%
\begin{sectionptx}{Calculating the Error in Approximating an Image}{}{Calculating the Error in Approximating an Image}{}{}{x:section:sec_err_approx_img}
In the context where a matrix represents an image, the operator aspect of the matrix is irrelevant \textemdash{} we are only interested in the matrix as a holder of information. In this situation, we think of an \(m \times n\) matrix as simply a long vector in \(\R^{mn}\) where we have folded the data into a rectangular array. If we are interested in determining the error in approximating an image by a compressed image, it makes sense to use the standard norm in \(\R^{mn}\) to determine length and distance. This leads to what is called the \terminology{Frobenius} norm of a matrix. The Frobenius norm \(||M||_F\) of an \(m \times n\) matrix \(M = [m_{ij}]\) is%
\begin{equation*}
||M||_F = \sqrt{ \sum m_{ij}^2 }\text{.}
\end{equation*}
%
\par
There is a natural corresponding inner product on the set of \(m \times n\) matrices (called the \terminology{Frobenius product}) defined by%
\begin{equation*}
\langle A,B \rangle = \sum a_{ij}b_{ij}\text{,}
\end{equation*}
where \(A = [a_{ij}]\) and \(B = [b_{ij}]\) are \(m \times n\) matrices. Note that%
\begin{equation*}
||A||_F = \sqrt{\langle A, A\rangle}\text{.}
\end{equation*}
%
\par
If an \(m \times n\) matrix \(M\) of rank \(r\) has a singular value decomposition \(M = U \Sigma V^{\tr}\), we have seen that we can write \(M\) as an outer product%
\begin{equation}
M = \sigma_1 \vu_1\vv_1^{\tr} + \sigma_2 \vu_2\vv_2^{\tr} + \sigma_3 \vu_3\vv_3^{\tr} + \cdots + \sigma_{r} \vu_{r}\vv_{r}^{\tr}\text{,}\label{x:men:eq_7_c_outer_product}
\end{equation}
where the \(\vu_i\) are the columns of \(U\) and the \(\vv_j\) the columns of \(V\). Each of the products \(\vu_i \vv_i^{\tr}\) is an \(m \times n\) matrix. Since the columns of \(\vu_i \vv_i^{\tr}\) are all scalar multiples of \(\vu_i\), the matrix \(\vu_i \vv_i^{\tr}\) is a rank 1 matrix. So \hyperref[x:men:eq_7_c_outer_product]{({\xreffont\ref{x:men:eq_7_c_outer_product}})} expresses \(M\) as a sum of rank 1 matrices. Moreover, if we let \(\vx\) and \(\vw\) be \(m \times 1\) vectors and let \(\vy\) and \(\vz\) be \(n \times 1\) vectors with \(\vy = [y_1 \ y_2 \ \ldots \ y_n]^{\tr}\) and \(\vz = [z_1 \ z_2 \ \ldots \ z_n]^{\tr}\), then%
\begin{align*}
\langle \vx\vy^{\tr}, \vw\vz^{\tr} \rangle \amp = \langle [y_1\vx \ y_2\vx \ \cdots \ y_n\vx], [z_1\vw \ z_2\vw \ \cdots \ z_n\vw] \rangle\\
\amp = \sum (y_i\vx) \cdot (z_i\vw)\\
\amp = \sum (y_iz_i)(\vx \cdot \vw)\\
\amp = (\vx \cdot \vw) \sum (y_iz_i)\\
\amp = (\vx \cdot \vw) (\vy \cdot \vz)\text{.}
\end{align*}
%
\par
Using the vectors from the singular value decomposition of \(M\) as in \hyperref[x:men:eq_7_c_outer_product]{({\xreffont\ref{x:men:eq_7_c_outer_product}})} we see that%
\begin{equation*}
\langle \vu_i\vv_i^{\tr}, \vu_j\vv_j^{\tr} \rangle = (\vu_i \cdot \vu_j)(\vv_i \cdot \vv_j) = \begin{cases}0, \amp \text{ if }  i\neq j, \\ 1, \amp \text{ if }  i = j. \end{cases}
\end{equation*}
%
\par
It follows that%
\begin{equation}
||M||_F^2 = \sum \sigma_i^2 (\vu_i \cdot \vu_i)(\vv_i \cdot \vv_i) = \sum \sigma_i^2\text{.}\label{x:men:eq_7_c_SVD_norm}
\end{equation}
%
\begin{activity}{}{g:activity:idp22542104}%
Verify \hyperref[x:men:eq_7_c_SVD_norm]{({\xreffont\ref{x:men:eq_7_c_SVD_norm}})} that \(||M||_F^2 = \sum \sigma_i^2\).%
\end{activity}%
When we used the singular value decomposition to approximate the image defined by \(M\), we replaced \(M\) with a matrix of the form%
\begin{equation}
M_k = \sigma_1 \vu_1\vv_1^{\tr} + \sigma_2 \vu_2\vv_2^{\tr} + \sigma_3 \vu_3\vv_3^{\tr} + \cdots + \sigma_{k} \vu_{k}\vv_{k}^{\tr}\text{.}\label{x:men:eq_7_c_outer_product_k}
\end{equation}
%
\par
We call \(M_k\) the rank \(k\) approximation to \(M\). Notice that the outer product expansion in \hyperref[x:men:eq_7_c_outer_product_k]{({\xreffont\ref{x:men:eq_7_c_outer_product_k}})} is in fact a singular value decomposition for \(M_k\). The error \(E_k\) in approximating \(M\) with \(M_k\) is%
\begin{equation}
E_k = M - M_k = \sigma_{k+1} \vu_{k+1}\vv_{k+1}^{\tr} + \sigma_{k+2} \vu_{k+2}\vv_{k+2}^{\tr} + \cdots + \sigma_{r} \vu_{r}\vv_{r}^{\tr}\text{.}\label{x:men:eq_7_c_outer_product_error}
\end{equation}
%
\par
Once again, notice that \hyperref[x:men:eq_7_c_outer_product_error]{({\xreffont\ref{x:men:eq_7_c_outer_product_error}})} is a singular value decomposition for \(E_k\). We define the relative error in approximating \(M\) with \(M_k\) as%
\begin{equation*}
\ds \frac{||E_k||}{||M||}\text{.}
\end{equation*}
%
\par
Now \hyperref[x:men:eq_7_c_SVD_norm]{({\xreffont\ref{x:men:eq_7_c_SVD_norm}})} shows that%
\begin{equation*}
\ds \frac{||E_k||}{||M||} = \sqrt{ \frac{\sum_{i=k+1}^r \sigma_i^2}{\sum_{i=1}^r \sigma_i^2} }\text{.}
\end{equation*}
%
\par
In applications, we often want to retain a certain degree of accuracy in our approximations and this error term can help us accomplish that.%
\par
In our flower example, the singular values of \(M\) are given in \hyperref[x:men:eq_7_c_Flower_sing_values]{({\xreffont\ref{x:men:eq_7_c_Flower_sing_values}})}. The relative error in approximating \(M\) with \(M_{12}\) is%
\begin{equation*}
\sqrt{ \frac{\sum_{i=13}^{16} \sigma_i^2}{\sum_{i=1}^{16} \sigma_i^2} } \approx 0.0018\text{.}
\end{equation*}
%
\par
Errors (rounded to 4 decimal places) for approximating \(M\) with some of the \(M_k\) are shown in \hyperref[x:table:T_7_c_Errors]{Table~{\xreffont\ref{x:table:T_7_c_Errors}}}%
\begin{tableptx}{\textbf{Errors in approximating \(M\) by \(M_k\)}}{x:table:T_7_c_Errors}{}%
\centering%
{\tabularfont%
\begin{tabular}{Acccccc}\hrulethin
\multicolumn{1}{AcA}{\(k\)}&\multicolumn{1}{cA}{10}&\multicolumn{1}{cA}{9}&\multicolumn{1}{cA}{8}&\multicolumn{1}{cA}{7}&\multicolumn{1}{cA}{6}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{\(\frac{||E_k||}{||M||}\)}&\multicolumn{1}{cA}{0.0070}&\multicolumn{1}{cA}{0.0146}&\multicolumn{1}{cA}{0.0252}&\multicolumn{1}{cA}{0.0413}&\multicolumn{1}{cA}{0.06426}\tabularnewline\hrulemedium
\multicolumn{1}{AcA}{\(k\)}&\multicolumn{1}{cA}{5}&\multicolumn{1}{cA}{4}&\multicolumn{1}{cA}{3}&\multicolumn{1}{cA}{2}&\multicolumn{1}{cA}{1}\tabularnewline\hrulethin
\multicolumn{1}{AcA}{\(\frac{||E_k||}{||M||}\)}&\multicolumn{1}{cA}{0.0918}&\multicolumn{1}{cA}{0.1231}&\multicolumn{1}{cA}{0.1590}&\multicolumn{1}{cA}{0.201}&\multicolumn{1}{cA}{0.2460}\tabularnewline\hrulethin
\end{tabular}
}%
\end{tableptx}%
\begin{activity}{}{g:activity:idp22579224}%
Let \(M\) represent the flower image.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the relative errors in approximating \(M\) by \(M_{13}\) and \(M_{14}\). You may use the fact that \(\sqrt{\sum_{i=1}^{16} \sigma_i^2} \approx 3102.0679\).%
\item{}About how much of the information in the image is contained in the rank 1 approximation? Explain.%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Condition Number of a Matrix}
\typeout{************************************************}
%
\begin{sectionptx}{The Condition Number of a Matrix}{}{The Condition Number of a Matrix}{}{}{x:section:sec_mtx_cond_num}
A singular value decomposition for a matrix \(A\) can tell us a lot about how difficult it is to accurately solve a system \(A \vx = \vb\). Solutions to systems of linear equations can be very sensitive to rounding as the next exercise demonstrates.%
\begin{activity}{}{x:activity:act_7_c_cond_num}%
Find the solution to each of the systems.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\left[ \begin{array}{cc} 1.0000\amp 1.0000 \\ 1.0000\amp 1.0005 \end{array} \right] \left[ \begin{array}{c} x \\ y \end{array} \right] = \left[ \begin{array}{c} 2.0000 \\ 2.0050 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{cc} 1.000\amp 1.000 \\ 1.000\amp 1.001 \end{array} \right] \left[ \begin{array}{c} x \\ y \end{array} \right] = \left[ \begin{array}{c} 2.000 \\ 2.005 \end{array} \right]\)%
\end{enumerate}
\end{activity}%
Notice that a simple rounding in the \((2,2)\) entry of the coefficient matrix led to a significantly different solution. If there are rounding errors at any stage of the Gaussian elimination process, they can be compounded by further row operations. This is an important problem since computers can only approximate irrational numbers with rational numbers and so rounding can be critical. Finding ways of dealing with these kinds of errors is an area of on-going research in numerical linear algebra. This problem is given a name.%
\begin{definition}{}{g:definition:idp22584472}%
A matrix \(A\) is \terminology{ill-conditioned} if relatively small changes in any entries of \(A\) can produce significant changes in solutions to the system \(A\vx = \vb\).%
\end{definition}
A matrix that is not ill-conditioned is said to be \terminology{well-conditioned}. Since small changes in entries of ill-conditioned matrices can lead to large errors in computations, it is an important problem in linear algebra to have a way to measure how ill-conditioned a matrix is. This idea will ultimately lead us to the condition number of a matrix.%
\par
Suppose we want to solve the system \(A \vx = \vb\), where \(A\) is an invertible matrix. \hyperref[x:activity:act_7_c_cond_num]{Activity~{\xreffont\ref{x:activity:act_7_c_cond_num}}} illustrates that if \(A\) is really close to being singular, then small changes in the entries of \(A\) can have significant effects on the solution to the system. So the system can be very hard to solve accurately if \(A\) is close to singular. It is important to have a sense of how ``good'' we can expect any calculated solution to be. Suppose we think we solve the system \(A \vx = \vb\) but, through rounding error in our calculation of \(A\), get a solution \(\vx'\) so that \(A \vx' = \vb'\), where \(\vb'\) is not exactly \(\vb\). Let \(\Delta \vx\) be the error in our calculated solution and \(\Delta \vb\) the difference between \(\vb'\) and \(\vb\). We would like to know how large the error \(||\Delta \vx||\) can be. But this isn't exactly the right question. We could scale everything to make \(||\Delta \vx||\) as large as we want. What we really need is a measure of the \emph{relative error} \(\frac{||\Delta \vx||}{||\vx||}\), or how big the error is compared to \(||\vx||\) itself. More specifically, we want to know how large the relative error in \(\Delta \vx\) is compared to the relative error in \(\Delta \vb\). In other words, we want to know how good the relative error in \(\Delta \vb\) is as a predictor of the relative error in \(\Delta \vx\) (we may have some control over the relative error in \(\Delta \vb\), perhaps by keeping more significant digits). So we want know if there is a best constant \(C\) such that%
\begin{equation*}
\frac{||\Delta \vx||}{||\vx||} \leq C \frac{||\Delta \vb||}{||\vb||}\text{.}
\end{equation*}
%
\par
This best constant \(C\) is the condition number \textemdash{} a measure of how well the relative error in \(\Delta \vb\) predicts the relative error in \(\Delta \vx\). How can we find \(C\)?%
\par
Since \(A \vx' = \vb'\) we have%
\begin{equation*}
A(\vx + \Delta \vx) = \vb + \Delta \vb\text{.}
\end{equation*}
%
\par
Distributing on the left and using the fact that \(A\vx = \vb\) gives us%
\begin{equation}
A\Delta \vx = \Delta \vb\text{.}\label{x:men:eq_7_c_cn}
\end{equation}
%
\par
We return for a moment to the operator norm of a matrix. This is an appropriate norm to use here since we are considering \(A\) to be a transformation. Recall that if \(A\) is an \(m \times n\) matrix, we defined the operator norm of \(A\) to be%
\begin{equation*}
||A|| = \max_{||\vx|| \neq \vzero} \left\{\frac{||A\vx||}{||\vx||} \right\} = \max_{||\vx||=1} \{||A\vx||\}\text{.}
\end{equation*}
One important property that the norm has is that if the product \(AB\) is defined, then%
\begin{equation*}
||AB|| \leq ||A|| \ ||B||\text{.}
\end{equation*}
To see why, notice that%
\begin{equation*}
\frac{||AB\vx||}{||\vx||} = \frac{||A(B\vx)||}{||B\vx||} \ \frac{||B\vx||}{||\vx||}\text{.}
\end{equation*}
%
\par
Now \(\frac{||A(B\vx)||}{||B\vx||} \leq ||A||\) and \(\frac{||B\vx||}{||\vx||} \leq ||B||\) by the definition of the norm, so we conclude that%
\begin{equation*}
\frac{||AB\vx||}{||\vx||} \leq ||A|| \ ||B||
\end{equation*}
for every \(\vx\). Thus,%
\begin{equation*}
||AB|| \leq ||A|| \ ||B||\text{.}
\end{equation*}
%
\par
Now we can find the condition number. From \(A \Delta \vx = \Delta \vb\) we have%
\begin{equation*}
\Delta \vx = A^{-1} \Delta \vb\text{,}
\end{equation*}
so%
\begin{equation}
||\Delta \vx|| \leq ||A^{-1}|| \ ||\Delta \vb||\text{.}\label{x:men:eq_7_c_cn2}
\end{equation}
%
\par
Similarly, \(\vb = A\vx\) implies that \(||\vb|| \leq ||A|| \ ||\vx||\) or%
\begin{equation}
\frac{1}{||\vx||} \leq \frac{||A||}{||\vb||}\text{.}\label{x:men:eq_7_c_cn3}
\end{equation}
%
\par
Combining \hyperref[x:men:eq_7_c_cn2]{({\xreffont\ref{x:men:eq_7_c_cn2}})} and \hyperref[x:men:eq_7_c_cn3]{({\xreffont\ref{x:men:eq_7_c_cn3}})} gives%
\begin{align*}
\frac{||\Delta \vx||}{||\vx||} \amp \leq \frac{||A^{-1}|| \ ||\Delta \vb||}{||\vx||}\\
\amp = ||A^{-1}|| \ ||\Delta \vb|| \left(\frac{1}{||\vx||}\right)\\
\amp \leq ||A^{-1}|| \ ||\Delta \vb||  \frac{||A||}{||\vb||}\\
\amp = ||A^{-1}|| \ ||A|| \ \frac{||\Delta \vb||}{||\vb||}\text{.}
\end{align*}
%
\par
This constant \(||A^{-1}|| \ ||A||\) is the best bound and so is called the condition number of \(A\).%
\begin{definition}{}{g:definition:idp22617240}%
The \terminology{condition number}\index{condition number of a matrix} of an invertible matrix \(A\) is the number \(||A^{-1}|| \ ||A||\).%
\end{definition}
How does a singular value decomposition tell us about the condition number of a matrix? Recall that the maximum value of \(||A\vx||\) for \(\vx\) on the unit \(n\)-sphere is \(\sigma_1\). So \(||A|| = \sigma_1\). If \(A\) is an invertible matrix and \(A = U \Sigma V^{\tr}\) is a singular value decomposition for \(A\), then%
\begin{equation*}
A^{-1} = (U \Sigma V^{\tr})^{-1} = (V^{\tr})^{-1} \Sigma^{-1} U^{-1} = V \Sigma^{-1} U^{\tr}\text{,}
\end{equation*}
where%
\begin{equation*}
\Sigma^{-1} = \left[ \begin{array}{ccccc} \frac{1}{\sigma_1}\amp \amp \amp \amp 0 \\ \amp  \frac{1}{\sigma_2}\amp \amp \amp  \\ \amp \amp  \frac{1}{\sigma_3}\amp \amp  \\ \amp   \amp  \amp  \ddots \amp   \\ 0\amp \amp \amp \amp  \frac{1}{\sigma_n} \end{array}  \right]\text{.}
\end{equation*}
%
\par
Now \(V \Sigma^{-1} U^{\tr}\) is a singular value decomposition for \(A^{-1}\) with the diagonal entries in reverse order, so%
\begin{equation*}
||A^{-1}|| = \frac{1}{\sigma_n}\text{.}
\end{equation*}
%
\par
Therefore, the condition number of \(A\) is%
\begin{equation*}
||A^{-1}|| \ ||A|| = \frac{\sigma_1}{\sigma_n}\text{.}
\end{equation*}
%
\begin{activity}{}{g:activity:idp22622744}%
Let \(A = \left[ \begin{array}{cc} 1.0000\amp 1.0000 \\ 1.0000\amp 1.0005 \end{array} \right]\). A computer algebra system gives the singular values of \(A\) as 2.00025003124999934 and 0.000249968750000509660. What is the condition number of \(A\). What does that tell us about \(A\)? Does this seem reasonable given the result of \hyperref[x:activity:act_7_c_cond_num]{Activity~{\xreffont\ref{x:activity:act_7_c_cond_num}}}?%
\end{activity}%
\begin{activity}{}{g:activity:idp22623640}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}What is the smallest the condition number of a matrix can be? Find an entire class of matrices with this smallest condition number.%
\item{}What is the condition number of an orthogonal matrix? Why does this make sense?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp22628248}{}\quad{}If \(P\) is an orthogonal matrix, what is \(||P \vx||\) for any vector \(\vx\)? What does this make \(||P||\)?%
\item{}What is the condition number of an invertible symmetric matrix in terms of its eigenvalues?%
\item{}Why do we not define the condition number of a non-invertible matrix? If we did, what would the condition number have to be? Why?%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Pseudoinverses}
\typeout{************************************************}
%
\begin{sectionptx}{Pseudoinverses}{}{Pseudoinverses}{}{}{x:section:sec_pseudoinverses}
Not every matrix is invertible, so we cannot always solve a matrix equation \(A \vx = \vb\). However, every matrix has a pseudoinverse \(A^+\) that acts something like an inverse. Even when we can't solve a matrix equation \(A \vx = \vb\) because \(\vb\) isn't in \(\Col A\), we can use the pseudoinverse of \(A\) to ``solve'' the equation \(A \vx = \vb\) with the ``solution'' \(A^+ \vb\). While not an exact solution, \(A^+ \vb\) turns out to be the best approximation to a solution in the least squares sense. We will use the singular value decomposition to find the pseudoinverse of a matrix.%
\begin{exploration}{}{x:exploration:pa_7_d_2}%
Let \(A = \left[\begin{array}{ccc} 1\amp 1\amp 0\\ 0\amp 1\amp 1 \end{array}  \right]\). The singular value decomposition of \(A\) is \(U \Sigma V^{\tr}\) where%
\begin{align*}
U \amp = \frac{\sqrt{2}}{2} \left[ \begin{array}{cr} 1\amp -1\\
1\amp 1 \end{array} \right],\\
\Sigma \amp = \left[ \begin{array}{ccc} \sqrt{3}\amp 0\amp 0\\
0\amp 1\amp 0 \end{array} \right],\\
V \amp = \frac{1}{6} \left[ \begin{array}{rrr} \sqrt{6}\amp -3\sqrt{2}\amp 2\sqrt{3}\\
2\sqrt{6}\amp 0\amp -2\sqrt{3}\\
\sqrt{6}\amp 3\sqrt{2}\amp 2\sqrt{3} \end{array} \right]\text{.}
\end{align*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why \(A\) is not an invertible matrix.%
\item{}Explain why the matrices \(U\) and \(V\) are invertible. How are \(U^{-1}\) and \(V^{-1}\) related to \(U^{\tr}\) and \(V^{\tr}\)?%
\item{}Recall that one property of invertible matrices is that the inverse of a product of invertible matrices is the product of the inverses in the reverse order. If \(A\) were invertible, then \(A^{-1}\) would be \(\left(U \Sigma V^{\tr}\right)^{-1} = V \Sigma^{-1} U^{\tr}\). Even though \(U\) and \(V\) are invertible, the matrix \(\Sigma\) is not. But \(\Sigma\) does contain non-zero eigenvalues that have reciprocals, so consider the matrix \(\Sigma^+ = \left[ \begin{array}{cc} \frac{1}{\sqrt{3}}\amp 0 \\ 0\amp 1 \\ 0\amp 0 \end{array} \right]\). Calculate the products \(\Sigma \Sigma^+\) and \(\Sigma^+ \Sigma\). How are the results similar to that obtained with a matrix inverse?%
\item{}The only matrix in the singular value decomposition of \(A\) that is not invertible is \(\Sigma\). But the matrix \(\Sigma^{+}\) acts somewhat like an inverse of \(\Sigma\), so let us define \(A^+\) as \(V \Sigma^+ U^{\tr}\). Now we explore a few properties of the matrix \(A^{+}\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Calculate \(AA^+\) and \(A^+A\) for \(A = \left[\begin{array}{ccc} 1\amp 1\amp 0\\ 0\amp 1\amp 1 \end{array} \right]\). What do you notice?%
\item{}Calculate \(A^+AA^+\) and \(AA^+A\) for \(A = \left[\begin{array}{ccc} 1\amp 1\amp 0\\ 0\amp 1\amp 1 \end{array} \right]\). What do you notice?%
\end{enumerate}
\end{enumerate}
\end{exploration}%
Only some square matrices have inverses. However, every matrix has a pseudoinverse. A pseudoinverse \(A^{+}\) of a matrix \(A\) provides something like an inverse when a matrix doesn't have an inverse. Pseudoinverses are useful to approximate solutions to linear systems. If \(A\) is invertible, then the equation \(A \vx = \vb\) has the solution \(\vx = A^{-1}\vb\), but when \(A\) is not invertible and \(\vb\) is not in \(\Col A\), then the equation \(A \vx = \vb\) has no solution. In the invertible case of an \(n \times n\) matrix \(A\), there is a matrix \(B\) so that \(AB = BA = I_n\). This also implies that \(BAB = B\) and \(ABA = A\). To mimic this situation when \(A\) is not invertible, we search for a matrix \(A^+\) (a pseudoinverse of \(A\)) so that \(AA^+A = A\) and \(A^+AA^+ = A^+\), as we saw in \hyperref[x:exploration:pa_7_d_2]{Preview Activity~{\xreffont\ref{x:exploration:pa_7_d_2}}}. Then it turns out that \(A^+\) acts something like an inverse for \(A\). In this case, we approximate the solution to \(A \vx = \vb\) by \(\vx^* = A^+\vb\), and we will see that the vector \(A\vx^* = AA^+\vb\) turns out to be the vector in \(\Col A\) that is closest to \(\vb\) in the least squares sense.%
\par
A reasonable question to ask is how we can find a pseudoinverse of a matrix \(A\). A singular value decomposition provides an answer to this question. If \(A\) is an invertible \(n \times n\) matrix, then 0 is not an eigenvalue of \(A\). As a result, in the singular value decomposition \(U \Sigma V^{\tr}\) of \(A\), the matrix \(\Sigma\) is an invertible matrix (note that \(U\), \(\Sigma\), and \(V\) are all \(n \times n\) matrices in this case). So%
\begin{equation*}
A^{-1} = \left(U \Sigma V^{\tr}\right)^{-1} = V \Sigma^{-1} U^{\tr}\text{,}
\end{equation*}
where%
\begin{equation*}
\Sigma^{-1} = \left[ \begin{array}{ccccc} \frac{1}{\sigma_1}\amp \amp \amp \amp  \\  \amp  \frac{1}{\sigma_2}\amp \amp 0\amp  \\  \amp \amp  \frac{1}{\sigma_3}\amp \amp  \\ \amp 0  \amp  \amp  \ddots \amp   \\ \amp \amp \amp \amp  \frac{1}{\sigma_n} \end{array}  \right]\text{.}
\end{equation*}
%
\par
In this case, \(V \Sigma^{-1} U^{\tr}\) is a singular value decomposition for \(A^{-1}\).%
\par
To understand in general how a pseudoinverse is found, let \(A\) be an \(m \times n\) matrix with \(m \neq n\), or an \(n \times n\) with rank less than \(n\). In these cases \(A\) does not have an inverse. But as in \hyperref[x:exploration:pa_7_d_2]{Preview Activity~{\xreffont\ref{x:exploration:pa_7_d_2}}}, a singular value decomposition provides a pseudoinverse \(A^+\) for \(A\). Let \(U \Sigma V^{\tr}\) be a singular value decomposition of an \(m \times n\) matrix \(A\) of rank \(r\), with%
\begin{equation*}
\Sigma = \left[ \begin{array}{ccccc|c} \sigma_1\amp \amp \amp \amp \amp  \\ \amp  \sigma_2\amp \amp 0\amp \amp 0 \\ \amp \amp  \sigma_3\amp \amp \amp  \\ \amp  0 \amp  \amp  \ddots \amp  \amp  \\ \amp \amp \amp \amp  \sigma_r \\ \hline \amp \amp 0\amp \amp \amp 0 \end{array}  \right]
\end{equation*}
%
\par
\index{pseudoinverse} The matrices \(U\) and \(V\) are invertible, but the matrix \(\Sigma\) is not if \(A\) is not invertible. If we let \(\Sigma^+\) be the \(n \times m\) matrix defined by%
\begin{equation*}
\Sigma^{+} = \left[ \begin{array}{ccccc|c} \frac{1}{\sigma_1}\amp \amp \amp \amp \amp  \\ \amp  \frac{1}{\sigma_2}\amp \amp 0\amp \amp 0 \\ \amp \amp  \frac{1}{\sigma_3}\amp \amp \amp  \\ \amp  0 \amp  \amp  \ddots \amp  \amp  \\ \amp \amp \amp \amp  \frac{1}{\sigma_r} \\ \hline \amp \amp 0\amp \amp \amp 0 \end{array}  \right]\text{,}
\end{equation*}
then \(\Sigma^{+}\) will act much like an inverse of \(\Sigma\) might. In fact, it is not difficult to see that%
\begin{equation*}
\Sigma\Sigma^{+} = \left[ \begin{array}{c|c} I_r\amp 0 \\ \hline 0\amp 0 \end{array}  \right] \text{ and }  \Sigma^{+}\Sigma = \left[ \begin{array}{c|c} I_r\amp 0 \\ \hline 0\amp 0 \end{array}  \right]\text{,}
\end{equation*}
where \(\Sigma\Sigma^{+}\) is an \(m \times m\) matrix and \(\Sigma^{+}\Sigma\) is an \(n \times n\) matrix.%
\par
The matrix%
\begin{equation}
A^{+} = V\Sigma^{+}U^{\tr}\label{x:men:eq_pseudoinverse}
\end{equation}
is a \terminology{pseudoinverse} of \(A\).%
\begin{activity}{}{g:activity:idp22988824}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the pseudoinverse \(A^+\) of \(A = \left[ \begin{array}{rc} 0\amp 5\\ 4\amp 3 \\ -2\amp 1 \end{array}  \right]\). Use the singular value decomposition \(U \Sigma V^{\tr}\) of \(A\), where%
\begin{equation*}
U =  \left[ \begin{array}{crc} \frac{\sqrt{2}}{2}\amp \frac{\sqrt{3}}{3}\amp 1 \\ \frac{\sqrt{2}}{2}\amp -\frac{\sqrt{3}}{3}\amp 0 \\ 0\amp \frac{\sqrt{3}}{3}\amp 0 \end{array}  \right], \ \Sigma =  \left[ \begin{array}{cc} \sqrt{40}\amp 0 \\ 0\amp \sqrt{15} \\ 0\amp 0 \end{array}  \right], \ V = \frac{1}{\sqrt{5}} \left[ \begin{array}{cr} 1\amp -2 \\ 2\amp 1 \end{array}  \right]\text{.}
\end{equation*}
%
\item{}The vector \(\vb = \left[ \begin{array}{c} 0\\0\\1 \end{array} \right]\) is not in \(\Col A\). The vector \(\vx^* = A^+ \vb\) is an approximation to a solution of \(A \vx = \vb\), and \(AA^+\vb\) is in \(\Col A\). Find \(A\vx^*\) and determine how far \(A\vx^*\) is from \(\vb\).%
\end{enumerate}
\end{activity}%
Pseudoinverses satisfy several properties that are similar to those of inverses. For example, we had an example in \hyperref[x:exploration:pa_7_d_2]{Preview Activity~{\xreffont\ref{x:exploration:pa_7_d_2}}} where \(AA^{+}A = A\) and \(A^+AA^+ = A^+\). That \(A^+\) always satisfies these properties is the subject of the next activity.%
\begin{activity}{}{x:activity:act_7_d_Moore-Penrose}%
Let \(A\) be an \(m \times n\) matrix with singular value decomposition \(U \Sigma V^{\tr}\). Let \(A^{+}\) be defined as in \hyperref[x:men:eq_pseudoinverse]{({\xreffont\ref{x:men:eq_pseudoinverse}})}.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(AA^{+}A = A\).%
\item{}Show that \(A^{+}AA^{+} = A^{+}\).%
\end{enumerate}
\end{activity}%
\hyperref[x:activity:act_7_d_Moore-Penrose]{Activity~{\xreffont\ref{x:activity:act_7_d_Moore-Penrose}}} shows that \(A^{+}\) satisfies properties that are similar to those of an inverse of \(A\). In fact, \(A^{+}\) satisfies several other properties (that together can be used as defining properties) as stated in the next theorem. The conditions of \hyperref[x:theorem:thm_7_d_pseudoinverse]{Theorem~{\xreffont\ref{x:theorem:thm_7_d_pseudoinverse}}} are called the \terminology{Penrose} or \terminology{Moore-Penrose} conditions.\footnote{\hyperref[x:theorem:thm_7_d_pseudoinverse]{Theorem~{\xreffont\ref{x:theorem:thm_7_d_pseudoinverse}}} is often given as the definition of a pseudoinverse.\label{g:fn:idp23009432}} Verification of the remaining parts of this theorem are left for the exercises.%
\begin{theorem}{The Moore-Penrose Conditions.}{}{x:theorem:thm_7_d_pseudoinverse}%
A pseudoinverse of a matrix \(A\) is a matrix \(A^+\) that satisfies the following properties.%
\begin{enumerate}
\item{}\(\displaystyle AA^{+}A = A\)%
\item{}\(\displaystyle A^{+}AA^{+} = A^{+}\)%
\item{}\(\displaystyle (AA^{+})^{\tr} = AA^{+}\)%
\item{}\(\displaystyle (A^{+}A)^{\tr} = A^{+}A\)%
\end{enumerate}
%
\end{theorem}
Also, there is a unique matrix \(A^+\) that satisfies these properties. The verification of this property is left to the exercises.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Least Squares Approximations}
\typeout{************************************************}
%
\begin{sectionptx}{Least Squares Approximations}{}{Least Squares Approximations}{}{}{x:section:sec_ls_approx_SVD}
The pseudoinverse of a matrix is also connected to least squares solutions of linear systems as we encountered in \hyperref[x:chapter:chap_orthogonal_basis]{Section~{\xreffont\ref{x:chapter:chap_orthogonal_basis}}}. Recall from \hyperref[x:chapter:chap_orthogonal_basis]{Section~{\xreffont\ref{x:chapter:chap_orthogonal_basis}}}, that if the columns of \(A\) are linearly independent, then the least squares solution to \(A\vx = \vb\) is \(\vx = \left(A^{\tr}A\right)^{-1}A^{\tr} \vb\). In this section we will see how to use a pseudoinverse to solve a least squares problem, and verify that if the columns of \(A\) are linearly dependent, then \(\left(A^{\tr}A\right)^{-1}A^{\tr}\) is in fact the pseudoinverse of \(A\).%
\par
Let \(U \Sigma V^{\tr}\) be a singular value decomposition for an \(m \times n\) matrix \(A\) of rank \(r\). Then the columns of%
\begin{equation*}
U = [\vu_1 \ \vu_2  \ \cdots  \ \vu_m]
\end{equation*}
form an orthonormal basis for \(\R^m\) and \(\{\vu_1, \vu_2, \ldots, \vu_r\}\) is a basis for \(\Col A\). Remember from \hyperref[x:chapter:chap_gram_schmidt]{Section~{\xreffont\ref{x:chapter:chap_gram_schmidt}}} that if \(\vb\) is any vector in \(\R^m\), then%
\begin{equation*}
\proj_{\Col A} \vb = (\vb \cdot \vu_1) \vu_1 + (\vb \cdot \vu_2) \vu_2 + \cdots + (\vb \cdot \vu_r) \vu_r
\end{equation*}
is the least squares approximation of the vector \(\vb\) by a vector in \(\Col A\). We can extend this sum to all of columns of \(U\) as%
\begin{equation*}
\proj_{\Col A} \vb = (\vb \cdot \vu_1) \vu_1 + (\vb \cdot \vu_2) \vu_2 + \cdots + (\vb \cdot \vu_r) \vu_r + 0 \vu_{r+1} + 0 \vu_{r+2} + \cdots + 0 \vu_m\text{.}
\end{equation*}
%
\par
It follows that%
\begin{align*}
\proj_{\Col A} \vb \amp = \sum_{i=1}^r \vu_i (\vu_i \cdot \vb)\\
\amp = \sum_{i=1}^r \vu_i (\vu_i^{\tr}\vb)\\
\amp = \sum_{i=1}^r (\vu_i\vu_i^{\tr}) \vb\\
\amp = \left(\sum_{i=1}^r (1)(\vu_i\vu_i^{\tr})\right)\vb + \left(\sum_{i=r+1}^m 0(\vu_i\vu_i^{\tr})\right) \vb\\
\amp = (UDU^{\tr}) \vb\text{,}
\end{align*}
where%
\begin{equation*}
D = \left[ \begin{array}{c|c} I_r \amp  \vzero \\ \hline \vzero \amp  \vzero \end{array}  \right]\text{.}
\end{equation*}
%
\par
Now, if \(\vz = A^{+} \vb\), then%
\begin{equation*}
A\vz = (U\Sigma V^{\tr})(V \Sigma^+ U^{\tr} \vb) = (U \Sigma \Sigma^+ U^{\tr})\vb = (UDU^{\tr})\vb = \proj_{\Col A} \vb\text{,}
\end{equation*}
and hence the vector \(A \vz = AA^+ \vb\) is the vector \(A\vx\) in \(\Col A\) that minimizes \(||A \vx - \vb||\). Thus, \(A\vz\) is in actuality the least squares approximation to \(\vb\). So a singular value decomposition allows us to construct the pseudoinverse of a matrix \(A\) and then directly solve the least squares problem.%
\par
If the columns of \(A\) are linearly independent, then we do not need to use an SVD to find the pseudoinverse, as the next activity illustrates.%
\begin{activity}{}{x:activity:act_7_b_lin_indep_cols}%
Having to calculate eigenvalues and eigenvectors for a matrix to produce a singular value decomposition to find pseudoinverse can be computationally intense. As we demonstrate in this activity, the process is easier if the columns of \(A\) are linearly independent. More specifically, we will prove the following theorem.%
\begin{theorem}{}{}{x:theorem:thm_7_d_SVD_cols_lin_indep}%
If the columns of a matrix \(A\) are linearly independent, then \(A^{+} = \left(A^{\tr}A\right)^{-1}A^{\tr}\).%
\end{theorem}
To see how, suppose that \(A\) is an \(m \times n\) matrix with linearly independent columns.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Given that the columns of \(A\) are linearly independent, what must be the relationship between \(n\) and \(m\)?%
\item{}Since the columns of \(A\) are linearly independent, it follows that \(A^{\tr}A\) is invertible (see \hyperref[x:activity:act_LS_invertible]{Activity~{\xreffont\ref{x:activity:act_LS_invertible}}}). So the eigenvalues of \(A^{\tr}A\) are all non-zero. Let \(\sigma_1\), \(\sigma_2\), \(\ldots\), \(\sigma_r\) be the singular values of \(A\). How is \(r\) related to \(n\), and what do \(\Sigma\) and \(\Sigma^{+}\) look like?%
\item{}Let us now investigate the form of the invertible matrix \(A^{\tr}A\) (note that neither \(A\) nor \(A^{\tr}\) is necessarily invertible). If a singular value decomposition of \(A\) is \(U \Sigma V^{\tr}\), show that%
\begin{equation*}
A^{\tr}A = V \Sigma^{\tr} \Sigma V^{\tr}\text{.}
\end{equation*}
%
\item{}Let \(\lambda_i = \sigma_i^2\) for \(i\) from 1 to \(n\). It is straightforward to see that \(\Sigma^{\tr} \Sigma\) is an \(n \times n\) diagonal matrix \(D\), where%
\begin{equation*}
D = \Sigma^{\tr} \Sigma = \left[ \begin{array}{ccccc} \lambda_1\amp \amp \amp \amp  \\ \amp  \lambda_2\amp \amp 0\amp  \\ \amp \amp  \lambda_3\amp \amp  \\ \amp   \amp  \amp  \ddots \amp    \\ \amp 0\amp \amp \amp  \lambda_n \end{array}  \right]\text{.}
\end{equation*}
Then \((A^{\tr}A)^{-1} = VD^{-1}V^{\tr}\). Recall that \(A^{+} = V \Sigma^{+} U^{\tr}\), so to relate \(A^{\tr}A\) to \(A^{+}\) we need a product that is equal to \(\Sigma^{+}\). Explain why%
\begin{equation*}
D^{-1} \Sigma^{\tr} = \Sigma^{+}\text{.}
\end{equation*}
%
\item{}Complete the activity by showing that%
\begin{equation*}
\left(A^{\tr}A\right)^{-1} A^{\tr} = A^{+}\text{.}
\end{equation*}
%
\end{enumerate}
\end{activity}%
Therefore, to calculate \(A^{+}\) and solve a least squares problem, \hyperref[x:theorem:thm_7_d_SVD_cols_lin_indep]{Theorem~{\xreffont\ref{x:theorem:thm_7_d_SVD_cols_lin_indep}}} shows that as long as the columns of \(A\) are linearly independent, we can avoid using a singular value decomposition of \(A\) in finding \(A^{+}\).%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_pseudo_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp23059352}%
Let%
\begin{equation*}
A = \left[ \begin{array}{ccc} 2\amp 5\amp 4\\6\amp 3\amp 0\\6\amp 3\amp 0\\2\amp 5\amp 4 \end{array}  \right]\text{.}
\end{equation*}
%
\par
The eigenvalues of \(A^{\tr}A\) are \(\lambda_1 = 144\), \(\lambda_2 = 36\), and \(\lambda_3=0\) with corresponding eigenvectors%
\begin{equation*}
\vw_1 = \left[ \begin{array}{c} 2\\2\\1 \end{array}  \right], \ \vw_1 = \left[ \begin{array}{r} -2\\1\\2 \end{array}  \right], \ \text{ and }  \ \vw_1 = \left[ \begin{array}{r} 1\\-2\\2 \end{array}  \right]\text{.}
\end{equation*}
%
\par
In addition,%
\begin{equation*}
A \vw_1 = \left[ \begin{array}{c} 18\\18\\18\\18 \end{array}  \right] \ \text{ and }  A \vw_2 = \left[ \begin{array}{r} 9\\-9\\-9\\9 \end{array}  \right]\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find orthogonal matrices \(U\) and \(V\), and the matrix \(\Sigma\), so that \(U \Sigma V^{\tr}\) is a singular value decomposition of \(A\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp23063576}{}\quad{}Normalizing the eigenvectors \(\vw_1\), \(\vw_2\), and \(\vw_3\) to normal eigenvectors \(\vv_1\), \(\vv_2\), and \(\vv_3\), respectively, gives us an orthogonal matrix%
\begin{equation*}
V = \left[  \begin{array}{crr} \frac{2}{3}\amp -\frac{2}{3}\amp \frac{1}{3}\\ \frac{2}{3}\amp \frac{1}{3}\amp - \frac{2}{3}\\  \frac{1}{3}\amp \frac{2}{3}\amp \frac{2}{3} \end{array}  \right]\text{.}
\end{equation*}
Now \(A \vv_i = A \frac{\vw_i}{||\vw_i||} = \frac{1}{||\vw_i||} A \vw_i\), so normalizing the vectors \(A \vw_1\) and \(A \vw_2\) gives us vectors%
\begin{equation*}
\vu_1 = \frac{1}{2} \left[ \begin{array}{c} 1\\1\\1\\1 \end{array}  \right] \ \text{ and }  \ \vu_2 = \frac{1}{2} \left[ \begin{array}{r} 1\\-1\\-1\\1 \end{array}  \right]
\end{equation*}
that are the first two columns of our matrix \(U\). Given that \(U\) is a \(4 \times 4\) matrix, we need to find two other vectors orthogonal to \(\vu_1\) and \(\vu_2\) that will combine with \(\vu_1\) and \(\vu_2\) to form an orthogonal basis for \(\R^4\). Letting \(\vz_1 = [1 \ 1 \ 1 \ 1]^{\tr}\), \(\vz_2 = [1 \ -1 \ -1 \ 1]^{\tr}\), \(\vz_3 = [1 \ 0 \ 0 \ 0]^{\tr}\), and \(\vz_4 = [0 \ 1 \ 0 \ 1]^{\tr}\), a computer algebra system shows that the reduced row echelon form of the matrix \([\vz_1 \ \vz_2 \ \vz_3 \ \vz_4]\) is \(I_4\), so that vectors \(\vz_1\), \(\vz_2\), \(\vz_3\), \(\vz_4\) are linearly independent. Letting \(\vw_1 = \vz_1\) and \(\vw_2 = \vz_2\), the Gram-Schmidt process shows that the set \(\{\vw_1, \vw_2, \vw_3, \vw_4\}\) is an orthogonal basis for \(\R^4\), where \(\vw_3 = \frac{1}{4} [2 \ 0 \ 0 \ -2]^{\tr}\) and (using \([1 \ 0 \ 0 \ -1]^{\tr}\) for \(\vw_3\)) \(\vw_4 = \frac{1}{4} [0 \ 2 \ -2 \ 0]^{\tr}\). The set \(\{\vu_1, \vu_2, \vu_3, \vu_4\}\) where \(\vu_1 = \frac{1}{2}[1 \ 1 \ 1 \ 1]^{\tr}\), \(\vu_2 = \frac{1}{2}[1 \ -1 \ -1 \ 1]^{\tr}\), \(\vu_3 = \frac{1}{\sqrt{2}}[1 \ 0 \ 0 \ -1]^{\tr}\) and \(\vu_4 = \frac{1}{\sqrt{2}}[0 \ 1 \ -1 \ 0]^{\tr}\) is an orthonormal basis for \(\R^4\) and we can let%
\begin{equation*}
U = \left[  \begin{array}{crrr} \frac{1}{2} \amp  \frac{1}{2} \amp  \frac{1}{\sqrt{2}} \amp  0 \\ \frac{1}{2} \amp  -\frac{1}{2} \amp  0 \amp  \frac{1}{\sqrt{2}} \\  \frac{1}{2} \amp  -\frac{1}{2} \amp  0 \amp  -\frac{1}{\sqrt{2}} \\ \frac{1}{2} \amp  \frac{1}{2} \amp  -\frac{1}{\sqrt{2}} \amp  0 \end{array}  \right]\text{.}
\end{equation*}
The singular values of \(A\) are \(\sigma_1 = \sqrt{\lambda_1} = 12\) and \(\sigma_2 = \sqrt{\lambda_2} = 6\), and so%
\begin{equation*}
\Sigma = \begin{bmatrix}12\amp 0\amp 0 \\ 0\amp 6\amp 0 \\0\amp 0\amp 0 \\ 0\amp 0\amp 0 \end{bmatrix}\text{.}
\end{equation*}
Therefore, a singular value decomposition of \(A\) is \(U \Sigma V^{\tr}\) of%
\begin{equation*}
\left[ \begin{array}{crrr} \frac{1}{2} \amp  \frac{1}{2} \amp  \frac{1}{\sqrt{2}} \amp  0 \\ \frac{1}{2} \amp  -\frac{1}{2} \amp  0 \amp  \frac{1}{\sqrt{2}} \\  \frac{1}{2} \amp  -\frac{1}{2} \amp  0 \amp  -\frac{1}{\sqrt{2}} \\ \frac{1}{2} \amp  \frac{1}{2} \amp  -\frac{1}{\sqrt{2}} \amp  0 \end{array}  \right] \begin{bmatrix}12\amp 0\amp 0 \\ 0\amp 6\amp 0 \\0\amp 0\amp 0 \\ 0\amp 0\amp 0 \end{bmatrix}  \left[ \begin{array}{rrc} \frac{2}{3}\amp \frac{2}{3}\amp \frac{1}{3}\\ -\frac{2}{3}\amp \frac{1}{3}\amp \frac{2}{3}\\  \frac{1}{3}\amp -\frac{2}{3}\amp \frac{2}{3} \end{array}  \right]\text{.}
\end{equation*}
%
\item{}Determine the best rank 1 approximation to \(A\). Give an appropriate numerical estimate as to how good this approximation is to \(A\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp23082776}{}\quad{}The outer product decomposition of \(A\) is%
\begin{equation*}
A = \sigma_1 \vu_1 \vv_1^{\tr} + \sigma_2 \vu_2 \vv_2^{\tr}\text{.}
\end{equation*}
So the rank one approximation to \(A\) is%
\begin{equation*}
\sigma_1 \vu_1 \vv_1^{\tr} = 12 \left(\frac{1}{2}\right) \left[ \begin{array}{c} 1\\1\\1\\1 \end{array}  \right] \left[ \begin{array}{ccc} \frac{2}{3} \amp  \frac{2}{3} \amp  \frac{1}{3} \end{array}  \right] = \left[ \begin{array}{ccc} 4\amp 4\amp 2\\ 4\amp 4\amp 2 \\ 4\amp 4\amp 2\\ 4\amp 4\amp 2 \end{array}   \right]\text{.}
\end{equation*}
The error in approximating \(A\) with this rank one approximation is%
\begin{equation*}
\sqrt{\frac{\sigma_2^2}{\sigma_1^2+\sigma_2^2}} = \sqrt{\frac{36}{180}} = \sqrt{\frac{1}{5}} \approx 0.447\text{.}
\end{equation*}
%
\item{}Find the pseudoinverse \(A^+\) of \(A\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp23093656}{}\quad{}Given that \(A = U \Sigma V^{\tr}\), we use the pseudoinverse \(\Sigma^+\) of \(\Sigma\) to find the pseudoinverse \(A^+\) of \(A\) by%
\begin{equation*}
A^+ = V \Sigma^+ U^{\tr}\text{.}
\end{equation*}
Now%
\begin{equation*}
\Sigma^+ = \left[  \begin{array}{ccc} \frac{1}{12}\amp 0\amp 0 \\ 0\amp \frac{1}{6}\amp 0 \\0\amp 0\amp 0 \\ 0\amp 0\amp 0 \end{array}  \right]\text{,}
\end{equation*}
so%
\begin{align*}
A^+ \amp = \left[  \begin{array}{crr} \frac{2}{3}\amp -\frac{2}{3}\amp \frac{1}{3}\\
\frac{2}{3}\amp \frac{1}{3}\amp - \frac{2}{3}\\
\frac{1}{3}\amp \frac{2}{3}\amp \frac{2}{3}\end{array} \right] \left[  \begin{array}{ccc} \frac{1}{12}\amp 0\amp 0\\
0\amp \frac{1}{6}\amp 0\\
0\amp 0\amp 0\\
0\amp 0\amp 0 \end{array} \right] \left[  \begin{array}{crrr} \frac{1}{2} \amp  \frac{1}{2} \amp  \frac{1}{\sqrt{2}} \amp  0\\
\frac{1}{2} \amp  -\frac{1}{2} \amp  0 \amp  \frac{1}{\sqrt{2}}\\
\frac{1}{2} \amp  -\frac{1}{2} \amp  0 \amp  -\frac{1}{\sqrt{2}}\\
\frac{1}{2} \amp  \frac{1}{2} \amp  -\frac{1}{\sqrt{2}} \amp  0 \end{array} \right]^{\tr}\\
\amp =  \frac{1}{72} \left[ \begin{array}{rrrr} -2\amp 6\amp 6\amp -2\\
4\amp 0\amp 0\amp 4\\
5\amp -3\amp -3\amp 5 \end{array} \right]\text{.}
\end{align*}
%
\item{}Let \(\vb = \left[ \begin{array}{c} 1\\0\\1\\0 \end{array}  \right]^{\tr}\). Does the matrix equation%
\begin{equation*}
A \vx = \vb
\end{equation*}
have a solution? If so, find the solution. If not, find the best approximation you can to a solution to this matrix equation.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp23102616}{}\quad{}Augmenting \(A\) with \(\vb\) and row reducing shows that%
\begin{equation*}
[A \ \vb ] \sim  \left[ \begin{array}{crrr} 2\amp 5\amp 4\amp 1\\ 0\amp -12\amp -12\amp -3 \\ 0\amp 0\amp 0\amp 1\\ 0\amp 0\amp 0\amp 0 \end{array}   \right]\text{,}
\end{equation*}
so \(\vb\) is not in \(\Col A\) and the equation \(A\vx = \vb\) has no solution. However, the best approximation to a solution to \(A \vx = \vb\) is found using the pseudoinverse \(A^+\) of \(A\). That best solution is%
\begin{align*}
\vx^*\amp = AA^+ \vb\\
\amp = \left[ \begin{array}{ccc} 2\amp 5\amp 4\\
6\amp 3\amp 0\\
6\amp 3\amp 0\\
2\amp 5\amp 4 \end{array} \right] \frac{1}{72} \left[ \begin{array}{rrrr} -2\amp 6\amp 6\amp -2\\
4\amp 0\amp 0\amp 4\\
5\amp -3\amp -3\amp 5 \end{array} \right] \left[ \begin{array}{c} 1\\
0\\
1\\
0 \end{array} \right]\\
\amp = \frac{1}{2} \left[ \begin{array}{c} 2\\
1\\
1\\
2 \end{array} \right]\text{.}
\end{align*}
%
\item{}Use the orthogonal basis \(\{\frac{1}{2}[1 \ 1 \ 1 \ 1]^{\tr}, \frac{1}{2}[1 \ -1 \ -1 \ 1]^{\tr}\}\) of \(\Col A\) to find the projection of \(\vb\) onto \(\Col A\). Compare to your solution in part (c).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp23106968}{}\quad{}The rank of \(A\) is 2 and an orthonormal basis for \(\Col A\) is \(\{\vu_1, \vu_2\}\), where \(\vu_1 = \frac{1}{2}[1 \ 1 \ 1 \ 1]^{\tr}\) and \(\vu_2 = \frac{1}{2}[1 \ -1 \ -1 \ 1]^{\tr}\). So%
\begin{align*}
\proj_{\Col A} \vb \amp = (\vb \cdot \vu_1) \vu_1 + (\vb \cdot \vu_2) \vu_2\\
\amp = \left(\frac{3}{2}\right)\left(\frac{1}{2}\right)[1 \ 1 \ 1 \ 1]^{\tr} + \left(\frac{1}{2}\right)\left(\frac{1}{2}\right)[1 \ -1 \ -1 \ 1]^{\tr}\\
\amp = \frac{1}{2}[2 \ 1 \ 1 \ 2]^{\tr}
\end{align*}
as expected from part (c).%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp23107352}%
\hyperref[x:table:T_Debt_per_capita]{Table~{\xreffont\ref{x:table:T_Debt_per_capita}}} shows the per capita debt in the U.S. in from 2014 to 2019 (source \href{https://www.statista.com/statistics/203064/national-debt-of-the-united-states-per-capita/}{statistica.com}\footnotemark{}).%
\begin{tableptx}{\textbf{U.S. per capita debt}}{x:table:T_Debt_per_capita}{}%
\centering%
{\tabularfont%
\begin{tabular}{ccccccc}
\multicolumn{1}{cA}{Year}&2014&2015&2016&2017&2018&2019\tabularnewline\hrulethin
\multicolumn{1}{cA}{Debt}&55905&56513&60505&62174&65697&69064
\end{tabular}
}%
\end{tableptx}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Set up a linear system of the form \(A \vx = \vb\) whose least squares solution provides a linear fit to the data.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp23114648}{}\quad{}A linear approximation \(f(x) = a_0 + a_1x\) to the system would satisfy the equation \(A \vx = \vb\), where \(A = \left[ \begin{array}{cc} 1\amp 2014 \\ 1\amp 2015 \\ 1\amp 2016 \\ 1\amp 2017 \\ 1\amp 2018 \\ 1\amp 2019 \end{array} \right]\), \(\vx = \left[ \begin{array}{c} a_0 \\ a_1 \end{array} \right]\), and \(\vb = \left[ \begin{array}{c} 55905 \\ 56513 \\ 60505 \\ 62174 \\ 65697 \\ 69064 \end{array} \right]\).%
\item{}Use technology to approximate a singular value decomposition (round to four decimal places). Use this svd to approximate the pseudoinverse of \(A\). Then use this pseudoinverse to approximate the least squares linear approximation to the system.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp23119768}{}\quad{}Technology shows that a singular value decomposition of \(A\) is approximately \(U \Sigma V^{\tr}\), where%
\begin{align*}
U \amp = \left[\begin{array}{rrrrrr} 0.4077 \amp  0.5980 \amp  -0.3997 \amp  -0.3615 \amp  -0.3233 \amp  -0.2851\\
0.4079 \amp  0.3589 \amp  -0.0621 \amp  0.1880 \amp  0.4381 \amp  0.6882\\
0.4081 \amp  0.1199 \amp  0.8817 \amp  -0.1181 \amp  -0.1178 \amp  -0.1176\\
0.4083 \amp  -0.1192 \amp  -0.1291 \amp  0.8229 \amp  -0.2251 \amp  -0.2730\\
0.4085 \amp  -0.3582 \amp  -0.1400 \amp  -0.2361 \amp  0.6677 \amp  -0.4285\\
0.4087 \amp  -0.5973 \amp  -0.1508 \amp  -0.2952 \amp  -0.4396 \amp  0.4160 \end{array}\right]\\
\Sigma \amp =  \left[\begin{array}{cc} 4939.3984 \amp  0.0\\
0.0 \amp  0.0021\\
0.0 \amp  0.0\\
0.0 \amp  0.0\\
0.0 \amp  0.0\\
0.0 \amp  0.0 \end{array}\right]\\
V \amp = \left[\begin{array}{cr} 0.0005 \amp  1.0000\\
1.0000 \amp  -0.0005 \end{array}\right]\text{.}
\end{align*}
Thus, with \(\Sigma^+ = \left[  \begin{array}{cccccc} \frac{1}{4939.3984}\amp 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp \frac{1}{0.0021}\amp 0\amp 0\amp 0\amp 0 \end{array}  \right]\), we have that the pseudoinverse of \(A\) is approximately%
\begin{equation*}
A^+ = V \Sigma^+ U^{\tr} = \left[\begin{array}{rrrrrr} 288.2381 \amp  173.0095 \amp  57.7809 \amp  -57.4476 \amp  -172.6762 \amp  -287.9048 \\ -0.14286 \amp  -0.0857 \amp  -0.0286 \amp  0.02857 \amp  0.0857 \amp  0.14286 \end{array} \right]\text{.}
\end{equation*}
So our least squares linear approximation is found by%
\begin{equation*}
A^{+} \vb = \left[ \begin{array}{r} -5412635.9714 \\ 2714.7429 \end{array}  \right]\text{.}
\end{equation*}
This makes our least squares linear approximation to be (to four decimal places)%
\begin{equation*}
f(x) = -5412635.9714 + 2714.7429x\text{.}
\end{equation*}
%
\item{}Calculate \(\left(A^{\tr}A\right)^{-1}A^{\tr}\) directly and compare to the pseudoinverse you found in part (b).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp23129240}{}\quad{}Calculating \(\left(A^{\tr}A\right)^{-1}A^{\tr}\) gives the same matrix as \(A^+\), so we obtain the same linear approximation.%
\item{}Use your approximation to estimate the U.S. per capita debt in 2020.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp23132952}{}\quad{}The approximate U.S. per capita debt in 2020 is%
\begin{equation*}
f(2020) = -5412635.9714 + 2714.7429(2020) = 71144.60\text{.}
\end{equation*}
%
\end{enumerate}
\end{example}
\footnotetext[54]{\nolinkurl{statista.com/statistics/203064/national-debt-of-the-united-states-per-capita/}\label{g:fn:idp23113624}}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_pseudo_summ}
%
\begin{itemize}[label=\textbullet]
\item{}The condition number of an \(m \times n\) matrix \(A\) is the number \(||A^{-1}|| \ ||A||\). The condition number provides a measure of how well the relative error in a calculated value \(\Delta \vb\) predicts the relative error in \(\Delta \vx\) when we are trying to solve a system \(A \vx = \vb\).%
\item{}A pseudoinverse \(A^{+}\) of a matrix \(A\) can be found through a singular value decomposition. Let \(U \Sigma V^{\tr}\) be a singular value decomposition of an \(m \times n\) matrix \(A\) of rank \(r\), with%
\begin{equation*}
\Sigma = \left[ \begin{array}{ccccc|c} \sigma_1\amp \amp \amp \amp \amp  \\ \amp  \sigma_2\amp \amp 0\amp \amp  \\ \amp \amp  \sigma_3\amp \amp \amp 0 \\ \amp  0 \amp  \amp  \ddots \amp  \amp  \\ \amp \amp \amp \amp  \sigma_r \\ \hline \amp \amp 0\amp \amp \amp 0 \end{array}  \right]
\end{equation*}
If \(\Sigma^+\) is the \(n \times m\) matrix defined by%
\begin{equation*}
\Sigma^{+} = \left[ \begin{array}{ccccc|c} \frac{1}{\sigma_1}\amp \amp \amp \amp \amp  \\ \amp  \frac{1}{\sigma_2}\amp \amp 0\amp \amp  \\ \amp \amp  \frac{1}{\sigma_3}\amp \amp \amp 0 \\ \amp  0 \amp  \amp  \ddots \amp  \amp  \\ \amp \amp \amp \amp  \frac{1}{\sigma_r} \\ \hline \amp \amp 0\amp \amp \amp 0 \end{array}  \right]\text{,}
\end{equation*}
then \(A^{+} = V\Sigma^{+}U^{\tr}\).%
\item{}A pseudoinverse \(A^{+}\) of a matrix \(A\) acts like an inverse for \(A\). So if we can't solve a matrix equation \(A \vx = \vb\) because \(\vb\) isn't in \(\Col A\), we can use the pseudoinverse of \(A\) to ``solve'' the equation \(A \vx = \vb\) with the ``solution'' \(A^+ \vb\). While not an exact solution, \(A^+ \vb\) turns out to be the best approximation to a solution in the least squares sense.%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_pseudo_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp23145368}%
Let \(A = \left[ \begin{array}{rcc} 20\amp 4\amp 32 \\ -4\amp 4\amp 2 \\ 35\amp 22\amp 26 \end{array}  \right]\). Then \(A\) has singular value decomposition \(U \Sigma V^{\tr}\) , where%
\begin{align*}
U \amp = \frac{1}{5}\left[ \begin{array}{crc} 3\amp 4\amp 0\\
0\amp 0\amp 5\\
4\amp -3\amp 0 \end{array} \right]\\
\Sigma \amp = \left[ \begin{array}{crc} 60\amp 0\amp 0\\
0\amp 15\amp 0\\
0\amp 0\amp 6 \end{array} \right]\\
V \amp = \frac{1}{3}\left[ \begin{array}{crr} 2\amp -1\amp -2\\
1\amp -2\amp 2\\
2\amp 2\amp 1 \end{array} \right]\text{.}
\end{align*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}What are the singular values of \(A\)?%
\item{}Write the outer product decomposition of \(A\).%
\item{}Find the best rank 1 approximation to \(A\). What is the relative error in approximating \(A\) by this rank 1 matrix?%
\item{}Find the best rank 2 approximation to \(A\). What is the relative error in approximating \(A\) by this rank 2 matrix?%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp23167384}%
Let \(A = \left[ \begin{array}{ccrr} 861\amp 3969\amp 70\amp 140 \\ 3969\amp 861\amp 70\amp 140 \\ 3969\amp 861\amp -70\amp -140 \\ 861\amp 3969\amp -70\amp -140 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find a singular value decomposition for \(A\).%
\item{}What are the singular values of \(A\)?%
\item{}Write the outer product decomposition of \(A\).%
\item{}Find the best rank 1, 2, and 3 approximations to \(A\). How much information about \(A\) does each of these approximations contain?%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp23167512}%
Assume that the number of feet traveled by a batted baseball at various angles in degrees (all hit at the same bat speed) is given in \hyperref[x:table:T_7_d_batting]{Table~{\xreffont\ref{x:table:T_7_d_batting}}}.%
\begin{tableptx}{\textbf{Distance traveled by batted ball}}{x:table:T_7_d_batting}{}%
\centering%
{\tabularfont%
\begin{tabular}{Alcccccc}\hrulethin
\multicolumn{1}{AlB}{Angle}&\multicolumn{1}{cA}{\(10^{\circ}\)}&\multicolumn{1}{cA}{\(20^{\circ}\)}&\multicolumn{1}{cA}{\(30^{\circ}\)}&\multicolumn{1}{cA}{\(40^{\circ}\)}&\multicolumn{1}{cA}{\(50^{\circ}\)}&\multicolumn{1}{cA}{\(60^{\circ}\)}\tabularnewline\hrulethin
\multicolumn{1}{AlB}{Distance}&\multicolumn{1}{cA}{116}&\multicolumn{1}{cA}{190}&\multicolumn{1}{cA}{254}&\multicolumn{1}{cA}{285}&\multicolumn{1}{cA}{270}&\multicolumn{1}{cA}{230}\tabularnewline\hrulethin
\end{tabular}
}%
\end{tableptx}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Plot the data and explain why a quadratic function is likely a better fit to the data than a linear function.%
\item{}Find the least squares quadratic approximation to this data. Plot the quadratic function on same axes as your data.%
\item{}At what angle (or angles), to the nearest degree, must a player bat the ball in order for the ball to travel a distance of 220 feet?%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp23176728}%
How close can a matrix be to being non-invertible? We explore that idea in this exercise. Let \(A = [a_{ij}]\) be the \(n \times n\) upper triangular matrix with 1s along the diagonal and with every other entry being \(-1\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}What is \(\det(A)\)? What are the eigenvalues of \(A\)? Is \(A\) invertible?%
\item{}Let \(B = [b_{ij}]\) be the \(n \times n\) matrix so that \(b_{n1} = -\frac{1}{2^{n-2}}\) and \(b_{ij} = a_{ij}\) for all other \(i\) and \(j\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}For the matrix \(B\) with \(n=3\), show that the equation \(B \vx = \vzero\) has a non-trivial solution. Find one non-trivial solution.%
\item{}For the matrix \(B\) with \(n=4\), show that the equation \(B \vx = \vzero\) has a non-trivial solution. Find one non-trivial solution.%
\item{}Use the pattern established in parts (i.) and (ii.) to find a non-trivial solution to the equation \(B \vx = \vzero\) for an arbitrary value of \(n\). Be sure to verify that you have a solution. Is \(B\) invertible?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp23198104}{}\quad{}For any positive integer \(m\), the sum \(1+\sum_{k=0}^{m-1} 2^k\) is the partial sum of a geometric series with ratio \(2\) and so \(1+\sum_{k=0}^{m-1} 2^k = 1+\frac{1-2^m}{1-2} = 2^m\).%
\item{}Explain why \(B\) is not an invertible matrix. Notice that \(A\) and \(B\) differ by a single entry, and that \(A\) is invertible and \(B\) is not. Let us examine how close \(A\) is to \(B\). Calculate \(|| A - B ||_F\)? What happens to \(||A - B||_F\) as \(n\) goes to infinity? How close can an invertible matrix be to becoming non-invertible?%
\end{enumerate}
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp23197336}%
Let \(A = \left[ \begin{array}{crr} 1\amp 0\amp 0 \\ 0\amp 1\amp -1 \\ 0\amp -1\amp 1 \end{array}  \right]\). In this exercise we find a matrix \(B\) so that \(B^2 = A\), that is, find a square root of the matrix \(A\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the eigenvalues and corresponding eigenvectors for \(A\) and \(A^{\tr}A\). Explain what you see.%
\item{}Find a matrix \(V\) that orthogonally diagonalizes \(A^{\tr}A\).%
\item{}\hyperlink{x:exercise:ex_7_c_Symmetric_SVD}{Exercise~{\xreffont 8}} in \hyperref[x:chapter:chap_SVD]{Section~{\xreffont\ref{x:chapter:chap_SVD}}} shows if \(U \Sigma V^{\tr}\) is a singular value decomposition for a symmetric matrix \(A\), then so is \(V \Sigma V^{\tr}\). Recall that \(A^n = \left(V \Sigma V^{\tr}\right)^n = V \Sigma^n V^{\tr}\) for any positive integer \(n\). We can exploit this idea to define \(\sqrt{A}\) to be the matrix%
\begin{equation*}
V \Sigma^{1/2}V^{\tr}\text{,}
\end{equation*}
where \(\Sigma^{1/2}\) is the matrix whose diagonal entries are the square roots of the corresponding entries of \(\Sigma\). Let \(B = \sqrt{A}\). Calculate \(B\) and show that \(B^2 = A\).%
\item{}Why was it important that \(A\) be a symmetric matrix for this process to work, and what had to be true about the eigenvalues of \(A\) for this to work?%
\item{}Can you extend the process in this exercise to find a cube root of \(A\)?%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp23279896}%
Let \(A\) be an \(m \times n\) matrix with singular value decomposition \(U \Sigma V^{\tr}\). Let \(A^{+}\) be defined as in \hyperref[x:men:eq_pseudoinverse]{({\xreffont\ref{x:men:eq_pseudoinverse}})}. In this exercise we prove the remaining parts of \hyperref[x:theorem:thm_7_d_pseudoinverse]{Theorem~{\xreffont\ref{x:theorem:thm_7_d_pseudoinverse}}}.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \((AA^{+})^{\tr} = AA^{+}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp23284760}{}\quad{}\(\Sigma \Sigma^+\) is a symmetric matrix.%
\item{}Show that \((A^{+}A)^{\tr} = A^{+}A\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp23290776}%
In this exercise we show that the pseudoinverse of a matrix is the unique matrix that satisfies the Moore-Penrose conditions. Let \(A\) be an \(m \times n\) matrix with singular value decomposition \(U \Sigma V^{\tr}\) and pseudoinverse \(X = V\Sigma^{+}U^{\tr}\). To show that \(A^{+}\) is the unique matrix that satisfies the Moore-Penrose conditions, suppose that there is another matrix \(Y\) that also satisfies the Moore-Penrose conditions.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(X = YAX\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp23288088}{}\quad{}Use the fact that \(X= XAX\).%
\item{}Show that \(Y = YAX\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp23287576}{}\quad{}Use the fact that \(Y= YAY\).%
\item{}How do the results of parts (a) and (b) show that \(A^{+}\) is the unique matrix satisfying the Moore-Penrose conditions?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp23298328}{}\quad{}Compare the results of (a) and (b).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp23298968}%
Find the pseudoinverse of the \(m \times n\) zero matrix \(A=0\). Explain the conclusion.%
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp23297176}%
In all of the examples that we have done finding a singular value decomposition of a matrix, it has been the case (though we haven't mentioned it), that if \(A\) is an \(m \times n\) matrix, then \(\rank(A) = \rank\left(A^{\tr}A\right)\). Prove this result.%
\end{divisionexercise}%
\begin{divisionexercise}{10}{}{}{g:exercise:idp23292952}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
A matrix has a pseudo-inverse if and only if the matrix is singular.%
\item{}\lititle{True\slash{}False.}\par%
The pseudoinverse of an invertible matrix \(A\) is the matrix \(A^{-1}\).%
\item{}\lititle{True\slash{}False.}\par%
If the columns of \(A\) are linearly dependent, then there is no least squares solution to \(A\vx = \vb\).%
\item{}\lititle{True\slash{}False.}\par%
If the columns of \(A\) are linearly independent, then there is a unique least squares solution to \(A\vx = \vb\).%
\item{}\lititle{True\slash{}False.}\par%
If \(T\) is the matrix transformation defined by a matrix \(A\) and \(S\) is the matrix transformation defined by \(A^{+}\), then \(T\) and \(S\) are inverse transformations.%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: GPS and Least Squares}
\typeout{************************************************}
%
\begin{sectionptx}{Project: GPS and Least Squares}{}{Project: GPS and Least Squares}{}{}{x:section:sec_proj_gps}
In this project we discuss some of the details about how the GPS works. The idea is based on intersections of spheres. To build a basic understanding of the system, we begin with a 2-dimensional example.%
\begin{project}{}{x:project:act_GPS_plane_ex}%
Suppose that there are three base stations \(A\), \(B\), and \(C\) in \(\R^2\) that can send and receive signals from your mobile phone. Assume that \(A\) is located at point \((-1,-2)\), \(B\) at point \((36,5)\), and \(C\) at point \((16,35)\). Also assume that your mobile phone location is point \((x,y)\). Based on the time that it takes to receive the signals from the three base stations, it can be determined that your distance to base station \(A\) is \(28\) km, your distance to base station \(B\) is \(26\) km, and your distance to base station \(C\) is \(14\) km using a coordinate system with measurements in kilometers based on a reference point chosen to be \((0,0)\). Due to limitations on the measurement equipment, these measurements all contain some unknown error which we will denote as \(z\). The goal is to determine your location in \(\R^2\) based on this information.%
\par
If the distance readings were accurate, then the point \((x,y)\) would lie on the circle centered at \(A\) of radius \(29\). The distance from \((x,y)\) to base station \(A\) can be represented in two different ways: \(28\) km and \(\sqrt{(x+1)^2 + (y+2)^2}\). However, there is some error in the measurements (due to the receiver clock and satellite clocks not being snychronized), so we really have%
\begin{equation*}
\sqrt{(x+1)^2 + (y+2)^2} + z = 28\text{,}
\end{equation*}
where \(z\) is the error. Similarly, \((x,y)\) must also satisfy%
\begin{equation*}
\sqrt{(x-36)^2 + (y-5)^2} + z = 26
\end{equation*}
and%
\begin{equation*}
\sqrt{(x-16)^2 + (y-35)^2} + z = 14\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain how these three equations can be written in the equivalent form%
\begin{align}
(x+1)^2+(y+2)^2\amp =(28-z)^2\label{x:mrow:eq_GPS_2D_1}\\
(x-36)^2 + (y-5)^2 \amp = (26-z)^2\label{x:mrow:eq_GPS_2D_2}\\
(x-16)^2 + (y-35)^2 \amp = (14-z)^2\text{.}\label{x:mrow:eq_GPS_2D_3}
\end{align}
\begin{figureptx}{Intersections of circles.}{x:figure:F_GPS_circles}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/7_d_Ex2_D_circles.pdf}
\end{image}%
\tcblower
\end{figureptx}%
%
\item{}If all measurements were accurate, your position would be at the intersection of the circles centered at \(A\) with radius \(28\) km, centered at \(B\) with radius \(26\) km, and centered at \(C\) with radius \(14\) km as shown in \hyperref[x:figure:F_GPS_circles]{Figure~{\xreffont\ref{x:figure:F_GPS_circles}}}. Even though the figure might seem to imply it, because of the error in the measurements the three circles do not intersect in one point. So instead, we want to find the best estimate of a point of intersection that we can. The system of \hyperref[x:mrow:eq_GPS_2D_1]{equations~({\xreffont\ref{x:mrow:eq_GPS_2D_1}})}, \hyperref[x:mrow:eq_GPS_2D_2]{({\xreffont\ref{x:mrow:eq_GPS_2D_2}})}, and \hyperref[x:mrow:eq_GPS_2D_3]{({\xreffont\ref{x:mrow:eq_GPS_2D_3}})} is non-linear and can be difficult to solve, if it even has a solution. To approximate a solution, we can linearize the system. To do this, show that if we subtract corresponding sides of equation  \hyperref[x:mrow:eq_GPS_2D_1]{({\xreffont\ref{x:mrow:eq_GPS_2D_1}})} from \hyperref[x:mrow:eq_GPS_2D_2]{({\xreffont\ref{x:mrow:eq_GPS_2D_2}})} and expand both sides, we can obtain the linear equation%
\begin{equation*}
37x + 7y + 2z = 712
\end{equation*}
in the unknowns \(x\), \(y\), and \(z\).%
\item{}Repeat the process in part (b), subtracting \hyperref[x:mrow:eq_GPS_2D_1]{({\xreffont\ref{x:mrow:eq_GPS_2D_1}})} from \hyperref[x:mrow:eq_GPS_2D_3]{({\xreffont\ref{x:mrow:eq_GPS_2D_3}})} and show that we can obtain the linear equation%
\begin{equation*}
17x + 37y + 14z = 1032
\end{equation*}
in \(x\), \(y\), and \(z\).%
\item{}We have reduced our system of three non-linear equations to the system%
\begin{alignat*}{4}
{37}x   \amp {}+{}   \amp {7}y   \amp {}+{}  \amp {2}z   \amp = \amp {}   \amp 712\\
{17}x    \amp {}+{}   \amp {37}y   \amp {}+{}   \amp {14}z  \amp = \amp {}  \amp 1032
\end{alignat*}
of two linear equations in the unknowns \(x\), \(y\), and \(z\). Use technology to find a pseudoinverse of the coefficient matrix of this system. Use the pseudoinverse to find the least squares solution to this system. Does your solution correspond to an approximate point of intersection of the three circles?%
\end{enumerate}
\end{project}%
\hyperref[x:project:act_GPS_plane_ex]{Project Activity~{\xreffont\ref{x:project:act_GPS_plane_ex}}} provides the basic idea behind GPS. Suppose you receive a signal from a GPS satellite. The transmission from satellite \(i\) provides four pieces of information \textemdash{} a location \((x_i,y_i,z_i)\) and a time stamp \(t_i\) according to the satellite's atomic clock. The time stamp allows the calculation of the distance between you and the \(i\)th satellite. The transmission travel time is calculated by subtracting the current time on the GPS receiver from the satellite's time stamp. Distance is then found by multiplying the transmission travel time by the rate, which is the speed of light \(c=299792.458\) km\slash{}s.\footnote{The signals travel in radio waves, which are electromagnetic waves, and travel at the speed of light. Also, \(c\) is the speed of light in a vacuum, but atmosphere is not too dense so we assume this value of \(c\)\label{g:fn:idp23345304}} So distance is found as \(c(t_i-d)\), where \(d\) is the time at the receiver. This signal places your location within in a sphere of that radius from the center of the satellite. If you receive a signal at the same time from two satellites, then your position is at the intersection of two spheres. As can be seen at left in \hyperref[x:figure:F_Spheres]{Figure~{\xreffont\ref{x:figure:F_Spheres}}}, that intersection is a circle. So your position has been narrowed quite a bit. Now if you receive simultaneous signals from three spheres, your position is narrowed to the intersection of three spheres, or two points as shown at right in \hyperref[x:figure:F_Spheres]{Figure~{\xreffont\ref{x:figure:F_Spheres}}}. So if we could receive perfect information from three satellites, then your location would be exactly determined.%
\begin{figureptx}{Intersections of spheres.}{x:figure:F_Spheres}{}%
\begin{sidebyside}{2}{0}{0}{0}%
\begin{sbspanel}{0.5}%
\includegraphics[width=\linewidth]{external/7_d_two_spheres.jpg}
\end{sbspanel}%
\begin{sbspanel}{0.5}%
\includegraphics[width=\linewidth]{external/7_d_three_spheres.jpg}
\end{sbspanel}%
\end{sidebyside}%
\tcblower
\end{figureptx}%
There is a problem with the above analysis \textemdash{} calculating the distances. These distances are determined by the time it takes for the signal to travel from the satellite to the GPS receiver. The times are measured by the clocks in the satellites and the clocks in the receivers. Since the GPS receiver clock is unlikely to be perfectly synchronized with the satellite clock, the distance calculations are not perfect. In addition, the rate at which the signal travels can change as the signal moves through the ionosphere and the troposphere. As a result, the calculated distance measurements are not exact, and are referred to as \terminology{pseudoranges}. In our calculations we need to factor in the error related to the time discrepancy and other factors. We will incorporate these errors into our measure of \(d\) and treat \(d\) as an unknown. (Of course, this is all more complicated that is presented here, but this provides the general idea.)%
\par
To ensure accuracy, the GPS uses signals from four satellites. Assume a satellite is positioned at point \((x_1,y_1,z_1)\) at a distance \(d_1\) from the GPS receiver located at point \((x,y,z)\). The distance can also be measured in two ways: as%
\begin{equation*}
\sqrt{(x-x_1)^2+(y-y_1)^2+(z-z_1)^2}\text{.}
\end{equation*}
and as \(c(t_1-d)\). So%
\begin{equation*}
c(t_1-d) = \sqrt{(x-x_1)^2 + (y-y_1)^2 + (z-z_1)^2}\text{.}
\end{equation*}
%
\par
Again, we are treating \(d\) as an unknown, so this equation has the four unknowns \(x\), \(y\), \(z\), and \(d\). Using signals from four satellites produces the system of equations%
\begin{align}
\sqrt{(x-x_1)^2 + (y-y_1)^2 + (z-z_1)^2} \amp =  c(t_1-d)\label{x:mrow:eq_GPS_1_sqrt}\\
\sqrt{(x-x_2)^2 + (y-y_2)^2 + (z-z_2)^2}  \amp =  c(t_2-d)\label{x:mrow:eq_GPS_2_sqrt}\\
\sqrt{(x-x_3)^2 + (y-y_3)^2 + (z-z_3)^2} \amp =  c(t_3-d)\label{x:mrow:eq_GPS_3_sqrt}\\
\sqrt{(x-x_4)^2 + (y-y_4)^2 + (z-z_4)^2} \amp =  c(t_4-d)\text{.}\label{x:mrow:eq_GPS_4_sqrt}
\end{align}
%
\begin{project}{}{x:project:act_GPS_3D}%
The system of equations \hyperref[x:mrow:eq_GPS_1_sqrt]{({\xreffont\ref{x:mrow:eq_GPS_1_sqrt}})}, \hyperref[x:mrow:eq_GPS_2_sqrt]{({\xreffont\ref{x:mrow:eq_GPS_2_sqrt}})}, \hyperref[x:mrow:eq_GPS_3_sqrt]{({\xreffont\ref{x:mrow:eq_GPS_3_sqrt}})}, and \hyperref[x:mrow:eq_GPS_4_sqrt]{({\xreffont\ref{x:mrow:eq_GPS_4_sqrt}})} is a non-linear system and is difficult to solve, if it even has a solution. We want a method that will provide at least an approximate solution as well as apply if we use more than four satellites. We choose a reference node (say \((x_1, y_1, z_1)\)) and make calculations relative to that node as we did in \hyperref[x:project:act_GPS_plane_ex]{Project Activity~{\xreffont\ref{x:project:act_GPS_plane_ex}}}.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}First square both sides of the equations \hyperref[x:mrow:eq_GPS_1_sqrt]{({\xreffont\ref{x:mrow:eq_GPS_1_sqrt}})}, \hyperref[x:mrow:eq_GPS_2_sqrt]{({\xreffont\ref{x:mrow:eq_GPS_2_sqrt}})}, \hyperref[x:mrow:eq_GPS_3_sqrt]{({\xreffont\ref{x:mrow:eq_GPS_3_sqrt}})}, and \hyperref[x:mrow:eq_GPS_4_sqrt]{({\xreffont\ref{x:mrow:eq_GPS_4_sqrt}})} to remove the roots. Then subtract corresponding sides of the new first equation (involving \((x_1,y_1,z_1)\)) from the new second equation  (involving \((x_2,y_2,z_2)\)) to show that we can obtain the linear equation%
\begin{equation*}
2(x_2-x_1)x + 2(y_2-y_1)y + 2(z_2-z_1)z + 2c^2(t_1-t_2)d = c^2(t_1^2-t_2^2)  - h_1 +h_2\text{,}
\end{equation*}
where \(h_i = x_i^2 + y_i^2 + z_i^2\). (Note that the unknowns are \(x\), \(y\), \(z\), and \(d\) \textemdash{} all other quantities are known.)%
\item{}Use the result of part (a) to write a linear system that can be obtained by subtracting the first equation from the third and fourth equations as well.%
\item{}The linearizations from part (b) determine a system \(A \vx = \vb\) of linear equations. Identify \(A\), \(\vx\), and \(\vb\). Then explain how we can approximate a best solution to this system in the least squares sense.%
\end{enumerate}
\end{project}%
We conclude this project with a final note. At times a GPS receiver may only be able to receive signals from three satellites. In these situations, the receiver can substitute the surface of the Earth as a fourth sphere and continue the computation.%
\end{sectionptx}
\end{chapterptx}
\end{partptx}
%
%
\typeout{************************************************}
\typeout{Chapter VII Vector Spaces}
\typeout{************************************************}
%
\begin{partptx}{Vector Spaces}{}{Vector Spaces}{}{}{x:part:part-vec-spaces}
 %
%
\typeout{************************************************}
\typeout{Section 31 Vector Spaces}
\typeout{************************************************}
%
\begin{chapterptx}{Vector Spaces}{}{Vector Spaces}{}{}{x:chapter:chap_vector_spaces}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp23370008}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What is a vector space?%
\item{}What is a subspace of a vector space?%
\item{}What is a linear combination of vectors in a vector space \(V\)?%
\item{}What is the span of a set of vectors in a vector space \(V\)?%
\item{}What special structure does the span of a set of vectors in a vector space \(V\) have?%
\item{}Why is the vector space concept important?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: The Hat Puzzle}
\typeout{************************************************}
%
\begin{sectionptx}{Application: The Hat Puzzle}{}{Application: The Hat Puzzle}{}{}{x:section:sec_appl_hat_puzzle}
In a New York Times article (April 10, 2001) ``Why Mathematicians Now Care About Their Hat Color'', the following puzzle is posed.%
\begin{quote}%
``Three players enter a room and a red or blue hat is placed on each person's head. The color of each hat is determined by a coin toss, with the outcome of one coin toss having no effect on the others. Each person can see the other players' hats but not his own. No communication of any sort is allowed, except for an initial strategy session before the game begins. Once they have had a chance to look at the other hats, the players must simultaneously guess the color of their own hats or pass. The group shares a hypothetical \textdollar{}3 million prize if at least one player guesses correctly and no players guess incorrectly.''\end{quote}
The game can be played with more players, and the problem is to find a strategy for the group that maximizes its chance of winning. One strategy is for a designated player to make a random guess and for the others to pass. This gives a 50\% chance of winning. However, there are much better strategies that provide a nearly 100\% probability of winning as the number of players increases. One such strategy is based on Hamming codes and subspaces of a particular vector space to implement the most effective approach.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_vec_space_intro}
We have previously seen that \(\R^n\) and the set of fixed size matrices have a nice algebraic structure when endowed with the addition and scalar multiplication operations. In fact, as we will see, there are many other sets of elements that have the same kind of structure with natural addition and scalar multiplication operations. Due to this underlying similar structure, these sets are connected in some way and can all be studied jointly. Mathematicians look for these kinds of connections between seemingly dissimilar objects and, from a mathematical standpoint, it is convenient to study all of these similar structures at once by combining them into a larger collection. This motivates the idea of a \terminology{vector space} that we will investigate in this chapter.%
\par
An example of a set that has a structure similar to vectors is a collection of polynomials. Let \(\pol_1\) be the collection of all polynomials of degree less than or equal to 1 with real coefficients. That is,%
\begin{equation*}
\pol_1 = \{ a_0 + a_1t : a_0, a_1 \in \R\}\text{.}
\end{equation*}
%
\par
So, for example, the polynomials \(2+t\), \(5t\), \(-7\), and \(\sqrt{12}-\pi t\) are in \(\pol_1\), but \(\sqrt{t}\) is not in \(\pol_1\). Two polynomials \(a(t)=a_0+a_1t\) and \(b(t)=b_0+b_1t\) in \(\pol_1\) are equal if \(a_0=b_0\) and \(a_1=b_1\).%
\par
We define addition of polynomials in \(\pol_1\) by adding the coefficients of the like degree terms. So if \(a(t) = a_0+a_1t\) and \(b(t) = b_0+b_1t\), then the polynomial sum of \(a(t)\) and \(b(t)\) is%
\begin{equation*}
a(t)+b(t) = (a_0+a_1t) + (b_0+b_1t) = (a_0+b_0) + (a_1+b_1)t\text{.}
\end{equation*}
%
\par
So, for example,%
\begin{equation*}
(2+3t)+(-1+5t) = (2+(-1)) + (3+5)t = 1+8 t\text{.}
\end{equation*}
%
\par
We now consider the properties of the addition operation. For example, we can ask if polynomial addition is commutative. That is, if \(a(t)\) and \(b(t)\) are in \(\pol_1\), must it be the case that%
\begin{equation*}
a(t)+b(t)~=~b(t)+a(t)?
\end{equation*}
%
\par
To show that addition is commutative in \(\pol_1\), we choose arbitrary polynomials \(a(t) = a_0 + a_1t\) and \(b(t) = b_0 + b_1t\) in \(\pol_1\). Then we have%
\begin{align*}
a(t)+b(t) \amp = (a_0+b_0) + (a_1+b_1)t\\
\amp = (b_0+a_0) + (b_1+a_1)t\\
\amp = b(t)+a(t)\text{.}
\end{align*}
%
\par
Note that in the middle step, we used the definition of equality of polynomials since \(a_0+b_0=b_0+a_0\) and \(a_1+b_1=b_1+a_1\) due to the fact that addition of real numbers is commutative. So addition of elements in \(\pol_1\) is a commutative operation.%
\begin{exploration}{}{x:exploration:pa_5_a}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Now we investigate other properties of addition in \(\pol_1\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}To show addition is associative in \(\pol_1\), we need to verify that if \(a(t) = a_0 + a_1t\), \(b(t) = b_0+b_1t\), and \(c(t) = c_0 + c_1t\) are in \(\pol_1\), it must be the case that%
\begin{equation*}
(a(t) + b(t)) + c(t) = a(t) + (b(t)+c(t))\text{.}
\end{equation*}
Either verify this property by using the definition of two polynomials being equal, or give a counterexample to show the equality fails in that case.%
\item{}Find a polynomial \(z(t) \in \pol_1\) such that%
\begin{equation*}
a(t) + z(t) = a(t)
\end{equation*}
for all \(a(t) \in \pol_1\). This polynomial is called the \emph{zero} polynomial or the \emph{additive identity} polynomial in \(\pol_1\).%
\item{}If \(a(t) = a_0 + a_1t\) is an element of \(\pol_1\), is there an element \(p(t) \in \pol_1\) such that%
\begin{equation*}
a(t) + p(t) = z(t)\text{,}
\end{equation*}
where \(z(t)\) is the additive identity polynomial you found above? If not, why not? If so, what polynomial is \(p(t)\)? Explain.%
\end{enumerate}
\item{}We can also define a multiplication of polynomials by scalars (real numbers).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}What element in \(\pol_1\) could be the scalar multiple \(\frac{1}{2} (2+3t)\)?%
\item{}In general, if \(k\) is a scalar and \(a(t) = a_0 + a_1t\) is in \(\pol_1\), how do we define the scalar multiple \(ka(t)\) in \(\pol_1\)?%
\item{}If \(k\) is a scalar and \(a(t) = a_0 + a_1t\) and \(b(t) = b_0 + b_1t\) are elements in \(\pol_1\), is it true that%
\begin{equation*}
k(a(t) + b(t)) = ka(t) + kb(t)?
\end{equation*}
If no, explain why. If yes, verify your answer using the definition of two polynomials being equal.%
\item{}If \(k\) and \(m\) are scalars and \(a(t) = a_0 + a_1t\) is an element in \(\pol_1\), is it true that%
\begin{equation*}
(k+m)a(t) = ka(t) + ma(t)?
\end{equation*}
If no, explain why. If yes, verify your answer.%
\item{}If \(k\) and \(m\) are scalars and \(a(t) = a_0 + a_1t\) is an element in \(\pol_1\), is it true that%
\begin{equation*}
(km)a(t) = k(ma(t))?
\end{equation*}
If no, explain why. If yes, verify your answer.%
\item{}If \(a(t) = a_0 + a_1t\) is an element of \(\pol_1\), is it true that%
\begin{equation*}
1a(t) = a(t)?
\end{equation*}
If no, explain why. If yes, verify your answer.%
\end{enumerate}
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Spaces with Similar Structure to \(\R^n\)}
\typeout{************************************************}
%
\begin{sectionptx}{Spaces with Similar Structure to \(\R^n\)}{}{Spaces with Similar Structure to \(\R^n\)}{}{}{x:section:sec_space_like_rn}
Mathematicians look for patterns and for similarities between mathematical objects. In doing so, mathematicians often consider larger collections of objects that are sorted according to their similarities and then study these collections rather than just the objects themselves. This perspective can be very powerful \textemdash{} whatever can be shown to be true about an arbitrary element in a collection will then be true for every specific element in the collection. In this section we study the larger collection of sets that share the algebraic structure of vectors in \(\R^n\). These sets are called \terminology{vector spaces}.%
\par
In \hyperref[x:exploration:pa_5_a]{Preview Activity~{\xreffont\ref{x:exploration:pa_5_a}}}, we showed that the set \(\pol_1\) of polynomials of degree less than or equal to one with real coefficients, with the operations of addition and scalar multiplication defined by%
\begin{equation*}
(a_0+a_1t)+(b_0+b_1t) = (a_0+b_0) + (a_1+b_1)t \ \ \ \text{ and }  \ \ \ k(a_0+a_1t) = (ka_0) + (ka_1)t\text{,}
\end{equation*}
has a structure similar to \(\R^2\).%
\par
By structure we mean how the elements in the set relate to each other under addition and multiplication by scalars. That is, if \(a(t)=a_0+a_1t\), \(b(t)\), and \(c(t)\) are elements of \(\pol_1\) and \(k\) and \(m\) are scalars, then%
\begin{enumerate}
\item{}\(a(t) + b(t)\) is an element of \(\pol_1\),%
\item{}\(a(t)+b(t) = b(t) + a(t)\),%
\item{}\((a(t)+b(t)) + c(t) = a(t) + (b(t)+c(t))\),%
\item{}there is a zero polynomial \(z(t)\) (namely, \(0+0t\)) in \(\pol_1\) so that \(a(t) + z(t) = a(t)\),%
\item{}there is an element \(-a(t)\) in \(\pol_1\) (namely, \((-a_0)+(-a_1)t\)) so that \(a(t) + (-a(t)) = z(t)\),%
\item{}\(k a(t)\) is an element of \(\pol_1\),%
\item{}\((k+m) a(t) = ka(t) + ma(t)\),%
\item{}\(k(a(t)+b(t)) = ka(t) + kb(t)\),%
\item{}\((km) a(t) = k(m a(t))\),%
\item{}\(1 a(t) = a(t)\).%
\end{enumerate}
%
\par
The properties we saw for polynomials in \(\pol_1\) stated above are the same as the properties for vector addition and multiplication by scalars in \(\R^n\), as well as matrix addition and multiplication by scalars identified in \hyperref[x:chapter:chap_matrix_operations]{Section~{\xreffont\ref{x:chapter:chap_matrix_operations}}}. This indicates that polynomials in \(\pol_1\), vectors in \(\R^n\), and the set of \(m \times n\) matrices behave in much the same way as regards their addition and multiplication by scalars. There is an even closer connection between linear polynomials and vectors in \(\R^2\). An element \(a(t) = a_0 + a_1t\) in \(\pol_1\) can be naturally associated with the vector \(\left[ \begin{array}{c} a_0 \\ a_1 \end{array}  \right]\) in \(\R^2\). All the results of polynomial addition and multiplication by scalars then translate to corresponding results of addition and multiplication by scalars of vectors in \(\R^2\). So for all intents and purposes, as far as addition and multiplication by scalars is concerned, there is no difference between elements in \(\pol_1\) and vectors in \(\R^2\) \textemdash{} the only difference is how we choose to present the elements (as polynomials or as vectors). This sameness of structure of our sets as it relates to addition and multiplication by scalars is the type of similarity mentioned in the introduction. We can study all of the types of objects that exhibit this same structure at one time by studying vector spaces.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Vector Spaces}
\typeout{************************************************}
%
\begin{sectionptx}{Vector Spaces}{}{Vector Spaces}{}{}{x:section:sec_vec_space}
We defined vector spaces in the context of subspaces of \(\R^n\) in \hyperref[x:definition:def_3_a_1]{Definition~{\xreffont\ref{x:definition:def_3_a_1}}}. In general, any set that has the same kind of additive and multiplicative structure as our sets of vectors, matrices, and linear polynomials is called a vector space. As we will see, the ideas that we introduced about subspaces of \(\R^n\) apply to vector spaces in general, so the material in this chapter should have a familiar feel.%
\begin{definition}{}{x:definition:def_vector_space}%
\index{vector space}%
A set \(V\) on which an operation of addition and a multiplication by scalars is defined is a \terminology{vector space} if for all \(\vu\), \(\vv\), and \(\vw\) in \(V\) and all scalars \(a\) and \(b\):%
\begin{enumerate}
\item{}\(\vu + \vv\) is an element of \(V\) (we say that \(V\) is \terminology{closed} under the addition in \(V\)),%
\item{}\(\vu + \vv = \vv + \vu\) (we say that the addition in \(V\) is \terminology{commutative}),%
\item{}\((\vu + \vv) + \vw = \vu + (\vv + \vw)\) (we say that the addition in \(V\) is \terminology{associative}),%
\item{}there is a zero vector \(\vzero\) in \(V\) so that \(\vu + \vzero = \vu\) (we say that \(V\) contains an \terminology{additive identity} \(\vzero\)),%
\item{}for each \(\vx\) in \(V\) there is an element \(\vy\) in \(V\) so that \(\vx + \vy = \vzero\) (we say that \(V\) contains an \terminology{additive inverse} \(\vy\) for each element \(\vx\) in \(V\)),%
\item{}\(a \vu\) is an element of \(V\) (we say that \(V\) is \terminology{closed} under multiplication by scalars),%
\item{}\((a+b) \vu = a\vu + b\vu\) (we say that \terminology{multiplication by scalars distributes over scalar addition}),%
\item{}\(a(\vu + \vv) = a\vu + a\vv\) (we say that \terminology{multiplication by scalars distributes over addition in \(V\)}),%
\item{}\((ab) \vu = a(b\vu)\),%
\item{}\(1 \vu = \vu\).%
\end{enumerate}
%
\end{definition}
\begin{paragraphs}{Note.}{g:paragraphs:idp23476376}%
\index{scalars}%
Unless otherwise stated, in this book the scalars will refer to real numbers. However, we can define vector spaces where scalars are complex numbers, or rational numbers, or integers modulo \(p\) where \(p\) is a prime number, or, more generally, elements of a field. A field is an algebraic structure which generalizes the structure of real numbers and rational numbers under the addition and multiplication operations. Since we will focus on the real numbers as scalars, the reader is not required to be familiar with the concept of a field.%
\end{paragraphs}%
\par
Because of the similarity of the way elements in vector spaces behave compared to vectors in \(\R^n\), we call the elements in a vector space \terminology{vectors}. There are many examples of vectors spaces, which is what makes this idea so powerful.%
\begin{example}{}{g:example:idp23473304}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}The space \(\R^n\) of all vectors with \(n\) components is a vector space using the standard vector addition and multiplication by scalars. The zero element is the zero vector \(\vzero\) whose components are all 0.%
\item{}The set \(\pol_1\) of all polynomials of degree less than or equal to 1 with addition and scalar multiplication as defined earlier. Recall that \(\pol_1\) is essentially the same as \(\R^2\).%
\item{}The properties listed in the introduction for \(\pol_1\) are equally true for the collection of all polynomials of degree less than or equal to some fixed number. We label as \(\pol_n\) this set of all polynomials of degree less than or equal to \(n\), with the standard addition and scalar multiplication. Note that \(\pol_n\) is essentially the same as \(\R^{n+1}\). More generally, the space \(\pol\) of all polynomials is also a vector space with standard addition and scalar multiplication.%
\item{}As a subspace of \(\R^n\), the eigenspace of an \(n \times n\) matrix corresponding to an eigenvalue \(\lambda\) is a vector space.%
\item{}As a subspace of \(\R^n\), the null space of an \(m \times n\) matrix is a vector space.%
\item{}As a subspace of \(\R^m\), the column space of an \(m \times n\) matrix is a vector space.%
\item{}The span of a set of vectors in \(\R^n\) is a subspace of \(\R^n\), and is therefore a vector space.%
\item{}Let \(V\) be a vector space and let \(\vzero\) be the additive identity in \(V\). The set \(\{\vzero\}\) is a vector space in which \(\vzero + \vzero = \vzero\) and \(k\vzero = \vzero\) for any scalar \(k\). This space is called the \emph{trivial} vector space.%
\item{}The space \(\M_{m \times n}\) (or \(\M_{m \times n}(\R)\) when it is important to indicate that the entries of our matrices are real numbers) of all \(m \times n\) matrices with real entries with the standard addition and multiplication by scalars we have already defined. In this case, \(\M_{m \times n}\) is essentially the same vector space as \(\R^{mn}\).%
\item{}The space \(\F\) of all functions from \(\R\) to \(\R\), where we define the sum of two functions \(f\) and \(g\) in \(\F\) as the function \(f+g\) satisfying%
\begin{equation*}
(f+g)(x) = f(x) + g(x)
\end{equation*}
for all real numbers \(x\), and the scalar multiple \(cf\) of the function \(f\) by the scalar \(c\) to be the function satisfying%
\begin{equation*}
(cf)(x) = cf(x)
\end{equation*}
for all real numbers \(x\). The verification of the vector space properties for this space is left to the reader.%
\item{}The space \(\R^\infty\) of all infinite real sequences \((x_1, x_2, x_3, \ldots)\). We define addition and scalar multiplication termwise:%
\begin{equation*}
(x_1, x_2, x_3, \ldots) + (y_1, y_2, y_3, \ldots) = (x_1+y_1, x_2+y_2, x_3+y_3, \ldots) \,\text{,}
\end{equation*}
%
\begin{equation*}
c(x_1, x_2, x_3, \ldots) = (cx_1, cx_2, cx_3, \ldots) \\text{,}
\end{equation*}
is a vector space. In addition, the set of convergent sequences inside \(\R^\infty\) forms a vector space using this addition and multiplication by scalars (as we did in \(\R^n\), we will call this set of convergent sequences a subspace of \(\R^{\infty}\)).%
\item{}(For those readers who are familiar with differential equations). The set of solutions to a second order homogeneous differential equation forms a vector space under addition and scalar multiplication defined as in the space \(\F\) above.%
\item{}The set of polynomials of positive degree in \(\pol_1\) is not a vector space using the standard addition and multiplication by scalars in \(\pol_1\) is not a vector space. Notice that \(t + (-t)\) is not a polynomial of positive degree, and so this set is not closed under addition.%
\item{}The color space where each color is assigned an RGB (red, green, blue) coordinate between 0 and 255, with addition and scalar multiplication defined component-wise, however, does not define a vector space. The color space is not closed under either operation due to the color coordinates being integers ranging from 0 to 255.%
\end{enumerate}
\end{example}
It is important to note that the set of defining properties of a vector space is intended to be a minimum set. Any other properties of a vector space must be verified or proved using the defining properties. For example, in \(\R^n\) it is clear that the scalar multiple \(0\vv\) is the zero vector for any vector \(\vv\) in \(\R^n\). This might be true in any vector space, but it is not a defining property. Therefore, if this property is true, then we must be able to prove it using just the defining properties. To see how this might work, let \(\vv\) be any vector in a vector space \(V\). We want to show that \(0 \vv = \vzero\) (the existence of the zero vector is property (4)). Using the fact that \(0+0 = 0\) and that scalar multiplication distributes over scalar addition, we can see that%
\begin{equation*}
0\vv = (0+0)\vv = 0\vv + 0\vv\text{.}
\end{equation*}
%
\par
Property (5) tells us that \(V\) contains an additive inverse for every vector in \(V\), so let \(\vu\) be an additive inverse of the vector \(0\vv\) in \(V\). Then \(0\vv + \vu = \vzero\)\footnote{It is very important to keep track of the different kinds of zeros here \textemdash{} the boldface zero \(\vzero\) is the additive identity in the vector space and the non-bold 0 is the scalar zero.\label{g:fn:idp23508504}} and so%
\begin{align*}
0\vv + \vu \amp = (0\vv + 0\vv) + \vu\\
\vzero \amp = 0\vv + (0\vv + \vu)\\
\vzero \amp = 0\vv + \vzero\text{.}
\end{align*}
%
\par
Now \(\vzero\) has the property that \(\vzero + \vw = \vw+ \vzero = \vw\) for any vector \(\vw\) in \(V\) (by properties (4) and (2)), and so we can conclude that%
\begin{equation*}
\vzero = 0\vv\text{.}
\end{equation*}
%
\begin{activity}{}{x:activity:act_5_a_1}%
Another property that will be useful is a cancellation property. In the set of real numbers we know that if \(a+b = c+b\), then \(a=c\), and we verify this by subtracting \(b\) from both sides. This is the same as adding the additive inverse of \(b\) to both sides, so we ought to be able to make the same argument using additive inverses in a vector space. To see how, let \(\vu\), \(\vv\), and \(\vw\) be vectors in a vector space and suppose that%
\begin{equation}
\vu + \vw = \vv + \vw\text{.}\label{x:men:eq_5_a_1}
\end{equation}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Why does our space contain an additive inverse \(\vz\) of \(\vw\)?%
\item{}Now add the vector \(\vz\) to both sides of equation \hyperref[x:men:eq_5_a_1]{({\xreffont\ref{x:men:eq_5_a_1}})} to obtain%
\begin{equation}
(\vu+\vw) + \vz = (\vv+\vw) + \vz\text{.}\label{x:men:eq_5_a_2}
\end{equation}
Which property of a vector space allows us to state the following equality?%
\begin{equation}
\vu+(\vw+\vz) = \vv+(\vw+\vz)\text{.}\label{x:men:eq_5_a_3}
\end{equation}
%
\item{}Now use the properties of additive inverses and the additive identity to explain why \(\vu = \vv\). Conclude that we have a cancellation law for addition in any vector space.%
\end{enumerate}
\end{activity}%
We should also note that the definition of a vector space only states the existence of a zero vector and an additive inverse for each vector in the space, and does not say that there cannot be more than one zero vector or more than one additive inverse of a vector in the space. The reason why is that the uniqueness of the zero vector and an additive inverse of a vector can be proved from the defining properties of a vector space, and so we don't list this consequence as a defining property. Similarly, the defining properties of a vector space do not state that the additive inverse of a vector \(\vv\) is the scalar multiple \((-1)\vv\). Verification of these properties are left for the exercises. We summarize the results of this section in the following theorem.%
\begin{theorem}{}{}{g:theorem:idp23523992}%
Let \(V\) be any vector space with identity \(\vzero\).%
\begin{itemize}[label=\textbullet]
\item{}\(0 \vv = \vzero\) for any vector \(\vv\) in \(V\).%
\item{}The vector \(\vzero\) is unique.%
\item{}\(c \vzero = \vzero\) for any scalar \(c\).%
\item{}For any \(\vv\) in \(V\), the additive inverse of \(\vv\) is unique.%
\item{}The additive inverse of a vector \(\vv\) in \(V\) is the vector \((-1)\vv\).%
\item{}If \(\vu\), \(\vv\), and \(\vw\) are in \(V\) and \(\vu + \vw = \vv + \vw\), then \(\vu = \vv\).%
\end{itemize}
%
\end{theorem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Subspaces}
\typeout{************************************************}
%
\begin{sectionptx}{Subspaces}{}{Subspaces}{}{}{x:section:sec_subspaces}
\begin{introduction}{}%
In \hyperref[x:chapter:chap_R_n]{Section~{\xreffont\ref{x:chapter:chap_R_n}}} we saw that \(\R^n\) contained subsets that we called subspaces that had the same algebraic structure as \(\R^n\). The same idea applies to vector spaces in general.%
\begin{activity}{}{x:activity:act_5_a_2}%
Let \(H = \left\{ at : a \in \R \right\}\). Notice that \(H\) is a subset of \(\pol_1\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Is \(H\) closed under the addition in \(\pol_1\)? Verify your answer.%
\item{}Does \(H\) contain the zero vector from \(\pol_1\)? Verify your answer.%
\item{}Is \(H\) closed under multiplication by scalars? Verify your answer.%
\item{}Explain why \(H\) satisfies every other property of the definition of a vector space automatically just by being a subset of \(\pol_1\) and using the same operations as in \(\pol_1\). Conclude that \(H\) is a vector space.%
\end{enumerate}
\end{activity}%
\hyperref[x:activity:act_5_a_2]{Activity~{\xreffont\ref{x:activity:act_5_a_2}}} illustrates an important point. There is a fundamental difference in the types of properties that define a vector space. Some of the properties that define a vector space are true for any subset of the vector space because they are properties of the operations (such as the commutative and associative properties). The other properties (closure, the inclusion of the zero vector, and the inclusion of additive inverses) are set properties, not properties of the operations. So these three properties have to be specifically checked to see if a subset of a vector space is also a vector space. This leads to the definition of a \terminology{subspace}, a subset of a vector space which is a vector space itself.%
\begin{definition}{}{x:definition:def_5_a_subspace}%
\index{vector space!subspace}%
\index{subspace of a vector space}%
A subset \(H\) of a vector space \(V\) is a \terminology{subspace} of \(V\) if%
\begin{enumerate}
\item{}whenever \(\vu\) and \(\vv\) are in \(H\) it is also true that \(\vu + \vv\) is in \(H\) (that is, \(H\) is \terminology{closed} under addition),%
\item{}whenever \(\vu\) is in \(H\) and \(a\) is a scalar it is also true that \(a\vu\) is in \(H\) (that is, \(H\) is \terminology{closed} under scalar multiplication),%
\item{}\(\vzero\) is in \(H\).%
\end{enumerate}
%
\end{definition}
\begin{activity}{}{x:activity:act_5_a_3}%
Is the given subset \(H\) a subspace of the indicated vector space \(V\)? Verify your answer.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(V\) is any vector space and \(H = \{\vzero\}\)%
\item{}\(V = M_{2 \times 2}\), the vector space of \(2 \times 2\) matrices and \(H = \left\{ \left[ \begin{array}{cc} 2x\amp y \\ 0 \amp x \end{array} \right] \big| \text{ \(x\) and \(y\) are scalars } \right\}\).%
\item{}\(V = \pol_2\), the vector space of all polynomials of degree less than or equal to 2 and \(H = \left\{ 2at^2+1 \mid a \text{ is a scalar } \right\}\).%
\item{}\(V=\pol_2\) and \(H=\left\{at\mid a \text{ is a scalar } \right\} \cup \left\{bt^2\mid b \text{ is a scalar } \right\}\).%
\item{}\(V=\F\) and \(H=\pol_2\).%
\end{enumerate}
\end{activity}%
There is an interesting subspace relationship between the spaces \(\pol_1, \pol_2, \pol_3, \ldots\) and \(\pol\). For every \(i\), \(\pol_i\) is a subspace of \(\pol\). Furthermore, \(\pol_1\) is a subspace of \(\pol_2\), \(\pol_2\) is a subspace of \(\pol_3\), and so on. Note however that a similar relationship does NOT hold for \(\R^n\), even though \(\pol_i\) looks like \(\R^{i+1}\). For example, \(\R^1\) is NOT a subspace of \(\R^2\). Similarly, \(\R^2\) is NOT a subspace of \(\R^3\). Since the vectors in different \(\R^n\)'s are of different sizes, none of the \(\R^i\)'s is a subset of another \(\R^n\) with \(i \neq n\), and hence, \(\R^i\) is not a subspace of \(\R^n\) when \(i\lt n\).%
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  The Subspace Spanned by a Set of Vectors}
\typeout{************************************************}
%
\begin{subsectionptx}{The Subspace Spanned by a Set of Vectors}{}{The Subspace Spanned by a Set of Vectors}{}{}{g:subsection:idp23736200}
In \(\R^n\) we showed that the span of any set of vectors forms a subspace of \(\R^n\). The same is true in any vector space. Recall that the span of a set of vectors in \(\R^n\) is the set of all linear combinations of those vectors. So before we can discuss the span of a set of vectors in a vector space, we need to extend the definition of linear combinations to vector spaces (compare to \hyperref[x:definition:x1_d_linear_combination]{Definition~{\xreffont\ref{x:definition:x1_d_linear_combination}}} and \hyperref[x:definition:def_1_d_span]{Definition~{\xreffont\ref{x:definition:def_1_d_span}}}).%
\begin{definition}{}{g:definition:idp23738120}%
Let \(V\) be a vector space. A \terminology{linear combination}\index{linear combination of vectors in a vector space} of vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) in \(V\) is a vector of the form%
\begin{equation*}
x_1\vv_1+x_2\vv_2 + \cdots + x_k \vv_k\text{,}
\end{equation*}
where \(x_1\), \(x_2\), \(\ldots\), \(x_k\) are scalars. The \terminology{span}\index{span of a set of vectors in a vector space} of the vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) is the collection of all linear combinations of \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\). That is,%
\begin{equation*}
\Span\{\vv_1, \vv_2, \ldots, \vv_k\} = \{x_1\vv_1+x_2\vv_2 + \cdots + x_k \vv_k \mid x_1, x_2, \ldots, x_k \text{ are scalars } \}\text{.}
\end{equation*}
%
\end{definition}
The argument that the span of any finite set of vectors in a vector space forms a subspace is the same as we gave for the span of a set of vectors in \(\R^n\) (see \hyperref[x:theorem:thm_3_a_span_subspace]{Theorem~{\xreffont\ref{x:theorem:thm_3_a_span_subspace}}}). The proof is left for the exercises.%
\begin{theorem}{}{}{x:theorem:thm_VS_span}%
Given a vector space \(V\) and vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_m\) in \(V\), \(\Span\{\vv_1, \vv_2, \ldots, \vv_m\}\) is a subspace of \(V\).%
\end{theorem}
\index{subspace of a vector space spanned by a set of vectors} The subspace \(\Span\{\vv_1, \vv_2, \ldots, \vv_m\}\) is called the \terminology{subspace of \(V\) spanned by \(\vv_1, \vv_2, \ldots, \vv_m\)}.%
\begin{activity}{}{x:activity:act_5_a_4}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(H = \left\{ a_2t^2-a_1t : a_2 \text{ and } a_1 \text{ are real numbers } \right\}\). Note that \(H\) is a subset of \(\pol_2\). Find two vectors \(\vv_1, \vv_2\) in \(\pol_2\) so that \(H= \Span\{\vv_1, \vv_2\}\) and hence conclude that \(H\) is a subspace of \(\pol_2\). (Note that the vectors \(\vv_1, \vv_2\) are not unique.)%
\item{}Let \(p_1(t) = 1-t^2\) and \(p_2(t) = 1+t^2\), and let \(S = \{p_1(t), p_2(t)\}\) in \(\pol_2\). Is the polynomial \(q(t) = 3-2t^2\) in \(\Span \ S\)? (Hint: Create a matrix equation of the form \(A \vx = \vb\) by setting up an appropriate polynomial equation involving \(p_1(t)\), \(p_2(t)\) and \(q(t)\). Under what conditions on \(A\) is the system \(A \vx = \vb\) consistent?)%
\item{}With \(S\) as in part (b), describe as best you can the subspace \(\Span \ S\) of \(\pol_2\).%
\end{enumerate}
\end{activity}%
\index{spanning set} Given a subspace \(H\), the set \(S\) such that \(H=\Span \ S\) is called a \terminology{spanning set} of \(H\). In order to determine if a set \(S=\{\vv_1, \vv_2, \ldots, \vv_k\}\) is a spanning set for \(H\), all we need to do is to show that for every \(\vb\) in \(H\), the equation%
\begin{equation*}
x_1 \vv_1 + x_2\vv_2 + \cdots + x_k \vv_k = \vb
\end{equation*}
has a solution. We will see important uses of special spanning sets called bases in the rest of this chapter.%
\end{subsectionptx}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_vec_space_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp23779336}%
Determine if each of the following sets is a vector space.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(V = \{(x,y,z) : x,y,z \in \R\}\) with addition and multiplication by scalars defined by%
\begin{equation*}
(a,b,c) \oplus (x,y,z) = (a+x, c+z, b+y) \ \text{ and }  \ k(x,y,z) = (kx,kz,ky)\text{,}
\end{equation*}
where \((a,b,c)\) and \((x,y,z)\) are in \(V\) and \(k \in \R\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp23779592}{}\quad{}We consider the vector space properties in \hyperref[x:definition:def_vector_space]{Definition~{\xreffont\ref{x:definition:def_vector_space}}}. Let \((a,b,c)\), \((u,v,w)\), and \((x,y,z)\) be in \(V\) and let \(k,m  \in \R\). By the definition of addition and multiplication by scalars, both \((a,b,c) + (x,y,z)\) and \(k (x,y,z)\) are in \(V\). Note also that%
\begin{align*}
(a,b,c) \oplus (x,y,z) \amp = (a+x, c+z, b+y)\\
\amp = (x+a, z+c, y+b)\\
\amp = (x,y,z) \oplus (a,b,c)\text{,}
\end{align*}
and so addition is commutative in \(V\). Since%
\begin{equation*}
((1,1,0)\oplus(0,1,1)) \oplus (0,0,1) = (1,1,2) \oplus (0,0,1) = (1,3,1)
\end{equation*}
and%
\begin{equation*}
(1,1,0)\oplus ((0,1,1) \oplus (0,0,1)) = (1,1,0) \oplus (0,2,1) = (1,1,3)\text{,}
\end{equation*}
we see that addition is not associative and conclude that \(V\) is not a vector space. At this point we can stop since we have shown that \(V\) is not a vector space.%
\item{}\(V = \{x \in \R : x > 0\}\) with addition \(\oplus\) and multiplication by scalars defined by%
\begin{equation*}
x \oplus y = xy  \ \text{ and }  \ kx = x^k\text{,}
\end{equation*}
where \(x\) and \(y\) are in \(V\), \(k \in \R\), and \(xy\) is the standard product of \(x\) and \(y\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp23782920}{}\quad{}We consider the vector space properties in \hyperref[x:definition:def_vector_space]{Definition~{\xreffont\ref{x:definition:def_vector_space}}}. Let \(x\), \(y\), and \(z\) be in \(V\) and let \(k,m  \in \R\). Since \(x\) and \(y\) are both positive real numbers, we know that \(xy\) is a positive real number. Thus, \(x \oplus y \in V\) and \(V\) is closed under its addition. Also, \(x^k\) is a positive real number, so \(x^k \in V\) as well. Now%
\begin{equation*}
x \oplus y = xy = yx = y \oplus x
\end{equation*}
and addition is commutative in \(V\). Also,%
\begin{equation*}
(x \oplus y) \oplus z = (xy) \oplus z = (xy)z = x(yz) = x \oplus (yz) = x \oplus (y \oplus z)
\end{equation*}
and addition is associative in \(V\). Since%
\begin{equation*}
1 \oplus x = 1x = x\text{,}
\end{equation*}
\(V\) contains an additive identity, which is \(1\). The fact that \(x\) is a positive real number implies that \(\frac{1}{x}\) is a positive real number. Thus, \(\frac{1}{x} \in V\) and%
\begin{equation*}
x \oplus \frac{1}{x} = x\left(\frac{1}{x}\right) = 1
\end{equation*}
and \(V\) contains an additive inverse for each of its elements. We have that%
\begin{align*}
(k+m)x \amp = x^{k+m} = x^kx^m = x^k \oplus x^m = k(x) \oplus m(x),\\
k(x \oplus +y) \amp = k(xy) = x^ky^k = x^k \oplus y^k = k(x) \oplus k(y),\\
(km)x \amp = x^{km} = \left(x^m\right)^k = (m(x))^k = k(m(x))\\
1x \amp = x^1 = x\text{.}
\end{align*}
So \(V\) satisfies all of the properties of a vector space.%
\item{}The set \(W\) of all \(2 \times 2\) matrices of the form \(\left[ \begin{array}{cc} a\amp 0\\b\amp 0 \end{array} \right]\) where \(a\) and \(b\) are real numbers using the standard addition and multiplication by scalars on matrices.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp23804808}{}\quad{}Recall that \(\M_{2 \times 2}\) is a vector space using the standard addition and multiplication by scalars on matrices. Any matrix of the form \(\left[ \begin{array}{cc} a\amp 0\\b\amp 0 \end{array}  \right]\)  can be written as%
\begin{equation*}
\left[ \begin{array}{cc} a\amp 0\\b\amp 0 \end{array}  \right] = a\left[ \begin{array}{cc} 1\amp 0\\0\amp 0 \end{array}  \right]  + b \left[ \begin{array}{cc} 0\amp 0\\1\amp 0 \end{array}  \right]\text{.}
\end{equation*}
So \(W = \Span\left\{\left[ \begin{array}{cc} 1\amp 0\\0\amp 0 \end{array}  \right],  \left[ \begin{array}{cc} 0\amp 0\\1\amp 0 \end{array}  \right]\right\}\) and \(W\) is a subspace of \(\M_{2 \times 2}\). Thus, \(W\) is a vector space.%
\item{}The set \(W\) of all functions \(f\) from \(\R\) to \(\R\) such that \(f(0) \geq 0\) using the standard addition and multiplication by scalars on functions.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp23810824}{}\quad{}We will show that \(W\) is not a vector space. Let \(f: \R \to \R\) be defined by \(f(x) = 1\). Then \(f(0) \geq 0\) and \(f \in W\). However, if \(h = (-1)f\), then \(h(0) = (-1)f(0) = -1\) and \(h \notin W\). It follows that \(W\) is not closed under multiplication by scalars and \(W\) is not a vector space.%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp23812488}%
Let \(V\) be a vector space and \(\vu\) and \(\vv\) vectors in \(V\). Also, let \(a\) and \(b\) be scalars. You may use the result of \hyperlink{x:exercise:ex_5_a_scalar_times_0}{Exercise~{\xreffont 5}} that \(c \vzero = \vzero\) for any scalar \(c\) in any vector space.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}If \(a \vv = b \vv\) and \(\vv \neq \vzero\), must \(a = b\)? Use the properties of a vector space or provide a counterexample to justify your answer.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp23821576}{}\quad{}We will show that this statement is true. Suppose \(a \vv = b \vv\) and \(\vv \neq \vzero\). Then \(\vzero = a \vv - b \vv = (a-b)\vv\). If \(a = b\), then we are done. So suppose \(a \neq b\). Then \(a-b \neq 0\) and \(\frac{1}{a-b}\) is a real number. Then%
\begin{align*}
\frac{1}{a-b} \vzero \amp = \frac{1}{a-b}((a-b)\vv)\\
\vzero \amp = \left(\frac{1}{a-b}(a-b)\right) \vv\\
\vzero \amp = \vv\text{.}
\end{align*}
But we assumed that \(\vv \neq \vzero\), so we can conclude that \(a = b\) as desired.%
\item{}If \(a \vu = a \vv\) and \(a \neq 0\), must \(\vu = \vv\)? Use the properties of a vector space or provide a counterexample to justify your answer.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp23826696}{}\quad{}We will show that this statement is true. Suppose \(a \vu = a \vv\) and \(a \neq 0\). Then \(\vzero = a \vu - a \vv = a(\vu - \vv)\). Since \(a \neq 0\), we know that \(\frac{1}{a}\) is a real number. Thus,%
\begin{align*}
\frac{1}{a} \vzero \amp = \frac{1}{a} \left(a (\vu - \vv)\right)\\
\vzero \amp = \left( \frac{1}{a}a\right) (\vu - \vv)\\
\vzero \amp = \vu - \vv\\
\vu \amp = \vv\text{.}
\end{align*}
%
\item{}If \(a\vu = b\vv\), must \(a = b\) and \(\vu = \vv\)? Use the properties of a vector space or provide a counterexample to justify your answer.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp23837064}{}\quad{}We will demonstrate that this statement is false with a counterexample. Let \(a = 1\), \(b = 2\), \(\vu = [2 \ 0]^{\tr}\) and \(\vv = [1 \ 0]^{\tr}\) in \(\R^2\). Then%
\begin{equation*}
a \vu = 1[2 \ 0]^{\tr} = [2 \ 0]^{\tr} = 2[1 \ 0]^{\tr} = b \vv\text{,}
\end{equation*}
but \(a \neq b\) and \(\vu \neq \vv\).%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_vec_space_summ}
%
\begin{itemize}[label=\textbullet]
\item{}A set \(V\) on which an operation of addition and a multiplication by scalars is defined is a vector space if for all \(\vu\), \(\vv\), and \(\vw\) in \(V\) and all scalars \(a\) and \(b\):%
\begin{enumerate}
\item{}\(\vu + \vv\) is an element of \(V\),%
\item{}\(\vu + \vv = \vv + \vu\),%
\item{}\((\vu + \vv) + \vw = \vu + (\vv + \vw)\),%
\item{}there is a zero vector \(\vzero\) in \(V\) so that \(\vu + \vzero = \vu\),%
\item{}for each \(\vx\) in \(V\) there is an element \(\vy\) in \(V\) so that \(\vx + \vy = \vzero\),%
\item{}\(a \vu\) is an element of \(V\),%
\item{}\((a+b) \vu = a\vu + b\vu\),%
\item{}\(a(\vu + \vv) = a\vu + a\vv\),%
\item{}\((ab) \vu = a(b\vu)\),%
\item{}\(1 \vu = \vu\).%
\end{enumerate}
%
\item{}A subset \(H\) of a vector space \(V\) is a subspace of \(V\) if%
\begin{enumerate}
\item{}whenever \(\vu\) and \(\vv\) are in \(H\) it is also true that \(\vu + \vv\) is in \(H\),%
\item{}whenever \(\vu\) is in \(H\) and \(a\) is a scalar it is also true that \(a\vu\) is in \(H\),%
\item{}\(\vzero\) is in \(H\).%
\end{enumerate}
%
\item{}A linear combination of vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) in a vector space \(V\) is a vector of the form%
\begin{equation*}
x_1\vv_1+x_2\vv_2 + \cdots + x_k \vv_k\text{,}
\end{equation*}
where \(x_1\), \(x_2\), \(\ldots\), \(x_k\) are scalars.%
\item{}The span of the vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) in a vector space \(V\) is the collection of all linear combinations of \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\). That is,%
\begin{equation*}
\Span\{\vv_1, \vv_2, \ldots, \vv_k\} = \{x_1\vv_1+x_2\vv_2 + \cdots + x_k \vv_k : x_1, x_2, \ldots, x_k \text{ are scalars } \}\text{.}
\end{equation*}
%
\item{}The span of any finite set of vectors in a vector space \(V\) is always a subspace of \(V\).%
\item{}This concept of vector space is important because there are many different types of sets (e.g., \(\R^n\), \(\M_{m \times n}\), \(\pol_n\), \(\F\)) that have similar structure, and we can relate them all as members of this larger collection of vector spaces.%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_vec_space_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp23874312}%
The definition of a vector space only states the existence of a zero vector and does not say how many zero vectors the space might have. In this exercise we show that the zero vector in a vector space is unique. To show that the zero vector is unique, we assume that two vectors \(\vzero_1\) and \(\vzero_2\) have the zero vector property.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Using the fact that \(\vzero_1\) is a zero vector, what vector is \(\vzero_1 + \vzero_2\)?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp23876360}{}\quad{}Use the fact that \(\vzero_1+\vv = \vv\) for any vector \(\vv\) in our vector space.%
\item{}Using the fact that \(\vzero_2\) is a zero vector, what vector is \(\vzero_1 + \vzero_2\)?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp23883400}{}\quad{}Same reasoning as in part (a).%
\item{}How do we conclude that the zero vector is unique?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp23887496}{}\quad{}Use the transitive property of equality.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp23886600}%
The definition of a vector space only states the existence of an additive inverse for each vector in the space, but does not say how many additive inverses a vector can have. In this exercise we show that the additive inverse of a vector in a vector space is unique. To show that a vector \(\vv\) has only one additive inverse, we suppose that \(\vv\) has two additive inverses, \(\vu\) and \(\vw\), and demonstrate that \(\vu = \vw\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}What equations must \(\vu\) and \(\vw\) satisfy if \(\vu\) and \(\vw\) are additive inverses of \(\vv\)?%
\item{}Use the equations from part (a) to show that \(\vu = \vw\). Clearly identify all vector space properties you use in your argument.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp23884680}%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp23885704}%
Let \(V\) be a vector space and \(\vv\) a vector in \(V\). In all of the vector spaces we have seen to date, the additive inverse of the vector \(\vv\) is equal to the scalar multiple \((-1)\vv\). This seems reasonable, but it is important to note that this result is not stated in the definition of a vector space, so this it is something that we need to verify. To show that \((-1)\vv\) is an additive inverse of the vector \(\vv\), we need to demonstrate that%
\begin{equation*}
\vv + (-1)\vv = \vzero\text{.}
\end{equation*}
Verify this equation, explicitly stating which properties you use at each step.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp23896712}{}\quad{}Use the fact that \(-1 + 1 = 0\).%
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{x:exercise:ex_5_a_scalar_times_0}%
It is reasonable to expect that if \(c\) is any scalar and \(\vzero\) is the zero vector in a vector space \(V\), then \(c \vzero = \vzero\). Use the fact that \(\vzero + \vzero = \vzero\) to prove this statement.%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp23892488}%
Let \(W_1, W_2\) be two subspaces of a vector space \(V\). Determine whether \(W_1 \cap W_2\) and \(W_1 \cup W_2\) are subspaces of \(V\). Justify each answer clearly.%
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp23900680}%
Find three vectors \(\vv_1, \vv_2, \vv_3\) to express%
\begin{equation*}
W=\left\{ \left[ \begin{array}{c} a+2b+c \\ b-3c \\ a-c \end{array}  \right] : a, b, c \text{ in }  \R \right\}
\end{equation*}
as \(\Span \{\vv_1, \vv_2, \vv_3\}\). How does this justify why \(W\) is a subspace of \(\R^3\)?%
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp23899400}%
Find three vectors \(\vv_1, \vv_2, \vv_3\) to express%
\begin{equation*}
W=\left\{ \left[ \begin{array}{cc} a+b \amp  a-2c \\ 3b+c \amp  a+b-c \end{array}  \right] : a, b, c \text{ in }  \R \right\}
\end{equation*}
as \(\Span \{\vv_1, \vv_2, \vv_3\}\). How does this justify why \(W\) is a subspace of \(\M_{2 \times 2}\)?%
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp23909384}%
Let \(\F\) be the set of all functions from \(\R\) to \(\R\), where we define the sum of two functions \(f\) and \(g\) in \(\F\) as the function \(f+g\) satisfying%
\begin{equation*}
(f+g)(x) = f(x) + g(x)
\end{equation*}
for all real numbers \(x\), and the scalar multiple \(cf\) of the function \(f\) by the scalar \(c\) to be the function satisfying%
\begin{equation*}
(cf)(x) = cf(x)
\end{equation*}
for all real numbers \(x\). Show that \(\F\) is a vector space using these operations.%
\end{divisionexercise}%
\begin{divisionexercise}{10}{}{}{g:exercise:idp23915400}%
Prove \hyperref[x:theorem:thm_VS_span]{Theorem~{\xreffont\ref{x:theorem:thm_VS_span}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp23914120}{}\quad{}Mimic the proof of \hyperref[x:theorem:thm_3_a_span_subspace]{Theorem~{\xreffont\ref{x:theorem:thm_3_a_span_subspace}}}.%
\end{divisionexercise}%
\begin{divisionexercise}{11}{}{}{g:exercise:idp23919624}%
Determine if each of the following sets of elements is a vector space or not. If appropriate, you can identify a set as a subspace of another vector space, or as a span of a collection of vectors to shorten your solution.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}A line through the origin in \(\R^n\).%
\item{}The first quadrant in \(\R^2\).%
\item{}The set of vectors \(\left\{ \left[ \begin{array}{c} a \\ 0 \end{array} \right] : a \text{ in } \Z \right\}\).%
\item{}The set of all differentiable functions from \(\R\) to \(\R\).%
\item{}The set of all functions from \(\R\) to \(\R\) which are increasing for every \(x\). (Assume that a function \(f\) is increasing if \(f(a) > f(b)\) whenever \(a > b\).)%
\item{}The set of all functions \(f\) from \(\R\) to \(\R\) for which \(f(c)=0\) for some fixed \(c\) in \(\R\).%
\item{}The set of polynomials of the form \(a+bt\), where \(a+b=0\).%
\item{}The set of all upper triangular \(4\times 4\) real matrices.%
\item{}The set of complex numbers \(\C\) where scalar multiplication is defined as multiplication by real numbers.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{12}{}{}{g:exercise:idp23933832}%
A reasonable way to extend the idea of the vector space \(\R^n\) to infinity is to let \(\R^\infty\) be the set of all sequences of real numbers. Define addition and multiplication by scalars on \(\R^{\infty}\) by%
\begin{equation*}
\{x_n\} + \{y_n\} = \{x_n + y_n\} \ \text{ and }  \ c\{x_n\} = \{cx_n\}
\end{equation*}
where \(\{x_n\}\) denotes the sequence \(x_1, x_2, x_3, \ldots\), \(\{y_n\}\) denotes the sequence \(y_1, y_2, y_3, \ldots\) and \(c\) is a scalar.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(\R^{\infty}\) is a vector space using these operations.%
\item{}Is the set of sequences that have infinitely many zeros a subspace of \(\R^{\infty}\)? Verify your answer.%
\item{}Is the set of sequences which are eventually zero a subspace of \(\R^{\infty}\)? Verify your answer. (A sequence \(\{x_n\}\) is \terminology{eventually zero} if there is an index \(k_0\) such that \(x_n = 0\) whenever \(n \geq k_0\).)%
\item{}Is the set of decreasing sequences a subspace of \(\R^{\infty}\)? Verify your answer. (A sequence \(\{x_n\}\) is \terminology{decreasing} if \(x_{n+1} \leq x_n\) for each \(n\).)%
\item{}Is the set of sequences in \(\R^{\infty}\) that have limits at infinity a subspace of \(\R^{\infty}\)?%
\item{}Let \(\ell^2\) be the set of all square summable sequences in \(\R^{\infty}\), that is sequences \(\{x_n\}\) so that \(\sum_{k = 1}^{\infty} x_k^2\) is finite. So, for example, the sequence \(\left\{\frac{1}{n}\right\}\) is in \(\ell^2\). Show that \(\ell^2\) is a subspace of \(\R^{\infty}\) (the set \(\ell^2\) is an example of what is called a \terminology{Hilbert space} by defining the inner product \(\langle \{x_n\}, \{y_n\} \rangle = \sum_{n = 1}^{\infty} x_ny_n\)). (Hint: show that \(2u^2 + 2v^2 - (u+v)^2 \geq 0\) for any real numbers \(u\) and \(v\).)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{13}{}{}{x:exercise:ex_5_a_sum}%
\index{subspace!sum}%
Given two subspaces \(H_1, H_2\) of a vector space \(V\), define%
\begin{equation*}
H_1+H_2 = \{ \vw \mid \vw=\vu+\vv \text{ where }  \vu \text{ in } H_1, \vv \text{ in } H_2\} \,\text{.}
\end{equation*}
Show that \(H_1+H_2\) is a subspace of \(V\) containing both \(H_1, H_2\) as subspaces. The space \(H_1+H_2\) is the sum of the subspaces \(H_1\) and \(H_2\).%
\end{divisionexercise}%
\begin{divisionexercise}{14}{}{}{g:exercise:idp23954952}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
The intersection of any two subspaces of \(V\) is also a subspace.%
\item{}\lititle{True\slash{}False.}\par%
The union of any two subspaces of \(V\) is also a subspace.%
\item{}\lititle{True\slash{}False.}\par%
If \(H\) is a subspace of a vector space \(V\), then \(-H=\{ (-1)\vv: \vv \text{ in } H\}\) is equal to \(H\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\vv\) is a nonzero vector in \(H\), a subspace of \(\R^n\), then \(H\) contains the line through the origin and \(\vv\) in \(\R^n\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\vv_1, \vv_2\) are nonzero, non-parallel vectors in \(H\), a subspace of \(\R^n\), then \(H\) contains the plane through the origin, \(\vv_1\) and \(\vv_2\) in \(\R^n\).%
\item{}\lititle{True\slash{}False.}\par%
The smallest subspace in \(\R^n\) containing a vector \(\vv\) is a line through the origin.%
\item{}\lititle{True\slash{}False.}\par%
The largest subspace of \(V\) is \(V\).%
\item{}\lititle{True\slash{}False.}\par%
The space \(\pol_1\) is a subspace of \(\pol_n\) for \(n \geq 1\).%
\item{}\lititle{True\slash{}False.}\par%
The set of constant functions from \(\R\) to \(\R\) is a subspace of \(\F\).%
\item{}\lititle{True\slash{}False.}\par%
The set of all polynomial functions with rational coefficients is a subspace of \(\F\).%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Hamming Codes and the Hat Puzzle}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Hamming Codes and the Hat Puzzle}{}{Project: Hamming Codes and the Hat Puzzle}{}{}{x:section:sec_proj_hamming_hat_puzzle}
Recall the hat problem from the beginning of this section. Three players are assigned either a red or blue hat and can only see the colors of the hats of the other players. The goal is to devise a high probability strategy for one player to correctly guess the color of their hat. The players have a 50\% chance of winning if one player guesses randomly and all of the others pass. However, the group can do better than 50\% with a reasonably simple strategy. There are 2 possibilities for each hat color for a total of \(2^3 = 8\) possible distributions of hat colors. Of these, only red-red-red and blue-blue-blue contain only one hat color, so \(6/8\) of \(3/4\) of the possible hat distributions have two hats of one color and one of the other color. So if a player sees two hats of the same color, that player guesses the other color and passes otherwise. This gives a 75\% chance of winning. This strategy will only work for three players, though. We want to develop an effective strategy that works for larger groups of players.%
\par
There is a strategy, based on \terminology{Hamming codes} that can be utilized when the number of players is of the form \(2^k-1\) with \(k \geq 2\). This strategy will provide a winning probability of%
\begin{equation*}
1 - 2^{-k}\text{.}
\end{equation*}
%
\par
Note that as \(k \to \infty\), this probability has a limit of 1. Note also that if \(k=2\) (so that there are 3 players), then the probability is \(\frac{3}{4}\) or \(75\%\) \textemdash{} the same strategy we came up with earlier.%
\par
To understand this strategy, we need to build a slightly different kind of vector space than we have seen until now, one that is based on a binary choice of red or blue. To do so, we identify the hat colors with numbers \textemdash{} 0 for red and 1 for blue. So let \(\mathbb{F} = \{0,1\}\). Assume there are \(n = 2^k-1\) players for some integer \(k \geq 2\). We can then view a distribution of hats among the \(n = 2^k-1\) players as a vector with \(n\) components from \(\mathbb{F}\). That is,%
\begin{equation*}
\mathbb{F}^n = \{ [\alpha_1 \ \alpha_2 \ \cdots \ \alpha_n]^{\tr} : \alpha_i \in \mathbb{F}\}\text{.}
\end{equation*}
%
\par
We can give some structure to both \(\mathbb{F}\) and \(\mathbb{F}^n\) by noting that we can define addition and multiplication in \(\mathbb{F}\) by%
\begin{align*}
{3} 0 + 0 \amp = 0,    \amp     0 + 1 \amp = 1 + 0 = 1, \amp   1+1 \amp = 0\\
0 \cdot 0 \amp = 0,  \amp         0 \cdot 1 \amp = 1 \cdot 0 = 0, \amp    1 \cdot 1 \amp = 1\text{.}
\end{align*}
%
\begin{project}{}{x:project:act_hat_1}%
Show that \(\mathbb{F}\) has the same structure as \(\R\). That is, show that for all \(x\), \(y\), and \(z\) in \(\mathbb{F}\), the following properties are satisfied.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(x + y \in \mathbb{F}\) and \(xy \in \mathbb{F}\)%
\item{}\(x + y = y + x\) and \(xy=yx\)%
\item{}\((x + y) + z = x + (y + z)\) and \((xy)z = x(yz)\)%
\item{}There is an element \(0\) in \(\mathbb{F}\) such that \(x+ 0 = x\)%
\item{}There is an element \(1\) in \(\mathbb{F}\) such that \((1)x = x\)%
\item{}There is an element \(-x\) in \(\mathbb{F}\) such that \(x+(-x) = 0\)%
\item{}If \(x \neq 0\), there is an element \(\frac{1}{x}\) in \(\mathbb{F}\) such that \(x\left(\frac{1}{x}\right) = 1\)%
\item{}\(x (y + z) = (x y) + (x z)\)%
\end{enumerate}
\end{project}%
\hyperref[x:project:act_hat_1]{Project Activity~{\xreffont\ref{x:project:act_hat_1}}} shows that \(\mathbb{F}\) has the same properties as \(\R\) \textemdash{} that is that \(\mathbb{F}\) is a field. Until now, we have worked with vector spaces whose scalars come from the set of real numbers, but that is not necessary. None of the results we have discovered so far about vector spaces require our scalars to come from \(\R\). In fact, we can replace \(\R\) with any field and all of the same vector space properties hold. It follows that \(V = \mathbb{F}^n\) is a vector space over \(\mathbb{F}\). As we did in \(\R^n\), we define the standard unit vectors \(\ve_1 = [1 \ 0 \ 0 \ \cdots \ 0]^{\tr}\), \(\ve_2 = [0 \ 1 \ 0 \ 0 \ \ldots \ 0]^{\tr}\), \(\ldots\), \(\ve_n = [0 \ 0 \ 0 \ \ldots \ 0 \ 1]^{\tr}\) in \(V = \mathbb{F}^n\).%
\par
Now we return to the hat puzzle. We have \(n = 2^k-1\) players. Label the players \(1\), \(2\), \(\ldots\), \(n\). We can now represent a random placements of hats on heads as a vector \(\vv = [\alpha_1 \ \alpha_2 \ \cdots \ \alpha_n]^{\tr}\) in \(V = \mathbb{F}^n\), where \(\alpha_i = 0\) in the \(i\)th entry represents a red hat and \(\alpha_i =1\) a blue hat on player \(i\). Since player \(i\) can see all of the other hats, from player \(i\)'s perspective the distribution of hats has the form%
\begin{equation*}
\vv = \vv_i+ \beta_i \ve_i\text{,}
\end{equation*}
where \(\beta_i\) is the unknown color of hat on player \(i\)'s head and%
\begin{equation*}
\vv_i = [\alpha_1 \ \alpha_2 \ \cdots \ \alpha_{i-1} \ 0 \ \alpha_{i+1} \ \cdots \alpha_n]^{\tr}\text{.}
\end{equation*}
%
\par
In order to analyze the vectors \(\vv\) from player \(i\)'s perspective and to devise an effective strategy, we will partition the set \(V\) into an appropriate disjoint union of subsets.%
\par
To provide a different way to look at players, we will use a subspace of \(V\). Let \(W\) be a subspace of \(V\) that has a basis of \(k\) vectors. The elements of \(W\) are the linear combinations of \(k\) basis vectors, and each basis vector in a linear combination has 2 possibilities for its weight (from \(\mathbb{F}\)). Thus, \(W\) contains exactly \(2^k = n+1\) vectors. We can then use the \(n=2^k-1\) nonzero vectors in \(W\) to represent our players. Each distribution of hats can be seen as a linear combination of the vectors in \(W\). Let \(\vw_1\), \(\vw_2\), \(\ldots\), \(\vw_{2^k-1}\) be the nonzero vectors in \(W\). We then define a function \(\varphi : V \to W\) as%
\begin{equation*}
\varphi([\alpha_1 \ \alpha_2 \ \cdots \ \alpha_n]^{\tr}) = \sum_{i=1}^{n} \alpha_i \vw_i
\end{equation*}
that identifies a distribution of hats with a vector in \(W\). The subspace that we need to devise our strategy is what is called a Hamming code.%
\begin{project}{}{x:project:act_hat_2}%
\index{Hamming code}%
Let%
\begin{equation*}
H = \left\{[\alpha_1 \ \alpha_2 \ \cdots \ \alpha_n]^{\tr} \in V : \sum_{i=1}^{n} \alpha_i \vw_i = \vzero\right\}\text{.}
\end{equation*}
%
\par
Show that \(H\) is a subspace of \(V\). (The subspace \(H\) is called the \(\left(2^k-1, 2^k-k-1\right)\) \terminology{Hamming code} (where the first component is the number of elements in a basis for \(V\) and the second the number of elements in a basis for \(H\)). Hamming codes are examples of linear codes \textemdash{} those codes that are subspaces of the larger vector space.)%
\end{project}%
Now for each \(i\) between 0 and \(n\) we define \(H_i = \ve_i + H\) as%
\begin{equation*}
H_i = \ve_i + H = \{\ve_i + \vh : \vh \in H\}\text{,}
\end{equation*}
where we let \(\ve_0 = \vzero\). The sets \(H_i\) are called \emph{cosets} of \(H\).%
\begin{project}{}{x:project:act_hat_3}%
To complete our strategy for the hat puzzle, we need to know some additional information about the sets \(H_i\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that the sets \(H_i\) are disjoint. That is, show that \(H_i \cap H_j = \emptyset\) if \(i \neq j\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp24053544}{}\quad{}If \(\vv \in H_i\) and \(\vv \in H_j\), what can we say about \(\ve_i - \ve_j\)?%
\item{}Since \(H_i \subseteq V\) for each \(i\), it follows that \(\bigcup_{i=0}^n H_i \subseteq V\). Now we show that \(V = \bigcup_{i=0}^n H_i\) by demonstrating that \(\bigcup_{i=0}^n H_i\) has exactly the same number of elements as \(V\). We will need one fact for our argument. We will see in a later section that \(H\) has a basis of \(n-k\) elements, so the number of elements in \(H\) is \(2^{n-k}\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Since the sets \(H_i\) are disjoint, the number of elements in \(\bigcup_{i=1}^n H_i\) is equal to the sum of the number of elements in each \(H_i\). Show that each \(H_i\) has the same number of elements as \(H\).%
\item{}Now use the fact that the number of elements in \(\bigcup_{i=0}^n H_i\) is equal to the sum of the number of elements in each \(H_i\) to argue that \(V = \bigcup_{i=0}^n H_i\).%
\end{enumerate}
\end{enumerate}
\end{project}%
The useful idea from \hyperref[x:project:act_hat_3]{Project Activity~{\xreffont\ref{x:project:act_hat_3}}} is that any hat distribution in \(V\) is in exactly one of the sets \(H_i\). Recall that a hat distribution \(\vv = [\alpha_1 \ \alpha_2 \ \cdots \ \alpha_n]^{\tr}\) in \(V\) can be written from player \(i\)'s perspective as%
\begin{equation*}
\vv = \vv_i + \beta_i \ve_i\text{,}
\end{equation*}
where \(\vv_i =  [\alpha_1 \ \alpha_2 \ \cdots \ \alpha_{i-1} \ 0 \ \alpha_{i+1} \ \cdots \ \cdots \ \alpha_n]^{\tr}\). Our strategy for the hat game can now be revealed.%
\begin{itemize}[label=\textbullet]
\item{}If \(\vv_i + \beta_i \ve_i\) is not in \(H\) for either choice of \(\beta_i\), then player \(i\) should pass.%
\item{}If \(\vv_i + \beta_i \ve_i\) is in \(H\), then player \(i\) guesses \(1 + \beta_i\).%
\end{itemize}
%
\begin{project}{}{x:project:act_hat_4}%
Let us analyze this strategy.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why every player guesses wrong if \(\vv\) is in \(H\).%
\item{}Now we see determine that our strategy is a winning strategy for all hat distributions \(\vv\) that are not in \(H\). First we need to know that these two options are the only ones. That is, show that it is not possible for \(\vv_i + \beta_i \ve_i\) to be in \(H\) for both choices of \(\beta_i\).%
\item{}Now we want to demonstrate that this is a winning strategy if \(\vv \notin H\). That is, at least one player guesses a correct hat color and no one else guesses incorrectly. So assume \(\vv \notin H\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}We know that \(\vv \in H_i\) for some unique choice of \(i\), so let \(\vv = \ve_i + h\) for some \(h \in H\). Explain why player \(i\) can correctly choose color \(1+ \alpha_i\).%
\item{}Finally, we need to argue that every player except player \(i\) must pass. So consider player \(j\), with \(j \neq i\). Recall that%
\begin{equation*}
\vv = \vv_j + \alpha_j \ve_j\text{.}
\end{equation*}
Analyze our strategy and the conditions under which player \(j\) does not pass. Show that this leads to a contradiction.%
\end{enumerate}
\end{enumerate}
\end{project}%
\hyperref[x:project:act_hat_4]{Project Activity~{\xreffont\ref{x:project:act_hat_4}}} completes our analysis of this strategy and shows that our strategy results in a win with probability%
\begin{equation*}
1 - \frac{|H|}{|V|} = 1 - \frac{2^{2^k-k-1}}{2^{2^k-1}} = 1 - 2^{-k}\text{.}
\end{equation*}
%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 32 Bases for Vector Spaces}
\typeout{************************************************}
%
\begin{chapterptx}{Bases for Vector Spaces}{}{Bases for Vector Spaces}{}{}{x:chapter:chap_bases}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp24080808}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What does it mean for a set \(\{\vv_1, \vv_2, \ldots, \vv_k\}\) of vectors in a vector space \(V\) to be linearly independent?%
\item{}What is another equivalent characterization of a linearly independent set?%
\item{}What does is mean for a set \(\{\vv_1, \vv_2, \ldots, \vv_k\}\) of vectors in a vector space \(V\) to be linearly dependent?%
\item{}Describe another characterization of a linearly dependent set.%
\item{}What is a basis for a vector space \(V\)?%
\item{}What makes a basis for a vector space useful?%
\item{}How can we find a basis for a vector space \(V\)?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Image Compression}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Image Compression}{}{Application: Image Compression}{}{}{x:section:sec_img_compress}
\begin{quote}%
If you painted a picture with a sky, clouds, trees, and flowers, you would use a different size brush depending on the size of the features. Wavelets are like those brushes.%
\nopagebreak\par%
\hfill\textemdash{}{\setlength{\tabcolsep}{0pt}\begin{tabular}[t]{l@{}}
Ingrid Daubechies
\end{tabular}}\\\par
\end{quote}
The advent of the digital age has presented many new opportunities for the collection, analysis, and dissemination of information. Along with these opportunities come new difficulties as well. All of this digital information must be stored in some way and be retrievable in an efficient manner. One collection of tools that is used to deal with these problems is wavelets. For example, the FBI fingerprint files contain millions of cards, each of which contains 10 rolled fingerprint impressions. Each card produces about 10 megabytes of data. To store all of these cards would require an enormous amount of space, and transmitting one full card over existing data lines is slow and inefficient. Without some sort of image compression, a sortable and searchable electronic fingerprint database would be next to impossible. To deal with this problem, the FBI adopted standards for fingerprint digitization using a wavelet compression standard.%
\par
Another problem with electronics is noise. Noise can be a big problem when collecting and transmitting data. Wavelet decomposition filters data by averaging and detailing. The detailing coefficients indicate where the details are in the original data set. If some details are very small in relation to others, eliminating them may not substantially alter the original data set. Similar ideas may be used to restore damaged audio,\footnote{see \href{https://ccrma.stanford.edu/groups/edison/brahms/brahms.html}{\nolinkurl{ccrma.stanford.edu/groups/edison/brahms/brahms.html}} for a discussion of the denoising of a Brahms recording\label{g:fn:idp24094888}} video, photographs, and medical information.\footnote{A review of wavelets in biomedical applications. M. Unser, A. Aldroubi. \pubtitle{Proceedings of the IEEE}, Volume: 84, Issue: 4 , Apr 1996\label{g:fn:idp24100136}}%
\par
We will consider wavelets as a tool for image compression. The basic idea behind using wavelets to compress images is that we start with a digital image, made up of pixels. Each pixel can be assigned a number or a vector (depending on the makeup of the image). The image can then be represented as a matrix (or a set of matrices) \(M\), where each entry in \(M\) represents a pixel in the image. As a simple example, consider the \(16 \times 16\) image of a flower as shown at left in \hyperref[x:figure:F_Flower_1]{Figure~{\xreffont\ref{x:figure:F_Flower_1}}}. (We will work with small images like this to make the calculations more manageable, but the ideas work for any size image. We could also extend our methods to consider color images, but for the sake of simplicity we focus on grayscale.)%
\begin{figureptx}{Left: A 16 by 16 pixel image. Right: The image compressed.}{x:figure:F_Flower_1}{}%
\begin{sidebyside}{2}{0}{0}{0}%
\begin{sbspanel}{0.5}%
\includegraphics[width=\linewidth]{external/flower.pdf}
\end{sbspanel}%
\begin{sbspanel}{0.5}%
\includegraphics[width=\linewidth]{external/flower_2.pdf}
\end{sbspanel}%
\end{sidebyside}%
\tcblower
\end{figureptx}%
This flower image is a gray-scale image, so each pixel has a numeric representation between 0 and 255, where 0 is black, 255 is white, and numbers between 0 and 255 represent shades of gray. The matrix for this flower image is%
\begin{equation}
\left[ {\scriptsize \begin{array}{cccccccccccccccc} 240\amp 240\amp 240\amp 240\amp 130\amp 130\amp 240\amp 130\amp 130\amp 240\amp 240\amp 240\amp 240\amp 240\amp 240\amp 240\\240\amp 240\amp 240\amp 130 \amp 175\amp 175\amp 130\amp 175\amp 175\amp 130\amp 240\amp 240\amp 240\amp 240\amp 240\amp 240\\ 240\amp 240\amp 130\amp 130\amp 175\amp 175\amp 130\amp 175\amp 175\amp 130\amp 130\amp 240\amp 240\amp 240\amp 240\amp 240 \\240\amp 130\amp 175\amp 175\amp 130\amp 175\amp 175\amp 175\amp 130\amp 175\amp 175\amp 130\amp 240\amp 240\amp 240\amp 240\\240\amp 240\amp 130\amp 175\amp 175\amp 130\amp 175\amp 130\amp 175 \amp 175\amp 130\amp 240\amp 240\amp 240\amp 240\amp 240\\255\amp 240\amp 240\amp 130\amp 130\amp 175\amp 175\amp 175\amp 130\amp 130\amp 240\amp 240\amp 225\amp 240\amp 240\amp 240\\240\amp 240 \amp 130\amp 175\amp 175\amp 130\amp 130\amp 130\amp 175\amp 175\amp 130\amp 240\amp 225\amp 255\amp 240\amp 240 \\240\amp 240\amp 130\amp 175\amp 130\amp 240\amp 130\amp 240\amp 130\amp 175\amp 130\amp 240\amp 255\amp 255\amp 255\amp 240\\240\amp 240\amp 240\amp 130\amp 240\amp 240\amp 75\amp 240\amp 240\amp 130\amp 240\amp 255\amp 255\amp 255\amp 255\amp 255\\240\amp 240\amp 240\amp 240\amp 240\amp 240 \amp 75\amp 240\amp 240\amp 240\amp 240\amp 240\amp 240\amp 240\amp 240\amp 240\\240\amp 240\amp 240 \amp 75\amp 75\amp 240\amp 75\amp 240\amp 75\amp 75\amp 240\amp 240\amp 240\amp 240\amp 240\amp 240\\50\amp 240\amp 240\amp 240\amp 75\amp 240\amp 75\amp 240\amp 75\amp 240\amp 240\amp 240\amp 240\amp 50\amp 240\amp 240 \\240\amp 75\amp 240\amp 240\amp 240\amp 75\amp 75\amp 75\amp 240\amp 240\amp 50\amp 240\amp 50\amp 240\amp 240\amp 50\\240\amp 240\amp 75\amp 240\amp 240\amp 240\amp 75\amp 240\amp 240\amp 50\amp 240\amp 50\amp 240\amp 240\amp 50\amp 240\\75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\\75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75 \end{array}  }\right]\text{.}\label{x:men:eq_flower_image_matrix}
\end{equation}
%
\par
Now we can apply wavelets to the image and compress it. Essentially, wavelets act by averaging and differencing. The averaging creates smaller versions of the image and the differencing keeps track of how far the smaller version is from a previous copy. The differencing often produces many small (close to 0) entries, and so replacing these entries with 0 doesn't have much effect on the image (this is called \emph{thresholding}). By introducing long strings of zeros into our data, we are able to store a (compressed) copy of the image in a smaller amount of space. For example, using a threshold value of 10 produces the flower image shown at right in \hyperref[x:figure:F_Flower_1]{Figure~{\xreffont\ref{x:figure:F_Flower_1}}}.%
\par
The averaging and differencing is done with special vectors (wavelets) that form a basis for a suitable function space. More details of this process can be found at the end of this section.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_bases_intro}
In \(\R^n\) we defined a basis for a subspace \(W\) of \(\R^n\) to be a minimal spanning set for \(W\), or a linearly independent spanning set (see \hyperref[x:definition:def_1_f_basis]{Definition~{\xreffont\ref{x:definition:def_1_f_basis}}}). So to consider the idea of a basis in a vector space, we will need the notion of linear independence in that context.%
\par
Since we can add vectors and multiply vectors by scalars in any vector space, and because we have a zero vector in any vector space, we can define linear independence of a finite set of vectors in any vector space as follows (compare to \hyperref[x:definition:def_linear_independence_Rn]{Definition~{\xreffont\ref{x:definition:def_linear_independence_Rn}}}).%
\begin{definition}{}{x:definition:def_vs_linear_independence}%
\index{linearly independent vectors in a vector space}%
\index{linear dependent vectors in a vector space}%
A set \(\{\vv_1, \vv_2, \ldots, \vv_k\}\) of vectors in a vector space \(V\) is \terminology{linearly independent} if the vector equation%
\begin{equation*}
x_1 \vv_1 + x_2 \vv_2 + \cdots + x_k \vv_k = \vzero
\end{equation*}
for scalars \(x_1, x_2, \ldots,
x_k\) has only the trivial solution%
\begin{equation*}
x_1 = x_2 = x_3 = \cdots = x_k = 0\text{.}
\end{equation*}
%
\par
If a set of vectors is not linearly independent, then the set is \terminology{linearly dependent}.%
\end{definition}
Alternatively, we say that the vectors \(\vv_1, \vv_2, \ldots, \vv_k\) are linearly independent (or dependent) if the set \(\{\vv_1, \vv_2, \ldots, \vv_k\}\) is linearly independent (or dependent).%
\begin{exploration}{}{x:exploration:pa_5_b}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}We can use the tools we developed to determine if a set of vectors in \(\R^n\) is linearly independent to answer the same questions for sets of vectors in other vector spaces. For example, consider the question of whether the set \(\{1+t, 1-t\}\) in \(\pol_1\) is linearly independent or dependent. To answer this question we need to determine if there is a non-trivial solution to the equation%
\begin{equation}
x_1 (1+t) + x_2(1-t) = 0\text{.}\label{x:men:eq_PA_5_b_1}
\end{equation}
Note that equation \hyperref[x:men:eq_PA_5_b_1]{({\xreffont\ref{x:men:eq_PA_5_b_1}})} can also be written in the form%
\begin{equation*}
(x_1+x_2) + (x_1-x_2)t = 0\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Recall that two polynomials are equal if all coefficients of like powers are the same. By equating coefficients of like power terms, rewrite equation \hyperref[x:men:eq_PA_5_b_1]{({\xreffont\ref{x:men:eq_PA_5_b_1}})} as an equivalent system of two equations in the two unknowns \(x_1\) and \(x_2\), and solve for \(x_1, x_2\).%
\item{}What does your answer to the previous part tell you about the linear independence or dependence of the set \(\{1+t, 1-t\}\) in \(\pol_1\)?%
\item{}Recall that in \(\R^n\), a set of two vectors is linearly dependent if and only if one of the vectors in the set is a scalar multiple of the other and linearly independent if neither vector is a scalar multiple of the other. Verify your answer to part (c) from a similar perspective in \(\pol_1\).%
\end{enumerate}
\item{}We can use the same type of method as in problem (1) to address the question of whether the set%
\begin{equation*}
\left\{ \left[ \begin{array}{cc} 1\amp 3 \\ 1\amp 2 \end{array}  \right], \left[ \begin{array}{cr} 1\amp -9 \\ 1\amp 8 \end{array}  \right], \left[ \begin{array}{cr} 1\amp -1 \\ 1\amp 4 \end{array}  \right] \right\}
\end{equation*}
is linearly independent or dependent in \(\M_{2 \times 2}\). To answer this question we need to determine if there is a non-trivial solution to the equation%
\begin{equation}
x_1\left[ \begin{array}{cc} 1\amp 3 \\ 1\amp 2 \end{array}  \right] + x_2\left[ \begin{array}{cr} 1\amp -9 \\ 1\amp 8 \end{array}  \right] + x_3 \left[ \begin{array}{cr} 1\amp -1 \\ 1\amp 4 \end{array}  \right] = \vzero\label{x:men:eq_PA_5_b_2}
\end{equation}
for some scalars \(x_1\), \(x_2\), and \(x_3\). Note that the linear combination on the left side of equation \hyperref[x:men:eq_PA_5_b_2]{({\xreffont\ref{x:men:eq_PA_5_b_2}})} has entries%
\begin{equation*}
\left[ \begin{array}{cc} x_1+x_2+x_3\amp 3x_1-9x_2-x_3 \\ x_1+x_2+x_3\amp 2x_1+8x_2+4x_3 \end{array}  \right]\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Recall that two matrices are equal if all corresponding entries are the same. Equate corresponding entries of the matrices in equation \hyperref[x:men:eq_PA_5_b_2]{({\xreffont\ref{x:men:eq_PA_5_b_2}})} to rewrite the equation as an equivalent system of four equations in the three unknowns \(x_1\), \(x_2\), and \(x_3\).%
\item{}Use appropriate matrix tools and techniques to find all solutions to the system from part (a).%
\item{}What does the set of solutions to the system from part (a) tell you about the linear independence or dependence of the set%
\begin{equation*}
\left\{ \left[ \begin{array}{cc} 1\amp 3 \\ 1\amp 2 \end{array}  \right], \left[ \begin{array}{cr} 1\amp -9 \\ 1\amp 8 \end{array}  \right], \left[ \begin{array}{cr} 1\amp -1 \\ 1\amp 4 \end{array}  \right] \right\}?
\end{equation*}
%
\item{}Recall that in \(\R^n\), a set of vectors is linearly dependent if and only if one of the vectors in the set is a linear combination of the others and linearly independent if no vector in the set is a linear combination of the others. Verify your answer to part (c) from a similar perspective in \(\M_{2 \times 2}\).%
\end{enumerate}
\item{}We will define a basis for a vector space to be a linearly independent spanning set. Which, if any, of the sets in parts (1) and (2) is a basis for its vector space? Explain.%
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Linear Independence}
\typeout{************************************************}
%
\begin{sectionptx}{Linear Independence}{}{Linear Independence}{}{}{x:section:sec_lin_indep}
The concept of linear independence, which we formally defined in \hyperref[x:exploration:pa_5_b]{Preview Activity~{\xreffont\ref{x:exploration:pa_5_b}}}, provides us with a process to determine if there is redundancy in a spanning set to obtain an efficient spanning set.%
\par
The definition tells us that a set \(\{\vv_1, \vv_2, \ldots, \vv_k\}\) of vectors in a vector space \(V\) is linearly dependent if there are scalars \(x_1\), \(x_2\), \(\ldots\), \(x_n\), not all of which are 0 so that%
\begin{equation*}
x_1 \vv_1 + x_2 \vv_2 + \cdots + x_k \vv_k = \vzero\text{.}
\end{equation*}
%
\par
As examples, we saw in \hyperref[x:exploration:pa_5_b]{Preview Activity~{\xreffont\ref{x:exploration:pa_5_b}}} that the set \(\{1+t, 1-t\}\) is linearly independent in \(\pol_1\). The set \(\{1+t, -1+2t+t^2, 1-8t-3t^2\}\), on the other hand, is linearly dependent in \(\pol_2\) since \(2(1+t) + 3(-1+2t+t^2) + (1-8t-3t^2) = 0\).%
\par
In addition to the definition, there are other ways to characterize linearly independent and dependent sets in vector spaces as the next theorems illustrate. These characterizations are the same as those we saw in \(\R^n\), and the proofs are essentially the same as well. The proof of \hyperref[x:theorem:thm_5_b_1]{Theorem~{\xreffont\ref{x:theorem:thm_5_b_1}}} is similar to that of \hyperref[x:theorem:thm_dependence]{Theorem~{\xreffont\ref{x:theorem:thm_dependence}}} and is left for the exercises.%
\begin{theorem}{}{}{x:theorem:thm_5_b_1}%
A set \(\{\vv_1, \vv_2, \ldots, \vv_k\}\) of vectors in a vector space \(V\) is linearly dependent if and only if at least one of the vectors in the set can be written as a linear combination of the remaining vectors in the set.%
\end{theorem}
\hyperref[x:theorem:thm_5_b_1]{Theorem~{\xreffont\ref{x:theorem:thm_5_b_1}}} is equivalent to the following theorem that provides the corresponding result for linearly independent sets.%
\begin{theorem}{}{}{x:theorem:thm_5_b_2}%
A set \(\{\vv_1, \vv_2, \ldots, \vv_k\}\) of vectors in a vector space \(V\) is linearly independent if and only if no vector in the set can be written as a linear combination of the remaining vectors in the set.%
\end{theorem}
One consequence of \hyperref[x:theorem:thm_5_b_1]{Theorem~{\xreffont\ref{x:theorem:thm_5_b_1}}} and \hyperref[x:theorem:thm_5_b_2]{Theorem~{\xreffont\ref{x:theorem:thm_5_b_2}}} is that if a spanning set is linearly dependent, then one of the vectors in the set can be written as a linear combination of the others. In other words, at least one of the vectors is redundant. In that case, we can find a smaller spanning set as the next theorem states. The proof of this theorem is similar to that of \hyperref[x:theorem:thm_minimal_spanning_set]{Theorem~{\xreffont\ref{x:theorem:thm_minimal_spanning_set}}} and is left for the exercises.%
\begin{theorem}{}{}{x:theorem:thm_5_b_3}%
Let \(\{\vv_1, \vv_2, \ldots, \vv_k\}\) be a set of vectors in a vector space \(V\). If for some \(i\) between 1 and \(k\), \(\vv_i\) is in \(\Span \{\vv_1, \vv_2, \ldots, \vv_{i-1}, \vv_{i+1}, \ldots, \vv_k\}\), then%
\begin{equation*}
\Span\{\vv_1, \vv_2, \ldots, \vv_k\} = \Span\{\vv_1, \vv_2, \ldots, \vv_{i-1}, \vv_{i+1}, \ldots, \vv_k\}\text{.}
\end{equation*}
%
\end{theorem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Bases}
\typeout{************************************************}
%
\begin{sectionptx}{Bases}{}{Bases}{}{}{x:section:sec_bases}
A basis for a vector space is a spanning set that is as small as it can be. We already saw how to define bases formally in \(\R^n\). We will now formally define a basis for a vector space and understand why with this definition a basis is a minimal spanning set. Bases are important because any vector in a vector space can be uniquely represented as a linear combination of basis vectors. We will see in later sections that this representation will allow us to identify any vector space with a basis of \(n\) vectors with \(\R^n\).%
\par
To obtain the formal definition of a basis, which is a minimal spanning set, we consider what additional property makes a spanning set a \terminology{minimal} spanning set. As a consequence of \hyperref[x:theorem:thm_5_b_3]{Theorem~{\xreffont\ref{x:theorem:thm_5_b_3}}}, if \(S\) is a spanning set that is linearly dependent, then we can find a proper subset of \(S\) that has the same span. Thus, the set \(S\) cannot be a minimal spanning set. However, if \(S\) is linearly independent, then no vector in \(S\) is a linear combination of the others and we need all of the vectors in \(S\) to form the span. This leads us to the following formal characterization of a minimal spanning set, called a \terminology{basis}.%
\begin{definition}{}{g:definition:idp24152360}%
\index{basis for a vector space}%
\index{vector space!basis}%
A \terminology{basis} for a vector space \(V\) is a subset \(S\) of \(V\) if%
\begin{enumerate}
\item{}\(\Span \ S = V\) and%
\item{}\(S\) is a linearly independent set.%
\end{enumerate}
%
\end{definition}
In other words, a basis for a vector space \(V\) is a linearly independent spanning set for \(V\). To put it another way, a basis for a vector space is a minimal spanning set for the vector space. Similar reasoning will show that a basis is also a maximal linearly independent set.%
\par
The key ideas to take from the previous theorems are:%
\begin{itemize}[label=\textbullet]
\item{}A basis for a vector space \(V\) is a minimal spanning set for \(V\).%
\item{}A basis for \(V\) is a subset \(S\) of \(V\) so that%
\begin{enumerate}
\item{}\(S\) spans \(V\) and%
\item{}\(S\) is linearly independent.%
\end{enumerate}
%
\item{}No vector in a basis can be written as a linear combination of the other vectors in the basis.%
\item{}If a subset \(S\) of a vector space \(V\) has the property that one of the vectors in \(S\) is a linear combination of the other vectors in \(S\), then \(S\) is not a basis for \(V\).%
\end{itemize}
%
\par
As an example of a basis of a vector space, we saw in \hyperref[x:exploration:pa_5_b]{Preview Activity~{\xreffont\ref{x:exploration:pa_5_b}}} that the set \(S = \{1-t, 1+t\}\) is both linearly independent and spans \(\pol_1\), and so \(S\) is a basis for \(\pol_1\).%
\begin{activity}{}{x:activity:act_5_b_1}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Is \(S = \{1+t, t, 1-t\}\) a basis for \(\pol_1\)? Explain.%
\item{}Explain why the set \(S = \{1, t, t^2, \ldots,
t^n\}\) is a basis for \(\pol_n\). This basis is called the \terminology{standard basis} for \(\pol_n\).%
\item{}Show that the set%
\begin{equation*}
\left\{ \left[ \begin{array}{cc} 1\amp 0 \\ 1\amp 1 \end{array}  \right], \left[ \begin{array}{cc} 0\amp 1 \\ 1\amp 1 \end{array}  \right], \left[ \begin{array}{cc} 1\amp 1 \\ 1\amp 0 \end{array}  \right], \left[ \begin{array}{cc} 1\amp 1 \\ 0\amp 1 \end{array}  \right] \right\}
\end{equation*}
is a basis for \(\M_{2 \times 2}\).%
\end{enumerate}
\end{activity}%
It should be noted that not every vector space has a finite basis. For example, the space \(\pol\) of all polynomials with real coefficients (of any degree) is a vector space, but no finite set of vectors will span \(\pol\). In fact, the infinite set \(\{1, t, t^2, \ldots\}\) is both linearly independent and spans \(\pol\), so \(\pol\) has an infinite basis.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Finding a Basis for a Vector Space}
\typeout{************************************************}
%
\begin{sectionptx}{Finding a Basis for a Vector Space}{}{Finding a Basis for a Vector Space}{}{}{x:section:sec_basis_vec_space}
We already know how to find bases for certain vector spaces, namely \(\Nul A\) and \(\Col A\), where \(A\) is any matrix. Finding a basis for a different kind of vector space will require other methods. Since a basis for a vector space is a minimal spanning set, to find a basis for a given vector space we might begin from scratch, starting with a given vector in the space and adding one vector at a time until we have a spanning set.%
\begin{activity}{}{x:activity:act_5_b_2}%
Let \(W = \{a + bt + ct^3 \mid a, b, c \text{ are scalars } \}\). We will find a basis of \(W\) that contains the polynomial \(3+t-t^3\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(\CS_1 = \{3+t-t^3\}\). Find a polynomial \(p(t)\) in \(W\) that is not in \(\Span \ \CS_1\). Explain why this means that the set \(\CS_1\) does not span \(W\).%
\item{}Let \(\CS_2 = \{3+t-t^3, p(t)\}\). Find a polynomial \(q(t)\) that is not in \(\Span \ \CS_2\). What does this mean about \(\CS_2\) being a possible spanning set of \(W\)?%
\item{}Let \(\CS_3 = \{3+t-t^3, p(t), q(t)\}\). Explain why the set \(\CS_3\) is a basis for \(W\).%
\end{enumerate}
\end{activity}%
Alternatively, we might construct a basis from a known spanning set.%
\begin{activity}{}{x:activity:act_5_b_2_b}%
Let \(W = \left\{\left[ \begin{array}{cc} v+z\amp w+z \\ x\amp y \end{array} \right] \left. \right| v, w, x, y, z \text{ are scalars } \right\}\). Assume that \(W\) is a subspace of \(\M_{2 \times 2}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find a set \(S\) of five \(2 \times 2\) matrices that spans \(W\) (since \(W\) is a span of a set of vectors in \(\M_{2 \times 2}\), \(W\) is a subspace of \(\M_{2 \times 2}\)). Without doing any computation, can this set \(S\) be a basis for \(W\)? Why or why not?%
\item{}Find a subset \(\CB\) of \(S\) that is a basis for \(W\).%
\end{enumerate}
\end{activity}%
\hyperref[x:activity:act_5_b_2]{Activity~{\xreffont\ref{x:activity:act_5_b_2}}} and \hyperref[x:activity:act_5_b_2_b]{Activity~{\xreffont\ref{x:activity:act_5_b_2_b}}} give us two ways of finding a basis for a subspace \(W\) of a vector space \(V\), assuming \(W\) has a basis with finitely many vectors. One way (illustrated in \hyperref[x:activity:act_5_b_2]{Activity~{\xreffont\ref{x:activity:act_5_b_2}}}) is to start by choosing any non-zero vector \(\vw_1\) in \(W\). Let \(\CS_1 = \{\vw_1\}\). If \(\CS_1\) spans \(W\), then \(\CS_1\) is a basis for \(W\). If not, there is a vector \(\vw_2\) in \(W\) that is not in \(\Span \ \CS_1\). Then \(\CS_2 = \{\vw_1, \vw_2\}\) is a linearly independent set. If \(\Span \ \CS_2 = W\), then \(\CS_2\) is a basis for \(W\) and we are done. If not, repeat the process. We will show later that this process must stop as long as we know that \(W\) has a basis with fini tely many vectors.%
\par
Another way (illustrated in \hyperref[x:activity:act_5_b_2_b]{Activity~{\xreffont\ref{x:activity:act_5_b_2_b}}}) to find a basis for \(W\) is to start with a spanning set \(\CS_1\) of \(W\). If \(\CS_1\) is linearly independent, then \(\CS_1\) is a basis for \(W\). If \(\CS_1\) is linearly dependent, then one vector in \(\CS_1\) is a linear combination of the others and we can remove that vector to obtain a new set \(\CS_2\) that also spans \(W\). If \(\CS_2\) is linearly independent, then \(\CS_2\) is a basis for \(W\). If not, we repeat the process as many times as needed until we arrive until at a subset \(\CS_k\) of \(\CS_1\) that is linearly independent and spans \(W\). We summarize these results in the following theorem.%
\begin{theorem}{}{}{g:theorem:idp24214696}%
Let \(W\) be a subspace of a finite-dimensional vector space \(V\). Then%
\begin{enumerate}
\item{}any linearly independent subset of \(W\) can be extended to a basis of \(W\),%
\item{}any subset of \(W\) that spans \(W\) can be reduced to a basis of \(W\).%
\end{enumerate}
%
\end{theorem}
We conclude this section with the result mentioned in the introduction \textemdash{} that every vector in a vector space with basis \(\B\) can be written in one and only one way as a linear combination of basis vectors. The proof is similar to that of \hyperref[x:theorem:thm_1_f_unique_representation]{Theorem~{\xreffont\ref{x:theorem:thm_1_f_unique_representation}}} and is left to the exercises.%
\begin{theorem}{}{}{x:theorem:thm_5_b_4}%
Let \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_n\) be vectors in a vector space \(V\) that make up a basis \(\B\) for \(V\). If \(\vu\) is a vector in \(V\), then \(\vu\) can be written in one and only one way as a linear combination of vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_n\) in \(\B\).%
\end{theorem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_bases_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp24231464}%
Let \(S = \{1, 1+t, 2-t^2, 1+t+t^2, t-t^2\}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Does \(S\) span \(\pol_2\)? Explain.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp24229544}{}\quad{}Let \(p(t) = a_0+a_1t+a_2t^2\) be an arbitrary vector in \(\pol_2\). If \(p(t)\) is in \(\Span \ S\), then there are weights \(c_1\), \(c_2\), \(c_3\), \(c_4\), and \(c_5\) such that%
\begin{equation*}
a_0+a_1t+a_2t^2 = c_1(1) + c_2(1+t) + c_3(2-t^2) + c_4(1+t+t^2) + c_5(t-t^2)\text{.}
\end{equation*}
Equating coefficients of like powers gives us the system%
\begin{align*}
{}c_1   \amp {}+{}   \amp {}c_2   \amp {}+{}  \amp {2}c_3  \amp {}+{}  \amp {}c_4  \amp {}{}  \amp {}     \amp = a_0\amp {}\\
{}     \amp {}{}     \amp {}c_2   \amp {}{}    \amp {}    \amp {}+{}  \amp {}c_4  \amp {}+{}  \amp {}c_5     \amp = a_1\amp {}\\
{}     \amp {}{}     \amp {}     \amp {}{}    \amp {-}c_3  \amp {}+{}  \amp {}c_4  \amp {}-{}  \amp {}c_5     \amp = a_2\amp {.}
\end{align*}
The reduced row echelon form of the coefficient matrix \(A\) is%
\begin{equation*}
\left[ \begin{array}{cccrr} 1\amp 0\amp 0\amp 2\amp -3 \\ 0\amp 1\amp 0\amp 1\amp 1 \\ 0\amp 0\amp 1\amp -1\amp 1 \end{array}  \right]\text{.}
\end{equation*}
Since there is a pivot in every row of \(A\), the system \(A \vx = \vb\) is always consistent. We conclude that \(S\) does span \(\pol_2\).%
\item{}Explain why \(S\) is not a basis for \(\pol_2\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp24238120}{}\quad{}The fact that the coefficient matrix \(A\) of our system has non-pivot columns means that each vector in \(\pol_2\) can be written in more than one way as a linear combination of vectors in \(S\). This means that \(S\) is not linearly independent and so cannot be a basis for \(\pol_2\).%
\item{}Find a subset of \(S\) that is a basis for \(\pol_2\). Explain your reasoning.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp24242600}{}\quad{}That the first three columns of \(A\) are pivot columns implies that the polynomials \(1\), \(1+t\), and \(2-t^2\) are linearly independent. Since there is a pivot in every row of \(A\), the three polynomials \(1\), \(1+t\), and \(2-t^2\) also span \(\pol_2\). So \(\{1, 1+t, 2-t^2\}\) is a subset of \(S\) that is a basis for \(\pol_2\).%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp24260744}%
Let \(U\) be the set of all matrices of real numbers of the form \(\left[ \begin{array}{cc} u \amp -u-x \\ 0 \amp x \end{array} \right]\) and \(W\) be the set of all real matrices of the form \(\left[ \begin{array}{cc} v \amp 0 \\ w \amp -v \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find a basis for \(U\) and a basis for \(W\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp24265864}{}\quad{}Every matrix in \(U\) has the form%
\begin{equation*}
\left[ \begin{array}{cc} u \amp  -u-x \\  0 \amp    x \end{array}  \right] = u\left[ \begin{array}{cr} 1 \amp  -1 \\  0 \amp  0 \end{array}  \right] + x\left[ \begin{array}{cr} 0 \amp  -1 \\  0 \amp  1 \end{array}  \right]\text{.}
\end{equation*}
Let \(S_U = \left\{ \left[ \begin{array}{cr} 1 \amp  -1 \\  0 \amp  0 \end{array}  \right], \left[ \begin{array}{cr} 0 \amp  -1 \\  0 \amp  1 \end{array}  \right] \right\}\). Then \(U = \Span \ S_U\) and \(U\) is a subspace of \(\M_{2 \times 2}\). If%
\begin{equation*}
c_1\left[ \begin{array}{cr} 1 \amp  -1 \\  0 \amp  0 \end{array}  \right] + c_2\left[ \begin{array}{cr} 0 \amp  -1 \\  0 \amp  1 \end{array}  \right] = 0\text{,}
\end{equation*}
then \(c_1=c_2=0\) and \(S_U\) is also linearly independent. This makes \(S_U\) a basis for \(U\). Similarly, every matrix in \(W\) has the form%
\begin{equation*}
\left[ \begin{array}{cc}   v \amp   0 \\ w \amp  -v \end{array}  \right] = v \left[ \begin{array}{cr} 1 \amp  0 \\  0 \amp  -1 \end{array}  \right] + w\left[ \begin{array}{cc} 0 \amp  0 \\  1 \amp  0 \end{array}  \right]\text{.}
\end{equation*}
Let \(S_W = \left\{ \left[ \begin{array}{cr} 1 \amp  0 \\  0 \amp  -1 \end{array}  \right] , \left[ \begin{array}{cc} 0 \amp  0 \\  1 \amp  0 \end{array}  \right] \right\}\). Then \(W = \Span \ S_W\) and \(W\) is a subspace of \(\M_{2 \times 2}\). If%
\begin{equation*}
c_1\left[ \begin{array}{cr} 1 \amp  0 \\  0 \amp  -1 \end{array}  \right] + c_2\left[ \begin{array}{cc} 0 \amp  0 \\  1 \amp  0 \end{array}  \right]  = 0\text{,}
\end{equation*}
then \(c_1=c_2=0\) and \(S_W\) is also linearly independent. This makes \(S_W\) a basis for \(W\).%
\item{}Let \(U + W = \{A+B : A \text{ is in } U \text{ and } B \text{ is in } W\}\). Show that \(U+W\) is a subspace of \(\M_{2 \times 2}\) and find a basis for \(U + W\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp24281224}{}\quad{}Every matrix in \(U+W\) has the form%
\begin{align*}
\left[ \begin{array}{cc} u \amp  -u-x\\
0 \amp    x \end{array} \right] \amp + \left[ \begin{array}{cc}   v \amp   0\\
w \amp  -v \end{array} \right] = \left[ \begin{array}{cc}   u+v \amp   -u-x\\
w \amp  x-v \end{array} \right]\\
\amp = u\left[ \begin{array}{cr}   1 \amp   -1\\
0 \amp  0 \end{array} \right] + x\left[ \begin{array}{cr}   0 \amp   -1\\
0 \amp  1 \end{array} \right]\\
\amp \qquad + v\left[ \begin{array}{cr}   1 \amp   0\\
0 \amp  -1 \end{array} \right] + w\left[ \begin{array}{cc}   0 \amp   0\\
1 \amp  0 \end{array} \right]\text{.}
\end{align*}
Let \(S = \left\{ \left[ \begin{array}{cr}   1 \amp   -1 \\ 0 \amp  0 \end{array}  \right], \left[ \begin{array}{cr}   0 \amp   -1 \\ 0 \amp  1 \end{array}  \right], \left[ \begin{array}{cr}   1 \amp   0 \\ 0 \amp  -1 \end{array}  \right], \left[ \begin{array}{cc}   0 \amp   0 \\ 1 \amp  0 \end{array}  \right] \right\}\). Then \(U+W = \Span \ S\) and \(U+W\) is a subspace of \(\M_{2 \times 2}\). If%
\begin{equation*}
c_1\left[ \begin{array}{cr}   1 \amp   -1 \\ 0 \amp  0 \end{array}  \right] + c_2\left[ \begin{array}{cr}   0 \amp   -1 \\ 0 \amp  1 \end{array}  \right] + c_3\left[ \begin{array}{cr}   1 \amp   0 \\ 0 \amp  -1 \end{array}  \right] + c_4\left[ \begin{array}{cc}   0 \amp   0 \\ 1 \amp  0 \end{array}  \right] = 0\text{,}
\end{equation*}
then%
\begin{align*}
{}c_1   \amp {}     \amp {}    \amp {}+{}  \amp {}c_3  \amp {}  \amp {}    \amp = \amp 0\amp {}\\
{-}c_1   \amp {}-{}   \amp {}c_2  \amp {}    \amp {}    \amp {}  \amp {}    \amp = \amp 0\amp {}\\
{}     \amp {}     \amp {}    \amp {}    \amp {}    \amp {}  \amp {}c_4  \amp = \amp 0\amp {}\\
{}       \amp {}     \amp {}c_2  \amp {}-{}  \amp {}c_3  \amp {}  \amp {}    \amp = \amp 0\amp {.}
\end{align*}
The reduced row echelon form of \(\left[ \begin{array}{rrrc} 1\amp 0\amp 1\amp 0 \\ -1\amp -1\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 1 \\ 0\amp 1\amp -1\amp 0 \end{array}  \right]\) is \(\left[ \begin{array}{ccrc} 1\amp 0\amp 1\amp 0 \\ 0\amp 1\amp -1\amp 0 \\ 0\amp 0\amp 0\amp 1 \\ 0\amp 0\amp 0\amp 0 \end{array}  \right]\). The vectors that correspond to the pivot columns are linearly independent and span \(U+W\), so a basis for \(U+W\) is%
\begin{equation*}
\left\{ \left[ \begin{array}{cr}   1 \amp   -1 \\ 0 \amp  0 \end{array}  \right], \left[ \begin{array}{cr}   0 \amp   -1 \\ 0 \amp  1 \end{array}  \right], \left[ \begin{array}{cc}   0 \amp   0 \\ 1 \amp  0 \end{array}  \right] \right\}\text{.}
\end{equation*}
%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_bases_summ}
The important idea in this section is that of a basis for a vector space. A basis is a minimal spanning set and another equivalent characterization of the ``minimal'' property is linear independence.%
\begin{itemize}[label=\textbullet]
\item{}A set \(\{\vv_1, \vv_2, \ldots, \vv_k\}\) of vectors in a vector space \(V\) is linearly independent if the vector equation%
\begin{equation*}
x_1 \vv_1 + x_2 \vv_2 + \cdots + x_k \vv_k = \vzero
\end{equation*}
for scalars \(x_1, x_2, \ldots,
x_k\) has only the trivial solution%
\begin{equation*}
x_1 = x_2 = x_3 = \cdots = x_k = 0\text{.}
\end{equation*}
If a set of vectors is not linearly independent, then the set is linearly dependent.%
\item{}A set \(\{\vv_1, \vv_2, \ldots, \vv_k\}\) of vectors in a vector space \(V\) is linearly independent if and only if none of the vectors in the set can be written as a linear combination of the remaining vectors in the set.%
\item{}A set \(\{\vv_1, \vv_2, \ldots, \vv_k\}\) of vectors in a vector space \(V\) is linearly dependent if and only if at least one of the vectors in the set can be written as a linear combination of the remaining vectors in the set.%
\item{}A basis for a vector space \(V\) is a subset \(S\) of \(V\) if%
\begin{enumerate}
\item{}\(\Span \ S = V\) and%
\item{}\(S\) is a linearly independent set.%
\end{enumerate}
%
\item{}A basis is important in that it provides us with an efficient way to represent any vector in the vector space \textemdash{} any vector can be written in one and only one way as a linear combination of vectors in a basis.%
\item{}To find a basis of a vector space, we can start with a spanning set \(S\) and toss out any vector in \(S\) that can be written as a linear combination of the remaining vectors in \(S\). We repeat the process with the remaining subset of \(S\) until we arrive at a linearly independent spanning set. Alternatively, we can find a spanning set for the space and remove any vector that is a linear combination of the others in the spanning set. We can repeat this process until we wind up with a linearly independent spanning set.%
\end{itemize}
%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_bases_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp24306696}%
Determine if the given sets are linearly independent or dependent in the indicated vector space. If dependent, write one of the vectors as a linear combination of the others. If independent, determine if the set is a basis for the vector space.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\{[1 \ 4 \ 6]^{\tr}, [2 \ -1 \ 3]^{\tr}, [0 \ 1 \ 5]^{\tr}\}\) in \(\R^3\)%
\item{}\(\{1-2t^2+t^3, 3-t+4t^3, 2-3t\}\) in \(\pol_3\)%
\item{}\(\{1+t, -1-5t+4t^2+t^3, 1+t^2+t^3, t+2t^3\}\) in \(\pol_3\)%
\item{}\(\left\{ \left[ \begin{array}{ccc} 1\amp 2\amp 0\\0\amp 1\amp 1 \end{array} \right], \left[ \begin{array}{crc} 1\amp -2\amp 0\\0\amp -1\amp 1 \end{array} \right], \left[ \begin{array}{ccr} 1\amp 2\amp 0\\0\amp 1\amp -1 \end{array} \right] \right\}\) in \(\M_{3 \times 2}\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp24315784}%
Let \(S = \{1+t+t^2, t+t^2, 1+t, 1+t^2\}\) in \(\pol_2\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that the set \(S\) spans \(\pol_2\).%
\item{}Show that the set \(S\) is linearly dependent.%
\item{}Find a subset of \(S\) that is a basis for \(\pol_2\). Be sure to verify that you have a basis.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp24309512}%
Find two different bases for \(\M_{2 \times 2}\). Explain how you know that each set is a basis.%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp24318216}%
The set \(W = \{at+bt^2 \mid a \text{ and } b \text{ are scalars } \}\) is a subspace of \(\pol_3\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find a set of vectors in \(\pol_3\) that spans \(W\).%
\item{}Find a basis for \(W\). Be sure to verify that you have a basis.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp24320136}%
Suppose that the set \(\{\vu, \vv, \vw\}\) is a basis for a vector space \(V\). Is the set \(\{\vu+\vv, \vu+\vw, \vv+\vw\}\) a basis for \(V\)? Verify your result.%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp24330504}%
Determine all scalars \(c\) so that the set \(\{c^2+t^2, c+2t, 1+t^2\}\) is a basis for \(\pol_2\).%
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp24330760}%
A symmetric matrix is a matrix \(A\) so that \(A^{\tr} = A\). Is it possible to find a basis for \(\M_{2 \times 2}\) consisting entirely of symmetric matrices? If so, exhibit one such basis. If not, explain why not.%
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp24332680}%
Find a basis of the subspace of \(\M_{2 \times 3}\) consisting of all matrices of the form \(\left[ \begin{array}{ccc} a\amp b\amp c \\ d\amp e\amp f \end{array} \right]\) where \(c=a-2d\) and \(f = b+3e\).%
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp24337160}%
Prove \hyperref[x:theorem:thm_5_b_1]{Theorem~{\xreffont\ref{x:theorem:thm_5_b_1}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp24335624}{}\quad{}Mimic the proof of \hyperref[x:theorem:thm_dependence]{Theorem~{\xreffont\ref{x:theorem:thm_dependence}}}.%
\end{divisionexercise}%
\begin{divisionexercise}{10}{}{}{g:exercise:idp24338056}%
Prove \hyperref[x:theorem:thm_5_b_3]{Theorem~{\xreffont\ref{x:theorem:thm_5_b_3}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp24334984}{}\quad{}Mimic the proof of \hyperref[x:theorem:thm_minimal_spanning_set]{Theorem~{\xreffont\ref{x:theorem:thm_minimal_spanning_set}}}.%
\end{divisionexercise}%
\begin{divisionexercise}{11}{}{}{g:exercise:idp24341512}%
Prove \hyperref[x:theorem:thm_5_b_4]{Theorem~{\xreffont\ref{x:theorem:thm_5_b_4}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp24341256}{}\quad{}Compare to \hyperref[x:theorem:thm_1_f_unique_representation]{Theorem~{\xreffont\ref{x:theorem:thm_1_f_unique_representation}}}.%
\end{divisionexercise}%
\begin{divisionexercise}{12}{}{}{x:exercise:problem_disjoint_subspaces}%
Show that if \(W_1, W_2\) are subspaces of \(V\) such that \(W_1\cap W_2 = \{ \vzero \}\), then for any linearly independent vectors \(\vu_1, \vu_2, \ldots, \vu_k\) in \(W_1\) and \(\vv_1, \vv_2, \ldots, \vv_\ell\) in \(W_2\), the set \(\{ \vu_1\), \(\vu_2\), \(\ldots\), \(\vu_k\), \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_\ell\}\) is linearly independent in \(V\).%
\end{divisionexercise}%
\begin{divisionexercise}{13}{}{}{g:exercise:idp24343816}%
Label each of the following statements as True or False. Provide justification for your response. Throughout, let \(V\) be a vector space.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
If \(\vv\) is in \(V\), then the set \(\{\vv\}\) is linearly independent.%
\item{}\lititle{True\slash{}False.}\par%
If a set of vectors span a subspace, then the set forms a basis of this subspace.%
\item{}\lititle{True\slash{}False.}\par%
If a linearly independent set of vectors spans a subspace, then the set forms a basis of this subspace.%
\item{}\lititle{True\slash{}False.}\par%
If the set \(S\) spans \(V\) and removing any vector from \(S\) makes it not a spanning set anymore, then \(S\) is a basis.%
\item{}\lititle{True\slash{}False.}\par%
If \(S\) is a linearly independent set in \(V\) and for every \(\vu\) in \(V\), adding \(\vu\) to \(S\) makes it not linearly independent anymore, then \(S\) is a basis.%
\item{}\lititle{True\slash{}False.}\par%
If a subset \(S\) of \(V\) spans \(V\), then \(S\) must be linearly independent.%
\item{}\lititle{True\slash{}False.}\par%
If a subset \(S\) of \(V\) is linearly independent, then \(S\) must span \(V\).%
\item{}\lititle{True\slash{}False.}\par%
If \(S\) is a linearly dependent set in \(V\), then every vector in \(S\) is a linear combination of the other vectors in \(S\).%
\item{}\lititle{True\slash{}False.}\par%
A vector space cannot have more than one basis.%
\item{}\lititle{True\slash{}False.}\par%
If \(\vu\) is a non-zero vector in \(V\), then there is a basis of \(V\) containing \(\vu\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\vu, \vv\) are two linearly independent vectors in \(V\), then there is a basis of \(V\) containing \(\vu, \vv\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\vu\) is in a basis of \(V\), then \(2\vu\) cannot be in a basis of \(V\).%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Image Compression with Wavelets}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Image Compression with Wavelets}{}{Project: Image Compression with Wavelets}{}{}{x:section:sec_proj_img_compress}
We return to the problem of image compression introduced at the beginning of this section. The first step in the wavelet compression process is to digitize an image. There are two important ideas about digitalization to understand here: intensity levels and resolution. In grayscale image processing, it is common to think of 256 different intensity levels, or scales, of gray ranging from 0 (black) to 255 (white). A digital image can be created by taking a small grid of squares (called pixels) and coloring each pixel with some shade of gray. The resolution of this grid is a measure of how many pixels are used per square inch. An example of a 16 by 16 pixel picture of a flower was shown in \hyperref[x:figure:F_Flower_1]{Figure~{\xreffont\ref{x:figure:F_Flower_1}}}.%
\par
An image can be thought of in several ways: as a two-dimensional array; as one long vector by stringing the columns together one after another; or as a collection of column vectors. For simplicity, we will use the last approach in this project. We call each column vector in a picture a \terminology{signal}. Wavelets are used to process signals. After processing we can apply some technique to compress the processed signals.%
\par
To process a signal we select a family of wavelets. There are many different families of wavelets \textemdash{} which family to use depends on the problem to be addressed. The simplest family of wavelets is the Haar family. More complicated families of wavelets are usually used in applications, but the basic ideas in wavelets can be seen through working with the Haar wavelets, and their relative simplicity will make the details easier to follow. Each family of wavelets has a father wavelet (usually denoted \(\varphi\)) and a mother wavelet (\(\psi\)).%
\par
Wavelets are generated from the mother wavelet by scalings and translations. To further simplify our work we will restrict ourselves to wavelets on [0,1], although this is not necessary. The advantage the wavelets have over other methods of data analysis (Fourier analysis for example) is that with the scalings and translations we are able to analyze both frequency on large intervals and isolate signal discontinuities on very small intervals. The way this is done is by using a large collection (infinite, in fact) of basis functions with which to transform the data. We'll begin by looking at how these basis functions arise.%
\par
If we sample data at various points, we can consider our data to represent a piecewise constant function obtained by partitioning [0,1] into \(n\) equal sized subintervals, where \(n\) represents the number of sample points. For the purposes of this project we will always choose \(n\) to be a power of 2. So we can consider all of our data to represent functions. For us, then, it is natural to look at these functions in the vector space of all functions from \(\R\) to \(\R\). Since our data is piecewise constant, we can really restrict ourselves to a subspace of this larger vector space \textemdash{} subspaces of piecewise constant functions. The most basic piecewise constant function on the interval \([0,1]\) is the one whose value is 1 on the entire interval. We define \(\varphi\) to be this constant function (called the characteristic function of the unit interval). That is%
\begin{equation*}
\varphi(x) = \begin{cases}1 \amp \text{ if }  0 \leq x \lt  1 \\ 0,   \amp  \text{ otherwise. } \end{cases}
\end{equation*}
%
\par
This function \(\varphi\) is the Father Haar wavelet.%
\begin{figureptx}{Graphs of \(\varphi(x)\), \(\varphi(2x)\), and \(\varphi(2x-1)\) from left to right.}{x:figure:F_phi_graphs_1}{}%
\begin{image}{0.15}{0.7}{0.15}%
\includegraphics[width=\linewidth]{external/5_b_Haar_1.pdf}
\end{image}%
\tcblower
\end{figureptx}%
This function \(\varphi\) may seem to be a very simple function but it has properties that will be important to us. One property is that \(\varphi\) satisfies a scaling equation. For example, \hyperref[x:figure:F_phi_graphs_1]{Figure~{\xreffont\ref{x:figure:F_phi_graphs_1}}} shows that%
\begin{equation*}
\varphi(x) = \varphi(2x) + \varphi(2x-1)
\end{equation*}
while \hyperref[x:figure:F_phi_graphs_2]{Figure~{\xreffont\ref{x:figure:F_phi_graphs_2}}} shows that%
\begin{equation*}
\varphi(x) = \varphi(2^2x) + \varphi(2^2x-1) +  \varphi(2^2x-2) + \varphi(2^2x-3)\text{.}
\end{equation*}
%
\begin{figureptx}{Graphs of \(\varphi(2^2x)\), \(\varphi(2^2x-1)\), \(\varphi(2^2x-2)\), and \(\varphi(2^2x-3)\), from left to right.}{x:figure:F_phi_graphs_2}{}%
\begin{image}{0.15}{0.7}{0.15}%
\includegraphics[width=\linewidth]{external/5_b_Haar_2.pdf}
\end{image}%
\tcblower
\end{figureptx}%
So \(\varphi\) is a sum of scalings and translations of itself. In general, for each positive integer \(n\) and integers \(k\) between 0 and \(2^n-1\) we define%
\begin{equation*}
\varphi_{n,k}(x) = \varphi\left(2^nx-k\right)\text{.}
\end{equation*}
%
\par
Then \(\varphi(x) = \sum_{k=0}^{2^{n}-1} \varphi_{n,k}(x)\) for each \(n\).%
\par
These functions \(\varphi_{n,k}\) are useful in that they form a basis for the vector space \(V_n\) of all piecewise constant functions on \([0,1]\) that have possible breaks at the points \(\frac{1}{2^n}\), \(\frac{2}{2^n}\), \(\frac{3}{2^n}\), \(\ldots\), \(\frac{2^n-1}{2^n}\). This is exactly the kind of space in which digital signals live, especially if we sample signals at \(2^n\) evenly spaced points on \([0,1]\). Let \(\CB_n = \{ \varphi_{n,k} : 0 \leq k \leq 2^n-1\}\). You may assume without proof that \(\CB_n\) is a basis of \(V_n\).%
\begin{project}{}{g:project:idp24397064}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Draw the linear combination \(2\varphi_{2,0} - 3\varphi_{2,1} + 17 \varphi_{2,2} + 30 \varphi_{2,3}\). What does this linear combination look like? Explain the statement made previously \textasciigrave{}\textasciigrave{}Notice that these \(2^n\) functions \(\varphi_{n,k}\) form a basis for the vector space of all piecewise constant functions on \([0,1]\) that have possible breaks at the points \(\frac{1}{2^n}\), \(\frac{2}{2^n}\), \(\frac{3}{2^n}\), \(\ldots\), \(\frac{2^n-1}{2^n}\)".%
\item{}Remember that we can consider our data to represent a piecewise constant function obtained by partitioning \([0,1]\) into \(n\) subintervals, where \(n\) represents the number of sample points. Suppose we collect the following data: \(10\), \(13\), \(21\), \(55\), \(3\), \(12\), \(4\), \(18\). Explain how we can use this data to define a piecewise constant function \(f\) on \([0,1]\). Express \(f\) as a linear combination of suitable functions \(\varphi_{n,k}\). Plot this linear combination of \(\varphi_{n,k}\) to verify.%
\end{enumerate}
\end{project}%
Working with functions can be more cumbersome than working with vectors in \(\R^n\), but the digital nature of our data makes it possible to view our piecewise constant functions as vectors in \(\R^n\) for suitable \(n\). More specifically, if \(f\) is an element in \(V_n\), then \(f\) is a piecewise constant function on \([0,1]\) with possible breaks at the points \(\frac{1}{2^n}\), \(\frac{2}{2^n}\), \(\frac{3}{2^n}\), \(\ldots\), \(\frac{2^n-1}{2^n}\). If \(f\) has the value of \(y_i\) on the interval between \(\frac{i-1}{2^n}\) and \(\frac{i}{2^n}\), then we can identify \(f\) with the vector \(\left[y_1 \ y_1 \ \ldots \ y_{2^n}\right]^{\tr}\).%
\begin{project}{}{x:project:act_wavelets_vectors}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Determine the vector in \(\R^8\) that is identified with \(\varphi\).%
\item{}Determine the value of \(m\) and the vectors in \(\R^m\) that are identified with \(\varphi_{2,0}\), \(\varphi_{2,1}\), \(\varphi_{2,2}\), and \(\varphi_{2,3}\).%
\end{enumerate}
\end{project}%
We can use the functions \(\varphi_{n,k}\) to represent digital signals, but to manipulate the data in useful ways we need a different perspective. A different basis for \(V_n\) (a \terminology{wavelet basis}) will allow us to identify the pieces of the data that are most important. We illustrate in the next activity with the spaces \(V_1\) and \(V_2\).%
\begin{project}{}{x:project:act_psi}%
The space \(V_1\) consists of all functions that are piecewise constant on \([0,1]\) with a possible break at \(x=\frac{1}{2}\). The functions \(\varphi = \varphi_{n,k}\) are used to records the values of a signal, and by summing these values we can calculate their average. Wavelets act by averaging and differencing, and so \(\varphi\) does the averaging. We need functions that will perform the differencing.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Define \(\{\psi_{0,0}\}\) as%
\begin{equation*}
\psi_{0,0}(x) = \begin{cases}1 \amp \text{ if }  0 \leq x \lt  \frac{1}{2} \\ -1 \amp \text{ if }  \frac{1}{2} \leq x \lt  1 \\ 0 \amp \text{ otherwise } \end{cases}\text{.}
\end{equation*}
A picture of \(\psi_{0,0}\) is shown in \hyperref[x:figure:F_psi_graphs]{Figure~{\xreffont\ref{x:figure:F_psi_graphs}}}. Since \(\psi_{0,0}\) assumes values of \(1\) and \(-1\), we can use \(\psi_{0,0}\) to perform differencing. The function \(\psi = \psi_{0,0}\) is the Mother Haar wavelet.\footnotemark{} Show that \(\{\varphi, \psi\}\) is a basis for \(V_1\).%
\begin{figureptx}{The graphs of \(\psi_{0,0}\), \(\psi_{1,0}\) and \(\psi_{1,1}\) from left to right.}{x:figure:F_psi_graphs}{}%
\begin{image}{0.15}{0.7}{0.15}%
\includegraphics[width=\linewidth]{external/5_b_Haar_3.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\item{}We continue in a manner similar to the one in which we constructed bases for \(V_n\). For \(k=0\) and \(k=1\), let \(\psi_{1,k} = \psi\left(2^1x-k\right)\). Graphs of \(\psi_{1,0}\) and \(\psi_{1,1}\) are shown in \hyperref[x:figure:F_psi_graphs]{Figure~{\xreffont\ref{x:figure:F_psi_graphs}}}. The functions \(\psi_{1,k}\) assume the values of \(1\) and \(-1\) on smaller intervals, and so can be used to perform differencing on smaller scale than \(\psi_{0,0}\). Show that \(\{\varphi_{0,0}, \psi_{0,0}, \psi_{1,0}, \psi_{1,1}\}\) is a basis for \(V_2\).%
\end{enumerate}
\end{project}%
\footnotetext[59]{The first mention of wavelets appeared in an appendix to the thesis of A. Haar in 1909.\label{g:fn:idp24427656}}%
As \hyperref[x:project:act_psi]{Project Activity~{\xreffont\ref{x:project:act_psi}}} suggests, we can make a basis for \(V_n\) from \(\varphi_{0,0}\) and functions of the form \(\psi_{n,k}\) defined by \(\psi_{n,k}(x) = \psi\left(2^nx-k\right)\) for \(k\) from 0 to \(2^n-1\). More specifically, if we let \(\CS_n = \{\psi_{n,k} : 0 \leq k \leq 2^n-1\}\), then the set%
\begin{equation*}
\CW_n = \{\varphi_{0,0}\} \cup \bigcup_{j=0}^{n-1} \CS_j
\end{equation*}
is a basis for \(V_n^{\perp}\) (we state this without proof). The functions \(\psi_{n,k}\) are the \emph{wavelets}.%
\begin{project}{}{x:project:act_wavelets_differencing}%
We can now write any function in \(V_n\) using the basis \(\CW_n\). As an example, the string 50, 16, 14, 28 represents a piecewise constant function which can be written as \(50 \varphi_{2,0} + 16 \varphi_{2,1} + 14 \varphi_{2,2} + 28 \varphi_{2,3}\), an element in \(V_2\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Specifically identify the functions in \(\CW_0\), \(\CW_1\), and \(\CW_2\), and \(\CW_3\).%
\item{}As mentioned earlier, we can identify a signal, and each wavelet function, with a vector in \(\R^m\) for an appropriate value of \(m\). We can then use this identification to decompose any signal as a linear combination of wavelets. We illustrate this idea with the signal \([50 \ 16 \ 14 \ 28]^{\tr}\) in \(\R^4\). Recall that we can represent this signal as the function \(f = 50 \varphi_{2,0} + 16 \varphi_{2,1} + 14 \varphi_{2,2} + 28 \varphi_{2,3}\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Find the the vectors \(\vw_1\), \(\vw_2\), \(\vw_3\), and \(\vw_4\) in \(\R^m\) that are identified with \(\varphi_{0,0}\), \(\psi_{0,0}\), \(\psi_{1,0}\), and \(\psi_{1,1}\), respectively.%
\item{}Any linear combination \(c_1\varphi_{0,0} + c_2 \psi_{0,0} + c_3 \psi_{1,0} + c_4\psi_{1,1}\) is then identified with the linear combination \(c_1 \vw_1 + c_2 \vw_2 + c_3 \vw_3 + c_4 \vw_4\). Use this idea to find the weights to write the function \(f\) as a linear combination of \(\varphi_{0,0}\), \(\psi_{0,0}\), \(\psi_{1,0}\), and \(\psi_{1,1}\).%
\end{enumerate}
\end{enumerate}
\end{project}%
Although is it not necessarily easy to observe, the weights in the decomposition \(f = 27 \varphi_{0,0} + 6 \psi_{0,0} + 17 \psi_{1,0} - 7 \psi_{1,1}\) are just averages and differences of the original weights in \(f = 50 \varphi_{2,0} + 16 \varphi_{2,1} + 14 \varphi_{2,2} + 28 \varphi_{2,3}\). To see how, notice that if we take the overall average of the original weights we obtain the value of \(27\). If we average the original weights in pairs (\(50\) and \(16\), and \(14\) and \(28\)) we obtain the values \(33\) and \(21\), and if we take average differences of the original weights in pairs (\(50\) and \(16\), and \(14\) and \(28\)) we obtain the values \(17\) and \(-7\). We can treat the signal \([33 \ 21]^{\tr}\) formed from the average of the pairs of the original weights as a smaller copy of the original signal. The average difference of the entries of this new signal is \(6\). So the weights in our final decomposition are obtained by differences between successive averages and certain coefficients. The coefficients in our final decomposition \(27 \varphi_{0,0} + 6 \psi_{0,0} + 17 \psi_{1,0} - 7 \psi_{1,1}\) are called \terminology{wavelet coefficients}. This is the idea that makes wavelets so useful for image compression. In many images, pixels that are near to each other often have similar coloring or shading. These pixels are coded with numbers that are close in value. In the differencing process, these numbers are replaced with numbers that are close to 0. If there is little difference in the shading of the adjacent pixels, the image will be changed only a little if the shadings are made the same. This results in replacing these small wavelet coefficients with zeros. If the processed vectors contain long strings of zeros, the vectors can be significantly compressed.%
\par
Once we have recognized the pattern in expressing our original function as an overall average and wavelet coefficients we can perform these operations more quickly with matrices.%
\begin{project}{}{x:project:act_wavelets_matrices}%
The process of averaging and differencing discussed in and following \hyperref[x:project:act_wavelets_differencing]{Project Activity~{\xreffont\ref{x:project:act_wavelets_differencing}}} can be viewed as a matrix-vector problem. As we saw in \hyperref[x:project:act_wavelets_differencing]{Project Activity~{\xreffont\ref{x:project:act_wavelets_differencing}}}, we can translate the problem of finding wavelet coefficients to the matrix world.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Consider again the problem of finding the wavelet coefficients contained in the vector \([27 \ 6 \ 17 \ - 7]^{\tr}\) for the signal \([50 \ 16 \ 14 \ 28]^{\tr}\). Find the matrix \(A_4\) that has the property that \(A_4 [50 \ 16 \ 14 \ 28]^{\tr} = [27 \ 6 \ 17 \ - 7]^{\tr}\). (You have already done part of this problem in \hyperref[x:project:act_wavelets_differencing]{Project Activity~{\xreffont\ref{x:project:act_wavelets_differencing}}}.) Explain how \(A_4\) performs the averaging and differencing discussed earlier.%
\item{}Repeat the process in part (a) to find the matrix \(A_8\) that converts a signal to its wavelet coefficients.%
\item{}The matrix \(A_i\) is called a \terminology{forward wavelet transformation matrix} and \(A_i^{-1}\) is the \terminology{inverse wavelet transform matrix}. Use \(A_8\) to show that the wavelet coefficients for the data string \([80 \ 48 \ 4 \ 36 \ 28 \ 64 \ 6 \ 50]^{\tr}\) are contained in the vector \([39.5 \ 2.5 \ 22 \ 9 \ 16 \ -16 \ -18 \ -22]^{\tr}\).%
\end{enumerate}
\end{project}%
Now we have all of the necessary background to discuss image compression. Suppose we want to store an image. We partition the image vertically and horizontally and record the color or shade at each grid entry. The grid entries will be our pixels. This gives a matrix, \(M\), of colors, indexed by pixels or horizontal and vertical position. To simplify our examples we will work in gray-scale, where our grid entries are integers between 0 (black) and 255 (white). We can treat each column of our grid as a piecewise constant function. As an example, the image matrix \(M\) that produced the picture at left in \hyperref[x:figure:F_Flower_1]{Figure~{\xreffont\ref{x:figure:F_Flower_1}}} is given in \hyperref[x:men:eq_flower_image_matrix]{({\xreffont\ref{x:men:eq_flower_image_matrix}})}.%
\par
We can then apply a 16 by 16 forward wavelet transformation matrix \(A_{16}\) to \(M\) to convert the columns to averages and wavelet coefficients that will appear in the matrix \(A_{16}M\). These wavelet coefficients allow us to compress the image \textemdash{} that is, create a smaller set of data that contains the essence of the original image.%
\par
Recall that the forward wavelet transformation matrix computes weighted differences of consecutive entries in the columns of the image matrix \(M\). If two entries in \(M\) are close in values, the weighted difference in \(A_{16}M\) will be close to 0. For our example, the matrix \(A_{16}M\) is approximately%
\begin{equation*}
\left[ {\tiny{ \begin{array}{cccccccccccccccc}  208.0\amp  202.0\amp  178.0\amp  165.0\amp 155.0\amp  172.0\amp  118.0\amp  172.0\amp  155.0\amp  153.0\amp  176.0\amp  202.0\amp  208.0\amp  210.0\amp 209.0\amp  208.0\\ 33.4\amp  24.1\amp - 0.625\amp  0.938\amp - 2.50\amp - 5.94\amp  42.8\amp - 5.94\amp - 2.50\amp  12.8\amp  0.938\amp  24.7\amp  30.6\amp  33.4\amp  32.5\amp  31.6 \\- 1.88\amp - 13.8\amp  19.4\amp  2.50\amp  0.0\amp - 2.50\amp  8.12\amp - 2.50 \amp  0.0\amp  2.50\amp  19.4\amp - 13.8\amp  1.88\amp - 3.75\amp - 1.88\amp  0.0\\ 17.5\amp  61.9\amp  61.9\amp  6.88\amp  0.0\amp  61.9\amp  0.0\amp  61.9\amp  0.0\amp  30.6\amp  65.0\amp  66.9\amp 66.9\amp  19.4\amp  66.9\amp  66.9\\ 0.0\amp  27.5\amp  43.8\amp  16.2\amp  0.0 \amp - 11.2\amp  16.2\amp - 11.2\amp  0.0\amp  16.2\amp  43.8\amp  27.5\amp  0.0\amp  0.0\amp  0.0\amp  0.0 \\ 3.75\amp  0.0\amp  27.5\amp - 11.2\amp  0.0\amp - 16.2\amp  22.5\amp - 16.2\amp 0.0\amp - 11.2\amp  27.5\amp  0.0\amp - 3.75\amp - 7.50\amp - 3.75\amp  0.0\\ 47.5\amp  0.0\amp  0.0\amp  13.8\amp  82.5\amp  0.0\amp  0.0\amp  0.0\amp  82.5\amp  13.8\amp  0.0\amp  3.75\amp 3.75\amp  51.2\amp  3.75\amp  3.75\\ 82.5\amp  41.2\amp  41.2\amp  82.5\amp 82.5\amp  41.2\amp  0.0\amp  41.2\amp  82.5\amp  35.0\amp  35.0\amp  35.0\amp  35.0\amp  82.5\amp  35.0\amp  35.0 \\ 0.0\amp  0.0\amp  0.0\amp  55.0\amp - 22.5\amp - 22.5\amp  55.0\amp - 22.5\amp - 22.5\amp  55.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\\ 0.0\amp 55.0\amp - 22.5\amp - 22.5\amp  22.5\amp  0.0\amp - 22.5\amp  0.0\amp  22.5\amp - 22.5\amp - 22.5\amp  55.0\amp 0.0\amp  0.0\amp  0.0\amp  0.0\\- 7.50\amp  0.0\amp - 55.0\amp  22.5\amp  22.5\amp - 22.5\amp  0.0\amp - 22.5\amp  22.5\amp  22.5\amp - 55.0\amp  0.0\amp  7.50\amp  0.0\amp  0.0\amp  0.0 \\ 0.0\amp  0.0\amp  0.0\amp  0.0\amp  22.5\amp - 55.0\amp  0.0\amp - 55.0\amp  22.5 \amp  0.0\amp  0.0\amp  0.0\amp - 15.0\amp  0.0\amp - 7.50\amp  0.0\\ 0.0\amp  0.0\amp 0.0\amp - 55.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp - 55.0\amp  0.0\amp  7.50\amp  7.50\amp  7.50\amp 7.50\amp  7.50\\ 95.0\amp  0.0\amp  0.0\amp - 82.5\amp  0.0\amp  0.0\amp  0.0\amp 0.0\amp  0.0\amp - 82.5\amp  0.0\amp  0.0\amp  0.0\amp  95.0\amp  0.0\amp  0.0\\ 0.0\amp - 82.5\amp  82.5\amp  0.0\amp  0.0\amp - 82.5\amp  0.0\amp - 82.5\amp  0.0\amp  95.0\amp - 95.0\amp  95.0 \amp - 95.0\amp  0.0\amp  95.0\amp - 95.0\\ 0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp 0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0 \end{array}  }} \right]\text{.}
\end{equation*}
%
\par
Note that there are many wavelet coefficients that are quite small compared to others \textemdash{} the ones where the weighted averages are close to 0. In a sense, the weighted differences tell us how much ``detail'' about the whole that each piece of information contains. If a piece of information contains only a small amount of information about the whole, then we shouldn't sacrifice much of the picture if we ignore the small ``detail'' coefficients. One way to ignore the small ``detail'' coefficients is to use \terminology{thresholding}.%
\par
With thresholding (this is \terminology{hard thresholding} or \terminology{keep or kill}), we decide on how much of the detail we want to remove (this is called the \terminology{tolerance}). So we set a tolerance and then replace each entry in our matrix \(A_{16}M\) whose absolute value is below the tolerance with 0 to obtain a new matrix \(M_1\). In our example, if you use a threshold value of 10 we obtain the new matrix \(M_1\):%
\begin{equation*}
\left[ {\tiny{\begin{array}{cccccccccccccccc}  208.0\amp  202.0\amp  178.0\amp  165.0\amp 155.0\amp  172.0\amp  118.0\amp  172.0\amp  155.0\amp  153.0\amp  176.0\amp  202.0\amp  208.0\amp  210.0\amp 209.0\amp  208.0\\ 33.4\amp  24.1\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp  42.8 \amp  0.0\amp  0.0\amp  12.8\amp  0.0\amp  24.7\amp  30.6\amp  33.4\amp  32.5\amp  31.6 \\ 0.0\amp - 13.8\amp  19.4\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp 0.0\amp  19.4\amp - 13.8\amp  0.0\amp  0.0\amp  0.0\amp  0.0\\ 17.5\amp  61.9\amp 61.9\amp  0.0\amp  0.0\amp  61.9\amp  0.0\amp  61.9\amp  0.0\amp  30.6\amp  65.0\amp  66.9\amp  66.9\amp  19.4\amp 66.9\amp  66.9\\ 0.0\amp  27.5\amp  43.8\amp  16.2\amp  0.0\amp - 11.2\amp 16.2\amp - 11.2\amp  0.0\amp  16.2\amp  43.8\amp  27.5\amp  0.0\amp  0.0\amp  0.0\amp  0.0 \\ 0.0\amp  0.0\amp  27.5\amp - 11.2\amp  0.0\amp - 16.2\amp  22.5\amp - 16.2\amp 0.0\amp - 11.2\amp  27.5\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\\ 47.5\amp 0.0\amp  0.0\amp  13.8\amp  82.5\amp  0.0\amp  0.0\amp  0.0\amp  82.5\amp  13.8\amp  0.0\amp  0.0\amp  0.0\amp  51.2\amp 0.0\amp  0.0\\ 82.5\amp  41.2\amp  41.2\amp  82.5\amp  82.5\amp  41.2\amp  0.0\amp 41.2\amp  82.5\amp  35.0\amp  35.0\amp  35.0\amp  35.0\amp  82.5\amp  35.0\amp  35.0 \\ 0.0\amp  0.0\amp  0.0\amp  55.0\amp - 22.5\amp - 22.5\amp  55.0\amp - 22.5\amp - 22.5\amp  55.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\\ 0.0\amp 55.0\amp - 22.5\amp - 22.5\amp  22.5\amp  0.0\amp - 22.5\amp  0.0\amp  22.5\amp - 22.5\amp - 22.5\amp  55.0\amp 0.0\amp  0.0\amp  0.0\amp  0.0\\ 0.0\amp  0.0\amp - 55.0\amp  22.5\amp  22.5\amp - 22.5\amp  0.0\amp - 22.5\amp  22.5\amp  22.5\amp - 55.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0 \\ 0.0\amp  0.0\amp  0.0\amp  0.0\amp  22.5\amp - 55.0\amp  0.0\amp - 55.0\amp  22.5 \amp  0.0\amp  0.0\amp  0.0\amp - 15.0\amp  0.0\amp  0.0\amp  0.0\\ 0.0\amp  0.0\amp 0.0\amp - 55.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp - 55.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp 0.0\\ 95.0\amp  0.0\amp  0.0\amp - 82.5\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp 0.0\amp - 82.5\amp  0.0\amp  0.0\amp  0.0\amp  95.0\amp  0.0\amp  0.0\\ 0.0\amp - 82.5\amp  82.5\amp  0.0\amp  0.0\amp - 82.5\amp  0.0\amp - 82.5\amp  0.0\amp  95.0\amp - 95.0\amp  95.0\amp - 95.0\amp  0.0\amp  95.0\amp - 95.0\\ 0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp 0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0\amp  0.0 \end{array}  }} \right]\text{.}
\end{equation*}
%
\par
We now have introduced many zeros in our matrix. This is where we compress the image. To store the original image, we need to store every pixel. Once we introduce strings of zeros we can identify a new code (say 256) that indicates we have a string of zeros. We can then follow that code with the number of zeros in the string. So if we had a string of 15 zeros in a signal, we could store that information in 2 bytes rather than 15 and obtain significant savings in storage. This process removes some detail from our picture, but only the small detail. To convert back to an image, we just undo the forward processing by multiplying our thresholded matrix \(M_1\) by \(A_{16}^{-1}\). The ultimate goal is to obtain significant compression but still have \(A_{16}^{-1}M_1\) retain all of the essence of the original image.%
\par
In our example using \(M_1\), the reconstructed image matrix is \(A_{16}^{-1}M_1\) (rounded to the nearest whole number) is%
\begin{equation*}
\left[  \begin{array}{cccccccccccccccc}  242\amp  240\amp  241\amp  237\amp 132\amp  138\amp  232\amp  138\amp  132\amp  238\amp  239\amp  240\amp  238\amp  244\amp 242\amp  240\\ 242\amp  240\amp  241\amp  127\amp  178\amp 183\amp  122\amp  183\amp  178\amp  128\amp  239\amp  240\amp  238\amp  244\amp  242\amp 240\\ 242\amp  240\amp  131\amp  127\amp  178\amp  183\amp 122\amp  183\amp  178\amp  128\amp  129\amp  240\amp  238\amp  244\amp  242\amp  240 \\ 242\amp  130\amp  176\amp  172\amp  132\amp  183\amp  167\amp 183\amp  132\amp  172\amp  174\amp  130\amp  238\amp  244\amp  242\amp  240 \\ 242\amp  240\amp  131\amp  177\amp  178\amp  133\amp  183\amp 133\amp  178\amp  178\amp  129\amp  240\amp  238\amp  244\amp  242\amp  240 \\ 242\amp  240\amp  241\amp  132\amp  132\amp  178\amp  183\amp 178\amp  132\amp  132\amp  239\amp  240\amp  238\amp  244\amp  242\amp  240 \\ 242\amp  240\amp  131\amp  177\amp  178\amp  133\amp  138\amp 133\amp  178\amp  178\amp  129\amp  240\amp  223\amp  244\amp  242\amp  240 \\ 242\amp  240\amp  131\amp  177\amp  132\amp  243\amp  138\amp 243\amp  132\amp  178\amp  129\amp  240\amp  253\amp  244\amp  242\amp  240 \\ 240\amp  240\amp  239\amp  124\amp  238\amp  234\amp  75\amp 234\amp  238\amp  130\amp  241\amp  244\amp  244\amp  248\amp  244\amp  244 \\ 240\amp  240\amp  239\amp  234\amp  238\amp  234\amp  75\amp 234\amp  238\amp  240\amp  241\amp  244\amp  244\amp  248\amp  244\amp  244 \\ 240\amp  240\amp  239\amp  69\amp  73\amp  234\amp  75\amp 234\amp  73\amp  75\amp  241\amp  244\amp  244\amp  240\amp  244\amp  244 \\ 50\amp  240\amp  239\amp  234\amp  73\amp  234\amp  75\amp 234\amp  73\amp  240\amp  241\amp  244\amp  244\amp  50\amp  244\amp  244 \\ 240\amp  75\amp  239\amp  248\amp  238\amp  69\amp  75\amp 69\amp  238\amp  240\amp  51\amp  240\amp  50\amp  240\amp  240\amp  50 \\ 240\amp  240\amp  74\amp  248\amp  238\amp  234\amp  75\amp 234\amp  238\amp  50\amp  241\amp  50\amp  240\amp  240\amp  50\amp  240 \\ 75\amp  75\amp  74\amp  83\amp  73\amp  69\amp  75\amp  69\amp 73\amp  75\amp  76\amp  75\amp  75\amp  75\amp  75\amp  75\\ 75\amp  75\amp  74\amp  83\amp  73\amp  69\amp  75\amp  69\amp  73\amp  75\amp  76\amp 75\amp  75\amp  75\amp  75\amp  75 \end{array}   \right]\text{.}
\end{equation*}
%
\par
We convert this into a gray-scale image and obtain the image at right in \hyperref[x:figure:F_Flower_1]{Figure~{\xreffont\ref{x:figure:F_Flower_1}}}. Compare this image to the original at right in \hyperref[x:figure:F_Flower_1]{Figure~{\xreffont\ref{x:figure:F_Flower_1}}}. It is difficult to tell the difference.%
\par
There is a Sage file you can use at \href{http://faculty.gvsu.edu/schlicks/Wavelets_Sage.html}{\nolinkurl{faculty.gvsu.edu/schlicks/Wavelets_Sage.html}} that allows you to create your own 16 by 16 image and process, process your image with the Haar wavelets in \(\R^{16}\), apply thresholding, and reconstruct the compressed image. matrix. You can create your own image, experiment with several different threshold levels, and choose the one that you feel gives the best combination of strings of 0s while reproducing a reasonable copy of the original image.%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 33 The Dimension of a Vector Space}
\typeout{************************************************}
%
\begin{chapterptx}{The Dimension of a Vector Space}{}{The Dimension of a Vector Space}{}{}{x:chapter:chap_dimension}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp24492040}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What is a finite dimensional vector space?%
\item{}What is the dimension of a finite dimensional vector space? What important result about bases of finite dimensional vector spaces makes dimension well-defined?%
\item{}What must be true about any linearly independent subset of \(n\) vectors in a vector space with dimension \(n\)? Why?%
\item{}What must be true about any subset of \(n\) vectors in a vector space with dimension \(n\) that spans the vector space? Why?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Principal Component Analysis}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Principal Component Analysis}{}{Application: Principal Component Analysis}{}{}{x:section:sec_appl_pca}
The discipline of statistics is based on the idea of analyzing data. In large data sets it is usually the case that one wants to understand the relationships between the different data in the set. This can be difficult to do when the data set is large and it is impossible to visually examine the data for patterns. Principal Component Analysis (PCA) is a tool for identifying and representing underlying patterns in large data sets, and PCA has been called one of the most important and valuable linear algebra tools for statistical analysis. PCA is used to transform a collection of variables into a (usually smaller) number of uncorrelated variables called principal components. The principal components form the most meaningful basis from which to view data by removing extraneous information and revealing underlying relationships in the data. This presents a framework for how to reduce a complex data set to a lower dimension while retaining the important attributes of the data set. The output helps the experimenter determine which dynamics in the data are important and which can be ignored.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_dims_intro}
In \hyperref[x:chapter:chap_bases_dimension]{Section~{\xreffont\ref{x:chapter:chap_bases_dimension}}} we learned that any two bases for a subspace of \(\R^n\) contain the same number of vectors. This allowed us to define the dimension of a subspace of \(\R^n\). In this section we extend the arguments we made in \hyperref[x:chapter:chap_bases_dimension]{Section~{\xreffont\ref{x:chapter:chap_bases_dimension}}} to arbitrary vector spaces and define the dimension of a vector space.%
\begin{exploration}{}{x:exploration:pa_5_c}%
The main tool we used to prove that any two bases for a subspace of \(\R^n\) must contain the same number of elements was \hyperref[x:theorem:thm_3_d_1]{Theorem~{\xreffont\ref{x:theorem:thm_3_d_1}}}. In this preview activity we will show that the same argument can be used for vector spaces. More specifically, we will prove a special case of the following theorem generalizing \hyperref[x:theorem:thm_3_d_1]{Theorem~{\xreffont\ref{x:theorem:thm_3_d_1}}}.%
\begin{theorem}{}{}{x:theorem:thm_5_c_1}%
If \(V\) is a vector space with a basis \(\CB = \{\vv_1, \vv_2, \ldots, \vv_k\}\) of \(k\) vectors, then any subset of \(V\) containing more than \(k\) vectors is linearly dependent.%
\end{theorem}
Suppose \(V\) is a vector space with basis \(\CB = \{\vv_1, \vv_2\}\). Consider the set \(U=\{\vu_1, \vu_2, \vu_3\}\) of vectors in \(V\). We will show that \(U\) is linearly dependent using a similar approach to the \hyperref[x:exploration:pa_3_d]{Preview Activity~{\xreffont\ref{x:exploration:pa_3_d}}}.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}What vector equation involving \(\vu_1, \vu_2, \vu_3\) do we need to solve to determine linear independence\slash{}dependence of these vectors? Use \(x_1, x_2, x_3\) for coefficients.%
\item{}Since \(\CB\) is a basis of \(V\), it spans \(V\). Using this information, rewrite the vectors \(\vu_i\) in terms of \(\vv_j\) and substitute into the above equation to obtain another equation in terms of \(\vv_j\).%
\item{}Since \(\CB\) is a basis of \(V\), the vectors \(\vv_1, \vv_2\) are linearly independent. Using the equation in the previous part, determine what this means about the coefficients \(x_1, x_2, x_3\).%
\item{}Express the conditions on \(x_1, x_2, x_3\) in the form of a matrix-vector equation. Explain why there are infinitely many solutions for \(x_i\)'s and why this means the vectors \(\vu_1, \vu_2, \vu_3\) are linearly dependent.%
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Finite Dimensional Vector Spaces}
\typeout{************************************************}
%
\begin{sectionptx}{Finite Dimensional Vector Spaces}{}{Finite Dimensional Vector Spaces}{}{}{x:section:sec_finite_dim_space}
\hyperref[x:theorem:thm_5_c_1]{Theorem~{\xreffont\ref{x:theorem:thm_5_c_1}}} shows that if sets \(\CB_1\) and \(\CB_2\) are finite bases for a vector space \(V\), which are linearly independent by definition, then each cannot contain more elements than the other, so the number of elements in each basis must be equal.%
\begin{theorem}{}{}{x:theorem:thm_5_c_2}%
If a non-trivial vector space \(V\) has a basis of \(n\) vectors, then every basis of \(V\) contains exactly \(n\) vectors.%
\end{theorem}
\hyperref[x:theorem:thm_5_c_2]{Theorem~{\xreffont\ref{x:theorem:thm_5_c_2}}} states that if a vector space \(V\) has a basis with a finite number of vectors, then the number of vectors in a basis for that vector space is a well-defined number. In other words, the number of vectors in a basis is an \terminology{invariant} of the vector space. This important number is given a name.%
\begin{definition}{}{g:definition:idp24544280}%
\index{vector space!finite dimensional}%
\index{vector space!dimension}%
A \terminology{finite-dimensional} vector space is a vector space that can be spanned by a finite number of vectors. The \terminology{dimension} of a non-trivial finite-dimensional vector space is the number of vectors in a basis for \(V\). The dimension of the trivial vector space is defined to be 0.%
\end{definition}
We denote the dimension of a finite dimensional vector space \(V\) by \(\dim(V)\).%
\par
Not every vector space is finite dimensional. We have seen, for example, that the vector space \(\pol\) of all polynomials, regardless of degree, is not a finite-dimensional vector space. In fact, the polynomials%
\begin{equation*}
1, t, t^2, \ldots, t^n, \ldots
\end{equation*}
are linearly independent, so \(\pol\) has an infinite linearly independent set and therefore has no finite basis. A vector space that has an infinite basis is called an \terminology{infinite dimensional} vector space.%
\begin{activity}{}{x:activity:act_5_c_1}%
Since columns of the \(n \times n\) identity matrix span \(\R^n\) and are linearly independent, the columns of \(I_n\) form a basis for \(\R^n\) (the standard basis). Consequently, we have that \(\dim(\R^n) = n\). In this activity we determine the dimensions of other familiar vector spaces. Find the dimensions of each of the indicated vector spaces. Verify your answers.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\pol_1\)%
\item{}\(\pol_2\)%
\item{}\(\pol_n\)%
\item{}\(\M_{2 \times 3}\)%
\item{}\(\M_{3 \times 4}\)%
\item{}\(\M_{k \times n}\)%
\end{enumerate}
\end{activity}%
Finding the dimension of a finite-dimensional vector space amounts to finding a basis for the space.%
\begin{activity}{}{x:activity:act_5_c_2}%
Let \(W = \{(a+b) + (a-b)t + (2a+3b)t^2 \mid a, b \text{ are scalars } \}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find a finite set of polynomials in \(W\) that span \(W\).%
\item{}Determine if the spanning set from part (a) is linearly independent or dependent. Clearly explain your process.%
\item{}What is \(\dim(W)\)? Explain.%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Dimension of a Subspace}
\typeout{************************************************}
%
\begin{sectionptx}{The Dimension of a Subspace}{}{The Dimension of a Subspace}{}{}{x:section:sec_dim_subspace}
Every subspace of a finite-dimensional vector space is a vector space, and since a subspace is contained in a vector space it is natural to think that the dimension of a subspace should be less than or equal to the dimension of the larger vector space. We verify that fact in this section.%
\begin{activity}{}{x:activity:act_5_c_3}%
Let \(V\) be a finite dimensional vector space of dimension \(n\) and let \(W\) be a subspace of \(V\). Explain why \(W\) cannot have dimension larger than \(\dim(V)\), and if \(W \neq V\) then \(\dim(W) \lt \dim(V)\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp24566424}{}\quad{}Use \hyperref[x:theorem:thm_5_c_1]{Theorem~{\xreffont\ref{x:theorem:thm_5_c_1}}}.%
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Conditions for a Basis of a Vector Space}
\typeout{************************************************}
%
\begin{sectionptx}{Conditions for a Basis of a Vector Space}{}{Conditions for a Basis of a Vector Space}{}{}{x:section:sec_cond_basis_vec_space}
There are two items we need to confirm before we can state that a subset \(\CB\) of a subspace \(W\) of a vector space is a basis for \(W\): the set \(\CB\) must be linearly independent and span \(W\). We can reduce the amount of work it takes to show that a set is a basis if we know the dimension of the vector space in advance.%
\begin{activity}{}{x:activity:act_5_c_5}%
Let \(W\) be a subspace of a vector space \(V\) with \(\dim(W) = k\). We know that every basis of \(W\) contains exactly \(k\) vectors.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Suppose that \(S\) is a subset of \(W\) that contains \(k\) vectors and is linearly independent. In this part of the activity we will show that \(S\) must span \(W\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Suppose that \(S\) does not span \(W\). Explain why this implies that \(W\) contains a set of \(k+1\) linearly independent vectors.%
\item{}Explain why the result of i tells us that \(S\) is a basis for \(W\).%
\end{enumerate}
\item{}Now suppose that \(S\) is a subset of \(W\) with \(k\) vectors that spans \(W\). In this part of the activity we will show that \(S\) must be linearly independent.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Suppose that \(S\) is not linearly independent. Explain why we can then find a proper subset of \(S\) that is linearly independent but has the same span as \(S\).%
\item{}Explain why the result of i tells us that \(S\) is a basis for \(W\).%
\end{enumerate}
\end{enumerate}
\end{activity}%
The result of \hyperref[x:activity:act_5_c_5]{Activity~{\xreffont\ref{x:activity:act_5_c_5}}} is summarized in the following theorem (compare to \hyperref[x:theorem:thm_3_d_basis_properties]{Theorem~{\xreffont\ref{x:theorem:thm_3_d_basis_properties}}}).%
\begin{theorem}{}{}{g:theorem:idp24587544}%
Let \(W\) be a subspace of dimension \(k\) of a vector space \(V\) and let \(S\) be a subset of \(W\) containing exactly \(k\) vectors.%
\begin{enumerate}
\item{}If \(S\) is linearly independent, then \(S\) is a basis for \(W\).%
\item{}If \(S\) spans \(W\), then \(S\) is a basis for \(W\).%
\end{enumerate}
%
\end{theorem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_dims_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp24594968}%
Find a basis and dimension for each of the indicated subspaces of the given vector spaces.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\{a+b(t+t^2) : a, b \in \R\}\) in \(\pol_2\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp24592792}{}\quad{}Let \(W = \{a+b(t+t^2) : a, b \in \R\}\). Every element in \(W\) has the form%
\begin{equation*}
a+b(t+t^2) = a(1) + b(t+t^2)\text{.}
\end{equation*}
So \(W = \Span\{1, t+t^2\}\). Since neither \(1\) nor \(t+t^2\) is a scalar multiple of the other, the set \(\{1, t+t^2\}\) is linearly independent. Thus, \(\{1, t+t^2\}\) is a basis for \(W\) and \(\dim(W) = 2\).%
\item{}\(\Span\left\{1, \frac{1}{1+x^2}, \frac{2+x^2}{1+x^2}, \arctan(x)\right\}\) in \(\F\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp24602904}{}\quad{}Let \(W = \Span\left\{1, \frac{1}{1+x^2}, \frac{2+x^2}{1+x^2}, \arctan(x)\right\}\). To find a basis for \(W\), we find a linearly independent subset of \(\left\{1, \frac{1}{1+x^2}, \frac{2+x^2}{1+x^2}, \arctan(x)\right\}\). Consider the equation%
\begin{equation*}
c_1 (1) + c_2\left(\frac{1}{1+x^2}\right) + c_3\left(\frac{2+x^2}{1+x^2}\right) + c_4 \arctan(x) = 0\text{.}
\end{equation*}
To find the weights \(c_i\) for which this equality of functions holds, we use the fact that the we must have equality for every \(x\). So we pick four different values for \(x\) to obtain a linear system that we can solve for the weights. Evaluating both sides of the equation at \(x=0\), \(x=1\), \(x=-1\), and \(x=2\) yields the equations%
\begin{align*}
{}c_1   \amp {}+{}   \amp {}c_2    \amp {}+{}  \amp {2}c_3  \amp {}  \amp {}    \amp = \ \amp 0\amp {}\\
{}c_1   \amp {}+{}   \amp {\frac{1}{2}}c_2  \amp {}+{}    \amp {\frac{3}{2}}c_3    \amp {}+{}  \amp {\frac{\pi}{4}}c_4    \amp = \ \amp 0\amp {}\\
{}c_1   \amp {}+{}   \amp {\frac{1}{2}}c_2  \amp {}+{}    \amp {\frac{3}{2}}c_3    \amp {}-{}  \amp {\frac{\pi}{4}}c_4    \amp = \ \amp 0\amp {}\\
{}c_1   \amp {}+{}   \amp {\frac{1}{5}}c_2  \amp {}+{}    \amp {\frac{6}{5}}c_3    \amp {}+{}  \amp {\arctan(2)}c_4    \amp = \ \amp 0\amp {.}
\end{align*}
The reduced row echelon form of the coefficient matrix%
\begin{equation*}
\left[  \begin{array}{cccr} 1\amp 1\amp 2\amp 0 \\ 1\amp \frac{1}{2}\amp \frac{3}{2}\amp \frac{\pi}{4} \\ 1\amp \frac{1}{2}\amp \frac{3}{2}\amp -\frac{\pi}{4} \\ 1\amp \frac{1}{5}\amp \frac{6}{5}\amp \arctan(2) \end{array}  \right]
\end{equation*}
is \(\left[ \begin{array}{cccc} 1\amp 0\amp 1\amp 0 \\ 0\amp 1\amp 1\amp 0 \\ 0\amp 0\amp 0\amp 1 \\ 0\amp 0\amp 0\amp 0 \end{array}  \right]\). The general solution to this linear system is \(c_1 = c_2 = -c_3\) and \(c_4 = 0\). Notice that%
\begin{equation*}
(-1)(1) + (-1)\left( \frac{1}{1+x^2}\right) + \frac{2+x^2}{1+x^2} = 0
\end{equation*}
or%
\begin{equation*}
\frac{2+x^2}{1+x^2} = 1 + \left( \frac{1}{1+x^2}\right)\text{,}
\end{equation*}
so \(\frac{2+x^2}{1+x^2}\) is a linear combination of the other vectors. The vectors corresponding to the pivot columns are linearly independent, so it follows that \(1\), \(\frac{1}{1+x^2}\), and \(\arctan(x)\) are linearly independent. We conclude that \(\left\{1, \frac{1}{1+x^2},  \arctan(x)\right\}\) is a basis for \(W\) and \(\dim(W) = 3\).%
\item{}\(\{p(t) \in \pol_n : p(-t) = p(t)\}\) in \(\pol_4\) (The polynomials with the property that \(p(-t) = p(t)\) are called \emph{even} polynomials.)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp24611864}{}\quad{}Let \(W = \{p(t) \in \pol_n : p(-t) = p(t)\}\) in \(\pol_4\). Let \(p(t) \in W\) and suppose that \(p(t) = a_0 + a_1t+a_2t^2+a_3t^3+a_4t^4\). Since \(p(-t)=p(t)\), we must have \(p(1) = p(-1)\) and \(p(3) = p(-3)\). Since \(p(1) = p(-1)\), it follows that%
\begin{equation*}
a_0+a_1+a_2+a_3+a_4 = a_0-a_1+a_2-a_3+a_4
\end{equation*}
or%
\begin{equation*}
a_1+a_3 = 0\text{.}
\end{equation*}
Similarly, the fact that \(p(3) = p(-3)\) yields the equation%
\begin{equation*}
a_0+3a_1+9a_2+27a_3+81a_4 = a_0-3a_1+9a_2-27a_3+81a_4
\end{equation*}
or%
\begin{equation*}
a_1+9a_3 = 0\text{.}
\end{equation*}
The reduced row echelon form of the coefficient matrix of system \(a_1+a_3 = 0\) and \(a_1+9a_3 = 0\) is \(I_2\), so it follows that \(a_1=a_3=0\). Thus, \(p(t) = a_0 + a_2t^2+a_4t^4\) and so \(W = \Span\{1, t^2, t^4\}\). Equating like terms in the equation%
\begin{equation*}
c_1(1) + c_2(t^2) + c_3(t^4) = 0
\end{equation*}
yields \(c_1=c_2=c_3=0\). We conclude that \(\{1,t^2,t^4\}\) is linearly independent and is therefore a basis for \(W\). Thus, \(\dim(W) = 3\). As an alternative solution, notice that \(p(t) = t\) is not in \(W\). So \(W \neq V\) and we know that \(\dim(W) \lt  \dim(V)\). Since \(1\), \(t^2\), and \(t^4\) are in \(W\), we can show as above that \(1\), \(t^2\), and \(t^4\) are linearly independent. We can conclude that \(\dim(W)=3\) since it cannot be \(4\).%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp24626712}%
Let \(U_{n \times n}\) be the set of all \(n \times n\) upper triangular matrices. Recall that a matrix \(A = [a_{ij}]\) is upper triangular if \(a_{ij} = 0\) whenever \(i > j\). That is, a matrix is upper triangular if all entries below the diagonal are 0.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(U_{n \times n}\) is a subspace of \(\M_{n \times n}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp24632088}{}\quad{}Since the \(n \times n\) zero matrix \(0_{n \times n}\) has all entries equal to 0, it follows that \(0_{n \times n}\) is in \(U_{n \times n}\). Let \(A = [a_{ij}]\) and \(B = [b_{ij}]\) be in \(U_{n \times n}\), and let \(C = [c_{ij}] = A+B\). Then \(c_{ij} = a_{ij} + b_{ij} = 0 + 0\) when \(i > j\). So \(C\) is an upper triangular matrix and \(U_{n \times n}\) is closed under addition. Let \(c\) be a scalar. The \(ij\)th entry of \(cA\) is \(ca_{ij} = c(0) = 0\) whenever \(i > j\). So \(cA\) is an upper triangular matrix and \(U_{n \times n}\) is closed under multiplication by scalars. We conclude that \(U_{n \times n}\) is a subspace of \(\M_{n \times n}\).%
\item{}Find the dimensions of \(U_{2 \times 2}\) and \(U_{3 \times 3}\). Explain. Make a conjecture as to what \(\dim(U_{n \times n})\) is in terms of \(n\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp24643480}{}\quad{}Let \(M_{11} = \left[ \begin{array}{cc} 1\amp 0 \\ 0\amp 0 \end{array}  \right]\), \(M_{12} = \left[ \begin{array}{cc} 0\amp 1 \\ 0\amp 0 \end{array}  \right]\), and \(M_{22}=\left[ \begin{array}{cc} 0\amp 0 \\ 0\amp 1 \end{array}  \right]\). We will show that \(S = \{M_{11}, M_{12}, M_{22}\}\) is a basis for \(U_{2 \times 2}\). Consider the equation%
\begin{equation*}
x_1M_{11} + x_2M_{12} + x_3 M_{22} = 0\text{.}
\end{equation*}
Equating like entries shows that \(x_1 = x_2 = x_3 = 0\), and so \(S\) is linearly independent. If \(\left[ \begin{array}{cc} a\amp b \\ 0\amp c \end{array}  \right]\) is in \(U_{2 \times 2}\), then%
\begin{equation*}
\left[ \begin{array}{cc} a\amp b \\ 0\amp c \end{array}  \right] = aM_{11} + bM_{12} + cM_{22}
\end{equation*}
and so \(S\) spans \(U_{2 \times 2}\). Thus, \(S\) is a basis for \(U_{2 \times 2}\) and so \(\dim(U_{2 \times 2}) = 3\). Similarly, for the \(3 \times 3\) case let \(M_{ij}\) for \(i \leq j\) be the \(3 \times 3\) matrix with a 1 in the \(ij\) position and 0 in every other position. Let%
\begin{equation*}
S = \{M_{11}, M_{12}, M_{13}, M_{22}, M_{23}, M_{33}\}\text{.}
\end{equation*}
Equating corresponding entries shows that if%
\begin{equation*}
x_1M_{11} +  x_2M_{12} + x_3M_{13} + x_4 M_{22}+ x_5 M_{23} + x_6M_{33} = 0\text{,}
\end{equation*}
then \(x_1 = x_2 = x_3 = x_4 = x_5 = x_6 = 0\). So \(S\) is a linearly independent set. If \(A = [a_{ij}]\) is in \(U_{3 \times 3}\), then \(A = \sum_{i \geq j} a_{ij}M_{ij}\) and \(S\) spans \(U_{3 \times 3}\). We conclude that \(S\) is a basis for \(U_{3 \times 3}\) and \(\dim(U_{3 \times 3}) = 6\). In general, for an \(n \times n\) matrix, the set of matrices \(M_{ij}\), one for each entry on and above the diagonal, is a basis for \(U_{n \times n}\). There are \(n\) such matrices for the entries on the diagonal. The number of entries above the diagonal is equal to half the total number of entries (\(n^2\)) minus half the number of entries on the diagonal (\(n\)). So there is a total of \(\frac{n^2-n}{2}\) such matrices for the entries above the diagonal. Therefore,%
\begin{equation*}
\dim(U_{n \times n}) = n+\frac{n^2-n}{2} = \frac{n^2+n}{2}\text{.}
\end{equation*}
%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_dims_summ}
%
\begin{itemize}[label=\textbullet]
\item{}A finite dimensional vector space is a vector space that can be spanned by a finite set of vectors.%
\item{}We showed that any two bases for a finite dimensional vector space \emph{must} contain the same number of vectors. Therefore, we can define the \terminology{dimension} of a finite dimensional vector space \(V\) to be the number of vectors in any basis for \(V\).%
\item{}If \(V\) is a vector space with dimension \(n\) and \(S\) is any linearly independent subset of \(V\) with \(n\) vectors, then \(S\) is a basis for \(V\). Otherwise, we could add vectors to \(S\) to make a basis for \(V\) and then \(V\) would have a basis of more than \(n\) vectors.%
\item{}If \(V\) is a vector space with dimension \(n\) and \(S\) is any subset of \(V\) with \(n\) vectors that spans \(V\), then \(S\) is a basis for \(V\). Otherwise, we could remove vectors from \(S\) to obtain a basis for \(V\) and then \(V\) would have a basis of fewer than \(n\) vectors.%
\item{}For any finite dimensional space \(V\) and a subspace \(W\) of \(V\), \(\dim(W)\leq \dim(V)\).%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_dims_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp24673432}%
Let \(W = \Span\{ 1+t^2, 2+t+2t^2+t^3, 1+t+t^3, t-t^2+t^3\}\) in \(\pol_3\). Find a basis for \(W\). What is the dimension of \(W\)?%
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp24672408}%
Let \(A = \left[ \begin{array}{cc} 1\amp 2\\1\amp 0 \end{array} \right]\) and \(B = \left[ \begin{array}{cr} 1\amp 0\\1\amp -1 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Are \(A\) and \(B\) linearly independent or dependent? Verify your result.%
\item{}Extend the set \(S = \{A,B\}\) to a basis for \(\M_{2 \times 2}\). That is, find a basis for \(\M_{2 \times 2}\) that contains both \(A\) and \(B\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp24684312}%
Let \(A = \left[ \begin{array}{ccc} 1\amp 2\amp 0 \\ 3\amp 0\amp 2 \end{array} \right]\), \(B = \left[ \begin{array}{rcc} -1\amp 1\amp 1 \\ 2\amp 4\amp 0 \end{array} \right]\), \(C= \left[ \begin{array}{crr} 5\amp 1\amp -3 \\ 0\amp -12\amp 4 \end{array} \right]\), \(D = \left[ \begin{array}{crr} 5\amp 4\amp -2 \\ 5\amp -8\amp 6 \end{array} \right]\), and \(E = \left[ \begin{array}{rcc} 2\amp 0\amp 0 \\ -2\amp 0\amp 1 \end{array} \right]\) in \(\M_{2 \times 3}\) and let \(S = \left\{ A, B, C, D, E \right\}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Is \(S\) a basis for \(\M_{2 \times 3}\)? Explain.%
\item{}Determine if \(S\) is a linearly independent or dependent set. Verify your result.%
\item{}Find a basis \(\CB\) for \(\Span \ S\) that is a subset of \(S\) and write all of the vectors in \(S\) as linear combinations of the vectors in \(\CB\).%
\item{}Extend your basis \(\CB\) from part (c) to a basis \(\M_{2 \times 3}\). Explain your method.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp24697112}%
Determine the dimension of each of the following vector spaces.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\Span\{2, 1+t, t^2\}\) in \(\pol_2\)%
\item{}The space of all polynomials in \(\pol_3\) whose constant terms is 0.%
\item{}\(\Nul \left[ \begin{array}{cc} 1\amp 2 \\ 2\amp 4 \end{array} \right]\)%
\item{}\(\Span\left\{ [1 \ 2 \ 0 \ 1 \ -1]^{\tr}, [0 \ 1 \ 1 \ 1 \ 0]^{\tr}, [0 \ 3 \ 2 \ 3 \ 0]^{\tr}, [-1 \ 0 \ 1 \ 1 \ 1]^{\tr}\right\}\) in \(\R^5\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp24709272}%
Let \(W\) be the set of matrices in \(\M_{2 \times 2}\) whose diagonal entries sum to 0. Show that \(W\) is a subspace of \(\M_{2 \times 2}\), find a basis for \(W\), and then find \(\dim(W)\).%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp24709144}%
Show that if \(W\) is a subspace of a finite dimensional vector space \(V\), then any basis of \(W\) can be extended to a basis of \(V\).%
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp24712472}%
Let \(W\) be the set of all polynomials \(a+bt+ct^2\) in \(\pol_2\) such that \(a+b+c = 0\). Show that \(W\) is a subspace of \(\pol_2\), find a basis for \(W\), and then find \(\dim(W)\).%
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp24717592}%
Suppose \(W_1, W_2\) are subspaces in a finite-dimensional space \(V\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that it is not true in general that \(\dim(W_1)+\dim(W_2) \leq \dim(V)\).%
\item{}Are there any conditions on \(W_1\) and \(W_2\) that will ensure that \(\dim(W_1)+\dim(W_2) \leq \dim(V)\)?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp24721688}{}\quad{}See \hyperlink{x:exercise:problem_disjoint_subspaces}{problem~{\xreffont 12}} in the previous section.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp24728600}%
Suppose \(W_1 \subseteq W_2\) are two subspaces of a finite-dimensional space. Show that if \(\dim(W_1)=\dim(W_2)\), then \(W_1=W_2\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp24721816}{}\quad{}Consider dimensions.%
\end{divisionexercise}%
\begin{divisionexercise}{10}{}{}{g:exercise:idp24727192}%
Suppose \(W_1, W_2\) are both three-dimensional subspaces inside \(\R^4\). In this exercise we will show that \(W_1\cap W_2\) contains a plane. Let \(\{\vu_1, \vu_2, \vu_3\}\) be a basis for \(W_1\) and let \(\{\vv_1, \vv_2, \vv_3\}\) be a basis for \(W_2\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}If \(\vv_1\), \(\vv_2\), and \(\vv_3\) are all in \(W_1\), explain why \(W_1 \cap W_2\) must contain a plane.%
\item{}Now we consider the case where not all of \(\vv_1\), \(\vv_2\), and \(\vv_3\) are in \(W_1\). Since the arguments will be the same, let us assume that \(\vv_1\) is not in \(W_1\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Explain why the set \(\CS = \{\vu_1, \vu_2, \vu_3, \vv_1\}\) is a basis for \(\R^4\).%
\item{}Explain why \(\vv_2\) and \(\vv_3\) can be written as linear combinations of the vectors in \(\CS\). Use these linear combinations to find two vectors that are in \(W_1 \cap W_2\). Then show that these vectors span a plane in \(W_1 \cap W_2\).%
\end{enumerate}
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{11}{}{}{g:exercise:idp24736920}%
A magic matrix is an \(n \times n\) matrix in which the sum of the entries along any row, column, or diagonal is the same. For example, the \(3 \times 3\) matrix%
\begin{equation*}
\left[ \begin{array}{rcc} 5\amp 4\amp 0 \\ -2\amp 3\amp 8 \\ 6\amp 2\amp 1 \end{array}  \right]
\end{equation*}
is a magic matrix. Note that the entries of a magic matrix can be any real numbers.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that the set of \(n \times n\) magic matrices is a subspace of \(\M_{n \times n}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp24739608}{}\quad{}Consider the row, column, and diagonal sums.%
\item{}Let \(V\) be the space of \(3 \times 3\) magic matrices. Find a basis for \(V\) and determine the dimension of \(V\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{12}{}{}{g:exercise:idp24742552}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
The dimension of a finite dimensional vector space is the minimum number of vectors needed to span that space.%
\item{}\lititle{True\slash{}False.}\par%
The dimension of a finite dimensional vector space is the maximum number of linearly independent vectors that can exist in that space.%
\item{}\lititle{True\slash{}False.}\par%
If \(n\) vectors span an \(n\)-dimensional vector space \(V\), then these vectors form a basis of \(V\).%
\item{}\lititle{True\slash{}False.}\par%
Any set of \(n\) vectors form a basis in an \(n\)-dimensional vector space.%
\item{}\lititle{True\slash{}False.}\par%
Every vector in a vector space \(V\) spans a one-dimensional subspace of \(V\).%
\item{}\lititle{True\slash{}False.}\par%
Any set of \(n\) linearly independent vectors in a vector space \(V\) of dimensional \(n\) is a basis for \(V\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\{\vv_1, \vv_2, \ldots, \vv_k\}\) is linearly independent in \(V\), then \(\dim(V)\geq k\).%
\item{}\lititle{True\slash{}False.}\par%
If a set of \(k\) vectors span \(V\), then any set of more than \(k\) vectors in \(V\) is linearly dependent.%
\item{}\lititle{True\slash{}False.}\par%
If an infinite set of vectors span \(V\), then \(V\) is infinite-dimensional.%
\item{}\lititle{True\slash{}False.}\par%
If \(W_1, W_2\) are both two-dimensional subspaces of a three dimensional vector space \(V\), then \(W_1\cap W_2\neq \{\vzero \}\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\dim(V)=n\) and \(W\) is a subspace of \(V\) with dimension \(n\), then \(W=V\).%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Understanding Principal Component Analysis}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Understanding Principal Component Analysis}{}{Project: Understanding Principal Component Analysis}{}{}{x:section:sec_proj_pca}
Suppose we were to film an experiment involving a ball that is bouncing up and down. Naively, we set up several cameras to follow the process of the experiment from different perspectives and collect the data. All of this data tells us something about the bouncing ball, but there may be no perspective that tells us an important piece of information \textemdash{} the axis along which the ball bounces. The question, then, is how we can extract from the data this important piece of information. Principal Component Analysis (PCA) is a tool for just this type of analysis.%
\begin{tableptx}{\textbf{SAT data}}{x:table:T_PCA_SAT_2}{}%
\centering%
{\tabularfont%
\begin{tabular}{lcccccccccc}
State&\(1\)&\(2\)&\(3\)&\(4\)&\(5\)&\(6\)&\(7\)&\(8\)&\(9\)&\(10\)\tabularnewline\hrulethin
EBRW&\(595\)&\(540\)&\(522\)&\(508\)&\(565\)&\(512\)&\(643\)&\(574\)&\(534\)&\(539\)\tabularnewline[0pt]
Math&\(571\)&\(536\)&\(493\)&\(493\)&\(554\)&\(501\)&\(655\)&\(566\)&\(534\)&\(539\)
\end{tabular}
}%
\end{tableptx}%
We will use an example to illustrate important concepts we will need. To realistically apply PCA we will have much more data than this, but for now we will restrict ourselves to only two variables so that we can visualize our results. \hyperref[x:table:T_PCA_SAT_2]{Table~{\xreffont\ref{x:table:T_PCA_SAT_2}}} presents information from ten states on two attributes related to the SAT \textemdash{} Evidence-Based Reading and Writing (EBRW) score and Math score. The SAT is made up of three sections: Reading, Writing and Language (also just called Writing), and Math. The The EBRW score is calculated by combining the Reading and Writing section scores \textemdash{} both the Math and EBRW are scored on a scale of 200-800.%
\par
Each attribute (Math, EBRW score) creates a vector whose entries are the student responses for that attribute. The data provides the average scores from participating students in each state. In this example we have two attribute vectors:%
\begin{align*}
\vx_1 \amp = [595 \ 540 \ 522 \ 508 \ 565 \ 512 \ 643 \ 574 \ 534 \ 539]^{\tr} \text{ and }\\
\vx_2 \amp = [571 \ 536 \ 493 \ 493 \ 554 \ 501 \ 655 \ 566 \ 534 \ 539]^{\tr}\text{.}
\end{align*}
%
\par
These vectors form the rows of a \(2 \times 10\) matrix%
\begin{equation*}
X_0 = \left[ \begin{array}{c} \vx_1^{\tr} \\ \vx_2^{\tr} \end{array}  \right] = \left[ \begin{array}{cccccccccc}  595\amp 540\amp 522\amp 508\amp 565\amp 512\amp 643\amp 574\amp 534\amp 539 \\ 571\amp 536\amp 493\amp 493\amp 554\amp 501\amp 655\amp 566\amp 534\amp 539 \end{array}  \right]
\end{equation*}
that makes up our data set. A plot of the data is shown at left in \hyperref[x:figure:F_PCA_data_plot]{Figure~{\xreffont\ref{x:figure:F_PCA_data_plot}}}, where the EBRW score is along the horizontal axis and the math score is along the vertical axis.%
\begin{figureptx}{Two views of the data set (EBRW horizontal, math vertical).}{x:figure:F_PCA_data_plot}{}%
\begin{image}{0.25}{0.5}{0.25}%
\includegraphics[width=\linewidth]{external/PCA_data.pdf}
\end{image}%
\tcblower
\end{figureptx}%
The question we want to answer is, how do we represent our data set so that the most important features in the data set are revealed?%
\begin{project}{}{x:project:act_PCA_centering}%
Before analyzing a data set there is often some initial preparation that needs to be made. Issues that have to be dealt with include the problem that attributes might have different units and scales. For example, in a data set with attributes about people, height could be measured in inches while weight is measured in pounds. It is difficult to compare variables when they are on different scales. Another issue to consider is that some attributes are independent of the others (height, for example does not depend on hair color), while some are interrelated (body mass index depends on height and weight). To simplify our work, we will not address these type of problems. The only preparation we will do with our data is to center it.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}An important piece of information about a one-dimensional data set \(\vx = [x_1 \ x_2 \ x_3 \ \cdots \ x_n]^{\tr}\)  is the sample average or mean%
\begin{equation*}
\overline{\vx} = \sum_{i=1}^n x_i\text{.}
\end{equation*}
Calculate the means \(\overline{\vx_1}\) and \(\overline{\vx_2}\) for our SAT data from the matrix \(X_0\).%
\item{}We can use these sample means to center our data at the origin by translating the data so that each column of our data matrix has mean \(0\). We do this by subtracting the mean for that row vector from each component of the vector. Determine the matrix \(X\) that contains the centered data for our SAT data set from matrix \(X_0\).%
\end{enumerate}
\end{project}%
A plot of the centered data for our SAT data is shown at right in \hyperref[x:figure:F_PCA_data_plot]{Figure~{\xreffont\ref{x:figure:F_PCA_data_plot}}}. Later we will see why centering the data is useful \textemdash{} it will more easily allow us to project onto subspaces. The goal of PCA is to find a matrix \(P\) so that \(PX = Y\), and \(Y\) is suitably transformed to identify the important aspects of the data. We will discuss what the important aspects are shortly. Before we do so, we need to discuss a way to compare the one dimensional data vectors \(\vx_1\) and \(\vx_2\).%
\begin{project}{}{x:project:act_PCA_covariance}%
To compare the two one dimensional data vectors, we need to consider variance and covariance.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}It is often useful to know how spread out a data set is, something the average doesn't tell us. For example, the data sets \([1 \ 2 \ 3]^{\tr}\) and \([-2 \ 0 \ 8]^{\tr}\) both have averages of \(2\), but the data in \([-2 \ 0 \ 8]^{\tr}\) is more spread out. Variance provides one measure of how spread out a one-dimensional data set \(\vx = [x_1 \ x_2 \ x_3 \ \cdots \ x_n]^{\tr}\) is. Variance is defined as%
\begin{equation*}
\var(\vx) = \frac{1}{n-1} \sum_{i=1}^{n} \left(x_i - \overline{\vx} \right)^2\text{.}
\end{equation*}
The variance provides a measure of how far from the average the data is spread.\footnotemark{}  Determine the variances of the two data vectors \(\vx_1\) and \(\vx_2\). Which is more spread out?%
\item{}In general, we will have more than one-dimensional data, as in our SAT data set. It will be helpful to have a way to compare one-dimensional data sets to try to capture the idea of variance for different data sets \textemdash{} how much the data in two different data sets varies from the mean with respect to each other. One such measure is covariance \textemdash{} essentially the average of all corresponding products of deviations from the means. We define the covariance of two data vectors \(\vx = [x_1 \ x_2 \ \cdots \ x_n]^{\tr}\) and \(\vy = [y_1 \ y_2 \ \cdots \ y_n]^{\tr}\) as%
\begin{equation*}
\cov(\vx,\vy) = \frac{1}{n-1} \sum_{i=1}^{n} \left(x_i-\overline{\vx}\right)\left(y_i-\overline{\vy}\right)\text{.}
\end{equation*}
Determine all covariances%
\begin{equation*}
\cov(\vx_1,\vx_1), \  \cov(\vx_1,\vx_2), \  \cov(\vx_2,\vx_1), \ \text{ and }  \ \cov(\vx_2,\vx_2)\text{.}
\end{equation*}
How are \(\cov(\vx_1,\vx_2)\) and \(\cov(\vx_2,\vx_1)\) related? How are  \(\cov(\vx_1,\vx_1)\) and \(\cov(\vx_2,\vx_2)\) related to variances?%
\item{}What is most important about covariance is its sign. Suppose \(\vy = [y_1 \ y_2 \ \ldots \ y_n]^{\tr}\), \(\vz = [z_1 \ z_2 \ \ldots \ z_n]^{\tr}\) and \(\cov(\vy,\vz) > 0\). Then if \(y_i\) is larger than \(y_j\) it is likely that \(z_i\) is also larger than \(z_j\). For example, if \(\vy\) is a vector that records a persons height from age \(2\) to \(10\) and \(\vz\) records that same person's weight in the same years, we might expect that when \(y_i\) increases so does \(z_i\). Similarly, if \(\cov(\vy,\vz) \lt 0\), then as one data set increases, the other decreases. As an example, if \(\vy\) records the number of hours a student spends playing video games each semester ad \(\vz\) gives the student's GPA for each semester, then we might expect that \(z_i\) decreases as \(y_i\) increases. When \(\cov(\vy,\vz) = 0\), then \(\vy\) and \(\vz\) are said to be uncorrelated or independent of each other. For our example \(\vx_1\) and \(\vx_2\), what does \(\cov(\vx_1,\vx_2)\) tell us about the relationship between \(\vx_1\) and \(\vx_2\)? Why should we expect this from the context?%
\item{}The covariance gives us information about the relationships between the attributes. So instead of working with the original data, we work with the covariance data. If we have \(m\) data vectors \(\vx_1\), \(\vx_2\), \(\ldots\), \(\vx_m\) in \(\R^n\), the \terminology{covariance matrix} \(C = [c_{ij}]\) satisfies \(c_{ij} = \cov(\vx_i, \vx_j)\). Calculate the covariance matrix for our SAT data. Then explain why \(C = \frac{1}{n-1}XX^{\tr}\).%
\end{enumerate}
\end{project}%
\footnotetext[60]{It might seem that we should divide by \(n\) instead of \(n-1\) in the variance, but it is generally accepted to do this for reasons we won't get into. Suffice it to say that if we are using a sample of the entire population, then dividing by \(n-1\) provides a variance whose square root is closer to the standard deviation than we would get if we divide by \(n\). If we are calculating the variance of an entire population, then we would divide by \(n\).\label{g:fn:idp24795416}}%
Recall that the goal of PCA is to find a matrix \(P\) such that \(PX = Y\) where \(P\) transforms the data set to a coordinate system in which the important aspects of the data are revealed. We are now in a position to discuss what that means.%
\par
An ideal view of our data would be one in which we can see the direction of greatest variance and one that minimizes redundancy. With redundant data the variables are not independent \textemdash{} that is, covariance is nonzero. So we would like the covariances to all be zero (or as close to zero as possible) to remove redundancy in our data. That means that we would like a covariance matrix in which the non-diagonal entries are all zero. This will be possible if the covariance matrix is diagonalizable.%
\begin{project}{}{x:project:act_PCA_diagonalize}%
Consider the covariance matrix \(C = \left[ \begin{array}{cc} 1760.18\amp 1967.62\\ 1967.62\amp 2319.29 \end{array} \right]\). Explain why we can find a matrix \(P\) with determinant 1 whose columns are unit vectors that diagonalizes \(C\). Then find such a matrix. Use technology as appropriate.%
\end{project}%
For our purposes, we want to diagonalize \(XX^{\tr}\) with \(PXX^{\tr}P^{-1}\), so the matrix \(P\) that serve our purposes is the one whose \emph{rows} are the eigenvectors of \(XX^{\tr}\). To understand why this matrix is the one we want, recall that we want to have \(PX = Y\), and we want to diagonalize \(XX^{\tr}\) to a diagonal covariance matrix \(YY^{\tr}\). In this situation we will have (recalling that \(P^{-1}=P^{\tr}\))%
\begin{equation*}
\frac{1}{n-1}YY^{\tr} = \frac{1}{n-1}(PX)(PX)^{\tr} = \frac{1}{n-1}P\left(XX^{\tr}\right)P^{\tr} = P\left(XX^{\tr}\right)P^{-1}\text{.}
\end{equation*}
%
\par
So the matrix \(P\) that we want is exactly the one that diagonalizes \(XX^{\tr}\).%
\begin{project}{}{x:project:act_PCA_max_min}%
There are two useful ways we can interpret the results of our work so far. An eigenvector of \(XX^{\tr}\) that corresponds to the largest (also called the \emph{dominant}) eigenvalue \(\lambda_1\) is \([0.66 \ 0.76]^{\tr}\). A plot of the centered data along with the eigenspace \(E_{\lambda_1}\) of \(XX^{\tr}\) spanned by \(\vv_1 = [0.66 \ 0.76]^{\tr}\) is shown at left in \hyperref[x:figure:F_PCA_pc]{Figure~{\xreffont\ref{x:figure:F_PCA_pc}}}. The eigenvector \(\vv_1\) is called the \terminology{first principal component} of \(X\). Notice that this line \(E_{\lambda_1}\) indicates the direction of greatest variation in the data. That is, the sum of the squares of the differences between the data points and the mean is as large as possible in this direction. In other words, when we project the data points onto \(E_{\lambda_1}\), as shown at right in \hyperref[x:figure:F_PCA_pc]{Figure~{\xreffont\ref{x:figure:F_PCA_pc}}}, the variation of the resulting points is larger than it is for any other line. In other words, the data is most spread out in this direction.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}There is another way we can interpret this result. If we drop a perpendicular from one of our data points to the space \(E_{\lambda_1}\) it creates a right triangle with sides of length \(a\), \(b\), and \(c\) as illustrated in the middle of \hyperref[x:figure:F_PCA_pc]{Figure~{\xreffont\ref{x:figure:F_PCA_pc}}}. Use this idea to explain why maximizing the variation also minimizes the sum of the squares of the distances from the data points to this line. As a result, we have projected our two-dimensional data onto the one-dimensional space that maximizes the variance of the data.%
\begin{figureptx}{The principal component.}{x:figure:F_PCA_pc}{}%
\begin{image}{0.15}{0.7}{0.15}%
\includegraphics[width=\linewidth]{external/PCA_spread.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\item{}Recall that the matrix (to two decimal places) \(P = \left[ \begin{array}{rr} - 0.66\amp -0.76\\ 0.76\amp 0.66 \end{array} \right]\) transforms the data set \(X\) to a new data set \(Y=PX\) whose covariance matrix is diagonal. Explain how the \(x\)-axis is related to the transformed data set \(Y\).%
\begin{figureptx}{Applying \(P\).}{x:figure:F_PCA_P}{}%
\begin{image}{0.15}{0.7}{0.15}%
\includegraphics[width=\linewidth]{external/PCA_P.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\end{enumerate}
\end{project}%
The result of \hyperref[x:project:act_PCA_max_min]{Project Activity~{\xreffont\ref{x:project:act_PCA_max_min}}} is that we have reduced the problem from considering the data in a two-dimensional space to a one-dimensional space \(E_{\lambda_1}\) where the most important aspect of the data is revealed. Of course, we eliminate some of the characteristics of the data, but the most important aspect is still included and highlighted. This is one of the important uses of PCA, data dimension reduction, which allows us to reveal key relationships between the variables that might not be evident when looking at a large dataset.%
\par
The second eigenvector of \(XX^{\tr}\) also has meaning. A picture of the eigenspace \(E_{\lambda_2}\) corresponding to the smaller eigenvector \(\lambda_2\) of \(XX^{\tr}\) is shown in \hyperref[x:figure:F_PCA_second_pc]{Figure~{\xreffont\ref{x:figure:F_PCA_second_pc}}}. The second eigenvector of \(XX^{\tr}\) is orthogonal to the first, and the direction of the second eigenvector tells us the direction of the second most amount of variance as can be seen in \hyperref[x:figure:F_PCA_second_pc]{Figure~{\xreffont\ref{x:figure:F_PCA_second_pc}}}.%
\begin{figureptx}{The second principal component.}{x:figure:F_PCA_second_pc}{}%
\begin{image}{0.25}{0.5}{0.25}%
\includegraphics[width=\linewidth]{external/PCA_second.pdf}
\end{image}%
\tcblower
\end{figureptx}%
To summarize, the unit eigenvector for the largest eigenvalue of \(XX^{\tr}\) indicates the direction in which the data has the greatest variance. The direction of the unit eigenvector for the smaller eigenvalue shows the direction in which the data has the second largest variance. This direction is also perpendicular to the first (indicating \(0\) covariance). The directions of the eigenvectors are called the \terminology{principal components} of \(X\). The eigenvector with the highest eigenvalue is the first principal component of the data set and the other eigenvectors are ordered by the eigenvalues, highest to lowest. The principal components provide a new coordinate system from which to view our data \textemdash{} one in which we can see the maximum variance and in which there is zero covariance.%
\begin{project}{}{x:project:act_PCA_variances}%
We can use the eigenvalues of \(XX^{\tr}\) to quantify the amount of variance that is accounted for by our projections. Notice that the points along the \(x\)-axis at right in \hyperref[x:figure:F_PCA_P]{Figure~{\xreffont\ref{x:figure:F_PCA_P}}} are exactly the numbers in the first row of \(Y\). These numbers provide the projections of the data in \(Y\) onto the \(x\)-axis \textemdash{} the axis along which the data has its greatest variance.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Calculate the variance of the data given by the first row of \(Y\). This is the variance of the data i the direction of the eigenspace \(E_{\lambda_1}\). How does the result compare to entries of the covariance matrix for \(Y\).%
\item{}Repeat part (a) for the data along the second row of \(Y\).%
\item{}The total variance of the data set is the sum of the variances. Explain why the amount of variance in the data that is accounted for in the direction of \(E_{\lambda_1}\) is%
\begin{equation*}
\frac{\lambda_1}{\lambda_1 + \lambda_2}\text{.}
\end{equation*}
Then calculate this amount for the SAT data.%
\end{enumerate}
\end{project}%
In general, PCA is most useful for larger data sets. The process is the same.%
\begin{itemize}[label=\textbullet]
\item{}Start with a set of data that forms the rows of an \(m \times n\) matrix. We center the data by subtracting the mean of each row from the entries of that row to create a centered data set in a matrix \(X\).%
\item{}The principal components of \(X\) are the eigenvectors of \(XX^{\tr}\), ordered so that they correspond to the eigenvalues of \(XX^{\tr}\) in decreasing order.%
\item{}Let \(P\) be the matrix whose rows are the principal components of \(X\), ordered from highest to lowest. Then \(Y = PX\) is suitably transformed to identify the important aspects of the data.%
\item{}If \(\lambda_1\), \(\lambda_2\), \(\ldots\), \(\lambda_n\) are the eigenvalues of \(XX^{\tr}\) in decreasing order, then the amount of variance in the data accounted for by the first \(r\) principal components is given by%
\begin{equation*}
\frac{\lambda_1+\lambda_2 + \cdots + \lambda_r}{\lambda_1+\lambda_2 + \cdots + \lambda_n}\text{.}
\end{equation*}
%
\item{}The first \(r\) rows of \(Y=PX\) provide the projection of the data set \(X\) onto an \(r\)-dimensional space spanned by the first \(r\) principal components of \(X\).%
\end{itemize}
%
\begin{project}{}{x:project:act_PCA_4D_SAT}%
Let us now consider a problem with more than two variables. We continue to keep the data set small so that we can conveniently operate with it. \hyperref[x:table:T_PCA_SAT]{Table~{\xreffont\ref{x:table:T_PCA_SAT}}} presents additional information from ten states on four attributes related to the SAT \textemdash{} Participation rates, Evidence-Based Reading and Writing (EBRW) score, Math score, and average SAT score. Use technology as appropriate for this activity.%
\begin{tableptx}{\textbf{SAT data}}{x:table:T_PCA_SAT}{}%
\centering%
{\tabularfont%
\begin{tabular}{lllllllllll}
State&\(1\)&\(2\)&\(3\)&\(4\)&\(5\)&\(6\)&\(7\)&\(8\)&\(9\)&\(10\)\tabularnewline\hrulethin
Rate&\(6\)&\(60\)&\(97\)&\(100\)&\(64\)&\(99\)&\(4\)&\(23\)&\(79\)&\(70\)\tabularnewline[0pt]
EBRW&\(595\)&\(540\)&\(522\)&\(508\)&\(565\)&\(512\)&\(643\)&\(574\)&\(534\)&\(539\)\tabularnewline[0pt]
Math&\(571\)&\(536\)&\(493\)&\(493\)&\(554\)&\(501\)&\(655\)&\(566\)&\(534\)&\(539\)\tabularnewline[0pt]
SAT&\(1166\)&\(1076\)&\(1014\)&\(1001\)&\(1120\)&\(1013\)&\(1296\)&\(1140\)&\(1068\)&\(1086\)
\end{tabular}
}%
\end{tableptx}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Determine the centered data matrix \(X\) for this data set.%
\item{}Find the covariance matrix for this data set. Round to four decimal places.%
\item{}Find the principal components of \(X\). Include at least four decimal places accuracy.%
\item{}How much variation is accounted for in the data by the first principal component? In other words, if we reduce this data to one dimension, how much of the variation do we retain? Explain.%
\item{}How much variation is accounted for in the data by the first two principal components? In other words, if we reduce this data to two dimensions, how much of the variation do we retain? Explain.%
\end{enumerate}
\end{project}%
We conclude with a comment. A reasonable question to ask is how we interpret the principal components. Let \(P\) be an orthogonal matrix such that \(PCP^{\tr}\) is the diagonal matrix with the eigenvalues of \(C\) along the diagonal, in decreasing order. We then have the new perspective \(Y = PX\) from which to view the data. The first principal component \(\vp_1\) (the first row of \(P\)) determines the new variable \(\vy_1 = [y_i]\) (the first row of \(Y\)) in the following manner. Let \(\vp_1 = [p_i]^{\tr}\) and let \(\vc_i\) represent the columns of \(X\) so that \(X = [\vc_1 \ \vc_2 \ \vc_3 \ \cdots \ \vc_{20}]\). Recognizing that%
\begin{equation*}
PX =  \left[  \begin{array}{c} \vp_1^{\tr} \\ \vp_2^{\tr} \\ \vp_3^{\tr} \\ \vp_4^{\tr} \\\vp_5^{\tr} \\ \vp_6^{\tr} \end{array}  \right]  [\vc_1 \ \vc_2 \ \vc_3 \ \cdots \ \vc_{20}]\text{,}
\end{equation*}
we have that%
\begin{equation*}
y_i = \vp_1^{\tr} \vc_i\text{.}
\end{equation*}
%
\par
That is,%
\begin{equation*}
\vy_1 = [\vp_1^{\tr}\vc_1 \ \vp_1^{\tr} \vc_2 \ \vp_1^{\tr} \vc_3 \ \cdots \ \vp_{1}^{\tr} \vc_{20}]\text{.}
\end{equation*}
%
\par
So each \(y_i\) is a linear combination of the original variables (contained in the \(\vc_i\)) with weights from the first principal component. The other new variables are obtained in the same way from the remaining principal components. So even though the principal components may not have an easy interpretation in context, they are connected to the original data in this way. By reducing the data to a few important principal components \textemdash{} that is, visualizing the data in a subspace of small dimension \textemdash{} we can account for almost all of the variation in the data and relate that information back to the original data.%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 34 Coordinate Vectors and Coordinate Transformations}
\typeout{************************************************}
%
\begin{chapterptx}{Coordinate Vectors and Coordinate Transformations}{}{Coordinate Vectors and Coordinate Transformations}{}{}{x:chapter:chap_coordinate_vectors_vector_spaces}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp24914056}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}How do we find the coordinate vector of a vector \(\vx\) with respect to a basis \(\CB = \{\vv_1, \vv_2, \ldots, \vv_n\}\)?%
\item{}How can we visualize coordinate systems?%
\item{}How do we identify a vector space of dimension \(n\) with \(\R^n\)?%
\item{}What properties does the coordinate transformation \(\vx \mapsto [\vx]_{\CB}\) have?%
\item{}How are the coordinates of a vector with respect to a basis related to its coordinates with respect to the standard basis?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Calculating Sums}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Calculating Sums}{}{Application: Calculating Sums}{}{}{x:section:sec_appl_sums}
Consider the problem of calculating sums of powers of integers. For example,%
\begin{align*}
\sum_{t=0}^{n-1} t \amp = \frac{1}{2}\left(n^2-n\right)\\
\sum_{t=0}^{n-1} t^2 \amp = \frac{1}{6}\left(2n^3-3n^2+1\right)\\
\sum_{t=0}^{n-1} t^3 \amp = \frac{1}{4}\left(n^4-2n^3+n^2\right)
\end{align*}
and so on. It is possible to prove these formulas if we have an idea of what the formula is, but how can one come up with these formulas in the first place? As we will see later in this section, we can make use of coordinate vectors to find and verify such formulas.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_coor_vec_intro}
In \hyperref[x:chapter:chap_coordinate_vectors]{Section~{\xreffont\ref{x:chapter:chap_coordinate_vectors}}} we defined the coordinate vector of a vector \(\vx\) in \(\R^n\) with respect to a basis \(\CB = \vv_1, \vv_2, \vv_n\}\) of \(\R^n\) to be the vector \([x_1 \ x_2 \ \ldots \ x_n]^{\tr}\) if%
\begin{equation*}
\vx = x_1 \vv_1 + x_2 \vv_2 + \cdots + x_n \vv_n\text{.}
\end{equation*}
%
\par
We can define coordinate vectors in vector spaces in the same way. As we will see, this is a powerful idea as it allows us to translate problems in finite dimensional vector spaces to problems in \(\R^n\) where we have many tools at our disposal. First we define a coordinate vector.%
\begin{definition}{}{g:definition:idp24926728}%
\index{coordinate vector with respect to a basis}%
\index{coordinates with respect to a basis}%
Let \(\CB = \{\vv_1, \vv_2, \ldots, \vv_n\}\) be an ordered basis for a vector space \(V\). For any vector \(\vx\) in \(V\), the \terminology{coordinate vector} of \(\vx\) with respect to \(\CB\) is the vector%
\begin{equation*}
[\vx]_{\CB} = \left[ \begin{array}{c} x_1 \\ x_2 \\ \vdots \\ x_n \end{array}  \right]\text{,}
\end{equation*}
where%
\begin{equation*}
\vx = x_1 \vv_1 + x_2 \vv_2 + \cdots + x_n \vv_n\text{.}
\end{equation*}
%
\par
The scalars \(x_1\), \(x_2\), \(\ldots\), \(x_n\) are the \terminology{coordinates of the vector} \(\vx\) \terminology{with respect to the basis}.%
\end{definition}
\begin{exploration}{}{x:exploration:pa_5_d}%
Let \(b_1(t) = 4+2t\), \(b_2(t) = -6+6t\), \(c_1(t) = 1+t\), \(c_2(t) = 1-t\), and let \(\CB = \{b_1(t), b_2(t)\}\) and \(\CC = \{c_1(t),
c_2(t)\}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(\CB\) and \(\CC\) are bases for \(\pol_1\).%
\item{}Let \(p(t) = 3 b_1(t) + 2 b_2(t)\). What is \([p(t)]_{\CB}\)?%
\item{}Find the coordinate vector of \(f(t) = 2+4t\) with respect to both \(\CB\) and \(\CC\).%
\item{}Find a polynomial \(r(t)\) such that \([r(t)]_{\CB} = [1 \ 3]^{\tr}\). Determine exactly how many such polynomials \(s(t)\) have the property that \([s(t)]_{\CB} = [1 \ 3]^{\tr}\). Explain your reasoning.%
\item{}Find a polynomial \(q(t)\) such that \([q(t)]_{\CC} = [1 \ 3]^{\tr}\). Determine exactly how many such polynomials \(s(t)\) have the property that \([s(t)]_{\CC} = [1 \ 3]^{\tr}\). Explain your reasoning.%
\item{}What specific property does a basis have that ensures the responses to parts (c) and (d) will always be the same?%
\end{enumerate}
\end{exploration}%
Recall that there is exactly one way to write a vector as a linear combination of basis vectors, so there is only one coordinate vector of a given vector with respect to a basis. Therefore, the coordinate vector of any vector is well-defined.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Coordinate Transformation}
\typeout{************************************************}
%
\begin{sectionptx}{The Coordinate Transformation}{}{The Coordinate Transformation}{}{}{x:section:sec_coord_trans}
\index{coordinate transformation}%
We have seen that both \(\R^2\) and \(\pol_1\) are vector spaces of dimension 2. Moreover, we can pair the polynomial \(a_0+a_1t\) in \(\pol_1\) with the vector \([a_0 \ a_1]\) in \(\R^2\) as the coordinate vector of \(a_0+a_1t\) with respect to the standard basis for \(\pol_1\). In this way we can see that the vector space \(\pol_1\) ``looks'' like the vector space \(\R^2\). In fact, using this same idea, we can show that any vector space of dimension \(n\) ``looks'' like \(\R^n\). The key idea to make this work is the coordinate transformation.%
\par
As we saw in \hyperref[x:exploration:pa_5_d]{Preview Activity~{\xreffont\ref{x:exploration:pa_5_d}}}, the coordinate vector of a vector with respect to a basis depends on the basis we choose. In this activity we found that \([2+4t]_{\CB} = \left[ 1 \ \frac{1}{3} \right]^{\tr}\) and \([2+4t]_{\CC} = [ 3 \ -1]^{\tr}\), where \(\CB = \{4+2t, -6+6t\}\) and \(\CC = \{1+t, 1-t\}\). These coordinate vectors allow us to identify polynomials in \(\pol_1\) with vectors in \(\R^2\). In this way, the coordinate vectors provide an identification between the vector space \(\pol_1\) and the vector space \(\R^2\) via a \terminology{coordinate transformation}.%
\begin{definition}{}{g:definition:idp24965128}%
\index{coordinate transformation}%
Let \(V\) be a vector space of dimension \(n\) with basis \(\CB\). The \terminology{coordinate transformation} \(T\) from \(V\) to \(\R^n\) with respect to the ordered basis \(\CB\) is the mapping defined by%
\begin{equation*}
T(\vx) = [\vx]_{\CB}
\end{equation*}
for any vector \(\vx\) in \(V\).%
\end{definition}
Coordinate transformations have several useful properties that allow us to use them to compare the structure of vector spaces.%
\begin{activity}{}{x:activity:act_5_d_2}%
Let \(a(t) = 8+2t\) and \(b(t) = -5+t\) in \(\pol_1\). Suppose we know \(\CC = \{1 + t, 2 - t\}\) is a basis of \(\pol_1\). Let \(T(\vx)=[\vx]_\CC\) for \(\vx\) in \(\pol_1\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}What are \(T(a(t))\) and \(T(b(t))\)?%
\item{}Find \(T(a(t)) + T(b(t))\).%
\item{}What is \(T(a(t)+b(t))= [a(t)+b(t)]_{\CC}\)?%
\item{}What is the relationship between \(T(a(t)) + T(b(t))\) and \(T(a(t)+b(t))\)?%
\item{}Show that if \(c\) is any scalar, then \(T(ca(t)) = cT(a(t))\).%
\item{}Where have we seen functions with these properties before?%
\end{enumerate}
\end{activity}%
\hyperref[x:activity:act_5_d_2]{Activity~{\xreffont\ref{x:activity:act_5_d_2}}} shows that coordinate transformations behave in ways similar to matrix transformations. Recall that if \(A\) is an \(m \times n\) matrix, then%
\begin{equation*}
A(\vu+\vw) = A\vu + A\vw \ \text{ and }  \ A(c\vu) = cA\vu
\end{equation*}
for any vectors \(\vu\) and \(\vw\) in \(\R^n\) and any scalar \(c\). Because of these properties, the transformation \(A\) preserves linear combinations. That is, if \(\vv_1\), \(\vv_2\), \(\ldots\), and \(\vv_n\) are any vectors in \(\R^n\) and \(c_1\), \(c_2\), \(\ldots\), \(c_n\) are any scalars, then%
\begin{equation*}
A(c_1 \vv_1 + c_2 \vv_2 + \cdots + c_n \vv_n) = c_1 A\vv_1 + c_2 A\vv_2 + \cdots + c_n A\vv_n\text{.}
\end{equation*}
%
\par
\hyperref[x:activity:act_5_d_2]{Activity~{\xreffont\ref{x:activity:act_5_d_2}}} contains the essential ideas to show that if \(T\) is a coordinate transformation from a vector space \(V\) with basis \(\CB\) to \(\R^n\), then%
\begin{equation*}
T(\vu+\vw) = T(\vu) + T(\vw) \ \text{ and }  \ T(c\vu) = cT(\vu)
\end{equation*}
for any vectors \(\vu\) and \(\vw\) in \(V\) and any scalar \(c\). The fact that any coordinate transformation satisfies these properties is contained in the following theorem.%
\begin{theorem}{}{}{x:theorem:thm_5_d_5}%
If a vector space \(V\) has a basis \(\CB\) of \(n\) vectors, then the coordinate mapping \(T : V \to \R^n\) defined by \(T(\vx) = [\vx]_{\CB}\) satisfies%
\begin{enumerate}
\item{}\(T(\vu + \vw) = T(\vu) + T(\vw)\) and%
\item{}\(\displaystyle T(c\vu) = cT(\vu)\)%
\end{enumerate}
%
\par
for any vectors \(\vu\) and \(\vw\) in \(V\) and any scalar \(c\).%
\end{theorem}
\begin{proof}{}{g:proof:idp24995848}
Let \(\vv_1\), \(\vv_2\), \(\ldots\), and \(\vv_n\) be vectors that form a basis \(\CB\) for a vector space \(V\). To verify the first property, let \(\vu\) and \(\vw\) be two arbitrary vectors from \(V\). We will show that \(T(\vu)+T(\vw) = T(\vu + \vw)\) for these two vectors. Consider \(T(\vu)\). If \(T(\vu)=[u_1 \ u_2 \ \ldots \ u_n]^{\tr}\), then the fact that \(T(\vu)= [\vu]_\CB\) implies that%
\begin{equation*}
\vu = u_1\vv_1 + u_2\vv_2 + \cdots + u_n \vv_n \,\text{.}
\end{equation*}
%
\par
Similarly, if \(T(\vw)=[w_1 \ w_2 \ \ldots \ w_n]^{\tr}\), then \(T(\vw)=[\vw]_\CB\) implies that%
\begin{equation*}
\vw = w_1\vv_1 + w_2\vv_2 + \cdots + w_n \vv_n \,\text{.}
\end{equation*}
%
\par
We then obtain%
\begin{align*}
\vu+\vw \amp = (u_1\vv_1 + u_2\vv_2 + \cdots + u_n \vv_n) + (w_1\vv_1 + w_2\vv_2 + \cdots + w_n \vv_n)\\
\amp = (u_1+w_1)\vv_1 + (u_2+w_2)\vv_2 + \cdots + (u_n+w_n) \vv_n\text{.}
\end{align*}
%
\par
Thus, by definition of \(T\) again,%
\begin{equation*}
T(\vu+\vw)= [\vu+\vw]_{\CB} = [(u_1+w_1) \ (u_2+w_2) \ \ldots \ (u_n+w_n)]^{\tr}\text{.}
\end{equation*}
%
\par
To show that \(T(\vu + \vw) = T(\vu) + T(\vw)\), note that%
\begin{align*}
T(\vu) + T(\vw) \amp = [u_1 \ u_2 \ \ldots \ u_n]^{\tr} + [w_1 \ w_2 \ \ldots \ w_n]^{\tr}\\
\amp = [(u_1+w_1) \ (u_2+w_2) \  \ldots \ (u_n+w_n)]^{\tr}\\
\amp = [\vu+\vw]_{\CB}\\
\amp = T(\vu+\vw)\text{.}
\end{align*}
%
\par
The proof of the second property is left for the exercises.%
\end{proof}
\hyperref[x:theorem:thm_5_d_5]{Theorem~{\xreffont\ref{x:theorem:thm_5_d_5}}} can be extended to any linear combination of vectors by mathematical induction. That is, if \(T : V \to \R^n\) is the coordinate mapping defined by \(T(\vx) = [\vx]_{\CB}\) for some basis \(\CB\), and if \(\vu_1\), \(\vu_2\), \(\ldots\), \(\vu_m\) are vectors in an \(n\)-dimensional vector space \(V\) with basis \(\CB\), and \(x_1\), \(x_2\), \(\ldots\), \(x_m\) are scalars, then \(T(x_1\vu_1+x_2\vu_2+\cdots +x_m\vu_m) = x_1T(\vu_1) + x_2T(\vu_2) + \cdots + x_mT(\vu_m)\). In other words,%
\begin{equation*}
[x_1\vu_1+x_2\vu_2+\cdots + x_m\vu_m]_{\CB}  =  x_1[\vu_1]_{\CB} +  x_2[\vu_2]_{\CB} + \cdots +  x_m[\vu_m]_{\CB}\text{.}
\end{equation*}
%
\par
In other words, coordinate transformations take linear combinations of vectors in \(V\) to linear combinations of vectors in \(\R^n\).%
\par
In essence, a coordinate transformation makes an identification of any vector space of dimension \(n\) with \(\R^n\). One of the driving forces behind this identification is the fact that, just as in \(\R^n\), if a vector space \(V\) has a basis of \(n\) elements, then any basis for \(V\) will have exactly \(n\) elements. The fact that any coordinate transformation from a vector space \(V\) to \(\R^n\) is linear means that elements in \(V\) behave the same way with respect to addition and multiplication by scalars in \(V\) as do their images in \(\R^n\) under the coordinate transformation. To make this identification complete, there are still two questions to address. First, given a coordinate transformation \(T\) from a vector space \(V\) to \(\R^n\), is is possible for two vectors in \(V\) to have the same image in \(\R^n\) (in other words, is \(T\) one-to-one), and is every vector in \(\R^n\) the image of some vector in \(V\) under \(T\) (in other words, does \(T\) map \(V\) onto \(\R^n\))? If the coordinate transformations are one-to-one and onto, then, in essence, any vector space of dimension \(n\) is just a copy of \(\R^n\). As a result, to understand any vector space with a basis of \(n\) vectors, it is enough to understand \(\R^n\).%
\begin{activity}{}{x:activity:act_5_d_6}%
Let \(V\) be a vector space with an ordered basis \(\CB = \{\vv_1, \vv_2, \ldots, \vv_n\}\). Then \(T\) maps \(V\) into \(\R^n\). We want to show that \(T\) maps \(V\) onto \(\R^n\). Recall that a function \(f\) from a set \(X\) to a set \(Y\) is onto if for any element \(y\) in \(Y\), there is an element \(x\) in \(X\) such that \(f(x) = y\). Let \(\vb = [b_1 \ b_2 \ \ldots \ b_n]^{\tr}\) be a vector in \(\R^n\). Must there be a vector \(\vv\) in \(V\) so that \(T(\vv) = \vb\)? If so, find such a vector. If not, explain why not.%
\end{activity}%
\hyperref[x:activity:act_5_d_6]{Activity~{\xreffont\ref{x:activity:act_5_d_6}}} shows that a coordinate transformation maps an \(n\)-dimensional vector space \(V\) onto \(\R^n\). What's more, any coordinate transformation is also one-to-one (the proof that \(T\) is one-to-one is left for the exercises). We summarize these results in the following theorem.%
\begin{theorem}{}{}{x:theorem:thm_5_d_6}%
If a vector space \(V\) has a basis \(\CB\) of \(n\) vectors, then the coordinate mapping \(T : V \to \R^n\) defined by \(T(\vx) = [\vx]_{\CB}\) is both one-to-one and onto.%
\end{theorem}
\hyperref[x:theorem:thm_5_d_5]{Theorem~{\xreffont\ref{x:theorem:thm_5_d_5}}} and \hyperref[x:theorem:thm_5_d_6]{Theorem~{\xreffont\ref{x:theorem:thm_5_d_6}}} show that a coordinate transformation from an \(n\)-dimensional vector space \(V\) with basis \(\CB\) to \(\R^n\) provides an identification of \(V\) with \(\R^n\), where a vector \(\vv\) in \(V\) is identified with the vector \([\vv]_{\CB}\). This identification allows us to transfer questions (linear dependence, independence, span) in \(V\) to \(\R^n\) where we can apply our knowledge of matrices to answer the questions.%
\begin{activity}{}{x:activity:act_5_d_7}%
Let \(V = \pol_3\) and let \(\CB = \{1,t,t^2,t^3\}\). Let \(S = \{1+t+t^2+t^3, t-t^3, 1+2t^2, 1+5t-t^3\}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find each of \([1+t+t^2+t^3]_{\CB}\), \([t-t^3]_{\CB}\), \([1+2t^2]_{\CB}\), and \([1+5t-t^3]_{\CB}\).%
\item{}Are the vectors \([1+t+t^2+t^3]_{\CB}\), \([t-t^3]_{\CB}\), \([1+2t^2]_{\CB}\), and \([1+5t-t^3]_{\CB}\) linearly independent or dependent? Explain. If the vectors are linearly dependent, write one of the vectors as a linear combination of the others.%
\item{}The coordinate transformation identifies the vectors in \(S = \{1+t+t^2+t^3, t-t^3, 1+2t^2, 1+5t-t^3\}\) with their coordinate vectors in \(\R^4\). Use that information to determine if \(S\) is a linearly independent or dependent set. If dependent, write one of the vectors in \(S\) as a linear combination of the others.%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_coord_vec_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp25054088}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the coordinate vector of \(\vv\) with respect to the ordered basis \(\CB\) in the indicated vector space.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}\(\CB = \{1+t, 2-t\}\) in \(\pol_1\) with \(\vv = 4+t\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp25060360}{}\quad{}Find the coordinate vector of \(\vv\) with respect to the ordered basis \(\CB\) in the indicated vector space.%
\par
We need to write \(\vv =4+t\) as a linear combination of \(1+t\) and \(2-t\). If \(4+t = c_1(1+t) + c_2(2-t)\), then equating coefficients of like power terms yields the equations \(4 = c_1 +2c_2\) and \(1 = c_1-c_2\). The solution to this system is \(c_1 = 2\) and \(c_2 = 1\), so \([4+t]_{\CB} = [2 \ 1]^{\tr}\).%
\item{}\(\CB = \left\{ \left[ \begin{array}{cc} 1\amp 0\\0\amp 1 \end{array} \right], \left[ \begin{array}{rc} 1\amp 0\\-1\amp 1 \end{array} \right], \left[ \begin{array}{cc} 1\amp 1\\0\amp 1 \end{array} \right], \left[ \begin{array}{cc} 0\amp 0\\0\amp 1 \end{array} \right] \right\}\) in \(\M_{2 \times 2}\) with \(\vv = \left[ \begin{array}{cc} 2\amp 3\\1\amp 0 \end{array} \right]\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp25065480}{}\quad{}Find the coordinate vector of \(\vv\) with respect to the ordered basis \(\CB\) in the indicated vector space.%
\par
We need to write \(\vv\) as a linear combination of the vectors in \(\CB\). If%
\begin{align*}
\left[ \begin{array}{cc} 2\amp 3\\
1\amp 0 \end{array} \right] \amp = c_1\left[ \begin{array}{cc} 1\amp 0\\
0\amp 1 \end{array} \right] + c_2 \left[ \begin{array}{rc} 1\amp 0\\
-1\amp 1 \end{array} \right]\\
\amp \qquad + c_3 \left[ \begin{array}{cc} 1\amp 1\\
0\amp 1 \end{array} \right] + c_4 \left[ \begin{array}{cc} 0\amp 0\\
0\amp 1 \end{array} \right]\text{,}
\end{align*}
equating corresponding components produces the system%
\begin{alignat*}{6}
{}c_1   \amp {}+{}  \amp {}c_2  \amp {}+{}  \amp {}c_3  \amp {}{}    \amp {}    \amp {}={}  \amp \ {}\amp 2\amp {}\\
{}    \amp {}{}    \amp {}    \amp {}{}    \amp {}c_3  \amp {}{}    \amp {}    \amp {}={}   \amp \ {}\amp 3\amp {}\\
{}    \amp {}{}    \amp {-}c_2  \amp {}{}    \amp {}    \amp {}{}    \amp {}    \amp {}={}   \amp \ {}\amp 1\amp {}\\
{}c_1    \amp {}+{}  \amp {}c_2  \amp {}+{}  \amp {}c_3  \amp {}+{}  \amp {}c_4  \amp {}={}  \amp  \ {}\amp 0\amp {.}
\end{alignat*}
The solution to this system is \(c_1 = 0\), \(c_2 = -1\), \(c_3 = 3\), and \(c_4 = -2\), so \([\vv]_{\CB} = [0 \ -1 \ 3 \ -2]^{\tr}\).%
\end{enumerate}
\item{}Find the vector \(\vv\) in the indicated vector space \(V\) given the basis \(\CB\) of \(V\) and \([\vv]_{\CB}\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}\(V = \pol_2\), \(\CB = \{1+t^2, 1+t, t+t^2\}\), \([\vv]_{\CB} = [2 \ 1 \ 3]^{\tr}\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp25185304}{}\quad{}Find the vector \(\vv\) in the indicated vector space \(V\) given the basis \(\CB\) of \(V\) and \([\vv]_{\CB}\).%
\par
Since \([\vv]_{\CB} = [2 \ 1 \ 3]^{\tr}\), it follows that%
\begin{equation*}
\vv = 2(1+t^2) + 1(1+t) + 3(t+t^2) = 3+4t+5t^2\text{.}
\end{equation*}
%
\item{}\(\CB = \left\{\cos(x), \frac{1}{1+x^2}\right\}\), \(V = \Span \ \CB\) as a subspace of \(\F\), \([\vx]_{\CB} = [2 \ -1]^{\tr}\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp25187736}{}\quad{}Find the vector \(\vv\) in the indicated vector space \(V\) given the basis \(\CB\) of \(V\) and \([\vv]_{\CB}\).%
\par
Since \([\vx]_{\CB} = [2 \ -1]^{\tr}\), it follows that%
\begin{equation*}
\vv = 2\cos(x) - \frac{1}{1+x^2}\text{.}
\end{equation*}
%
\end{enumerate}
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp25192088}%
Let \(p_1(t)=1\), \(p_2(t)=2-t\), \(p_3(t)=3+t-t^2\), \(p_4(t)= t+t^3\), and \(p_5(t) = 2t+t^2+t^4\) be in \(\pol_4\). Also, let \(\CB = \{1, t ,t^2, t^3, t^4\}\) be the standard basis for \(\pol_4\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find \([p_1(t)]_{\CB}\), \([p_2(t)]_{\CB}\), \([p_3(t)]_{\CB}\), \([p_4(t)]_{\CB}\), and \([p_5(t)]_{\CB}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp25189144}{}\quad{}The coordinate vectors of a polynomial with respect to the standard basis in \(\pol_4\) is found by just reading off the coefficients of the polynomial. So%
\begin{align*}
[p_1(t)]_{\CB} \amp = [1 \ 0 \ 0 \ 0 \ 0 ]^{\tr}\\
[p_2(t)]_{\CB} \amp = [2 \ -1 \ 0 \ 0 \ 0]^{\tr}\\
[p_3(t)]_{\CB} \amp = [3 \ 1 \ -1 \ 0 \ 1]^{\tr}\\
[p_4(t)]_{\CB} \amp = [0 \ 1 \ 0 \ 1 \ 0]^{\tr}\\
[p_5(t)]_{\CB} \amp = [0 \ 2 \ 1 \ 0 \ 1]^{\tr}\text{.}
\end{align*}
%
\item{}Use the result of part (a) to explain why \(\{p_1(t), p_2(t),
p_3(t), p_4(t), p_5(t)\}\) is a basis for \(\pol_4\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp25198872}{}\quad{}Let%
\begin{align*}
\CC \amp = \{p_1(t), p_2(t), p_3(t), p_4(t), p_5(t)\} \text{ and }\\
\CS \amp = \{[p_1(t)]_{\CB}, [p_2(t)]_{\CB}, [p_3(t)]_{\CB}, [p_4(t)]_{\CB},[p_5(t)]_{\CB}\}\text{.}
\end{align*}
Since the coordinate transformation is one-to-one and onto, the two sets \(\CC\) and \(\CS\) are either both linearly dependent or linearly independent. Technology shows that the reduced row echelon form of%
\begin{equation*}
[[p_1(t)]_{\CB} \ [p_2(t)]_{\CB} \ [p_3(t)]_{\CB} \ [p_4(t)]_{\CB} \ [p_5(t)]_{\CB}]
\end{equation*}
is \(I_5\), so the sets \(\CC\) and \(\CS\) are both linearly linearly independent. Since \(\dim(\pol_4) = 5\), it follows that any linearly independent set of five vectors in \(\pol_4\) is a basis for \(\pol_4\). Therefore, the set \(\CC\) is a basis for \(\pol_4\).%
\item{}Let \(p(t) = 2-t+t^2-3t^3+4t^4\). Find \([p(t)]_{\CB}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp25212184}{}\quad{}Similar to part (a), we have \([p(t)]_{\CB} = [2 \ -1 \ 1 \ -3 \ 4]^{\tr}\).%
\item{}Use the coordinate vectors in parts (a) and (c) to write \(p(t)\) as a linear combination of \(p_1(t)\), \(p_2(t)\), \(p_3(t)\), \(p_4(t)\), and \(p_5(t)\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp25204504}{}\quad{}Technology shows that the reduced row echelon form of the augmented matrix%
\begin{equation*}
[[p_1(t)]_{\CB} \ [p_2(t)]_{\CB} \ [p_3(t)]_{\CB} \ [p_4(t)]_{\CB} \ [p_5(t)]_{\CB} \ | \ [p(t)]_{\CB}]
\end{equation*}
is%
\begin{equation*}
\left[ \begin{array}{ccccc|r} 1\amp 0\amp 0\amp 0\amp 0\amp -25 \\ 0\amp 1\amp 0\amp 0\amp 0\amp  9 \\ 0\amp 0\amp 1\amp 0\amp 0\amp 3 \\ 0\amp 0\amp 0\amp 1\amp 0\amp -3 \\ 0\amp 0\amp 0\amp 0\amp 1\amp 4 \end{array}  \right]\text{.}
\end{equation*}
So%
\begin{equation*}
[p(t)]_{\CB} = -25[p_1(t)]_{\CB} +  9[p_2(t)]_{\CB} + 3[p_3(t)]_{\CB} - 3[p_4(t)]_{\CB} +  4[p_5(t)]_{\CB}
\end{equation*}
and%
\begin{equation*}
p(t) = -25p_1(t) + 9p_2(t) + 3p_3(t) - 3p_4(t) + 4p_5(t)\text{.}
\end{equation*}
%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_coord_vec_summ}
The key idea in this handout is the coordinate vector with respect to a basis.%
\begin{itemize}[label=\textbullet]
\item{}If \(\CB = \{\vv_1, \vv_2, \vv_3, \ldots, \vv_n\}\) is a basis for a vector space \(V\), then the coordinate vector of \(\vx\) in \(V\) with respect to \(\CB\) is the vector%
\begin{equation*}
[\vx]_{\CB} = [x_1, x_2, \ldots, x_n]^{\tr}\text{,}
\end{equation*}
where%
\begin{equation*}
\vx = x_1 \vv_1 + x_2 \vv_2 + \cdots + x_n \vv_n\text{.}
\end{equation*}
%
\item{}The coordinate transformation \(\vx \mapsto [\vx]_{\CB}\) is a one-to-one and onto transformation from an \(n\)-dimensional vector space \(V\) to \(\R^n\) which preserves linear combinations.%
\item{}The coordinate transformation \(\vx \mapsto [\vx]_{\CB}\) allows us to translate problems in arbitrary vector spaces to \(\R^n\) where we have already developed tools to solve the problems.%
\end{itemize}
%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_coord_vec_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp25216152}%
Let \(\CB=\left\{ \left[ \begin{array}{r} 0\\1\\-1 \end{array} \right], \left[ \begin{array}{c} 1\\2\\0 \end{array} \right] \right\}\) be a basis of the subspace defined by the equation \(y-4x+z=0\). Find the coordinates of the vector \(\vb=\left[ \begin{array}{c} 3\\4\\2 \end{array} \right]\) with respect to the basis \(\CB\).%
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp25224472}%
Given basis \(\CB=\{1+t, 2+t^2, t+t^2\}\) of \(\pol_2\),%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}For which \(p(t)\) in \(\pol_2\) is \([p(t)]_{\CB} = [ 1 \ -1 \ 3 ]^\tr\)?%
\item{}Determine coordinates of \(q(t)=-1+t+2t^2\) in \(\pol_2\) with respect to basis \(\CB\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp25223704}%
Find two different bases \(\CB_1\) and \(\CB_2\) of \(\R^2\) so that \([\vb]_{\CB_1} = [\vb]_{\CB_2} = \left[ \begin{array}{c} 2\\1 \end{array} \right]\), where \(\vb=\left[ \begin{array}{c} 5\\3 \end{array} \right]\).%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp25232024}%
If \([\vb_1]_{\CB} = \left[ \begin{array}{c} 1\\2 \end{array} \right]\) and \([\vb_2]_{\CB} = \left[ \begin{array}{c} 2\\1 \end{array} \right]\) with respect to some basis \(\CB\), where \(\vb_1=\left[ \begin{array}{c} 1\\2\\3 \end{array} \right]\) and \(\vb_2=\left[ \begin{array}{c} 2\\1\\3 \end{array} \right]\), what are the coordinates of \(\left[ \begin{array}{r} -2\\3\\1 \end{array} \right]\)?%
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp25235864}%
If \([\vb_1]_{\CB} = \left[ \begin{array}{c} 1\\1 \end{array} \right]\) and \([\vb_2]_{\CB} = \left[ \begin{array}{c} 2\\1 \end{array} \right]\) with respect to some basis \(\CB\), where \(\vb_1=\left[ \begin{array}{c} 3\\1\\3 \end{array} \right]\) and \(\vb_2=\left[ \begin{array}{c} 4\\1\\5 \end{array} \right]\), what are the vectors in \(\CB\)?%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp25240472}%
Let \(\CB=\{ \vv_1, \vv_2, \ldots, \vv_n\}\) be a basis for a vector space \(V\). Describe how the coordinates of a vector with respect to \(\CB\) will change if \(\vv_1\) is replaced with \(\frac{1}{2}\vv_1\).%
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp25238040}%
Let \(\CB = \{1, t, 1+t^2\}\) in \(\pol_2\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(\CB\) is a basis for \(\pol_2\).%
\item{}Let \(p_1(t) = 1+2t^2\), \(p_2(t)=1+t+2t^2\), and \(p_3(t) = 2-t+t^2\) in \(\pol_2\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Find \([p_1(t)]_{\CB}\), \([p_2(t)]_{\CB}\), and \([p_3(t)]_{\CB}\).%
\item{}Use the coordinate vectors in part i. to determine if the set \(\{p_1(t),
p_2(t), p_3(t)\}\) is linearly independent or dependent.%
\end{enumerate}
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp25250840}%
Let \(W = \Span\{2+4t+6t^3, 3-t^2, 3-t^2+9t^3\}\) in \(\pol_3\). Let \(\CB = \{1, t, t^2, t^3\}\) be the standard basis for \(\pol_3\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Calculate \([2+4t+6t^3]_{\CB}\), \([3-t^2]_{\CB}\) and \([3-t^2+9t^3]_{\CB}\).%
\item{}Use the coordinate vectors from part (a) to determine if the polynomials \(2+4t+6t^3\), \(3-t^2\), and \(3-t^2+9t^3\) are linearly independent or dependent.%
\item{}Let \(p(t) = 4+2t-t^2+9t^3\). Find \([p(t)]_{\CB}\).%
\item{}Use the calculations from parts (a) and (c) to determine if \(p(t)\) is in \(W\). If so, write \(p(t)\) as a linear combination of the polynomials \(2+4t+6t^3\), \(3-t^2\), and \(3-t^2+9t^3\). If not, explain why not.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp25265304}%
Let \(\CB=\left\{ \left[\begin{array}{cc} 1\amp 0\\0\amp 0 \end{array}  \right], \left[\begin{array}{cc} 1\amp 1\\0\amp 0 \end{array} \right], \left[\begin{array}{cc} 0\amp 0\\1\amp 1 \end{array} \right], \left[\begin{array}{cc} 1\amp 2\\2\amp 1 \end{array} \right] \right\}\) in \(M_{2\times 2}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(\CB\) is a basis of \(M_{2\times 2}\).%
\item{}Let%
\begin{equation*}
A = \left[\begin{array}{cc} 1\amp 2\\2\amp 1 \end{array} \right] \, , \, B=\left[\begin{array}{cc} 1\amp 1\\1\amp 1 \end{array} \right] \, , \, C= \left[\begin{array}{cc} 2\amp 1\\1\amp 1 \end{array}  \right]\, , \, D= \left[\begin{array}{cc} 1\amp 1\\1\amp 0 \end{array} \right]\text{.}
\end{equation*}
Find \([A]_\CB,[B]_\CB, [C]_\CB, [D]_\CB\).%
\item{}Determine if the set \(\{A, B, C, D\}\) is linearly dependent or independent.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{10}{}{}{g:exercise:idp25272216}%
Let \(V\) be a vector space of dimension \(n\) and let \(\CB\) be a basis for \(V\). Show that the coordinate transformation \(T\) from \(V\) to \(\R^n\) defined by \(T(\vx) = [\vx]_{\CB}\) satisfies \(T(\vzero_V) = \vzero\), where \(\vzero_V\) is the additive identity in \(V\).%
\end{divisionexercise}%
\begin{divisionexercise}{11}{}{}{g:exercise:idp25270168}%
Prove the second property of \hyperref[x:theorem:thm_5_d_5]{Theorem~{\xreffont\ref{x:theorem:thm_5_d_5}}}. That is, if a vector space \(V\) has a basis of \(n\) vectors, then the coordinate mapping \(T : V \to \R^n\) defined by \(T(\vx) = [\vx]_{\CB}\) satisfies%
\begin{equation*}
T(c\vu) = cT(\vu)
\end{equation*}
for any vector \(\vu\) in \(V\) and any scalar \(c\).%
\end{divisionexercise}%
\begin{divisionexercise}{12}{}{}{g:exercise:idp25284632}%
Prove \hyperref[x:theorem:thm_5_d_6]{Theorem~{\xreffont\ref{x:theorem:thm_5_d_6}}} by demonstrating that if \(V\) is a vector space with ordered basis \(\CB = \{\vv_1, \vv_2, \ldots, \vv_n\}\), then the coordinate mapping \(T : V \to \R^n\) defined by \(T(\vx) = [\vx]_{\CB}\) is one-to-one.%
\end{divisionexercise}%
\begin{divisionexercise}{13}{}{}{g:exercise:idp25281304}%
The coordinate transformation \(T\) is one-to-one, so it has an inverse \(T^{-1}\). Let \(V\) be an \(n\)-dimensional vector space that has a basis \(\CB\), and let \(T : V \to \R^n\) be the coordinate transformation defined by \(T(\vx) = [\vx]_{\CB}\). Let \(S = \{\vu_1, \vu_2, \ldots, \vu_k\}\) be a subset of \(V\) and let \(R = \{[\vu_1]_{\CB}, [\vu_2]_{\CB}, \ldots, [\vu_k]_{\CB}\}\) in \(\R^n\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Suppose \(\vx\) is in \(V\) with \(\vx = x_1 \vu_1 + x_2\vu_2 + \cdots + x_k \vu_k\). Write the vector \([\vx]_{\CB}\) as a linear combination of the vectors in \(R\). Explain your reasoning and explain how your result shows that \(T\) preserves linear combinations.%
\item{}Now suppose \(\vw\) is in \(V\) so that \([\vw]_{\CB} = w_1 [\vu_1]_{\CB} + w_2[\vu_2]_{\CB} + \cdots + w_k [\vu_k]_{\CB}\). Write the vector \(\vw\) as a linear combination of the vectors in \(S\). Explain your reasoning and explain how your result shows that \(T^{-1}\) preserves linear combinations.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25301528}{}\quad{}Apply \(T\) to an appropriate vector and use the fact that \(T\) is one-to-one.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{14}{}{}{g:exercise:idp25296408}%
Let \(S = \{\vu_1, \vu_2, \ldots, \vu_k\}\) be a subset of an \(n\)-dimensional vector space \(V\) with basis \(\CB\). Let \(R = \{[\vu_1]_{\CB}, [\vu_2]_{\CB}, \ldots, [\vu_k]_{\CB}\}\) in \(\R^n\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that if \(S\) is linearly independent in \(V\), then \(R\) is linearly independent in \(\R^n\).%
\item{}Is the converse of part (a) true? That is, if \(R\) is linearly independent in \(\R^n\), must \(S\) be linearly independent in \(V\)? Justify your answer.%
\item{}Repeat parts (a) and (b), replacing ``linearly independent'' with ``linearly dependent''.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{15}{}{}{g:exercise:idp25303192}%
Let \(V\) be an \(n\)-dimensional vector space with basis \(\CB\), and let \(S=\{\vw_1, \vw_2, \ldots, \vw_k\}\) be a subset of \(V\) that spans \(V\). Prove that \(\{[\vw_1]_\CB, [\vw_2]_\CB, \ldots, [\vw_k]_\CB\}\) spans \(\R^n\).%
\end{divisionexercise}%
\begin{divisionexercise}{16}{}{}{x:exercise:ex_5_d_T_VW}%
Suppose \(\CB_1\) is a basis of a vector space \(V\) with \(n\) elements and \(\CB_2\) is a basis of \(W\) with \(n\) elements. Show that the map \(T_{VW}\) which sends every \(\vx\) in \(V\) to the vector \(\vy\) in \(W\) such that \([\vx]_{\CB_1} = [\vy]_{\CB_2}\) is one-to-one and onto.%
\end{divisionexercise}%
\begin{divisionexercise}{17}{}{}{g:exercise:idp25316888}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
The coordinates of a non-zero vector cannot be the same in the coordinate systems defined by two different bases.%
\item{}\lititle{True\slash{}False.}\par%
The coordinate vector of the zero vector with respect to any basis is always the zero vector.%
\item{}\lititle{True\slash{}False.}\par%
If \(W\) is a \(k\) dimensional subset of an \(n\) dimensional vector space \(V\), and \(\CB\) is a basis of \(W\), then \([\vw]_{\CB}\) is a vector in \(\R^n\) for any \(\vw\) in \(W\).%
\item{}\lititle{True\slash{}False.}\par%
The order of vectors in a basis do not affect the coordinates of vectors with respect to this basis.%
\item{}\lititle{True\slash{}False.}\par%
If \(T\) is a coordinate transformation from a vector space \(V\) with basis \(\CB\) to \(\R^n\), then the vector \([\vx]_{\CB}\) is unique to the vector \(\vx\) in \(V\).%
\item{}\lititle{True\slash{}False.}\par%
If \(T\) is a coordinate transformation from a vector space \(V\) with basis \(\CB\) to \(\R^n\), then there is always a vector \(\vx\) in \(V\) that maps to any vector \(\vb\) in \(\R^n\).%
\item{}\lititle{True\slash{}False.}\par%
A coordinate transformation from a vector space \(V\) with basis \(\CB\) to \(\R^n\) always maps the additive inverse of a vector \(\vx\) in \(V\) to the additive inverse of the vector \([\vx]_{\CB}\) in \(\R^n\).%
\item{}\lititle{True\slash{}False.}\par%
A coordinate transformation provides a unique identification of vectors in an \(n\)-dimensional vector space with vectors in \(\R^n\) in a way that preserves the algebraic structure of the spaces.%
\item{}\lititle{True\slash{}False.}\par%
If the coordinate vector of \(\vx\) in a vector space \(V\) is \(\left[ \begin{array}{r} 1\\-1\\2 \end{array} \right]\), then the coordinate vector of \(2\vx\) is \(\left[ \begin{array}{r} 2\\-2\\4 \end{array} \right]\).%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Finding Formulas for Sums of Powers}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Finding Formulas for Sums of Powers}{}{Project: Finding Formulas for Sums of Powers}{}{}{x:section:sec_proj_sum_powers}
One way to derive formulas for sums of powers of whole numbers is to use different bases and coordinate vectors. One basis that will be useful is a basis of polynomials created by the binomial coefficients. Recall that the binomial coefficient \(\binom{n}{k}\) is equal to%
\begin{equation*}
\binom{n}{k} = \frac{n!}{k!(n-k)!}\text{,}
\end{equation*}
with \(\binom{n}{k} = 0\) if \(n \lt  k\).%
\par
The binomial coefficient can be rewritten in a way to make it applicable to polynomials as%
\begin{equation*}
\binom{n}{k} = \frac{n(n-1)(n-2) \cdots (n-k+1)}{k!} = \Pi_{i=1}^k \frac{1}{i}(n-i+1)\text{.}
\end{equation*}
%
\par
With this representation of \(\binom{n}{k}\), we can define a new polynomial in \(t\) of degree \(k\) as%
\begin{equation*}
p_{k}(t) = \frac{1}{k!}(t)(t-1)(t-2) \cdots (t-k+1)
\end{equation*}
with \(p_0(t) = 1\). For example,%
\begin{align*}
p_1(t) \amp = \frac{1}{1!}t = t\\
p_2(t) \amp = \frac{1}{2}(t)(t-1) = \frac{1}{2}\left(t^2-t\right)\\
p_3(t) \amp = \frac{1}{6}(t)(t-1)(t-2) = \frac{1}{6}\left(t^3-3t^2+2t\right)\text{.}
\end{align*}
%
\par
These ``generalized binomial coefficients'' appear in Newton's generalized binomial theorem.%
\par
Two facts will make these polynomials useful for our sums.%
\begin{project}{}{x:project:act_hockey_stick}%
Our polynomials \(p_k(t)\) are defined in terms of binomial coefficients. A useful identity will relate sums of binomial coefficients to other binomial coefficients. This identity, called the \emph{hockey-stick identity} after the way it can be visualized on Pascal's triangle, is as follows:%
\begin{equation*}
\sum_{k=0}^{n-1} \binom{k}{r} = \binom{n}{r+1}
\end{equation*}
for positive integers \(r\). Use the definition of the binomial coefficients and some algebra to verify the hockey-stick identity.%
\end{project}%
The second useful fact about our polynomials \(p_k(t)\) is that they form a basis for \(\pol_n\).%
\begin{project}{}{x:project:act_binomial_basis}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(k\) be a positive integer. Explain why \(p_k(0) = p_k(1) = p_k(2) = \cdots = p_k(k-1) = 0\) and \(p_k(k) = 1\).%
\item{}Let \(\CP_n = \{p_0(t), p_1(t),
p_2(t), \ldots, p_n(t)\}\). Show that \(\CP_n\) is a basis for \(\pol_n\). (Hint: Let \(c_1\), \(c_2\), \(\ldots\), \(c_n\) be scalars and consider the equation%
\begin{equation*}
c_0p_0(t) + c_1p_1(t) + c_2 p_2(t) + \cdots + c_np_n(t) = 0\text{.}
\end{equation*}
Evaluate this equation at \(t=0\), \(t=1\), \(\ldots\), \(t=n\) and use the result of part (a).)%
\end{enumerate}
\end{project}%
Now we have the tools we need to derive our formulas. To simplify computations, we will change coordinates to the standard basis \(\CS_n = \{1, t, t^2, \ldots,
t^n\}\) for \(\pol_n\).%
\par
We will illustrate the process of deriving formulas for our sums with the sum \(\sum_{t=0}^{n-1} t\). We want to write \(t\) as a linear combination of the vectors in \(\CP_1\) so that we can utilize the hockey-stick identity. In this case, by definition we have \(t = p_1(t)\). It follows by the hockey-stick identity and the fact that \(p_1(t) = \binom{t}{1}\) that%
\begin{align*}
\sum_{t=0}^{n-1} t \amp = \sum_{t=0}^{n-1}  p_1(t)\\
\amp = \sum_{t=0}^{n-1}  \binom{t}{1}\\
\amp = \binom{n}{2}\\
\amp = \frac{1}{2}\left(n^2-n\right)\text{.}
\end{align*}
%
\par
This is exactly the formula we saw at the beginning of this section. The cases for sums of higher powers work the same way, but we will need to do a little more work to write \(t^n\) in terms of the basis vectors in \(\CP_n\).%
\begin{project}{}{x:project:act_sum_2}%
Consider the sum \(\sum_{t=0}^{n-1} t^2\). We want to write \(t^2\) as a linear combination of \(p_0(t)\), \(p_1(t)\) and \(p_2(t)\). To do so, we will use the coordinate vectors with respect to \(\CS_2\) and do our work in \(\R^3\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find \([p_0(t)]_{\CS_2}\), \([p_1(t)]_{\CS_2}\), \([p_2(t)]_{\CS_2}\), and \([t^2]_{\CS_2}\).%
\item{}Use the coordinate vectors from part (a) to write \(t^2\) as a linear combination of the vectors in \(\CP_2\).%
\item{}Use the result of part (b) and, the hockey-stick identity, and the fact that \(p_k(t) = \binom{t}{k}\) to find a formula for \(\sum_{t=0}^{n-1} t^2\).%
\end{enumerate}
\end{project}%
Deriving formulas for higher powers involves the same process, just with more algebra.%
\begin{project}{}{x:project:act_sums_higher}%
Use the process outlined in \hyperref[x:project:act_sum_2]{Project Activity~{\xreffont\ref{x:project:act_sum_2}}} to derive formulas for the following sums.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\sum_{t=0}^{n-1} t^3\)%
\item{}\(\sum_{t=0}^{n-1} t^4\)%
\end{enumerate}
\end{project}%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 35 Inner Product Spaces}
\typeout{************************************************}
%
\begin{chapterptx}{Inner Product Spaces}{}{Inner Product Spaces}{}{}{x:chapter:chap_inner_products}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp25379096}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What is an inner product space?%
\item{}What is an orthogonal set in an inner product space?%
\item{}What is an orthogonal basis for an inner product space?%
\item{}How do we find the coordinate vector for a vector in an inner product space relative to an orthogonal basis for the space?%
\item{}What is the projection of a vector orthogonal to a subspace and why are such orthogonal projections important?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Fourier Series}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Fourier Series}{}{Application: Fourier Series}{}{}{x:section:sec_appl_fourier}
In calculus, a Taylor polynomial for a function \(f\) is a polynomial approximation that fits \(f\) well around the center of the approximation. For this reason, Taylor polynomials are good \terminology{local} approximations, but they are not in general good global approximations. In particular, if a function \(f\) has periodic behavior it is impossible to model \(f\) well globally with polynomials that have infinite limits at infinity. For these kinds of functions, trigonometric polynomials are better choices. Trigonometric polynomials lead us to Fourier series, and we will investigate how inner products allow us to use trigonometric polynomials to model musical tones later in this section.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_inner_prod_intro}
In \hyperref[x:chapter:chap_dot_product]{Section~{\xreffont\ref{x:chapter:chap_dot_product}}} we were introduced to inner products in \(\R^n\). The concept of an inner product can be extended to vector spaces, as we will see in this section. This will allow us to measure lengths and angles between vectors and define orthogonality in certain vector spaces.%
\par
Recall that an inner product on \(\R^n\) assigns to each pair of vectors \(\vu\) and \(\vv\) the scalar \(\langle \vu, \vv \rangle\). Thus, an inner product on \(\R^n\) defines a mapping from \(\R^n \times \R^n\) to \(\R\). Recall also that an inner product on \(\R^n\) is commutative, distributes over vector addition, and respects scalar multiplication, and the inner product of a vector in \(\R^n\) by itself is always non-negative and is equal to 0 only when the vector is the zero vector. We will investigate this ideas in vector spaces in \hyperref[x:exploration:pa_6_c]{Preview Activity~{\xreffont\ref{x:exploration:pa_6_c}}}.%
\begin{exploration}{}{x:exploration:pa_6_c}%
Consider the vector space \(\pol_2\) of polynomials of degree less than or equal to \(2\) with real coefficients. Define a mapping from \(\pol_2 \times \pol_2\) to \(\R\) by%
\begin{equation*}
\langle p(t), q(t) \rangle = \int_0^1 p(t)q(t) \, dt\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Calculate \(\langle 1, t \rangle\).%
\item{}If \(p(t)\) and \(q(t)\) are in \(\pol_2\), is it true that%
\begin{equation*}
\langle p(t) , q(t) \rangle = \langle q(t), p(t) \rangle?
\end{equation*}
Verify your answer.%
\item{}If \(p(t)\), \(q(t)\), and \(r(t)\) are in \(\pol_2\), is it true that%
\begin{equation*}
\langle p(t)+q(t), r(t) \rangle = \langle p(t), r(t) \rangle + \langle q(t), r(t) \rangle?
\end{equation*}
Verify your answer.%
\item{}If \(p(t)\) and \(q(t)\) are in \(\pol_2\) and \(c\) is a scalar, is it true that%
\begin{equation*}
\langle cp(t) , q(t) \rangle = c\langle p(t) , q(t) \rangle?
\end{equation*}
Verify your answer.%
\item{}If \(p(t)\) is in \(\pol_2\), must it be the case that \(\langle p(t) , p(t) \rangle \geq 0\)? When is \(\langle p(t) , p(t) \rangle = 0\)?%
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Inner Product Spaces}
\typeout{************************************************}
%
\begin{sectionptx}{Inner Product Spaces}{}{Inner Product Spaces}{}{}{x:section:sec_inner_prod_spaces}
As we saw in \hyperref[x:exploration:pa_6_c]{Preview Activity~{\xreffont\ref{x:exploration:pa_6_c}}}, we can define a mapping from \(\pol_2 \times \pol_2\) to \(\R\) that has the same properties of inner products on \(\R^n\). So we can extend the definition of inner product to arbitrary vector spaces.%
\begin{definition}{}{x:definition:def_6_c_inner_product}%
\index{inner product}%
\index{inner product space}%
An \terminology{inner product} \(\langle \ , \ \rangle\) on a vector space \(V\) is a mapping from \(V \times V \to \R\) satisfying%
\begin{enumerate}
\item{}\(\langle \vu , \vv \rangle = \langle \vv , \vu \rangle\) for all \(\vu\) and \(\vv\) in \(V\),%
\item{}\(\langle \vu + \vv , \vw \rangle = \langle \vu , \vw \rangle + \langle \vv , \vw \rangle\) for all \(\vu\), \(\vv\), and \(\vw\) in \(V\),%
\item{}\(\langle c\vu , \vv \rangle = c\langle \vu , \vv \rangle\) for all \(\vu\), \(\vv\) in \(V\) and all scalars \(c\),%
\item{}\(\langle \vu , \vu \rangle \geq 0\) for all \(\vu\) in \(V\) and \(\langle \vu , \vu \rangle = 0\) if and only if \(\vu = \vzero\).%
\end{enumerate}
%
\par
An \terminology{inner product space} is a vector space on which an inner product is defined.%
\end{definition}
\begin{activity}{}{g:activity:idp25417880}%
Consider the mapping \(\langle \ , \ \rangle\) from \(\pol_1 \times \pol_1\) to \(\R\) defined by%
\begin{equation*}
\langle p(t), q(t) \rangle = p'(0)q'(0)\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that this mapping satisfies the second property of an inner product.%
\item{}Although this mapping satisfies the first three properties of an inner product, show that this mapping does not satisfy the fourth property and so is not an inner product.%
\end{enumerate}
\end{activity}%
Since inner products in vector spaces are defined in the same way as inner products in \(\R^n\), they will satisfy the same properties. Some of these properties are summarized in the following theorem.%
\begin{theorem}{}{}{x:theorem:thm_6_c_inner_product_properties}%
Let \(\langle \ , \ \rangle\) be an inner product on \(\R^n\) and let \(\vu, \vv\), and \(\vw\) be vectors in \(\R^n\) and \(c\) a scalar. Then%
\begin{enumerate}
\item{}\(\displaystyle \langle \vzero , \vv \rangle = \langle \vv , \vzero \rangle = 0\)%
\item{}\(\displaystyle \langle \vu , c\vv \rangle = c\langle \vu , \vv \rangle\)%
\item{}\(\displaystyle \langle \vu , \vv+\vw \rangle = \langle \vu , \vv \rangle + \langle \vu , \vw \rangle\)%
\item{}\(\displaystyle \langle \vu - \vv, \vw \rangle = \langle \vu , \vw \rangle - \langle \vv , \vw \rangle\)%
\end{enumerate}
%
\end{theorem}
One special inner product is indicated in \hyperref[x:exploration:pa_6_c]{Preview Activity~{\xreffont\ref{x:exploration:pa_6_c}}}. Recall that \(C[a,b]\) is the vector space of continuous functions on the closed interval \([a,b]\). Let \(\langle \ ,  \ \rangle\) map from \(C[a,b] \times C[a,b]\) to \(\R\) be defined by%
\begin{equation*}
\langle f(x), g(x) \rangle = \int_a^b f(x)g(x) \, dx\text{.}
\end{equation*}
%
\par
The verification that this mapping is an inner product is left to the exercises.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Length of a Vector}
\typeout{************************************************}
%
\begin{sectionptx}{The Length of a Vector}{}{The Length of a Vector}{}{}{x:section:sec_vec_length}
We can use inner products to define the length of any vector in an inner product space and the distance between two vectors in an inner product space. The idea comes right from the relationship between lengths of vectors in \(\R^n\) and inner products on \(\R^n\) (compare to \hyperref[x:definition:def_6_a_length_Rn]{Definition~{\xreffont\ref{x:definition:def_6_a_length_Rn}}}).%
\begin{definition}{}{g:definition:idp25173656}%
\index{length of a vector in an inner product space}%
Let \(\vv\) be a vector in an inner product space. The \terminology{length} of \(\vv\) is the real number%
\begin{equation*}
||\vv|| = \sqrt{\langle \vv, \vv \rangle}\text{.}
\end{equation*}
%
\end{definition}
The length of a vector in a vector space is also called \terminology{magnitude} or \terminology{norm}. Just as with inner products on \(\R^n\) we can use the notion of length to define unit vectors in inner product spaces (compare to \hyperref[x:definition:def_6_a_unit_vector]{Definition~{\xreffont\ref{x:definition:def_6_a_unit_vector}}}).%
\begin{definition}{}{g:definition:idp25179032}%
\index{unit vector in an inner product space}%
A vector \(\vv\) in an inner product space is a \terminology{unit vector} if \(||\vv|| = 1\).%
\end{definition}
We can find a unit vector in the direction of a nonzero vector \(\vv\) in an inner product space by dividing by the norm of the vector. That is, the vector \(\ds \frac{\vv}{||\vv||}\) is a unit vector in the direction of the vector \(\vv\), provided that \(\vv\) is not zero.%
\par
We define the distance between vectors \(\vu\) and \(\vv\) in an inner product space in the same way we defined distance using the dot product (compare to \hyperref[x:definition:def_6_a_distance]{Definition~{\xreffont\ref{x:definition:def_6_a_distance}}}).%
\begin{definition}{}{g:definition:idp25442696}%
\index{distance between vectors in an inner product space}%
Let \(\vu\) and \(\vv\) be vectors in an inner product space. The \terminology{distance between} \(\vu\) and \(\vv\) is the length of the difference \(\vu - \vv\) or%
\begin{equation*}
|| \vu - \vv ||\text{.}
\end{equation*}
%
\end{definition}
\begin{activity}{}{x:activity:act_6_c_Frobenius}%
The \terminology{trace} (see \hyperref[x:definition:def_trace]{Definition~{\xreffont\ref{x:definition:def_trace}}}) of an \(n \times n\) matrix \(A = [a_{ij}]\) is the sum of the diagonal entries of \(A\). That is,%
\begin{equation*}
\trace(A) = a_{11} + a_{22} + \cdots + a_{nn}\text{.}
\end{equation*}
%
\par
If \(A\) and \(B\) are in the space \(\M_{n \times n}\) of \(n \times n\) matrices with real entries, we define the mapping \(\langle  \ , \ \rangle\) from \(\M_{n \times n} \times \M_{n \times n}\) to \(\R\) by%
\begin{equation*}
\langle A, B \rangle = \trace\left(AB^{\tr}\right)\text{.}
\end{equation*}
%
\par
This mapping is an inner product on the space \(\M_{n \times n}\) called the \terminology{Frobenius} inner product (details are in the exercises). Let \(A = \left[ \begin{array}{cc} 1\amp 0\\2\amp 2 \end{array} \right]\) and \(B = \left[ \begin{array}{rc} -3\amp 4\\-2\amp 1 \end{array} \right]\) in \(\M_{2 \times 2}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the length of the vectors \(A\) and \(B\) using the Frobenius inner product.%
\item{}Find the distance between \(A\) and \(B\) using the Frobenius inner product.%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Orthogonality in Inner Product Spaces}
\typeout{************************************************}
%
\begin{sectionptx}{Orthogonality in Inner Product Spaces}{}{Orthogonality in Inner Product Spaces}{}{}{x:section:sec_inner_prod_orthog}
We defined orthogonality in \(\R^n\) using inner products in \(\R^n\) (see \hyperref[x:definition:def_6_a_orthogonal_dot_product]{Definition~{\xreffont\ref{x:definition:def_6_a_orthogonal_dot_product}}}) and the angle between vectors. We can extend those ideas to any inner product space.%
\par
\index{angle between vectors} If \(\vu\) and \(\vv\) are nonzero vectors in an inner product space, then the \terminology{angle \(\theta\) between \(\vu\) and \(\vv\)} is such that%
\begin{equation*}
\cos(\theta) = \frac{\langle \vu, \vv \rangle}{||\vu||\, ||\vv||}\text{.}
\end{equation*}
and \(0\leq \theta \leq \pi\). This angle is well-defined due to the Cauchy-Schwarz inequality \(|\langle \vu, \vv \rangle| \leq ||\vu||\, ||\vv||\) whose proof is left to the exercises.%
\par
With the angle between vectors in mind, we can define orthogonal vectors in an inner product space.%
\begin{definition}{}{g:definition:idp25463432}%
\index{orthogonal vectors in inner product spaces}%
Vectors \(\vu\) and \(\vv\) in an inner product space are \terminology{orthogonal} if%
\begin{equation*}
\langle \vu, \vv \rangle = 0\text{.}
\end{equation*}
%
\end{definition}
Note that this defines the zero vector to be orthogonal to every vector.%
\begin{activity}{}{g:activity:idp25473416}%
In this activity we use the Frobenius inner product (see \hyperref[x:activity:act_6_c_Frobenius]{Activity~{\xreffont\ref{x:activity:act_6_c_Frobenius}}}). Let \(A = \left[ \begin{array}{cc} 0\amp 1\\1\amp 0 \end{array} \right]\) and \(B = \left[ \begin{array}{cc} 0\amp 1\\0\amp 0 \end{array} \right]\) in \(\M_{2 \times 2}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find a nonzero vector in \(\M_{2 \times 2}\) orthogonal to \(A\).%
\item{}Find the angle between \(A\) and \(B\).%
\end{enumerate}
\end{activity}%
Using orthogonality we can generalize the notions of orthogonal sets and bases, orthonormal bases and orthogonal complements we defined in \(\R^n\) to all inner product spaces in a natural way.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Orthogonal and Orthonormal Bases in Inner Product Spaces}
\typeout{************************************************}
%
\begin{sectionptx}{Orthogonal and Orthonormal Bases in Inner Product Spaces}{}{Orthogonal and Orthonormal Bases in Inner Product Spaces}{}{}{x:section:sec_inner_prog_orthog_bases}
As we did with inner products in \(\R^n\), we define an orthogonal set to be one in which all of the vectors in the set are orthogonal to each other (compare to \hyperref[x:definition:def_6_b_orthogonal_set_Rn]{Definition~{\xreffont\ref{x:definition:def_6_b_orthogonal_set_Rn}}}).%
\begin{definition}{}{g:definition:idp25478792}%
\index{orthogonal set in an inner product space}%
A subset \(S\) of an inner product space for which \(\langle \vu, \vv \rangle = 0\) for all \(\vu \neq \vv\) in \(S\) is called an \terminology{orthogonal set}.%
\end{definition}
As in \(\R^n\), an orthogonal set of nonzero vectors is always linearly independent. The proof is similar to that of \hyperref[x:theorem:thm_6_b_Orth_li]{Theorem~{\xreffont\ref{x:theorem:thm_6_b_Orth_li}}} and is left to the exercises.%
\begin{theorem}{}{}{x:theorem:thm_6_c_Orth_li_ips}%
Let \(\{\vv_1, \vv_2, \ldots, \vv_m\}\) be a set of nonzero orthogonal vectors in an inner product space. Then the vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_m\) are linearly independent.%
\end{theorem}
A basis that is also an orthogonal set is given a special name (compare to \hyperref[x:definition:def_6_b_orthogonal_basis_Rn]{Definition~{\xreffont\ref{x:definition:def_6_b_orthogonal_basis_Rn}}}).%
\begin{definition}{}{g:definition:idp25486216}%
\index{orthogonal basis in an inner product space}%
An \terminology{orthogonal basis} \(\CB\) for a subspace \(W\) of an inner product space is a basis of \(W\) that is also an orthogonal set.%
\end{definition}
Using inner products in \(\R^n\), we saw that the representation of a vector as a linear combination of vectors in an orthogonal basis was quite elegant. The same is true in any inner product space. To see this, let \(\CB = \{\vv_1, \vv_2, \ldots, \vv_m\}\) be an orthogonal basis for a subspace \(W\) of an inner product space and let \(\vx\) be any vector in \(W\). We know that%
\begin{equation*}
\vx = x_1\vv_1 + x_2\vv_2 + \cdots + x_m \vv_m
\end{equation*}
for some scalars \(x_1\), \(x_2\), \(\ldots\), \(x_m\). If \(1\leq k\leq m\), then, using inner product properties and the orthogonality of the vectors \(\vv_i\), we have%
\begin{equation*}
\langle \vv_k, \vx \rangle = x_1 \langle \vv_k, \vv_1 \rangle + x_2 \langle \vv_k, \vv_2 \rangle + \cdots + x_m \langle \vv_k, \vv_m \rangle = x_k \langle \vv_k, \vv_k \rangle\text{.}
\end{equation*}
%
\par
So%
\begin{equation*}
x_k = \ds \frac{\langle \vx, \vv_k \rangle}{\langle \vv_k,  \vv_k \rangle}\text{.}
\end{equation*}
%
\par
Thus, we can calculate each weight individually with two simple inner product calculations.%
\par
We summarize this discussion in the next theorem (compare to \hyperref[x:theorem:thm_6_b_orth_dcomp]{Theorem~{\xreffont\ref{x:theorem:thm_6_b_orth_dcomp}}}).%
\begin{theorem}{}{}{x:theorem:thm_6_c_orth_dcomp}%
Let \(\CB = \{\vv_1, \vv_2, \ldots, \vv_m\}\) be an orthogonal basis for a subspace of an inner product space. Let \(\vx\) be a vector in \(W\). Then%
\begin{equation}
\vx = \ds \frac{\langle \vx, \vv_1 \rangle}{\langle \vv_1, \vv_1 \rangle} \vv_1 +  \frac{\langle \vx, \vv_2 \rangle}{\langle \vv_2, \vv_2 \rangle} \vv_2 + \cdots + \frac{\langle \vx, \vv_m \rangle}{\langle \vv_m, \vv_m \rangle} \vv_m\text{.}\label{x:men:eq_6_c_orth_decomp_ips}
\end{equation}
%
\end{theorem}
\begin{activity}{}{g:activity:idp25493512}%
Let \(p_1(t) = 1-t\), \(p_2(t) = -2+4t+4t^2\), and \(p_3(t) = 7-41t+40t^2\) be vectors in the inner product space \(\pol_2\) with inner product defined by \(\langle p(t),
q(t) \rangle = \int_0^1 p(t)q(t) \ dt\). Let \(\CB = \{p_1(t), p_2(t), p_3(t)\}\). You may assume that \(\CB\) is an orthogonal basis for \(\pol_2\). Let \(z(t) = 4-2t^2\). Find the weight \(x_3\) so that \(z(t) = x_1p_1(t) + x_2 p_2(t) + x_3 p_3(t)\). Use technology as appropriate to evaluate any integrals.%
\end{activity}%
The decomposition \hyperref[x:men:eq_6_c_orth_decomp_ips]{({\xreffont\ref{x:men:eq_6_c_orth_decomp_ips}})} is even simpler if \(\langle \vv_k, \vv_k \rangle = 1\) for each \(k\). Recall that%
\begin{equation*}
\langle \vv, \vv \rangle = || \vv ||^2\text{,}
\end{equation*}
so the condition \(\langle \vv, \vv \rangle  = 1\) implies that the vector \(\vv\) has norm 1. As with inner products in \(\R^n\), an orthogonal basis with this additional condition is given a special name (compare to \hyperref[x:definition:def_6_b_orthonormal_basis]{Definition~{\xreffont\ref{x:definition:def_6_b_orthonormal_basis}}}).%
\begin{definition}{}{g:definition:idp25506184}%
\index{orthonormal basis in an inner product space}%
An \terminology{orthonormal basis} \(\CB = \{\vv_1, \vv_2, \ldots, \vv_m\}\) for a subspace \(W\) of an inner product space is an orthogonal basis such that \(|| \vv_k || = 1\) for \(1\leq k\leq m\).%
\end{definition}
If \(\CB = \{\vv_1, \vv_2, \ldots, \vv_m\}\) is an orthonormal basis for a subspace \(W\) of an inner product space and \(\vx\) is a vector in \(W\), then \hyperref[x:men:eq_6_c_orth_decomp_ips]{({\xreffont\ref{x:men:eq_6_c_orth_decomp_ips}})} becomes%
\begin{equation}
\vx = \langle \vx, \vv_1 \rangle \vv_1 +  \langle \vx, \vv_2 \rangle \vv_2 + \cdots + \langle \vx, \vv_m \rangle \vv_m\text{.}\label{x:men:eq_6_c_orthnorm_decomp_ips}
\end{equation}
%
\par
Recall that we can construct an orthonormal basis from an orthogonal basis by dividing each basis vector by its magnitude.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Orthogonal Projections onto Subspaces}
\typeout{************************************************}
%
\begin{sectionptx}{Orthogonal Projections onto Subspaces}{}{Orthogonal Projections onto Subspaces}{}{}{x:section:sec_orthog_proj_subspace}
In \hyperref[x:chapter:chap_gram_schmidt]{Section~{\xreffont\ref{x:chapter:chap_gram_schmidt}}} we saw how to project a vector \(\vv\) in \(\R^n\) onto a subspace \(W\) of \(\R^n\). The same process works for vectors in any inner product space.%
\begin{definition}{}{g:definition:idp25513992}%
\index{orthogonal projection onto a subspace}%
\index{projection orthogonal to a subspace}%
Let \(W\) be a subspace of an inner product space \(V\) and let \(\CB = \{\vw_1, \vw_2, \ldots, \vw_m\}\) be an orthogonal basis for \(W\). For a vector \(\vv\) in \(V\), the \terminology{orthogonal projection of \(\vv\) onto \(W\)} is the vector%
\begin{equation*}
\proj_W \vv =  \frac{\langle \vv, \vw_1 \rangle }{\langle \vw_1, \vw_1 \rangle } \vw_1 + \frac{\langle \vv, \vw_2 \rangle }{\langle \vw_2, \vw_2 \rangle }  \vw_2 + \cdots + \frac{\langle \vv, \vw_m \rangle}{\langle \vw_m, \vw_m \rangle} \vw_m\text{.}
\end{equation*}
%
\par
The \terminology{projection of \(\vv\) orthogonal to \(W\)} is the vector%
\begin{equation*}
\proj_{W^{\perp}} \vv = \vv - \proj_W \vv\text{.}
\end{equation*}
%
\end{definition}
The notation \(\proj_{W^{\perp}} \vv\) indicates that we expect this vector to be orthogonal to every vector in \(W\).%
\begin{activity}{}{x:activity:act_6_c_orth_projection}%
In \hyperref[x:chapter:chap_gram_schmidt]{Section~{\xreffont\ref{x:chapter:chap_gram_schmidt}}} we showed that in the inner product space \(\R^n\) using the dot product as inner product, if \(W\) is a subspace of \(\R^n\) and \(\vv\) is in \(\R^n\), then \(\proj_{W^{\perp}} \vv\) is orthogonal to every vector in \(W\). In this activity we verify that same fact in an inner product space. That is, assume that \(\CB = \{\vw_1, \vw_2, \ldots, \vw_m\}\) is an orthogonal basis for a subspace \(W\) of an inner product space \(V\) and \(\vv\) is a vector in \(V\). Follow the indicated steps to show that \(\proj_{W^{\perp}} \vv\) is orthogonal to every vector in \(\CB\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(\vz\) be the projection of \(\vv\) onto \(W\). Write \(\vz\) in terms of the basis vectors in \(\CB\).%
\item{}The vector \(\vv - \vz\) is the projection of \(\vv\) orthogonal to \(W\). Let \(k\) be between \(1\) and \(m\). Use the result of part (a) to show that \(\vv - \vz\) is orthogonal to \(\vw_k\). \hyperlink{x:exercise:ex_6_c_orth_complement_basis}{Exercise~{\xreffont 16}} then shows that \(\vv - \vz\) is orthogonal to every vector in \(W\).%
\end{enumerate}
\end{activity}%
\hyperref[x:activity:act_6_c_orth_projection]{Activity~{\xreffont\ref{x:activity:act_6_c_orth_projection}}} shows that the vector \(\vv - \vw\) is orthogonal to vector for \(W\). So, in fact, \(\proj_{W^\perp} \vv\) is the projection of \(\vv\) onto the orthogonal complement of \(W\), which will be defined shortly.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Best Approximations in Inner Product Spaces}
\typeout{************************************************}
%
\begin{sectionptx}{Best Approximations in Inner Product Spaces}{}{Best Approximations in Inner Product Spaces}{}{}{x:section:sec_inner_prod_approx}
We have seen, e.g., linear regression to fit a line to a set of data, that we often want to find a vector in a subspace that ``best'' approximates a given vector in a vector space. As in \(\R^n\), the projection of a vector onto a subspace has this important property. That is, \(\proj_W \vv\) is the vector in \(W\) closest to \(\vv\) and therefore the best approximation of \(\vv\) by a vector in \(W\). To see that this is true in any inner product space, we first need a generalization of the Pythagorean Theorem that holds in inner product spaces.%
\begin{theorem}{Generalized Pythagorean Theorem.}{}{x:theorem:thm_6_c_PT}%
Let \(\vu\) and \(\vv\) be orthogonal vectors in an inner product space \(V\). Then%
\begin{equation*}
||\vu - \vv||^2 = ||\vu||^2 + ||\vv||^2\text{.}
\end{equation*}
%
\end{theorem}
\begin{proof}{}{g:proof:idp25545864}
Let \(\vu\) and \(\vv\) be orthogonal vectors in an inner product space \(V\). Then%
\begin{align*}
|| \vu - \vv ||^2 \amp = \langle \vu-\vv, \vu-\vv \rangle\\
\amp = \langle \vu,\vu \rangle - 2 \langle \vu,\vv \rangle + \langle \vv, \vv \rangle\\
\amp = \langle \vu,\vu \rangle - 2 (0) + \langle \vv, \vv \rangle\\
\amp = ||\vu||^2 + ||\vv||^2\text{.}\qedhere
\end{align*}
%
\end{proof}
Note that replacing \(\vv\) with \(-\vv\) in the theorem also shows that \(||\vu + \vv||^2 = ||\vu||^2 + ||\vv||^2\) if \(\vu\) and \(\vv\) are orthogonal.%
\par
Now we will prove that the projection of a vector \(\vu\) onto a subspace \(W\) of an inner product space \(V\) is the best approximation in \(W\) to the vector \(\vu\).%
\begin{theorem}{}{}{x:theorem:thm_6_c_best_approx}%
Let \(W\) be a subspace of an inner product space \(V\) and let \(\vu\) be a vector in \(V\). Then%
\begin{equation*}
||\vu - \proj_W \vu || \lt  || \vu - \vx ||
\end{equation*}
for every vector \(\vx\) in \(W\) different from \(\proj_W \vu\).%
\end{theorem}
\begin{proof}{}{g:proof:idp25554696}
Let \(W\) be a subspace of an inner product space \(V\) and let \(\vu\) be a vector in \(V\). Let \(\vx\) be a vector in \(W\). Now%
\begin{equation*}
\vu - \vx = (\vu-\proj_W \vu) + (\proj_W \vu - \vx)\text{.}
\end{equation*}
%
\par
Since both \(\proj_W \vu\) and \(\vx\) are in \(W\), we know that \(\proj_W \vu - \vx\) is in \(W\). Since \(\proj_{W^{\perp}} \vu = \vu-\proj_W \vu\) is orthogonal to every vector in \(W\), we have that \(\vu-\proj_W \vu\) is orthogonal to \(\proj_W \vu - \vx\). We can now use the Generalized Pythagorean Theorem to conclude that%
\begin{equation*}
||\vu - \vx||^2 = ||\vu-\proj_W \vu||^2 + ||\proj_W \vu - \vx||^2\text{.}
\end{equation*}
%
\par
Since \(\vx \neq \proj_W \vu\), it follows that \(||\proj_W \vu - \vx||^2 > 0\) and%
\begin{equation*}
|| \vu - \vx ||^2 > ||\vu - \proj_W \vu ||^2\text{.}
\end{equation*}
%
\par
Since norms are nonnegative, we can conclude that \(||\vu - \proj_W \vu || \lt || \vu - \vx ||\) as desired.%
\end{proof}
\hyperref[x:theorem:thm_6_c_best_approx]{Theorem~{\xreffont\ref{x:theorem:thm_6_c_best_approx}}} shows that the distance from \(\proj_W \vv\) to \(\vv\) is less than the distance from any other vector in \(W\) to \(\vv\). So \(\proj_W \vv\) is the best approximation to \(\vv\) of all the vectors in \(W\).%
\par
In \(\R^n\) using the dot product as inner product, if \(\vv = [v_1 \ v_2 \ v_3 \ \ldots \ v_n]^{\tr}\) and \(\proj_W \vv =  [w_1 \ w_2 \ w_3 \ \ldots \ w_n]^{\tr}\), then the square of the error in approximating \(\vv\) by \(\proj_W \vv\) is given by%
\begin{equation*}
|| \vv - \proj_W \vv ||^2 = \sum_{i=1}^n (v_i - w_i)^2\text{.}
\end{equation*}
%
\par
So \(\proj_W \vv\) minimizes this sum of squares over all vectors in \(W\). As a result, we call \(\proj_W \vv\) the \terminology{least squares approximation} to \(\vv\).%
\begin{activity}{}{g:activity:idp25565448}%
The set \(\CB = \{1, t-\frac{1}{2}, t^3-\frac{9}{10}t+\frac{1}{5}\}\) is an orthogonal basis for a subspace \(W\) of the inner product space \(\pol_3\) using the inner product \(\langle p(t),
q(t) \rangle = \int_0^1 p(t)q(t) \ dt\). Find the polynomial in \(W\) that is closest to the polynomial \(r(t) = t^2\) and give a numeric estimate of how good this approximation is.%
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Orthogonal Complements}
\typeout{************************************************}
%
\begin{sectionptx}{Orthogonal Complements}{}{Orthogonal Complements}{}{}{x:section:sec_orthog_comp_ip}
If we have a set of vectors \(S\) in an inner product space \(V\), we can define the orthogonal complement of \(S\) as we did in \(\R^n\) (see \hyperref[x:definition:def_6_a_orth_complement]{Definition~{\xreffont\ref{x:definition:def_6_a_orth_complement}}}).%
\begin{definition}{}{g:definition:idp25580296}%
\index{orthogonal complement in inner product spaces}%
The \terminology{orthogonal complement} of a subset \(S\) of an inner product space \(V\) is the set%
\begin{equation*}
S^{\perp} = \{\vv \in V : \langle \vv, \vu \rangle = 0 \text{ for all } \vu \in S\}\text{.}
\end{equation*}
%
\end{definition}
As we saw in \(\R^n\), to show that a vector is in the orthogonal complement of a subspace, it is enough to show that the vector is orthogonal to every vector in a basis for that subspace. The same is true in any inner product space. The proof is left to the exercises.%
\begin{theorem}{}{}{x:theorem:thm_6_e_ip_orth_complement_basis}%
Let \(\CB = \{\vw_1, \vw_2, \ldots, \vw_m\}\) be a basis for a subspace \(W\) of an inner product space \(V\). A vector \(\vv\) in \(V\) is orthogonal to every vector in \(W\) if and only if \(\vv\) is orthogonal to every vector in \(\CB\).%
\end{theorem}
\begin{activity}{}{g:activity:idp25582216}%
Consider \(\pol_2\) with the inner product \(\langle p(t), q(t) \rangle = \int_0^1 p(t)q(t) \ dt\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find \(\langle p(t), 1-t \rangle\) where \(p(t)=a+bt+ct^2\) is in \(\pol_2\).%
\item{}Describe as best you can the orthogonal complement of \(\Span\{1-t\}\) in \(\pol_2\). Is \(p(t)=1-2t-2t^2\) in this orthogonal complement? Is \(p(t)=1+t-t^2\)?%
\end{enumerate}
\end{activity}%
As was the case in \(\R^n\), give a subspace \(W\) of an inner product space \(V\), any vector in \(V\) can be written uniquely as a sum of a vector in \(W\) and a vector in \(W^{\perp}\).%
\begin{activity}{}{x:activity:act_6_c_decompose_ips}%
Let \(V\) be an inner product space of dimension \(n\), and let \(W\) be a subspace of \(V\). Let \(\vx\) be any vector in \(V\). We will demonstrate that \(\vx\) can be written uniquely as a sum of a vector in \(W\) and a vector in \(W^{\perp}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why \(\proj_W \vx\) is in \(W\).%
\item{}Explain why \(\proj_{W^{\perp}} \vx\) is in \(W^{\perp}\).%
\item{}Explain why \(\vx\) can be written as a sum of vectors, one in \(W\) and one in \(W^{\perp}\).%
\item{}Now we demonstrate the uniqueness of this decomposition. Suppose \(\vx = \vw+\vw_1\) and \(\vx = \vu+\vu_1\), where \(\vw\) and \(\vu\) are in \(W\) and \(\vw_1\) and \(\vu_1\) are in \(W^{\perp}\). Show that \(\vw=\vu\) and \(\vw_1 = \vu_1\), so that the representation of \(\vx\) as a sum of a vector in \(W\) and a vector in \(W^{\perp}\) is unique. (Hint: What is \(W \cap W^{\perp}\)?)%
\end{enumerate}
\end{activity}%
We summarize the result of \hyperref[x:activity:act_6_c_decompose_ips]{Activity~{\xreffont\ref{x:activity:act_6_c_decompose_ips}}}.%
\begin{theorem}{}{}{x:theorem:thm_6_c_decompose_ips}%
Let \(V\) be a finite dimensional inner product space, and let \(W\) be a subspace of \(V\). Any vector in \(V\) can be written in a unique way as a sum of a vector in \(W\) and a vector in \(W^{\perp}\).%
\end{theorem}
\hyperref[x:theorem:thm_6_c_decompose_ips]{Theorem~{\xreffont\ref{x:theorem:thm_6_c_decompose_ips}}} is useful in many applications. For example, to compress an image using wavelets, we store the image as a collection of data, then rewrite the data using a succession of subspaces and their orthogonal complements.  This new representation allows us to visualize the data in a way that compression is possible.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_inner_prod_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp25611144}%
Let \(V = \pol_3\) be the inner product space with inner product%
\begin{equation*}
\langle p(t), q(t) \rangle = \int_{-1}^1 p(t)q(t) \ dt\text{.}
\end{equation*}
%
\par
Let \(p_1(t) = 1+t\), \(p_2(t) = 1-3t\), \(p_3(t) = 3t-5t^3\), and \(p_4(t) = 1-3t^2\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that the set \(\CB = \{p_1(t), p_2(t),
p_3(t), p_4(t)\}\) is an orthogonal basis for \(V\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp25617288}{}\quad{}All calculations are done by hand or with a computer algebra system, so we leave those details to the reader.%
\par
If we show that the set \(\CB\) is an orthogonal set, then \hyperref[x:theorem:thm_6_c_Orth_li_ips]{Theorem~{\xreffont\ref{x:theorem:thm_6_c_Orth_li_ips}}} shows that \(\CB\) is linearly independent. Since \(\dim(\pol_3) = 4\), the linearly independent set \(\CB\) that contains four vectors must be a basis for \(\pol_3\). To determine if the set \(\CB\) is an orthogonal set, we must calculate the inner products of pairs of distinct vectors in \(\CB\). Since \(\langle 1+t,1-3t \rangle = 0\), \(\langle 1+t,3t-5t^3 \rangle = 0\), \(\langle 1+t,1-3t^2 \rangle = 0\), \(\langle 1-3t,3t-5t^3 \rangle = 0\), \(\langle 1-3t,1-3t^2 \rangle = 0\), and \(\langle 3t-5t^3,1-3t^2 \rangle = 0\), we conclude that \(\CB\) is an orthogonal basis for \(\pol_3\).%
\item{}\hyperref[x:theorem:thm_6_c_orth_dcomp]{Use~{\xreffont\ref{x:theorem:thm_6_c_orth_dcomp}}} to write the polynomial \(q(t) = t^2+t^3\) as a linear combination of the basis vectors in \(\CB\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp25628680}{}\quad{}All calculations are done by hand or with a computer algebra system, so we leave those details to the reader.%
\par
We can write the polynomial \(q(t) = 1+t+t^2+t^3\) as a linear combination of the basis vectors in \(\CB\) as follows:%
\begin{align*}
q(t) = \frac{\langle q(t),p_1(t) \rangle }{\langle p_1(t),p_1(t) \rangle} p_1(t) \amp + \frac{\langle q(t),p_2(t) \rangle }{\langle p_2(t),p_2(t) \rangle} p_2(t)\\
\amp + \frac{\langle q(t),p_3(t) \rangle }{\langle p_3(t),p_3(t) \rangle} p_3(t) + \frac{\langle q(t),p_4(t) \rangle }{\langle p_4(t),p_4(t) \rangle} p_4(t)\text{.}
\end{align*}
Now%
\begin{equation*}
\begin{array}{ccc} \langle q(t), p_1(t) \rangle = \frac{16}{15}, \amp \langle q(t), p_2(t) \rangle = -\frac{8}{15}, \amp \langle q(t), p_3(t) \rangle = -\frac{8}{35}, \\ \langle q(t), p_4(t) \rangle = -\frac{8}{15}, \amp \langle p_1(t), p_1(t)\rangle  = \frac{8}{3}, \amp \langle p_2(t), p_2(t) \rangle = 8, \\ \langle p_3(t), p_3(t) \rangle = \frac{8}{7}, \amp \langle p_4(t), p_4(t) \rangle = \frac{8}{5} \amp \end{array}
\end{equation*}
so%
\begin{align*}
q(t) \amp = \frac{ \frac{16}{15} }{ \frac{8}{3} } p_1(t) - \frac{ \frac{8}{15} }{ 8 } p_2(t) -  \frac{ \frac{8}{35} }{ \frac{8}{7} } p_3(t) -  \frac{ \frac{8}{15} }{ \frac{8}{5} } p_4(t)\\
\amp \frac{2}{5} p_1(t) - \frac{1}{15}p_2(t) - \frac{1}{5} p_3(t) - \frac{1}{3} p_4(t)\text{.}
\end{align*}
%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp25623688}%
Let \(V\) be the inner product space \(\R^4\) with inner product defined by%
\begin{equation*}
\langle [u_1 \ u_2 \ u_3 \ u_4]^{\tr}, [v_1 \ v_2 \ v_3 \ v_4]^{\tr} \rangle = u_1v_1 + 2u_2v_2 + 3u_3v_3 + 4u_4v_4\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(W\) be the plane spanned by \([-1 \ 1\ 0 \ 1]^{\tr}\) and \([6 \ 1 \ 7 \ 1]^{\tr}\) in \(V\). Find the vector in \(W\) that is closest to the vector \([2 \ 0 \ -1 \ 3]^{\tr}\). Exactly how close is your best approximation to the vector \([2 \ 0 \ -1 \ 3]^{\tr}\)?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp25633032}{}\quad{}The vector we're looking for is the projection of \([2 \ 0 \ -1 \ 3]^{\tr}\) onto the plane. A spanning set for the plane is \(\CB = \{[-1 \ 1 \ 0 \ 1]^{\tr}, [6 \ 1 \ 7 \ 1]^{\tr}\}\). Neither vector in \(\CB\) is a scalar multiple of the other, so \(\CB\) is a basis for the plane. Since%
\begin{equation*}
\langle [-1 \ 1 \ 0 \ 1]^{\tr}, [6 \ 1 \ 7 \ 1]^{\tr} \rangle = -6 + 2 + 0 + 4 =  0\text{,}
\end{equation*}
the set \(\CB\) is an orthogonal basis for the plane. The projection of the vector \(\vv = [2 \ 0 \ -1 \ 3]^{\tr}\) onto the plane spanned by \(\vw_1 =  [-1 \ 1 \ 0 \ 1]^{\tr}\) and \(\vw_2 = [6 \ 1 \ 7 \ 1]^{\tr}\) is given by%
\begin{align*}
\frac{\langle \vv, \vw_1\rangle}{\langle \vw_1, \vw_1\rangle } \vw_1 + \frac{\langle \vv, \vw_2 \rangle }{\langle \vw_2, \vw_2 \rangle}\vw_2 \amp = \frac{10}{7} [-1 \ 1\ 0 \ 1]^{\tr} + \frac{3}{189}  [6 \ 1 \ 7 \ 1]^{\tr}\\
\amp = \frac{1}{189}[-252 \ 273 \ 21 \ 273]{\tr}\\
\amp = \left[ -\frac{4}{3} \ \frac{13}{9} \ \frac{1}{9} \ \frac{13}{9} \right]\text{.}
\end{align*}
To measure how close close \(\left[ -\frac{4}{3} \ \frac{13}{9} \ \frac{1}{9} \ \frac{13}{9} \right]\) is to \([2 \ 0 \ -1 \ 3]^{\tr}\), we calculate%
\begin{align*}
\left|\left| \left[ -\frac{4}{3} \ \frac{13}{9} \ \frac{1}{9} \ \frac{13}{9} \right]   - [2 \ 0 \ -1 \ 3]^{\tr} \right|\right| \amp = \left|\left| \left[-\frac{10}{3} \ \frac{13}{9} \ \frac{10}{9} \ -\frac{14}{9} \right] \right| \right|\\
\amp = \sqrt{ \frac{100}{9} + \frac{338}{81} + \frac{300}{81} + \frac{784}{81}}\\
\amp = \frac{1}{9} \sqrt{2322}\\
\amp \approx 5.35\text{.}
\end{align*}
%
\item{}Express the vector \([2 \ 0 \ -1 \ 3]^{\tr}\) as the sum of a vector in \(W\) and a vector orthogonal to \(W\).%
\par
If \(\vv = [2 \ 0 \ -1 \ 3]^{\tr}\), then \(\proj_{W} \vv\) is in \(W\) and%
\begin{equation*}
\proj_{W^{\perp}} \vv = \vv - \proj_{W} \vv =  \left[-\frac{10}{3} \ \frac{13}{9} \ \frac{10}{9} \ -\frac{14}{9} \right]
\end{equation*}
is in \(W^{\perp}\), and \(\vv = \proj_{W} \vv + \proj_{W^{\perp}} \vv\).%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_inner_prod_summ}
%
\begin{itemize}[label=\textbullet]
\item{}An inner product \(\langle \ , \ \rangle\) on a vector space \(V\) is a mapping from \(V \times V \to \R\) satisfying%
\begin{enumerate}
\item{}\(\langle \vu , \vv \rangle = \langle \vv , \vu \rangle\) for all \(\vu\) and \(\vv\) in \(V\),%
\item{}\(\langle \vu + \vv , \vw \rangle = \langle \vu , \vw \rangle + \langle \vv , \vw \rangle\) for all \(\vu\), \(\vv\), and \(\vw\) in \(V\),%
\item{}\(\langle c\vu , \vv \rangle = c\langle \vu , \vv \rangle\) for all \(\vu\), \(\vv\) in \(V\) and \(c \in \R\),%
\item{}\(\langle \vu , \vu \rangle \geq 0\) for all \(\vu\) in \(V\) and \(\langle \vu , \vu \rangle = 0\) if and only if \(\vu = \vzero\).%
\end{enumerate}
%
\item{}An inner product space is a pair \(V\), \(\langle \ , \ \rangle\) where \(V\) is a vector space and \(\langle \ , \ \rangle\) is an inner product on \(V\).%
\item{}The length of a vector \(\vv\) in an inner product space \(V\) is defined to be the real number \(||\vv|| = \sqrt{\langle \vv, \vv \rangle}\).%
\item{}The distance between two vectors \(\vu\) and \(\vv\) in an inner product space \(V\) is the scalar \(|| \vu - \vv ||\).%
\item{}The angle \(\theta\) between two vectors \(\vu\) and \(\vv\) is the angle which satisfies \(0\leq \theta \leq \pi\) and%
\begin{equation*}
\cos(\theta) = \frac{\langle \vu, \vv \rangle}{||\vu|| ||\vv||} \,\text{.}
\end{equation*}
%
\item{}Two vectors \(\vu\) and \(\vv\) in an inner product space \(V\) are orthogonal if \(\langle \vu, \vv \rangle = 0\).%
\item{}A subset \(S\) of an inner product space is an orthogonal set if \(\langle \vu, \vv \rangle = 0\) for all \(\vu \neq \vv\) in \(S\).%
\item{}A basis for a subspace of an inner product space is an orthogonal basis if the basis is also an orthogonal set.%
\item{}Let \(\CB = \{\vv_1, \vv_2, \ldots, \vv_m\}\) be an orthogonal basis for a subspace of an inner product space \(V\). Let \(\vx\) be a vector in \(W\). Then%
\begin{equation*}
\vx = \sum_{i=1}^m c_i \vv_i\text{,}
\end{equation*}
where%
\begin{equation*}
c_i = \frac{\langle \vx, \vv_i \rangle}{\langle \vv_i, \vv_i \rangle}
\end{equation*}
for each \(i\).%
\item{}An orthogonal basis \(\CB = \{\vv_1, \vv_2, \ldots, \vv_m\}\) for a subspace \(W\) of an inner product space \(V\) is an orthonormal basis if \(|| \vv_k || = 1\) for each \(k\) from 1 to \(m\).%
\item{}If \(\CB = \{\vw_1, \vw_2, \ldots, \vw_m\}\) is an orthogonal basis for \(V\) and \(\vx \in V\), then%
\begin{equation*}
[\vx]_{\CB} = \left[ \begin{array}{c} \frac{\langle \vx, \vw_1 \rangle}{\langle \vw_1, \vw_1 \rangle} \\ \frac{\langle \vx, \vw_2 \rangle}{\langle \vw_2, \vw_2 \rangle} \\ \vdots \\ \frac{\langle \vx, \vw_m \rangle}{\langle \vw_m, \vw_m \rangle} \end{array}  \right]\text{.}
\end{equation*}
%
\item{}The projection of the vector \(\vv\) in an inner product space \(V\) onto a subspace \(W\) of \(V\) is the vector%
\begin{equation*}
\proj_W \vv =  \frac{\langle \vv, \vw_1 \rangle }{\langle \vw_1, \vw_1 \rangle } \vw_1 + \frac{\langle \vv, \vw_2 \rangle }{\langle \vw_2, \vw_2 \rangle }  \vw_2 + \cdots + \frac{\langle \vv, \vw_m \rangle}{\langle \vw_m, \vw_m \rangle} \vw_m\text{,}
\end{equation*}
where \(\{\vw_1, \vw_2, \ldots, \vw_m\}\) is an orthogonal basis of \(W\). Projections are important in that \(\proj_W \vv\) is the best approximation of the vector \(\vv\) by a vector in \(W\) in the least squares sense.%
\item{}With \(W\) as in (a), the projection of \(\vv\) orthogonal to \(W\) is the vector%
\begin{equation*}
\proj_{W^{\perp}} \vv = \vv - \proj_W \vv\text{.}
\end{equation*}
The norm of \(\proj_{W^{\perp}} \vv\) provides a measure of how well \(\proj_W \vv\) approximates the vector \(\vv\).%
\item{}The orthogonal complement of a subset \(S\) of an inner product space \(V\) is the set%
\begin{equation*}
S^{\perp} = \{\vv \in V : \langle \vv, \vu \rangle = 0 \text{ for all }  \vu \in S\}\text{.}
\end{equation*}
%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_inner_prod_exer}
\begin{divisionexercise}{1}{}{}{x:exercise:ex_6_c_Cab}%
Let \(C[a,b]\) be the set of all continuous real valued functions on the interval \([a,b]\). If \(f\) is in \(C[a,b]\), we can extend \(f\) to a continuous function from \(\R\) to \(\R\) by letting \(F\) be the function defined by%
\begin{equation*}
F(x) = \begin{cases}f(a) \amp \text{ if }  x \lt  a \\ f(x) \amp \text{ if }  a \leq x \leq b \\ f(b) \amp  \text{ if }  b \lt  x \end{cases}\text{.}
\end{equation*}
In this way we can view \(C[a,b]\) as a subset of \(\F\), the vector space of all functions from \(\R\) to \(\R\). Verify that \(C[a,b]\) is a vector space.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25702920}{}\quad{}Use properties of continuous functions.%
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{x:exercise:ex_6_c_inner_products}%
Use the definition of an inner product to determine which of the following defines an inner product on the indicated space. Verify your answers.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\langle \vu, \vv \rangle = u_1v_1 - u_2v_1 - u_1v_2 + 3u_2v_2\) for \(\vu = [u_1 \ u_2]^{\tr}\) and \(\vv = [v_1 \ v_2]^{\tr}\) in \(\R^2\)%
\item{}\(\langle f, g \rangle = \int_a^b f(x)g(x) \ dx\) for \(f,g \in C[a,b]\) (where \(C[a,b]\) is the vector space of all continuous functions on the interval \([a,b]\))%
\item{}\(\langle f, g \rangle = f'(0)g'(0)\) for \(f,g \in D(-1,1)\) (where \(D(a,b)\) is the vector space of all differentiable functions on the interval \((a,b)\))%
\item{}\(\langle \vu, \vv \rangle = (A\vu) \cdot (A\vv)\) for \(\vu, \vv \in \R^n\) and \(A\) an invertible \(n \times n\) matrix%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp25705992}%
We can sometimes visualize an inner product in \(\R^2\) or \(\R^3\) (or other spaces) by describing the unit circle \(S^1\), where%
\begin{equation*}
S^1 = \{ \vv \in V : || \vv || = 1\}
\end{equation*}
in that inner product space. For example, in the inner product space \(\R^2\) with the dot product as inner product, the unit circle is just our standard unit circle. Inner products, however, can distort this familiar picture of the unit circle. Describe the points on the unit circle \(S^1\) in the inner product space \(\R^2\) with inner product \(\langle [u_1 \ u_2], [v_1 \ v_2] \rangle = 2u_1v_1 + 3u_2v_2\) using the following steps.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(\vx = [x \ y] \in \R^2\). Set up an equation in \(x\) and \(y\) that is equivalent to the vector equation \(|| \vx || = 1\).%
\item{}Describe the graph of the equation you found in \(\R^2\). It should have a familiar form. Draw a picture to illustrate. What do you think of calling this graph a ``circle''?%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{x:exercise:ex_6_c_ip_1}%
Define \(\langle \ , \ \rangle\) on \(\R^2\) by \(\langle [u_1 \ u_2]^\tr , [v_1 \ v_2]^\tr \rangle = 4u_1v_1+2u_2v_2\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(\langle \ , \ \rangle\) is an inner product.%
\item{}The inner product \(\langle \ , \ \rangle\) can be represented as a matrix transformation \(\langle \vu, \vv \rangle = \vu^{\tr} A \vv\), where \(\vu\) and \(\vv\) are written as column vectors. Find a matrix \(A\) that represents this inner product.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{x:exercise:ex_6_c_ip_2}%
This exercise is a generalization of \hyperlink{x:exercise:ex_6_c_ip_1}{Exercise~{\xreffont 4}}. Define \(\langle \ ,  \ \rangle\) on \(\R^n\) by%
\begin{equation*}
\langle [u_1 \ u_2 \ \cdots \ u_n]^{\tr},  [v_1 \ v_2 \ \cdots \ v_n]^{\tr} \rangle = a_1u_1v_1+a_2u_2v_2+ \cdots + a_nu_nv_n
\end{equation*}
for some positive scalars \(a_1\), \(a_2\), \(\ldots\), \(a_n\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(\langle \ , \ \rangle\) is an inner product.%
\item{}The inner product \(\langle \ , \ \rangle\) can be represented as a matrix transformation \(\langle \vu, \vv \rangle = \vu^{\tr} A \vv\), where \(\vu\) and \(\vv\) are written as column vectors. Find a matrix \(A\) that represents this inner product.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp25739032}%
Is the sum of two inner products on an inner product space \(V\) an inner product on \(V\)? If yes, prove it. If no, provide a counterexample. (By the sum of inner products we mean a function \(\langle \ , \ \rangle\) satisfying%
\begin{equation*}
\langle \vu, \vv \rangle = \langle \vu, \vv \rangle_1 + \langle \vu, \vv \rangle_2
\end{equation*}
for all \(\vu\) and \(\vv\) in \(V\), where \(\langle \ , \ \rangle_1\) and \(\langle \ , \ \rangle_2\) are inner products on \(V\).)%
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp25749016}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Does \(\langle \vu, \vv \rangle = \vu^{\tr}A\vv\) define an inner product on \(\R^n\) for every \(n \times n\) matrix \(A\)? Verify your answer.%
\item{}If your answer to part (a) is no, are there any types of matrices for which \(\langle \vu, \vv \rangle = \vu^{\tr}A\vv\) defines an inner product?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25750040}{}\quad{}See \hyperlink{x:exercise:ex_6_c_ip_1}{Exercise~{\xreffont 4}} and \hyperlink{x:exercise:ex_6_c_ip_2}{Exercise~{\xreffont 5}}.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{x:exercise:ex_6_c_trace}%
The trace of an \(n \times n\) matrix \(A = [a_{ij}]\) has some useful properties.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(\trace(A+B) = \trace(A) + \trace(B)\) for any \(n \times n\) matrices \(A\) and \(B\).%
\item{}Show that \(\trace(cA) = c \trace(A)\) for any \(n \times n\) matrix \(A\) and any scalar \(c\).%
\item{}Show that \(\trace\left(A^{\tr}\right) = \trace(A)\) for any \(n \times n\) matrix.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp25764248}%
Let \(V\) be an inner product space and \(\vu, \vv\) be two vectors in \(V\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Check that if \(\vv=\vzero\), the Cauchy-Schwarz inequality%
\begin{equation*}
|\langle \vu, \vv \rangle | \leq ||\vu || ||\vv ||
\end{equation*}
holds.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25761304}{}\quad{}Evaluate each side of the inequality.%
\item{}Assume \(\vv \neq \vzero\). Let \(\lambda= \langle \vu, \vv \rangle / ||\vv||^2\) and \(\vw=\vu - \lambda \vv\). Use the fact that \(||\vw||^2 \geq 0\) to conclude the Cauchy-Schwarz inequality in this case.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25766680}{}\quad{}Write \(||\vw||^2\) as an inner product and expand.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{10}{}{}{x:exercise:ex_6_c_Frobenius}%
The \terminology{Frobenius inner product} is defined as%
\begin{equation*}
\langle A, B \rangle = \trace\left(AB^{\tr}\right)\text{.}
\end{equation*}
for \(n \times n\) matrices \(A\) and \(B\). Verify that \(\langle A, B \rangle\) defines an inner product on \(\M_{n \times n}\).%
\end{divisionexercise}%
\begin{divisionexercise}{11}{}{}{g:exercise:idp25773592}%
Let \(A = [a_{ij}]\) and \(B = [b_{ij}]\) be two \(n \times n\) matrices.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that if \(n=2\), then the Frobenius inner product (see \hyperlink{x:exercise:ex_6_c_Frobenius}{Exercise~{\xreffont 10}}) of \(A\) and \(B\) is%
\begin{equation*}
\langle A, B \rangle = a_{11}b_{11} + a_{12}b_{12} + a_{21}b_{21} + a_{22}b_{22}\text{.}
\end{equation*}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25770648}{}\quad{}Expand the inner product.%
\item{}Extend part (a) to the general case. That is, show that for an arbitrary \(n\),%
\begin{equation*}
\langle A, B \rangle = \sum_{i=1}^n \sum_{j=1}^n a_{ij}b_{ij}\text{.}
\end{equation*}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25780376}{}\quad{}Expand the inner product.%
\item{}Compare the Frobenius inner product to the scalar product of two vectors.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25778072}{}\quad{}Convert \(A\) and \(B\) to vectors in \(\R^{n^2}\) whose entries are the entries in the first row followed by the entries in the second row and so on.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{12}{}{}{g:exercise:idp25780504}%
Let \(\CB = \{[1 \ 1 \ 1]^{\tr}, [1 \ -1 \ 0]^{\tr}\}\) and let \(W = \Span \ \CB\) in \(\R^3\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(\CB\) is an orthogonal basis for \(W\), using the dot product as inner product.%
\item{}Explain why the vector \(\vv = [0 \ 2 \ 2]^{\tr}\) is not in \(W\).%
\item{}Find the vector in \(W\) that is closest to \(\vv\). How close is this vector to \(\vv\)?%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{13}{}{}{g:exercise:idp25792536}%
Let \(\R^3\) be the inner product space with inner product%
\begin{equation*}
\langle [u_1 \ u_2 \ u_3]^{\tr}, [v_1 \ v_2 \ v_3]^{\tr}\rangle = u_1v_1 + 2u_2v_2 + u_3v_3\text{.}
\end{equation*}
Let \(\CB = \{[1 \ 1 \ 1]^{\tr}, [1 \ -1 \ 1]^{\tr}\}\) and let \(W = \Span \ \CB\) in \(\R^3\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(\CB\) is an orthogonal basis for \(W\), using the given inner product.%
\item{}Explain why the vector \(\vv = [0 \ 2 \ 2]^{\tr}\) is not in \(W\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25790616}{}\quad{}Try to write \(\vv\) in terms of the basis vectors for \(W\).%
\item{}Find the vector in \(W\) that is closest to \(\vv\). How close is this vector to \(\vv\)?%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{14}{}{}{g:exercise:idp25796888}%
Let \(\pol_2\) be the inner product space with inner product%
\begin{equation*}
\langle p(t), q(t) \rangle = \int_0^1 p(t)q(t) \ dt\text{.}
\end{equation*}
Let \(\CB = \{1, 1-2t\}\) and let \(W = \Span \ \CB\) in \(\pol_2\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(\CB\) is an orthogonal basis for \(W\), using the given inner product.%
\item{}Explain why the polynomial \(q(t) = t^2\) is not in \(W\).%
\item{}Find the vector in \(W\) that is closest to \(q(t)\). How close is this vector to \(q(t)\)?%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{15}{}{}{g:exercise:idp25802776}%
Prove the remaining properties of \hyperref[x:theorem:thm_6_c_inner_product_properties]{Theorem~{\xreffont\ref{x:theorem:thm_6_c_inner_product_properties}}}. That is, if \(\langle \ , \ \rangle\) is an inner product on a vector space \(V\) and \(\vu, \vv\), and \(\vw\) are vectors in \(V\) and \(c\) is any scalar, then%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\langle \vzero , \vv \rangle = \langle \vv , \vzero \rangle = 0\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25805208}{}\quad{}Use the fact that \(\vzero = \vzero + \vzero\).%
\item{}\(\langle \vu , c\vv \rangle = c\langle \vu , \vv \rangle\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25803288}{}\quad{}Use the fact that \(\langle \vu, \vv \rangle = \langle \vv, \vu \rangle\).%
\item{}\(\langle \vv+\vw , \vu \rangle = \langle \vv , \vu \rangle + \langle \vw , \vu \rangle\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25807512}{}\quad{}Same hint as part (b).%
\item{}\(\langle \vu - \vv, \vw \rangle = \langle \vw, \vu - \vv \rangle= \langle \vu , \vw \rangle - \langle \vv , \vw \rangle= \langle \vw, \vu \rangle - \langle \vw, \vv \rangle\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25814808}{}\quad{}Use the fact that \(\vu - \vv = \vu + (-\vv)\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{16}{}{}{x:exercise:ex_6_c_orth_complement_basis}%
Prove the following theorem referenced in \hyperref[x:activity:act_6_c_orth_projection]{Activity~{\xreffont\ref{x:activity:act_6_c_orth_projection}}}. \begin{theorem}{}{}{x:theorem:thm_6_a_orth_complement_basis}%
Let \(\CB = \{\vw_1, \vw_2, \ldots, \vw_m\}\) be a basis for a subspace \(W\) of an inner product space \(V\). A vector \(\vv\) in \(V\) is orthogonal to every vector in \(W\) if and only if \(\vv\) is orthogonal to every vector in \(\CB\).%
\end{theorem}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25816216}{}\quad{}Mimic \hyperref[x:theorem:thm_6_a_orth_complement_basis]{Theorem~{\xreffont\ref{x:theorem:thm_6_a_orth_complement_basis}}}.%
\end{divisionexercise}%
\begin{divisionexercise}{17}{}{}{g:exercise:idp25816344}%
Prove \hyperref[x:theorem:thm_6_c_Orth_li_ips]{Theorem~{\xreffont\ref{x:theorem:thm_6_c_Orth_li_ips}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25819160}{}\quad{}Mimic \hyperref[x:theorem:thm_6_b_Orth_li]{Theorem~{\xreffont\ref{x:theorem:thm_6_b_Orth_li}}}.%
\end{divisionexercise}%
\begin{divisionexercise}{18}{}{}{x:exercise:ex_6_c_all_ips}%
Let \(V\) be a vector space with basis \(\{\vv_1, \vv_2, \ldots, \vv_n\}\). Define \(\langle \ , \ \rangle\) as follows:%
\begin{equation*}
\langle \vu, \vw \rangle = \sum_{i=1}^n u_iw_i
\end{equation*}
if \(\vu = \sum_{i=1}^n u_i \vv_i\) and \(\vw = \sum_{i=1}^n w_i \vv_i\) in \(V\). (Since the representation of a vector as a linear combination of basis elements is unique, this mapping is well-defined.) Show that \(\langle \ , \ \rangle\) is an inner product on \(V\) and conclude that any finite dimensional vector space can be made into an inner product space.%
\end{divisionexercise}%
\begin{divisionexercise}{19}{}{}{g:exercise:idp25824280}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
An inner product on a vector space \(V\) is a function from \(V\) to the real numbers.%
\item{}\lititle{True\slash{}False.}\par%
If \(\langle \ , \ \rangle\) is an inner product on a vector space \(V\), and if \(\vv\) is a vector in \(V\), then the set \(W = \{\vx \in V : \langle \vx,\vv \rangle = 0\}\) is a subspace of \(V\).%
\item{}\lititle{True\slash{}False.}\par%
There is exactly one inner product on each inner product space.%
\item{}\lititle{True\slash{}False.}\par%
If \(\vx\), \(\vy\), and \(\vz\) are vectors in an inner product space with \(\langle \vx, \vz \rangle = \langle \vy, \vz \rangle\), then \(\vx = \vy\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\langle \vx, \vy \rangle = 0\) for all vectors \(\vy\) in an inner product space \(V\), then \(\vx = \vzero\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\vu\) and \(\vv\) are vectors in an inner product space and the distance from \(\vu\) to \(\vv\) is the same as the distance from \(\vu\) to \(-\vv\), then \(\vu\) and \(\vv\) are orthogonal.%
\item{}\lititle{True\slash{}False.}\par%
If \(W\) is a subspace of an inner product space and a vector \(\vv\) is orthogonal to every vector in a basis of \(W\), then \(\vv\) is in \(W^{\perp}\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\{\vv_1, \vv_2, \vv_3\}\) is an orthogonal basis for an inner product space \(V\), then so is \(\{c\vv_1, \vv_2, \vv_3\}\) for any nonzero scalar \(c\).%
\item{}\lititle{True\slash{}False.}\par%
An inner product \(\langle \vu, \vv \rangle\) in an inner product space \(V\) results in another vector in \(V\).%
\item{}\lititle{True\slash{}False.}\par%
An inner product in an inner product space \(V\) is a function that maps pairs of vectors in \(V\) to the set of non-negative real numbers.%
\item{}\lititle{True\slash{}False.}\par%
The vector space of all \(n \times n\) matrices can be made into an inner product space.%
\item{}\lititle{True\slash{}False.}\par%
Any non-zero multiple of an inner product on space \(V\) is also an inner product on \(V\).%
\item{}\lititle{True\slash{}False.}\par%
Every set of \(k\) non-zero orthogonal vectors in a vector space \(V\) of dimension \(k\) is a basis for \(V\).%
\item{}\lititle{True\slash{}False.}\par%
For any finite-dimensional inner product space \(V\) and a subspace \(W\) of \(V\), \(W\) is a subspace of \((W^\perp)^\perp\).%
\item{}\lititle{True\slash{}False.}\par%
If \(W\) is a subspace of an inner product space, then \(W\cap W^\perp = \{\vzero\}\).%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Fourier Series and Musical Tones}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Fourier Series and Musical Tones}{}{Project: Fourier Series and Musical Tones}{}{}{x:section:sec_proj_fourier}
Joseph Fourier first studied trigonometric polynomials to understand the flow of heat in metallic plates and rods. The resulting series, called Fourier series, now have applications in a variety of areas including electrical engineering, vibration analysis, acoustics, optics, signal processing, image processing, geology, quantum mechanics, and many more. For our purposes, we will focus on synthesized music.%
\par
Pure musical tones are periodic sine waves. Simple electronic circuits can be designed to generate alternating current. Alternating current is current that is periodic, and hence is described by a combination of \(\sin(kx)\) and \(\cos(kx)\) for integer values of \(k\). To synthesize an instrument like a violin, we can project the instrument's tones onto trigonometric polynomials \textemdash{} and then we can produce them electronically. As we will see, these projections are least squares approximations onto certain vector spaces. The website \href{http://www.falstad.com/fourier/}{\nolinkurl{falstad.com/fourier/}} provides a tool for hearing sounds digitally created by certain functions. For example, you can listen to the sound generated by a sawtooth function \(f\) of the form%
\begin{equation*}
f(x) = \begin{cases}x \amp \text{ if }  -\pi \lt  x \leq \pi, \\ f(x-2\pi),   \amp  \text{ if }  \pi \lt  x, \\ f(x+2\pi),   \amp  \text{ if }  x \leq -\pi. \end{cases}
\end{equation*}
%
\par
Try out some of the tones on this website (click on the Sound button to hear the tones). You can also alter the tones by clicking on any one of the white dots and moving it up or down. and play with the buttons. We will learn much about what this website does in this project.%
\par
\index{trigonometric polynomial} Pure tones are periodic and so are modeled by trigonometric functions. In general, trigonometric polynomials can be used to produce good approximations to periodic phenomena. A \terminology{trigonometric polynomial} is an object of the form%
\begin{align*}
c_0 + c_1 \cos(x) + d_1\sin(x) \amp + c_2\cos(2x) + d_2\sin(2x) + \cdots\\
\amp + c_n \cos(nx) + d_n \sin(nx) + \cdots\text{,}
\end{align*}
where the \(c_i\) and \(d_j\) are real constants. With judicious choices of these constants, we can approximate periodic and other behavior with trigonometric polynomials. The first step for us will be to understand the relationships between the summands of these trigonometric polynomials in the inner product space \(C[-\pi, \pi]\)\footnote{With suitable adjustments, we can work over any interval that is convenient, but for the sake of simplicity in this project, we will restrict ourselves to the interval \([-\pi, \pi]\).\label{g:fn:idp25873944}} of continuous functions from \([\pi, \pi]\) to \(\R\) with the inner product%
\begin{equation}
\langle f,g \rangle = \frac{1}{\pi} \int_{-\pi}^{\pi} f(x)g(x) \ dx\text{.}\label{x:men:eq_ip_fourier}
\end{equation}
%
\par
Our first order of business is to verify that \hyperref[x:men:eq_ip_fourier]{({\xreffont\ref{x:men:eq_ip_fourier}})} is, in fact, an inner product.%
\begin{project}{}{x:project:act_Fourier_inner_product}%
Let \(C[a,b]\) be the set of continuous real-valued functions on the interval \([a,b]\). In \hyperlink{x:exercise:ex_6_c_Cab}{Exercise~{\xreffont 1}} in \hyperref[x:chapter:chap_inner_products]{Section~{\xreffont\ref{x:chapter:chap_inner_products}}} we are asked to show that \(C[a,b]\) is a vector space, while \hyperlink{x:exercise:ex_6_c_inner_products}{Exercise~{\xreffont 2}} in \hyperref[x:chapter:chap_inner_products]{Section~{\xreffont\ref{x:chapter:chap_inner_products}}} asks us to show that \(\langle f,g \rangle = \int_a^b f(x)g(x) \, dx\) defines an inner product on \(C[a,b]\). However, \hyperref[x:men:eq_ip]{({\xreffont\ref{x:men:eq_ip}})} is slightly different than this inner product. Show that any positive scalar multiple of an inner product is an inner product, and conclude that \hyperref[x:men:eq_ip_fourier]{({\xreffont\ref{x:men:eq_ip_fourier}})} defines an inner product on \(C[-\pi,\pi]\). (We will see why we introduce the factor of \(\frac{1}{\pi}\) later.)%
\end{project}%
Now we return to our inner product space \(C[-\pi,\pi]\) with inner product \hyperref[x:men:eq_ip_fourier]{({\xreffont\ref{x:men:eq_ip_fourier}})}. Given a function \(g\) in \(C[-\pi,\pi]\), we approximate \(g\) using only a finite number of the terms in a trigonometric polynomial. Let \(W_n\) be the subspace of \(C[-\pi,\pi]\) spanned by the functions%
\begin{equation*}
1, \cos(x), \sin(x), \cos(2x), \sin(2x), \ldots, \cos(nx), \sin(nx)\text{.}
\end{equation*}
%
\par
One thing we need to know is the dimension of \(W_n\).%
\begin{project}{}{x:project:act_Fourier_W1}%
We start with the initial case of \(W_1\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show directly that the functions \(1\), \(\cos(x)\), and \(\sin(x)\) are orthogonal.%
\item{}What is the dimension of \(W_1\)? Explain.%
\end{enumerate}
\end{project}%
Now we need to see if what happened in \hyperref[x:project:act_Fourier_W1]{Project Activity~{\xreffont\ref{x:project:act_Fourier_W1}}} happens in general. A few tables of integrals and some basic facts from trigonometry can help.%
\begin{project}{}{x:project:act_Fourier_Wn}%
A table of integrals shows the following for \(k \neq m\) (up to a constant):%
\begin{align}
\int \cos(mx)\cos(kx) \ dx \amp = \frac{1}{2} \left( \frac{\sin((k-m)x)}{k-m} + \frac{\sin((k+m)x)}{k+m} \right)\label{x:mrow:eq_cos_cos_int}\\
\int \sin(mx)\sin(kx) \ dx \amp = \frac{1}{2} \left( \frac{\sin((k-m)x)}{k-m} - \frac{\sin((k+m)x)}{k+m} \right)\label{x:mrow:eq_sin_sin_int}\\
\int \cos(mx)\sin(kx) \ dx \amp = \frac{1}{2} \left( \frac{\cos((m-k)x)}{m-k} - \frac{\sin((m+k)x)}{m+k} \right)\label{x:mrow:eq_cos_sin_int1}\\
\int \cos(mx)\sin(mx) \ dx \amp = -\frac{1}{2m} \cos^2(mx)\label{x:mrow:eq_cos_sin_int2}
\end{align}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Use \hyperref[x:mrow:eq_cos_cos_int]{({\xreffont\ref{x:mrow:eq_cos_cos_int}})} to show that \(\cos(mx)\) and \(\cos(kx)\) are orthogonal in \(C[-\pi,\pi]\) if \(k \neq m\).%
\item{}Use \hyperref[x:mrow:eq_sin_sin_int]{({\xreffont\ref{x:mrow:eq_sin_sin_int}})} to show that \(\sin(mx)\) and \(\sin(kx)\) are orthogonal in \(C[-\pi,\pi]\) if \(k \neq m\).%
\item{}Use \hyperref[x:mrow:eq_cos_sin_int1]{({\xreffont\ref{x:mrow:eq_cos_sin_int1}})} to show that \(\cos(mx)\) and \(\sin(kx)\) are orthogonal in \(C[-\pi,\pi]\) if \(k \neq m\).%
\item{}Use \hyperref[x:mrow:eq_cos_sin_int2]{({\xreffont\ref{x:mrow:eq_cos_sin_int2}})} to show that \(\cos(mx)\) and \(\sin(mx)\) are orthogonal in \(C[-\pi,\pi]\).%
\item{}What is \(\dim(W_n)\)? Explain.%
\end{enumerate}
\end{project}%
Once we have an orthogonal basis for \(W_n\), we might want to create an orthonormal basis for \(W_n\). Throughout the remainder of this project, unless otherwise specified, you should use a table of integrals or any appropriate technological tool to find integrals for any functions you need.%
\begin{project}{}{x:project:act_Fourier_basis}%
Show that the set%
\begin{equation*}
\CB_n = \left\{\frac{1}{\sqrt{2}}, \cos(x), \cos(2x), \ldots, \cos(nx), \sin(x), \sin(2x), \ldots, \sin(nx)\right\}
\end{equation*}
is an orthonormal basis for \(W_n\). Use the fact that the norm of a vector \(\vv\) in an inner product space with inner product \(\langle \ , \ \rangle\) is defined to be \(\sqrt{\langle \vv, \vv \rangle}\). (This is where the factor of \(\frac{1}{\pi}\) will be helpful.)%
\end{project}%
Now we need to recall how to find the best approximation to a vector by a vector in a subspace, and apply that idea to approximate an arbitrary function \(g\) with a trigonometric polynomial in \(W_n\). Recall that the best approximation of a function \(g\) in \(C[-\pi,\pi]\) is the projection of \(g\) onto \(W_n\). If we have an orthonormal basis \(\{h_0, h_1, h_2, \ldots,
h_{2n}\}\) of \(W_n\), then the projection of \(g\) onto \(W_n\) is%
\begin{equation*}
\proj_{W_n} g = \langle g,h_0 \rangle h_0 + \langle g,h_1 \rangle h_1 + \langle g,h_2 \rangle h_2 + \ldots + \langle g,h_{2n} \rangle h_{2n}\text{.}
\end{equation*}
%
\par
With this idea, we can find formulas for the coefficients when we project an arbitrary function onto \(W_n\).%
\begin{project}{}{x:project:act_Fourier_projection}%
\index{Fourier coefficients}%
If \(g\) is an arbitrary function in \(C[-\pi,\pi]\), we will write the projection of \(g\) onto \(W_n\) as%
\begin{align*}
a_0\left(\frac{1}{\sqrt{2}}\right) + a_1 \cos(x) + b_1\sin(x) + a_2 \cos(2x) \amp + b_2 \sin(2x) + \cdots\\
\amp + a_n \cos(nx) + b_n \sin(nx)\text{.}
\end{align*}
%
\par
The \(a_i\) and \(b_j\) are the \terminology{Fourier coefficients} for \(f\). The expression \(a_n\cos(nx)+b_n\sin(nx)\) is called the \(n\)th \terminology{harmonic} of \(g\). The first harmonic is called the \terminology{fundamental frequency}. The human ear cannot hear tones whose frequencies exceed 20000 Hz, so we only hear finitely many harmonics (the projections onto \(W_n\) for some \(n\)).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that%
\begin{equation}
a_0 =  \frac{1}{\sqrt{2}\pi}\int_{-\pi}^{\pi} g(x) \ dx\text{.}\label{x:men:eq_a0}
\end{equation}
Explain why \(\frac{a_0}{\sqrt{2}}\) gives the average value of \(g\) on \([-\pi,\pi]\). You may want to go back and review average value from calculus. This is saying that the best constant approximation of \(g\) on \([-\pi,\pi]\) is its average value, which makes sense.%
\item{}Show that for \(m \geq 1\),%
\begin{equation}
a_m =  \frac{1}{\pi}\int_{-\pi}^{\pi} g(x) \cos(mx) \ dx\text{.}\label{x:men:eq_am}
\end{equation}
%
\item{}Show that for \(m \geq 1\),%
\begin{equation}
b_m =  \frac{1}{\pi}\int_{-\pi}^{\pi} g(x) \sin(mx) \ dx\text{.}\label{x:men:eq_bm}
\end{equation}
%
\end{enumerate}
\end{project}%
Let us return to the sawtooth function defined earlier and find its Fourier coefficients.%
\begin{project}{}{x:project:act_Fourier_sawtooth}%
Let \(f\) be defined by \(f(x) = x\) on \([-\pi,\pi]\) and repeated periodically afterwards with period \(2\pi\). Let \(p_n\) be the projection of \(f\) onto \(W_n\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Evaluate the integrals to find the projection \(p_1\).%
\item{}Use appropriate technology to find the projections \(p_{10}\), \(p_{20}\), and \(p_{30}\) for the sawtooth function \(f\). Draw pictures of these approximations against \(f\) and explain what you see.%
\item{}Now we find formulas for all the Fourier coefficients. Use the fact that \(x \cos(mx)\) is an odd function to explain why \(a_m = 0\) for each \(m\). Then show that \(b_m = (-1)^{m+1}\frac{2}{m}\) for each \(m\).%
\item{}Go back to the website \href{http://www.falstad.com/fourier/}{\nolinkurl{falstad.com/fourier/}} and replay the sawtooth tone. Explain what the white buttons represent.%
\end{enumerate}
\end{project}%
\begin{project}{}{x:project:act_Fourier_square_sums}%
This activity is not connected to the idea of musical tones, so can be safely ignored if so desired. We conclude with a derivation of a very fascinating formula that you may have seen for \(\ds \sum_{n=1}^{\infty} \frac{1}{n^2}\). To do so, we need to analyze the error in approximating a function \(g\) with a function in \(W_n\).%
\par
Let \(p_n\) be the projection of \(g\) onto \(W_n\). Notice that \(p_n\) is also in \(W_{n+1}\). It is beyond the scope of this project, but in ``nice'' situations we have \(||g - p_n|| \to 0\) as \(n \to \infty\). Now \(g - p_n\) is orthogonal to \(p_n\), so the Pythagorean theorem shows that%
\begin{equation*}
||g-p_n||^2 + ||p_n||^2 = ||g||^2\text{.}
\end{equation*}
%
\par
Since \(||g-p_n||^2 \to 0\) as \(n \to \infty\), we can conclude that%
\begin{equation}
\lim_{n \to \infty} ||p_n||^2 = ||g||^2\text{.}\label{x:men:eq_Fourier_norm}
\end{equation}
%
\par
We use these ideas to derive a formula for \(\ds \sum_{n=1}^{\infty} \frac{1}{n^2}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Use the fact that \(\CB_n\) is an orthonormal basis to show that%
\begin{equation*}
||p_n||^2 = a_0^2 +a_1^2 + b_1^2 + \cdots + a_n^2 + b_n^2\text{.}
\end{equation*}
Conclude that%
\begin{equation}
||g||^2 = a_0^2 +a_1^2 + b_1^2 + \cdots + a_n^2 + b_n^2 + \cdots\text{.}\label{x:men:eq_norms}
\end{equation}
%
\item{}For the remainder of this activity, let \(f\) be the sawtooth function defined by \(f(x) = x\) on \([-\pi,\pi]\) and repeated periodically afterwards. We determined the Fourier coefficients \(a_i\) and \(b_j\) of this function in \hyperref[x:project:act_Fourier_sawtooth]{Project Activity~{\xreffont\ref{x:project:act_Fourier_sawtooth}}}.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Show that%
\begin{equation*}
a_0^2 +a_1^2 + b_1^2 + \cdots + a_n^2 + b_n^2 + \cdots = 4\sum_{n=1}^{\infty} \frac{1}{n^2}\text{.}
\end{equation*}
%
\item{}Calculate \(||f||^2\) using the inner product and compare to \hyperref[x:men:eq_norms]{({\xreffont\ref{x:men:eq_norms}})} to find a surprising formula for \(\sum_{n=1}^{\infty} \frac{1}{n^2}\).%
\end{enumerate}
\end{enumerate}
\end{project}%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 36 The Gram-Schmidt Process in Inner Product Spaces}
\typeout{************************************************}
%
\begin{chapterptx}{The Gram-Schmidt Process in Inner Product Spaces}{}{The Gram-Schmidt Process in Inner Product Spaces}{}{}{x:chapter:chap_gram_schmidt_ips}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp25956504}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What is the Gram-Schmidt process and why is it useful?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Gaussian Quadrature}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Gaussian Quadrature}{}{Application: Gaussian Quadrature}{}{}{x:section:sec_appl_gaussian_quad}
Since integration of functions is difficult, approximation techniques for definite integrals are very important. In calculus we are introduced to various methods, e.g., the midpoint, trapezoid, and Simpson's rule, for approximating definite integrals. These methods divide the interval of integration into subintervals and then use values of the integrand at the endpoints to approximate the integral. These are useful methods when approximating integrals from tabulated data, but there are better methods for other types of integrands. If we make judicious choices in the points we use to evaluate the integrand, we can obtain more accurate results with less work. One such method is Gaussian quadrature (which, for example, is widely used in solving problems of radiation heat transfer in direct integration of the equation of transfer of radiation over space), which we explore later in this section. This method utilizes the Gram-Schmidt process to produce orthogonal polynomials.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_gram_schmidt_intro}
We have seen that orthogonal bases make computations very convenient, and the Gram-Schmidt process allowed us to create orthogonal bases in \(\R^n\) using the dot product as inner product. In this section we will see how the Gram-Schmidt process works in any inner product space.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Gram-Schmidt Process using Inner Products}
\typeout{************************************************}
%
\begin{sectionptx}{The Gram-Schmidt Process using Inner Products}{}{The Gram-Schmidt Process using Inner Products}{}{}{x:section:sec_gram_schmidt_inner_prod}
Our goal is to understand how the Gram-Schmidt process works in any inner product space.%
\begin{exploration}{}{x:exploration:pa_6_d_2}%
Let \(p_1(t) = t\), \(p_2(t) = 1+t+t^2\), and \(p_3(t) = t^3\) in \(\pol_3\). Let \(\CB = \{p_1(t), p_2(t),
p_3(t)\}\) and let \(W= \Span \ \CB\). Throughout this activity, use the inner product on \(\pol_3\) defined by%
\begin{equation*}
\langle p(t), q(t) \rangle = \int_{-1}^1 p(t)q(t) \, dt\text{.}
\end{equation*}
%
\par
We want to construct an orthogonal basis \(\CC = \{q_1(t), q_2(t),
q_3(t)\}\) from \(\CB\) so that \(\Span \ \CC = \Span \ \CB\). To begin, we start by letting \(q_1(t) = p_1(t)\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Now we want to find a polynomial in \(W\) that is orthogonal to \(q_1(t)\). Let \(W_1 = \Span\{q_1(t)\}\). Explain why \(q_2(t) = \proj_{\perp W_1} p_2(t)\) is in \(W\) and is orthogonal to \(q_1(t)\). Then calculate the polynomial \(q_2(t)\).%
\item{}Next we need to find a third polynomial \(q_3(t)\) that is in \(W\) and is orthogonal to both \(q_1(t)\) and \(q_2(t)\). Let \(W_2 = \Span \{q_1(t), q_2(t)\}\). Explain why \(q_3(t) = \proj_{\perp W_2} p_3(t)\) is in \(W\) and is orthogonal to both \(q_1(t)\) and \(q_2(t)\). Then calculate the polynomial \(q_3(t)\).%
\item{}Explain why the set \(\{q_1(t),
q_2(t), q_3(t)\}\) is an orthogonal basis for \(W\).%
\end{enumerate}
\end{exploration}%
\hyperref[x:exploration:pa_6_d_2]{Preview Activity~{\xreffont\ref{x:exploration:pa_6_d_2}}} shows the first steps of the Gram-Schmidt process to construct an orthogonal basis from any basis of an inner product space. To understand how the process works in general, let \(\{\vv_1, \vv_2, \ldots, \vv_m\}\) be a basis for a subspace \(W\) of an inner product space \(V\). Let \(\vw_1 = \vv_1\) and let \(W_1 = \Span\{\vw_1\}\). Since \(\vw_1 = \vv_1\) we have that \(W_1 = \Span\{\vw_1\} = \Span\{\vv_1\}\). Now consider the subspace%
\begin{equation*}
W_2 = \Span\{\vv_1, \vv_2\}
\end{equation*}
of \(W\). The vectors \(\vv_1 = \vw_1\) and \(\vv_2\) are possibly not orthogonal, but we know the orthogonal projection of \(\vv_2\) onto \(W_1^{\perp}\) is orthogonal to \(\vw_1\). Let%
\begin{equation*}
\vw_2 = \proj_{\perp W_1} \vv_2 = \vv_2 -  \frac{\langle \vv_2, \vw_1 \rangle}{\langle \vw_1, \vw_1 \rangle} \vw_1\text{.}
\end{equation*}
%
\par
Then \(\{\vw_1, \vw_2\}\) is an orthogonal set. Note that \(\vw_1 = \vv_1 \neq \vzero\), and the fact that \(\vv_2 \notin W_1\) implies that \(\vw_2 \neq \vzero\). So the set \(\{\vw_1, \vw_2\}\) is linearly independent, being a set of non-zero orthogonal vectors. Now the question is whether \(\Span\{\vw_1,\vw_2\} = \Span\{\vv_1,\vv_2\}\). Note that \(\vw_2\) is a linear combination of \(\vv_1\) and \(\vv_2\), so \(\vw_2\) is in \(\Span\{\vv_1,\vv_2\}\). Since \(\Span\{\vw_1,\vw_2\}\) is a 2-dimensional subspace of the 2-dimensional space \(W_2\), it must be true that \(\Span\{\vw_1,\vw_2\} = W_2 = \Span\{\vv_1,\vv_2\}\).%
\par
Now we take the next step, adding \(\vv_3\) into the mix. Let%
\begin{equation*}
W_3 = \Span\{\vv_1,\vv_2,\vv_3\} = \Span\{\vw_1, \vw_2, \vv_3\}\text{.}
\end{equation*}
%
\par
The vector%
\begin{equation*}
\vw_3 = \proj_{\perp W_2} \vv_3 = \vv_3 - \frac{\langle \vv_3, \vw_1 \rangle}{\langle \vw_1, \vw_1 \rangle} \vw_1 - \frac{\langle \vv_3, \vw_2 \rangle}{\langle \vw_2, \vw_2 \rangle} \vw_2
\end{equation*}
is orthogonal to both \(\vw_1\) and \(\vw_2\) and, by construction, \(\vw_3\) is a linear combination of \(\vv_1\), \(\vv_2\), and \(\vv_3\). So \(\vw_3\) is in \(W_3\). The fact that \(\vv_3 \notin W_2\) implies that \(\vw_3 \neq \vzero\) and \(\{\vw_1, \vw_2, \vw_3\}\) is a linearly independent set. Since \(\Span\{\vw_1, \vw_2, \vw_3\}\) is a 3-dimensional subspace of the 3-dimensional space \(W_3\), we conclude that \(\Span\{\vw_1, \vw_2, \vw_3\}\) equals \(\Span\{\vv_1,\vv_2,\vv_3\}\).%
\par
We continue inductively in this same manner. If we have constructed a set \(\{\vw_1\), \(\vw_2\), \(\vw_3\), \(\ldots\), \(\vw_{k-1}\}\) of \(k-1\) orthogonal vectors such that%
\begin{equation*}
\Span\{\vw_1, \vw_2, \vw_3, \ldots, \vw_{k-1}\} = \Span\{\vv_1,\vv_2,\vv_3, \ldots, \vv_{k-1}\}\text{,}
\end{equation*}
then we let%
\begin{align*}
\vw_{k} \amp = \proj_{\perp W_{k-1}} \vv_k\\
\amp = \vv_k - \ds \frac{\langle \vv_k, \vw_1 \rangle}{\langle \vw_1, \vw_1 \rangle} \vw_1 - \frac{\langle \vv_k, \vw_2 \rangle}{\langle \vw_2, \vw_2 \rangle} \vw_2 - \cdots - \frac{\langle \vv_k, \vw_{k-1} \rangle}{\langle \vw_{k-1}, \vw_{k-1} \rangle} \vw_{k-1}\text{,}
\end{align*}
where%
\begin{equation*}
W_{k-1} = \Span\{\vw_1, \vw_2, \vw_3, \ldots, \vw_{k-1}\}\text{.}
\end{equation*}
%
\par
We know that \(\vw_k\) is orthogonal to \(\vw_1\), \(\vw_2\), \(\ldots\), \(\vw_{k-1}\). Since \(\vw_1\), \(\vw_2\), \(\ldots\), \(\vw_{k-1}\), and \(\vv_k\) are all in \(W_k = \Span\{\vv_1, \vv_2, \ldots, \vv_k\}\) we see that \(\vw_k\) is also in \(W_k\). Since \(\vv_k \notin W_{k-1}\) implies that \(\vw_k \neq \vzero\) and \(\{\vw_1, \vw_2, \ldots, \vw_k\}\) is a linearly independent set. Then \(\Span\{\vw_1, \vw_2, \vw_3, \ldots, \vw_{k}\}\) is a \(k\)-dimensional subspace of the \(k\)-dimensional space \(W_k\), so it follows that%
\begin{equation*}
\Span\{\vw_1, \vw_2, \vw_3, \ldots, \vw_{k}\} = W_k = \Span\{\vv_1,\vv_2,\vv_3, \ldots, \vv_{k}\}\text{.}
\end{equation*}
%
\par
This process will end when we have an orthogonal set \(\{\vw_1\), \(\vw_2\), \(\vw_3\), \(\ldots\), \(\vw_{m}\}\) with \(\Span\{\vw_1\), \(\vw_2\), \(\vw_3\), \(\ldots\), \(\vw_{m}\}\) = \(W\).%
\par
We summarize the process in the following theorem.%
\begin{theorem}{The Gram-Schmidt Process.}{}{x:theorem:thm_6_d_Gram_Schmidt_ips}%
\index{Gram-Schmidt Process!in an inner product space}%
Let \(\{\vv_1, \vv_2, \ldots, \vv_m\}\) be a basis for a subspace \(W\) of an inner product space \(V\). The set \(\{\vw_1, \vw_2, \vw_3, \ldots, \vw_{m}\}\) defined by%
\begin{itemize}[label=\textbullet]
\item{}\(\vw_1 = \vv_1\),%
\item{}\(\vw_2 = \vv_2 - \ds \frac{\langle \vv_2, \vw_1 \rangle}{\langle \vw_1, \vw_1 \rangle} \vw_1\),%
\item{}\(\vw_3 = \vv_3 - \ds \frac{\langle \vv_3, \vw_1 \rangle}{\langle \vw_1, \vw_1 \rangle} \vw_1 - \frac{\langle \vv_3, \vw_2 \rangle}{\langle \vw_2, \vw_2 \rangle} \vw_2\), \(\vdots\)%
\item{}\(\vw_m = \vv_m - \ds \frac{\langle \vv_m, \vw_1 \rangle}{\langle \vw_1, \vw_1 \rangle} \vw_1 - \frac{\langle \vv_m, \vw_2 \rangle}{\langle \vw_2, \vw_2 \rangle} \vw_2 - \cdots - \frac{\langle \vv_m, \vw_{m-1} \rangle}{\langle \vw_{m-1}, \vw_{m-1} \rangle} \vw_{m-1}\).%
\end{itemize}
%
\par
is an orthogonal basis for \(W\). Moreover,%
\begin{equation*}
\Span\{\vw_1, \vw_2, \vw_3, \ldots, \vw_{k}\} = \Span\{\vv_1,\vv_2,\vv_3, \ldots, \vv_{k}\}
\end{equation*}
for \(1\leq k\leq m\).%
\end{theorem}
The Gram-Schmidt process builds an orthogonal basis \(\{\vw_1, \vw_2, \vw_3, \ldots, \vw_{m}\}\) for us from a given basis. To make an orthonormal basis \(\{\vu_1, \vu_2, \vu_3, \ldots, \vu_{m}\}\), all we need do is normalize each basis vector: that is, for each \(i\), we let%
\begin{equation*}
\vu_i = \ds \frac{\vw_i}{|| \vw_i ||} \,\text{.}
\end{equation*}
%
\begin{activity}{}{x:activity:act_6_d_gs_example}%
Use the Gram-Schmidt process to find an orthogonal basis for%
\begin{equation*}
W = \Span \left\{\left[ \begin{array}{c} 1\\0\\0\\1 \end{array}  \right], \left[ \begin{array}{r} 0\\1\\0\\-2 \end{array}  \right], \left[ \begin{array}{r} 4\\0\\-1\\0 \end{array}  \right] \right\}
\end{equation*}
using the inner product%
\begin{equation*}
\left\langle [u_1 \ u_2 \ u_3 \ u_4]^{\tr}, [v_1 \ v_2 \ v_3 \ v_4]^{\tr} \right\rangle = u_1v_1 + 2u_2v_2 + 2u_3v_3 + u_4v_4\text{.}
\end{equation*}
%
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_gram_schmidt_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp26336136}%
Let \(W = \Span\{M_1, M_2, M_3, M_4\}\), where%
\begin{equation*}
M_1 = \left[ \begin{array}{ccc} 1\amp 0\amp 1\\0\amp 0\amp 1\\0\amp 0\amp 0 \end{array}  \right], M_2 = \left[ \begin{array}{ccc} 0\amp 0\amp 2\\1\amp 0\amp 1\\0\amp 1\amp 0 \end{array}  \right], M_3 = \left[ \begin{array}{ccc} 0\amp 0\amp 0\\4\amp 1\amp 3\\0\amp 0\amp 0 \end{array}  \right], \text{ and }  M_4 = \left[ \begin{array}{ccc} 12\amp 0\amp 0\\0\amp 0\amp 0\\1\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
%
\par
Find an orthonormal basis for \(W\) using the Frobenius inner product \(\langle A,B \rangle = \trace\left(AB^{\tr}\right)\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp26338184}{}\quad{}Recall that the Frobenius inner product is just like a dot product for matrices. First note that \(M_1\), \(M_2\), \(M_3\), and \(M_4\) are linearly independent. We let \(N_1 = M_1\) and the Gram-Schmidt process gives us%
\begin{align*}
N_2 \amp = M_2 - \frac{\langle M_2,N_1 \rangle}{\langle N_1,  N_1\rangle} N_1\\
\amp =\left[  \begin{array}{ccc} 0\amp 0\amp 2\\
1\amp 0\amp 1\\
0\amp 1\amp 0 \end{array} \right] - \frac{3}{3}\left[ \begin{array}{ccc} 1\amp 0\amp 1\\
0\amp 0\amp 1\\
0\amp 0\amp 0 \end{array} \right]\\
\amp = \left[ \begin{array}{rcc} -1\amp 0\amp 1\\
1\amp 0\amp 0\\
0\amp 1\amp 0 \end{array} \right]
\end{align*}
%
\begin{align*}
N_3 \amp = M_3 - \frac{\langle M_3,N_1 \rangle}{\langle N_1,  N_1\rangle} N_1 - \frac{\langle M_3,N_2 \rangle}{\langle N_2,  N_2\rangle} N_2\\
\amp = \left[ \begin{array}{ccc} 0\amp 0\amp 0\\
4\amp 1\amp 3\\
0\amp 0\amp 0 \end{array} \right] - \frac{3}{3}\left[ \begin{array}{ccc} 1\amp 0\amp 1\\
0\amp 0\amp 1\\
0\amp 0\amp 0 \end{array} \right] - \frac{4}{4}\left[ \begin{array}{rcc} -1\amp 0\amp 1\\
1\amp 0\amp 0\\
0\amp 1\amp 0 \end{array} \right]\\
\amp = \left[ \begin{array}{crr} 0\amp 0\amp -2\\
3\amp 1\amp 2\\
0\amp -1\amp 0 \end{array} \right]
\end{align*}
and%
\begin{align*}
N_4 \amp = M_4 - \frac{\langle M_4,N_1 \rangle}{\langle N_1,  N_1\rangle} N_1 - \frac{\langle M_4,N_2 \rangle}{\langle N_2,  N_2\rangle} N_2 - \frac{\langle M_4,N_3 \rangle}{\langle N_3,  N_3\rangle} N_3\\
\amp = \left[ \begin{array}{ccc} 12\amp 0\amp 0\\
0\amp 0\amp 0\\
1\amp 0\amp 0 \end{array} \right] - \frac{12}{3}\left[ \begin{array}{ccc} 1\amp 0\amp 1\\
0\amp 0\amp 1\\
0\amp 0\amp 0 \end{array} \right] - \frac{-12}{4}\left[ \begin{array}{rcc} -1\amp 0\amp 1\\
1\amp 0\amp 0\\
0\amp 1\amp 0 \end{array} \right] - \frac{0}{19} \left[ \begin{array}{crr} 0\amp 0\amp -2\\
3\amp 1\amp 2\\
0\amp -1\amp 0 \end{array} \right]\\
\amp =\left[  \begin{array}{ccr} 5\amp 0\amp -1\\
3\amp 0\amp -4\\
1\amp 3\amp 0 \end{array} \right]\text{.}
\end{align*}
%
\par
Then \(\{N_1, N_2, N_3, N_4\}\) is an orthogonal basis for \(W\). An orthonormal basis is found by dividing each vector by its magnitude, so%
\begin{equation*}
\left\{\frac{1}{\sqrt{3}}\left[ \begin{array}{ccc} 1\amp 0\amp 1\\0\amp 0\amp 1\\0\amp 0\amp 0 \end{array}  \right], \frac{1}{2}\left[ \begin{array}{rcc} -1\amp 0\amp 1\\1\amp 0\amp 0\\0\amp 1\amp 0 \end{array}  \right],  \frac{1}{\sqrt{19}}\left[  \begin{array}{crr} 0\amp 0\amp -2\\3\amp 1\amp 2\\0\amp -1\amp 0 \end{array}  \right], \frac{1}{\sqrt{61}}\left[  \begin{array}{ccr} 5\amp 0\amp -1\\3\amp 0\amp -4\\1\amp 3\amp 0 \end{array}  \right]\right\}
\end{equation*}
is an orthonormal basis for \(W\).%
\end{example}
\begin{example}{}{g:example:idp26352136}%
Consider the space \(V = C[0,1]\) with inner product \(\langle f,g \rangle = \int_0^1 f(x)g(x) \ dx\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the polynomial in \(\pol_2\) (considered as a subspace of \(V\)) that is closest to \(h(x) = \sqrt{x}\). Use technology to calculate any required integrals. Draw a graph of your approximation against the graph of \(h\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp26363912}{}\quad{}Our job is to find \(\proj_{\pol_2} h(x)\). To do this, we need an orthogonal basis of \(\pol_2\). We apply the Gram-Schmidt process to the standard basis \(\{1, t, t^2\}\) of \(\pol_2\) to obtain an orthogonal basis \(\{p_1(t),
p_2(t), p_3(t)\}\) of \(\pol_2\). We start with \(p_1(t) = 1\), then%
\begin{align*}
p_2(t) \amp = t - \frac{\left\langle t,1 \right\rangle}{\langle 1,1 \rangle} (1)\\
\amp = t - \frac{1}{2}
\end{align*}
and%
\begin{align*}
p_3(t) \amp = t^2 - \frac{\left\langle t^2,1 \right\rangle}{\langle 1,1 \rangle} (1) - \frac{\left\langle t^2,t-\frac{1}{2} \right\rangle}{\left\langle t-\frac{1}{2},t-\frac{1}{2} \right\rangle} \left(t-\frac{1}{2}\right)\\
\amp = t^2 - \frac{1}{3} - \frac{\frac{1}{12}}{\frac{1}{12}}\left(t-\frac{1}{2}\right)\\
\amp = t^2-t + \frac{1}{6}\text{.}
\end{align*}
Then%
\begin{align*}
\proj_{\pol_2} \sqrt{x} \amp = \frac{\left\langle \sqrt{x},1 \right\rangle}{\langle 1,1 \rangle} (1) + \frac{\left\langle \sqrt{x},x-\frac{1}{2} \right\rangle}{\left\langle x-\frac{1}{2},x-\frac{1}{2} \right\rangle} \left(x-\frac{1}{2}\right)\\
\amp  \qquad + \frac{\left\langle \sqrt{x}, x^2-x + \frac{1}{12} \right\rangle}{\left\langle  x^2-x + \frac{1}{6}, x^2-x + \frac{1}{6} \right\rangle} \left( x^2-x + \frac{1}{6}\right)\\
\amp = \frac{\frac{2}{3}}{1}(1) + \frac{\frac{1}{15}}{\frac{1}{12}} \left(x-\frac{1}{2}\right) - \frac{\frac{1}{315}}{\frac{1}{180}} \left(x^2-x + \frac{1}{6}\right)\\
\amp = \frac{2}{3} + \left(\frac{4}{5}\right)\left(x-\frac{1}{2}\right) - \frac{4}{7}\left(x^2-x + \frac{1}{6}\right)\\
\amp = -\frac{4}{7}x^2 + \frac{48}{35}x + \frac{6}{35}\text{.}
\end{align*}
A graph of the approximation and \(h\) are shown in \hyperref[x:figure:F_6_e_example_2j]{Figure~{\xreffont\ref{x:figure:F_6_e_example_2j}}} \begin{figureptx}{The graphs of \(\sqrt{x}\) and \(-\frac{4}{7}x^2 + \frac{48}{35}x + \frac{6}{35}\).}{x:figure:F_6_e_example_2j}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/6_e_example_2.pdf}
\end{image}%
\tcblower
\end{figureptx}%
%
\item{}Provide a numeric measure of the error in approximating \(\sqrt{x}\) by the polynomial you found in part (a).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp26372616}{}\quad{}The norm of \(\proj_{\pol_2^{\perp}} \sqrt{x} = \sqrt{x} - \left(-\frac{4}{7}x^2 + \frac{48}{35}x + \frac{6}{35}\right)\) tells us how well our projection \(-\frac{4}{7}x^2 + \frac{48}{35}x + \frac{6}{35}\) approximates \(\sqrt{x}\). Technology shows that%
\begin{equation*}
||\proj_{\pol_2^{\perp}} \sqrt{x}|| = \sqrt{ \langle \proj_{\pol_2^{\perp}} \sqrt{x}, \proj_{\pol_2^{\perp}} \sqrt{x}\rangle } = \frac{\sqrt{2}}{70} \approx 0.02\text{.}
\end{equation*}
%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_gram_schmidt_summ_ips}
%
\begin{itemize}[label=\textbullet]
\item{}The Gram-Schmidt process produces an orthogonal basis from any basis. It works as follows. Let \(\{\vv_1, \vv_2, \ldots, \vv_m\}\) be a basis for a subspace \(W\) of \(\R^n\). The set \(\{\vw_1\), \(\vw_2\), \(\vw_3\), \(\ldots\), \(\vw_{m}\}\) defined by%
\begin{itemize}[label=$\circ$]
\item{}\(\vw_1 = \vv_1\),%
\item{}\(\vw_2 = \vv_2 - \ds \frac{\vv_2 \cdot \vw_1}{\vw_1 \cdot \vw_1} \vw_1\),%
\item{}\(\vw_3 = \vv_3 - \ds \frac{ \vv_3 \cdot \vw_1 }{ \vw_1 \cdot \vw_1 } \vw_1 - \frac{ \vv_3 \cdot \vw_2 }{ \vw_2 \cdot \vw_2 } \vw_2\), \(\vdots\)%
\item{}\(\vw_m = \vv_m - \ds \frac{ \vv_m \cdot \vw_1 }{ \vw_1 \cdot \vw_1 } \vw_1 - \frac{ \vv_m \cdot \vw_2 }{ \vw_2 \cdot \vw_2 } \vw_2 - \cdots - \frac{ \vv_m \cdot \vw_{m-1} }{ \vw_{m-1} \cdot \vw_{m-1} } \vw_{m-1}\).%
\end{itemize}
is an orthogonal basis for \(W\). Moreover,%
\begin{equation*}
\Span\{\vw_1, \vw_2, \vw_3, \ldots, \vw_{k}\} = \Span\{\vv_1,\vv_2,\vv_3, \ldots, \vv_{k}\}
\end{equation*}
for each \(k\) between 1 and \(m\).%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_gram_schmidt_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp26383624}%
Let \(V = C[0,1]\) with the inner product \(\langle f(t),
g(t) \rangle = \int_0^1 f(t) g(t) \, dt\). Let \(W = \pol_2\). Note that \(W\) is a subspace of \(V\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the polynomial \(q(t)\) in \(W\) that is closest to the function \(h\) defined by \(h(t) = \frac{2}{1+t^2}\) in the least squares sense. That is, find the projection of \(h(t)\) onto \(W\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp26390536}{}\quad{}Recall the work done in \hyperref[x:activity:act_6_d_gs_examples]{Activity~{\xreffont\ref{x:activity:act_6_d_gs_examples}}}.%
\item{}Find the second order Taylor polynomial \(P_2(t)\) for \(h(t)\) centered at 0.%
\item{}Plot \(h(t)\), \(q(t)\), and \(P_2(t)\) on the same set of axes. Which approximation provides a better fit to \(h\) on this interval. Why?%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp26395656}%
Each set \(S\) is linearly independent. Use the Gram-Schmidt process to create an orthogonal set of vectors with the same span as \(S\). Then find an orthonormal basis for the same span.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(S = \{1+t, 1-t, t-t^2\}\) in \(\pol_2\) using the inner product%
\begin{equation*}
\langle p(t), q(t) \rangle = \int_{-1}^1 p(t) q(t) \, dt
\end{equation*}
%
\item{}\(S = \left\{\left[ \begin{array}{c} 1\\0\\2 \end{array} \right], \left[ \begin{array}{r} -1\\1\\1 \end{array} \right], \left[ \begin{array}{c} 1\\1\\0 \end{array} \right] \right\}\) in \(\R^3\) using the inner product \(\langle \vu, \vv \rangle = (A\vu) \cdot (A\vv)\), where \(A = \left[ \begin{array}{ccc} 1\amp 0\amp 1 \\ 0\amp 1\amp 1\\1\amp 1\amp 0 \end{array} \right]\)%
\item{}\(S = \left\{\left[ \begin{array}{c} 1\\0\\1\\0\\1\\0\\1 \end{array}  \right], \left[ \begin{array}{r} -1\\2\\3\\0\\1\\0\\1 \end{array}  \right], \left[ \begin{array}{r} 1\\0\\4\\-1\\2\\0\\1 \end{array}  \right], \left[ \begin{array}{c} 1\\1\\1\\1\\1\\1\\1 \end{array}  \right]\right\}\) in \(\R^7\) with the weighted inner product%
\begin{align*}
\langle [u_1 \ \amp u_2 \ u_3 \ u_4 \ u_5 \ u_6 \ u_7]^{\tr}, [v_1 \ v_2 \ v_3 \ v_4 \ v_5 \ v_6 \ v_7]^{\tr} \rangle\\
\amp = u_1v_1 + u_2v_2 + u_3v_3 + 2u_4v_4 + 2u_5v_5 + u_6v_6 + u_7v_7
\end{align*}
%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp26408200}%
Let \(S = \{1, \cos(t), \sin(t)\}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(S\) is a linearly independent set in \(C[0,\pi]\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp26401032}{}\quad{}Evaluate an appropriate linear combination at several well chosen values to set up a linear system.%
\item{}Use the Gram-Schmidt process to find an orthogonal basis from \(S\) using the inner product%
\begin{equation*}
\langle f(t), g(t) \rangle = \int_{0}^{\pi} f(t) g(t) \, dt
\end{equation*}
%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp26406536}%
Find an orthonormal basis for the subspace \(W\) of \(\pol_2\) that consists of all polynomials of the form \(a + (a+b)t + bt^2\) using the inner product \(\langle p(t),
q(t) \rangle = \int_0^1 p(t)q(t) \, dt\).%
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp26409864}%
The Legendre polynomials form an orthonormal basis for the infinite dimensional inner product space \(\pol\) of all polynomials using the inner product%
\begin{equation*}
\langle p(t), q(t) \rangle = \int_{-1}^{1} p(t) q(t) \, dt\text{.}
\end{equation*}
The Legendre polynomials have applications to differential equations, statistics, numerical analysis, and physics (e.g., they appear when solving Schrödinger equation in three dimensions for a central force). The Legendre polynomials are found by using the Gram-Schmidt process to find an orthogonal basis from the standard basis \(\{1, t, t^2, \cdots\}\) for \(\pol\). Find the first four Legendre polynomials by creating a orthonormal basis from the set \(\{1,t,t^2, t^3\}\).%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp26414728}%
The sine integral function \(\Si\) has applications in physics and engineering. We define \(\Si\) as%
\begin{equation*}
\Si(x) = \int_0^x \frac{\sin(t)}{t} \, dt\text{.}
\end{equation*}
Since we cannot find an elementary formula for \(\Si(x)\), we use approximations. Find the best approximation to \(\Si(x)\) in \(\pol_3\) with inner product \(\langle p(t),
q(t) \rangle = \int_0^1 p(t)q(t) \, dt\). Use appropriate technology for computations and round output six places to the right of the decimal.%
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp26420872}%
Recall from \hyperlink{x:exercise:ex_6_c_all_ips}{Exercise~{\xreffont 18}} in \hyperref[x:chapter:chap_inner_products]{Section~{\xreffont\ref{x:chapter:chap_inner_products}}} that any finite dimensional vector space \(V\) can be made into an inner product space by setting a basis \(\CB\) for \(V\) and defining%
\begin{equation}
\langle \vu, \vw \rangle_{\CB} = \sum_{i=1}^n u_iw_i\label{x:men:ex_6_e_ip}
\end{equation}
if \(\vu = \sum_{i=1}^n u_i \vv_i\) and \(\vw = \sum_{i=1}^n w_i \vv_i\) in \(V\). Let  \(V = \pol_2\) and \(\CB = \{1+t, 1+t^2, t+t^2\}\). (You may assume that \(\CB\) is a basis for \(V\).)%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find an orthogonal basis for \(V\) containing the polynomial \(p_1(t) = 1\) using the inner product \hyperref[x:men:ex_6_e_ip]{({\xreffont\ref{x:men:ex_6_e_ip}})}.%
\item{}Find the best approximation possible as measured by the inner product \hyperref[x:men:ex_6_e_ip]{({\xreffont\ref{x:men:ex_6_e_ip}})} to the polynomial \(1 + 2t + 3t^2\) by a polynomial in \(\pol_1\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp26428808}%
Let \(a\), \(b\), and \(c\) be three distinct fixed real numbers and define \(\langle \ , \ \rangle : \pol_2 \to \R\) by%
\begin{equation}
\langle p(t),q(t) \rangle = p(a)q(a) + p(b)q(b) + p(c)q(c)\text{.}\label{x:men:eq_ip}
\end{equation}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(\langle \ , \ \rangle\) as defined in \hyperref[x:men:eq_ip]{({\xreffont\ref{x:men:eq_ip}})} is an inner product on \(\pol_2\).%
\item{}Is the mapping%
\begin{equation}
\langle p(t),q(t) \rangle = p(a)q(a) + p(b)q(b)\label{x:men:eq_ip_2}
\end{equation}
an inner product on \(\pol_2\)? Justify your answer.%
\item{}Let \(a = -1\), \(b=1\), and \(c=3\). Find an orthogonal basis for \(\pol_2\) using the inner product \hyperref[x:men:eq_ip]{({\xreffont\ref{x:men:eq_ip}})}.%
\item{}Continue with \(a = -1\), \(b=1\), and \(c=3\). Find the best approximation possible as measured by the inner product \hyperref[x:men:eq_ip]{({\xreffont\ref{x:men:eq_ip}})} to the polynomial \(1 + 2t + 3t^2\) by a polynomial in \(\pol_1\).%
\item{}If \(W = \Span\{1+t\}\), find \(W^{\perp}\) in \(\pol_2\) with respect to the inner product \hyperref[x:men:eq_ip]{({\xreffont\ref{x:men:eq_ip}})}.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp26442504}%
Label each of the following statements as True or False. Provide justification for your response. Throughout, let \(V\) be a vector space.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
If \(\{\vw_1, \vw_2\}\) is a basis for a subspace \(W\) of an inner product space \(V\), then the vector \(\frac{\vv \cdot \vw_1}{\vw_1 \cdot \vw_1} \vw_1 + \frac{\vv \cdot \vw_2}{\vw_2 \cdot \vw_2} \vw_2\) is the vector in \(W\) closest to \(\vv\).%
\item{}\lititle{True\slash{}False.}\par%
If \(W\) is a subspace of an inner product space \(V\), then the vector \(\vv - \proj_{W} \vv\) is orthogonal to every vector in \(W\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\vu_1\), \(\vu_2\), \(\vu_3\) are vectors in an inner product space \(V\), then the Gram-Schmidt process constructs an orthogonal set of vectors \(\{\vv_1, \vv_2, \vv_3\}\) with the same span as \(\{\vu_1, \vu_2, \vu_3\}\).%
\item{}\lititle{True\slash{}False.}\par%
Any set \(\{\vu_1, \vu_2, \ldots, \vu_k\}\) of orthogonal vectors in an inner product space \(V\) can be extended to an orthogonal basis of \(V\).%
\item{}\lititle{True\slash{}False.}\par%
Every nontrivial finite dimensional subspace of an inner product space has an orthogonal basis.%
\item{}\lititle{True\slash{}False.}\par%
In any inner product space \(V\), if \(W\) is a subspace satisfying \(W^\perp=\{\vzero\}\), then \(W=V\).%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Gaussian Quadrature and Legendre Polynomials}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Gaussian Quadrature and Legendre Polynomials}{}{Project: Gaussian Quadrature and Legendre Polynomials}{}{}{x:section:sec_proj_gaussian_quad}
\index{Gaussian quadrature}%
Simpson's rule is a reasonably accurate method for approximating definite integrals since it models the integrand on subintervals with quadratics. For that reason, Simpson's rule provides exact values for integrals of all polynomials of degree less than or equal to 2. In Gaussian quadrature, we will use a family of polynomials to determine points at which to evaluate an integral of the form \(\int_{-1}^1 f(t) \ dt\). By allowing ourselves to select evaluation points that are not uniformly distributed across the interval of integration, we will be able to approximate our integrals much more efficiently. The method is constructed so as to obtain exact values for as large of degree polynomial integrands as possible. As a result, if we can approximate our integrand well with polynomials, we can obtain very good approximations with Gaussian quadrature with minimal effort.%
\par
The Gaussian quadrature approximation has the form%
\begin{equation}
\int_{-1}^1 f(t) \ dt \approx w_1f(t_1) + w_2 f(t_2) + \cdots + w_n f(t_n) = \sum_{i=1}^n w_if(t_i)\text{,}\label{x:men:eq_GQ}
\end{equation}
where the \(w_i\) (weights) and the \(t_i\) (nodes) are points in the interval \([-1,1]\)\footnote{As we will see later, integrations over \([a,b]\) can be converted to an integral over \([-1,1]\) with a change of variable.\label{g:fn:idp26460808}}. Gaussian quadrature describes how to find the weights and the points in \hyperref[x:men:eq_GQ]{({\xreffont\ref{x:men:eq_GQ}})} to obtain suitable approximations. We begin to explore Gaussian quadrature with the simplest cases.%
\begin{project}{}{x:project:act_GQ_1}%
In this activity we find through direct calculation the node and weight with \(n=1\) so that%
\begin{equation}
w_1f(t_1) \approx \int_{-1}^1 f(t) \ dt\text{.}\label{x:men:eq_GQ1}
\end{equation}
%
\par
There are two unknowns in this situation (\(w_1\) and \(t_1\)) and so we will need 2 equations to find these unknowns. Keep in mind that we want to have the approximation \hyperref[x:men:eq_GQ1]{({\xreffont\ref{x:men:eq_GQ1}})} be exact for as large of degree polynomials as possible.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Assume equality in \hyperref[x:men:eq_GQ1]{({\xreffont\ref{x:men:eq_GQ1}})} if we choose \(f(t) = 1\). Use the resulting equation to find \(w_1\).%
\item{}Assume equality in \hyperref[x:men:eq_GQ1]{({\xreffont\ref{x:men:eq_GQ1}})} if we choose \(f(t) = t\). Use the resulting equation to find \(t_1\).%
\item{}Verify that \hyperref[x:men:eq_GQ1]{({\xreffont\ref{x:men:eq_GQ1}})} is in fact an equality for any linear polynomial of the form \(f(t) = a_0+a_1t\), using the values of \(w_1\) and \(t_1\) you found%
\end{enumerate}
\end{project}%
We do one more specific case before considering the general case.%
\begin{project}{}{x:project:act_GQ_2}%
In this problem we find through direct calculation the nodes and weights with \(n=2\) so that%
\begin{equation}
w_1f(t_1) + w_2f(t_2) \approx \int_{-1}^1 f(t) \ dt\text{.}\label{x:men:eq_GQ2}
\end{equation}
%
\par
There are four unknowns in this situation (\(w_1, w_2\) and \(t_1, t_2\)) and so we will need 4 equations to find these unknowns. Keep in mind that we want to have the approximation \hyperref[x:men:eq_GQ2]{({\xreffont\ref{x:men:eq_GQ2}})} be exact for as large of degree polynomials as possible. In this case we will use \(f(t)=1\), \(f(t)=t\), \(f(t)=t^2\), and \(f(t)=t^3\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Assume equality in \hyperref[x:men:eq_GQ2]{({\xreffont\ref{x:men:eq_GQ2}})} if we choose \(f(t) = 1\). This gives us an equation in \(w_1\) and \(w_2\). Find this equation.%
\item{}Assume equality in \hyperref[x:men:eq_GQ2]{({\xreffont\ref{x:men:eq_GQ2}})} if we choose \(f(t) = t\). This gives us an equation in \(w_1, w_2\) and \(t_1,t_2\). Find this equation.%
\item{}Assume equality in \hyperref[x:men:eq_GQ2]{({\xreffont\ref{x:men:eq_GQ2}})} if we choose \(f(t) = t^2\). This gives us an equation in \(w_1, w_2\) and \(t_1,t_2\). Find this equation.%
\item{}Assume equality in \hyperref[x:men:eq_GQ2]{({\xreffont\ref{x:men:eq_GQ2}})} if we choose \(f(t) = t^3\). This gives us an equation in \(w_1, w_2\) and \(t_1,t_2\). Find this equation.%
\item{}Solve this system of 4 equations in 4 unknowns. You can do this by hand or with any other appropriate tool. Show that \(t_1\) and \(t_2\) are the roots of the polynomial \(t^2 - \frac{1}{3}\).%
\item{}Verify that \hyperref[x:men:eq_GQ2]{({\xreffont\ref{x:men:eq_GQ2}})} is in fact an equality with the values of \(w_1, w_2\) and \(t_1, t_2\) you found for any polynomial of the form \(f(t) = a_0+a_1t+a_2t^2 + a_3t^3\).%
\end{enumerate}
\end{project}%
Other than solving a system of linear equations as in \hyperref[x:project:act_GQ_2]{Project Activity~{\xreffont\ref{x:project:act_GQ_2}}}, it might be reasonable to ask what the connection is between Gaussian quadrature and linear algebra. We explore that connection now.%
\par
In the general case, we want to find the weights and nodes to make the approximation exact for as large degree polynomials as possible. We have \(2n\) unknowns \(w_1\), \(w_2\), \(\ldots\), \(w_n\) and \(t_1\), \(t_2\), \(\ldots\), \(t_n\), so we need to impose \(2n\) conditions to determine the unknowns. We will require equality for the \(2n\) functions \(t^i\) for \(i\) from 0 to \(2n-1\). This yields the equations%
\begin{align*}
w_1\cdot 1 + w_2\cdot 1 + \cdots + w_n\cdot 1 \amp = \int_{-1}^1 1 \ dt = t\biggm|_{-1}^1 = 2\\
w_1t_1 + w_2t_2 + \cdots + w_nt_n \amp = \int_{-1}^1 t \ dt = \frac{t^2}{2}\biggm|_{-1}^1 = 0\\
w_1t_1^2 + w_2t_2^2 + \cdots + w_nt_n^2 \amp = \int_{-1}^1 t^2 \ dt = \frac{t^3}{3}\biggm|_{-1}^1 = \frac{2}{3}\\
\qquad \vdots \amp  \qquad \vdots\\
w_1t_1^{2n-1} + w_2t_2^{2n-1} + \cdots + w_nt_n^{2n-1} \amp = \int_{-1}^1 t^{2n-1} \ dt = \frac{t^{2n}}{2n}\biggm|_{-1}^1 = 0\text{.}
\end{align*}
%
\par
In the \(i\)th equation the right hand side is%
\begin{equation*}
\int_{-1}^1 t^i \ dt = \frac{t^{i+1}}{i+1}\biggm|_{-1}^1 = \begin{cases}\frac{2}{i+1} \amp  \text{ if \(i\) is even } , \\ 0      \amp  \text{ if \(i\) is odd } . \end{cases}
\end{equation*}
%
\begin{project}{}{x:project:act_GQ_n}%
It is inefficient to always solve these systems of equations to find the nodes and weights, especially since there is a more elegant way to find the nodes.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Use appropriate technology to find the equations satisfied by the \(t_i\) for \(n=3\), \(n=4\), and \(n=5\).%
Now we will see the more elegant way to find the nodes. As we will show for some cases, the nodes can be found as roots of a set of orthogonal polynomials in \(\pol_n\) with the inner product \(\langle f(t),
g(t) \rangle = \int_{-1}^1 f(t)g(t) \ dt\). Begin with the basis \(\CS_n = \{1, t, t^2, \ldots,
t^n\}\) of \(\pol_n\). Use appropriate technology to find an orthogonal basis \(B_5\) for \(\pol_5\) obtained by applying the Gram-Schmidt process to \(\CS_n\). The polynomials in this basis are called \terminology{Legendre polynomials}. Check that the nodes are roots of the Legendre polynomials by finding roots of these polynomials using any method. Explain why the \(t_i\) appear to be roots of the Legendre polynomials.%
\end{enumerate}
\end{project}%
Although it would take us beyond the scope of this project to verify this fact, the nodes in the \(n\)th Gaussian quadrature approximation \hyperref[x:men:eq_GQ]{({\xreffont\ref{x:men:eq_GQ}})} are in fact the roots of the \(n\)th order Legendre polynomial. In other words, if \(p_n(t)\) is the \(n\)th order Legendre polynomial, then \(t_1\), \(t_2\), \(\ldots\), \(t_n\) are the roots of \(p_n(t)\) in \([-1,1]\). Gaussian quadrature as described in \hyperref[x:men:eq_GQ]{({\xreffont\ref{x:men:eq_GQ}})} using the polynomial \(p_n(t)\) is exact if the integrand \(f(t)\) is a polynomial of degree less than \(2n\).%
\par
We can find the corresponding weights, \(w_i\), using the formula\footnote{Abramowitz, Milton; Stegun, Irene A., eds. (1972), sec. 25.4, Integration, Handbook of Mathematical Functions (with Formulas, Graphs, and Mathematical Tables), Dover,\label{g:fn:idp26513928}}%
\begin{equation}
w_i = \frac{2}{(1-t_i^2)(q_n '(t_i))^2}\text{,}\label{x:men:eq_GQ_weights}
\end{equation}
where \(q_i(t)\) is the \(i\)th order Legendre polynomial scaled so that \(q_i(1)=1\).%
\begin{project}{}{x:project:act_GQ_example}%
Let us see now how good the integral estimates are with Gaussian quadrature method using an example. Use Gaussian quadrature with the indicated value of \(n\) to approximate \(\displaystyle \int_{-1}^1 e^t\cos(t) \ dt\). Be sure to explain how you found your nodes and weights (approximate the nodes and weights to 8 decimal places). Compare the approximations with the actual value of the integral. Use technology as appropriate to help with calculations.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(n=3\)%
\item{}\(n=4\)%
\item{}\(n=5\)%
\end{enumerate}
\end{project}%
Our Gaussian quadrature formula was derived for integrals on the interval \([-1,1]\). We conclude by seeing how a definite integral on an interval can be converted to one on the interval \([-1,1]\).%
\begin{project}{}{g:project:idp26517000}%
Consider the problem of approximating an integral of the form \(I = \int_a^b g(x) \ dx\). Show that the change of variables \(x = \frac{(b - a)t}{2} + \frac{a + b}{2}\), \(f(t) = \frac{(b - a)g(x)}{2}\) reduces the integral \(I\) to the form \(I = \int_{-1}^1 f(t) \ dt\). (This change of variables can be derived by finding a linear function that maps the interval \([a,b]\) to the interval \([-1,1]\).)%
\end{project}%
\end{sectionptx}
\end{chapterptx}
\end{partptx}
%
%
\typeout{************************************************}
\typeout{Chapter VIII Linear Transformations}
\typeout{************************************************}
%
\begin{partptx}{Linear Transformations}{}{Linear Transformations}{}{}{x:part:part-lin-trans}
 %
%
\typeout{************************************************}
\typeout{Section 37 Linear Transformations}
\typeout{************************************************}
%
\begin{chapterptx}{Linear Transformations}{}{Linear Transformations}{}{}{x:chapter:chap_linear_transformation}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp26523528}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What is a linear transformation?%
\item{}What is the kernel of a linear transformation? What algebraic structure does a kernel of a linear transformation have?%
\item{}What is a one-to-one linear transformation? How does its kernel tell us if a linear transformation is one-to-one?%
\item{}What is the range of a linear transformation? What algebraic property does the range of a linear transformation possess?%
\item{}What is an onto linear transformation? What relationship is there between the codomain and range if a linear transformation is onto?%
\item{}What is an isomorphism of vector spaces?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Fractals}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Fractals}{}{Application: Fractals}{}{}{x:section:sec_appl_fractals}
Sierpinski triangles and Koch's curves have become common phrases in many mathematics departments across the country. These objects are examples of what are called \terminology{fractals}, beautiful geometric constructions that exhibit self-similarity. Fractals are applied in a variety of ways: they help make smaller antennas (e.g., for cell phones) and are used in fiberoptic cables, among other things. In addition, fractals can be used to model realistic objects, such as the Black Spleenwort fern depicted as a fractal image in \hyperref[x:figure:F_Fern]{Figure~{\xreffont\ref{x:figure:F_Fern}}}. As we will see later in this section, one way to construct a fractal is with an \terminology{Iterated Function System} (IFS).%
\begin{figureptx}{An approximation of the Black Spleenwort fern.}{x:figure:F_Fern}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/Fern.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_lin_trans_intro}
We have encountered functions throughout our study of mathematics \textemdash{} we explore graphs of functions in algebra and differentiate and integrate functions in calculus. In linear algebra we have investigated special types of functions, e.g., matrix and coordinate transformations, that preserve the vector space structure. Any function that has the same properties as matrix and coordinate transformations is a \terminology{linear transformation}. Linear transformations are important in linear algebra in that we can study similarities and connections between vector spaces by examining transformations between them. Linear transformations model or approximately model certain real-life processes (like discrete dynamical systems, geometrical transformations, Google PageRank, etc.). Also, we can determine the behavior of an entire linear transformation by knowing how it acts on just a basis.%
\begin{definition}{}{g:definition:idp26538760}%
\index{linear transformation}%
A \terminology{linear transformation} from a vector space \(V\) to a vector space \(W\) is a function \(T: V \to W\) such that%
\begin{enumerate}
\item{}\(T(\vu + \vv) = T(\vu) + T(\vv)\) and%
\item{}\(\displaystyle T(c\vv) = cT(\vv)\)%
\end{enumerate}
%
\par
for all \(\vu, \vv\) in \(V\) and all scalars \(c\).%
\end{definition}
These transformations are called \terminology{linear} because they respect linear combinations. We can combine both parts of this definition into one statement and say that a mapping \(T\) from a vector space \(V\) to a vector space \(W\) is a linear transformation if%
\begin{equation*}
T(a\vu + b \vv) = aT(\vu) + bT(\vv)
\end{equation*}
for all vectors \(\vu, \vv\) in \(V\) and all scalars \(a\) and \(b\). We can extend this property of a linear transformation (by mathematical induction) to any finite linear combination of vectors. That is, if \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) are any vectors in the domain of a linear transformation \(T\) and \(c_1\), \(c_2\), \(\ldots\), \(c_k\) are any scalars, then%
\begin{equation*}
T(c_1 \vv_1 + c_2 \vv_2 + \cdots + c_k \vv_k) = c_1 T(\vv_1) + c_2 T(\vv_2) + \cdots + c_k T(\vv_k)\text{.}
\end{equation*}
%
\par
This is the property that \(T\) respects linear combinations.%
\begin{exploration}{}{x:exploration:pa_8_a}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Consider the transformation \(T: \R^3 \to \R^2\) defined by%
\begin{equation*}
T\left( \left[ \begin{array}{cc} x\\y\\z \end{array}  \right] \right) = \left[ \begin{array}{c} x+y\\z+3 \end{array}  \right]
\end{equation*}
Check that \(T\) is not linear by finding two vectors \(\vu, \vv\) which violate the additive property of linear transformations.%
\item{}Consider the transformation \(T : \pol_2 \to \pol_3\) defined by%
\begin{equation*}
T(a_0+a_1t+a_2t^2) = (a_0+a_1) + a_2t + a_1t^3
\end{equation*}
Check that \(T\) is a linear transformation.%
\item{}Let \(\D\) be the set of all differentiable functions from \(\R\) to \(\R\). Since \(\frac{d}{dx}(0) = 0\), \((f+g)'(x) = f'(x)+g'(x)\) and \((cf)'(x) = cf'(x)\) for any differentiable functions \(f\) and \(g\) and any scalar \(c\), it follows that \(\D\) is a subspace of \(\F\), the vector space of all functions from \(\R\) to \(\R\). Let \(T: \D \to \F\) be defined by \(T(f) = f'\). Check that \(T\) is a linear transformation.%
\item{}Every matrix transformation is a linear transformation, so we might expect that general linear transformations share some of the properties of matrix transformations. Let \(T\) be a linear transformation from a vector space \(V\) to a vector space \(W\). Use the linearity properties to show that \(T(\vzero_V) = \vzero_W\), where \(\vzero_V\) is the additive identity in \(V\) and \(\vzero_W\) is the additive identity in \(W\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp26301704}{}\quad{}\(0_V + 0_V = 0_V\).%
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Onto and One-to-One Transformations}
\typeout{************************************************}
%
\begin{sectionptx}{Onto and One-to-One Transformations}{}{Onto and One-to-One Transformations}{}{}{x:section:sec_onto_oneone}
Recall that in \hyperref[x:chapter:chap_matrix_transformations]{Section~{\xreffont\ref{x:chapter:chap_matrix_transformations}}} we expressed existence and uniqueness questions for matrix equations in terms of one-to-one and onto properties of matrix transformations. The question about the existence of a solution to the matrix equation \(A \vx = \vb\) for any vector \(\vb\), where \(A\) is an \(m \times n\) matrix, is also a question about the existence of a vector \(\vx\) so that \(T(\vx) = \vb\), where \(T(\vx) = A \vx\). If, for each \(\vb\) in \(\R^m\) there is at least one \(\vx\) with \(T(\vx) = \vb\), then \(T\) is an onto transformation. We can make a similar definition for any linear transformation.%
\begin{definition}{}{g:definition:idp26789768}%
\index{linear transformation!onto}%
A linear transformation \(T\) from a vector space \(V\) to a vector space \(W\) is \terminology{onto} if each \(\vb\) in \(W\) is the image of at least one \(\vx\) in \(V\).%
\end{definition}
Similarly, the uniqueness of a solution to \(A \vx = \vb\) for any \(\vb\) in \(\Col A\) is the same as saying that for any \(\vb\) in \(\R^m\), there is at most one \(\vx\) in \(\R^n\) such that \(T(\vx) = \vb\). A matrix transformation with this property is one-to-one, and we can make a similar definition for any linear transformation.%
\begin{definition}{}{g:definition:idp26794632}%
\index{linear transformation!onto}%
A linear transformation \(T\) from a vector space \(V\) to a vector space \(W\) is \terminology{one-to-one} if each \(\vb\) in \(W\) is the image of at most one \(\vx\) in \(V\).%
\end{definition}
With matrix transformations we saw that there are easy pivot criteria for determining whether a matrix transformation is one-to-one (a pivot in each column) or onto (a pivot in each row). If a linear transformation can be represented as a matrix transformation, then we can use these ideas. However, not every general linear transformation can be easily viewed as a matrix transformation, and in those cases we might have to resort to applying the definitions directly.%
\begin{activity}{}{g:activity:idp26804488}%
For each of the following transformations, determine if \(T\) is one-to-one and\slash{}or onto.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(T: \pol_2 \to \pol_1\) defined by \(T(a_0+a_1t+a_2t^2) = a_0+(a_1+a_2)t\).%
\item{}\(T: \D \to \F\) defined by \(T(f) = f'\).%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Kernel and Range of Linear Transformation}
\typeout{************************************************}
%
\begin{sectionptx}{The Kernel and Range of Linear Transformation}{}{The Kernel and Range of Linear Transformation}{}{}{x:section:sec_kernel_range}
As we saw in \hyperref[x:exploration:pa_8_a]{Preview Activity~{\xreffont\ref{x:exploration:pa_8_a}}}, any linear transformation sends the additive identity to the additive identity. If \(T\) is a matrix transformation defined by \(T(\vx) = A \vx\) for some \(m \times n\) matrix \(A\), then we have seen that the set of vectors that \(T\) maps to the zero vector is \(\Nul A = \{\vx : A \vx = \vzero\}\), which is also \(\Ker(T) = \{\vx : T(\vx) = \vzero\}\). We can extend this idea of the kernel of a matrix transformation to any linear transformation \(T\).%
\begin{definition}{}{g:definition:idp26810120}%
\index{linear transformation!kernel}%
Let \(T : V \to W\) be a linear transformation from the vector space \(V\) to the vector space \(W\). The \terminology{kernel} of \(T\) is the set%
\begin{equation*}
\Ker(T) = \{\vx \in V : T(\vx) = \vzero_W\}\text{,}
\end{equation*}
where \(\vzero_W\) is the additive identity in \(W\).%
\end{definition}
Just as the null space of an \(m \times n\) matrix \(A\) is a subspace of \(\R^n\), the kernel of a linear transformation from a vector space \(V\) to a vector space \(W\) is a subspace of \(V\). The proof is left to the exercises.%
\begin{theorem}{}{}{x:theorem:thm_8_a_ker}%
Let \(T : V \to W\) be a linear transformation from a vector space \(V\) to vector space \(W\). Then \(\Ker(T)\) is a subspace of \(V\).%
\end{theorem}
The kernel of a linear transformation provides a convenient way to determine if the linear transformation is one-to-one. If \(T\) is one-to-one, then the only solution to \(T(\vx) = \vzero_W\) is \(\vzero_V\) and \(\Ker(T)\) contains only the zero vector. If \(T\) is not one-to-one (and the domain of \(T\) is not just \(\{\vzero\}\)), then the number of solutions to \(T(\vx) = \vzero_W\) is infinite and \(\Ker(T)\) contains more than just the zero vector. We formalize this idea in the next theorem. (Compare to \hyperref[x:theorem:thm_3_b_one_to_one_kernel]{Theorem~{\xreffont\ref{x:theorem:thm_3_b_one_to_one_kernel}}}.) The formal proof is left for the exercises.%
\begin{theorem}{}{}{x:theorem:thm_8_a_one_to_one_kernel}%
A linear transformation \(T\) from a vector space \(V\) to a vector space \(W\) is one-to-one if and only if \(\Ker(T) = \{\vzero_V\}\), where \(\vzero_V\) is the additive identity in \(V\).%
\end{theorem}
\begin{activity}{}{x:activity:act_8_a_Lin_trans_1}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(T: \pol_1 \to \pol_2\) be defined by \(T(a_0+a_1t) = a_1t^2\). Find \(\Ker(T)\). Is \(T\) one-to-one? Explain.%
\item{}Let \(T: \D \to \F\) be defined by \(T(f) = f'\). Find \(\Ker(T)\).%
\end{enumerate}
\end{activity}%
Recall that the matrix-vector product \(A \vx\) is a linear combination of the columns of \(A\) and the set of all vectors of the form \(A \vx\) is the column space of \(A\). For the matrix transformation \(T\) defined by \(T(\vx) = A\vx\), the set of all vectors of the form \(A \vx\) is also the range of the transformation \(T\). We can extend this idea to arbitrary linear transformations to define the \terminology{range} of a transformation.%
\begin{definition}{}{g:definition:idp26834952}%
\index{range of a linear transformation}%
Let \(T : V \to W\) be a linear transformation from the vector space \(V\) to the vector space \(W\). The \terminology{range} of \(T\) is the set%
\begin{equation*}
\Range(T) = \{T(\vx) : \vx \text{ is in }  V\}\text{.}
\end{equation*}
%
\end{definition}
\index{image of a linear transformation} We also call the range of a transformation \(T\) the \terminology{image} of the transformation and use the notation \(\Image(T)\).%
\par
If \(T(\vx) = A\vx\) for some \(m \times n\) matrix \(A\), we know that \(\Col A\), the span of the columns of \(A\), is a subspace of \(\R^m\). Consequently, the range of \(T\), which is also the column space of \(A\), is a subspace of \(\R^m\). In general, the range of a linear transformation \(T\) from \(V\) to \(W\) is a subspace of \(W\). The fact that the range of a transformation is a subspace of the codomain is a consequence of the more general result in the following theorem.%
\begin{theorem}{}{}{g:theorem:idp26847752}%
Let \(T : V \to W\) be a linear transformation from a vector space \(V\) to vector space \(W\), and let \(U\) be a subspace of \(V\). Then the set%
\begin{equation*}
T(U) = \{T(\vu) : \vu \in U\}
\end{equation*}
is a subspace of \(W\).%
\end{theorem}
\begin{proof}{}{g:proof:idp26850184}
Let \(T : V \to W\) be a linear transformation from a vector space \(V\) to vector space \(W\), and let \(U\) be a subspace of \(V\). Let \(\vzero_V\) and \(\vzero_W\) be the additive identities in \(V\) and \(W\), respectively. We have already shown that \(T(\vzero_V) = \vzero_W\), so \(\vzero_W\) is in \(T(U)\). To show that \(T(U)\) is a subspace of \(W\) we must also demonstrate that \(\vw+\vz\) is in \(T(U)\) whenever \(\vw\) and \(\vz\) are in \(T(U)\) and that \(a\vw\) is in \(T(U)\) whenever \(a\) is a scalar and \(\vw\) is in \(T(U)\). Let \(\vw\) and \(\vz\) be in \(T(U)\). Then \(T(\vx) = \vw\) and \(T(\vy) = \vz\) for some vectors \(\vx\) and \(\vy\) in \(U\). Since \(U\) is a subspace of \(V\), we know that \(\vx+\vy\) is in \(U\). The fact that \(T\) is a linear transformation means that%
\begin{equation*}
T(\vx + \vy) = T(\vx) + T(\vy) = \vw + \vz\text{.}
\end{equation*}
%
\par
So \(\vw+\vz\) is in \(T(U)\).%
\par
Finally, let \(a\) be a scalar. Since \(U\) is a subspace of \(V\), we know that \(a \vx\) is in \(U\). The linearity of \(T\) gives us%
\begin{equation*}
T(a\vx) = aT(\vx) = a\vw\text{,}
\end{equation*}
so \(a \vw\) is in \(T(U)\). We conclude that \(T(U)\) is a subspace of \(W\).%
\end{proof}
Letting \(U = V\) shows that \(\Range(T)\) is a subspace of \(W\).%
\par
The subspace \(\Range(T)\) provides us with a convenient criterion for the transformation \(T\) being onto. The transformation \(T\) is onto if for each \(\vb\) in \(W\), there is at least one \(\vx\) for which \(T(\vx)=\vb\). This means that every \(\vb\) in \(W\) belongs to \(\Range(T)\) for \(T\) to be onto.%
\begin{theorem}{}{}{x:theorem:thm_8_a_onto_range}%
A linear transformation \(T\) from a vector space \(V\) to a vector space \(W\) is onto if and only if \(\Range(T) = W\).%
\end{theorem}
\begin{activity}{}{g:activity:idp26875656}%
Let \(T: \pol_1 \to \pol_2\) be defined by \(T(a_0+a_1t) = a_1t^2\) as in \hyperref[x:activity:act_8_a_Lin_trans_1]{Activity~{\xreffont\ref{x:activity:act_8_a_Lin_trans_1}}}. Describe the vectors in \(\Range(T)\). Is \(T\) onto? Explain.%
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Isomorphisms}
\typeout{************************************************}
%
\begin{sectionptx}{Isomorphisms}{}{Isomorphisms}{}{}{x:section:sec_isomorph}
If \(V\) is a vector space with a basis \(\CB = \{\vv_1, \vv_2, \ldots, \vv_n\}\), we have seen that the coordinate transformation \(T : V \to \R^n\) defined by \(T(\vx) = [\vx]_{\CB}\) is a linear transformation that is both one-to-one and onto. This allows us to uniquely identify any vector in \(V\) with a vector in \(\R^n\) so that the vector space structure is preserved. In other words, the vector space \(V\) is for all intents and purposes the same as the vector space \(\R^n\), except for the way we represent the vectors. This is a very powerful idea in that it shows that any vector space of dimension \(n\) is essentially the same as \(\R^n\). When this happens we say that the vectors spaces are \terminology{isomorphic} and call the coordinate transformation an \terminology{isomorphism}.%
\begin{definition}{}{g:definition:idp26889096}%
An \terminology{isomorphism} \index{isomorphism} from a vector space \(V\) to a vector space \(W\) is a linear transformation \(T : V \to W\) that is one-to-one and onto.%
\end{definition}
We this terminology, we summarize the discussion in the following theorem.%
\begin{theorem}{}{}{g:theorem:idp26884104}%
If \(V\) is a vector space of dimension \(n\), then any coordinate transformation from \(V\) to \(\R^n\) is an isomorphism.%
\end{theorem}
\index{vector spaces!isomorphic} It is left for the exercises to show that if \(T: V \to W\) is an isomorphism, then \(T^{-1}: W \to V\) is also an isomorphism. So if there is an isomorphism from a vector space \(V\) to a vector space \(W\), then there is an isomorphism from \(W\) to \(V\). As a result, we say that \(V\) and \(W\) are \terminology{isomorphic} vector spaces. It is also true that any vector space is isomorphic to itself, and that if \(V\) is isomorphic to \(W\) and \(W\) is isomorphic to \(U\), then \(V\) and \(U\) are also isomorphic. The proof of this result is left to the exercises. From this last property we can conclude that any two vector spaces of dimension \(n\) are isomorphic vector spaces. This validates statement we made earlier that the vector space \(\pol_n\) is for all intents and purposes the same as \(\R^{n+1}\), since the two spaces are isomorphic. So to understand all finite dimensional vector spaces, it is enough to understand \(\R^n\). The next activity provides a little practice with isomorphisms.%
\begin{activity}{}{g:activity:idp26898312}%
Assume that each of the following maps is a linear transformation. Which, if any, is an isomorphism? Justify your reasoning.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(T: \pol_1 \to \R^2\) defined by \(T(a_0+a_1t) = \left[ \begin{array}{c} a_1\\a_1+a_0 \end{array} \right]\)%
\item{}\(T: \M_{2 \times 2} \to \pol_3\) defined by \(T\left(\left[ \begin{array}{cc} a\amp b\\ c\amp d \end{array} \right] \right) = a+bt+ct^2+dt^3\).%
\item{}\(T: \R^2 \to \pol_2\) defined by \(T\left(\left[ \begin{array}{c} a\\ b \end{array} \right] \right) = a+bt+at^2\).%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_lin_trans_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp26903944}%
Let \(T : \pol_2 \to \pol_3\) be defined by \(T(p(t)) = tp(t)+p(0)\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(T\) is a linear transformation.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp26905352}{}\quad{}To show that \(T\) is a linear transformation we must show that \(T(p(t)+q(t)) = T(p(t)) + T(q(t))\) and \(T(cp(t)) = cT(p(t))\) for every \(p(t)\), \(q(t)\) in \(\pol_2\) and any scalar \(c\). Let \(p(t)\) and \(q(t)\) be in \(\pol_2\). Then%
\begin{align*}
T(p(t)+q(t)) \amp = t(p(t)+q(t)) + (p(0)+q(0))\\
\amp = \left(tp(t) + p(0)\right) + \left( tq(t) + q(0) \right)\\
\amp = T(p(t)) + T(q(t))
\end{align*}
and%
\begin{equation*}
T(cp(t)) = tcp(t) + cp(0) = c\left(tp(t)+p(0)\right) = cT(p(t))\text{.}
\end{equation*}
Therefore, \(T\) is a linear transformation.%
\item{}Is \(T\) one-to-one? Justify your answer.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp26911752}{}\quad{}To determine if \(T\) is one-to-one, we find \(\Ker(T)\). Suppose \(p(t) = a_0+a_1t+a_2t^2  \in \Ker(T)\). Then%
\begin{equation*}
0 = T(p(t)) = tp(t) + p(0) =  a_0t+a_1t^2 +a_2t^3 + a_0\text{.}
\end{equation*}
Equating coefficients on like powers of \(t\) shows that \(a_0 = a_1 = a_2 = 0\). Thus, \(p(t) = 0\) and \(\Ker(T) = \{0\}\). Thus, \(T\) is one-to-one.%
\item{}\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Find three different polynomials in \(\Range(T)\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp26920840}{}\quad{}We can find three polynomials in \(\Range(T)\) by applying \(T\) to three different polynomials in \(\pol_2\). So three polynomials in \(\Range(T)\) are%
\begin{align*}
T(1) \amp = t+1\\
T(t) \amp = t^2\\
T\left(t^2\right) \amp = t^3\text{.}
\end{align*}
%
\item{}Find, if possible, a polynomial that is not in \(\Range(T)\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp26916616}{}\quad{}A polynomial \(q(t) = b_0+b_1t+b_2t^2+b_3t^3\) is in \(\Range(T)\) if \(q(t) = T(p(t))\) for some polynomial \(p(t) = a_0+a_1t+a_2t^2\) in \(\pol_2\). This would require that%
\begin{equation*}
b_0+b_1t+b_2t^2+b_3t^3 = a_0t+a_1t^2+a_2t^3+a_0\text{.}
\end{equation*}
But this would mean that \(b_0 = a_0 = b_1\). So the polynomial \(1+2t+t^2+t^3\) is not in \(\Range(T)\).%
\item{}Describe \(\Range(T)\). What is \(\dim(\Range(T))\)? Is \(T\) an onto transformation? Explain.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp26925832}{}\quad{}Let \(q(t)\) be in \(\Range(T)\). Then \(q(t) = T(p(t))\) for some polynomial \(p(t) = a_0+a_1t+a_2t^2 \in \pol_2\). Thus,%
\begin{align*}
q(t) \amp = T(p(t))\\
\amp = tp(t) + p(0)\\
\amp = a_0t+a_1t^2+a_2t^3 + a_0\\
\amp = a_0(t+1) + a_1t^2 +a_2t^3\text{.}
\end{align*}
Therefore, \(\Range(T) = \Span\{t+1, t^2, t^3\}\). Since the reduced row echelon form of \(\left[ \begin{array}{ccc} 1\amp 0\amp 0\\1\amp 0\amp 0\\0\amp 1\amp 0\\0\amp 0\amp 1 \end{array}  \right]\) is \(\left[ \begin{array}{ccc} 1\amp 0\amp 0\\0\amp 1\amp 0\\0\amp 0\amp 1\\0\amp 0\amp 0 \end{array}  \right]\), we conclude that the set \(\{t+1, t^2, t^3\}\) is linearly independent. Thus, \(\dim(\Range(T)) = 3\). Since \(\Range(T)\) is a three-dimensional subspace of the four-dimensional space \(\pol_3\), it follows that \(T\) is not onto.%
\end{enumerate}
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp26940424}%
Let \(T: \M_{2 \times 2} \to \pol_3\) be defined by%
\begin{equation*}
T\left( \left[ \begin{array}{cc} a\amp b\\c\amp d \end{array}  \right] \right) = a+bt+ct^2+dt^3\text{.}
\end{equation*}
%
\par
Show that \(T\) is an isomorphism from \(\M_{2 \times 2}\) to \(\pol_3\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp26939144}{}\quad{}We need to show that \(T\) is a linear transformation, and that \(T\) is both one-to-one and onto. We start by demonstrating that \(T\) is a linear transformation. Let \(A = [a_{ij}]\) and \(B = [b_{ij}]\) be in \(\M_{2 \times 2}\). Then%
\begin{align*}
T(A+B) \amp = T\left([a_{ij}+b_{ij}]\right)\\
\amp = \left(a_{11}+b_{11}\right) + \left(a_{12}+b_{12}\right)t + \left(a_{21}+b_{21}\right)t^2 + \left(a_{22}+b_{22}\right)t^3\\
\amp = \left(a_{11}+a_{12}t+a_{21}t^2+a_{22}t^3\right)  +  \left(b_{11}+b_{12}t+b_{21}t^2+b_{22}t^3\right)\\
\amp = T(A) + T(B)\text{.}
\end{align*}
%
\par
Now let \(c\) be a scalar. Then%
\begin{align*}
T(cA) \amp = T\left([ca_{ij}]\right)\\
\amp = \left(ca_{11}\right)+\left(ca_{12}\right)t+\left(ca_{21}\right)t^2+\left(ca_{22}\right)t^3\\
\amp = c\left(a_{11}+a_{12}t+a_{21}t^2+a_{22}t^3\right)\\
\amp = cT(A)\text{.}
\end{align*}
%
\par
Therefore, \(T\) is a linear transformation.%
\par
Next we determine \(\Ker(T)\). Suppose \(A \in \Ker(T)\). Then%
\begin{equation*}
0 = T(A) = a_{11}+a_{12}t+a_{21}t^2+a_{22}t^3\text{.}
\end{equation*}
%
\par
Equating coefficients of like powers of \(T\) shows that \(a_{ij} = 0\) for each \(i\) and \(j\). Thus, \(A = 0\) and \(\Ker(T) = \{0\}\). It follows that \(T\) is one-to-one.%
\par
Finally, we show that \(\Range(T) = \pol_3\). If \(q(t) = a+bt+ct^2+dt^3 \in \pol_3\), then \(T\left( \left[ \begin{array}{cc} a\amp b\\c\amp d \end{array} \right] \right) = q(t)\). So every polynomial in \(\pol_3\) is the image of some matrix in \(\M_{2 \times 2}\). It follows that \(T\) is onto and also that \(T\) is an isomorphism.%
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_lin_trans_summ}
Let \(T\) be a linear transformation from a vector space \(V\) to a vector space \(W\).%
\begin{itemize}[label=\textbullet]
\item{}A function \(T\) from a vector space \(V\) to a vector space \(W\) is a linear transformation if%
\begin{enumerate}
\item{}\(T(\vu + \vv) = T(\vu) + T(\vv)\) for all \(\vu\) and \(\vv\) in \(V\) and%
\item{}\(T(c\vu) = cT(\vu)\) for all \(\vu\) in \(V\) and all scalars \(c\).%
\end{enumerate}
%
\item{}Let \(T\) be a linear transformation from a vector space \(V\) to a vector space \(W\). The kernel of \(T\) is the set%
\begin{equation*}
\Ker(T) = \{\vx \in V : T(\vx) = \vzero\}\text{.}
\end{equation*}
The kernel of \(T\) is a subspace of \(V\).%
\item{}Let \(T\) be a linear transformation from a vector space \(V\) to a vector space \(W\). The transformation \(T\) is one-to-one if every vector in \(W\) is the image under \(T\) of at most one vector in \(V\). A linear transformation \(T\) is one-to-one if and only if \(\Ker(T) = \{\vzero\}\).%
\item{}Let \(T\) be a linear transformation from a vector space \(V\) to a vector space \(W\). The range of \(T\) is the set%
\begin{equation*}
\Range(T) = \{T(\vv) : \vv \in V\}\text{.}
\end{equation*}
The range of \(T\) is a subspace of \(W\).%
\item{}Let \(T\) be a linear transformation from a vector space \(V\) to a vector space \(W\). The linear transformation \(T\) is onto if every vector in \(W\) is the image under \(T\) of at least one vector in \(V\). The transformation \(T\) is onto if and only if \(\Range(T) = W\).%
\item{}An isomorphism from a vector space \(V\) to a vector space \(W\) is a linear transformation \(T : V \to W\) that is one-to-one and onto.%
\end{itemize}
%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_lin_trans_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp26976136}%
We have seen that \(C[a,b]\), the set of all continuous functions on the interval \([a,b]\) is a vector space. Let \(T : C[a,b] \to \R\) be defined by \(T(f) = \int_a^b f(x) \, dx\). Show that \(T\) is a linear transformation.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp26979336}{}\quad{}Use properties of the definite integral from calculus.%
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{x:exercise:ex_8_a_reflexive_isomorphism}%
If \(V\) is a vector space, prove that the identity map \(I:V \to V\) defined by \(I(\vx) = \vx\) for all \(\vx \in V\) is an isomorphism.%
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{x:exercise:ex_8_a_inverse_isomorphism}%
Let \(V\) and \(W\) be vector spaces and let \(T : V \to W\) be an isomorphism. Since \(T\) is a bijection, \(T\) has an inverse function. (Recall that two functions \(f\) and \(g\) are inverses if \((f \circ g)(x) = x\) for all \(x\) in the domain of \(g\) and \((g \circ f)(x) = x\) for all \(x\) in the domain of \(f\).)%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(T: \pol_1 \to \R^2\) be defined by \(T(a_0+a_1t) = \left[ \begin{array}{c} a_0\\a_0+a_1 \end{array} \right]\). Assume that \(T\) is an isomorphism. Show that \(S: \R^2 \to \pol_1\) defined by \(S([a \ b]^{\tr}) = a + (b-a)t\) is the inverse of \(T\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp26994696}{}\quad{}Show that \((S \circ T)(\vx) = \vx\) for all \(\vx\) in \(V\) and that \((T \circ S)(\vw) = \vw\) for all \(\vw\) in \(W\).%
\item{}Let \(T: \M_{2 \times 2} \to \pol_3\) be defined by \(T\left(\left[ \begin{array}{cc} a\amp b\\ c\amp d \end{array} \right] \right) = a+bt+ct^2+dt^3\). Assume that \(T\) is an isomorphism. Find the inverse of \(T\). Make sure to verify that your conjectured function really is the inverse of \(T\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp26992392}{}\quad{}Define \(S : \pol_3 \to \M_{2 \times 2}\) by%
\begin{equation*}
S(a+bt+ct^2+dt^3) = \left[ \begin{array}{cc} a\amp b\\ c\amp d \end{array}  \right]\text{.}
\end{equation*}
%
\item{}Now assume that \(T\) is an arbitrary isomorphism from a vector space \(V\) to a vector space \(W\). We know that the inverse \(T^{-1}\) of \(T\) satisfies the property that \(T^{-1}(\vw) = \vv\) whenever \(T(\vv) = \vw\). From previous courses you have probably shown that the inverse of a bijection is a bijection. You may assume this as fact. Show that \(T^{-1}\) is a linear transformation and, consequently, an isomorphism.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp26999944}{}\quad{}The property that \(T^{-1}(\vw) = \vv\) whenever \(T(\vv) = \vw\) is the key to this problem.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp27000328}%
Let \(U\), \(V\), and \(W\) be vector spaces, and let \(S : U \to V\) and \(T : V \to W\) be linear transformations. Determine which of the following is true. If a statement is true, verify it. If a statement is false, give a counterexample. (The result of this exercise, along with \hyperlink{x:exercise:ex_8_a_reflexive_isomorphism}{Exercise~{\xreffont 2}} and \hyperlink{x:exercise:ex_8_a_inverse_isomorphism}{Exercise~{\xreffont 3}}, shows that the isomorphism relation is reflexive, symmetric, and transitive. Thus, isomorphism is an equivalence relation.)%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(T \circ S\) is a linear transformation%
\item{}\(S \circ T\) is a linear transformation%
\item{}If \(S\) and \(T\) are one-to-one, then \(T \circ S\) is one-to-one.%
\item{}If \(S\) and \(T\) are onto, then \(T \circ S\) is onto.%
\item{}If \(S\) and \(T\) are isomorphisms, then \(T \circ S\) is an isomorphism.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp27011464}%
For each of the following maps, determine which is a linear transformation. If a mapping is not a linear transformation, provide a specific example to show that a property of a linear transformation is not satisfied. If a mapping is a linear transformation, verify that fact and then determine if the mapping is one-to-one and\slash{}or onto. Throughout, let \(V\) and \(W\) be vector spaces.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(T: V \to V\) defined by \(T(\vx) = -\vx\)%
\item{}\(T: V \to V\) defined by \(T(\vx) = 2\vx\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp27021960}%
Let \(V\) be a vector space. In this exercise, we will investigate mappings \(T: V \to V\) of the form \(T(\vx) = k\vx\), where \(k\) is a scalar.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}For which values, if any, of \(k\) is \(T\) a linear transformation?%
\item{}For which values of \(k\), if any, is \(T\) an isomorphism?%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{x:exercise:ex_8_a_isomorphic_dimension}%
Let \(V\) be a finite-dimensional vector space and \(W\) a vector space. Show that if \(T: V \to W\) is an isomorphism, and \(\CB = \{\vv_1, \vv_2, \ldots, \vv_n\}\) is a basis for \(V\), then \(\CC = \{T(\vv_1)\), \(T(\vv_2)\), \(\ldots\), \(T(\vv_n)\}\) is a basis for \(W\). Hence, if \(V\) is a vector space of dimension \(n\) and \(W\) is isomorphic to \(V\), then \(\dim(W) = n\) as well.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27028104}{}\quad{}Use the fact that \(T\) is one-to-one to show that \(\CC\) is linearly independent, and that \(T\) is onto to show that \(\CC\) spans \(W\).%
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp27037064}%
Let \(W = \left\{ \left[ \begin{array}{rr} x \amp -x \\ -x \amp x \end{array} \right] : x \text{ is a scalar } \right\}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(W\) is a subspace of \(\M_{2 \times 2}\).%
\item{}The space \(W\) is isomorphic to \(\R^k\) for some \(k\). Determine the value of \(k\) and explain why \(W\) is isomorphic to \(\R^k\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp27033992}%
Is it possible for a vector space to be isomorphic to one of its proper subspaces? Justify your answer.%
\end{divisionexercise}%
\begin{divisionexercise}{10}{}{}{g:exercise:idp27042312}%
Prove \hyperref[x:theorem:thm_8_a_ker]{Theorem~{\xreffont\ref{x:theorem:thm_8_a_ker}}}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27042440}{}\quad{}See \hyperref[x:exploration:pa_3_b]{Preview Activity~{\xreffont\ref{x:exploration:pa_3_b}}}.%
\end{divisionexercise}%
\begin{divisionexercise}{11}{}{}{g:exercise:idp27040776}%
Prove \hyperref[x:theorem:thm_8_a_one_to_one_kernel]{Theorem~{\xreffont\ref{x:theorem:thm_8_a_one_to_one_kernel}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27041544}{}\quad{}Show that \(T(\vv) = \vw\) has at most one solution for each \(\vw\) in \(W\).%
\end{divisionexercise}%
\begin{divisionexercise}{12}{}{}{g:exercise:idp26784008}%
Let \(V\) and \(W\) be finite dimensional vector spaces, and let \(T : V \to W\) be a linear transformation. Show that \(T\) is uniquely determined by its action on a basis for \(V\). That is, if \(\CB = \{\vv_1, \vv_2, \ldots, \vv_n\}\) is a basis for \(V\), and we know \(T(\vv_1)\), \(T(\vv_2)\), \(\ldots\), \(T(\vv_n)\), then we know exactly which vector \(T(\vu)\) is for any \(\vu\) in \(V\).%
\end{divisionexercise}%
\begin{divisionexercise}{13}{}{}{g:exercise:idp26782344}%
Let \(V\) and \(W\) be vectors spaces and let \(T: V \to W\) be a linear transformation. If \(V'\) is a subspace of \(V\), let%
\begin{equation*}
T(V') = \{T(\vv) : \vv \text{ is in }  V'\}\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(T(V')\) is a subspace of \(W\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27128840}{}\quad{}Think of \(T(V')\) as the range of a suitable restriction of \(T\).%
\item{}Suppose \(\dim(V') = n\). Show that \(\dim(T(V')) \leq n\). Can \(\dim(T(V')) = n\) even if \(T\) is not an isomorphism? Justify your answer.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27123080}{}\quad{}Use Exercise 12. It is possible.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{14}{}{}{x:exercise:ex_8_a_transformation_space}%
Let \(V\) and \(W\) be vector spaces and define \(L(V,W)\) to be the set of all linear transformations from \(V\) to \(W\). If \(S\) and \(T\) are in \(L(V,W)\) and \(c\) is a scalar, then define \(S+T\) and \(cT\) as follows:%
\begin{equation*}
(S+T)(\vv) = S(\vv) + T(\vv) \ \ \text{ and }  \ \ (cT)(\vv) = cT(\vv)
\end{equation*}
for all \(\vv\) in \(V\). Prove that \(L(V,W)\) is a vector space.%
\end{divisionexercise}%
\begin{divisionexercise}{15}{}{}{x:exercise:ex_8_a_rank_nullity}%
The Rank-Nullity Theorem (\hyperref[x:theorem:thm_3_d_rank_nullity]{Theorem~{\xreffont\ref{x:theorem:thm_3_d_rank_nullity}}}) states that the rank plus the nullity of a matrix equals the number of columns of the matrix. There is a similar result for linear transformations that we prove in this exercise. Let \(T : V \to W\) be a linear transformation from an \(n\) dimensional vector space \(V\) to an \(m\) dimensional vector space \(W\). Show that \(\dim(\Ker(T)) + \dim(\Range(T)) = n\). (Hint: Let \(\{\vv_1, \vv_2, \ldots, \vv_k\}\) be a basis for \(\Ker(T)\). Extend this basis to a basis \(\{\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\), \(\vv_{k+1}\), \(\ldots\), \(\vv_n\}\) of \(V\). Use this basis to find a basis for \(\Range(T)\).)%
\end{divisionexercise}%
\begin{divisionexercise}{16}{}{}{g:exercise:idp27144968}%
Let \(V\) and \(W\) be vector spaces with \(\dim(V) = n = \dim(W)\), and let \(T: V \to W\) be a linear transformation.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Prove or disprove: If \(T\) is one-to-one, then \(T\) is also onto.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27146120}{}\quad{}Use \hyperlink{x:exercise:ex_8_a_rank_nullity}{Exercise~{\xreffont 15}}.%
\item{}Prove or disprove: If \(T\) is onto, then \(T\) is also one-to-one.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27152136}{}\quad{}Use \hyperlink{x:exercise:ex_8_a_rank_nullity}{Exercise~{\xreffont 15}}.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{17}{}{}{g:exercise:idp27148296}%
Suppose \(\CB_1\) is a basis of an \(n\)-dimensional vector space \(V\), and \(\CB_2\) is a basis of an \(n\)-dimensional vector space \(W\). Show that the map \(T_{VW}\) which sends every \(\vx\) in \(V\) to the vector \(\vy\) in \(W\) such that \([\vx]_{\CB_1} = [\vy]_{\CB_2}\) is a linear transformation. (Combined with \hyperlink{x:exercise:ex_5_d_T_VW}{Exercise~{\xreffont 16}} in \hyperref[x:chapter:chap_coordinate_vectors]{Section~{\xreffont\ref{x:chapter:chap_coordinate_vectors}}}, we can conclude that \(T_{VW}\) is an isomorphism. Thus, any two vectors space of dimension \(n\) are isomorphic.)%
\end{divisionexercise}%
\begin{divisionexercise}{18}{}{}{g:exercise:idp27161992}%
There is an important characterization of linear functionals that we examine in this problem. A linear functional is a linear transformation from a vector space \(V\) to \(\R\). Let \(V\) be a finite-dimensional inner product space, and let \(T: V \to \R\) be a linear functional. We will show that there is a vector \(\vx\) in \(V\) such that%
\begin{equation*}
T(\vv) = \langle \vv, \vx \rangle
\end{equation*}
for every vector \(\vv\) in \(V\). That is, every linear functional on an inner product space can be represented by an inner product with a fixed vector. Let \(n = \dim(V)\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Suppose \(\Range(T) = \{\vzero\}\). What vector could we use for \(\vx\)?%
\item{}Now assume that \(\dim(\Range(T)) > 0\). Note that \(\Range(T)\) is a subspace of \(\R\). Explain why the only subspaces of \(\R\) are \(\{0\}\) and \(\R\). Conclude that \(\Range(T) = \R\).%
\item{}Explain why \(\dim(\Ker(T)) = n-1\).%
\item{}Let \(\{\vv_1, \vv_2, \ldots, \vv_{n-1}\}\) be an orthonormal basis for \(\Ker(T)\). Explain why there is a vector \(\vv_n\) in \(V\) so that \(\{\vv_1, \vv_2, \ldots, \vv_{n-1}, \vv_n\}\) is an orthonormal basis for \(V\).%
\item{}Let \(\vv\) be in \(V\). Explain why%
\begin{equation*}
\vv = \langle \vv, \vv_1 \rangle \vv_1 + \langle \vv, \vv_2 \rangle \vv_2 + \cdots + \langle \vv, \vv_n \rangle \vv_n\text{.}
\end{equation*}
%
\item{}Now explain why%
\begin{equation*}
T(\vv) = \langle \vv, \vv_n \rangle T(\vv_n)\text{.}
\end{equation*}
%
\item{}Finally, explain why \(T(\vv_n)\vv_n\) is the vector \(\vx\) we are looking for.%
\item{}As an example, let \(T: \pol_2 \to \R\) be defined by \(T(p(t)) = p(0)\). Consider \(\pol_2\) as an inner product space with the inner product%
\begin{equation*}
\langle p(t), q(t) \rangle = \int_{-1}^1 p(t)q(t) \, dt\text{.}
\end{equation*}
Find a polynomial \(h(t)\) so that%
\begin{equation*}
T(p(t)) = \int_{-1}^1 p(t)h(t) \, dt
\end{equation*}
for every \(p(t)\) in \(\pol_2\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{19}{}{}{g:exercise:idp27174408}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
The mapping \(T: \M_{2 \times 2} \to \R\) defined by \(T(A) = \det(A)\) is a linear transformation.%
\item{}\lititle{True\slash{}False.}\par%
The mapping \(T: \M_{2 \times 2} \to \M_{2 \times 2}\) defined by \(T(A) = A^{-1}\) if \(A\) is invertible and \(T(A) = 0\) (the zero matrix) otherwise is a linear transformation.%
\item{}\lititle{True\slash{}False.}\par%
Let \(V\) and \(W\) be vector spaces. The mapping \(T: V \to W\) defined by \(T(\vx) = \vzero\) is a linear transformation.%
\item{}\lititle{True\slash{}False.}\par%
If \(T: V \to W\) is a linear transformation, and if \(\{\vv_1, \vv_2\}\) is a linearly independent set in \(V\), then the set \(\{T(\vv_1), T(\vv_2)\}\) is a linearly independent set in \(W\).%
\item{}\lititle{True\slash{}False.}\par%
A one-to-one transformation is a transformation where each input has a unique output.%
\item{}\lititle{True\slash{}False.}\par%
A one-to-one linear transformation is a transformation where each output can only come from a unique input.%
\item{}\lititle{True\slash{}False.}\par%
Let \(V\) and \(W\) be vector spaces with \(\dim(V) = 2\) and \(\dim(W) = 3\). A linear transformation from \(V\) to \(W\) cannot be onto.%
\item{}\lititle{True\slash{}False.}\par%
Let \(V\) and \(W\) be vector spaces with \(\dim(V) = 3\) and \(\dim(W) = 2\). A linear transformation from \(V\) to \(W\) cannot be onto.%
\item{}\lititle{True\slash{}False.}\par%
Let \(V\) and \(W\) be vector spaces with \(\dim(V) = 3\) and \(\dim(W) = 2\). A linear transformation from \(V\) to \(W\) cannot be one-to-one.%
\item{}\lititle{True\slash{}False.}\par%
If \(\vw\) is in the range of a linear transformation \(T\), then there is an \(\vx\) in the domain of \(T\) such that \(T(\vx)=\vw\).%
\item{}\lititle{True\slash{}False.}\par%
If \(T\) is a one-to-one linear transformation, then \(T(\vx)=\vzero\) has a non-trivial solution.%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Fractals via Iterated Function Systems}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Fractals via Iterated Function Systems}{}{Project: Fractals via Iterated Function Systems}{}{}{x:section:sec_proj_fractals}
In this project we will see how to use linear transformations to create iterated functions systems to generate fractals. We illustrate the idea with the Sierpinski triangle, an approximate picture of which is shown at left in \hyperref[x:figure:F_Striangle]{Figure~{\xreffont\ref{x:figure:F_Striangle}}}.%
\begin{figureptx}{Left: An approximation of the Sierpinski triangle. Right: A triangle.}{x:figure:F_Striangle}{}%
\begin{sidebyside}{2}{0.1}{0.1}{0.2}%
\begin{sbspanel}{0.3}%
\includegraphics[width=\linewidth]{external/8_a_Striangle_9.pdf}
\end{sbspanel}%
\begin{sbspanel}{0.3}%
\includegraphics[width=\linewidth]{external/8_a_triangle.pdf}
\end{sbspanel}%
\end{sidebyside}%
\tcblower
\end{figureptx}%
To make this figure, we need to identify linear transformations that can be put together to produce the Sierpinski triangle.%
\begin{project}{}{x:project:act_IFS_1}%
Let \(\vv_1 = [0 \ 0]^{\tr}\), \(\vv_2 = [1 \ 0]^{\tr}\), and \(\vv_3 = \left[ \frac{1}{2} \ \frac{\sqrt{3}}{4}\right]^{\tr}\) be three vectors in the plane whose endpoints form the vertices of a triangle \(P_0\) as shown at right in \hyperref[x:figure:F_Striangle]{Figure~{\xreffont\ref{x:figure:F_Striangle}}}. Let \(T : \R^2 \to \R^2\) be the linear transformation defined by \(T(\vx) = A \vx\), where \(A = \frac{1}{2} I_2 = \frac{1}{2} \left[ \begin{array}{cc} 1\amp 0 \\ 0\amp 1 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}What are \(T(\vv_1)\), \(T(\vv_2)\), and \(T(\vv_3)\)? Draw a picture of the figure \(T(P_0)\) whose vertices are the endpoint of these three vectors. How is \(T(P_0)\) related to \(P_0\)?%
\item{}Since the transformation \(T\) shrinks objects, we call \(T\) (or \(A\)) a \terminology{contraction mapping}). Notice that when we apply \(T\) to \(P_0\) it creates a smaller version of \(P_0\). The next step is to use \(T\) to make three smaller copies of \(P_0\), and then translate these copies to the vertices of \(P_0\). A translation can be performed by adding a vector to \(T(P_0)\). Find \(C_1\) so that \(C_1(P_0)\) is a copy of \(P_0\) half the size and translated so that \(C_1(\vv_1) = \vv_1\). Then find \(C_2\) so that \(C_2(P_0)\) is a copy of \(P_0\) half the size and translated so that \(C_2(\vv_2) = \vv_2\). Finally, find \(C_3\) so that \(C_3(P_0)\) is a copy of \(P_0\) half the size and translated so that \(C_3(\vv_3) = \vv_3\). Draw pictures of each to illustrate.%
\end{enumerate}
\end{project}%
\hyperref[x:project:act_IFS_1]{Project Activity~{\xreffont\ref{x:project:act_IFS_1}}} contains the information we need to create an iterated function system to produce the Sierpinski triangle. One more step should help us understand the general process.%
\begin{project}{}{x:project:act_IFS_2}%
Using the results of \hyperref[x:project:act_IFS_1]{Project Activity~{\xreffont\ref{x:project:act_IFS_1}}}, define \(P_{1,i}\) to be \(C_i(P_0)\) for each \(i\). That is, \(P_{1,1} = C_1(P_0)\), \(P_{1,2} = C_2(P_0)\), and \(P_{1,3} = C_3(P_0)\). So \(P_{1,i}\) is a triangle half the size of the original translated to the \(i\)th vertex of the original. Let \(P_1 = \bigcup_{i=1}^3 P_{1,i}\). That is, \(P_1\) is the union of the shaded triangles in \hyperref[x:figure:F_IFS_2]{Figure~{\xreffont\ref{x:figure:F_IFS_2}}}.%
\begin{figureptx}{\(C_1(P_0)\), \(C_2(P_0)\), and \(C_3(P_0)\).}{x:figure:F_IFS_2}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/8_a_IFS_2.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Apply \(C_1\) from \hyperref[x:project:act_IFS_1]{Project Activity~{\xreffont\ref{x:project:act_IFS_1}}} to \(P_1\). What is the resulting figure? Draw a picture to illustrate.%
\item{}Apply \(C_2\) from \hyperref[x:project:act_IFS_1]{Project Activity~{\xreffont\ref{x:project:act_IFS_1}}} to \(P_1\). What is the resulting figure? Draw a picture to illustrate.%
\item{}Apply \(C_3\) from \hyperref[x:project:act_IFS_1]{Project Activity~{\xreffont\ref{x:project:act_IFS_1}}} to \(P_1\). What is the resulting figure? Draw a picture to illustrate.%
\end{enumerate}
\end{project}%
The procedures from \hyperref[x:project:act_IFS_1]{Project Activity~{\xreffont\ref{x:project:act_IFS_1}}} and \hyperref[x:project:act_IFS_2]{Project Activity~{\xreffont\ref{x:project:act_IFS_2}}} can be continued, replacing \(P_0\) with \(P_1\), then \(P_2\), and so on. In other words, for \(i\) = 1, 2, and 3, let \(P_{2,i} = C_i(P_1)\). Then let \(P_2 = \bigcup_{i=1}^3 P_{2,i}\). A picture of \(P_2\) is shown at left in \hyperref[x:figure:F_Striangle_2]{Figure~{\xreffont\ref{x:figure:F_Striangle_2}}}. We can continue this procedure, each time replacing \(P_{j-1}\) with \(P_j\). A picture of \(P_9\) is shown at right in \hyperref[x:figure:F_Striangle_2]{Figure~{\xreffont\ref{x:figure:F_Striangle_2}}}.%
\begin{figureptx}{Left: \(P_2\). Right: \(P_9\).}{x:figure:F_Striangle_2}{}%
\begin{sidebyside}{2}{0.1}{0.1}{0.2}%
\begin{sbspanel}{0.3}%
\includegraphics[width=\linewidth]{external/8_a_Striangle_2.pdf}
\end{sbspanel}%
\begin{sbspanel}{0.3}%
\includegraphics[width=\linewidth]{external/8_a_Striangle_9.pdf}
\end{sbspanel}%
\end{sidebyside}%
\tcblower
\end{figureptx}%
If we continue this process, taking the limit as \(i\) approaches infinity, the resulting sequence of sets converges to a fixed set, in this case the famous Sierpinski triangle. So the picture of \(P_9\) in \hyperref[x:figure:F_Striangle_2]{Figure~{\xreffont\ref{x:figure:F_Striangle_2}}} is a close approximation of the Sierpinski triangle. This algorithm for building the Sierpinski triangle is called the \terminology{deterministic algorithm}. A Sage cell to illustrate this algorithm for producing approximations to the Sierpinski triangle can be found at \href{http://faculty.gvsu.edu/schlicks/STriangle_Sage.html}{\nolinkurl{faculty.gvsu.edu/schlicks/STriangle_Sage.html}}.%
\par
\index{iterated function system}\index{attractor} In general, an iterated function system (IFS) is a finite set \(\{f_1, f_2, \ldots,
f_m\}\) of contraction mappings from \(\R^2\) to \(\R^2\). If we start with a set \(S_0\), and let \(S_1 = \bigcup f_i(S_0)\), \(S_2 = \bigcup f_i(S_1)\), and so on, then in ``nice'' cases (we won't concern ourselves with what ``nice'' means here), the sequence \(S_0\), \(S_1\), \(S_2\), \(\ldots\) converges in some sense to a fixed set. That fixed set is called the \terminology{attractor} of the iterated function system. It is that attractor that is the fractal. It is fascinating to note that our starting set does not matter, the same attractor is obtained no matter which set we use as \(S_0\).%
\par
\index{self-similar set} One aspect of a fractal is that fractals are self-similar, that is they are made up of constituent pieces that look just like the whole. More specifically, a subset \(S\) of \(\R^2\) is \terminology{self-similar} if it can be expressed in the form%
\begin{equation*}
S = S_1 \cup S_2 \cup \cdots \cup S_k
\end{equation*}
for non-overlapping sets \(S_1\), \(S_2\), \(\ldots\), \(S_k\), each of which is congruent to \(S\) by the same scaling factor. So, for example, the Sierpinski triangle is self-similar, made up of three copies of the whole, each contracted by a factor of \(2\).%
\par
If we have an IFS, then we can determine the attractor by drawing the sequence of sets that the IFS generates. A more interesting problem is, given a self-similar figure, whether we can construct an IFS that has that figure as its attractor.%
\begin{project}{}{x:project:IFS_carpet}%
A picture of an emerging Sierpinski carpet is shown at left in \hyperref[x:figure:F_Scarpet]{Figure~{\xreffont\ref{x:figure:F_Scarpet}}}. A Sage cell to illustrate this algorithm for producing approximations to the Sierpinski carpet can be found at \href{http://faculty.gvsu.edu/schlicks/SCarpet_Sage.html}{\nolinkurl{faculty.gvsu.edu/schlicks/SCarpet_Sage.html}}. In this activity we will see how to find an iterated function system that will generate this fractal.%
\begin{figureptx}{A Sierpinski carpet.}{x:figure:F_Scarpet}{}%
\begin{image}{0.25}{0.5}{0.25}%
\includegraphics[width=\linewidth]{external/8_a_Scarpet_6.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}To create an IFS to generate this fractal, we need to understand how many self-similar pieces make up this figure. Use the image at right in \hyperref[x:figure:F_Scarpet]{Figure~{\xreffont\ref{x:figure:F_Scarpet}}} to determine how many pieces we need.%
\item{}For each of the self-similar pieces identified in part (a), find a linear transformation and a translation that maps the entire figure to the self-similar piece.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27264136}{}\quad{}You could assume that the carpet is embedded in the unit square.%
\item{}Test your IFS to make sure that it actually generates the Sierpinski carpet. There are many websites that allow you to do this, one of which is \href{http://cs.lmu.edu/\~ray/notes/ifs/}{\nolinkurl{cs.lmu.edu/\~ray/notes/ifs/}}. In this program, the mapping \(f\) defined by%
\begin{equation*}
f(\vx) = \left[ \begin{array}{cc} a\amp b\\c\amp d \end{array}  \right] \vx + \left[ \begin{array}{c} e\\f \end{array} \ \right]
\end{equation*}
is represented as the string%
\begin{equation*}
a \ b \ c \ d \ e \ f\text{.}
\end{equation*}
Most programs generally use a different algorithm to create the attractor, plotting points instead of sets. In this algorithm, each contraction mapping is assigned a probability as well (the larger parts of the figure are usually given higher probabilities), so you will enter each contraction mapping in the form%
\begin{equation*}
a \ b \ c \ d \ e \ f \ p
\end{equation*}
where \(p\) is the probability attached to that contraction mapping. As an example, the IFS code for the Sierpinski triangle is%
\begin{equation*}
\begin{array}{ccccccc} 0.5  \amp 0  \amp 0  \amp 0.5  \amp 0  \amp 0    \amp 0.33 \\ 0.5  \amp 0  \amp 0  \amp 0.5  \amp 1  \amp 0    \amp 0.33 \\ 0.5  \amp 0  \amp 0  \amp 0.5  \amp 0.5  \amp 0.43  \amp 0.33 \end{array}
\end{equation*}
%
\end{enumerate}
\end{project}%
The contraction mappings we have used so far only involve contractions and translations. But we do not have to restrict ourselves to just these types of contraction mappings.%
\begin{project}{}{x:project:act_Fractal_reflection}%
Consider the fractal represented in \hyperref[x:figure:F_Fractal_reflection]{Figure~{\xreffont\ref{x:figure:F_Fractal_reflection}}}. Find an IFS that generates this fractal. Explain your reasoning.%
\par
Check with a fractal generator to ensure that you have an appropriate IFS.%
\begin{figureptx}{A fractal.}{x:figure:F_Fractal_reflection}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/8_a_Reflection_fractal.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27270920}{}\quad{}Two reflections are involved.%
\end{project}%
We conclude our construction of fractals with one more example. The contraction mappings in iterated function can also involve rotations.%
\begin{project}{}{x:project:act_Fractal_dragon}%
Consider the Lévy Dragon fractal shown at left in \hyperref[x:figure:F_Fractal_rotation]{Figure~{\xreffont\ref{x:figure:F_Fractal_rotation}}}. Find an IFS that generates this fractal. Explain your reasoning.%
\par
Check with a fractal generator to ensure that you have an appropriate IFS.%
\begin{figureptx}{The Lévy Dragon.}{x:figure:F_Fractal_rotation}{}%
\begin{sidebyside}{2}{0}{0}{0}%
\begin{sbspanel}{0.5}%
\includegraphics[width=\linewidth]{external/8_a_Rotation_fractal.pdf}
\end{sbspanel}%
\begin{sbspanel}{0.5}%
\includegraphics[width=\linewidth]{external/8_a_Rotation_triangles.pdf}
\end{sbspanel}%
\end{sidebyside}%
\tcblower
\end{figureptx}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27271560}{}\quad{}Two rotations are involved \textemdash{} think of the fractal as contained in a blue triangle as shown at right in \hyperref[x:figure:F_Fractal_rotation]{Figure~{\xreffont\ref{x:figure:F_Fractal_rotation}}}.%
\end{project}%
We end this project with a short discussion of fractal dimension. The fractals we have seen are very strange in many ways, one of which is dimension. We have studied the dimensions of subspaces of \(R^n\) \textemdash{} each subspace has an integer dimension that is determined by the number of elements in a basis. Fractals also have a dimension, but the dimension of a fractal is generally not an integer. To try to understand fractal dimension, notice that a line segment is self-similar. We can break up a line segment into 2 non-overlapping line segments of the same length, or 3, or 4, or any number of non-overlapping segments of the same length. A square is slightly different. We can partition a square into 4 non-overlapping squares, or 9 non-overlapping squares, or \(n^2\) non-overlapping squares for any positive integer \(n\) as shown in \hyperref[x:figure:F_Squares]{Figure~{\xreffont\ref{x:figure:F_Squares}}}.%
\begin{figureptx}{Self-similar squares.}{x:figure:F_Squares}{}%
\begin{image}{0.15}{0.7}{0.15}%
\includegraphics[width=\linewidth]{external/8_a_similar.pdf}
\end{image}%
\tcblower
\end{figureptx}%
Similarly, we can break up a cube into \(n^3\) non-overlapping congruent cubes. A line segment lies in a one-dimensional space, a square in a two-dimensional space, and a cube in the three-dimensional space. Notice that these dimensions correspond to the exponent of the number of self-similar pieces with scaling \(n\) into which we can partition the object. We can use this idea of dimension in a way that we can apply to fractals. Let \(d(\text{ object } )\) be the dimension of the object. We can partition a square into \(n^2\) non-overlapping squares, so%
\begin{equation*}
d(\text{ square } ) = 2 = 2\frac{\ln(n)}{\ln(n)} = \frac{\ln(n^2)}{\ln(n)} = \frac{\ln(\text{ number of self-similar pieces } )}{\ln(\text{ scaling factor } )}\text{.}
\end{equation*}
%
\par
Similarly, for the cube we have%
\begin{equation*}
d(\text{ cube } ) = 3 = 3\frac{\ln(n)}{\ln(n)} = \frac{\ln(n^3)}{\ln(n)} = \frac{\ln(\text{ number of self-similar pieces } )}{\ln(\text{ scaling factor } )}\text{.}
\end{equation*}
%
\par
We can then take this as our definition of the dimension of a self-similar object, when the scaling factors are all the same (the fern fractal in \hyperref[x:figure:F_Fern]{Figure~{\xreffont\ref{x:figure:F_Fern}}} is an example of a fractal generated by an iterated function system in which the scaling factors are not all the same).%
\begin{definition}{}{g:definition:idp27277576}%
\index{Hausdorff dimension}%
The \terminology{fractal} or \terminology{Hausdorff} dimension \(h\) of a self-similar set \(S\) is%
\begin{equation*}
h(S) = \frac{\ln(\text{ number of self-similar pieces } )}{\ln(\text{ scaling factor } )}\text{.}
\end{equation*}
%
\end{definition}
\begin{project}{}{x:project:act_IFS_dimension}%
Find the fractal dimensions of the Sierpinski triangle and the Sierpinski carpet. These are well-known and you can look them up to check your result. Then find the fractal dimension of the fractal with IFS%
\begin{equation*}
\begin{array}{ccccccc} 0.38 \amp 0 \amp 0 \amp 0.38 \amp -0.59   \amp 0.81  \amp 0.2 \\ 0.38 \amp 0 \amp 0 \amp 0.38 \amp -0.95   \amp -0.31   \amp 0.2 \\ 0.38 \amp 0 \amp 0 \amp 0.38 \amp 0   \amp -1     \amp 0.2 \\ 0.38 \amp 0 \amp 0 \amp 0.38 \amp 0.95   \amp -0.31   \amp 0.2 \\ 0.38 \amp 0 \amp 0 \amp 0.38 \amp 0.59   \amp 0.81   \amp 0.2 \end{array}\text{.}
\end{equation*}
%
\par
You might want to draw this fractal using an online generator.%
\end{project}%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 38 The Matrix of a Linear Transformation}
\typeout{************************************************}
%
\begin{chapterptx}{The Matrix of a Linear Transformation}{}{The Matrix of a Linear Transformation}{}{}{x:chapter:chap_transformation_matrix}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp27285896}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}How do we represent a linear transformation from \(\R^n\) to \(\R^m\) as a matrix transformation?%
\item{}How can we represent any linear transformation from a finite dimensional vector space \(V\) to a finite dimensional vector space \(W\) as a matrix transformation?%
\item{}In what ways is representing a linear transformation as a matrix transformation useful?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Secret Sharing Algorithms}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Secret Sharing Algorithms}{}{Application: Secret Sharing Algorithms}{}{}{x:section:sec_appl_secret}
Suppose we have a secret that we want or need to share with several individuals. For example, a bank manager might want to share pieces of the code for the bank vault with several employees so that if the manager is not available, some subset of these employees could open the vault if they work together. This allows for a significant amount of security while still making the code available if needed. As another example, in order to keep passwords secure (as they can be hard to remember), a person could implement a secret sharing scheme. The person would generate a set of shares for a given password and store them in several different places. If the person forgets the password, it can be reconstructed by collecting some set of these shares. Since the shares can be stored in many different places, the password is relatively secure.%
\par
The idea behind secret sharing algorithms is to break a secret into a number (\(n\)) of pieces and give each person one piece. A code is then created in such a way that any \(k\) individuals could combine their information and learn the secret, but no group of fewer than \(k\) individuals could. This is called a \((k,n)\) threshold scheme. One secret sharing algorithm is Shamir's Secret Sharing, which, as we will see later in this section, involves Lagrange polynomials. In order to implement this algorithm, we will use matrices of linear transformations from polynomials spaces to \(\R^n\).%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_mtxof_trans_intro}
A matrix transformation \(T\) from \(\R^n\) to \(\R^m\) is a linear transformation defined by \(T(\vx) = A\vx\) for some \(m \times n\) matrix \(A\). We can use the matrix to quickly determine if \(T\) is one-to-one or onto \textemdash{} if every column of \(A\) contains a pivot, then \(T\) is one-to-one, and if every row of \(A\) contains a pivot, then \(T\) is onto. As we will see, we can represent any linear transformation between finite dimensional vector spaces as a matrix transformation, which will allow us to use all of the tools we have developed for matrices to study linear transformations. We will begin with linear transformations from \(\R^n\) to \(\R^m\).%
\begin{exploration}{}{x:exploration:pa_8_b}%
Let \(T\) be a linear transformation from \(\R^2\) into \(\R^3\). Let's also say that we know the following information about \(T\):%
\begin{equation*}
T\left(\left[ \begin{array}{c} 1 \\ 0 \end{array}  \right] \right) = \left[ \begin{array}{r} 0 \\ 2 \\ -1 \end{array}  \right] \text{ and } T\left(\left[ \begin{array}{c} 0 \\ 1 \end{array}  \right] \right) = \left[ \begin{array}{r} 1 \\ 3 \\ 2 \end{array}  \right]\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find \(T\left(\left[ \begin{array}{c} 2 \\ 0 \end{array} \right] \right)\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27303176}{}\quad{}Use the fact that \(T\) is a linear transformation.%
\item{}Find \(T\left(\left[ \begin{array}{c} 1 \\ 1 \end{array} \right] \right)\).%
\item{}Find \(T\left(\left[ \begin{array}{c} 2 \\ 3 \end{array} \right] \right)\).%
\item{}Find \(T\left(\left[ \begin{array}{c} a \\ b \end{array} \right] \right)\) for any real numbers \(a\) and \(b\).%
\item{}Is it possible to find a matrix \(A\) so that \(T\left(\left[ \begin{array}{c} a \\ b \end{array} \right] \right) = A \left[ \begin{array}{c} a \\ b \end{array} \right]\) for any real numbers \(a\) and \(b\)? If so, what is this matrix? If not, why not?%
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Linear Transformations from \(\R^n\) to \(\R^m\)}
\typeout{************************************************}
%
\begin{sectionptx}{Linear Transformations from \(\R^n\) to \(\R^m\)}{}{Linear Transformations from \(\R^n\) to \(\R^m\)}{}{}{x:section:sec_trans_rn_rm}
\hyperref[x:exploration:pa_8_b]{Preview Activity~{\xreffont\ref{x:exploration:pa_8_b}}} illustrates the method for representing a linear transformation from \(\R^n\) to \(\R^m\) as a matrix transformation. We now consider the general context.%
\begin{activity}{}{x:activity:act_8_b_matrix_of_a_linear_transformation}%
Let \(T\) be a linear transformation from \(\R^n\) to \(\R^m\). Let \(\ve_i\) be the \(i\)th column of the \(n \times n\) identity matrix. In other words,%
\begin{equation*}
\ve_1 = \left[ \begin{array}{c} 1 \\ 0 \\ 0 \\ 0 \\ \vdots \\ 0 \end{array}  \right], \ve_2 = \left[ \begin{array}{c} 0 \\ 1 \\ 0 \\ 0  \\ \vdots \\ 0 \end{array}  \right], \ve_3 = \left[ \begin{array}{c} 0 \\ 0 \\ 1 \\ 0 \\ \vdots \\ 0 \end{array}  \right], \ldots , \ve_n = \left[ \begin{array}{c} 0 \\ 0 \\ 0 \\ \vdots \\ 1 \end{array}  \right]
\end{equation*}
in \(\R^n\). The set \(\{\ve_1, \ve_2, \ldots, \ve_n\}\) is the standard basis for \(\R^n\). Let \(\vx = \left[ \begin{array}{c} x_1 \\ x_2 \\ \vdots \ \\x_n \end{array}  \right]\) in \(\R^n\). Explain why \(T(\vx) = A \vx\) for every \(\vx\) in \(\R^n\), where%
\begin{equation*}
A = [\ T(\ve_1) \; \;  T(\ve_2)  \; \; \cdots \; \;  T(\ve_n) \ ]
\end{equation*}
is the matrix with columns \(T(\ve_1)\), \(T(\ve_2)\), \(\ldots\), \(T(\ve_n)\).%
\end{activity}%
As we discovered in \hyperref[x:activity:act_8_b_matrix_of_a_linear_transformation]{Activity~{\xreffont\ref{x:activity:act_8_b_matrix_of_a_linear_transformation}}}, a linear transformation from \(\R^n\) to \(\R^m\) can be expressed as a matrix transformation where the columns of the matrix are given by the images of \(\ve_i\), the columns of the \(n\times n\) identity matrix. This matrix is called the \terminology{standard matrix} of the linear transformation.%
\begin{definition}{}{g:definition:idp27321096}%
\index{linear transformation!standard matrix}%
If \(T\) is a linear transformation from \(\R^n\) to \(\R^m\), the \terminology{standard matrix} for \(T\) is the matrix%
\begin{equation*}
A= [ \ T(\ve_1) \;\;  T(\ve_2)  \;\; \cdots \;\;  T(\ve_n) \ ]
\end{equation*}
with columns \(T(\ve_1)\), \(T(\ve_2)\), \(\ldots\), \(T(\ve_n)\), where \(\{\ve_1, \ve_2, \ldots, \ve_n\}\) is the standard basis for \(\R^n\). More specifically, \(T(\vx)=A\vx\) for every \(\vx\) in \(\R^n\).%
\end{definition}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Matrix of a Linear Transformation}
\typeout{************************************************}
%
\begin{sectionptx}{The Matrix of a Linear Transformation}{}{The Matrix of a Linear Transformation}{}{}{x:section:sec_mtx_lin_trans}
We saw in the previous section that any linear transformation \(T\) from \(\R^n\) to \(\R^m\) is in fact a matrix transformation. In this section, we turn to the general question \textemdash{} can any linear transformation \(T: V \to W\) between an \(n\)-dimensional vector space \(V\) and an \(m\)-dimensional vector space \(W\) be represented in some way as a matrix transformation? This will allow us to extend ideas like eigenvalues and eigenvectors to arbitrary linear transformations. We begin by investigating an example.%
\par
The general process for representing a linear transformation as a matrix transformation is as described in \hyperref[x:activity:act_8_b_general_matrix_transformation]{Activity~{\xreffont\ref{x:activity:act_8_b_general_matrix_transformation}}}.%
\begin{activity}{}{x:activity:act_8_b_general_matrix_transformation}%
Let \(V\) be an \(n\) dimensional vector space and \(W\) an \(m\) dimensional vector space and suppose \(T : V \to W\) is a linear transformation. Let \(\CB = \{\vv_1, \vv_2, \ldots, \vv_n\}\) be a basis for \(V\) and \(\CC = \{\vw_1, \vw_2, \ldots, \vw_m\}\) a basis for \(W\). Let \(\vv\) be in \(V\) so that%
\begin{equation*}
\vv = c_1\vv_1 + c_2 \vv_2 + \cdots + c_n \vv_n
\end{equation*}
for some scalars \(c_1\), \(c_2\), \(\ldots\), \(c_n\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}What is \([\vv]_\CB\)?%
\item{}Note that for \(\vv\) in \(V\), \(T(\vv)\) is an element in \(W\), so we can consider the coordinate vector for \(T(\vv)\) with respect to \(\CC\). Explain why%
\begin{equation}
[T(\vv)]_{\CC} = c_1[T(\vv_1)]_{\CC} + c_2 [T(\vv_2)]_{\CC} + \cdots + c_n [T(\vv_n)]_{\CC}\text{.}\label{x:men:eq_Matrix_linear_transformation}
\end{equation}
%
\item{}Now find a matrix \(A\) so that Equation \hyperref[x:men:eq_Matrix_linear_transformation]{({\xreffont\ref{x:men:eq_Matrix_linear_transformation}})} has the form \([T(\vv)]_{\CC} = A [\vv]_{\CB}\). We call the matrix \(A\) to be the \terminology{matrix for \(T\) relative to the bases \(\CB\) and \(\CC\)} and denote this matrix as \([T]_{\CB}^{\CC}\) so that%
\begin{equation*}
[T(\vv)]_{\CC} = [T]_{\CB}^{\CC} [\vv]_{\CB}\text{.}
\end{equation*}
%
\end{enumerate}
\end{activity}%
\hyperref[x:activity:act_8_b_general_matrix_transformation]{Activity~{\xreffont\ref{x:activity:act_8_b_general_matrix_transformation}}} shows us how to represent a linear transformation as a matrix transformation.%
\begin{definition}{}{g:definition:idp27354248}%
\index{linear transformation!matrix of}%
Let \(T\) be a linear transformation from a vector space \(V\) with basis \(\CB = \{\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_n\}\) to a vector space \(W\) with basis \(\CC = \{\vw_1, \vw_2, \ldots, \vw_m\}\). The \terminology{matrix for \(T\) with respect to the bases \(\CB\) and \(\CC\)} is the matrix%
\begin{equation*}
[T]_{\CB}^{\CC} = \left[  [T(\vv_1)]_{\CC}  \ [T(\vv_2)]_{\CC}  \ [T(\vv_3)]_{\CC}  \ \cdots  \ [T(\vv_n)]_{\CC} \right]\text{.}
\end{equation*}
%
\end{definition}
If \(V=W\) and \(\CC = \CB\), then we use the notation \([T]_{\CB}\) as a shorthand for \([T]_{\CB}^{\CC}\). The matrix \([T]_{\CB}^{\CC}\) has the property that%
\begin{equation*}
[T(\vv)]_{\CC} = A [\vv]_{\CB}
\end{equation*}
for any \(\vv\) in \(V\). Recall that we can find the unique vector in \(W\) whose coordinate vector is \([T(\vv)]_{\CC}\), so we have completely realized the transformation \(T\) as a matrix transformation. In essence, we are viewing \(T\) as a composite of linear transformations, first from \(V\) to \(\R^n\), then from \(\R^n\) to \(\R^m\), then from \(\R^m\) to \(W\) as illustrated in \hyperref[x:figure:F_matrix_transformation]{Figure~{\xreffont\ref{x:figure:F_matrix_transformation}}}.%
\begin{figureptx}{Visualizing the matrix of a linear transformation.}{x:figure:F_matrix_transformation}{}%
\begin{image}{0.3}{0.4}{0.3}%
\includegraphics[width=\linewidth]{external/visualize_mtx_lin_trans.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\begin{activity}{}{g:activity:idp27374984}%
Let \(T : \pol_2 \to \pol_1\) be defined by \(T(p(t))= p'(t)\). Let \(\CB = \{1+t,1-t,t+t^2\}\) be a basis for \(\pol_2\) and \(\CC = \{1, t\}\) be a basis for \(\pol_1\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the matrix \([T]_{\CB}^{\CC}\).%
\item{}Find \([1+t+t^2]_{\CB}\). Then use the matrix \([T]_{\CB}^{\CC}\) to calculate \([T(1+t+t^2)]_{\CC}\). (Hint: Use the fact that \(1+t+t^2 =\frac{1}{2}(1+t) + \frac{1}{2}(1-t) + (t+t^2)\).)%
\item{}Calculate \(T(1+t+t^2)\) directly from the definition of \(T\) and compare to your result from part (b).%
\item{}Find the matrix \([T]_{\CB}^{\CC'}\) where \(\CC'=\{1+t, 1-t\}\). Use the facts that%
\begin{equation*}
-1 = -\frac{1}{2}(1+t) - \frac{1}{2}(1-t) \ \text{ and }  \ 1+2t = \frac{3}{2}(1+t) - \frac{1}{2}(1-t)\text{.}
\end{equation*}
%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  A Connection between \(\Ker(T)\) and a Matrix Representation of \(T\)}
\typeout{************************************************}
%
\begin{sectionptx}{A Connection between \(\Ker(T)\) and a Matrix Representation of \(T\)}{}{A Connection between \(\Ker(T)\) and a Matrix Representation of \(T\)}{}{}{x:section:sec_ker_mtx}
Recall that we defined the kernel of a matrix transformation \(T: \R^n \to \R^m\) to be the set of vectors that \(T\) maps to the zero vector. If \(T\) is the matrix transformation defined by \(T(\vx) = A \vx\), we also saw that the kernel of \(T\) is the same as the null space of \(A\). We can make a similar connection between a linear transformation and a matrix that defines the transformation.%
\begin{activity}{}{x:activity:act_8_b_kernel}%
Let \(T : V \to W\) be a linear transformation from an \(n\)-dimensional vector space \(V\) to an \(m\)-dimensional vector space \(W\) with additive identity \(\vzero_W\). Let \(\CB\) be a basis for \(V\) and \(\CC\) a basis for \(W\). Let \(A = [T]_{\CB}^{\CC}\). Let \(T'\) be the matrix transformation from \(\R^n\) to \(\R^m\) defined by \(T'(\vx) = A\vx\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that if \(\vv\) is in \(\Ker(T)\), then \([\vv]_{\CB}\) is in \(\Nul A\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27393688}{}\quad{}Apply an appropriate coordinate transformation.%
\item{}Let \(\CB = \{\vv_1, \vv_2, \ldots, \vv_n\}\) and \(\CC = \{\vw_1, \vw_2, \ldots, \vw_m\}\). Show that if the vector \(\vx = [x_1 \ x_2 \ \cdots \ x_n]\) is in \(\Nul A\), then the vector \(\vv = x_1\vv_1 + x_2 \vv_2 + \cdots x_n \vv_n\) is in \(\Ker(T)\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27393176}{}\quad{}Note that \([\vv]_{\CB} = \vx\).%
\end{enumerate}
\end{activity}%
\hyperref[x:activity:act_8_b_kernel]{Activity~{\xreffont\ref{x:activity:act_8_b_kernel}}} shows that if \(T: V \to W\) is a linear transformation from an \(n\)-dimensional vector space \(V\) with basis \(\CB\) to an \(m\)-dimensional vector space \(W\) with basis \(\CC\), then there is a one-to-one correspondence between vectors in \(\Ker(T)\) and vectors in \(\Nul [T]_{\CB}^{\CC}\). Recall that \(T\) is one-to-one if and only if \(\Ker(T) = \{\vzero_V\}\), where \(\vzero_V\) is the additive identity of \(V\). So \(T\) will be one-to-one if and only if \(\Nul [T]_{\CB}^{\CC} = \{\vzero\}\). In other words, \(T\) is one-to-one if and only if every column of \(\Nul [T]_{\CB}^{\CC}\) is a pivot column. Note that this does not depend on the basis \(\CB\) and \(\CC\).%
\par
A similar argument shows that \(T\) is onto if and only if every row of \([T]_{\CB}^{\CC}\) contains a pivot. This is left to the exercises (see \hyperlink{x:exercise:ex_8_b_range_matrix}{Exercise~{\xreffont 7}}).%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_mtxof_trans_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp27415704}%
Let \(T: \pol_1 \to \pol_2\) be defined by \(T(a+bt) = b+at+2at^2\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(T\) is a linear transformation.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp27408024}{}\quad{}Let \(p(t)=a+bt\) and \(q(t)=c+dt\) be in \(\pol_1\). Then%
\begin{align*}
T(p(t)+q(t)) \amp = T((a+c)+(b+d)t)\\
\amp = (b+d) + (a+c)t + 2(a+c)t^2\\
\amp = (b+at+2at^2) + (d+ct+2ct^2)\\
\amp = T(p(t)) + T(q(t))\text{.}
\end{align*}
Now let \(k\) be a scalar. Then%
\begin{align*}
T(kp(t)) \amp = T((ka)+(kb)t)\\
\amp = (kb)+(ka)t + (2ka)t^2\\
\amp = k(b+at+2at^2)\\
\amp =kT(p(t))\text{.}
\end{align*}
We conclude that \(T\) is a linear transformation.%
\item{}Let \(\CS_1 = \{1,t\}\) and \(\CS_2 = \{1,t,t^2\}\) be the standard bases for \(\pol_1\) and \(\pol_2\), respectively. Find the matrix \([T]_{\CS_2}^{\CS_1}\) for \(T\) with respect to \(\CS_2\) and \(\CS_1\). Use the matrix \([T]_{\CS_2}^{\CS_1}\) to determine if \(T\) is one-to-one and\slash{}or onto. Explain your reasoning. Use technology as appropriate.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp27421336}{}\quad{}Recall that%
\begin{equation*}
[T]_{\CS_1}^{\CS_2} = \left[ [T(1)]_{\CS_2} \ [T(t)]_{\CS_2} \right] = \left[ [t+2t^2]_{\CS_2} \ [1]_{\CS_2} \right] = \left[ \begin{array}{cc} 0\amp 1\\1\amp 0\\2\amp 0 \end{array}  \right]\text{.}
\end{equation*}
The linear transformation \(T\) is one-to-one and\slash{}or onto if and only if the matrix transformation defined by \([T]_{\CS_1}^{\CS_2}\) is one-to-one and\slash{}or onto. The reduced row echelon form of \([T]_{\CS_1}^{\CS_2}\) is \(\left[ \begin{array}{cc} 1\amp 0\\0\amp 1\\0\amp 0 \end{array}  \right]\). Since \([T]_{\CS_1}^{\CS_2}\) has a pivot in every column, \(T\) is one-to-one. However, \([T]_{\CS_1}^{\CS_2}\) does not have a pivot in every row, so \(T\) is not onto.%
\item{}Use the matrix \([T]_{\CS_2}^{\CS_1}\) to find \(\Ker(T)\) and \(\Range(T)\). If possible, find a basis for each.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp27431704}{}\quad{}From part (b) we know that \(T\) is one-to-one, so \(\Ker(T) = \{0\}\). Let \(q(t)\) be a polynomial in \(\Range(T)\). Then there is a polynomial \(p(t)\) in \(\pol_1\) such that \(T(p(t)) = q(t)\). It follows that%
\begin{equation*}
[q(t)]_{\CS_2} = [T(p(t))]_{\CS_2} = [T]_{\CS_1}^{\CS_2}[p(t)]_{\CS_1}\text{.}
\end{equation*}
If \(p(t) = a_0+a_1t\), then%
\begin{align*}
_{\CS_2}  \amp = \left[ \begin{array}{cc} 0\amp 1\\
1\amp 0\\
2\amp 0 \end{array} \right] \left[ \begin{array}{c} a_0\\
a_1 \end{array} \right]\\
\amp = \left[ \begin{array}{c} a_1\\
a_0\\
2a_0 \end{array} \right]\\
\amp = a_0\left[ \begin{array}{c} 0\\
1\\
2 \end{array} \right] + a_1\left[ \begin{array}{c} 1\\
0\\
0\end{array} \right]\text{.}
\end{align*}
Thus, \(q(t)\) is in the span of \(t+2t^2\) and \(1\). So \(\Range(T) = \Span\{1, t+2t^2\}\). Since neither \(1\) nor \(t+2t^2\) is a scalar multiple of the other, the vectors \(1\) and \(t+2t^2\) are linearly independent. Thus, \(\{1, t+2t^2\}\) is a basis for \(\Range(T)\).%
\item{}Let \(\CB = \{1+t,1-t\}\) be a basis for \(\pol_1\) and \(\CC = \{t, 1-t^2, t+t^2\}\) a basis for \(\pol_2\). Find the matrix \([T]_{\CB}^{\CC}\) for \(T\) with respect to \(\CC\) and \(\CB\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp27442840}{}\quad{}Recall that%
\begin{equation*}
[T]_{\CB}^{\CC} = \left[ [T(1+t)]_{\CC} \ [T(1-t)]_{\CC} \right] = \left[ [1+2t+t^2]_{\CC} \ [1-t^2]_{\CC} \right] = \left[ \begin{array}{cc} 0\amp 0\\1\amp 1\\2\amp 0 \end{array}  \right]\text{.}
\end{equation*}
%
\item{}Find \(T(1-2t)\) using the matrix \([T]_{\CB}^{\CC}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp27445912}{}\quad{}To find \(T(1-2t)\) using the matrix \([T]_{\CB}^{\CC}\), recall that \([T(p(t))]_{\CC} = [T]_{\CB}^{\CC}[p(t)]_{\CB}\). First note that \([1-2t]_{\CB} = \left[ \begin{array}{r} -\frac{1}{2} \\ \frac{3}{2} \end{array}  \right]\). So%
\begin{equation*}
[T(p(t))]_{\CC} =  [T]_{\CB}^{\CC}[p(t)]_{\CB} = \left[ \begin{array}{cc} 1\amp 0\\0\amp 1\\0\amp 0 \end{array}  \right] \left[ \begin{array}{r} -\frac{1}{2} \\ \frac{3}{2} \end{array}  \right] = \left[ \begin{array}{r} 0 \\ 1 \\ -1 \end{array}  \right]\text{.}
\end{equation*}
This makes%
\begin{equation*}
T(1-2t) = 0(t) + (1)\left(1-t^2\right) + (-1)\left(t+t^2\right) = 1-t-2t^2\text{.}
\end{equation*}
To check, using the definition of \(T\) we have%
\begin{equation*}
T(1-2t) = t(1-2t) + (1-2t) = 1-t-2t^2\text{.}
\end{equation*}
%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp27443864}%
Let \(T: V \to W\) be a linear transformation. Let \(\CB = \{\vv_1, \vv_2, \ldots, \vv_n\}\) be a basis for \(V\) and \(\CC = \{\vw_1, \vw_2, \ldots, \vw_m\}\) a basis for \(W\). Let \(S: \R^n \to \R^m\) be the matrix transformation defined by \(S(\vx) = [T]_{\CB}^{\CC} \vx\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(\vw\) be a vector in \(\Range(T)\). Show that \([\vw]_{\CC}\) is in \(\Col \ [T]_{\CB}^{\CC}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp27453336}{}\quad{}Let \(\vw\) be a vector in \(\Range(T)\). Then there exists a vector \(\vv\) in \(V\) such that \(T(\vv) = \vw\). It follows that%
\begin{equation*}
[\vw]_{\CC} = [T(\vv)]_{\CC} = [T]_{\CB}^{\CC} [\vv]_{\CC}\text{.}
\end{equation*}
Recall that the vectors of the form \(A \vz\) all linear combinations of the columns of \(A\) with weights from \(\vz\), so  \([T]_{\CB}^{\CC} [\vv]_{\CC}\) (or \([\vw]_{\CC}\)) is in \(\Col \ [T]_{\CB}^{\CC}\).%
\item{}If \(\vw \in \Range(T)\), part (a) shows that \([\vw]_{\CC}\) is in \(\Col \ [T]_{\CB}^{\CC}\). So the coordinate transformation \([ \ ]_{\CC}\) maps \(\Range(T)\) into \(\Col \ [T]_{\CB}^{\CC}\). Define \(R: \Range(T) \to \Col \ [T]_{\CB}^{\CC}\) to be this coordinate transformation. That is, \(R(\vw) = [\vw]_{\CC}\). (As a coordinate transformation, \(R\) is a linear transformation.) We know that coordinate transformations are one-to-one. Show that \(R\) is also an onto transformation.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp27461400}{}\quad{}Let \(R: \Range(T) \to \Col \ [T]_{\CB}^{\CC}\) be the coordinate transformation \(R(\vw) = [\vw]_{\CC}\) for each \(\vw \in \Range(T)\). To show that \(R\) is an onto transformation, let \(\vy\) be in \(\Col [T]_{\CB}^{\CC}\). Then there exists \(\vx = [x_1 \ x_2 \ \ldots \ x_n]^{\tr}\) in \(\R^n\) such that \([T]_{\CB}^{\CC}\vx=\vy\). Let \(\vv = x_1\vv_1 + x_2 \vv_2 + \cdots + x_n \vv_n\). Then \(\vv\) is in \(V\) and \([\vv]_{\CB} = \vx\). Also,%
\begin{equation*}
[T(\vv)]_{\CC} = [T]_{\CB}^{\CC} [\vv]_{\CB} = [T]_{\CB}^{\CC} \vx = \vy\text{.}
\end{equation*}
So if we let \(\vw=T(\vv)\) in \(\Range(T)\), then \([\vw]_{\CC} = \vy\) and \(\vy\) and \(R\) is onto.%
\item{}What can we conclude about the relationship between the vector spaces \(\Range(T)\) and \(\Col \ [T]_{\CB}^{\CC}\)? What must then be true about \(\dim(\Range(T))\) and \(\dim(\Col \ [T]_{\CB}^{\CC})\)?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp27467288}{}\quad{}Since \(R\) is a one-to-one and onto linear transformation from \(\Range(T)\) to \(\Col \ [T]_{\CB}^{\CC}\), it follows that \(\Range(T)\) and \(\Col \ [T]_{\CB}^{\CC}\) are isomorphic vector spaces, and therefore we also have \(\dim(\Range(T)) = \dim(\Col \ [T]_{\CB}^{\CC})\).%
\end{enumerate}
When we connect the results of this example with the result of \hyperlink{x:exercise:ex_8_b_Ker_Nul}{Exercise~{\xreffont 5}} in this section, we obtain a linear transformation analog of the Rank-Nullity Theorem. That is, if \(T\) is a linear transformation from a vector space \(V\) of dimension \(n\) with basis \(\CB\) to a vector space \(W\) of dimension \(m\) with basis \(\CC\), then%
\begin{equation*}
\dim(\Ker(T)) + \dim(\Range(T)) = \dim(\Nul \ [T]_{\CB}^{\CC}) + \dim(\Col \ [T]_{\CB}^{\CC}) = n\text{.}
\end{equation*}
%
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_mtxof_trans_summ}
%
\begin{itemize}[label=\textbullet]
\item{}The standard matrix for a linear transformation \(T\) from \(\R^n\) to \(\R^m\) is the matrix%
\begin{equation*}
A = [T(\ve_1) \ T(\ve_2) \ \cdots \ T(\ve_n)]\text{,}
\end{equation*}
where \(\{\ve_1, \ve_2, \ldots, \ve_n\}\) is the standard basis for \(\R^n\). Then \(T\) is the matrix transformation defined by \(T(\vx) = A\vx\) for all \(\vx\) in \(\R^n\).%
\item{}Let \(V\) be an \(n\) dimensional vector space and \(W\) an \(m\) dimensional vector space and suppose \(T : V \to W\) is a linear transformation. Let \(\CB = \{\vv_1, \vv_2, \ldots, \vv_n\}\) be a basis for \(V\) and \(\CC = \{\vw_1, \vw_2, \ldots, \vw_m\}\) a basis for \(W\). The matrix for \(T\) with respect to the bases \(\CB\) and \(\CC\) is the matrix%
\begin{equation*}
[T]_{\CB}^{\CC} = \left[  [T(\vv_1)]_{\CC}  \ [T(\vv_2)]_{\CC}  \ [T(\vv_3)]_{\CC}  \ \cdots  \ [T(\vv_n)]_{\CC} \right]\text{.}
\end{equation*}
If \(V=W\) and \(\CC = \CB\), then we use the notation \([T]_{\CB}\) as a shorthand for \([T]_{\CB}^{\CC}\). The matrix \([T]_{\CB}^{\CC}\) has the property that%
\begin{equation*}
[T(\vv)]_{\CC} = [T]_{\CB}^{\CC} [\vv]_{\CB}
\end{equation*}
for any \(\vv\) in \(V\).%
\item{}The matrix for a linear transformation allows us to represent any linear transformation between finite dimensional vectors spaces as a matrix transformation. When we view a linear transformation \(T\) as a matrix transformation, we are able to use the matrix tools we have developed to understand \(T\).%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_mtxof_trans_exer}
\begin{divisionexercise}{1}{}{}{x:exercise:ex_8_b_matrix_transformation}%
Let \(T : \pol_2 \to \pol_3\) be defined by \(T(a_0+a_1t+a_2t^2) = (a_0+a_1) + (a_1+a_2)t + a_0t^2 + a_2t^3\). You may assume that \(T\) is a linear transformation. Let \(\CB = \{p_0(t), p_1(t), p_2(t)\}\), where \(p_0(t)=1\), \(p_1(t)=t\), and \(p_2(t) = t^2\), be the standard basis for \(\pol_2\) and \(\CC=\{q_0(t),
q_1(t), q_2(t), q_3(t)\}\), where \(q_0(t) = 1\), \(q_1(t) = t\), \(q_2(t)=t^2\), and \(q_3(t)=t^3\), the standard basis for \(\pol_3\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Write the polynomial \(r(t) = r_0+r_1t+r_2t^2\) as a linear combination \(c_0p_0(t)+c_1p_1(t)+c_2p_2(t)\) of the basis vectors in \(\CB\). Identify the weights \(c_0\), \(c_1\), and \(c_2\). What is \([r(t)]_{\CB}\)?%
\item{}Without doing any calculations, explain why%
\begin{equation*}
T(r(t)) = c_0T(p_0(t)) + c_1 T(p_1(t)) + c_2 T(p_2(t))\text{.}
\end{equation*}
%
\item{}Without doing any calculations, explain why%
\begin{equation*}
[T(r(t))]_{\CC} = c_0[T(p_0(t))]_{\CC} + c_1 [T(p_1(t))]_{\CC} + c_2 [T(p_2(t))]_{\CC}\text{.}
\end{equation*}
%
\item{}Explicitly determine \([T(p_0(t))]_{\CC}\), \([T(p_1(t))]_{\CC}\), \([T(p_2(t))]_{\CC}\).%
\item{}Combine the results of parts (c) and (d) to find a matrix \(A\) so that%
\begin{equation*}
[T(r(t))]_{\CC} = A [r(t)]_{\CB}\text{.}
\end{equation*}
%
\item{}Use the matrix \(A\) to find \([T(1+t-t^2)]_{\CC}\). Then use this vector to calculate \(T(1+t-t^2)\).%
\item{}Calculate \(T(1+t-t^2)\) directly from the rule for \(T\) and compare to the previous result.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp27521560}%
Let \(V\) and \(W\) be finite dimensional vector spaces, and let \(S\) and \(T\) be linear transformations from \(V\) to \(W\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}The sum \(S+T\) is defined as%
\begin{equation*}
(S+T)(\vx) = S(\vx) + T(\vx)
\end{equation*}
for all \(\vx\) in \(V\). Let \(\CB\) be a basis for \(V\) and \(\CC\) a basis for \(W\). Is the statement%
\begin{equation*}
[S+T]_{\CB}^{\CC} = [S]_{\CB}^{\CC} + [T]_{\CB}^{\CC}
\end{equation*}
true or false? If true, prove the statement. If false, provide a counterexample.%
\item{}The scalar multiple \(cT\), for a scalar \(c\), is defined as%
\begin{equation*}
(cT)(\vx) = cT(\vx)
\end{equation*}
for all \(\vx\) in \(V\). Let \(\CB\) be a basis for \(V\) and \(\CC\) a basis for \(W\). Is the statement%
\begin{equation*}
[cT]_{\CB}^{\CC} = c[T]_{\CB}^{\CC}
\end{equation*}
true or false? If true, prove the statement. If false, provide a counterexample.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp27531672}%
If \(V\), \(W\), and \(U\) are finite dimensional vector spaces and \(S:V \to W\) and \(T:W \to U\) are linear transformations, the composite \(T \circ S\) is defined as%
\begin{equation*}
(T \circ S)(\vx) = T(S(\vx))
\end{equation*}
for all \(\vx\) in \(V\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Prove that \(T \circ S:V\to U\) is a linear transformation.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27531928}{}\quad{}Use the linearity of both \(S\) and \(T\).%
\item{}Let \(\CB\) be a basis for \(V\), \(\CC\) a basis for \(W\), and \(\CD\) a basis for \(U\). Is the statement%
\begin{equation*}
[T \circ S]_{\CB}^{\CD} = [T]_{\CC}^{\CD}  [S]_{\CB}^{\CC}
\end{equation*}
true or false? If true, prove the statement. If false, provide a counterexample.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp27546904}%
Let \(V\) be a finite dimensional vector space with basis \(\CB\), and let \(T\) be a one-to-one linear transformation from \(V\) to \(V\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Use the matrix \([T]_{\CB}\) to explain why \(T\) is also onto. (Recall that we use the shorthand notation \([T]_{\CB}\) for \([T]_{\CB}^{\CB}\).)%
\item{}Since \(T\) is one-to-one and onto, as a function \(T\) has an inverse defined by%
\begin{equation*}
T^{-1}(\vy) = \vx
\end{equation*}
whenever \(T(\vx) = \vy\). Show that \(T^{-1}\) is a linear transformation from \(V\) to \(V\).%
\item{}Is the statement%
\begin{equation*}
[T^{-1}]_{\CB} = \left([T]_{\CB}\right)^{-1}
\end{equation*}
true or false? If true, prove the statement. If false, provide a counterexample.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{x:exercise:ex_8_b_Ker_Nul}%
Let \(V\) and \(W\) be vector spaces with \(\dim(V) = n\) and \(\dim(W) = m\), and let \(T : V \to W\). Let \(\CB\) be a basis for \(V\) and \(\CC\) a basis for \(W\). There is a connection between \(\Ker(T)\) and \(\Nul [T]_{\CB}^{\CC}\). Find the connection and verify it.%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{x:exercise:ex_8_b_transformation_space}%
Let \(V\) and \(W\) be vector spaces and define \(L(V,W)\) to be the set of all linear transformations from \(V\) to \(W\) as in \hyperlink{x:exercise:ex_8_a_transformation_space}{Exercise~{\xreffont 14}} of \hyperref[x:chapter:chap_linear_transformation]{Section~{\xreffont\ref{x:chapter:chap_linear_transformation}}}. The set \(L(V,W)\) is a vector space with the operations as as follows for \(S\) and \(T\) are in \(L(V,W)\) and \(c\) a scalar:%
\begin{equation*}
(S+T)(\vv) = S(\vv) + T(\vv) \ \ \text{ and }  \ \ (cT)(\vv) = cT(\vv)
\end{equation*}
for all \(\vv\) in \(V\). If \(\dim(V) = n\) and \(\dim(W) = m\), find the dimension of \(L(V,W)\). (Hint: Let \(\CB\) be a basis for \(V\) and \(\CC\) a basis for \(W\). What can be said about the mapping that sends \(T\) in \(L(V,W)\) to \([T]_{\CB}^{\CC}\)? Then use \hyperlink{x:exercise:ex_8_a_isomorphic_dimension}{Exercise~{\xreffont 7}} in \hyperref[x:chapter:chap_linear_transformation]{Section~{\xreffont\ref{x:chapter:chap_linear_transformation}}}.)%
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{x:exercise:ex_8_b_range_matrix}%
Let \(T : V \to W\) be a linear transformation from an \(n\)-dimensional vector space \(V\) to an \(m\)-dimensional vector space \(W\). Let \(\CB\) be a basis for \(V\) and \(\CC\) a basis for \(W\). Let \(A = [T]_{\CB}^{\CC}\). Let \(T'\) be the matrix transformation from \(\R^n\) to \(\R^m\) defined by \(T'(\vx) = A\vx\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that if \(\vw\) is in \(\Range(T)\), then \([\vw]_{\CC}\) is in the range of \(T'\).%
\item{}Let \(\CB = \{\vv_1, \vv_2, \ldots, \vv_n\}\) and \(\CC = \{\vw_1, \vw_2, \ldots, \vw_m\}\). Show that if the vector \(\vy = [y_1 \ y_2 \ \cdots \ y_m]\) is in the range of \(T'\), then the vector \(\vw = y_1\vw_1 + y_2 \vw_2 + \cdots y_m \vw_m\) is in the range of \(T\).%
\item{}Explain why the results of (a) and (b) show that \(T\) is onto if and only if every row of \(A = [T]_{\CB}^{\CC}\) contains a pivot.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27586968}{}\quad{}How do we tell if \(T'\) is onto?%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp27582488}%
Our entire world is always in a state of motion. Much of this motion is vibration. When the vibrations of one object some into contact with the vibrations of another, the vibrations can be amplified. This is called \terminology{resonance}. Resonance appears in our lives every day, e.g., resonance is how food cooks in a microwave oven. A great example of resonance is the collapse of the Tacoma Narrows Bridge (just Google Tacoma Narrows Bridge to find some fascinating videos of this event). Resonance can be seen in mathematical models of vibrations. One such model is the second order differential equation%
\begin{equation*}
y'' + y = \cos(t)\text{,}
\end{equation*}
where \(y\) is some unknown function of \(t\) and \(y''\) is its second derivative. You can think of this system as modeling the oscillatory motion of a spring with mass attached to it. The cosine function in this example exerts an external force on the mass-spring system. (You don't need to know how this model is derived for this project.) Our goal is to find the solutions to this differential equation within the subspace \(Y = \Span \ \CB\), where \(\CB = \{\cos(t), \sin(t),
t\cos(t), t\sin(t)\}\) of \(\D^2\), the subspace of \(\F\) of functions with second derivatives (you may assume that \(\D^2\) is a subspace of \(\F\)). Let \(T: \D^2 \to \D^2\) be defined by \(T(f) = f'' + f\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(T\) is a linear transformation.%
\item{}Show that \(\CB\) is a basis for \(Y\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27589912}{}\quad{}You might consider using the Wronskian.%
\item{}Find the matrix of \(T\) with respect to the basis \(\CB\). That is, find \([T]_{\CB}\). (Recall that we use the shorthand notation \([T]_{\CB}\) for \([T]_{\CB}^{\CB}\).)%
\item{}Use the matrix to find \emph{all} solutions to the equation \(T(f) = \cos(t)\) in \(Y\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27598488}{}\quad{}If \(T(f) = \cos(t)\), what does that say about the relationship between \([T]_{\CB}^{\CB}\), \([\cos(t)]_{\CB}\), and \([f]_{\CB}\)?%
\item{}Sketch a few graphs of solutions to \(T(f) = \cos(t)\) and explain what they look like and how they are related to resonance.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp27597464}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
If \(T\) is a linear transformation from a vector space \(V\) to a vector space \(W\), and \([T]_{\CB}^{\CC}\) is a \(3 \times 2\) matrix for some bases \(\CB\) of \(V\) and \(\CC\) of \(W\), then \(T\) cannot be one-to-one.%
\item{}\lititle{True\slash{}False.}\par%
If \(T\) is a linear transformation from a vector space \(V\) to a vector space \(W\), and \([T]_{\CB}^{\CC}\) is a \(2 \times 3\) matrix for some bases \(\CB\) of \(V\) and \(\CC\) of \(W\), then \(T\) cannot be onto.%
\item{}\lititle{True\slash{}False.}\par%
If \(T\) is a linear transformation from a vector space \(V\) to a vector space \(W\), and \([T]_{\CB}^{\CC}\) is a \(2 \times 3\) matrix for some bases \(\CB\) of \(V\) and \(\CC\) of \(W\), then \(T\) cannot be one-to-one.%
\item{}\lititle{True\slash{}False.}\par%
If \(T\) is a linear transformation from a vector space \(V\) to a vector space \(W\), and \([T]_{\CB}^{\CC}\) is a \(3 \times 2\) matrix for some bases \(\CB\) of \(V\) and \(\CC\) of \(W\), then \(T\) cannot be onto.%
\item{}\lititle{True\slash{}False.}\par%
Let \(S\) be a linear transformation from a vector space \(Y\) to a vector space \(V\) and let \(T\) be a linear transformation from a vector space \(V\) to a vector space \(W\). Suppose that \([S]_{\CS}^{\CB}\) is a \(3 \times 4\) matrix and \([T]_{\CB}^{\CC}\) is a \(4 \times 2\) matrix for some bases \(\CS\) of \(Y\), \(\CB\) of \(V\) and \(\CC\) of \(W\). Then \([T \circ S]_{\CS}^{\CC}\) is a \(3 \times 2\) matrix.%
\item{}\lititle{True\slash{}False.}\par%
Let \(V\) be a finite dimensional vector space of dimension \(n\) with basis \(\CB\) and \(W\) a finite dimensional vector space of dimension \(m\) with basis \(\CC\). Let \(T\) be a linear transformation from \(V\) to \(W\). If the columns of the matrix \([T]_{\CB}^{\CC}\) are linearly independent, then the transformation \(T\) is onto.%
\item{}\lititle{True\slash{}False.}\par%
Let \(V\) be a finite dimensional vector space of dimension \(n\) with basis \(\CB\) and \(W\) a finite dimensional vector space of dimension \(m\) with basis \(\CC\). Let \(T\) be a linear transformation from \(V\) to \(W\). If the columns of the matrix \([T]_{\CB}^{\CC}\) are linearly independent, then the transformation \(T\) is one-to-one.%
\item{}\lititle{True\slash{}False.}\par%
Let \(V\) be a finite dimensional vector space of dimension \(n\) with basis \(\CB\) and \(W\) a finite dimensional vector space of dimension \(m\) with basis \(\CC\). Let \(T\) be a linear transformation from \(V\) to \(W\). If the matrix \([T]_{\CB}^{\CC}\) has \(n\) pivots, then the transformation \(T\) is onto.%
\item{}\lititle{True\slash{}False.}\par%
Let \(V\) be a finite dimensional vector space of dimension \(n\) with basis \(\CB\) and \(W\) a finite dimensional vector space of dimension \(m\) with basis \(\CC\). Let \(T\) be a linear transformation from \(V\) to \(W\). If the matrix \([T]_{\CB}^{\CC}\) has \(n\) pivots, then the transformation \(T\) is one-to-one.%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Shamir's Secret Sharing and Lagrange Polynomials}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Shamir's Secret Sharing and Lagrange Polynomials}{}{Project: Shamir's Secret Sharing and Lagrange Polynomials}{}{}{x:section:sec_proj_secret}
Shamir's Secret Sharing is a secret sharing algorithm developed by the Israeli cryptographer Adi Shamir, who also contributed to the invention of RSA algorithm. The idea is to develop shares related to a secret code that can be distributed in such a way that the secret code can only be discovered if a fixed number of shareholders combine their information.%
\par
The general algorithm for Shamir's Secret Sharing works by creating a polynomial with the secret code as one coefficient and random remaining coefficients. More specifically, to create a \((k,n)\) threshold scheme, let \(a_0\) be the secret code. Then choose at random \(k-1\) positive integers \(a_1\), \(a_2\), \(\ldots\), \(a_{k-1}\) and create the polynomial%
\begin{equation*}
p(t) = a_0 + a_1t + a_2 t^2 + \cdots + a_{k-1}t^{k-1}\text{,}
\end{equation*}
where \(a_0\) is the secret. Evaluate \(p(t)\) at \(n\) different inputs \(t_1\), \(t_2\), \(\ldots\), \(t_n\) to create \(n\) points \(P_1 = (t_1,p(t_1))\), \(P_2 = (t_2,p(t_2))\), \(\ldots\), \(P_n = (t_n, p(t_n))\). Each participant is given one of these points. Since any collection of \(k\) distinct points on the graph of \(p(t)\) uniquely determines \(p(t)\), any combination of \(k\) of the participants could reconstruct \(p(t)\). The secret is then \(p(0)\). (Note that, except under very restrictive circumstances, there is no polynomial of degree less than \(k\) that passes through the given \(k\) points, so it would be impossible for fewer than \(k\) participants to reconstruct the message.)%
\par
As an example, let our secret code be \(a_0=1234\). Let \(n = 6\) and \(k = 3\). Choose two positive integers at random, say \(a_1 = 100\) and \(a_2 = 38\). Then%
\begin{equation}
p(t) = 1234 + 100t + 38t^2\text{.}\label{x:men:eq_SSS_polynomial}
\end{equation}
%
\par
Now we construct \(6\) points by selecting \(6\) positive integers, say \(t_1 = 1\), \(t_2 = 2\), \(t_3 = 3\), \(t_4 = 4\), \(t_5 = 5\), and \(t_6 = 6\). Evaluating \(p(t)\) at \(t=1\) through \(t=6\) gives us the points \(P_1 = (1,1372)\), \(P_2 = (2,1586)\), \(P_3 = (3,1876)\), \(P_4 = (4,2242)\), \(P_5 = (5,2684)\), and \(P_6 = (6,3202)\). Our goal is to find the polynomial that passes through any three of these points. The next activity will show us how to find this polynomial.%
\begin{project}{}{x:project:act_SSS_1}%
It will be instructive to work in the most general setting. We restrict ourselves to the degree \(2\) case for now. Given three scalars \(t_1\), \(t_2\), and \(t_3\), define \(L: \pol_2 \to \R^3\) by%
\begin{equation*}
L(q(t)) = \left[ \begin{array}{c} q(t_1) \\  q(t_2) \\ q(t_3) \end{array}  \right]\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(L\) is a linear transformation.%
\item{}Find the matrix for \(L\) with respect to the standard bases \(\CB = \{\ve_1, \ve_2, \ve_3\}\) for \(\R^3\) and \(\CS = \{1,t,t^2\}\) for \(\pol_2\).%
\item{}Recall that the matrix \([L]_{\CS}^{\CB}\) has the property that \([L(q(t))]_{\CB} = [L]_{\CS}^{\CB}[q(t)]_{\CS}\) for any \(q(t)\) in \(\pol_2\). So \(L\) is one-to-one if and only if the matrix transformation defined by \([L]_{\CS}^{\CB}\) is one-to-one. Use this idea to show that \(L\) is one-to-one if and only if \(t_1\), \(t_2\), and \(t_3\) are all different. Use appropriate technology where needed.%
\item{}Given three points \((t_1,y_1)\),  \((t_2,y_2)\), and \((t_3,y_3)\) with distinct \(t_1\), \(t_2\), and \(t_3\), our objective is to find a polynomial \(p(t)\) such that \(p(t_i) = y_i\) for \(i\) from \(1\) to \(3\). We proceed by finding quadratic polynomials \(\ell_1(t)\), \(\ell_2(t)\), and \(\ell_3(t)\) so that \(L(\ell_1(t)) =  \ve_1\), \(L(\ell_2(t)) = \ve_2\), and \(L(\ell_3(t)) = \ve_3\). Explain why, if \(\ell_1(t)\), \(\ell_2(t)\), and \(\ell_3(t)\) are quadratic polynomials so that \(L(\ell_1(1)) =  \ve_1\), \(L(\ell_2(t)) = \ve_2\), and \(L(\ell_3(t)) = \ve_3\), and \(t_1\), \(t_2\), and \(t_3\) are all different, then the polynomial \(p(t)\) will satisfy%
\begin{equation}
p(t) = y_1 \ell_1(t) + y_2 \ell_2(t) + y_3 \ell_3(t)\text{.}\label{x:men:eq_Lagrange_2}
\end{equation}
%
\end{enumerate}
\end{project}%
\index{Lagrange polynomials} The polynomials \(\ell_1(t)\), \(\ell_2(t)\), and \(\ell_3(t)\) in \hyperref[x:project:act_SSS_1]{Project Activity~{\xreffont\ref{x:project:act_SSS_1}}} are examples of \terminology{Lagrange polynomials}. \hyperref[x:project:act_SSS_1]{Project Activity~{\xreffont\ref{x:project:act_SSS_1}}} shows that fitting polynomials to given points can be accomplished easily using linear combinations of Lagrange polynomials. Now we want to better understand the general formulas for the Lagrange polynomials \(\ell_i(t)\).%
\begin{project}{}{x:project:act_Lagrange_polynomials}%
In this activity we see how to find the quadratic polynomials \(\ell_1(t)\), \(\ell_2(t)\), and \(\ell_3(t)\) so that \(L(\ell_1(t)) =  \ve_1\), \(L(\ell_2(t)) = \ve_2\), and \(L(\ell_3(t)) = \ve_3\). Let \(\CB = \{\ve_1, \ve_2, \ve_3\}\) and \(\CS = \{1,t,t^2\}\) be the standard bases for \(\R^3\) and \(\pol_2\), respectively.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why the problem of solving the equations \(L(\ell_1(t)) =  \ve_1\), \(L(\ell_2(t)) = \ve_2\), and \(L(\ell_3(t)) = \ve_3\) is equivalent to solving the equations%
\begin{equation*}
[L]_{\CS}^{\CB}[\ell_1(t)]_{\CS} = \ve_1, \  [L]_{\CS}^{\CB}[\ell_2(t)]_{\CS} =\ve_2, \ \text{ and }  \  [L]_{\CS}^{\CB}[\ell_3(t)]_{\CS} =\ve_3\text{.}
\end{equation*}
%
\item{}Explain why we can solve the equations%
\begin{equation*}
[L]_{\CS}^{\CB}[\ell_1(t)]_{\CS} = \ve_1, \ [L]_{\CS}^{\CB}[\ell_2(t)]_{\CS} = \ve_2, \ \text{ and }  \  [L]_{\CS}^{\CB}[\ell_3(t)]_{\CS}= \ve_3
\end{equation*}
all at once by solving the matrix equation%
\begin{equation*}
[L]_{\CS}^{\CB} \left[[\ell_1(t)]_{\CS} \ [\ell_2(t)]_{\CS} \ [\ell_3(t)]_{\CS} \right] =  I_3\text{.}
\end{equation*}
What does this tell us about the relationship between the matrix \(\left[[\ell_1(t)]_{\CS} \ [\ell_2(t)]_{\CS} \ [\ell_3(t)]_{\CS} \right]\) and \([L]_{\CS}^{\CB}\)?%
\item{}Technology shows that%
\begin{equation*}
\left([L]_{\CS}^{\CB}\right)^{-1} = \left[  \begin{array}{ccc} \frac{t_2t_3}{(t_1-t_2)(t_1-t_3)} \amp  \frac{t_1t_3}{(t_2-t_1)(t_2-t_3)} \amp  \frac{t_1t_2}{(t_3-t_1)(t_3-t_2)}  \\  -\frac{t_2+t_3}{(t_1-t_2)(t_1-t_3)} \amp  -\frac{t_1+t_3}{(t_2-t_1)(t_2-t_3)} \amp  -\frac{t_1+t_2}{(t_3-t_1)(t_3-t_2)} \\  \frac{1}{(t_1-t_2)(t_1-t_3)} \amp  \frac{1}{(t_2-t_1)(t_2-t_3)} \amp  \frac{1}{(t_3-t_1)(t_3-t_2)} \end{array}  \right]\text{.}
\end{equation*}
Use \(\left([L]_{\CS}^{\CB}\right)^{-1}\) to determine the coefficients of \(\ell_1(t)\). Then apply some algebra to show that%
\begin{equation*}
\ell_1(t) =  \frac{(t-t_2)(t-t_3)}{(t_1-t_2)(t_1-t_3)}\text{.}
\end{equation*}
%
\item{}Find similar expressions for \(\ell_2(t)\) and \(\ell_3(t)\). Explain why these three polynomials satisfy the required conditions that \(L(\ell_1(t)) = \ve_1\), \(L(\ell_2(t)) = \ve_2\), and \(L(\ell_3(t)) = \ve_3\).%
\end{enumerate}
\end{project}%
Now we return to our secret code with \(a_0 = 1234\).%
\begin{project}{}{g:project:idp27742360}%
Pick any three of the points \(P_1 = (1,1372)\), \(P_2 = (2,1586)\), \(P_3 = (3,1876)\), \(P_4 = (4,2242)\), \(P_5 = (5,2684)\), and \(P_6 = (6,3202)\). Use the Lagrange polynomials \(\ell_1(t)\), \(\ell_2(t)\), and \(\ell_3(t)\) and \hyperref[x:men:eq_Lagrange_2]{({\xreffont\ref{x:men:eq_Lagrange_2}})} to find the polynomial whose graph contains the three chosen points. How does your polynomial compare to the polynomial \(p(t)\) in \hyperref[x:men:eq_SSS_polynomial]{({\xreffont\ref{x:men:eq_SSS_polynomial}})}?%
\end{project}%
The Shamir Secret Sharing algorithm depends on being able to find a unique polynomial \(p(t)\) that passes through the created points. We can understand this result with Lagrange polynomials.%
\begin{project}{}{x:project:act_SSS_Lagrange_general}%
The process described in \hyperref[x:project:act_Lagrange_polynomials]{Project Activity~{\xreffont\ref{x:project:act_Lagrange_polynomials}}} for finding the Lagrange polynomials can be applied to any number of points. Let \((t_1,y_1)\), \((t_2,y_2)\), \((t_3,y_3)\), \(\ldots\), \((t_n,y_n)\), and \((t_{n+1},y_{n+1})\) be \(n+1\) points with distinct \(t_i\). Generalizing the results of \hyperref[x:project:act_Lagrange_polynomials]{Project Activity~{\xreffont\ref{x:project:act_Lagrange_polynomials}}}, define \(\ell_i(t)\) for \(i\) from \(1\) to \(n+1\) as follows:%
\begin{align*}
\ell_i(t) \amp = \frac{(t-t_1)(t-t_2) \cdots (t-t_{i-1})(t-t_{i+1}) \cdots (t-t_n)(t-t_{n+1})}{(t_i-t_1)(t_i-t_2) \cdots (t_i-t_{i-1})(t_i-t_{i+1}) \cdots (t_i-t_n)(t_i-t_{n+1})}\\
\amp =\prod_{j \neq i} \frac{t-t_j}{t_i-t_j}\text{.}
\end{align*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why \(\ell_i(t)\) is in \(\pol_n\) for each \(i\).%
\item{}Explain why the polynomial \(\ell_i(t)\) satisfies \(\ell_i(t_i) = 1\) and \(\ell_i(t_j) = 0\) if \(i \neq j\).%
\item{}Let \(p(t) =  \sum_{i=1}^{n+1} y_i \ell_i(t)\). That is, \(p(t)\) is the polynomial%
\begin{equation*}
\sum_{i=1}^{n+1} y_i \frac{(t-t_1)(t-t_2) \dots (t-t_{i-1})(t-t_{i+1}) \dots (t-t_n)(t-t_{n+1})}{(t_i-t_1)(t_i-t_2) \dots (t_i-t_{i-1})(t_i-t_{i+1}) \dots (t_i-t_n)(t_i-t_{n+1})}\text{.}
\end{equation*}
Explain why \(p(t)\) is a polynomial in \(\pol_n\) whose graph contains the points \((t_i,y_i)\) for \(i\) from \(1\) to \(n+1\).%
\item{}The previous part demonstrates that there is always a polynomial in \(\pol_n\) whose graph contains the points \((t_1,y_1)\), \((t_2,y_2)\), \((t_3,y_3)\), \(\ldots\), \((t_n,y_n)\), and \((t_{n+1},y_{n+1})\) with distinct \(t_i\). The last piece we need is to know that this polynomial is unique. Use the fact that a polynomial of degree \(n\) can have at most \(n\) roots to show that if \(f(t)\) and \(g(t)\) are two polynomials in \(\pol_n\) whose graphs contain the points \((t_1,y_1)\), \((t_2,y_2)\), \(\ldots\), \((t_n,y_n)\), \((t_{n+1},y_{n+1})\) with distinct values of \(t_i\), then \(f(t) = g(t)\). This completes Shamir's Secret Sharing algorithm.%
\end{enumerate}
\end{project}%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 39 Eigenvalues of Linear Transformations}
\typeout{************************************************}
%
\begin{chapterptx}{Eigenvalues of Linear Transformations}{}{Eigenvalues of Linear Transformations}{}{}{x:chapter:chap_transformations_eigenvalues}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp27772056}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}How, and under what conditions, do we define the eigenvalues and eigenvectors of a linear transformation?%
\item{}How do we find eigenvalues and eigenvectors of a linear transformation? What important result provides that our method for doing so works?%
\item{}What does it mean to diagonalize a linear transformation?%
\item{}Under what conditions is a linear transformation diagonalizable?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: Linear Differential Equations}
\typeout{************************************************}
%
\begin{sectionptx}{Application: Linear Differential Equations}{}{Application: Linear Differential Equations}{}{}{x:section:sec_appl_diff_eq}
A body in motion obeys Newton's second law that force equals mass times acceleration, or \(F = ma\). Here \(F\) is the force acting on the object, \(m\) the mass of the object, and \(a\) the acceleration of the object. For example, if a mass is hanging from a spring, gravity acts to pull the mass downward and the spring acts to pull the mass up. Hooke?s law says that the force of the spring acting on mass is proportional to the displacement \(y\) of the spring from equilibrium. There is also a damping force that weakens the action of the spring that might be due to air resistance or the medium in which the system is enclosed. If this mass is not too large, then the resistance can be taken to be proportional to the velocity \(v\) of the mass. This produces forces \(F = ma\) acting to pull the mass down and forces \(-ky\) and \(- bv\) acting in the opposite direction, where \(k\) and \(b\) are constant. A similar type of analysis would apply to an object falling through the air, where the resistant to falling could be proportional to the velocity of the object. Since \(v = y'\) and \(a = y''\), equating the forces produces the equation'%
\begin{equation}
my'' = -ky-by'\text{.}\label{x:men:eq_de_1}
\end{equation}
%
\par
\index{differential equation} Any equation that involves one or more derivatives of a function is called a \terminology{differential equation}. Differential equations are used to model many different types of behavior in disciplines including engineering, physics, economics, and biology. Later in this section we will see how differential equations can be represented by linear transformations, and see how we can exploit this idea to solve certain types of differential equations (including \hyperref[x:men:eq_de_1]{({\xreffont\ref{x:men:eq_de_1}})}).%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_eigen_trans_intro}
Recall that a scalar \(\lambda\) is an eigenvalue of an \(n \times n\) matrix \(A\) if \(A \vx = \lambda \vx\) for some nonzero vector \(\vx\) in \(\R^n\). Now that we have seen how to represent a linear transformation \(T\) from a finite dimensional vector space \(V\) to itself with a matrix transformation, we can exploit this idea to define and find eigenvalues and eigenvectors for \(T\) just as we did for matrices.%
\begin{definition}{}{g:definition:idp27790232}%
\index{linear transformation!eigenvalue}%
\index{linear transformation!eigenvector}%
Let \(V\) be an \(n\) dimensional vector space and \(T : V \to V\) a linear transformation. A scalar \(\lambda\) is an \terminology{eigenvalue} for \(T\) if there is a nonzero vector \(\vv\) in \(V\) such that \(T(\vv) = \lambda \vv\). The vector \(\vv\) is an \terminology{eigenvector} for \(T\) corresponding to the eigenvalue \(\lambda\).%
\end{definition}
We can exploit the fact that we can represent linear transformations as matrix transformations to find eigenvalues and eigenvectors of a linear transformation.%
\begin{exploration}{}{x:exploration:pa_8_c}%
Let \(T: \pol_1 \to \pol_1\) be defined by \(T(a_0+a_1t) = (a_0+2a_1) + (3a_0+2a_1)t\). Assume \(T\) is a linear transformation.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(\CS = \{1,t\}\) be the standard basis for \(\pol_1\). Find the matrix \([T]_{\CS}\). (Recall that we use the shorthand notation \([T]_{\CS}\) for \([T]_{\CS}^{\CS}\).)%
\item{}Check that \(\lambda_1 = 4\) and \(\lambda_2 = -1\) are the eigenvalues of \([T]_{\CS}\). Find an eigenvector \(\vv_1\) for \([T]_{\CS}\) corresponding to the eigenvalue \(4\) and an eigenvector \(\vv_2\) of \([T]_{\CS}\) corresponding to the eigenvalue \(-1\).%
\item{}Find the vector in \(\pol_1\) corresponding to the eigenvector of \([T]_{\CS}\) for the eigenvalue \(\lambda=4\). Check that this is an eigenvector of \(T\).%
\item{}Explain why in general, if \(V\) is a vector space with basis \(\CB\) and \(S: V \to V\) is a linear transformation, and if \(\vv\) in \(V\) satisfies \([\vv]_{\CB}=\vw\), where \(\vw\) is an eigenvector of \([S]_{\CB}\) with eigenvalue \(\lambda\), then \(\vv\) is an eigenvector of \(S\) with eigenvalue \(\lambda\).%
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Finding Eigenvalues and Eigenvectors of Linear Transformations}
\typeout{************************************************}
%
\begin{sectionptx}{Finding Eigenvalues and Eigenvectors of Linear Transformations}{}{Finding Eigenvalues and Eigenvectors of Linear Transformations}{}{}{x:section:sec_find_eigen_trans}
\hyperref[x:exploration:pa_8_c]{Preview Activity~{\xreffont\ref{x:exploration:pa_8_c}}} presents a method for finding eigenvalues and eigenvectors of linear transformations. That is, if \(T: V \to V\) is a linear transformation and \(\CB\) is a basis for \(V\), and if \(\vx\) is an eigenvector of \([T]_{\CB}\) with eigenvalue \(\lambda\), then the vector \(\vv\) in \(V\) satisfying \([\vv]_{\CB}=\vx\) has the property that%
\begin{equation*}
[T(\vv)]_{\CB} = [T]_{\CB}[\vv]_{\CB} = [T]_{\CB} \vx = \lambda \vx = [\lambda \vv]_{\CB}\text{.}
\end{equation*}
%
\par
Since the coordinate transformation is one-to-one, it follows that \(T(\vv) = \lambda \vv\) and \(\vv\) is an eigenvector of \(T\) with eigenvalue \(\lambda\). So every eigenvector of \([T]_{\CB}\) corresponds to an eigenvector of \(T\). The fact that every eigenvector and eigenvalue of \(T\) can be obtained from the eigenvectors and eigenvalues of \([T]_{\CB}\) is left as \hyperlink{x:exercise:ex_8_c_evals_linear_Transformations}{Exercise~{\xreffont 2}}. Now we address the question of how eigenvalues of matrices of \(T\) with respect to different bases are related.%
\begin{activity}{}{x:activity:act_8_c_ltev2}%
Let \(T: \pol_1 \to \pol_1\) be defined by \(T(a_0+a_1t) = (a_0+2a_1) + (2a_1+3a_0)t\). Assume \(T\) is a linear transformation.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(\CB = \{1+t,1-t\}\) be a basis for \(\pol_1\). Find the matrix \([T]_{\CB}\).%
\item{}Check that \(\lambda_1 = 4\) and \(\lambda_2 = -1\) are the eigenvalues of \([T]_{\CB}\). Find an eigenvector \(\vw_1\) for \([T]_{\CB}\) corresponding to the eigenvalue \(4\).%
\item{}Use the matrix \([T]_{\CB}\) to find an eigenvector for \(T\) corresponding to the eigenvalue \(\lambda_1 = 4\).%
\end{enumerate}
\end{activity}%
You should notice that the matrices \([T]_{\CS}\) and \([T]_{\CB}\) in \hyperref[x:exploration:pa_8_c]{Preview Activity~{\xreffont\ref{x:exploration:pa_8_c}}} and \hyperref[x:activity:act_8_c_ltev2]{Activity~{\xreffont\ref{x:activity:act_8_c_ltev2}}} are similar matrices. In fact, if \(P = \underset{\CB \leftarrow \CS}{P}\) is the change of basis matrix from \(\CS\) to \(\CB\), then \(P^{-1}[T]_{\CB}P = [T]_{\CS}\). So it must be the case that \([T]_{\CS}\) and \([T]_{\CB}\) have the same eigenvalues. What \hyperref[x:exploration:pa_8_c]{Preview Activity~{\xreffont\ref{x:exploration:pa_8_c}}}, \hyperlink{x:exercise:ex_8_c_evals_linear_Transformations}{Exercise~{\xreffont 2}}, and \hyperref[x:activity:act_8_c_ltev2]{Activity~{\xreffont\ref{x:activity:act_8_c_ltev2}}} demonstrate is that the eigenvalues of a linear transformation \(T : V \to V\) for any \(n\) dimensional vector space \(V\) are the eigenvalues of the matrix for \(T\) relative to any basis for \(V\). We can then find corresponding eigenvectors for \(T\) using the methods demonstrated in \hyperref[x:exploration:pa_8_c]{Preview Activity~{\xreffont\ref{x:exploration:pa_8_c}}} and \hyperref[x:activity:act_8_c_ltev2]{Activity~{\xreffont\ref{x:activity:act_8_c_ltev2}}}. That is, if \(\vv\) is an eigenvector of \(T\) with eigenvalue \(\lambda\), and \(\CB\) is any basis for \(V\), then%
\begin{equation*}
[T]_{\CB}[\vv]_{\CB} = [T(\vv)]_{\CB} = [\lambda \vv]_{\CB} = \lambda [\vv]_{\CB}\text{.}
\end{equation*}
%
\par
Thus, once we find an eigenvector \([\vv]_{\CB}\) for \([T]_{\CB}\), the vector \(\vv\) is an eigenvector for \(T\). This is summarized in the following theorem.%
\begin{theorem}{}{}{g:theorem:idp27847576}%
Let \(V\) be a finite dimensional vector space with basis \(\CB\) and let \(T : V \to V\) be a linear transformation. A vector \(\vv\) in \(V\) is an eigenvector of \(T\) with eigenvalue \(\lambda\) if and only if the vector \([\vv]_{\CB}\) is an eigenvector of \([T]_{\CB}\) with eigenvalue \(\lambda\).%
\end{theorem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Diagonalization}
\typeout{************************************************}
%
\begin{sectionptx}{Diagonalization}{}{Diagonalization}{}{}{x:section:sec_diagonal}
If \(T : V \to V\) is a linear transformation, one important question is that of finding a basis for \(V\) in which the matrix of \(T\) has as simple a form as possible. In other words, can we find a basis \(\CB\) for \(V\) for which the matrix \([T]_{\CB}\) is a diagonal matrix? Since any matrices for \(T\) with respect to different bases are similar, this will happen if we can diagonalize any matrix for \(T\).%
\begin{definition}{}{g:definition:idp27853464}%
\index{linear transformation!diagonalizable}%
Let \(V\) be a vector space of dimension \(n\). A linear transformation \(T\) from \(V\) to \(V\) is \terminology{diagonalizable} if there is a basis \(\CB\) for \(V\) for which \([T]_{\CB}\) is a diagonalizable matrix.%
\end{definition}
Now the question is, if \(T\) is diagonalizable, how do we find a basis \(\CB\) to that \([T]_{\CB}\) is a diagonal matrix? Recall that to diagonalize a matrix \(A\) means to find a matrix \(P\) so that \(P^{-1}AP\) is a diagonal matrix.%
\begin{activity}{}{x:activity:act_8_c_lt_diag}%
Let \(V = \pol_1\), \(T : V \to V\) defined by \(T(a_0+a_1t) = (a_0+2a_1) + (2a_0+a_1)t\). Let \(\CS = \{1, t\}\) be the standard basis for \(V\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the matrix \([T]_{\CS}\) for \(T\) relative to the basis \(\CS\).%
\item{}Use the fact that the eigenvalues of \([T]_{\CS}\) are \(-1\) and 3 with corresponding eigenvectors \(\left[ \begin{array}{r} -1 \\ 1 \end{array} \right]\) and \(\left[ \begin{array}{c} 1 \\ 1 \end{array} \right]\), respectively, to find a matrix \(P\) so that \(P^{-1}[T]_{\CS}P\) is a diagonal matrix \(D\).%
\item{}To find a basis \(\CB\) for which \([T]_{\CB}\) is diagonal, note that if \(P= \underset{\CB \leftarrow \CS}{P}\) of the previous part is the change of basis matrix from \(\CS\) to \(\CB\), then the matrices of \(T\) with respect to \(\CS\) and \(\CB\) are related by \(P^{-1} [T]_{\CS} P = [T]_{\CB}\), which makes \([T]_{\CB}\) a diagonal matrix. Use the fact that \(P[\vx]_{\CB}=[\vx]_{\CS}\) to find the vectors in the basis \(\CB\).%
\item{}Now show directly that \([T]_{\CB} = D\), where \(\CB = \{\vv_1, \vv_2\}\), and verify that we have found a basis for \(V\) for which the matrix for \(T\) is diagonal.%
\end{enumerate}
\end{activity}%
The general idea for diagonalizing a linear transformation is contained in \hyperref[x:activity:act_8_c_lt_diag]{Activity~{\xreffont\ref{x:activity:act_8_c_lt_diag}}}. Let \(V\) be an \(n\) dimensional vector space and assume the linear transformation \(T : V \to V\) is diagonalizable. So there exists a basis \(\CB = \{\vv_1, \vv_2, \ldots, \vv_n\}\) for which \([T]_{\CB}\) is a diagonal matrix. To find this basis, note that for any other basis \(\CC\) we know that \([T]_{\CC}\) and \([T]_{\CB}\) are similar. That means that there is an invertible matrix \(P\) so that \(P^{-1}[T]_{\CC}P = [T]_{\CB}\), where \(P= \underset{\CC \leftarrow \CB}{P}\) is the change of basis matrix from \(\CB\) to \(\CC\). So to find \(\CB\), we choose \(\CB\) so that \(P\) is the matrix that diagonalizes \([T]_{\CC}\). Using the definition of the change of basis matrix, we then know that each basis vector \(\vv_i\) in \(\CB\) satisfies \([\vv_i]_{\CC}=P[\vv_i]_{\CB}= P \ve_i\). From this we can find \(\vv_i\). Note that a standard basis \(\CS\) is often a convenient choice to make for the basis \(\CC\). This is the process we used in \hyperref[x:activity:act_8_c_lt_diag]{Activity~{\xreffont\ref{x:activity:act_8_c_lt_diag}}}.%
\par
Recall that an \(n \times n\) matrix \(A\) is diagonalizable if and only if \(A\) has \(n\) linearly independent eigenvectors. We apply this idea to a linear transformation as well, summarized by the following theorem.%
\begin{theorem}{}{}{g:theorem:idp27883032}%
Let \(V\) be a vector space of dimension \(n\) and let \(T\) be a linear transformation from \(V\) to \(V\). Then the following are equivalent.%
\begin{enumerate}
\item{}\(T\) is diagonalizable.%
\item{}There exists a basis \(\CB\) for \(V\) and an invertible matrix \(P\) so that \(P^{-1}[T]_{\CB}P\) is a diagonal matrix.%
\item{}There exists a basis \(\CC\) of \(V\) for which the matrix \([T]_{\CC}\) has \(n\) linearly independent eigenvectors.%
\item{}There exists a basis of \(V\) consisting of eigenvectors of \(T\).%
\end{enumerate}
%
\end{theorem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_eigen_trans_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp27894936}%
Let \(T : \M_{2 \times 2} \to \M_{2 \times 2}\) be defined by \(T(A) = A^{\tr}\). Let \(M_1 = \left[ \begin{array}{cc} 1\amp 1\\0\amp 0 \end{array} \right]\), \(M_2 = \left[ \begin{array}{cc} 0\amp 1\\1\amp 0 \end{array} \right]\), \(M_3 = \left[ \begin{array}{cc} 1\amp 0\\1\amp 0 \end{array} \right]\), and \(M_4 = \left[ \begin{array}{cc} 0\amp 0\\1\amp 1 \end{array} \right]\), and let \(\CB\) be the ordered basis \(\{M_1, M_2, M_3, M_4\}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(T\) is a linear transformation.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp27900568}{}\quad{}Let \(A\) and \(B\) be \(2 \times 2\) matrices and let \(c\) be a scalar. The properties of the transpose show that%
\begin{equation*}
T(A+B) = (A+B)^{\tr} = A^{\tr}+B^{\tr} = T(A) + T(B)
\end{equation*}
and%
\begin{equation*}
T(cA) = (cA)^{\tr} = cA^{\tr} = cT(A)\text{.}
\end{equation*}
Thus, \(T\) is a linear transformation.%
\item{}Find \([T]_{\CB}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp27902104}{}\quad{}Notice that \(T(M_1) = M_3\), \(T(M_2) = M_2\), \(T(M_3) = M_1\), and \(T(M_4) = M_1-M_3+M_4\). So%
\begin{equation*}
[T]_{\CB} = \left[ [T(M_1)_{\CB} \  [T(M_2)_{\CB} \  [T(M_3)_{\CB} \  [T(M_4)_{\CB} \right] = \left[ \begin{array}{cccr} 0\amp 0\amp 1\amp 1 \\ 0\amp 1\amp 0\amp 0 \\ 1\amp 0\amp 0\amp -1  \\ 0\amp 0\amp 0\amp 1 \end{array}  \right]\text{.}
\end{equation*}
%
\item{}Is the matrix \([T]_{\CB}\) diagonalizable? If so, find a matrix \(P\) such that \(P^{-1}[T]_{\CB}P\) is a diagonal matrix. Use technology as appropriate.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp27911064}{}\quad{}Technology shows that the characteristic polynomial of \([T]_{\CB}\) is \(p(\lambda) = (\lambda+1)(\lambda-1)^3\). It follows that the eigenvalues of \([T]_{\CB}\) are \(-1\) and \(1\) (with multiplicity \(3\)). Technology also shows that \(\{[-1 \ 0 \ 1 \ 0]^{\tr}\}\) is a basis for the eigenspace corresponding to the eigenvalue \(-1\) and the vectors \([1 \ 0 \ 0 \ 1]^{\tr}\), \([1 \ 0 \ 1 \ 0]^{\tr}\), \([0 \ 1 \ 0 \ 0]^{\tr}\) form a basis for the eigenspace corresponding to the eigenvalue \(1\). If we let \(P = \left[ \begin{array}{cccr} 1\amp 1\amp 0\amp -1 \\ 0\amp 0\amp 1\amp 0 \\ 0\amp 1\amp 0\amp 1 \\ 1\amp 0\amp 0\amp 0 \end{array} \right]\), then \(P^{-1}AP = \left[ \begin{array}{cccr} 1\amp 0\amp 0\amp 0 \\ 0\amp 1\amp 0\amp 0 \\ 0\amp 0\amp 1\amp 0 \\ 0\amp 0\amp 0\amp -1 \end{array} \right]\).%
\item{}Use part (c) to find a basis \(\CC\) for \(\M_{2 \times 2}\) for which \([T]_{\CC}\) is a diagonal matrix.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp27909912}{}\quad{}Suppose \(\CC = \{Q_1, Q_2, Q_3, Q_4\}\) is a basis for \(\M_{2 \times 2}\) satisfying \(P^{-1}[T]_{\CB}P = [T]_{\CC}\). This makes \([T]_{\CC}\) a diagonal matrix with the eigenvalues of \([T]_{\CB}\) along the diagonal. In this case we have%
\begin{equation*}
[T]_{\CC}[A]_{\CC} = P^{-1}[T]_{\CB}P[A]_{\CC}
\end{equation*}
for any matrix \(A \in \M_{2 \times 2}\). So \(P\) is the change of basis matrix from \(\CC\) to \(\CB\). That is, \(P[A]_{\CC} = [A]_{\CB}\). It follows that \(P[Q_i]_{\CC} = [Q_i]_{\CB}\) for \(i\) from \(1\) to \(4\). Since \([Q_i]_{\CC} = \ve_i\), we can see that \([Q_i]_{\CB}\) is the \(i\)th column of \(P\). So the  columns of \(P\) provide the weights for basis vectors in \(\CC\) in terms of the basis \(\CB\). Letting \(Q_1 = M_1+M_4\), \(Q_2 = M_1+M_3\), \(Q_3=M_2\), and \(Q_4 = M_3-M_1\), we conclude that \([T]_{\CC}\) is a diagonal matrix.%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp27927704}%
We have shown that every linear transformation from a finite dimensional vector space \(V\) to \(V\) can be represented as a matrix transformation. As a consequence, all such linear transformations have eigenvalues. In this example we consider the problem of determining eigenvalues and eigenvectors of the linear transformation \(T : V \to V\) defined by \(T(f)(x) = \int_0^x f(t) \, dt\), where \(V\) is the vector space of all infinitely differentiable functions from \(\R\) to \(\R\). Suppose that \(f\) is an eigenvector of \(T\) with a nonzero eigenvalue.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Use the Fundamental Theorem of Calculus to show that \(f\) must satisfy the equation \(f'(x) = \frac{1}{\lambda} f(x)\) for some scalar nonzero scalar \(\lambda\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp27929880}{}\quad{}Assuming that \(f\) is an eigenvector of \(T\) with nonzero eigenvalue \(\lambda\), then%
\begin{equation}
\lambda f(x) = T(f)(x) = \int_0^x f(t) \, dt\text{.}\label{x:men:eq_8_c_integral}
\end{equation}
Recall that \(\frac{d}{dx}  \int_0^x f(t) \, dt = f(x)\) by the Fundamental Theorem of Calculus. Differentiating both sides of \hyperref[x:men:eq_8_c_integral]{({\xreffont\ref{x:men:eq_8_c_integral}})} with respect to \(x\) leaves us with \(\lambda f'(x) = f(x)\), or \(f'(x) = \frac{1}{\lambda} f(x)\).%
\item{}From calculus, we know that the functions \(f\) that satisfy the equation \(f(x) = \frac{1}{\lambda} f'(x)\) all have the form \(f(x) = Ae^{x/\lambda}\) for some scalar \(A\). So if \(f\) is an eigenvector of \(T\), then \(f(x) = Ae^{x/\lambda}\) for some scalar \(A\). Show, however, that \(Ae^{x/\lambda}\) cannot be an eigenvector of \(T\). Thus, \(T\) has no eigenvectors with nonzero eigenvalues.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp27942680}{}\quad{}We can directly check from the definition that \(T(f)\) is not a multiple of \(f(x)=Ae^{x/\lambda}\) unless \(A=0\), which is not allowed. Another method is to note that \(T(f)(0) = \int_0^0 f(t) \, dt = 0\) by definition. But if \(f(x) = Ae^{x/\lambda}\), then%
\begin{equation*}
0 = T(f)(0) = \lambda f(0) = Ae^{0/\lambda} = A\text{.}
\end{equation*}
This means that  \(f(x) = 0e^{x/\lambda} = 0\). But \(0\) can never be an eigenvector by definition. So \(T\) has no eigenvectors with nonzero eigenvalues.%
\item{}Now show that \(0\) is not an eigenvalue of \(T\). Conclude that \(T\) has no eigenvalues or eigenvectors.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp27680792}{}\quad{}Suppose that \(0\) is an eigenvalue of \(T\). Then there is a nonzero function \(g\) such that \(T(g) = 0\). In other words, \(0 = \int_0^x g(t) \, dt\). Again, differentiating with respect to \(x\) yields the equation \(g(x) = 0\). So \(T\) has no eigenvectors with eigenvalue \(0\). Since \(T\) does not have a nonzero eigenvalue or zero as an eigenvalue, \(T\) has no eigenvalues (and eigenvectors).%
\item{}Explain why this example does not contradict the statement that every linear transformation from a finite dimensional vector space \(V\) to \(V\) has eigenvalues.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp27680152}{}\quad{}The reason this example does not contradict the statement is that \(V\) is an infinite dimensional vector space. In fact, the linearly independent monomials \(t^m\) are all in \(V\) for any positive integer \(m\).%
\end{enumerate}
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_eigen_trans_summ}
%
\begin{itemize}[label=\textbullet]
\item{}We can define eigenvalues and eigenvectors of a linear transformation \(T : V \to V\), where \(V\) is a finite dimensional vector space. In this case, a scalar \(\lambda\) is an eigenvalue for \(T\) if there exists a non-zero vector \(\vv\) in \(V\) so that \(T(\vv) = \lambda \vv\).%
\item{}To find eigenvalues and eigenvectors of a linear transformation \(T : V \to V\), where \(V\) is a finite dimensional vector space, we find the eigenvalues and eigenvectors for \([T]_{\CB}\), where \(\CB\) is an basis for \(V\). If \(\CC\) is any other basis for \(V\), then \([T]_{\CB}\) and \([T]_{\CC}\) are similar matrices and have the same eigenvalues. Once we find an eigenvector \([\vv]_{\CB}\) for \([T]_{\CB}\), then \(\vv\) is an eigenvector for \(T\).%
\item{}A linear transformation \(T: V \to V\), where \(V\) is a finite dimensional vector space, is diagonalizable if there is a basis \(\CC\) for \(V\) for which \([T]_{\CC}\) is a diagonalizable matrix.%
\item{}To determine if a linear transformation \(T : V \to V\) is diagonalizable, we pick a basis \(\CC\) for \(V\). If the matrix \([T]_{\CC}\) has \(n\) linearly independent eigenvectors, then \(T\) is diagonalizable.%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_eigen_trans_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp27968392}%
Let \(V = \pol_2\) and define \(T: V \to V\) by \(T(p(t)) = \frac{d}{dt} \left((1-t)p(t)\right)\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(T\) is a linear transformation.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27970312}{}\quad{}Use properties of the derivative.%
\item{}Let \(\CS\) be the standard basis for \(\pol_2\). Find \([T]_{\CS}\).%
\item{}Find the eigenvalues and a basis for each eigenspace of \(T\).%
\item{}Is \(T\) diagonalizable? If so, find a basis \(\CB\) for \(\pol_2\) so that \([T]_{\CB}\) is a diagonal matrix. If not, explain why not.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{x:exercise:ex_8_c_evals_linear_Transformations}%
Let \(T: V \to V\) be a linear transformation, and let \(\CB\) be a basis for \(V\). Show that every eigenvector \(\vv\) of \(T\) with eigenvalue \(\lambda\) corresponds to an eigenvector of \([T]_{\CB}\) with eigenvalue \(\lambda\).%
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{x:exercise:ex_8_c_derivative_operator}%
Let \(C^{\infty}\) be the set of all functions from \(\R\) to \(\R\) that have derivatives of all orders.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why \(C^{\infty}\) is a subspace of \(\F\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27982088}{}\quad{}Use properties of differentiable functions.%
\item{}Let \(D : C^{\infty} \to C^{\infty}\) be defined by \(D(f) = f'\). Explain why \(D\) is a linear transformation.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27983496}{}\quad{}Use properties of the derivative.%
\item{}Let \(\lambda\) be any real number and let \(f_{\lambda}\) be the exponential function defined by \(f_{\lambda}(x) = e^{\lambda x}\). Show that \(f_{\lambda}\) is an eigenvector of \(D\). What is the corresponding eigenvalue? How many eigenvalues does \(D\) have?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27995912}{}\quad{}What is \(D\left(e^{\lambda x}\right)\)?%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp27996424}%
Consider \(D : \pol_2 \to \pol_2\), where \(D\) is the derivative operator defined as in \hyperlink{x:exercise:ex_8_c_derivative_operator}{Exercise~{\xreffont 3}}. Find all of the eigenvalues of \(D\) and a basis for each eigenspace of \(D\).%
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp27996040}%
Let \(n\) be a positive integer and define \(T : \M_{n \times n} \to M_{n \times n}\) by \(T(A) = A^{\tr}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(T\) is a linear transformation.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27998088}{}\quad{}Use properties of the matrix transpose.%
\item{}Is \(\lambda = 1\) an eigenvalue of \(T\)? Explain. If so, describe in detail the vectors in the corresponding eigenspace.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp28005128}{}\quad{}For which matrices is \(T(A) = A\)?%
\item{}Does \(T\) have any other eigenvalues? If so, what are they and what are the vectors in the corresponding eigenspaces? If not, why not?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp28003080}{}\quad{}When is it possible to have \(A^{\tr} = \lambda A\)?%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp28004360}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
The number 0 cannot be an eigenvalue of a linear transformation.%
\item{}\lititle{True\slash{}False.}\par%
The zero vector cannot be an eigenvector of a linear transformation.%
\item{}\lititle{True\slash{}False.}\par%
If \(\vv\) is an eigenvector of a linear transformation \(T\), then so is \(2\vv\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\vv\) is an eigenvector of a linear transformation \(T\), then \(\vv\) is also an eigenvector of the transformation \(T^2 = T \circ T\).%
\item{}\lititle{True\slash{}False.}\par%
If \(\vv\) and \(\vu\) are eigenvectors of a linear transformation \(T\) with the same eigenvalue, then \(\vv+\vu\) is also an eigenvector of \(T\) with the same eigenvalue.%
\item{}\lititle{True\slash{}False.}\par%
If \(\lambda\) is an eigenvalue of a linear transformation \(T\), then \(\lambda^2\) is an eigenvalue of \(T^2\).%
\item{}\lititle{True\slash{}False.}\par%
Let \(S\) and \(T\) be two linear transformations with the same domain and codomain. If \(\vv\) is an eigenvector of both \(S\) and \(T\), then \(\vv\) is an eigenvector of \(S+T\).%
\item{}\lititle{True\slash{}False.}\par%
Let \(V\) be a vector space and let \(T : V \to V\) be a linear transformation. Then \(T\) has 0 as an eigenvalue if and only if \([T]_{\CB}\) has linearly dependent columns for any basis \(\CB\) of \(V\).%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Linear Transformations and Differential Equations}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Linear Transformations and Differential Equations}{}{Project: Linear Transformations and Differential Equations}{}{}{x:section:sec_proj_diff_eq}
There are many different types of differential equations, but we will focus on differential equations of the form \(my'' = -ky-by'\) presented in the introduction, a second order (the highest derivative in the equation is a second order derivative) linear (the coefficients are constants) differential equation (also called damped harmonic oscillators).%
\par
To solve a differential equation means to find all solutions to the differential equation. That is, find all functions \(y\) that satisfy the differential equation. For example, since \(\frac{d}{dt} t^2 = 2t\), we see that \(y = t^2\) satisfies the differential equation \(y' = 2t\). But \(t^2\) is not the only solution to this differential equation. In fact, \(y = t^2 + C\) is a solution to \(y' = 2t\) for any scalar \(C\). We will see how to represent solutions to the differential equation \(my'' = -ky-by'\) in this project.%
\par
The next activity shows that the set of solutions to the linear differential equation \(my'' = -ky-by'\) is a subspace of the vector space \(\F\) of all functions from \(\R\) to \(\R\). So we should expect close connections between differential equations and linear algebra, and we will make some of these connections as we proceed.%
\begin{project}{}{x:project:act_de_operator}%
We can represent differential equations using linear transformations. To see how, let \(D\) be the function from the space \(C^1\) of all differentiable real-valued functions to \(\F\) given by \(D(f)  = \frac{df}{dt}\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show that \(D\) is a linear transformation.%
\item{}In order for a function \(f\) to be a solution to a differential equation of the form \hyperref[x:men:eq_de_1]{({\xreffont\ref{x:men:eq_de_1}})}, it is necessary for \(f\) to be twice differentiable. We will assume that \(D\) acts only on such functions from this point on. Use the fact that \(D\) is a linear transformation to show that the differential equation \(my'' = -ky-by'\) can be written in the form%
\begin{equation*}
\left(mD^2 + bD \right)(y) = -ky\text{.}
\end{equation*}
%
\end{enumerate}
\end{project}%
\hyperref[x:project:act_de_operator]{Project Activity~{\xreffont\ref{x:project:act_de_operator}}} shows that any solution to the differential equation \(my'' = -ky-by'\) is an eigenvector for the linear transformation \(mD^2 + bD\). That is, the solutions to the differential equation \(my'' = -ky-by'\) form the eigenspace \(E_{-k}\) of \(mD^2 + bD\) with eigenvalue \(-k\). The eigenvectors for a linear transformation acting on a function space are also called \emph{eigenfunctions}. To build up to solutions to the second order differential equation we have been considering, we start with solutions to the first order equation.%
\begin{project}{}{x:project:act_de_first_order}%
Let \(k\) be a scalar. Show that the solutions of the differential equation \(D(y) = -ky\) form a one-dimensional subspace of \(\F\). Find a basis for this subspace. Note that this is the eigenspace of the transformation \(D\) corresponding to the eigenvalue \(-k\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp28040072}{}\quad{}To find the solutions to \(y' = -ky\), write \(y'\) as \(\frac{dy}{dt}\) and express the equation \(y' = -ky\) in the form \(\frac{dy}{dt} = -ky\). Divide by \(y\) and integrate with respect to \(t\).%
\end{project}%
Before considering the general second order differential equation, we start with a simpler example.%
\begin{project}{}{x:project:act_de_Hooke}%
As a specific example of a second order linear equation, as discussed at the beginning of this section, Hooke's law states that if a mass is hanging from a spring, the force acting on the spring is proportional to the displacement of the spring from equilibrium. If we let \(y\) be the displacement of the object from its equilibrium, and ignore any resistance, the position of the mass-spring system can be represented by the differential equation%
\begin{equation*}
mD^2(y) = -ky\text{,}
\end{equation*}
where \(m\) is the mass of the object and \(k\) is a positive constant that depends on the spring. Assuming that the mass is positive, we can divide both sides by \(m\) and rewrite this differential equation in the form%
\begin{equation}
D^2(y) = -cy\label{x:men:eq_de_Hooke}
\end{equation}
where \(c = \frac{k}{m}\). So the solutions to the differential equation \hyperref[x:men:eq_de_Hooke]{({\xreffont\ref{x:men:eq_de_Hooke}})} make up the eigenspace \(E_{-c}\) for \(D^2\) with eigenvalue \(-c\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Since the derivatives of \(\sin(t)\) and \(\cos(t)\) are scalar multiples of \(\sin(t)\) and \(\cos(t)\), it may be reasonable that these make up solutions to \hyperref[x:men:eq_de_Hooke]{({\xreffont\ref{x:men:eq_de_Hooke}})}. Show that \(y_1 = c\cos(t)\) and \(y_2 = c\sin(t)\) are both in \(E_{-c}\).%
\item{}As functions, the cosine and sine are related in many ways (e.g., the Pythagorean Identity). An important property for this application is the linear independence of the cosine and sine. Show, using the \emph{definition} of linear independence, that the cosine and sine functions are linearly independent in \(\F\).%
\item{}Part (a) shows that there are at least two different functions in \(E_{-c}\). To solve a differential equation is to find all of the solutions to the differential equation. In other words, we want to completely determine the eigenspace \(E_{-c}\). We have already seen that any function \(y\) of the form \(y(t) = c_1 \cos(t) + c_2 \sin(t)\) is a solution to the differential equation \(mD^2(y) = -ky\). The theory of linear differential equations tells us that there is a unique solution to \(mD^2(y) = -ky\) if we specify two initial conditions. What this means is that to show that any solution \(z\) to the differential equation \(mD^2(y) = -ky\) with two initial values \(z(t_0)\) and \(z'(t_0)\) for some scalar \(t_0\) is of the form \(y = c_1 \cos(t) + c_2 \sin(t)\), we need to verify that there are values of \(c_1\) and \(c_2\) such that \(y(t_0) = z(t_0)\) and \(y'(t_0) = z'(t_0)\). Here we will use this idea to show that any function in \(E_{-c}\) is a linear combination of \(\cos(t)\) and \(\sin(t)\). That is, that the set \(\{\cos, \sin\}\) spans \(E_{-c}\). Let \(y = c_1 \cos(t) + c_2 \sin(t)\). Show that there are values for \(c_1\) and \(c_2\) such that%
\begin{equation*}
y(0) = z'(0) \ \text{ and }  \ y'(0) = z'(0)\text{.}
\end{equation*}
This result, along with part(b), shows that \(\{\cos, \sin\}\) is a basis for \(E_{-c}\). (Note: That the solutions to differential equation \hyperref[x:men:eq_de_Hooke]{({\xreffont\ref{x:men:eq_de_Hooke}})} involve sines and cosines models the situation that a mass hanging from a spring will oscillate up and down.)%
\end{enumerate}
\end{project}%
As we saw in \hyperref[x:project:act_de_first_order]{Project Activity~{\xreffont\ref{x:project:act_de_first_order}}}, the eigenspace \(E_{k}\) of the linear transformation \(D\) is one-dimensional. The key idea in \hyperref[x:project:act_de_Hooke]{Project Activity~{\xreffont\ref{x:project:act_de_Hooke}}} that allowed us to find a basis for the eigenspace of \(D^2\) with eigenvalue \(-c\) is that \(\cos\) and \(\sin\) are linearly independent eigenfunctions that span \(E_{-c}\). We won't prove this result, but the general theory of linear differential equations states that if \(y_1\), \(y_2\), \(\ldots\), \(y_n\) are linearly independent solutions to the \(n\)th order linear differential equation%
\begin{equation*}
\left(a_nD^n + a_{n-1}D^{n-1} + \cdots + a_1D\right)(y) = -a_0y\text{,}
\end{equation*}
then \(\{y_1, y_2, \ldots,
y_n\}\) is a basis for the eigenspace of the linear transformation \(a_nD^n + a_{n-1}D^{n-1} + \cdots + a_1D\) with eigenvalue \(-a_0\). Any basis for the solution set to a differential equation is called a \terminology{fundamental set of solutions} for the differential equation. Consequently, it is important to be able to determine when a set of functions is linearly independent. One tool for doing so is the Wronskian, which we study in the next activity.%
\begin{project}{}{x:project:act_de_Wronskian}%
Suppose we have \(n\) functions \(f_1\), \(f_2\), \(\ldots\), \(f_n\), each with \(n-1\) derivatives. To determine the independence of the functions we must understand the solutions to the equation%
\begin{equation}
c_1 f_1(t) + c_2f_2(t) + \cdots + c_nf_n(t) = 0\text{.}\label{x:men:eq_de_Wronskian_1}
\end{equation}
%
\par
We can differentiate both sides of Equation \hyperref[x:men:eq_de_Wronskian_1]{({\xreffont\ref{x:men:eq_de_Wronskian_1}})} to obtain the new equation%
\begin{equation*}
c_1 f'_1(t(t)) + c_2f'_2(t) + \cdots + c_nf'_n(t) = 0\text{.}
\end{equation*}
%
\par
We can continue to differentiate as long as the functions are differentiable to obtain the system%
\begin{align*}
{f_1(t)}c_1         \amp {}+{}  \amp {f_2(t)}c_2        \amp {}+{}  \amp \cdots \amp {}+{} \amp {f_n(t)}c_n        \amp = \amp \ 0\amp {}\\
{f'_1(t)}c_1         \amp {}+{}  \amp {f'_2(t)}c_2        \amp {}+{}  \amp \cdots \amp {}+{} \amp {f'_n(t)}c_n        \amp = \amp \ 0\amp {}\\
{f''_1(t)}c_1         \amp {}+{}  \amp {f''_2(t)}c_2        \amp {}+{}  \amp \cdots \amp {}+{} \amp {f''_n(t)}c_n        \amp = \amp \ 0\amp {}\\
{}             \amp {}    \amp             \amp     \amp \vdots \amp       \amp             \amp  \amp \amp {}\\
{f^{(n-1)}_1(t)}c_1   \amp {}+{}  \amp {f^{(n-1)}_2(t)}c_2  \amp {}+{}  \amp \cdots \amp {}+{} \amp {f^{(n-1)}_n(t)}c_n  \amp = \amp \ 0\amp 
\end{align*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Write this system in matrix form, with coefficient matrix%
\begin{equation*}
A  = \left[ \begin{array}{ccccc} f_1(t) \amp f_2(t)\amp  \cdots \amp f_n(t) \\ f'_1(t) \amp f'_2(t)\amp  \cdots \amp f'_n(t) \\ \vdots \amp       \amp       \amp  \\ f^{(n-1)}_1(t) \amp f^{(n-1)}_2(t)\amp  \cdots \amp f^{(n-1)}_n(t) \end{array}  \right]\text{.}
\end{equation*}
%
\item{}The matrix in part (a) is called the Wronskian matrix of the system. The scalar%
\begin{equation*}
W(f_1, f_2, \ldots, f_n) = \det(A)
\end{equation*}
is called the Wronskian of \(f_1\), \(f_2\), \(\ldots\), \(f_n\). What must be true about the Wronskian for our system to have a unique solution? If the system has a unique solution, what is the solution? What does this result tell us about the functions \(f_1\), \(f_2\), \(\ldots\), \(f_n\)?%
\item{}Use the Wronskian to show that the cosine and sine functions are linearly independent.%
\end{enumerate}
\end{project}%
We can apply the Wronskian to help find bases for the eigenspace of the linear transformation \(mD^2 + bD\) with eigenvalue \(k\).%
\begin{project}{}{g:project:idp28092552}%
The solution to the Hooke's Law differential equation in \hyperref[x:project:act_de_Hooke]{Project Activity~{\xreffont\ref{x:project:act_de_Hooke}}} indicates that the spring will continue to oscillate forever. In reality, we know that this does not happen. In the non-ideal case, there is always some force (e.g., friction, air resistance, a physical damper as in a piston) that acts to dampen the motion of the spring causing the oscillations to die off. Damping acts to oppose the motion, and we generally assume that the faster an object moves, the higher the damping. For this reason we assume the damping force is proportional to the velocity. That is, the damping force has the form \(-by'\) for some positive constant \(b\). This produces the differential equation%
\begin{equation}
my'' + by' + ky = 0\label{x:men:act_de_second}
\end{equation}
or \(\left(mD^2+bD\right) = -ky\). We will find bases for the eigenspace of the linear transformation \(mD^2+bD\) with eigenvalue \(-k\) in this activity.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Since derivatives of exponential functions are still exponential functions, it seems reasonable to try an exponential function as a solution to \hyperref[x:men:act_de_second]{({\xreffont\ref{x:men:act_de_second}})}. Show that if \(y = e^{rt}\) for some constant \(r\) is a solution to \hyperref[x:men:act_de_second]{({\xreffont\ref{x:men:act_de_second}})}, then \(mr^2+br+k = 0\). The equation \(mr^2+br+k = 0\) is the characteristic or auxiliary equation for the differential equation.%
\item{}Part (a) shows that our solutions to the differential equation \hyperref[x:men:act_de_second]{({\xreffont\ref{x:men:act_de_second}})} are exponential of the form \(e^{rt}\), where \(r\) is a solution to the auxiliary equation. Recall that if we can find two linearly independent solutions to \hyperref[x:men:act_de_second]{({\xreffont\ref{x:men:act_de_second}})}, then we have found a basis for the eigenspace \(E_{-k}\) of \(mD^2+bD\) with eigenvalue \(-k\). The quadratic equation shows that the roots of the auxiliary equation are%
\begin{equation*}
\frac{-b \pm \sqrt{b^2-4mk}}{2m}\text{.}
\end{equation*}
As we will see, our basis depends on the types of roots the auxiliary equation has.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Assume that the roots \(r_1\) and \(r_2\) of the auxiliary equation are real and distinct. That means that \(y_1 = e^{r_1t}\) and \(y_2 = e^{r_2t}\) are eigenfunctions in \(E_{-k}\). Use the Wronskian to show that \(\{y_1, y_2\}\) is a basis for \(E_{-k}\) in this case. Then describe the behavior of an arbitrary eigenfunction in \(E_{-2}\) if \(mD^2+bD = D^2+3D\) and how it relates to damping. Draw a representative solution to illustrate. (In this case we say that the system is \terminology{overdamped}. These systems can oscillate at most once, then they quickly damp to \(0\).)%
\item{}Now suppose that we have a repeated real root \(r\) to the auxiliary equation. Then there is only one exponential function \(y_1 = e^{rt}\) in \(E_{-k}\). In this case, show that \(y_2 = te^{rt}\) is also in \(E_{-k}\) and that \(\{y_1, y_2\}\) is a basis for \(E_{-k}\). Then describe the behavior of an arbitrary eigenfunction in \(E_{-1}\) if \(mD^2+bD = D^2+2D\) and how it relates to damping. Draw a representative solution to illustrate. (In this case we say that the system is \terminology{critically damped}. These systems behave similar to the overdamped systems in that they do not oscillations. However, if the damping is reduced just a little, the system can oscillate.)%
\item{}The last case is when the auxiliary equation has complex roots \(z_1 = u+vi\) and \(z_2 = u-vi\). We want to work with real valued functions, so we need to determine real valued solutions from these complex roots. To resolve this problem, we note that if \(x\) is a real number, then \(e^{ix} = \cos(x) + i \sin(x)\). So%
\begin{equation*}
e^{(u+vi)t} = e^{ut}e^{ivt} = e^{ut} \cos(vt) + e^{ut}\sin(vt)i\text{.}
\end{equation*}
Show that \(\{e^{ut} \cos(vt),
e^{ut} \sin(vt)\}\) is a basis for \(E_{-k}\) in this case. Then describe the behavior of an arbitrary eigenfunction in \(E_{-5}\) if \(mD^2+bD = D^2+2D\) and how it relates to damping. Draw a representative solution to illustrate. (In this case we say that the system is \terminology{underdamped}. These systems typically exhibit some oscillation.)%
\end{enumerate}
\end{enumerate}
\end{project}%
\hyperref[x:project:act_de_Wronskian]{Project Activity~{\xreffont\ref{x:project:act_de_Wronskian}}} tells us that if \(W(f_1, f_2, \ldots,
f_n)\) is not zero, then \(f_1\), \(f_2\), \(\ldots\), \(f_n\) are linearly independent. You might wonder what conclusion we can draw if \(W(f_1, f_2, \ldots,
f_n)\) is zero.%
\begin{project}{}{g:project:idp28116360}%
In this activity we consider the Wronskian of two different pairs of functions.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Calculate \(W\left(t, 2t\right)\). Are \(t\) and \(2t\) linearly independent or dependent? Explain.%
\item{}Now let \(f(t) = t|t|\) and \(g(t) = t^2\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Calculate \(f'(t)\) and \(g'(t)\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp28125704}{}\quad{}Recall that \(|x| = \begin{cases}x \amp \text{ if } x \geq 0 \\ -x \amp \text{ if } x \lt 0. \end{cases}\)%
\item{}Calculate \(W\left(f, g \right)\). Are \(f\) and \(g\) linearly independent or dependent in \(\F\)? Explain.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp28121224}{}\quad{}Consider the cases when \(t \geq 0\) and \(t \lt 0\).%
\item{}What conclusion can we draw about the functions \(f_1\), \(f_2\), \(\ldots\), \(f_n\) if \(W(f_1,f_2, \ldots, f_n)\) is zero? Explain.%
\end{enumerate}
\end{enumerate}
\end{project}%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Section 40 The Jordan Canonical Form}
\typeout{************************************************}
%
\begin{chapterptx}{The Jordan Canonical Form}{}{The Jordan Canonical Form}{}{}{x:chapter:chap_JCF}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp28128648}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What is the Jordan canonical form of a square matrix?%
\item{}What is a generalized eigenvector of a matrix and how are generalized eigenvectors related to the Jordan canonical form of a matrix?%
\item{}What does it mean for a vector space \(V\) to be a direct sum of subspaces \(V_1\), \(V_2\), \(\ldots\), \(V_m\)?%
\item{}What is a nilpotent matrix? How do nilpotent matrices play a role in the Jordan canonical form?%
\item{}What does it mean for a subspace \(W\) of a vector space \(V\) to be invariant under a linear transformation \(T: V \to V\)?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Application: The Bailey Model of an Epidemic}
\typeout{************************************************}
%
\begin{sectionptx}{Application: The Bailey Model of an Epidemic}{}{Application: The Bailey Model of an Epidemic}{}{}{x:section:sec_appl_epidemic}
The COVID-19 epidemic has generated many mathematical and statistical models to try to understand the spread of the virus. In 1950 Norman Bailey proposed a simple stochastic model of the spread of an epidemic. The solution to the model involves matrix exponentials and the Jordan canonical form is a useful tool for calculating matrix exponentials.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Introduction}
\typeout{************************************************}
%
\begin{sectionptx}{Introduction}{}{Introduction}{}{}{x:section:sec_jordan_intro}
We have seen several different matrix factorizations so far, eigenvalue decomposition (\hyperref[x:chapter:chap_diagonalization]{Section~{\xreffont\ref{x:chapter:chap_diagonalization}}}), singular value decomposition (\hyperref[x:chapter:chap_SVD]{Section~{\xreffont\ref{x:chapter:chap_SVD}}}), QR factorization (\hyperref[x:chapter:chap_gram_schmidt]{Section~{\xreffont\ref{x:chapter:chap_gram_schmidt}}}), and LU factorization (\hyperref[x:chapter:chap_det_properties]{Section~{\xreffont\ref{x:chapter:chap_det_properties}}}). In this section, we investigate the Jordan canonical form, which in a way generalizes the eigenvalue decomposition. For matrices with an eigenvalue decomposition, the geometric multiplicity of each eigenvalue (the dimension of the corresponding eigenspace) must equal its algebraic multiplicity (the number of times the eigenvalue occurs as a root of the characteristic polynomial of the matrix). We know that not every matrix has an eigenvalue decomposition. However, every square matrix has a Jordan canonical form, in which we use generalized eigenvectors and block diagonal form to approximate the eigenvalue decomposition behavior. At the end of the section we provide a complete proof of the existence of the Jordan canonical form.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  When an Eigenvalue Decomposition Does Not Exist}
\typeout{************************************************}
%
\begin{sectionptx}{When an Eigenvalue Decomposition Does Not Exist}{}{When an Eigenvalue Decomposition Does Not Exist}{}{}{x:section:sec_eigen_dne}
Recall that an eigenvalue decomposition of a matrix exists if and only if the algebraic multiplicity equals the geometric multiplicity for each eigenvalue. In this case, the whole space \(\R^n\) decomposes into eigenspaces corresponding to the eigenvalues and the matrix acts as scalar multiplication in each eigenspace. We consider some cases where such a decomposition exists and some where it does not to notice some differences between the two cases and think of ways to improvise the situation.%
\begin{exploration}{}{x:exploration:pa_JCF}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}All of the matrices below have only one eigenvalue, \(\lambda = 2\) and characteristic polynomial \((\lambda-2)^n\) where \(n\) is the matrix size. Therefore, for each case, the algebraic multiplicity of this eigenvalue is the size of the matrix. Find the geometric multiplicity of this eigenvalue (i.e. the dimension of \(\Nul(A-\lambda I)\)) in each case below to see how the behavior is different in each case.%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}\(A = \left[ \begin{array}{cc} 2\amp 0 \\ 0\amp 2 \end{array} \right]\)%
\item{}\(B = \left[ \begin{array}{cc} 2\amp 1 \\ 0\amp 2 \end{array} \right]\)%
\item{}\(C = \left[ \begin{array}{ccc} 2\amp 1\amp 0 \\ 0\amp 2\amp 1 \\ 0\amp 0\amp 2 \end{array} \right]\)%
\item{}\(D = \left[ \begin{array}{ccc} 2\amp 1\amp 0 \\ 0\amp 2\amp 0 \\ 0\amp 0\amp 2 \end{array} \right]\)%
\item{}\(E = \left[ \begin{array}{ccc} 2\amp 0\amp 0 \\ 0\amp 2\amp 0 \\ 0\amp 0\amp 2 \end{array} \right]\)%
\item{}\(F = \left[ \begin{array}{cccc} 2\amp 1\amp 0\amp 0 \\ 0\amp 2\amp 0\amp 0 \\ 0\amp 0\amp 2\amp 1\\ 0\amp 0\amp 0\amp 2 \end{array} \right]\)%
\item{}\(G = \left[ \begin{array}{cccc} 2\amp 1\amp 0\amp 0 \\ 0\amp 2\amp 1\amp 0 \\ 0\amp 0\amp 2\amp 1\\ 0\amp 0\amp 0\amp 2 \end{array} \right]\)%
\item{}\(H = \left[ \begin{array}{cccc} 2\amp 1\amp 0\amp 0 \\ 0\amp 2\amp 0\amp 0 \\ 0\amp 0\amp 2\amp 0\\ 0\amp 0\amp 0\amp 2 \end{array} \right]\)%
\item{}\(J = \left[ \begin{array}{cccc} 2\amp 1\amp 0\amp 0 \\ 0\amp 2\amp 1\amp 0 \\ 0\amp 0\amp 2\amp 0\\ 0\amp 0\amp 0\amp 2 \end{array} \right]\)%
\end{enumerate}
\item{}In the examples above, the only matrices where the algebraic and geometric multiplicities of the eigenvalue 2 are equal are the diagonal matrices, which obviously have eigenvalue decompositions. The existence of ones above the diagonal destroys this property. However, the positioning of the ones is also strategical. By letting ones above the diagonal determine how to split the matrix into diagonal blocks, we can categorize the matrices. For example, the matrix in part (c) has one big \(3\times 3\) block with twos on the diagonal and ones above, while the matrix in part (d) as a \(2\times 2\) block at the top left and another \(1\times 1\) block at the bottom right. Determine the blocks for all matrices above, and identify the relationship between the number of blocks and the geometric multiplicity.%
\item{}For this last problem, we will focus on the matrix \(B\) above. We know that \(\vv_1 = \left[ \begin{array}{c} 1 \\ 0 \end{array} \right]\) is an eigenvector. We do not have a second linearly independent eigenvector, i.e. a solution to \((B-2I_2)\vx=\vzero\). However, since \((B-2I_2)^2=0\), we know that \((B-2I_2)(B-2I_2)\vx = \vzero\) for any \(\vx\), which implies that \((B-2I_2)\vx\) always lies inside the eigenspace corresponding to \(\lambda=2\). Find \((B-2I_2)\left[ \begin{array}{c} 0 \\ 1 \end{array} \right]\) to verify this works in this particular case.%
\end{enumerate}
\end{exploration}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Generalized Eigenvectors and the Jordan Canonical Form}
\typeout{************************************************}
%
\begin{sectionptx}{Generalized Eigenvectors and the Jordan Canonical Form}{}{Generalized Eigenvectors and the Jordan Canonical Form}{}{}{x:section:sec_gen_eigen_jordan}
If an \(n \times n\) matrix \(A\) has \(n\) linearly independent eigenvectors, then \(A\) is similar to a diagonal matrix with the eigenvalues along the diagonal. You discovered in \hyperref[x:exploration:pa_JCF]{Preview Activity~{\xreffont\ref{x:exploration:pa_JCF}}} a matrix without enough linearly independent eigenvectors can look close enough to a diagonal matrix. We now investigate whether this generalized representation can be achieved for all matrices.%
\begin{activity}{}{x:activity:act_JC_intro}%
Let \(M = \left[ \begin{array}{crc} 3 \amp -1 \amp 0 \\ 4 \amp 7 \amp 0 \\ 0\amp 0\amp 1 \end{array} \right]\). The characteristic polynomial of \(M\) is \((\lambda-1)(\lambda - 5)^2\), so the eigenvalues of \(M\) are \(1\) and \(5\). For \(\lambda=1\), the eigenspace is also one dimensional and the vector \(\vv_1 = \left[ \begin{array}{c} 0 \\ 0 \\ 1 \end{array} \right]\) is an eigenvector. For \(\lambda=5\), although the algebraic multiplicity is 2, the corresponding eigenspace is only one dimensional and \(\vv_0 = \left[ \begin{array}{r} -1\\2\\0 \end{array} \right]\) is an eigenvector. We cannot diagonalize matrix \(M\) because we do not have a second linearly independent eigenvector for the eigenvalue 5, which would give us the three linearly independent eigenvectors we need. Using the idea as in the last problem of \hyperref[x:exploration:pa_JCF]{Preview Activity~{\xreffont\ref{x:exploration:pa_JCF}}}, we can find another linearly independent vector to give us a matrix close to a diagonal matrix.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find a vector \(\vv_3\) that is in \(\Nul (M-5I_3)^2\) but not in \(\Nul (M-5I_3)\). Then calculate \((M-5I_3)\vv_3\). How is \(\vv_2 = (M-5I_3)\vv_3\) related to \(\vv_0\)?%
\item{}Let \(C = [\vv_1 \ \vv_2 \ \vv_3]\) and calculate the product \(C^{-1}MC\) for our example matrix \(M\).%
\item{}Describe the effect of \((M-I_3)\) and \((M-5I_3)^2\) on each of the column vectors of \(C\) and explain why this justifies that \((M-I_3)(M-5I_3)^2\) is the 0 matrix.%
\end{enumerate}
\end{activity}%
\index{defective matrix} \hyperref[x:activity:act_JC_intro]{Activity~{\xreffont\ref{x:activity:act_JC_intro}}} shows that even if a matrix does not have a full complement of linearly independent eigenvectors, we can still almost diagonalize the matrix. If \(A\) is an \(n \times n\) matrix that does not have \(n\) linearly independent eigenvectors (such matrices area called \terminology{defective}), the matrix \(A\) is still similar to a matrix that is ``close'' to diagonal. \hyperref[x:activity:act_JC_intro]{Activity~{\xreffont\ref{x:activity:act_JC_intro}}} shows this for the case of a \(3 \times 3\) matrix \(M\) with two eigenvalues, \(\lambda_1, \lambda_2\) with \(\lambda_2\) having algebraic multiplicity two, and each eigenvalue with one-dimensional eigenspace. Let \(\vv_1\) be an eigenvector corresponding to \(\lambda_1\). Because \(\lambda_2\) also has a one-dimensional eigenspace, the matrix \(M\) has only two linearly independent eigenvectors. Nevertheless, as you saw in \hyperref[x:activity:act_JC_intro]{Activity~{\xreffont\ref{x:activity:act_JC_intro}}}, we can find a vector \(\vv_3\) that is in \(\Nul (M - \lambda_2 I)^2\) but not in \(\Nul (M-\lambda_2 I)\). From this, it follows that the vector \(\vv_2 = (M - \lambda_2 I) \vv_3\) is an eigenvector of \(M\) with eigenvalue \(\lambda_2\). In this case we have \(M \vv_3 = \lambda_2 \vv_3 + \vv_2\) and%
\begin{align*}
M [\vv_1 \ \vv_2 \ \vv_3] \amp = [M\vv_1 \ M\vv_2 \ M\vv_3]\\
\amp = [\lambda_1 \vv_1 \, \ \, \lambda_2 \vv_2 \, \ \, \lambda_2 \vv_3+\vv_2 ]\\
\amp = [\vv_1  \  \vv_2 \ \vv_3]  \left[ \begin{array}{ccc} \lambda_1 \amp 0 \amp  0\\
0 \amp  \lambda_2 \amp  1\\
0 \amp  0 \amp  \lambda_2 \end{array} \right]\text{.}
\end{align*}
%
\par
The lack of two linearly independent eigenvectors for this eigenvalue of algebraic multiplicity two will ensure that \(M\) is not similar to a diagonal matrix, but \(M\) is similar to a matrix with a diagonal block of the form \(\left[ \begin{array}{cc} \lambda \amp 1 \\ 0 \amp \lambda \end{array} \right]\). So even though the matrix \(M\) is not diagonalizable, we can find an invertible matrix \(C = [\vv_1 \ \vv_2 \ \vv_3]\) so that \(C^{-1}AC\) is almost diagonalizable.%
\par
In general, suppose we have an eigenvalue \(\lambda\) of a matrix \(A\) with algebraic multiplicity two but with a one-dimensional eigenspace. Then the eigenvalue \(\lambda\) is deficient \textemdash{} that is \(\dim(E_{\lambda})\) is strictly less than the algebraic multiplicity of \(\lambda\), i.e. \(E_{\lambda}\) does not contain enough linearly independent eigenvectors for \(\lambda\). But as we saw above, we may be able to find a vector \(\vv_2\) that is in \(\Nul (A - \lambda I)^2\) but not in \(\Nul (A - \lambda I)\). When we let \(\vv_1 = (A-\lambda I) \vv_2\), we then also have%
\begin{equation}
\vzero = (A-\lambda I)^2\vv_2 = (A-\lambda I)\vv_1\label{x:men:eq_JCF_gen_eigenvector}
\end{equation}
as we argued in \hyperref[x:exploration:pa_JCF]{Preview Activity~{\xreffont\ref{x:exploration:pa_JCF}}}, and so \(\vv_1\) is an eigenvector for \(A\) with eigenvalue \(\lambda\). It is vectors that satisfy an equation like \hyperref[x:men:eq_JCF_gen_eigenvector]{({\xreffont\ref{x:men:eq_JCF_gen_eigenvector}})} that drive the Jordan canonical form. These vectors are similar to eigenvectors and are called \terminology{generalized eigenvectors}.%
\begin{definition}{}{g:definition:idp28199944}%
\index{eigenvector!generalized}%
Let \(A\) be an \(n \times n\) matrix with eigenvalue \(\lambda\). A \terminology{generalized eigenvector} of \(A\) corresponding to the eigenvalue \(\lambda\) is a non-zero vector \(\vx\) satisfying%
\begin{equation*}
(A - \lambda I_n)^m \vx = \vzero
\end{equation*}
for some positive integer \(m\).%
\end{definition}
In other words, a generalized eigenvector of an \(n \times n\) matrix \(A\) corresponding to the eigenvalue \(\lambda\) is a nonzero vector in \(\Nul (A - \lambda I_n)^m\) for some \(m\). Note that every eigenvector of \(A\) is a generalized eigenvector (with \(m=1\)). In \hyperref[x:exploration:pa_JCF]{Preview Activity~{\xreffont\ref{x:exploration:pa_JCF}}}, \(A\) is a \(3 \times 3\) matrix with eigenvalue \(\lambda = 5\) having algebraic multiplicity 2 and geometric multiplicity 1. We were able to see that \(A\) is similar to a matrix of the form \(\left[ \begin{array}{ccc} 1 \amp 0\amp 0 \\ 0 \amp 5 \amp 1 \\ 0\amp 0\amp 5 \end{array} \right]\) because of the existence of a generalized eigenvector for the eigenvalue \(5\).%
\par
The example in \hyperref[x:exploration:pa_JCF]{Preview Activity~{\xreffont\ref{x:exploration:pa_JCF}}} presents the basic idea behind how we can find a ``simple'' matrix that is similar to any square matrix, even if that matrix is not diagonalizable. The key is to find generalized eigenvectors for eigenvalues whose algebraic multiplicities exceed their geometric multiplicities. One way to do this is indicated in \hyperref[x:exploration:pa_JCF]{Preview Activity~{\xreffont\ref{x:exploration:pa_JCF}}} and in the next activity.%
\begin{activity}{}{x:activity:act_JCF_gev_2}%
Let%
\begin{equation*}
A = \left[ \begin{array}{ccr} 5\amp 1\amp -4\\4\amp 3\amp -5\\3\amp 1\amp -2 \end{array}  \right]\text{.}
\end{equation*}
%
\par
The matrix \(A\) has \(\lambda = 2\) as its only eigenvalue, and the geometric multiplicity of \(\lambda\) as an eigenvalue is 1. For this activity you may use the fact that the reduced row echelon forms of \(A-2I\), \((A-2I)^2\), and \((A-2I)^3\) are, respectively,%
\begin{equation*}
\left[ \begin{array}{ccr} 1\amp 0\amp -1\\0\amp 1\amp -1\\0\amp 0\amp 0 \end{array}  \right], \ \left[ \begin{array}{ccr} 1\amp 0\amp -1\\0\amp 0\amp 0\\0\amp 0\amp 0 \end{array}  \right], \ \left[ \begin{array}{ccc} 0\amp 0\amp 0\\0\amp 0\amp 0\\0\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}To begin, we look for a vector \(\vv_3\) that is in \(\Nul (A-2I_3)^3\) that is not in \(\Nul (A-2I_3)^2\). Find such a vector.%
\item{}Let \(\vv_2 = (A-2I_3)\vv_3\). Show that \(\vv_2\) is in \(\Nul (A-2I_3)^2\) but is not in \(\Nul (A-2I_3)\).%
\item{}Let \(\vv_1 = (A-2I_3)\vv_2\). Show that \(\vv_1\) is an eigenvector of \(A\) with eigenvalue \(2\). That is, \(\vv_1\) is in \(\Nul (A-2I_3)\).%
\item{}Let \(C = [\vv_1 \ \vv_2 \ \vv_3]\). Calculate the matrix product \(C^{-1}AC\). What do you notice?%
\end{enumerate}
\end{activity}%
It is the equations \((A- 2I)\vv_{i+1} = \vv_i\) from \hyperref[x:activity:act_JCF_gev_2]{Activity~{\xreffont\ref{x:activity:act_JCF_gev_2}}} that give us this simple form \(\left[ \begin{array}{ccc} 2\amp 1\amp 0 \\ 0\amp 2\amp 1 \\ 0\amp 0\amp 2 \end{array}  \right]\). To better understand why, notice that the equations imply that \(A\vv_{i+1} = 2\vv_{i+1} + \vv_i\). So if \(C = [\vv_1 \ \vv_2 \ \vv_3]\), then%
\begin{align*}
AC \amp = [A\vv_1 \ A\vv_2 \ A\vv_3]\\
\amp = [2 \vv_1 \ 2 \vv_2 + \vv_1 \ 2 \vv_3 + \vv_2]\\
\amp = [\vv_1 \ \vv_2 \ \vv_3] \left[ \begin{array}{ccc} 2\amp 1\amp 0\\
0\amp 2\amp 1\\
0\amp 0\amp 2 \end{array} \right]\text{.}
\end{align*}
%
\par
This method will provide us with the Jordan canonical form. The major reason that this method always works is contained in the following theorem whose proof follows from the proof of the existence of the Jordan canonical form (presented later).%
\begin{theorem}{}{}{x:theorem:thm_JCF_1}%
Let \(A\) be an \(n \times n\) matrix with eigenvalue \(\lambda\) of algebraic multiplicity \(m\). Then there is a positive integer \(p \leq m\) such that \(\dim(\Nul(A-\lambda I_n)^p) = m\).%
\end{theorem}
To find generalized eigenvectors, then, we find a value of \(p\) so that \(\dim(\Nul(A-\lambda I_n)^p) = m\) and then find a vector \(\vv_p\) that is in \(\Nul(A-\lambda I_n)^p\) but not in \(\Nul(A-\lambda I_n)^{p-1}\). Successive multiplications by \(A - \lambda I_n\) provide a sequence of generalized eigenvectors. The sequence%
\begin{equation*}
\vv_p \underset{A - \lambda I_n}{\rightarrow} \vv_{p-1} \underset{A - \lambda I_n}{\rightarrow}  \cdots \ \underset{A - \lambda I_n}{\rightarrow} \vv_1 \underset{A - \lambda I_n}{\rightarrow} \vzero
\end{equation*}
is called a \terminology{generalized eigenvector chain}.%
\begin{activity}{}{x:activity:act_JCF_3}%
Let%
\begin{equation*}
A = \left[ \begin{array}{rrrc} 0\amp 0\amp 0\amp 2 \\ -6\amp 0\amp -2\amp 10 \\ -1\amp -1\amp 1\amp 3 \\ -3\amp -1\amp -1\amp 7 \end{array}  \right]\text{.}
\end{equation*}
%
\par
The only eigenvalue of \(A\) is \(\lambda = 2\) and \(\lambda\) has geometric multiplicity 2. The vectors \([0 \ -1 \ 1 \ 0]\) and \([1 \ 2\ 0 \  1]^{\tr}\) are eigenvectors for \(A\). The reduced row echelon forms for \(A - \lambda I_4\), \((A - \lambda I_4)^2\), \((A - \lambda I_4)^3\) are, respectively,%
\begin{equation*}
\left[ \begin{array}{cccr} 1\amp 0\amp 0\amp -1 \\ 0\amp 1\amp 1\amp -2 \\ 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0 \end{array}  \right], \ \left[ \begin{array}{cccr} 1\amp 1\amp 1\amp -3 \\ 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0 \end{array}  \right], \ \text{ and }  \ \left[ \begin{array}{cccc} 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Identify the smallest value of \(p\) as in \hyperref[x:theorem:thm_JCF_1]{Theorem~{\xreffont\ref{x:theorem:thm_JCF_1}}}.%
\item{}Find a vector \(\vv_3\) in \(\Nul (A - \lambda I_4)^3\) that is not in \(\Nul (A - \lambda I_4)^2\).%
\item{}Now let \(\vv_2 = (A-\lambda I_4) \vv_3\) and \(\vv_1 = (A-\lambda I_4) \vv_2\). What special property does \(\vv_1\) have?%
\item{}Find a fourth vector \(\vv_0\) so that \(\{\vv_0, \vv_1, \vv_2, \vv_3\}\) is a basis of \(\R^4\) consisting of generalized eigenvectors of \(A\). Let \(C = [\vv_0 \ \vv_1 \ \vv_2 \ \vv_3]\). Calculate the product \(C^{-1}AC\). What do you see?%
\end{enumerate}
\end{activity}%
\index{Jordan block} The previous activities illustrate the general idea for almost diagonalizing an arbitrary square matrix. First let \(A\) be an \(n \times n\) matrix with an eigenvalue \(\lambda\) of of algebraic multiplicity \(n\) and geometric multiplicity \(k\). If \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_k\) are linearly independent eigenvectors for \(A\), then we can extend the set as we did in \hyperref[x:activity:act_JCF_3]{Activity~{\xreffont\ref{x:activity:act_JCF_3}}} above with generalized eigenvectors to a basis \(\{\vv_1, \vv_2, \ldots, \vv_k, \vv_{k+1}, \ldots, \vv_n\}\) of \(\R^n\). The matrix \(C = [\vv_1 \ \vv_2 \ \cdots \ \vv_n]\) has the property that \(C^{-1}AC\) is almost diagonal. By almost, we mean that \(C^{_1}AC\) has block matrices along the diagonal that look like%
\begin{equation}
\left[ \begin{array}{ccccccc} \lambda\amp 1\amp 0 \amp \cdots\amp 0\amp 0\amp 0 \\ 0\amp \lambda\amp 1\amp \cdots\amp 0\amp 0\amp 0 \\ 0\amp 0\amp \lambda\amp \cdots\amp 0\amp 0\amp 0 \\ \amp \amp \amp \ddots\amp \ddots\amp \amp  \\ 0\amp 0\amp 0\amp \cdots\amp \lambda\amp 1\amp 0 \\ 0\amp 0\amp 0\amp \cdots\amp 0\amp \lambda\amp 1 \\  0\amp 0\amp 0\amp \cdots\amp 0\amp 0\amp \lambda \end{array}  \right]\text{.}\label{x:men:eq_JCF_10}
\end{equation}
and has zeros everywhere else. A matrix of the form \hyperref[x:men:eq_JCF_10]{({\xreffont\ref{x:men:eq_JCF_10}})} is called a \terminology{Jordan block}.%
\par
\index{Jordan canonical form}\index{Jordan normal form} If \(A\) is an \(n \times n\) matrix with eigenvalues \(\lambda_1\), \(\lambda_2\), \(\ldots\), \(\lambda_k\), we repeat this process with every eigenvalue of \(A\) to construct an invertible matrix \(C\) so that \(C^{-1}AC\) is of the form%
\begin{equation}
\left[ \begin{array}{cccc} J_1\amp 0\amp \cdots\amp 0 \\ 0\amp J_2\amp \cdots\amp 0 \\ \vdots\amp \vdots\amp \ddots\amp \vdots \\ 0\amp 0\amp \cdots\amp J_t \end{array}  \right]\text{,}\label{x:men:eq_JCF_11}
\end{equation}
where each matrix \(J_i\) is a Jordan block (note that a \(1 \times 1\) Jordan block is allowable). The form in \hyperref[x:men:eq_JCF_11]{({\xreffont\ref{x:men:eq_JCF_11}})} is called the \terminology{Jordan canonical form} or \terminology{Jordan normal form} of the matrix \(A\). Later in this section we will prove the following theorem.%
\begin{theorem}{}{}{x:theorem:thm_JCF}%
Every square matrix is similar to a matrix in Jordan canonical form.%
\end{theorem}
Another example may help illustrate the process.%
\begin{activity}{}{g:activity:idp28362504}%
Let \(A = \left[ \begin{array}{rrcccc} 4\amp -1\amp 1\amp 0\amp 0\amp 0 \\ 0\amp 3\amp 1\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 3\amp 1\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 3\amp 1\amp 0 \\ -1\amp 1\amp 0\amp 0\amp 4\amp 1 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 4 \end{array}  \right]\). The eigenvalues of \(A\) are \(3\) and \(4\), both with algebraic multiplicity 3. A basis for the eigenspace \(E_3\) corresponding to the eigenvalue \(3\) is \(\{[1 \ 1 \ 0 \ 0 \ 0 \ 0]^{\tr}\}\) and a basis for the eigenspace \(E_4\) corresponding to the eigenvalue \(4\) is \(\{[1 \ 0 \ 0 \ 0 \ 0 \ 1]^{\tr}, [1 \ 1 \ 1 \ 1 \ 1 \ 0]^{\tr}\}\). In this activity we find a Jordan canonical form of \(A\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Assume that the reduced row echelon forms of \(A -3 I_6\), \((A-3I_6)^2\), and \((A-3I_6)^3\) are, respectively,%
\begin{equation*}
\left[ \begin{array}{crcccc} 1\amp -1\amp 0\amp 0\amp 0\amp 0\\ 0\amp 0\amp 1\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 0\amp 0\amp 1\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 1 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \end{array}  \right], \ \left[ \begin{array}{crcccc} 1\amp -1\amp 0\amp 0\amp 0\amp 0\\ 0\amp 0\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 0\amp 0\amp 1\amp 0\\ 0\amp 0\amp 0\amp 0\amp 0\amp 1 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \end{array}  \right],  \ \text{ and }  \  \left[ \begin{array}{crcccc} 1\amp -1\amp 0\amp 0\amp 0\amp 0\\ 0\amp 0\amp 0\amp 0\amp 1\amp 0\\ 0\amp 0\amp 0\amp 0\amp 0\amp 1\\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
Find a vector \(\vv_3\) that is in \(\Nul (A-3I_6)^3\) but not in \(\Nul (A-3I_6)^2\). Then let \(\vv_2 = (A-3I_6)\vv_3\) and \(\vv_1 = (A-3I_6)\vv_2\). Notice that we obtain a string of three generalized eigenvectors.%
\item{}Assume that the reduced row echelon forms of \(A -4 I_6\) and \((A-4I_6)^2\) are, respectively,%
\begin{equation*}
\left[ \begin{array}{ccccrr} 1\amp 0\amp 0\amp 0\amp -1\amp -1\\ 0\amp 1\amp 0\amp 0\amp -1\amp 0\\ 0\amp 0\amp 1\amp 0\amp -1\amp 0\\ 0\amp 0\amp 0\amp 1\amp -1\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \end{array}  \right] \  \text{ and }  \ \left[ \begin{array}{cccrcr} 1\amp 0\amp 0\amp -4\amp 3\amp -1\\ 0\amp 1\amp 0\amp -3\amp 2\amp 0 \\ 0\amp 0\amp 1\amp -2\amp 1\amp 0\\ 0\amp 0\amp 0\amp 0\amp 0\amp 0\\ 0\amp 0\amp 0\amp 0\amp 0\amp 0\\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
Find a vector \(\vv_5\) that is in \(\Nul (A-4I_6)^2\) but not in \(\Nul (A-4I_6)\). Then let \(\vv_4 = (A-4I_6)\vv_5\). Notice that we obtain a string of two generalized eigenvectors.%
\item{}Find a generalized eigenvector \(\vv_6\) for \(A\) such that \(\{\vv_1, \vv_2, \vv_3, \vv_4, \vv_5, \vv_6\}\) is a basis for \(\R^6\). Let \(C = [\vv_1 \ \vv_2 \ \vv_3 \ \vv_4 \ \vv_5 \ \vv_6]\). Calculate \(J=C^{-1}AC\). Make sure that \(J\) is a matrix in Jordan canonical form.%
\item{}How does the matrix \(J\) tell us about the eigenvalues of \(A\) and their algebraic multiplicities?%
\item{}How many Jordan blocks are there in \(J\) for the eigenvalue \(3\)? How many Jordan blocks are there in \(J\) for the eigenvalue \(4\)? How do these numbers compare to the geometric multiplicities of \(3\) and \(4\) as eigenvalues of \(A\)?%
\end{enumerate}
\end{activity}%
The previous activities highlight some of the information that a Jordan canonical tells us about a matrix. Assuming that \(C^{-1}AC = J\), where \(J\) is in Jordan canonical form, we can say the following.%
\begin{itemize}[label=\textbullet]
\item{}Since similar matrices have the same eigenvalues, the eigenvalues of \(J\), and therefore of \(A\), are the diagonal entries of \(J\). Moreover, the number of times a diagonal entry appears in \(J\) is the algebraic multiplicity of the eigenvalue. This is also the sum of the sizes of all Jordan blocks corresponding to \(\lambda\).%
\item{}Given an eigenvalue \(\lambda\), its geometric multiplicity is the number of Jordan blocks corresponding to \(\lambda\).%
\item{}Each generalized eigenvector leads to a Jordan block for that eigenvector. The number of Jordan blocks corresponding to \(\lambda\) of size at least \(j\) is \(s_j = \dim\left(\Nul (A - \lambda I)^j\right) - \dim\left(\Nul (A -  \lambda I)^{j-1}\right)\). Thus, the number of Jordan blocks of size exactly \(j\) is%
\begin{equation*}
s_j-s_{j+1} = 2 \dim\left(\Nul (A - \lambda I )^j\right) - \dim\left(\Nul (A - \lambda I )^{j+1}\right)  - \dim\left(\Nul (A - \lambda I)^{j-1}\right)\text{.}
\end{equation*}
%
\end{itemize}
%
\par
One interesting consequence of the existence of the Jordan canonical form is the famous Cayley-Hamilton Theorem.%
\begin{corollary}{The Cayley-Hamilton Theorem.}{}{g:corollary:idp28385544}%
\index{Cayley-Hamilton Theorem}%
Let \(A\) be a square matrix with characteristic polynomial \(p(x)\). Then \(p(A) = 0\).%
\end{corollary}
The proof of the Cayley-Hamilton Theorem follows from \hyperlink{x:exercise:ex_8_d_upper_triangular}{Exercise~{\xreffont 22}} that shows that every upper triangular matrix satisfies its characteristic polynomial. If \(A\) is a square matrix, then there exists a matrix \(C\) such that \(C^{-1}AC = T\), where \(T\) is in Jordan canonical form (that is, \(T\) is upper triangular). So \(A\) is similar to \(T\). If \(p(x)\) is the characteristic polynomial of \(A\), \hyperref[x:activity:act_4_c_2]{Activity~{\xreffont\ref{x:activity:act_4_c_2}}} in \hyperref[x:chapter:chap_diagonalization]{Section~{\xreffont\ref{x:chapter:chap_diagonalization}}} tells us that \(p(x)\) is the characteristic polynomial of \(T\). Therefore, \(p(T) = 0\). Then \hyperlink{x:exercise:ex_Cayley-Hamilton}{Exercise~{\xreffont 14}} in \hyperref[x:chapter:chap_diagonalization]{Section~{\xreffont\ref{x:chapter:chap_diagonalization}}} shows that \(p(A) = Cp(T)C^{-1} = 0\) and \(A\) satisfies its characteristic polynomial.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Geometry of Matrix Transformations using the Jordan Canonical Form}
\typeout{************************************************}
%
\begin{sectionptx}{Geometry of Matrix Transformations using the Jordan Canonical Form}{}{Geometry of Matrix Transformations using the Jordan Canonical Form}{}{}{x:section:sec_mtx_jordan_geom}
\begin{figureptx}{The image of the matrix transformation \(T(\vx) = \frac{1}{3} { \left[ \begin{array}{rr} 8\amp -1 \\ -2 \amp 7
\end{array}  \right] }\vx\).}{x:figure:F_JCF_geometry_1}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/JCF_geometry_1.pdf}
\end{image}%
\tcblower
\end{figureptx}%
Recall that we can visualize the action of a matrix transformation defined by a diagonalizable matrix by using a change of basis. For example, let \(T(\vx) = A\vx\), where \(A = \frac{1}{3}\left[ \begin{array}{rr} 8\amp -1\\-2\amp 7 \end{array} \right]\). The eigenvalues of \(A\) are \(\lambda_1 = 3\) and \(\lambda_2 = 2\) with corresponding eigenvectors \(\vv_1 = [-1 \ 1]^{\tr}\) and \([1 \ 2]^{\tr}\). So \(A\) is diagonalizable by the matrix \(P = \left[ \begin{array}{rc} -1\amp 1\\1\amp 2 \end{array} \right]\), with \(P^{-1}AP = D = \left[ \begin{array}{cc} 3\amp 0\\0\amp 2 \end{array} \right]\). Note that \(T(\vx) = PDP^{-1} \vx\). Now \(P^{-1}\) is a change of basis matrix from the standard basis to the basis \(\CB = \{\vv_1, \vv_2\}\), and \(D\) stretches space in the direction of \(\vv_1\) by a factor of 3 and stretches space in the direction of \(\vv_2\) by a factor of 2, and then \(P\) changes basis back to the standard basis. This is illustrated in \hyperref[x:figure:F_JCF_geometry_1]{Figure~{\xreffont\ref{x:figure:F_JCF_geometry_1}}}.%
\par
In general, if an \(n \times n\) matrix \(A\) is diagonalizable, then there is a basis \(\CB = \{\vv_1, \vv_2, \ldots, \vv_n\}\) of \(\R^n\) consisting of eigenvectors of \(A\). Assume that \(A \vv_i = \lambda_i \vv_i\) for each \(i\). Letting \(P = [\vv_1 \ \vv_2 \ \ldots \ \vv_n]\) we know that%
\begin{equation*}
P^{-1}AP = D\text{,}
\end{equation*}
where \(D\) is the diagonal matrix with \(\lambda_1\), \(\lambda_2\), \(\ldots\), \(\lambda_n\) down the diagonal. If \(T\) is the matrix transformation defined by \(T(\vx) = A\vx\), then%
\begin{equation*}
T(\vx) = PDP^{-1} \vx\text{.}
\end{equation*}
%
\par
Now \(P^{-1}\) is a change of basis matrix from the standard basis to the basis \(\CB\), and \(D\) stretches or contracts space in the direction of \(\vv_i\) by the factor \(\lambda_i\), and then \(P\) changes basis back to the standard basis. In this way we can visualize the action of the matrix transformation using the basis \(\CB\). If \(A\) is not a diagonalizable matrix, we can use the Jordan canonical form to understand the action of the transformation defined by \(A\). We start by analyzing shears.%
\begin{figureptx}{Left: A shear in the \(x\)-direction. Right: A shear in the direction of the line \(y=x\).}{x:figure:F_JCF_shear_1}{}%
\begin{sidebyside}{2}{0}{0}{0}%
\begin{sbspanel}{0.5}%
\includegraphics[width=\linewidth]{external/JCF_shear_1.pdf}
\end{sbspanel}%
\begin{sbspanel}{0.5}%
\includegraphics[width=\linewidth]{external/JCF_shear_2.pdf}
\end{sbspanel}%
\end{sidebyside}%
\tcblower
\end{figureptx}%
\begin{activity}{}{x:activity:act_JCF_shears}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Recall from \hyperref[x:chapter:chap_matrix_transformations]{Section~{\xreffont\ref{x:chapter:chap_matrix_transformations}}} that a matrix transformation \(T\) defined by \(T(\vx) = A\vx\), where \(A\) is of the form \(\left[ \begin{array}{cc} 1\amp a\\0\amp 1 \end{array}  \right]\) performs a shear in the \(x\) direction, as illustrated at left in \hyperref[x:figure:F_JCF_shear_1]{Figure~{\xreffont\ref{x:figure:F_JCF_shear_1}}}. That is, while \(T(\ve_1) = \ve_1\), it is the case that \(T(\ve_2) = \ve_2 + [a \ 0]^{\tr}\). In other words, \(T(\ve_2)-\ve_2 = [a \ 0]^{\tr}\) is in \(\Span \{\ve_1\}\). But we can say something more. Show that if \(\vx = [x_1 \ x_2]^{\tr}\) is not in \(\Span \{\ve_1\}\), then%
\begin{equation*}
T(\vx) = \vx + x_2[a \ 0]^{\tr}\text{.}
\end{equation*}
%
\par
The result is that if \(\vx\) is not in \(\Span \{\ve_1\}\), then \(T(\vx) - \vx\) is in \(\Span \{\ve_1\}\). This leads us to a general definition of a shear.%
\begin{definition}{}{g:definition:idp28428168}%
\index{shear}%
A matrix transformation \(T\) is a \terminology{shear} in the direction of the line \(\ell\) (through the origin) in \(\R^2\) if%
\begin{enumerate}
\item{}\(T(\vx) = \vx\) for all \(\vx\) in \(\ell\) and%
\item{}\(T(\vx) - \vx\) is in \(\ell\) for all \(\vx\) not in \(\ell\).%
\end{enumerate}
%
\end{definition}
\item{}Let \(S(\vx) = M\vx\), where \(M = \left[ \begin{array}{cr} 3\amp -2\\2\amp -1 \end{array} \right]\). Also let \(\vv_1 = [1 \ 1]^{\tr}\) and \(\vv_2 = [1 \ 0]^{\tr}\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Let \(\vx = \left[ \begin{array}{c} t\\t \end{array} \right]\) for some scalar \(t\). Calculate \(S(\vx) = \vx\). How is this related to the eigenvalues of \(M\)?%
\item{}Let \(\vx\) be any vector not in \(\Span\{ [1 \ 1]^{\tr}\}\). Show that \(S(\vx) - \vx\) is in \(\Span\{ [1 \ 1]^{\tr}\}\).%
\item{}Explain why \(S\) is a shear and how \(S\) is related to the image at right in \hyperref[x:figure:F_JCF_shear_1]{Figure~{\xreffont\ref{x:figure:F_JCF_shear_1}}}.%
\end{enumerate}
\end{enumerate}
\end{activity}%
As we did with diagonalizable matrices, we can understand a general matrix transformation of the form \(T(\vx) = A\vx\) by using a Jordan canonical form of \(A\). In this context, we will encounter matrices of the form \(B = \left[ \begin{array}{cc} c\amp a\\0\amp c \end{array} \right] = \left[ \begin{array}{cc} c\amp 0 \\ 0\amp c \end{array} \right] \left[ \begin{array}{cc} 1\amp \frac{a}{c} \\ 0\amp 1 \end{array} \right]\) for some positive constant \(c\). If \(S(\vx) = B\vx\), then \(S\) performs a shear in the direction of \(\ve_1\) and then an expansion or contraction in all directions by a factor of \(c\). We illustrate with an example.%
\begin{activity}{}{x:activity:act_JCF_geometry}%
Let \(T(\vx) = A\vx\), where \(A = \left[ \begin{array}{cr} 3\amp -1\\1\amp 1 \end{array} \right]\). The only eigenvalue of \(A\) is \(\lambda = 2\), and this eigenvalue has algebraic multiplicity 2 and geometric multiplicity 1. The vector \(\vv_1 = [1 \ 1]^{\tr}\) is an eigenvector for \(A\) with eigenvalue \(2\), and \(\vv_2 = [1 \ 0]^{\tr}\) satisfies \((A - 2I_2)\vv_2 = \vv_1\). Let \(C = \left[ \begin{array}{cc} 1\amp 1\\1\amp 0 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why \(T(\vx) = CJC^{-1}\vx\), where \(J = \left[ \begin{array}{cc} 2\amp 1\\0\amp 2 \end{array} \right]\).%
\item{}The matrix \(C\) is a change of basis matrix \(\underset{\CB \leftarrow \CS}{P}\) from some basis \(\CS\) to another basis \(\CB\). Specifically identify \(\CS\) and \(\CB\).%
\item{}If we begin with an arbitrary vector \(\vx\), then \([\vx]_{\CS} = \vx\). How is \(C \vx\) related to \(\CB\)?%
\item{}Describe in detail what \(J\) does to a vector in the \(\CB\) coordinate system.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp28464520}{}\quad{}\(J = \left[ \begin{array}{cc} 2\amp 0\\0\amp 2 \end{array} \right] \left[ \begin{array}{cc} 1\amp \frac{1}{2}\\0\amp 1 \end{array} \right]\).%
\item{}Put this all together to describe the action of \(T\) as illustrated in \hyperref[x:figure:F_JCF_shear_3]{Figure~{\xreffont\ref{x:figure:F_JCF_shear_3}}}. The word shear should appear in your explanation.%
\end{enumerate}
\end{activity}%
\begin{figureptx}{Change of basis, a shear, and scaling.}{x:figure:F_JCF_shear_3}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/JCF_shear_3.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\hyperref[x:activity:act_JCF_geometry]{Activity~{\xreffont\ref{x:activity:act_JCF_geometry}}} provides the essential ideas to understand the geometry of a general linear transformation using the Jordan canonical form. Let \(T(\vx) = A\vx\) with \(A \vv_1 = \lambda \vv_1\) and \(A \vv_2 = \lambda \vv_2 + \vv_1\). Then \(T\) maps \(\Span \{\vv_1\}\) to \(\Span \{\vv_1\}\) by a factor of \(\lambda\), and \(T\) maps \(\Span\{\vv_2\}\) to the line containing the terminal point of \(\vv_1\) in the direction of \(\vv_2\). The matrix \(C\) performs a change of basis from the standard basis to the basis \(\{\vv_1, \vv_2\}\), then the matrix \(\left[ \begin{array}{cc} 2\amp 1\\0\amp 2 \end{array} \right]\) performs an expansion by a factor of 2 in all directions and a shear in the direction of \(\vv_1\).%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Proof of the Existence of the Jordan Canonical Form}
\typeout{************************************************}
%
\begin{sectionptx}{Proof of the Existence of the Jordan Canonical Form}{}{Proof of the Existence of the Jordan Canonical Form}{}{}{x:section:sec_jordan_proof}
While we have constructed an algorithm to find a Jordan canonical form of a square matrix, we haven't yet addressed the question of whether every square matrix has a Jordan canonical form. We do that in this section.%
\par
Consider that any vector \([a \ b]^{\tr}\) in \(\R^2\) can be written as the sum \([a \ 0]^{\tr} + [0 \ b]^{\tr}\). The vector \([a \ 0]^{\tr}\) is a vector in the subspace \(W_1 = \{ [x \ 0]^{\tr} | x \in \R\} = \Span \{[1 \ 0]^{\tr}\}\) and the vector \([0 \ b]^{\tr}\) is in the subspace \(W_2 = \{[0 \ y]^{\tr} | y \in \R\} = \Span\{[0 \ 1]^{\tr}\). Also notice that \(W_1 \cap W_2 = \{\vzero\}\). We can extend this idea to \(\R^3\), where any vector \([a \ b \ c]^{\tr}\) can be written as \([a \ 0 \ 0]^{\tr} + [0 \ b \ 0]^{\tr} + [0 \ 0 \ c]^{\tr}\), with \([a \ 0 \ 0]^{\tr}\) in \(W_1 = \Span\{[1 \ 0 \ 0]^{\tr}\), \([0 \ b \ 0]^{\tr}\) in \(W_2 = \Span\{[0 \ 1 \ 0]^{\tr}\), and \([0 \ 0 \ c]^{\tr}\) in \(W_3 = \Span\{[0 \ 0 \ 1]^{\tr}\). In this situation we write \(\R^2 = W_1 \oplus W_2\) and \(\R^3 = W_1 \oplus W_2 \oplus W_3\) and say that \(\R^2\) is the direct sum of \(W_1\) and \(W_2\), while \(\R^3\) is the direct sum of \(W_1\), \(W_2\), and \(W_3\).%
\begin{definition}{}{g:definition:idp28487816}%
\index{direct sum of subspaces}%
A vector space \(V\) is a \terminology{direct sum} of subspaces \(V_1\), \(V_2\), \(\ldots\), \(V_m\) if every vector \(\vv\) in \(V\) can be written uniquely as a sum%
\begin{equation*}
\vv = \vv_1+\vv_2+\vv_3+\cdots + \vv_m\text{,}
\end{equation*}
with \(\vv_i \in V_i\) for each \(i\).%
\end{definition}
If \(V\) is a direct sum of subspaces \(V_1\), \(V_2\), \(\ldots\), \(V_m\), then we write%
\begin{equation*}
V = V_1 \oplus V_2 \oplus \cdots \oplus V_m\text{.}
\end{equation*}
%
\par
Some useful facts about direct sums are given in the following theorem. The proofs are left for the exercises.%
\begin{theorem}{}{}{x:theorem:thm_Direct_sum_properties}%
Let \(V\) be a vector space that is a direct sum \(V = V_1 \oplus V_2 \oplus \cdots \oplus V_m\) for some positive integer \(m\).%
\begin{enumerate}
\item{}\(V_i \cap V_j = \{\vzero\}\) whenever \(i \neq j\).%
\item{}If \(V\) is finite dimensional, and if \(\CB_i\) is a basis for \(V_i\), then the set \(\CB = \cup_{i=1}^m \CB_i\) is a basis for \(V\).%
\item{}\(\dim(V) = \dim(V_1) + \dim(V_2) + \cdots + \dim(V_m)\).%
\end{enumerate}
%
\end{theorem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Nilpotent Matrices and Invariant Subspaces}
\typeout{************************************************}
%
\begin{sectionptx}{Nilpotent Matrices and Invariant Subspaces}{}{Nilpotent Matrices and Invariant Subspaces}{}{}{x:section:sec_mtx_nilpotent}
We will prove the existence of the Jordan canonical form in two steps. In the next subsection \hyperref[x:lemma:lem_JCF_1]{Lemma~{\xreffont\ref{x:lemma:lem_JCF_1}}} will show that every linear transformation can be diagonalized in some form, and \hyperref[x:lemma:lem_JCF_2]{Lemma~{\xreffont\ref{x:lemma:lem_JCF_2}}} will provide the specific Jordan canonical form. Before we proceed to the lemmas, there are two concepts we need to introduce \textemdash{} nilpotent matrices and invariant subspaces. We don't need these concepts beyond our proof, so we won't spend a lot of time on them.%
\begin{activity}{}{x:activity:act_nilpotent_intro}%
Let \(A = \left[ \begin{array}{rr} 1\amp 1\\-1\amp -1 \end{array} \right]\) and \(B = \left[ \begin{array}{rcr} 2\amp 1\amp -3 \\ -2\amp 1\amp 1 \\ 2\amp 1\amp -3 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Calculate the positive integer powers of \(A\) and \(B\). What do you notice?%
\item{}Compare the eigenvalues of \(A\) to the eigenvalues of \(B\). What do you notice?%
\end{enumerate}
\end{activity}%
\hyperref[x:activity:act_nilpotent_intro]{Activity~{\xreffont\ref{x:activity:act_nilpotent_intro}}} shows that there are some matrices whose powers eventually become the zero matrix, and that there might be some connection to the eigenvalues of these matrices. Such matrices are given a special name.%
\begin{definition}{}{g:definition:idp28500104}%
\index{nilpotent matrix}%
\index{nilpotent transfomation}%
A square matrix \(A\) is \terminology{nilpotent} if \(A^m =0\) for some positive integer \(m\). Correspondingly, a linear transformation \(T\) from a \(n\)-dimensional vector space \(V\) to \(V\) is \terminology{nilpotent} if \(T^m = 0\) for some positive integer \(m\).%
\end{definition}
\index{index} Nilpotent matrices are the essential obstacle to the diagonalization process. If \(A\) is a nilpotent matrix, the smallest positive integer \(m\) such that \(A^m = 0\) is called the \terminology{index} of \(A\).%
\par
A characterization of nilpotent matrices is given in the following theorem.%
\begin{theorem}{}{}{x:theorem:thm_nilpotent_evals}%
A square matrix \(A\) is nilpotent if and only if \(0\) is the only eigenvalue of \(A\).%
\end{theorem}
The proof is left to the exercises.%
\par
We have seen that if \(T: V \to V\) is a linear transformation from a vector space to itself, and if \(\lambda\) is an eigenvalue of \(T\) with eigenvector \(\vv\), then \(T(\vv) = \lambda \vv\). In other words, \(T\) maps every vector in \(W = \Span\{\vv\}\) to a vector in \(W\). When this happens we say that \(W\) is invariant under the transformation \(T\).%
\begin{definition}{}{x:definition:def_JCF_invariant}%
\index{invariant subspace}%
A subspace \(W\) of a vector space \(V\) is \terminology{invariant} under a linear transformation \(T: V \to V\) if \(T(\vw) \in W\) whenever \(\vw\) is in \(W\).%
\end{definition}
So, for example, every eigenspace of a transformation is invariant under the transformation. Other spaces that are always invariant are \(V\) and \(\{\vzero\}\).%
\begin{activity}{}{g:activity:idp28523528}%
Let \(V\) be a vector space and let \(T : V \to V\) be a linear transformation.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(V = \R^2\) and \(T\) the linear transformation defined by \(T([x \ y]^{\tr}) = [x-y \ y-x]^{\tr}\). Find two invariant subspaces besides \(V\) or \(\{\vzero\}\) for \(T\).%
\item{}Recall that \(\Ker(T) = \{\vv \in V : T(\vv) = \vzero\}\). Is \(\Ker(T)\) invariant under \(T\)? Explain.%
\item{}Recall that \(\Range(T) = \{\vw \in V : \vw = T(\vv) \text{ for some } \vv \in V\}\). Is \(\Range(T)\) invariant under \(T\)? Explain.%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Jordan Canonical Form}
\typeout{************************************************}
%
\begin{sectionptx}{The Jordan Canonical Form}{}{The Jordan Canonical Form}{}{}{x:section:sec_jordan}
We are now ready to prove the existence of the Jordan canonical form.%
\begin{lemma}{}{}{x:lemma:lem_JCF_1}%
Let \(V\) be an \(n\)-dimensional vector space and let \(T: V \to V\) be a linear transformation. Let \(\lambda_1\), \(\lambda_2\), \(\ldots\), \(\lambda_r\) be the distinct eigenvalues for \(T\). Then there are integers \(s_1\), \(s_2\), \(\ldots\), \(s_r\) such that%
\begin{equation*}
V = \Ker(T-\lambda_1 I)^{s_1} \oplus \Ker(T-\lambda_2 I)^{s_2} \oplus \Ker(T-\lambda_3 I)^{s_3} \oplus \cdots \oplus \Ker(T-\lambda_r I)^{s_r}\text{.}
\end{equation*}
%
\end{lemma}
An example might help illustrate the lemma.%
\begin{activity}{}{x:activity:act_JCF_Lemma_1}%
Let \(T:\pol_4 \to \pol_4\) be defined by%
\begin{align*}
T\left(a_0+a_1t+a_2t^2+a_3t^3+a_4t^4\right) = (2a_0+a_1) \amp + (a_1-a_2)t + (a_0+a_1)t^2\\
\amp \qquad + (-a_0-a_1+a_2+2a_3-a_4)t^3 + (2a_4)t^4\text{.}
\end{align*}
%
\par
The matrix of \(T\) with respect to the standard basis \(\CS = \{1,t,t^2,t^3,t^4\}\) is%
\begin{equation*}
A = [T]_{\CS} = \left[ \begin{array}{rrrcc} 2\amp 1\amp 0\amp 0\amp 0\\0\amp 1\amp -1\amp 0\amp 0 \\ 1\amp 1\amp 0\amp 0\amp 0 \\ -1\amp -1\amp 1\amp 2\amp -1 \\ 0\amp 0\amp 0\amp 0\amp 2 \end{array}  \right]\text{.}
\end{equation*}
%
\par
The eigenvalues of \(A\) (and \(T\)) are \(2\) and \(1\), and the algebraic multiplicity of the eigenvalue \(2\) is 2 while its geometric multiplicity is 1, and the algebraic multiplicity of the eigenvalue \(1\) is 3 while its geometric multiplicity is also 1.%
\par
For every \(t\), we can find \(\Ker(T-\lambda I)^t\) using \(\Nul (A- \lambda I)^t\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Technology shows that \(\dim(\Nul A-I) = 1\), \(\dim(\Nul A-I)^2 = 2\), and \(\dim(\Nul A-I)^3 = 3\). A basis for \(\Nul (A-I)^3\) is \(\CC_1 = \left\{ [-1 \ 1 \ 0 \ 0 \ 0]^{\tr}, [1 \ 0 \ 1 \ 0 \ 0]^{\tr}, [1 \ 0 \ 0 \ 1 \ 0]^{\tr}\right\}\). Find a basis \(\CB_1\) for \(\Ker\left((T-I)^3\right)\).%
\item{}Technology also shows that \(\dim(\Nul A-2I) = 1\) and \(\dim(\Nul A-2I)^2 = 2\). A basis for \(\Nul (A-2I)^2\) is \(\CC_2 = \left\{ [0 \ 0 \ 0 \ 1 \ 0]^{\tr}, [0 \ 0 \ 0 \ 0 \ 1]^{\tr}\right\}\). Find a basis \(\CC_2\) for \(\Ker\left((T-2I)^2\right)\).%
\item{}Identify the \(\lambda_i\) and \(s_i\) in \hyperref[x:activity:act_JCF_Lemma_1]{Activity~{\xreffont\ref{x:activity:act_JCF_Lemma_1}}}. Let \(\CB = \CB_1 \cup \CB_2\). Find the matrix \([T]_{\CB}\).%
\end{enumerate}
\end{activity}%
Since each \(\Ker(T-\lambda_i I)^{s_i}\) in \hyperref[x:activity:act_JCF_Lemma_1]{Activity~{\xreffont\ref{x:activity:act_JCF_Lemma_1}}} is \(T\) invariant, \(T\) maps vectors in \(\Ker(T-\lambda_i I)^{s_i}\) back into \(\Ker(T-\lambda_i I)^{s_i}\). So \(T\) applied to each \(\Ker(T-\lambda_i I)^{s_i}\) provides a matrix \(B_i\). Applying \(T\) to each \(\Ker(T-\lambda_i I)^{s_i}\) produces the matrix%
\begin{equation*}
\left[ \begin{array}{cccc} B_1  \amp  \amp \cdots \amp    \\  \amp B_2 \amp \cdots \amp  0  \\ 0  \amp  \amp  \ddots \amp    \\   \amp  \amp \cdots \amp   B_r \end{array}  \right]\text{,}
\end{equation*}
where the \(B_i\) are square matrices corresponding to the eigenvalues of \(T\). These blocks are determined by the restriction of \(T\) to the spaces \(\Ker(T-\lambda_i I)^{s_i}\) with respect to the found basis. To obtain the Jordan canonical form, we need to know that we can always choose these basis to create the correct block matrices. \hyperref[x:lemma:lem_JCF_2]{Lemma~{\xreffont\ref{x:lemma:lem_JCF_2}}} will provide those details.%
\begin{proof}{Proof of Lemma~{\xreffont\ref*{x:lemma:lem_JCF_1}}.}{g:proof:idp28557704}
Choose a \(\lambda_i\) and, for convenience, label it \(\lambda\). For each positive integer \(j\), let \(W_j = \Ker(T - \lambda I)^j\). If \((T-\lambda I)^j\vx = \vzero\), then%
\begin{equation*}
(T-\lambda I)^{j+1}(\vx) = (T-\lambda I) (T-\lambda I)^j(\vx) = (T-\lambda I)(\vzero) = \vzero\text{,}
\end{equation*}
so \(W_j \subseteq W_{j+1}\). Thus we have the containments%
\begin{equation*}
W_1 \subseteq W_2 \subseteq W_3 \subseteq \cdots W_k \subseteq \cdots\text{.}
\end{equation*}
%
\par
Now \(V\) is finite dimensional, so this sequence must reach equality at some integer \(t\). That is, \(W_t = W_{t+1} = \cdots\). Let \(s\) be the smallest positive integer for which this happens.%
\par
We plan to show that \(V = \Ker(T - \lambda I)^s \oplus \Range(T - \lambda I)^s\). We begin by demonstrating that \(\Ker(T - \lambda I)^s\cap \Range(T - \lambda I)^s = \{0\}\). Let \(\vv \in \Ker(T - \lambda I)^s \cap \Range(T - \lambda I)^s\). Then \((T - \lambda I)^s(\vv) = \vzero\) and there exists \(\vu \in V\) such that \((T-\lambda I)^s(\vu) = \vv\). It follows that%
\begin{equation*}
(T-\lambda I)^{2s}(\vu) = (T-\lambda I)^s(T-\lambda I)^{s}(\vu) = (T-\lambda I)^s(\vv) = \vzero\text{.}
\end{equation*}
%
\par
But \(\Ker(T - \lambda I)^{2s} = \Ker(T- \lambda I)^s\), so%
\begin{equation*}
\vzero = (T-\lambda I)^{2s}(\vu) = (T-\lambda I)^{s}(\vu) = \vv\text{.}
\end{equation*}
%
\par
We conclude that \(\Ker(T - \lambda I)^s \cap \Range(T - \lambda I)^s = \{\vzero\}\).%
\par
Now we will show that \(V =  \Ker(T - \lambda I)^s \oplus \Range(T - \lambda I)^s\). Let \(\vz = \vz_1 + \vz_2\) with \(\vz_1 \in  \Ker(T - \lambda I)^s\) and \(\vz_2 \in  \Range(T - \lambda I)^s\). First we will show that \(\vz\) is uniquely represented in this way. Suppose \(\vz = \vz'_1+\vz'_2\) with \(\vz'_1 \in  \Ker(T - \lambda I)^s\) and \(\vz'_2 \in  \Range(T - \lambda I)^s\). Then%
\begin{equation*}
\vz_1+\vz_2 = \vz'_1+\vz'_2
\end{equation*}
and%
\begin{equation*}
\vz_1-\vz'_1 = \vz'_2-\vz_2\text{.}
\end{equation*}
%
\par
But \(\Ker(T - \lambda I)^s \cap \Range(T - \lambda I)^s = \{\vzero\}\), so \(\vz_1-\vz'_1 = \vzero\) and \(\vz'_2-\vz_2=\vzero\) which means \(\vz_1=\vz'_1\) and \(\vz_2 = \vz'_2\). Now let \(Z =  \Ker(T - \lambda I)^s \oplus \Range(T - \lambda I)^s\). We then know that%
\begin{equation*}
\dim(Z) = \dim\left(\Ker(T - \lambda I)^s\right) + \dim\left(\Range(T - \lambda I)^s\right)\text{.}
\end{equation*}
%
\par
Also, the Rank-Nullity Theorem shows that%
\begin{equation*}
\dim(V) =  \dim\left(\Ker(T - \lambda I)^s\right) + \dim\left(\Range(T - \lambda I)^s\right)\text{.}
\end{equation*}
%
\par
So \(Z\) is a subspace of \(V\) with \(\dim(Z) = \dim(V)\). We conclude that \(Z = V\) and that%
\begin{equation*}
V =  \Ker(T - \lambda I)^s \oplus \Range(T - \lambda I)^s\text{.}
\end{equation*}
%
\par
Next we demonstrate that \(\Ker(T - \lambda I)^s\) and \(\Range(T - \lambda I)^s\) are invariant under \(T\). Note that%
\begin{equation*}
T(T - \lambda I) = T^2 - \lambda T = (T-\lambda I)T\text{,}
\end{equation*}
and \(T\) commutes with \((T - \lambda I)\). By induction, \(T\) commutes with \((T - \lambda I)^s\). Suppose that \(\vv \in \Ker(T - \lambda I)^s\). Then%
\begin{equation*}
(T - \lambda I)^sT(\vv) = T(T - \lambda I)^s(\vv) = T(\vzero) = \vzero\text{.}
\end{equation*}
%
\par
So \(T(\vv) \in \Ker(T - \lambda I)^s\). Similarly, suppose that \(\vv \in \Range(T - \lambda I)^s\). Then there is a \(\vu \in V\) such that \((T - \lambda I)^s(\vu) = \vv\). Then%
\begin{equation*}
T(\vv) = T\left((T - \lambda I)^s(\vu)\right) = \left(T(T - \lambda I)^s\right)(\vu) = \left((T - \lambda I)^sT\right)(\vu) = (T - \lambda I)^s(T(\vu))\text{,}
\end{equation*}
and \(T(\vv) \in \Range(T - \lambda I)^s\).%
\par
We conclude our proof by induction on the number \(r\) of eigenvalues of \(T\). Suppose that \(r=1\) and so \(T\) has exactly one eigenvalue \(\lambda\). Then \(T-\lambda I\) has only zero as an eigenvalue (otherwise, there is \(\mu \neq 0\) such that \((T - \lambda I) - \mu I = T - (\lambda+\mu)I\) has a nontrivial kernel. This makes \(\lambda+\mu\) an eigenvalue of \(T\).) In this situation, \(T - \lambda I\) is nilpotent and so \((T - \lambda I)^t = 0\) for some positive integer \(t\). If \(s\) is the smallest such power, then \(V =  \Ker(T - \lambda I)^s\) and \(\Range(T - \lambda I)^s = \{\vzero\}\). So every vector in \(V\) is in \(\Ker(T - ]lambda I)^s\) and%
\begin{equation*}
V = \Ker(T - \lambda I)^s \oplus \Range(T - \lambda I)^s = \Ker(T - \lambda I)^s\text{.}
\end{equation*}
%
\par
Thus, the statement is true when \(r=1\). Assume that the statement is true for linear transformations with fewer than \(r\) eigenvalues. Now assume that \(T\) has distinct eigenvalues \(\lambda_1\), \(\lambda_2\), \(\ldots\), \(\lambda_r\). By our previous work, we know that%
\begin{equation*}
V = \Ker(T - \lambda_1 I)^{s_1} \oplus \Range(T - \lambda_1 I)^{s_1}
\end{equation*}
for some positive integer \(s_1\). Let \(V_1 = \Range(T - \lambda_1 I)^{s_1}\). Since \(\Range(T - \lambda_1 I)^{s_1}\) is \(T\) invariant, we know that \(T\) maps \(V_1\) to \(V_1\). The eigenvalues of \(T\) on \(V_1\) are \(\lambda_2\), \(\lambda_3\), \(\ldots\), \(\lambda_r\). By our induction hypothesis, we have%
\begin{equation*}
V_1 = \Ker(T-\lambda_2 I)^{s_2} \oplus \Ker(T-\lambda_3 I)^{s_3} \oplus \cdots \oplus \Ker(T-\lambda_r I)^{s_r}\text{,}
\end{equation*}
for some positive integers \(s_2\), \(s_3\), \(\ldots\), \(s_r\), which makes%
\begin{equation*}
V = \Ker(T-\lambda_1 I)^{s_1} \oplus \Ker(T-\lambda_2 I)^{s_2} \oplus \Ker(T-\lambda_3 I)^{s_3} \oplus \cdots \oplus \Ker(T-\lambda_r I)^{s_r}\text{.}\qedhere
\end{equation*}
%
\end{proof}
\hyperref[x:lemma:lem_JCF_1]{Lemma~{\xreffont\ref{x:lemma:lem_JCF_1}}} tells us that \(T\) has a diagonal form with block matrices down the diagonal. To obtain a Jordan canonical form, we need to identify the correct bases for the summands of \(V\). (Lemma is due to Mark Wildon from ``A SHORT PROOF OF THE EXISTENCE OF JORDAN NORMAL FORM''.)%
\begin{lemma}{}{}{x:lemma:lem_JCF_2}%
Let \(V\) be an \(n\)-dimensional vector space and let \(T : V \to V\) be a linear transformation such that \(T^s = 0\) for some positive integer \(s\). Then there exist vectors \(\vu_1\), \(\vu_2\), \(\ldots\), \(\vu_k\) and natural numbers \(a_1\), \(a_2\), \(\ldots\), \(a_k\) such that \(T^{a_i}(\vu_i) = 0\) for \(i\) from \(1\) to \(k\) and the vectors%
\begin{equation*}
\vu_1, T(\vu_1), \ldots,T^{a_1-1}(\vu_1), \vu_2, T(\vu_2), \ldots,T^{a_2-1}(\vu_2), \ldots, \vu_k, T(\vu_k), \ldots,T^{a_k-1}(\vu_k)
\end{equation*}
are non-zero vectors that form a basis of \(V\).%
\end{lemma}
Notice the similarity of \hyperref[x:lemma:lem_JCF_2]{Lemma~{\xreffont\ref{x:lemma:lem_JCF_2}}} to chains of generalized eigenvectors. An example might help illustrate \hyperref[x:lemma:lem_JCF_2]{Lemma~{\xreffont\ref{x:lemma:lem_JCF_2}}}.%
\begin{activity}{}{x:activity:act_JCF_Lem_2}%
Let \(T: \pol_5 \to \pol_5\) be defined by%
\begin{equation*}
T(a_0+a_1t+a_2t^2+a_3t^3+a_4t^4+a_5t^5) = (-a_1-a_4+a_5)t + (-a_0-a_1+a_3-a_4+a_5)t^2 + (a_1+a_4)t^4\text{.}
\end{equation*}
%
\par
Let \(\CS = \{1,t,t^2,t^3,t^4,t^5\}\) be the standard basis for \(\pol_5\). Then%
\begin{equation*}
A = [T]_{\CS} = \left[ \begin{array}{rrccrc} 0\amp 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp -1\amp 0\amp 0\amp -1\amp 1 \\ -1\amp -1\amp 0\amp 1\amp -1\amp 1 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 1\amp 0\amp 0\amp 1\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \end{array} \right]\text{.}
\end{equation*}
%
\par
Technology shows that the only eigenvalue of \(A\) is \(0\) and that the geometric multiplicity of \(0\) is \(3\). Since \(0\) is the only eigenvalue of \(A\), we know that \(A\) (and \(T\)) is nilpotent. Using technology we find that the reduced row echelon forms of \(A\) and \(A^2\) and respectively,%
\begin{equation*}
\left[ \begin{array}{cccrcc} 1\amp 0\amp 0\amp -1\amp 0\amp 0 \\ 0\amp 1\amp 0\amp 0\amp 1\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 1 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \end{array}  \right], \ \left[ \begin{array}{cccrcc} 0\amp 0\amp 0\amp 0\amp 0\amp 1 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \end{array}  \right]\text{,}
\end{equation*}
while \(A^3 = 0\). We see that \(\dim(\Ker(T^3)) \dim(\Nul A^3) = 6\) while \(\dim(\Ker(T^2)) = \dim(\Nul A^2) = 5\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Notice that the vector \(\vv_1 = [0 \ 0 \ 0 \ 0 \ 0 \ 1]^{\tr}\) is in \(\Nul A^3\) but not in \(\Nul A^2\). Use this vector to construct one chain \(u_1\), \(T(u_1)\), and \(T^2(u_1)\) of generalized eigenvectors starting with a vector \(u_1\) that is in \(\Ker(T^3)\) but not in \(\Ker(T^2)\). What can we say about the vector \(T^2(u_1)\) in relation to eigenvectors of \(T\)?%
\item{}We know two other eigenvectors of \(T\), so we need another chain of generalized eigenvectors to provide a basis of \(\pol_5\) of generalized eigenvectors. Use the fact that \(\vv_2 = [1 \ 0 \ 0 \ 0 \ 0 \ 0]^{\tr}\) is in \(\Nul A^2\) but not in \(\Nul A\) to find another generalized eigenvector \(u_2\) in \(\Ker(T^2)\) that is not in \(\Ker(T)\). Then create a chain \(u_2\) and \(T(u_2)\) of generalized eigenvectors. What is true about \(T(u_2)\) in relation to eigenvectors of \(T\)?%
\item{}Let \(u_3 = 1+t^3\) be a third eigenvector of \(T\). Explain why \(\{u_1, T(u_1), T^2(u_1),
u_2, T(u_2), u_3\}\) is a basis of \(\pol_5\). Identify the values of \(k\) and the \(a_i\) in Lemma 40.12.%
\end{enumerate}
\end{activity}%
Notice that if we let \(C = [[T^2(\vu_1)]_{\CS} \ [T(\vu_1)]_{\CS} \ [\vu_1]_{\CS} \  [T(\vu_4)]_{|CS} \ [\vu_4]_{|CS} \ [\vu_6]_{\CS}]\) using the vectors from \hyperref[x:activity:act_JCF_Lem_2]{Activity~{\xreffont\ref{x:activity:act_JCF_Lem_2}}} we should find that%
\begin{equation*}
C^{-1}AC = \left[ \begin{array}{cccccc} 0\amp 1\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 1\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 1\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \end{array}  \right]\text{,}
\end{equation*}
and this basis provides a matrix that produces a Jordan canonical form of \(A\).%
\par
\hyperref[x:lemma:lem_JCF_2]{Lemma~{\xreffont\ref{x:lemma:lem_JCF_2}}} provides the sequences of generalized eigenvectors that we need to make the block matrices in Jordan canonical form. This works as follows. Start, for example, with \(\vu_1\), \(T(\vu_1)\), \(\ldots\),\(T^{a_1-1}(\vu_1)\). Since \(T^s = 0\), we know that \(T\) is nilpotent and so has only \(0\) as an eigenvalue. If \(T\) has a nonzero eigenvalue \(\lambda\), we replace \(T\) with \(T - \lambda I\).%
\par
Let \(\CB = \{T^{a_1-1}(\vu_1), T^{a_1-2}(\vu_1), \ldots, T^2(\vu_1), T(\vu_1), \vu_1\}\). Then%
\begin{align*}
_{\CB} \amp = [T^{a_1}(\vu_1)]_{\CB} = [0 \ 0 \ 0 \  \  \ldots \ 0 \ 0]^{\tr}\\
[T(T^{a_1-2}(\vu_1))]_{\CB} \amp = [T^{a_1-1}(\vu_1)]_{\CB} = [1 \ 0 \ 0 \ \  \ldots \ 0 \ 0]^{\tr}\\
[T(T^{a_1-3}(\vu_1))]_{\CB} \amp = [T^{a_1-3}(\vu_1)]_{\CB} = [0 \ 1 \ 0 \ \  \ldots \ 0 \ 0]^{\tr}\\
\amp \vdots\\
[T(T^2(\vu_1))]_{\CB} \amp = [T^3(\vu_1)]_{\CB} = [0 \ 0 \ 0 \  \ \ldots \ 0 \ 1 \ 0 \ 0 \ 0]^{\tr}\\
[T(T(\vu_1))]_{\CB} \amp = [T^2(\vu_1)]_{\CB} = [0 \ 0 \ 0 \  \ \ldots \ 0 \ 0 \ 1 \ 0 \ 0]^{\tr}\\
[T(\vu_1)]_{\CB} \amp = [T(\vu_1)]_{\CB} = [0 \ 0 \ 0 \  \ \ldots \ 0 \ 0 \ 0 \ 1 \ 0]^{\tr}\text{.}
\end{align*}
%
\par
This makes%
\begin{equation*}
[T]_{\CB} =  \left[ \begin{array}{cccccccc} 0\amp 1\amp 0\amp 0\amp \cdots \amp  0 \amp 0\amp 0 \\ 0\amp 0\amp 1\amp 0\amp \cdots \amp  0\amp 0\amp 0 \\ \amp \amp \ddots\amp \ddots \amp \amp   \amp \amp \\ \amp \amp \amp \amp \vdots \amp \amp \amp   \\ 0\amp 0\amp 0\amp 0\amp \cdots \amp  0\amp 1\amp 0 \\ 0\amp 0\amp 0\amp 0\amp \cdots \amp  0\amp 0\amp 1 \\ 0\amp 0\amp 0\amp 0\amp \cdots \amp  0\amp 0\amp 0 \end{array}  \right]\text{,}
\end{equation*}
which gives one Jordan block.%
\begin{proof}{Proof of Lemma~{\xreffont\ref*{x:lemma:lem_JCF_2}}.}{g:proof:idp28655256}
If \(T(\vx) = \vzero\) for all \(\vx \in V\), then we can choose \(\vu_1\), \(\vu_2\), \(\ldots\), \(\vu_{\dim(V)}\) to form any basis of \(V\) and all \(a_i = 1\). So we can assume that \(T\) is a nonzero transformation. Also, we claim that \(T(V)\) is a proper subset of \(V\) (recall that \(T(V)\) is the same as \(\Range(T)\)). If not, \(V = T(V) = T^2(V) = \cdots = T^m(V)\) for any positive integer \(m\). But this contradicts the fact that \(T^s = 0\).%
\par
We proceed by induction on \(\dim(V)\). If \(\dim(V) = 1\), since\(T(V)\) is a proper subset of \(V\) the only possibility is that \(T(V) = \{\vzero\}\). We have already discussed this case. For the inductive step assume that the lemma is true for any vector space of positive integer dimension less than \(\dim(V)\). Our assumption that \(T\) is a nonzero transformation allows us to conclude that \(0 \subset T(V) \subset V\). Thus, \(1 \leq \dim(T(V)) \lt  \dim(V)\). We apply the inductive hypothesis to the transformation \(T: T(V) \to T(V)\) to find integers \(b_1\), \(b_2\), \(\ldots\), \(b_{\ell}\) and vectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_{\ell}\) in \(T(V)\) such that the vectors%
\begin{equation}
\vv_1, T(\vv_1), \ldots, T^{b_1-1}(\vv_1), \ldots, \vv_{\ell}, T(\vv_{\ell}), \ldots, T^{b_{\ell}-1}(\vv_{\ell})\label{x:men:eq_JCF_TV_basis}
\end{equation}
form a basis for \(T(V)\) and \(T^{b_i}(\vv_i) = \vzero\) for \(1 \leq i \leq \ell\).%
\par
Now \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_{\ell}\) are in \(T(V)\), so there exist \(\vu_1\), \(\vu_2\), \(\ldots\), \(\vu_{\ell}\) in \(V\) such that \(T(\vu_i) = \vv_i\) for each \(1 \leq i \leq \ell\). This implies that \(T^j(\vu_i) = T^{j-1}(T(\vu_i)) = T^{j-1}(\vv_i)\) for all \(i\) and all positive integers \(j\). The vectors \(T^{b_1-1}(\vv_1)\), \(T^{b_2-1}(\vv_2)\), \(\ldots\), \(T^{b_{\ell}-1}(\vv_{\ell})\) are linearly independent and \(T\left(T^{b_i-1}(\vv_i)\right) = T^{b_i}(\vv_i) = 0\) for \(1 \leq i \leq \ell\), so the vectors \(T^{b_1-1}(\vv_1)\), \(T^{b_2-1}(\vv_2)\), \(\ldots\), \(T^{b_{\ell}-1}(\vv_{\ell})\) are all in \(\Ker(T)\). Extend the set \(\left\{T^{b_1-1}(\vv_1), T^{b_2-1}(\vv_2), \ldots, T^{b_{\ell}-1}(\vv_{\ell})\right\}\) to a basis of \(\Ker(T)\) with the vectors \(\vw_1\), \(\vw_2\), \(\ldots\), \(\vw_m\) for \(m = \dim(\Ker(T)) - \ell\). That is, the set%
\begin{equation}
\left\{T^{b_1-1}(\vv_1), T^{b_2-1}(\vv_2), \ldots, T^{b_{\ell}-1}(\vv_{\ell}), \vw_1, \vw_2, \ldots, \vw_m\right\}\label{x:men:eq_JCF_Ker_basis}
\end{equation}
is a basis for \(\Ker(T)\). We will now show that the vectors%
\begin{equation}
\vu_1, T(\vu_1), \ldots, T^{b_1}(\vu_1), \ldots, \vu_{\ell}, T(\vu_{\ell}), \ldots, T^{b_{\ell}}(\vu_{\ell}), \vw_1, \ldots, \vw_m\label{x:men:eq_JCF_V_basis}
\end{equation}
form a basis for \(V\). To demonstrate linear independence, suppose that%
\begin{align}
c_{1,0}\vu_1\amp +c_{1,1}T(\vu_1)+ \cdots + c_{1,b_1}T^{b_1}(\vu_1) +  \cdots\notag\\
\amp + c_{\ell,0}\vu_{\ell} +  c_{\ell,1}T(\vu_{\ell})+ \cdots + c_{\ell,b_{\ell}}T^{b_{\ell}}(\vu_{\ell}) +  d_1\vw_1+  \cdots +  d_m\vw_m = \vzero\label{x:mrow:eq_JCF_Lemma_2}
\end{align}
for some scalars \(c_{i,j}\) and \(d_k\). Apply \(T\) to this linear combination to obtain the vector equation%
\begin{align*}
c_{1,0}T(\vu_1)\amp +c_{1,1}T^2(\vu_1)+ \cdots + c_{1,b_1}T^{b_1+1}(\vu_1) +  \cdots\\
\amp + c_{\ell,0}T(\vu_{\ell}) +  c_{\ell,1}T^2(\vu_{\ell})+ \cdots + c_{\ell,b_{\ell}}T^{b_{\ell}+1}(\vu_{\ell}) +  d_1T(\vw_1)+  \cdots \\
\amp +  d_mT(\vw_m) = \vzero\text{.}
\end{align*}
%
\par
Using the relationship \(T^j(\vu_i) = T^{j-1}(\vv_i)\) gives us the equation%
\begin{align*}
c_{1,0}\vv_1\amp +c_{1,1}T(\vv_1)+ \cdots + c_{1,b_1}T^{b_1}(\vv_1) +  \cdots\\
\amp + c_{\ell,0}\vv_{\ell} +  c_{\ell,1}T(\vv_{\ell})+ \cdots + c_{\ell,b_{\ell}}T^{b_{\ell}}(\vv_{\ell}) +  d_1T(\vw_1)+  \cdots +  d_mT(\vw_m) = \vzero\text{.}
\end{align*}
%
\par
Recall that \(T^{b_i}(\vv_i) = 0\) and that \(\vw_1\), \(\vw_2\), \(\ldots\), \(\vw_m\) are in \(\Ker(T)\) to obtain the equation%
\begin{align*}
c_{1,0}\vv_1 \amp +c_{1,1}T(\vv_1)+ \cdots + c_{1,b_1-1}T^{b_1-1}(\vv_1) +  \cdots + c_{\ell,0}\vv_{\ell} +  c_{\ell,1}T(\vv_{\ell})+ \cdots\\
\amp + c_{\ell,b_{\ell}-1}T^{b_{\ell}-1}(\vv_{\ell}) = \vzero\text{.}
\end{align*}
%
\par
But this final equation is a linear combination of the basis elements in \hyperref[x:men:eq_JCF_TV_basis]{({\xreffont\ref{x:men:eq_JCF_TV_basis}})} of \(T(V)\), and so the scalars are all \(0\). Replacing these scalars with \(0\) in \hyperref[x:mrow:eq_JCF_Lemma_2]{({\xreffont\ref{x:mrow:eq_JCF_Lemma_2}})} results in%
\begin{equation*}
c_{1,b_1}T^{b_1}(\vu_1) +  \cdots + c_{\ell,b_{\ell}}T^{b_{\ell}}(\vu_{\ell}) +  d_1\vw_1+  \cdots +  d_m\vw_m = 0\text{.}
\end{equation*}
%
\par
But this is a linear combination of vectors in a basis for \(\Ker(T)\) and so all of the scalars are also \(0\). Hence, the vectors \(\vu_1\), \(T(\vu_1)\), \(\ldots\), \(T^{b_1}(\vu_1)\), \(\ldots\), \(\vu_{\ell}\), \(T(\vu_{\ell})\), \(\ldots\), \(T^{b_{\ell}}(\vu_{\ell})\), \(\vw_1\), \textellipsis{}, \(\vw_m\) are linearly independent.%
\par
The Rank-Nullity Theorem shows tells us that \(\dim(V) = \dim(T(V)) + \dim(\Ker(T))\). The vectors in \hyperref[x:men:eq_JCF_Ker_basis]{({\xreffont\ref{x:men:eq_JCF_Ker_basis}})} form a basis for \(\Ker(T)\), and so \(\dim(\Ker(T)) = \ell + m\). The vectors in \hyperref[x:men:eq_JCF_TV_basis]{({\xreffont\ref{x:men:eq_JCF_TV_basis}})} form a basis for \(T(V)\), so \(\dim(T(V)) = b_1+b_2+ \cdots + b_{\ell}\). Thus,%
\begin{equation*}
\dim(V) = \ell+m + b_1+b_2+ \cdots + b_{\ell} = m + (b_1-1) + (b_2-1) + \cdots + (b_{\ell}-1)\text{.}
\end{equation*}
%
\par
But this is exactly the number of vectors in our claimed basis \hyperref[x:men:eq_JCF_V_basis]{({\xreffont\ref{x:men:eq_JCF_V_basis}})}. This verifies \hyperref[x:lemma:lem_JCF_2]{Lemma~{\xreffont\ref{x:lemma:lem_JCF_2}}} with \(k=\ell+m\), \(a_i=b_i+1\) for \(1 \leq i \leq \ell\), \(u_{j+\ell} = w_j\) and \(a_{j+\ell} = 1\) for \(1 \leq m\).%
\end{proof}
We return to \hyperref[x:activity:act_JCF_Lemma_1]{Activity~{\xreffont\ref{x:activity:act_JCF_Lemma_1}}} to illustrate the use of \hyperref[x:lemma:lem_JCF_2]{Lemma~{\xreffont\ref{x:lemma:lem_JCF_2}}}.%
\begin{example}{}{x:example:ex_JCF_Lemma_2_2}%
We work with the transformation \(T:\pol_4 \to \pol_4\) defined by%
\begin{align*}
T\left(a_0+a_1t+a_2t^2+a_3t^3+a_4t^4\right) = (2a_0+a_1) \amp + (a_1-a_2)t + (a_0+a_1)t^2\\
\amp + (-a_0-a_1+a_2+2a_3-a_4)t^3 \\
\amp + (2a_4)t^4\text{.}
\end{align*}
%
\par
Recall from \hyperref[x:activity:act_JCF_Lemma_1]{Activity~{\xreffont\ref{x:activity:act_JCF_Lemma_1}}} that%
\begin{equation*}
\pol_4 = \Ker(T-I)^{3} \oplus \Ker(T-2 I)^{2}\text{.}
\end{equation*}
and a basis for \(\Ker(T-I)^3\) is \(\CB_1 = \{p_1(t), p_2(t),
p_3(t)\}\) with \(p_1(t) = -1+t\), \(p_2(t) = 1+t^2\), and \(p_3(t)= 1+t^3\). Since%
\begin{equation*}
(T-I)(p_1(t)) = 0, \ (T-I)(p_2(t)) = -p_1(t), \ \text{ and }  \ (T-I)(p_3(t)) = p_2(t)
\end{equation*}
we see that \(T-I\) maps \(\Ker(T-I)^3\) to \(\Ker(T-I)^3\). The matrix of \(T-I\) with respect to \(\CB_1\) is%
\begin{equation*}
[T-I]_{\CB_1} = \left[ \begin{array}{crc} 0\amp -1\amp 0 \\ 0\amp 0\amp 1 \\ 0\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
%
\par
The reduced row echelon form of \([T-I]_{\CB_1}\) and \([T-I]_{\CB_1}^2\) are%
\begin{equation*}
\left[ \begin{array}{ccc} 0\amp 1\amp 0 \\ 0\amp 0\amp 1 \\ 0\amp 0\amp 0 \end{array}  \right] \ \text{ and }  \ \left[ \begin{array}{ccr} 0\amp 0\amp 1 \\ 0\amp 0\amp 0 \\ 0\amp 0\amp 0 \end{array}  \right]\text{,}
\end{equation*}
while \([T-I]_{\CB_1}^3 = 0\). This makes \((T-I)^3 = 0\). We apply \hyperref[x:lemma:lem_JCF_2]{Lemma~{\xreffont\ref{x:lemma:lem_JCF_2}}} to \(T-I : \Ker(T-I)^3 \to \Ker(T-I)^3\). We choose \(u_1\) to be a vector in \(\Ker(T-I)^3\) that is not in \(\Ker(T-I)^2\). Once such vector has \([u_1]_{\CB_1} = [0 \ 0 \ 1]^{\tr}\), or \(u_1=1+t^3\). We then let \(u_2 = (T-I)(u_1) = 1+t^2\) and \(u_1 = (T-I)(u_2) = 1-t\). This gives us the basis \(\{1-t, 1+t^2, 1+t^3\}\) for \(\Ker(T-I)^3\).%
\par
We can also apply \hyperref[x:lemma:lem_JCF_2]{Lemma~{\xreffont\ref{x:lemma:lem_JCF_2}}} to \(T-2I : \Ker(T-I)^2 \to \Ker(T-I)^2\). Since%
\begin{equation*}
(T-2I)(p_4(t)) = 0 \ \text{ and }  \ (T-2I)(p_5(t)) = -p_4(t)\text{,}
\end{equation*}
we have that%
\begin{equation*}
[T-2I]_{\CB_2} = \left[ \begin{array}{cr} 0\amp -1 \\ 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
%
\par
It follows that \((T-2I)^2 = 0\). Selecting \(u_4 = t^4\) and letting \(u_5 = (T-2I)(u_4) = -p_4(t)\), we obtain the basis \(\{-t^3, t^4\}\) for \(\Ker(T-I)^2\).%
\par
Let \(q_1(t) = 1+t^3\), \(q_2(t) = 1+t^2\), \(q_3(t) = 1-t\), \(q_4(t) = t^4\), and \(q_5(t) = -t^3\), and let \(\CC = \{q_1(t), q_2(t),
q_3(t), q_4(t), q_5(t)\}\). Since%
\begin{align*}
T(q_1(t)) \amp = q_1(t)+q_2(t)\\
T(q_2(t)) \amp = q_2(t)+q_3(t)\\
T(q_3(t)) \amp = q_3(t)\\
T(q_4(t)) \amp = 2q_4(t)+q_5(t)\\
T(q_5(t)) \amp = 2q_5(t)
\end{align*}
it follows that%
\begin{equation*}
[T]_{\CC} = \left[ \begin{array}{ccccc} 1\amp 1\amp 0\amp 0\amp 0 \\ 0\amp 1\amp 1\amp 0\amp 0 \\ 0\amp 0\amp 1\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 2\amp 1 \\ 0\amp 0\amp 0\amp 0\amp 2 \end{array}  \right]\text{,}
\end{equation*}
and we have found a basis for \(\pol_4\) for which the matrix \(T\) has a Jordan canonical form.%
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Examples}
\typeout{************************************************}
%
\begin{sectionptx}{Examples}{}{Examples}{}{}{x:section:sec_jordan_exam}
What follows are worked examples that use the concepts from this section.%
\begin{example}{}{g:example:idp28741656}%
Find a Jordan form \(J\) for each of the following matrices. Find a matrix \(C\) such that \(C^{-1}AC = J\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(A = \left[ \begin{array}{cc} 1\amp 1\\1\amp 1 \end{array} \right]\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp28742296}{}\quad{}The eigenvalues of \(A\) are \(2\) and \(0\) with corresponding eigenvectors \([1 \ 1]^{\tr}\) and \([-1 \ 1]^{\tr}\). Since we have a basis for \(\R^2\) consisting of eigenvectors for \(A\), we know that \(A\) is diagonalizable. Moreover, Jordan canonical form of \(A\) is \(J = \left[ \begin{array}{cc} 2\amp 0\\0\amp 0 \end{array} \right]\) and \(C^{-1}AC = J\), where \(C = \left[ \begin{array}{cr} 1\amp -1\\1\amp 1 \end{array} \right]\).%
\item{}\(A = \left[ \begin{array}{ccc} 0\amp 1\amp 1\\0\amp 0\amp 0 \\ 0\amp 0\amp 0 \end{array} \right]\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp28746264}{}\quad{}Since \(A\) is upper triangular, its eigenvalues are the diagonal entries. So the only eigenvalue of \(A\) is 0, and technology shows that this eigenvalue has geometric multiplicity 2. An eigenvector for \(A\) is \(\vv_1 = [1 \ 0 \ 0]^{\tr}\). A vector \(\vv_2\) that satisfies \(A \vv_2 = \vv_1\) is \(\vv_2 = [0 \ 0 \ 1]^{\tr}\). Letting \(C = \left[ \begin{array}{ccr} 1\amp 0\amp 0\\0\amp 0\amp -1\\0\amp 1\amp 1 \end{array}  \right]\) gives us%
\begin{equation*}
\C^{-1}AC = \left[ \begin{array}{ccc} 0\amp 1\amp 0 \\ 0\amp 0\amp 0 \\ 0\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
%
\item{}\(A = \left[ \begin{array}{cccc} 1\amp 1\amp 1\amp 1\\0\amp 1\amp 0\amp 0\\0\amp 0\amp 2\amp 2 \\ 0\amp 0\amp 0\amp 2 \end{array} \right]\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp28745368}{}\quad{}Again, \(A\) is upper triangular, so the eigenvalues of \(A\) are \(2\) and \(1\), both of algebraic multiplicity 2 and geometric multiplicity 1. Technology shows that the reduced row echelon forms of \(A - 2I_4\) and \((A-2I_4)^2\) are%
\begin{equation*}
\left[ \begin{array}{ccrc} 1\amp 0\amp -1\amp 0 \\ 0\amp 1\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 1 \\ 0\amp 0\amp 0\amp 0 \end{array}  \right] \ \text{ and }  \ \left[ \begin{array}{ccrc} 1\amp 0\amp -1\amp 1 \\ 0\amp 1\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
Now \(\vv_2 = [-1 \ 0 \ 0 \ 1]^{\tr}\) is in \(\Nul (A-2I_4)^2\), and%
\begin{equation*}
\vv_1 = (A-2I_4)\vv_2 = [2 \ 0 \ 2 \ 0]^{\tr}\text{.}
\end{equation*}
Notice that  Let \(\vv_1\) is an eigenvector of \(A\) with eigenvalue \(2\). Technology also shows that the reduced row echelon forms of \(A-I_4\) and \((A-I_4)^2\) are%
\begin{equation*}
\left[ \begin{array}{cccc} 0\amp 1\amp 0\amp 0 \\ 0\amp 0\amp 1\amp 0 \\ 0\amp 0\amp 0\amp 1 \\ 0\amp 0\amp 0\amp 0 \end{array}  \right] \ \text{ and }  \ \left[ \begin{array}{cccc} 0\amp 0\amp 1\amp 0 \\ 0\amp 0\amp 0\amp 1 \\ 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0 \end{array}  \right]\text{.}
\end{equation*}
Now \(\vv_4 = [0 \ 1 \ 0 \ 0]^{\tr}\) is in \(\Nul (A-I_4)^2\), and%
\begin{equation*}
\vv_3 = (A-I_4)\vv_4 = [1 \ 0 \ 0 \ 0]^{\tr}\text{.}
\end{equation*}
Notice that  Let \(\vv_3\) is an eigenvector of \(A\) with eigenvalue \(1\). Letting \(C = \left[ \begin{array}{crcc} 2\amp -1\amp 1\amp 0 \\ 0\amp 0\amp 0\amp 1 \\ 2\amp 0\amp 0\amp 0 \\ 0\amp 1\amp 0\amp 0 \end{array}  \right]\) gives us%
\begin{equation*}
C^{-1}AC = \left[ \begin{array}{cccc} 2\amp 1\amp 0\amp 0 \\ 0\amp 2\amp 0\amp 0 \\ 0\amp 0\amp 1\amp 1 \\ 0\amp 0\amp 0\amp 1 \end{array}  \right]\text{.}
\end{equation*}
%
\end{enumerate}
\end{example}
\begin{figureptx}{The effect of the transformation \(T\).}{x:figure:F_JCF_example_3_d}{}%
\begin{image}{0.25}{0.5}{0.25}%
\includegraphics[width=\linewidth]{external/JCF_example_3_d.pdf}
\end{image}%
\tcblower
\end{figureptx}%
\begin{example}{}{g:example:idp28766232}%
Let \(T(\vx) = A\vx\), where \(A = \left[ \begin{array}{crc} 0\amp 1\amp 1 \\ 0\amp 1\amp 2 \\ 1\amp -1\amp 2 \end{array} \right]\). Assume that \(P^{-1}AP = \left[ \begin{array}{ccc} 1\amp 1\amp 0 \\ 0\amp 1\amp 1 \\ 0\amp 0\amp 1 \end{array} \right]\), where \(P = \left[ \begin{array}{ccc} 1\amp 0\amp 4 \\ 0\amp 2\amp 4 \\ 1\amp 2\amp 0 \end{array} \right]\). Find a specific coordinate system in which it is possible to succinctly describe the action of \(T\), then describe the action of \(T\) on \(\R^3\) in as much detail as possible.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp28762136}{}\quad{}First note that \(1\) is the only eigenvalue of \(A\). Since \(A\) does not have \(0\) as an eigenvalue, it follows that \(A\) is invertible and so \(T\) is both one-to-one and onto. Let \(\vv_1 = [4 \ 4 \ 0]^{\tr}\), \(\vv_2 = [0 \ 2 \ 2]^{\tr}\), and \(\vv_3 = [1 \ 0 \ 1]^{\tr}\). We have that%
\begin{align*}
(A-I_3)\vv_3 \amp = \vv_2\\
(A-I_3)\vv_2 \amp = \vv_1\\
(A-I_3) \vv_1 \amp = \vzero\text{.}
\end{align*}
and so%
\begin{align*}
T(\vv_3) \amp = A \vv_3 = \vv_3+\vv_2\\
T(\vv_2) \amp = A \vv_2 = \vv_2+\vv_1\\
T(\vv_1) \amp = A\vv_1 = \vv_1\text{.}
\end{align*}
%
\par
If we consider the coordinate system in \(\R^3\) defined by the basis \(\{\vv_1, \vv_2, \vv_3\}\) as shown in blue in \hyperref[x:figure:F_JCF_example_3_d]{Figure~{\xreffont\ref{x:figure:F_JCF_example_3_d}}}, the fact that \(T(\vv_1) = \vv_1\) shows that \(T\) fixes all vectors in \(\Span\{\vv_1\}\). That \(T(\vv_2) = \vv_2+\vv_1\) tells us that \(T\) maps \(\Span\{\vv_2\}\) onto \(\Span\{\vv_1+\vv_2\}\), and \(T(\vv_3) = \vv_3+\vv_2\) shows that \(T\) maps \(\Span\{\vv_3\}\) onto \(\Span\{\vv_2+\vv_3\}\). So \(T\) sends the box defined by \(\vv_1\), \(\vv_2\), and \(\vv_3\) onto the box defined by \(\vv_1\), \(\vv_1+\vv_2\), and \(\vv_2+\vv_3\) (in red in \hyperref[x:figure:F_JCF_example_3_d]{Figure~{\xreffont\ref{x:figure:F_JCF_example_3_d}}}. So the action of \(T\) is conveniently viewed in the coordinate system determined by the columns of a matrix \(P\) that converts \(A\) into its Jordan canonical form.%
\end{example}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Summary}
\typeout{************************************************}
%
\begin{sectionptx}{Summary}{}{Summary}{}{}{x:section:sec_jordan_summ}
%
\begin{itemize}[label=\textbullet]
\item{}Any square matrix \(A\) is similar to a Jordan canonical form%
\begin{equation*}
\left[ \begin{array}{cccc} J_1\amp 0\amp \cdots\amp 0 \\ 0\amp J_2\amp \cdots\amp 0 \\ \vdots\amp \vdots\amp \ddots\amp \vdots \\ 0\amp 0\amp \cdots\amp J_t \end{array}  \right]\text{,}
\end{equation*}
where each matrix \(J_i\) is a Jordan block of the form%
\begin{equation*}
\left[ \begin{array}{ccccccc} \lambda\amp 1\amp 0 \amp \cdots\amp 0\amp 0\amp 0 \\ 0\amp \lambda\amp 1\amp \cdots\amp 0\amp 0\amp 0 \\ 0\amp 0\amp \lambda\amp \cdots\amp 0\amp 0\amp 0 \\ \amp \amp \amp \ddots\amp \ddots\amp \amp  \\ 0\amp 0\amp 0\amp \cdots\amp \lambda\amp 1\amp 0 \\ 0\amp 0\amp 0\amp \cdots\amp 0\amp \lambda\amp 1 \\  0\amp 0\amp 0\amp \cdots\amp 0\amp 0\amp \lambda \end{array}  \right]\text{.}
\end{equation*}
with \(\lambda\) as a eigenvalue of \(A\).%
\item{}A generalized eigenvector of an \(n \times n\) matrix \(A\) corresponding to an eigenvalue \(\lambda\) of \(A\) is a non-zero vector \(\vx\) satisfying%
\begin{equation*}
(A - \lambda I_n)^m \vx = \vzero
\end{equation*}
for some positive integer \(m\). If \(A\) is an \(n \times n\) matrix, then we can find a basis of \(\R^n\) consisting of generalized eigenvectors \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_n\) of \(A\) so that that matrix \(C = [\vv_1 \ \vv_2 \ \cdots \ \vv_n]\) has the property that \(C^{-1}AC\) is a Jordan canonical form.%
\item{}A vector space \(V\) is a direct sum of subspaces \(V_1\), \(V_2\), \(\ldots\), \(V_m\) if every vector \(\vv\) in \(V\) can be written uniquely as a sum%
\begin{equation*}
\vv = \vv_1+\vv_2+\vv_3+\cdots + \vv_m\text{,}
\end{equation*}
with \(\vv_i \in V_i\) for each \(i\).%
\item{}A square matrix \(A\) is nilpotent if and only if \(0\) is the only eigenvalue of \(A\). The Jordan form of a matrix \(A\) can always be written in the form \(D + N\), where \(D\) is a diagonal matrix and \(N\) is a nilpotent matrix.%
\item{}A subspace \(W\) of a vector space \(V\) is invariant under a linear transformation \(T: V \to V\) if \(T(\vw) \in W\) whenever \(\vw\) is in \(W\).%
\end{itemize}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Exercises  Exercises}
\typeout{************************************************}
%
\begin{exercises-section}{Exercises}{}{Exercises}{}{}{x:exercises:sec_jordan_exer}
\begin{divisionexercise}{1}{}{}{g:exercise:idp28807192}%
Find a Jordan canonical form for each of the following matrices.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(A=\left[ \begin{array}{rcr} 10\amp 6\amp 1\\2\amp 12\amp -1\\-4\amp 12\amp 14 \end{array} \right]\)%
\item{}\(B=\left[ \begin{array}{rcc} 8\amp 4\amp 1\\0\amp 9\amp 0\\-1\amp 4\amp 10 \end{array} \right]\)%
\item{}\(C=\left[ \begin{array}{crc} 6\amp 0\amp 0\\2\amp 4\amp 2\\8\amp -8\amp 14 \end{array} \right]\)%
\item{}\(D=\left[ \begin{array}{rrrr} 3\amp 3\amp -2\amp 1\\-2\amp -3\amp 3\amp -2\\0\amp -1\amp 2\amp -1\\5\amp 8\amp -6\amp 4 \end{array} \right]\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{2}{}{}{g:exercise:idp28811160}%
Let \(a \neq 0\). Show that the Jordan canonical form of \(\left[ \begin{array}{cccc} 4\amp 1\amp 0\amp 0 \\ 0\amp 4\amp a\amp 0 \\ 0\amp 0\amp 4\amp 0 \\ 0\amp 0\amp 0\amp 4 \end{array} \right]\) is independent of the value of \(a\).%
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{}{g:exercise:idp28816024}%
Show that the Jordan canonical form of \(\left[ \begin{array}{cccc} 2\amp 1\amp 0\amp 0 \\ 0\amp 2\amp 1\amp 0 \\ 0\amp 0\amp 2\amp a \\ 0\amp 0\amp 0\amp 4 \end{array} \right]\) is independent of the value of \(a\).%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{}{g:exercise:idp28816152}%
Let \(A\) and \(B\) be similar matrices with \(B = Q^{-1}AQ\). Let \(U = C_1^{-1}AC_1\) be the Jordan canonical form of \(A\). Show that \(U = C_2^{-1}BC_2\) is also the Jordan canonical form of \(B\) with \(C_2 = Q^{-1}C_1\).%
\end{divisionexercise}%
\begin{divisionexercise}{5}{}{}{g:exercise:idp28822680}%
Find all of the Jordan canonical forms for \(2 \times 2\) matrices and \(3 \times 3\) matrices.%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{}{g:exercise:idp28824728}%
Find the Jordan canonical form of \(\left[ \begin{array}{crrcr} 1\amp 2\amp 2\amp 1\amp -2\\0\amp -1\amp 0\amp 0\amp 0 \\ 0\amp 0\amp -1\amp 1\amp 2 \\ 0\amp 2\amp 0\amp 1\amp 0 \\ 0\amp 1\amp 0\amp 1\amp 1 \end{array} \right]\).%
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{}{g:exercise:idp28821016}%
For the matrix \(A\), find a matrix \(C\) so that \(J = C^{-1}AC\) is the Jordan canonical form of \(A\), where first: the diagonal entries do not increase as we move down the matrix \(J\) and, second: the Jordan blocks do not increase in size as we move down the matrix \(J\).%
\begin{equation*}
A =  \left[ \begin{array}{rrrrrrrrrr} 3\amp -1\amp 1\amp -1\amp 1\amp -1\amp 1\amp -1\amp 1\amp -1 \\ 1\amp 1\amp 1\amp 1\amp -1\amp 1\amp -1\amp 1\amp -1\amp 1 \\ 1\amp -1\amp 3\amp 1\amp -1\amp 1\amp -1\amp 1\amp -1\amp 1\\ 1\amp -1\amp 1\amp 1\amp 1\amp 1\amp -1\amp 1\amp -1\amp 1  \\ 0\amp 0\amp 0\amp 0\amp 2\amp 2\amp 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0\amp 0\amp 2\amp 2\amp 0\amp 0\amp 0\\ 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 2\amp 2\amp 0\amp 0\\ -1\amp 1\amp -1\amp 1\amp -1\amp 1\amp -1\amp 3\amp 1\amp -1\\ 2\amp -2\amp 2\amp -2\amp 2\amp -2\amp 2\amp -2\amp 4\amp 0\\ 1\amp -1\amp 1\amp -1\amp 1\amp -1\amp 1\amp -1\amp 1\amp 3 \end{array}  \right]\text{.}
\end{equation*}
%
\end{divisionexercise}%
\begin{divisionexercise}{8}{}{}{g:exercise:idp28826008}%
A polynomial in two variables is an object of the form%
\begin{equation*}
p(s,t) = \sum a_{ij}s^it^j
\end{equation*}
where \(i\) and \(j\) are integers greater than or equal to 0. For example,%
\begin{equation*}
q(s,t) = 3 + 2st + 4s^2t - 10st^3+5s^2t^2
\end{equation*}
is a polynomial in two variables. The degree of a monomial \(s^it^j\) is \(i+j\). The degree of a polynomial \(p(s,t)\) is the largest degree of any monomial summand of \(p(s,t)\). So the degree of the example polynomial \(q(s,t)\) is 4. Two polynomials in \(V\) are equal if they have the same coefficients on like terms. We add two polynomials in the variables \(s\) and \(t\) by adding coefficients of like terms. Scalar multiplication is done by multiplying each coefficient by the given scalar. Let \(V\) be the set of all polynomials of two variables of degree less than or equal to 2. You may assume that \(V\) is a vector space under this addition and multiplication by scalars and has the standard additive identity and additive inverses. Define \(F: V \to V\) and \(G: V \to V\) by%
\begin{equation*}
F(p(s,t)) = s\frac{\partial p}{\partial s}(s,t) + \frac{\partial p}{\partial t}(s,t)
\end{equation*}
and%
\begin{equation*}
G(p(s,t)) = \frac{\partial p}{\partial s}(s,t) + t\frac{\partial p}{\partial t}(s,t)\text{.}
\end{equation*}
That is,%
\begin{align*}
F(a_0+a_1s+a_2t+a_3st +a_4s^2+a_5t^2) \amp = s(a_1+a_3t+2a_4s) + (a_2+a_3s+2a_5t)\\
\amp = a_2 + (a_1+a_3)s + 2a_5t + a_3st + 2a_4s^2
\end{align*}
and%
\begin{align*}
G(a_0+a_1s+a_2t+a_3st +a_4s^2+a_5t^2) \amp = (a_1+a_3t+2a_4s) + t(a_2+a_3s+2a_5t)\\
\amp = a_1 + 2a_4s + (a_2+a_3)t + a_3st + 2a_5t^2\text{.}
\end{align*}
You may assume that \(F\) and \(G\) are linear transformations.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why \(\CB = \{1,s,t,st,s^2,t^2\}\) is a basis for \(V\).%
\item{}Explain why \(F\) and \(G\) are not diagonalizable.%
\item{}Show that \([F]_{\CB}\) and \([G]_{\CB}\) have the same Jordan canonical form. (So different transformations can have the same Jordan canonical form.)%
\item{}Find ordered bases \(\CC_F = \{f_1, f_2, f_3, f_4, f_5, f_6\}\) and \(\CC_G = \{g_1, g_2, g_3, g_4, g_5, g_6\}\) of \(V\) for which \([F]_{\CC_F}\) and \([G]_{\CC_G}\) are the Jordan canonical form from (c).%
\item{}Recall that a linear transformation can be defined by its action on a basis. So define \(H : V \to V\) satisfying \(H(g_i) = f_i\) for \(i\) from 1 to 6. Show that \(H\) is invertible and that \(G = H^{-1}FH\). This is the essential argument to show that any two linear transformations with the same matrix with respect to some bases are similar.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp28585496}{}\quad{}What matrix is \([H]_{\CC_G}^{\CC_F}\)?%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{9}{}{}{g:exercise:idp28579992}%
Let%
\begin{equation*}
\vv_p \underset{A - \lambda I}{\rightarrow} \vv_{p-1} \underset{A - \lambda I}{\rightarrow}  \cdots \ \underset{A - \lambda I}{\rightarrow} \vv_1 \underset{A - \lambda I}{\rightarrow} \vzero
\end{equation*}
be a chain of generalized eigenvectors for a matrix \(A\) corresponding to the eigenvalue \(\lambda\). In this problem we show that the set \(\{\vv_1, \vv_2, \ldots, \vv_p\}\) is linearly independent.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Explain why \(\vv_k\) is in \(\Nul (A-\lambda I)^k\) for any \(k\) between \(1\) and \(p\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp28582552}{}\quad{}Show that \(\vv_{1} = (A-\lambda I)^{k-1} \vv_k\) for each \(k\) from \(2\) to \(p\).%
\item{}Consider the equation%
\begin{equation}
x_1 \vv_1 + x_2 \vv_2 + \cdots + x_p \vv_p = \vzero\label{x:men:ex_new_1}
\end{equation}
for scalars \(x_1\), \(x_2\), \(\ldots\), \(x_p\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Multiply both sides of \hyperref[x:men:ex_new_1]{({\xreffont\ref{x:men:ex_new_1}})} on the left by \((A-\lambda I)^{p-1}\). Explain why we can then conclude that \(x_p = 0\).%
\item{}Rewrite \hyperref[x:men:ex_new_1]{({\xreffont\ref{x:men:ex_new_1}})} using the result from part i. Explain how we can then demonstrate that \(x_{p-1} = 0\).%
\item{}Describe how we can use the process in parts i. and ii. to show that \(x_1 = x_2 = \cdots x_p = 0\). What does this tell us about the set \(\{\vv_1, \vv_2, \ldots, \vv_p\}\)?%
\end{enumerate}
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{10}{}{}{g:exercise:idp28888344}%
Find, if possible, a matrix transformation \(T:\R^3 \to \R^3\) for which there is a one-dimensional invariant subspace but no two-dimensional invariant subspace. If not possible, explain why.%
\end{divisionexercise}%
\begin{divisionexercise}{11}{}{}{g:exercise:idp28886168}%
Let \(A = \left[ \begin{array}{cr} 2\amp -1\\1\amp 0 \end{array} \right]\). It is the case that \(A\) has a single eigenvalue of algebraic multiplicity 2 and geometric multiplicity 1.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find a matrix \(C\) whose columns are generalized eigenvectors for \(A\) so that \(C^{-1}AC=J\) is in Jordan canonical form.%
\item{}Determine the entries of \(J^k\) and then find the entries of \(A^k\) for any positive integer \(k\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{12}{}{}{g:exercise:idp28894744}%
Let \(\vv_1\), \(\vv_2\), \(\ldots\), \(\vv_r\) be eigenvectors of an \(n \times n\) matrix \(A\), and let \(W = \Span\{\vv_1, \vv_2, \ldots, \vv_r\}\). Show that \(W\) is invariant under the matrix transformation \(T\) defined by \(T(\vx) = A\vx\).%
\end{divisionexercise}%
\begin{divisionexercise}{13}{}{}{g:exercise:idp28898584}%
Let \(A\) be an \(n \times n\) matrix with eigenvalue \(\lambda\). Let \(S:\R^n \to \R^n\) be defined by \(S(\vx) = B\vx\) for some matrix \(B\). Show that if \(B\) commutes with \(A\), then the eigenspace \(E_\lambda\) of \(A\) corresponding to the eigenvalue \(\lambda\) is invariant under \(S\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp28899608}{}\quad{}Show that \((A - \lambda I)S(\vx) = \vzero\)%
\end{divisionexercise}%
\begin{divisionexercise}{14}{}{}{g:exercise:idp28901272}%
Determine which of the following matrices is nilpotent. Justify your answers. For each nilpotent matrix, find its index.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\left[ \begin{array}{rc} -2\amp 1\\-4\amp 2 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{cc} 0\amp 1\\1\amp 0 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{rrc} -1\amp 1\amp 0\\0\amp -1\amp 1 \\1\amp -3\amp 2 \end{array} \right]\)%
\item{}\(\left[ \begin{array}{cc} xy\amp x^2\\-y^2\amp -xy \end{array} \right]\) for any real numbers \(x\) and \(y\)%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{15}{}{}{g:exercise:idp28908440}%
Find two different nonzero \(3 \times 3\) nilpotent matrices whose index is \(2\). If no such matrices exist, explain why.%
\end{divisionexercise}%
\begin{divisionexercise}{16}{}{}{g:exercise:idp28907928}%
Find, if possible, \(4 \times 4\) matrices whose indices are \(1\), \(2\), \(3\), and \(4\). If not possible, explain why.%
\end{divisionexercise}%
\begin{divisionexercise}{17}{}{}{g:exercise:idp28909976}%
Let \(V\) be an \(n\)-dimensional vector space and let \(W\) be a subspace of \(V\). Show that \(V = W \oplus W^{\perp}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp28916504}{}\quad{}Use a projection onto a subspace.%
\end{divisionexercise}%
\begin{divisionexercise}{18}{}{}{g:exercise:idp28919192}%
Let \(W\) be a subspace of an \(n\)-dimensional vector space \(V\) that is invariant under a transformation \(T: V \to V\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Show by example that \(W^{\perp}\) need not be invariant under \(T\).%
\item{}Show that if \(T\) is an isometry, then \(W^{\perp}\) is invariant under \(T\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{19}{}{}{g:exercise:idp28919576}%
Let \(T: \pol_2 \to \pol_2\) be defined by \(T(p(t)) = p(t) - p'(t)\), where \(p'(t)\) is the derivative of \(p(t)\). That is, if \(p(t) = a_0+a_1t+a_2t^2\), then \(p'(t) = a_1 + 2a_2t\). Find a basis \(\CB\) for \(\pol_2\) in which the matrix \([T]_{\CB}\) is in Jordan canonical form.%
\end{divisionexercise}%
\begin{divisionexercise}{20}{}{}{g:exercise:idp28928152}%
Let \(A\) be an \(n \times n\) nilpotent matrix with index \(m\). Since \(A^{m-1}\) is not zero, there is a vector \(\vv \in \R^n\) such that \(A^{m-1} \vv \neq \vzero\). Prove that the vectors \(\vv\), \(A\vv\), \(A^2\vv\), \(\ldots\), \(A^{m-1}\vv\) are linearly independent.%
\end{divisionexercise}%
\begin{divisionexercise}{21}{}{}{g:exercise:idp28939416}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Let \(A = \left[ \begin{array}{rcr} 2\amp 1\amp -3 \\ -2\amp 1\amp 1 \\ 2\amp 1\amp -3 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}Show that \(A\) is nilpotent.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp28937880}{}\quad{}Calculate powers of \(A\).%
\item{}Calculate the matrix product \((I_3-A)\left(I_3+A+A^2\right)\). What do you notice and what does this tell us about \((I_3-A)\)?%
\end{enumerate}
\item{}If \(A\) is an \(n \times n\) nilpotent matrix, show that \(I - A\) is nonsingular, where \(I\) is the \(n \times n\) identity matrix.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{22}{}{}{x:exercise:ex_8_d_upper_triangular}%
In this exercise we show that every upper triangular matrix satisfies its characteristic polynomial.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}To illustrate how this will work, consider a \(3 \times 3\) example. Let \(T = \left[ \begin{array}{ccc} \lambda_1\amp a\amp b \\ 0\amp \lambda_2\amp c \\ 0\amp 0\amp \lambda_3 \end{array} \right]\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}What is the characteristic polynomial \(p(x)\) of \(T\)?%
\item{}Consider the matrices \(S_1 = T-\lambda_1 I_3\), \(S_2 = T-\lambda_2 I_3\), and \(S_3 = T-\lambda_3 I_3\). Show that the first column of \(S_1\) is \(\vzero^{\tr}\), the first two columns of \(S_1S_2\) are \(\vzero^{\tr}\), and that every column of \(S_1S_2S_3\) is \(\vzero^{\tr}\). Conclude that \(T\) satisfies its characteristic polynomial.%
\end{enumerate}
\item{}Now we consider the general case. Suppose \(T\) is an \(n \times n\) upper triangular matrix with diagonal entries \(\lambda_1\), \(\lambda_2\), \(\ldots\), \(\lambda_n\) in order. For \(i\) from \(1\) to \(n\) let \(S_i = T - \lambda_i I_n\). Show that for each \(k\) from 1 to \(n\), the first \(k\) columns of \(S_1S_2 \cdots S_k\) are equal to \(\vzero{\tr}\). Then explain how this demonstrates that \(T\) satisfies its characteristic polynomial.%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{23}{}{}{g:exercise:idp28950424}%
Prove \hyperref[x:theorem:thm_nilpotent_evals]{Theorem~{\xreffont\ref{x:theorem:thm_nilpotent_evals}}} that a square matrix \(A\) is nilpotent if and only if 0 is the only eigenvalue of \(A\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint 1}.\hypertarget{g:hint:idp28962840}{}\quad{}For one direction, use the Cayley-Hamilton Theorem.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint 2}.\hypertarget{g:hint:idp28959640}{}\quad{}If \(A\) is nilpotent and \(\vx\) is an eigenvector of \(A\), what is \(A^m \vx\) for every positive integer \(m\)? If \(0\) is the only eigenvalue of \(A\), what is the characteristic polynomial of \(A\)?%
\end{divisionexercise}%
\begin{divisionexercise}{24}{}{}{x:exercise:ex_Direct_sum_properties}%
Let \(V\) be a vector space that is a direct sum \(V = V_1 \oplus V_2 \oplus \cdots \oplus V_m\) for some positive integer \(m\). Prove the following.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(V_i \cap V_j = \{\vzero\}\) whenever \(i \neq j\).%
\item{}If \(V\) is finite dimensional, and if \(\CB_i\) is a basis for \(V_i\), then the set \(\CB = \cup_{i=1}^m \CB_i\) is a basis for \(V\).%
\item{}\(\dim(V) = \dim(V_1) + \dim(V_2) + \cdots + \dim(V_m)\).%
\end{enumerate}
\end{divisionexercise}%
\begin{divisionexercise}{25}{}{}{g:exercise:idp28968856}%
Label each of the following statements as True or False. Provide justification for your response.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\lititle{True\slash{}False.}\par%
If \(A\) is an \(n \times n\) nilpotent matrix, then the index of \(A\) is \(n\).%
\item{}\lititle{True\slash{}False.}\par%
The Jordan canonical form of a matrix is unique.%
\item{}\lititle{True\slash{}False.}\par%
Every nilpotent matrix is singular.%
\item{}\lititle{True\slash{}False.}\par%
Eigenvectors of a linear transformation \(T\) are also generalized eigenvectors of \(T\).%
\item{}\lititle{True\slash{}False.}\par%
It is possible for a generalized eigenvector of a matrix \(A\) to correspond to a scalar that is not an eigenvalue for \(A\).%
\item{}\lititle{True\slash{}False.}\par%
The vectors in a cycle of generalized eigenvectors of a matrix are linearly independent.%
\item{}\lititle{True\slash{}False.}\par%
A Jordan canonical form of a diagonal matrix \(A\) is \(A\).%
\item{}\lititle{True\slash{}False.}\par%
Let \(V\) be a finite-dimensional vector space and let \(T\) be a linear transformation from \(V\) to \(V\). Let \(J\) be a Jordan canonical form for \(T\). If \(\CB\) is a basis of \(V\), then a Jordan canonical form of \([T]_{\CB}\) is \(J\).%
\item{}\lititle{True\slash{}False.}\par%
Matrices with the same Jordan canonical form are similar.%
\item{}\lititle{True\slash{}False.}\par%
Let \(V\) be a finite-dimensional vector space and let \(T\) be a linear transformation from \(V\) to \(V\). If \(\CB\) is an ordered basis for \(V\), then \(T\) is nilpotent if and only if \([T]_{\CB}\) is nilpotent.%
\item{}\lititle{True\slash{}False.}\par%
If \(T\) is a linear transformation whose only eigenvalue is 0, then \(T\) is the zero transformation.%
\item{}\lititle{True\slash{}False.}\par%
Let \(V\) be a finite-dimensional vector space and let \(T\) be a linear transformation from \(V\) to \(V\). Then \(V = T(V) \oplus \Ker(T)\).%
\item{}\lititle{True\slash{}False.}\par%
Let \(V\) be a finite-dimensional vector space and let \(T\) be a linear transformation from \(V\) to \(V\). If \(\CB\) is an ordered basis for \(V\) of generalized eigenvectors of \(T\), then \([T]_{\CB}\) is a Jordan canonical form for \(T\).%
\end{enumerate}
\end{divisionexercise}%
\end{exercises-section}
%
%
\typeout{************************************************}
\typeout{Subsection  Project: Modeling an Epidemic}
\typeout{************************************************}
%
\begin{sectionptx}{Project: Modeling an Epidemic}{}{Project: Modeling an Epidemic}{}{}{x:section:sec_proj_epidemic}
The COVID-19 epidemic has generated many mathematical and statistical models to try to understand the spread of the virus. In this project we examine a simple stochastic model of the spread of an epidemic proposed by Norman Bailey in 1950.\footnote{Bailey, N. T. J. (1950) A simple stochastic epidemic. \pubtitle{Biometrika} 37\label{g:fn:idp29000216}} This is a model of a relatively mild epidemic in which no one dies from the disease. Of course, mathematicians build on simple models to form more complicated and realistic ones, but this is a good, and accessible, starting point. Bailey writes about the the difficulties in the stochastic\footnote{The word \terminology{stochastic} refers to quantities that have a random pattern that can be analyzed statistically, but generally cannot be predicted precisely.\label{g:fn:idp28998552}} analysis of epidemics. For example, the overall epidemic ``can often be broken down into smaller epidemics occurring in separate regional subdivisions'' and that these regional epidemics ``are not necessarily in phase and often interact with each other''. This is behavior that has been clearly evident in the COVID-19 epidemic in the US. Even within a single district, ``it is obvious that a given infectious individual has not the same chance of infecting each inhabitant. He will probably be in close contact with a small number of people only, perhaps of the order of 10-50, depending on the nature of his activities.'' But then the epidemic for the whole district will ``be built up from epidemics taking place in several relatively small groups of associates and acquaintances.'' So we can see in this analysis that an epidemic can spread from small, localized areas.%
\par
Bailey begins by considering a community of \(n\) persons who are susceptible to a disease, and supposes introducing a single infected individual into the community. Bailey makes the following assumptions: ``We shall assume that the infection spreads by contact between the members of the community, and that it is not sufficiently serious for cases to be withdrawn from circulation by isolation or death; also that no case becomes clear of infection during the course of the main part of the epidemic.''%
\par
To see how the Bailey's model is constructed, we make some assumption. We split the population at any time into two groups, those infected with the disease, and those susceptible, i.e., not currently infected. We assume that once an individual is infected, that individual is always infected. A person catches the disease by interacting with an infected individual. That is, if a susceptible individual meets an infected individual, the chance that the susceptible person contracts the disease is \(\beta\) (the \emph{infection rate}). For example, there many be a \(5\%\) chance that an encounter between a susceptible individual and an infected individual results in the susceptible individual contracting the disease. (Of course, there are many variables involved in such an interaction \textemdash{} if no one is wearing masks and the interaction involves close contact, the infection rate would be higher than if everyone practices social distancing. For the sake of simplicity, we assume one infection rate for the entire population.) With this simple model we assume assume homogeneous mixing in the population. That is, it is equally likely that any one individual will interact with any other in a given time frame. Let \(y\) be the number of susceptible individuals in the population at time \(t\). Then the number of infected individuals is the total population minus \(y\), or \(n+1-y\) (recall that we introduced an infected individual into the population). So the change in the number of susceptible individuals in the population at time \(s\) is \(-\beta y(n+1-y)\) (since the \(n+1-y\) susceptible individuals interact with the \(y\) infected individuals). That is, \(\frac{dy}{ds} = -\beta y (n+1-y)\). Bailey makes the substitution of \(t\) for \(\beta s\) to simplify the equation. This substitution makes \(ds = \frac{1}{\beta} dt\), which produces the differential equation \(\beta \frac{dy}{dt} =  -\beta y (n+1-y)\). The \(\beta\)s cancel, producing the differential equation%
\begin{equation}
\frac{dy}{dt} = -y(n+1-y)\label{x:men:eq_Bailey_1}
\end{equation}
with \(y(0) = n\).%
\par
The above analysis is just to provide some background. Bailey's stochastic model deals with probabilities instead of actual numbers, so we now take that approach. For \(r\) from \(0\) to \(n\), let \(p_r(t)\) be the probability that there are \(r\) susceptible individuals still uninfected at time \(t\). Similar to the case describe above, if there are \(r\) susceptible individuals, any one of them can be come infected by interacting with the \(n-r+1\) infected individuals. With that in mind, Bailey's model of the spread of the disease is the system%
\begin{equation*}
\begin{cases}\frac{d p_r(t)}{dt} = (r+1)(n-r)p_{r+1}(t) - r(n-r+1)p_r(t) \amp \text{ for }  0 \leq r \leq n-1\\ \frac{dp_n(t)}{dt} = -np_n(t). \amp \end{cases}
\end{equation*}
%
\par
That is,%
\begin{align*}
\frac{d p_0(t)}{dt} \amp = np_1(t)\\
\frac{d p_1(t)}{dt} \amp = 2(n-1)p_2(t) - np_1(t)\\
\frac{d p_2(t)}{dt} \amp = 3(n-2)p_3(t) - 2(n-1)p_2(t)\\
\vdots \amp\\
\frac{d p_{n-1}(t)}{dt} \amp = np_n(t) - (n-1)(2)p_{n-1}(t)\\
\frac{dp_n(t)}{dt} \amp = -np_n(t)\text{.}
\end{align*}
%
\par
Since we start with \(n\) susceptible individuals, at time \(0\) we have \(p_r(0) = 0\) if \(0 \leq r \leq n-1\) and \(p_n(0)=1\).%
\par
This system can be written in matrix form. Let \(P(t) = \left[ \begin{array}{c} p_0(t)\\p_1(t) \\ \vdots \\ p_n(t) \end{array}  \right]\). Assuming that we can differentiate a vector function component-wise, our system becomes%
\begin{equation*}
\frac{dP(t)}{dt} = A P(t)
\end{equation*}
with initial condition \(P(0) = [0 \ 0 \ \ldots \ 0 \ 1]^{\tr}\), where%
\begin{equation*}
A=\left[ \begin{array}{ccccccccc} 0\amp n\amp 0\amp 0\amp 0\amp  \cdots \amp 0\amp 0\amp 0 \\ 0\amp -n\amp 2(n-1)\amp 0 \amp 0\amp \cdots \amp 0\amp 0\amp 0 \\ 0\amp 0\amp -2(n-1)\amp 3(n-2) \amp 0 \amp  \cdots \amp  0\amp 0\amp 0 \\ \amp \amp \amp \amp \vdots\amp \amp \amp \amp  \\ 0\amp 0\amp  \cdots \amp -r(n-r+1) \amp (r+1)(n-r)\amp  0 \amp  \cdots \amp  0\amp 0 \\ \amp \amp \amp \amp \vdots\amp \amp \amp \amp   \\ 0\amp 0\amp 0\amp \cdots\amp 0\amp 0 \amp 0\amp -2(n-1)\amp n \\ 0\amp 0\amp 0\amp \cdots\amp 0\amp 0\amp 0\amp 0\amp -n \end{array}  \right]\text{.}
\end{equation*}
%
\par
The solution to this system involves a matrix exponential \(e^{At}\). The matrix exponential acts much like our familiar exponential function in that%
\begin{equation*}
\frac{d}{dx}e^{At} = Ae^{At}\text{.}
\end{equation*}
%
\par
With this in mind it is not difficult to see that Bailey's system has solution \(P(t)= e^{At}P(0)\). To truly understand this solution, we need to make sense of the matrix exponential \(e^{At}\).%
\par
We can make sense of the matrix exponential by utilizing the Taylor series of the exponential function centered at the origin. From calculus we know that%
\begin{equation}
e^x = 1 + x + \frac{1}{2!}x^2 + \frac{1}{3!}x^3 + \cdots + \frac{1}{n!}x^n + \cdots = \sum_{k\geq 0} \frac{1}{k!}x^k\text{.}\label{x:men:eq_exponential}
\end{equation}
%
\par
Since powers of square matrices are defined, the matrix exponential \(e^M\) of a square matrix \(M\) is then%
\begin{equation}
e^M = I_n + M + \frac{1}{2!}M^2 + \frac{1}{3!}M^3 + \cdots + \frac{1}{n!}M^n + \cdots = \sum_{k\geq 0} \frac{1}{k!}M^k\text{.}\label{x:men:eq_Matrix_exponential}
\end{equation}
%
\par
Just as in calculus, \(e^M\) converges for any square matrix \(M\).%
\begin{project}{}{g:project:idp29026200}%
If \(M\) is diagonalizable, then \(e^M\) can be found fairly easily. Assume that there is an invertible matrix \(P\) such that \(P^{-1}MP = D\), where \(D = \left[ \begin{array}{ccccc} \lambda_1\amp  0 \amp 0\amp  \cdots \amp 0 \\ 0 \amp  \lambda_2\amp  0 \amp \cdots \amp  0 \\ \vdots \amp \vdots \amp \vdots \amp  \ddots \amp  \vdots \\ 0 \amp  0\amp  0 \amp \cdots \amp  \lambda_n \end{array}  \right]\) is a diagonal matrix.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Use \hyperref[x:men:eq_Matrix_exponential]{({\xreffont\ref{x:men:eq_Matrix_exponential}})} to explain why%
\begin{equation*}
e^M = Pe^DP^{-1}\text{.}
\end{equation*}
%
\item{}Now show that%
\begin{equation*}
e^M = P\left[ \begin{array}{ccccc} e^{\lambda_1}\amp  0 \amp 0\amp  \cdots \amp 0 \\ 0 \amp  e^{\lambda_2}\amp  0 \amp \cdots \amp  0 \\ \vdots \amp \vdots \amp \vdots \amp  \ddots \amp  \vdots \\ 0 \amp  0\amp  0 \amp \cdots \amp  e^{\lambda_n} \end{array}  \right]P^{-1}\text{.}
\end{equation*}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp29033368}{}\quad{}What is \(D^k\) for any positive integer \(k\)? Then add corresponding components and compare to \hyperref[x:men:eq_exponential]{({\xreffont\ref{x:men:eq_exponential}})}.%
\end{enumerate}
\end{project}%
As we will see, the matrix \(A\) in Bailey's model is not diagonalizable, so we need to learn how to consider the matrix exponential in this new situation. In these cases we can utilize the Jordan canonical form. Of course, the computations are more complicated. We first illustrate with the \(2 \times 2\) case.%
\begin{project}{}{g:project:idp29032600}%
Assume \(A\) is a \(2 \times 2\) matrix with Jordan canonical form\(J = \left[ \begin{array}{cc} \lambda\amp 1\\0\amp \lambda \end{array}  \right]\), where \(C^{-1}AC = J\). The same argument as above shows that%
\begin{equation*}
e^A = Ce^JC^{-1}
\end{equation*}
and so we only have to be able to find \(e^J\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}We can write \(J\) in the form \(J = D+N\), where \(D\) is a diagonal matrix and \(N\) is a nilpotent matrix. Find \(D\) and \(N\) in this case and explain why \(N\) is a nilpotent matrix.%
\item{}Find the index of \(N\). That is, find the smallest positive power \(s\) of \(N\) such that \(N^s = 0\). Then use \hyperref[x:men:eq_Matrix_exponential]{({\xreffont\ref{x:men:eq_Matrix_exponential}})} to find \(e^N\).%
\item{}Assume that the matrix exponential satisfies the standard property of exponential functions that \(e^{R+S} = e^Re^S\) for any \(n \times n\) matrices \(R\) and \(S\). Use this property to explain why%
\begin{equation*}
e^J = \left[ \begin{array}{cc} e^{\lambda}\amp e^{\lambda}\\0\amp e^{\lambda} \end{array}  \right]\text{.}
\end{equation*}
%
\item{}Use the previous information to calculate \(e^M\) where \(M = \left[ \begin{array}{cr} 3 \amp -1 \\ 4 \amp 7 \end{array} \right]\).%
\end{enumerate}
\end{project}%
Now we turn to the general case of finding the matrix exponential \(e^M\) when \(M\) is not diagonalizable. Suppose \(C\) is an invertible matrix and \(C^{-1}MC = J\), where \(J\) is a Jordan canonical form of \(M\). Then we have%
\begin{equation*}
e^M = e^{CJC^{-1}} = C e^J C^{-1}\text{.}
\end{equation*}
%
\par
We know that \(J\) can be written in the form \(D+N\), where \(D\) is a diagonal matrix and \(N\) is an upper triangular matrix with zeros on the diagonal and some ones along the superdiagonal. It follows that%
\begin{equation*}
e^M = Ce^De^NC^{-1}\text{.}
\end{equation*}
%
\par
We know that if%
\begin{equation*}
D = \left[ \begin{array}{cccccc} \lambda_1\amp 0\amp 0\amp \cdots\amp 0\amp 0 \\  0\amp \lambda_2\amp 0\amp \cdots\amp 0\amp 0 \\ \amp \amp \amp \ddots\amp \amp  \\ 0\amp 0\amp 0\amp \cdots\amp 0\amp \lambda_n \end{array}  \right]\text{,}
\end{equation*}
then%
\begin{equation*}
e^D = \left[ \begin{array}{cccccc} e^{\lambda_1}\amp 0\amp 0\amp \cdots\amp 0\amp 0 \\  0\amp e^{\lambda_2}\amp 0\amp \cdots\amp 0\amp 0 \\ \amp \amp \amp \ddots\amp \amp  \\ 0\amp 0\amp 0\amp \cdots\amp 0\amp e^{\lambda_n} \end{array}  \right]\text{.}
\end{equation*}
%
\par
So finding \(e^M\) boils down to determining \(e^N\).%
\par
Now \(N\) has only zero as an eigenvalue, so \(N\) is nilpotent. If \(k\) is the index of \(N\), then%
\begin{equation*}
e^N = I + N + \frac{1}{2}N^2 + \frac{1}{3!}N^3 + \cdots + \frac{1}{(k-1)!}N^{k-1}\text{.}
\end{equation*}
%
\begin{project}{}{g:project:idp29058072}%
Let us apply this analysis to a specific case of Bailey's model, with \(n = 5\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find the entries of \(A\) when \(n = 5\).%
\item{}Let \(J\) be a Jordan canonical form for \(A\). Explain why the solution to Bailey's model has the form%
\begin{equation*}
P(t)= e^{At}P(0) = Ce^{Dt}e^{Nt}C^{-1}P(0)\text{,}
\end{equation*}
where \(C\) is an invertible matrix, \(Dt\) is a diagonal matrix, and \(Nt\) is a nilpotent matrix.%
\item{}Find a Jordan canonical form \(J\) for \(A\).%
\item{}Find a diagonal matrix \(D\) and a nilpotent matrix \(N\) so that \(J = D+N\).%
\item{}Find \(e^{Dt}\) and \(e^{Nt}\). Then show that%
\begin{equation*}
P(t) = \left[  \begin{array}{c} 1+\frac{172}{3}e^{-5t}-80te^{-5t}+\frac{125}{3}e^{-8t}-200te^{-8t}-100e^{-9t} \\ -\frac{220}{3}e^{-5t}+80te^{-5t}-\frac {320}{3}e^{-8t}+320te^{-8t} +180e^{-9t} \\ 10e^{-5t}+80e^{-8t}-120te^{-8t}-90e^{-9t} \\ \frac{10}{3}e^{-5t}-\frac{40}{3}e^{-8t}+10e^{-9t} \\ \frac{5}{3}e^{-5t}- \frac{5}{3}e^{-8t} \\ e^{-5t} \end{array}  \right]\text{.}
\end{equation*}
%
\end{enumerate}
\end{project}%
One use for mathematical models like Bailey's is to make predictions that can help set policies. Recall that we made the substitution of \(t\) for \(\beta s\) in our original equation in order to make the equations dimensionless, where \(\beta\) is the infection rate \textemdash{} the rate at which people catch the disease. Let us replace \(t\) with \(\beta s\) in our solution and analyze the effect of changing the value of \(\beta\).%
\begin{project}{}{g:project:idp29062680}%
If \(\beta = 1\), then the disease is easily transmitted from person to person. If \(\beta\) can be made smaller, then the disease is not so easily transmitted. We continue to work with the case where \(n=5\). Plot the curves \(p_0(t)\) through \(p_5(t)\) on the same set of axes for the following values of \(\beta\):%
\begin{equation*}
(a) \beta =1  (b) \beta = 0.5  (c) \beta = 0.25\text{.}
\end{equation*}
%
\par
Explain what you see and how this might be related to the phrase ``flattening the curve'' used during the COVID-19 pandemic of 2020.%
\end{project}%
In general, the matrix \(A\) has eigenvalues of algebraic multiplicity 1 and 2. When \(n\) is even, \(n+1\) is odd and the eigenvalues of \(A\) will occur in pairs except for the single eigenvalue \(0\) of multiplicity 1. When \(n\) is odd, \(n+1\) is even and we have two eigenvalues of multiplicity 1: \(0\) and the eigenvalue \(-r(n-r+1)\) when \(r = \frac{n+1}{2}\), at which \(-r(n-r+1) = -\frac{(n+1)^2}{4}\). It can be shown, although we won't do it here, that every eigenvalue of algebraic multiplicity 2 has geometric multiplicity 1. This information completely determines the Jordan canonical formof \(A\).%
\end{sectionptx}
\end{chapterptx}
\end{partptx}
%
\appendix%
%
\part*{Appendices}%
%
%
\typeout{************************************************}
\typeout{Appendix A Complex Numbers}
\typeout{************************************************}
%
\begin{appendixptx}{Complex Numbers}{}{Complex Numbers}{}{}{x:appendix:app_complex_numbers}
\begin{introduction}{}%
\begin{objectives}{Focus Questions}{g:objectives:idp29080984}
By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.%
%
\begin{itemize}[label=\textbullet]
\item{}What is a complex number?%
\item{}How is the sum and product of two complex numbers defined?%
\item{}How do we find the multiplicative inverse of a nonzero complex number?%
\item{}What general structure does the set of complex numbers have?%
\end{itemize}
\end{objectives}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Complex Numbers}
\typeout{************************************************}
%
\begin{sectionptx}{Complex Numbers}{}{Complex Numbers}{}{}{x:section:sec_complex_numbers}
Complex numbers are usually introduced as a tool to solve the quadratic equation \(x^2+1 = 0\). However, that is not how complex numbers first came to light. The story actually involves solutions to the general cubic equation. The interested reader could consult Chapter 6 of William Dunham's excellent book \pubtitle{Journey Through Genius}. In this appendix we touch on the basics of complex numbers to provide enough context for the section on complex eigenvalues.%
\par
A complex number is defined by a pair of real numbers - the \terminology{real part} of the complex number and the \terminology{imaginary part} of the complex number.%
\begin{definition}{}{g:definition:idp29094808}%
\index{complex number}%
A \terminology{complex number} is a number of the form%
\begin{equation*}
a+bi
\end{equation*}
where \(a\) and \(b\) are real numbers and \(i^2 = -1\).%
\end{definition}
\index{complex number!real part}\index{complex number!imaginary part} The number \(a\) is the real part of the complex number and the number \(b\) is the imaginary part. We often write%
\begin{equation*}
z = a+bi
\end{equation*}
for a complex number \(z\), \(\text{ Re } (z)\) for the real part of \(z\) and \(\text{ Im } (z)\) for the imaginary part of \(z\). That is, if \(z = a+bi\) with \(a\) and \(b\) real numbers, then \(\text{ Re } (z) = a\) and \(\text{ Im } (z) = b\). We say that two complex numbers \(a+bi\) and \(c+di\) are equal if \(a=c\) and \(b=d\).%
\par
There is an arithmetic of complex numbers that is determined by an addition and multiplication of complex numbers. Adding complex numbers is natural:%
\begin{equation*}
(a+bi) + (c+di) = (a+c) + (b+d)i\text{.}
\end{equation*}
%
\par
That is, to add two complex numbers we add their real parts together and their imaginary parts together.%
\begin{activity}{}{g:activity:idp29098136}%
Multiplication of complex numbers is is also done in a natural way.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}By expanding the product as usual, treating \(i\) as we would any real number, and exploiting the fact that \(i^2 = -1\), explain why we define the product of complex numbers \(a+bi\) and \(c+di\) as%
\begin{equation*}
(a+bi)(c+di) = (ac-bd) + (bc+ad)i\text{.}
\end{equation*}
%
\item{}Use the definitions of addition and multiplication to write each of the sums or products as a complex number in the form \(a+bi\).%
\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item{}\((2+3i) + (7-4i)\)%
\item{}\((4-2i)(3+i)\)%
\item{}\((2+i)i - (3+4i)\)%
\end{enumerate}
\end{enumerate}
\end{activity}%
It isn't difficult to show that the set of complex numbers, which we denote by \(\C\), satisfies many useful and familiar properties.%
\begin{activity}{}{x:activity:act_complex_field}%
Show that \(\C\) has the same structure as \(\R\). That is, show that for all \(u\), \(w\), and \(z\) in \(\C\), the following properties are satisfied.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(w+z \in \C\) and \(wz \in \C\)%
\item{}\(w+z=z+w\) and \(wz=zw\)%
\item{}\((w+z) + u = w + (z + u)\) and \((wz)u = w(zu)\)%
\item{}There is an element \(0\) in \(\C\) such that \(z+ 0 = z\)%
\item{}There is an element \(1\) in \(\C\) such that \((1)z = z\)%
\item{}There is an element \(-z\) in \(\C\) such that \(z+(-z) = 0\)%
\item{}If \(z \neq 0\), there is an element \(\frac{1}{z}\) in \(\C\) such that \(z\left(\frac{1}{z}\right) = 1\)%
\item{}\(u (w + z) = (u w) + (u z)\)%
\end{enumerate}
\end{activity}%
The result of \hyperref[x:activity:act_complex_field]{Activity~{\xreffont\ref{x:activity:act_complex_field}}} is that, just like \(\R\), the set \(\C\) is a field. If we wanted to, we could define vector spaces over \(\C\) just like we did over \(\R\). The same results hold.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Conjugates and Modulus}
\typeout{************************************************}
%
\begin{sectionptx}{Conjugates and Modulus}{}{Conjugates and Modulus}{}{}{x:section:sec_conj_modulus}
We can draw pictures of complex numbers in the plane. We let the \(x\)-axis be the real axis for a complex number and the \(y\)-axis the imaginary axis. That is, if \(z=a+bi\) we can think of \(z\) as a directed line segment from the origin to the point \((a,b)\), where the terminal point of the segment is \(a\) units from the imaginary axis and \(b\) units from the real axis. For example, the complex numbers \(3+4i\) and \(-8+3i\) are shown in \hyperref[x:figure:F_complex_numbers]{Figure~{\xreffont\ref{x:figure:F_complex_numbers}}}.%
\begin{figureptx}{Two complex numbers.}{x:figure:F_complex_numbers}{}%
\begin{image}{0.35}{0.3}{0.35}%
\includegraphics[width=\linewidth]{external/A_complex_numbers.pdf}
\end{image}%
\tcblower
\end{figureptx}%
We can also think of the complex number \(z = a+bi\) as the vector \([a \ b]^{\tr}\). In this way, the set \(\C\) is a two-dimensional vector space over \(\R\) with basis \(\{1, i\}\). Each of these complex numbers has a length that we call the \terminology{norm} or \terminology{modulus} of the complex number. We denote the norm of a complex number \(a+bi\) as \(|a+bi|\). The distance formula or the Pythagorean theorem show that%
\begin{equation*}
|a+bi| = \sqrt{a^2+b^2}\text{.}
\end{equation*}
%
\par
\index{complex conjugate} Note that%
\begin{equation*}
a^2+b^2 = a^2-b^2i^2 = (a+bi)(a-bi)
\end{equation*}
so the norm of the complex number \(a+bi\) can also be viewed as a square root of the product of \(a+bi\) with \(a-bi\). The number \(a-bi\) is called the \terminology{complex conjugate} of \(a+bi\). If we let \(z = a+bi\), we denote the complex conjugate of \(z\) as \(\overline{z}\). So \(\overline{a+bi} = a-bi\).%
\begin{activity}{}{g:activity:idp28871448}%
Let \(w = 2+3i\) and \(z = -1+5i\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}Find \(\overline{w}\) and \(\overline{z}\).%
\item{}Compute \(|w|\) and \(|z|\).%
\item{}Compute \(w\overline{w}\) and \(z \overline{z}\).%
\item{}Let \(z\) be an arbitrary complex number. There is a relationship between \(|z|\), \(z\), and \(\overline{z}\). Find and verify this relationship.%
\item{}What is \(\overline{z}\) if \(z \in \R\)?%
\end{enumerate}
\end{activity}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Complex Vectors}
\typeout{************************************************}
%
\begin{sectionptx}{Complex Vectors}{}{Complex Vectors}{}{}{x:section:sec_complex_vect}
A vector can have real and imaginary parts, too. For example, the vector \(\vv = \left[ \begin{array}{c} 1+i \\ 2-i \end{array}  \right]\) can be written as%
\begin{equation*}
\vv = \left[ \begin{array}{c} 1+i \\ 2-i \end{array}  \right] = \left[ \begin{array}{c} 1 \\ 2 \end{array}  \right] + \left[ \begin{array}{r} i \\ -i \end{array}  \right] = \left[ \begin{array}{c} 1 \\ 2 \end{array}  \right] + i\left[ \begin{array}{r} 1 \\ -1 \end{array}  \right]\text{.}
\end{equation*}
%
\par
The vector \(\text{ Re } (\vv) = \left[ \begin{array}{c} 1 \\ 2 \end{array}  \right]\) is the real part of \(\vv\) and the vector \(\text{ Im } (z) = \left[ \begin{array}{r} 1 \\ -1 \end{array}  \right]\) is the imaginary part of \(\vv\). In this way any vector \(\vv\) with complex entries can be written in the form%
\begin{equation*}
\vv = \vx + i \vy\text{,}
\end{equation*}
where \(\vx = \text{ Re } (z)\) and \(\vy = \text{ Im } (z)\) are vectors with real entries. The conjugate of the vector \(\vv = \vx + i \vy\) is the vector \(\overline{\vv} = \vx - i \vy\). We can do the same with matrices with complex entries.%
\par
There are several properties of complex conjugates that can be useful. Suppose \(r\) is a complex number, \(\vv\) is a vector with possibly complex entries, and \(A\) and \(B\) are matrices with possibly complex entries. Assume all products that are listed are defined. Then%
\begin{enumerate}
\item{}\(\displaystyle \overline{r\vv} = \overline{r} \ \overline{\vv}\)%
\item{}\(\displaystyle \overline{rA} = \overline{r} \ \overline{A}\)%
\item{}\(\displaystyle \overline{A\vv} = \overline{A} \ \overline{\vv}\)%
\item{}\(\displaystyle \overline{AB} = \overline{A} \ \overline{B}\)%
\end{enumerate}
%
\par
These properties can be verified using complex arithmetic. For example, to verify the first property, let \(r = a+ib\) be a complex number and let \(\vv = \vx + i \vy\) be a complex vector. The operations on complex numbers give us%
\begin{align*}
\overline{r\vv} \amp = \overline{(a\vx-b\vy) + i(a\vy + b\vx)}\\
\amp = (a\vx-b\vy) - i(a\vy + b\vx)\\
\amp = (a-ib)(\vx - i\vy)\\
\amp = \overline{r} \overline{\vv}\text{.}
\end{align*}
%
\par
Verification of the remaining properties is left to the reader.%
\end{sectionptx}
\end{appendixptx}
%
%
\typeout{************************************************}
\typeout{Appendix B Answers and Hints for Selected Exercises}
\typeout{************************************************}
%
\begin{solutions-chapter}{Answers and Hints for Selected Exercises}{}{Answers and Hints for Selected Exercises}{}{}{x:solutions:app_answers}
\par\medskip
\noindent\textbf{\Large{}I\space\textperiodcentered\space{}Systems of Linear Equations\\
1\space\textperiodcentered\space{}Introduction to Systems of Linear Equations\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp8544264}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp8557832-back}{}\quad{}Consider the equations%
\begin{equation*}
b_1x_1+b_2x_2 + b_3x_3 = s\text{,}
\end{equation*}
and%
\begin{equation*}
k(b_1x_1+b_2x_2 + b_3x_3) = ks\text{.}
\end{equation*}
%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp8551304-back}{}\quad{}Similar to part (a)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp8551944}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp8552328-back}{}\quad{}A cup of coffee costs \textdollar{}\(1.95\), a muffin costs \textdollar{}\(2.05\), and a scone costs \textdollar{}\(2.15\).%
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp8558600}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp8567816-back}{}\quad{}%
\begin{align*}
x_1 \amp = \frac{x_2+200+0+0}{4}\\
x_2 \amp = \frac{x_1+x_3+200+0}{4}\\
x_3 \amp = \frac{x_2+0+400+0}{4}
\end{align*}
%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp8570888-back}{}\quad{}\(x_1 = 75\), \(x_2 = 100\) and \(x_3 = 125\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp8575624}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp8575880-back}{}\quad{}%
\begin{alignat*}{4}
{}x_1   \amp {}+{}  \amp {}x_2  \amp {}+{}  \amp {}x_3  \amp = \amp \ 0\amp {}\\
{}x_1   \amp {}+{}  \amp {}x_2  \amp {}+{}  \amp {}x_3  \amp = \amp \ 1\amp {.}
\end{alignat*}
%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp8579208-back}{}\quad{}Impossible.%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp8580488-back}{}\quad{}Impossible.%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp8579336-back}{}\quad{}%
\begin{alignat*}{4}
{}x_1   \amp {}+{}  \amp {}x_2  \amp {}+{}  \amp {}x_3  \amp = \amp  \ 0\amp {}\\
{2}x_1   \amp {}+{}  \amp {2}x_2  \amp {}+{}  \amp {2}x_3  \amp = \amp  \ 0\amp {.}
\end{alignat*}
%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp8584712}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp8588296-back}{}\quad{}This system has the solution \(x_1 = \frac{h-10}{3h-5}\) and \(x_2 = \frac{5}{3h-5}\) as long as \(h \neq \frac{5}{3}\).%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp8589576-back}{}\quad{}When \(h = \frac{5}{3}\), \(x_1 = 2-\frac{5}{3}x_2\) and \(x_2\) is free.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{11}{True\slash{}False Questions.}{x:exercise:sec_intro_le_tf}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp8608904-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp8609544-back}{}\quad{}F%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp8615560-back}{}\quad{}F%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}2\space\textperiodcentered\space{}The Matrix Representation of a Linear System\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp9009288}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9010440-back}{}\quad{}\(h=6\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9011208-back}{}\quad{}\(h=6\) and \(k \neq -2\).%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9013896-back}{}\quad{}\(h=6\) and \(k=-2\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp9019528}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9020680-back}{}\quad{}%
\begin{alignat*}{5}
{}x_1   \amp {}+{}  \amp {}x_2  \amp {}{}    \amp {}    \amp {}={}  \amp \ {}\amp 1\amp {}\\
{-}x_1  \amp {}+{}  \amp {}x_2  \amp {}{}    \amp {}    \amp {}={}   \amp \ {-}\amp 3\amp {}\\
{}x_1    \amp {}+{}  \amp {}x_2  \amp {}+{}  \amp {}x_3  \amp {}={}  \amp  \ {}\amp 1\amp {.}
\end{alignat*}
%
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp9028232}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9028872-back}{}\quad{}Student 2's hunch is correct.%
\end{divisionsolution}%
\begin{divisionsolution}{6}{}{g:exercise:idp9029000}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9026056-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9027720-back}{}\quad{}T%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9037064-back}{}\quad{}T%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9039112-back}{}\quad{}F%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9039496-back}{}\quad{}F%
\item[(k)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9036680-back}{}\quad{}F%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}3\space\textperiodcentered\space{}Row Echelon Forms\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp9435912}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9443848-back}{}\quad{}Augmented matrix: \(\left[ \begin{array}{ccr|c} 1\amp 1\amp -1\amp 4 \\ 1\amp 2\amp 2\amp 3 \\ 2\amp 3\amp -3\amp 11 \end{array} \right]\), solution \(x_1 = 1\), \(x_2=2\), and \(x_3 = -1\)%
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp9448456}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9443080-back}{}\quad{}Row operations show this.%
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp9455368}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9456776-back}{}\quad{}\(x_1 = -4-6x_3\), \(x_2 = -3-4x_3\), \(x_3\) is free, \(x_4 = 3\)%
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp9464456}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp9461256-back}{}\quad{}In order to help see what happens in the general case, substitute some numbers in place of the \(\blacksquare\)'s and \textasteriskcentered{}'s and answer the question for that specific system first. Then determine if your answer generalizes.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9464584-back}{}\quad{}Yes%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9462152-back}{}\quad{}\(\left[ \begin{array}{cc|c} \blacksquare\amp *\amp * \\ 0\amp \blacksquare\amp * \end{array} \right]\), \(\left[ \begin{array}{cc|c} 0\amp \blacksquare\amp * \\ 0\amp 0\amp * \end{array} \right]\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp9461512}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9466504-back}{}\quad{}This is not possible.%
\end{divisionsolution}%
\begin{divisionsolution}{11}{}{g:exercise:idp9469832}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9468296-back}{}\quad{}\(\left[ \begin{array}{cc} \blacksquare\amp *\\0\amp * \end{array} \right]\) and \(\left[ \begin{array}{cc} 0\amp *\\0\amp 0 \end{array} \right]\).%
\end{divisionsolution}%
\begin{divisionsolution}{12}{}{g:exercise:idp9475336}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9469704-back}{}\quad{}T%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9474952-back}{}\quad{}T%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9483400-back}{}\quad{}F%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9476872-back}{}\quad{}T%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9482120-back}{}\quad{}F%
\item[(k)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9485704-back}{}\quad{}F%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}4\space\textperiodcentered\space{}Vector Representation\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{x:exercise:sec_vec_rep_exer}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9909768-back}{}\quad{}\(\vw = \left(-\frac{9}{4}\right) \vu + \left(\frac{7}{4}\right) \vv\)%
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp9918344}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9925256-back}{}\quad{}Only when \(w_1 = 2w_2 - 3w_3\). \(0 \vu + 0\vv = \vzero\)%
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{x:exercise:ex_1_d_acid}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9941512-back}{}\quad{}We cannot make the desired solution.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9942280-back}{}\quad{}We can make the desired chemical solution with \(\frac{3}{4}\) of solution \(\vv_1\) for every \(\frac{1}{4}\) of solution \(\vv_2\).%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9939464-back}{}\quad{}If \(\vw = \left[ \begin{array}{c} w_1\\w_2\\w_3 \end{array} \right]\) can be made from our original two chemical solutions, then \(w_3+\frac{3}{23}w_1-\frac{7}{23}w_2 = 0\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp9936008}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9946632-back}{}\quad{}\(\Span\{\vv_1\}\) is the line in \(\R^2\) through the origin and the point \((1,1)\).%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9948808-back}{}\quad{}\(\Span\{\vv_1,\vv_3\}\) is the plane in \(\R^3\) through the origin and the points \((1,1,1)\) and \((2,0,1)\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp9958792}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9951752-back}{}\quad{}Yes.%
\end{divisionsolution}%
\begin{divisionsolution}{11}{}{g:exercise:idp9964936}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9967496-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9970824-back}{}\quad{}T%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9974920-back}{}\quad{}T%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9977864-back}{}\quad{}F%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9988360-back}{}\quad{}T%
\item[(k)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9989256-back}{}\quad{}T%
\item[(m)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp9993096-back}{}\quad{}F%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}5\space\textperiodcentered\space{}The Matrix-Vector Form of a Linear System\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp10360968}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp10364168-back}{}\quad{}The matrix-vector form of the system is \(A \vx = \vb\), where \(\vx = \left[ \begin{array}{c} x_1\\x_2\\x_3\\x_4 \end{array} \right]\).%
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp10371208}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp10374792-back}{}\quad{}The system corresponding to this matrix-vector equation is%
\begin{alignat*}{4}
{2}x_1   \amp {}+{}   \amp {3}x_2   \amp {}+{}  \amp {4}x_3     \amp {}={}  \amp {}4\amp {}\\
{}x_1   \amp {}-{}   \amp {2}x_2  \amp {}+{}  \amp {3}x_3     \amp {}={}  \amp {-}6\amp {.}
\end{alignat*}
%
\par
The solution to the system is%
\begin{alignat*}{1}
x_1 \amp = - \frac{17}{7}x_3 -  \frac{10}{7}\\
x_2 \amp = \frac{2}{7}x_3 +  \frac{16}{7}\\
x_3 \amp \text{ is free } \text{.}
\end{alignat*}
%
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{x:exercise:ex_1_e_scalar_product}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp10389896-back}{}\quad{}Both methods give%
\begin{equation*}
\left[ \begin{array}{cc} a \amp  b\\ c\amp  d \end{array}  \right]\left[ \begin{array}{c} x_1 \\ x_2 \end{array}  \right] = \left[ \begin{array}{c} x_1a+x_2b \\ x_1c+x_2d \end{array}  \right]\text{.}
\end{equation*}
%
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp10389128}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp10391560-back}{}\quad{}\(b_3\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp10387848-back}{}\quad{}If \(b_3 = 9\), then \(a = 2\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp10398472}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp10398984-back}{}\quad{}\(A\left[ \begin{array}{r} -1 \\ 6 \\ -5 \end{array} \right] = \left[ \begin{array}{r} 3 \\ -5 \end{array} \right]\).%
\end{divisionsolution}%
\begin{divisionsolution}{11}{}{g:exercise:idp10406152}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp10408328-back}{}\quad{}The general solution to the system is%
\begin{equation*}
\left[ \begin{array}{c} x\\y\\z \end{array}  \right]  = y \left[ \begin{array}{c} 2\\1\\0 \end{array}  \right] + z \left[ \begin{array}{r} -1\\0\\1 \end{array}  \right] +  \left[ \begin{array}{c} 3\\0\\0 \end{array}  \right]\text{.}
\end{equation*}
%
\par
The solution is the plane in \(\R^3\) through the points \((3,0,0)\), \((5,1,0)\), and \((2,0,1)\).%
\end{divisionsolution}%
\begin{divisionsolution}{13}{}{g:exercise:idp10410120}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp10421512-back}{}\quad{}\(\vx = \left[ \begin{array}{r} 2\\-1\\-1 \end{array} \right]\)%
\end{divisionsolution}%
\begin{divisionsolution}{14}{}{g:exercise:idp10419080}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp10417800-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp10430856-back}{}\quad{}T%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp10433928-back}{}\quad{}T%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp10435848-back}{}\quad{}T%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp10441480-back}{}\quad{}T%
\item[(k)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp10441736-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}6\space\textperiodcentered\space{}Linear Dependence and Independence\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp10955544}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp10953752-back}{}\quad{}Linearly dependent. Change the last entry in \(\vv_3\) to a \(0\).%
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp10963224}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp10963480-back}{}\quad{}Only two of the solutions are necessary.%
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp10960536}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp10959768-back}{}\quad{}\(\{\vv_1, \vv_2, \vv_3\}\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp10968472-back}{}\quad{}\(\vu = \vv_1 - 2 \vv_2 + 3 \vv_3\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp10969880}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp10966552-back}{}\quad{}This is not possible.%
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp10980120}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp10980888-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp10989592-back}{}\quad{}T%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp10990872-back}{}\quad{}T%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp10992408-back}{}\quad{}T%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp10991256-back}{}\quad{}T%
\item[(k)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp11002520-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}7\space\textperiodcentered\space{}Matrix Transformations\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp11547784}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp11545736-back}{}\quad{}If \(\vx = \left[ \begin{array}{c} x_1\\x_2\\x_3 \end{array} \right]\), then \(T(\vx) = \left[ \begin{array}{c} x_1+2x_2+x_3 \\ x_1-3x_3 \end{array} \right]\).%
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp11547528}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp11555976-back}{}\quad{}\(\left[ \begin{array}{r} 12 \\ -11 \end{array} \right]\).%
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp11554440}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp11555848-back}{}\quad{}\(A = \left[ \begin{array}{cr} 2\amp -1\\1\amp 3 \end{array} \right]\)%
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp11561224}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp11563272-back}{}\quad{}\(T\) defined by \(T(\vx) = \left[ \begin{array}{ccc} 1\amp 0\amp 0 \\ 0\amp 1\amp 0 \\ 0\amp 0\amp 1 \\0\amp 0\amp 0 \end{array} \right]\vx\)%
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp11567624}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp11570056-back}{}\quad{}Not possible.%
\end{divisionsolution}%
\begin{divisionsolution}{11}{}{g:exercise:idp11572744}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp11312904-back}{}\quad{}\(L= \vv+c\vw\), \(T(L) = T(\vv+c\vw) = T(\vv) + cT(\vw)\)%
\end{divisionsolution}%
\begin{divisionsolution}{12}{}{g:exercise:idp11312136}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp11313800-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp11584024-back}{}\quad{}F%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp11586840-back}{}\quad{}T%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp11591448-back}{}\quad{}F%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp11596056-back}{}\quad{}F%
\item[(k)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp11598360-back}{}\quad{}F%
\item[(m)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp11603864-back}{}\quad{}T%
\item[(o)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp11608600-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}II\space\textperiodcentered\space{}Matrices\\
8\space\textperiodcentered\space{}Matrix Operations\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp12068616}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12068872-back}{}\quad{}\(AB = \left[ \begin{array}{cc} a\amp b \\ c\amp d \\ 0\amp 0 \end{array} \right]\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12064904-back}{}\quad{}\(AB = [-2 \ -2]\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp12077960}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12073480-back}{}\quad{}\(X = \left[ \begin{array}{cc} 0\amp 1\\0\amp 0 \end{array} \right]\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12073864-back}{}\quad{}There is no matrix \(X\) with this property.%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12085512-back}{}\quad{}\(X = \left[ \begin{array}{cc} 2x_{21}\amp 1+2x_{22}\\x_{21}\amp x_{22} \end{array} \right]\), where \(x_{21}\) and \(x_{22}\) can be any scalars%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp12087176}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12095240-back}{}\quad{}\(A^m \vv = 2^m \vv\)%
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp12093704}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12100616-back}{}\quad{}\(A^2=0\) and \(B^3=0\)%
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp12099848}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12112264-back}{}\quad{}\([a_{ij} + b_{ij}] = [b_{ij} + a_{ij}]\)%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12110216-back}{}\quad{}\([a_{ij} + 0] = [a_{ij}]\)%
\item[(e)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12112648-back}{}\quad{}\([(a+b)a_{ij}] = [aa_{ij}] + [ba_{ij}]\)%
\item[(g)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12116872-back}{}\quad{}\([(ab)a_{ij}] = a[ba_{ij}]\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{11}{}{g:exercise:idp11852168}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12197640-back}{}\quad{}Let \(A^{\tr} = [a'_{ij}]\). Then \(a'_{ij} = a_{ji}\) by definition of the transpose. Let \(\left(A^{\tr}\right)^{\tr} = [a''_{ij}]\). Then \(a''_{ij} = a'_{ji} = a_{ij}\). So the \(ij\)th entry of \(\left(A^{\tr}\right)^{\tr}\) is the same as the \(ij\)th entry of \(A\), and we conclude that \(\left(A^{\tr}\right)^{\tr} = A\).%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12198024-back}{}\quad{}The \(ij\)th entry of \(aA\) is \(aa_{ij}\), so the \(ij\)th entry of \((aA)^{\tr}\) is \(aa_{ji}\). But \(aa_{ji}\) is also the \(ij\)th entry of \(aA^{\tr}\). We conclude that \((aA)^{\tr} = aA^{\tr}\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{13}{}{g:exercise:idp12217224}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12224648-back}{}\quad{}If \(A=\left[ \begin{array}{cr} \cos(\alpha) \amp -\sin(\alpha) \\ \sin(\alpha) \amp \cos(\alpha) \end{array} \right]\), \(B = \left[ \begin{array}{cr} \cos(\beta) \amp -\sin(\beta) \\ \sin(\beta) \amp \cos(\beta) \end{array} \right]\), then \(AB\) equals \(\left[ \begin{array}{cr} \cos(\alpha+\beta) \amp -\sin(\alpha+\beta) \\ \sin(\alpha+\beta) \amp \cos(\alpha+\beta) \end{array} \right]\)%
\end{divisionsolution}%
\begin{divisionsolution}{14}{}{g:exercise:idp12222728}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12225544-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12232328-back}{}\quad{}F%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12232840-back}{}\quad{}F%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12236936-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}9\space\textperiodcentered\space{}Introduction to Eigenvalues and Eigenvectors\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp12515224}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12516376-back}{}\quad{}Eigenvector with eigenvalue \(-1\).%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12516120-back}{}\quad{}Eigenvector with eigenvalue \(3\).%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12527896-back}{}\quad{}Eigenvector with eigenvalue \(2\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp12530328}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12532120-back}{}\quad{}Eigenvalue%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12538264-back}{}\quad{}Eigenvalue%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12537496-back}{}\quad{}Not an eigenvalue%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12538520-back}{}\quad{}Not an eigenvalue%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{x:exercise:ex_2_b_Markov}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12547864-back}{}\quad{}\(a_3 = 804,000\) \(s_3= 196,000\), \(a_4 = 782,400\), \(s_4 = 217,600\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12552216-back}{}\quad{}\(a_{k+1}=0.9a_k+0.3s_k\), \(s_{k+1}=0.1a_k+0.7s_k\)%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12561944-back}{}\quad{}\(\left[ \begin{array}{c} a_{k+1} \\ s_{k+1} \end{array} \right]\) equals \(\left[ \begin{array}{cc} 0.9 \amp 0.3 \\ 0.1 \amp 0.7 \end{array} \right] \left[ \begin{array}{c} a_k \\ s_k \end{array} \right]\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{6}{}{g:exercise:idp12558104}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12563224-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12568216-back}{}\quad{}T%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12569112-back}{}\quad{}T%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12576792-back}{}\quad{}T%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12579224-back}{}\quad{}F%
\item[(k)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12588440-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}10\space\textperiodcentered\space{}The Inverse of a Matrix\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{x:exercise:ex_2_c_unique_inverse}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12886552-back}{}\quad{}\(C = (I)C = C = (BA)C = BAC = B(AC) = B(I) = B\)%
\end{divisionsolution}%
\begin{divisionsolution}{2}{}{x:exercise:ex_2_c_2by2_inverse}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp12886296-back}{}\quad{}You may need to consider different cases, e.g., when \(a=0\) and when \(a \neq 0\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp12899736}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12897176-back}{}\quad{}\(\left[ \begin{array}{cc} 1\amp k \\ 0\amp 1 \end{array} \right]^{-1} = \left[ \begin{array}{cr} 1\amp -k \\ 0\amp 1 \end{array} \right]\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12899352-back}{}\quad{}Row reduce \([A \ | \ I_2]\)%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12904856-back}{}\quad{}Row reduce \([A \ | \ I_3]\) to see that \(A^{-1} = \left[ \begin{array}{crc} 1\amp -k\amp mk-\ell \\ 0\amp 1\amp -m \\ 0\amp 0\amp 1 \end{array} \right]\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp12901528}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12906648-back}{}\quad{}\(c \neq -4\)%
\end{divisionsolution}%
\begin{divisionsolution}{8}{}{g:exercise:idp12914328}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12912536-back}{}\quad{}T%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12919064-back}{}\quad{}T%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12929304-back}{}\quad{}T%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12927128-back}{}\quad{}T%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp12925208-back}{}\quad{}F%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}11\space\textperiodcentered\space{}The Invertible Matrix Theorem\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp13212440}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13223448-back}{}\quad{}\(A\) is invertible as long as \(\left[ \begin{array}{c} a \\ b\\ c \end{array} \right]\) is not in the plane in \(\R^3\) through the origin and the points \((1,-1,1)\) and \((2,1,1)\)%
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp13231128}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13231384-back}{}\quad{}T%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13241240-back}{}\quad{}T%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13243288-back}{}\quad{}T%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13248920-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}III\space\textperiodcentered\space{}The Vector Space \(\R^n\)\\
12\space\textperiodcentered\space{}The Structure of \(\R^n\)\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp13589016}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13594520-back}{}\quad{}Closed under addition, not closed under multiplication by scalars, contains the zero vector.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13593752-back}{}\quad{}Not closed under addition, closed under multiplication by scalars, contains the zero vector.%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13589144-back}{}\quad{}Not closed under addition, not closed under multiplication by scalars, does not contain the zero vector.%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13601048-back}{}\quad{}Not closed under addition, closed under multiplication by scalars, contains the zero vector.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp13600152}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13597720-back}{}\quad{}The only such set is the empty set.%
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp13610520}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13606040-back}{}\quad{}\(\R^2\).%
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp13776008}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item[(i)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13774728-back}{}\quad{}As an example, let \(\vv = [2 \ 1]^{\tr}\) in \(\R^2\).%
\par
\(I_2\)%
\item[(ii)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13782408-back}{}\quad{}As an example, let \(\vv = [2 \ 1]^{\tr}\) in \(\R^2\).%
\par
\(2I_2\)%
\item[(iii)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13783816-back}{}\quad{}As an example, let \(\vv = [2 \ 1]^{\tr}\) in \(\R^2\).%
\par
\([3\ve_1 \ -\ve_2]\)%
\item[(iv)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13792136-back}{}\quad{}As an example, let \(\vv = [2 \ 1]^{\tr}\) in \(\R^2\).%
\par
\(\left[ \frac{a}{2}\ve_1 \ b\ve_2\right] [2 \ 1]^{\tr} = a\ve_1 + b\ve_2 = [a \ b]^{\tr}\)%
\end{enumerate}
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13786504-back}{}\quad{}\(\vw_1 + \vw_2 = A_1 \vv + A_2 \vv = (A_1+A_2) \vv\), \(c\vw_1 = c(A_1\vv) = A_1(c\vv)\), \(0\vv = \vzero\).%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp13791240-back}{}\quad{}There is more than one possibility.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13797384-back}{}\quad{}\(\{\vzero\}\), \(\R^m\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp13798024}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13804936-back}{}\quad{}\(W_1 \cap W_2\) is a subspace of \(\R^n\), \(W_1 \cup W_2\) is not in general a subspace of \(\R^n\)%
\end{divisionsolution}%
\begin{divisionsolution}{11}{}{g:exercise:idp13807496}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13803144-back}{}\quad{}Not in general a subspace of \(\R^n\).%
\end{divisionsolution}%
\begin{divisionsolution}{14}{}{g:exercise:idp13832712}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13830152-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13826824-back}{}\quad{}T%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13837832-back}{}\quad{}T%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13840648-back}{}\quad{}F%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13846280-back}{}\quad{}T%
\item[(k)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp13855496-back}{}\quad{}F%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}13\space\textperiodcentered\space{}The Null Space and Column Space of a Matrix\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp14228248}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp14230168-back}{}\quad{}A basis for \(\Col A\) is \(\left\{ \left[ \begin{array}{c} 1 \\ 0 \\ 1 \end{array} \right], \left[ \begin{array}{c} 3 \\ 2 \\ 5 \end{array} \right]\right\}\), a basis for \(\Nul A\) is \(\left\{\left[ \begin{array}{r} -2 \\ 1 \\ 0 \\ 0 \end{array} \right], \left[ \begin{array}{r} -7 \\ 0 \\ 1 \\ 1 \end{array} \right] \right\}\).%
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp14237976}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp14236056-back}{}\quad{}\(a=-3\) and \(b = 0\)%
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp14236568}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp14235544-back}{}\quad{}\(\left[ \begin{array}{ccc} 1\amp 0\amp 1\\0\amp 1\amp 1 \end{array} \right]\)%
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp14249368}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp14250648-back}{}\quad{}Not possible%
\end{divisionsolution}%
\begin{divisionsolution}{8}{}{g:exercise:idp14248856}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp14245912-back}{}\quad{}T%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp14244248-back}{}\quad{}T%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp14253464-back}{}\quad{}T%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp14265880-back}{}\quad{}F%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp14260632-back}{}\quad{}F%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}14\space\textperiodcentered\space{}Eigenspaces of a Matrix\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp14623448}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp14633304-back}{}\quad{}\(\left\{\left[ \begin{array}{r} -1 \\ 1 \end{array} \right]\right\}\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp14631000-back}{}\quad{}\(\left\{\left[ \begin{array}{r} -2 \\ 1 \end{array} \right]\right\}\)%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp14631128-back}{}\quad{}\(\left\{\left[ \begin{array}{r} -1 \\ 1 \end{array} \right]\right\}\)%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp14629848-back}{}\quad{}\(\left\{\left[ \begin{array}{c} 0 \\ 1 \\ 1 \end{array} \right]\right\}\)%
\item[(e)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp14632664-back}{}\quad{}\(\left\{\left[ \begin{array}{r} -1 \\ 2 \\ 1 \end{array} \right]\right\}\)%
\item[(f)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp14639960-back}{}\quad{}\(\left\{\left[ \begin{array}{r} -1 \\ 1 \\ 0 \end{array} \right], \left[ \begin{array}{r} -2 \\ 0 \\ 1 \end{array} \right]\right\}\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp14376152}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp14937736-back}{}\quad{}\(a=0\), \(b=4\)%
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp14937352}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp14938248-back}{}\quad{}Eigenvalue \(0\), eigenspace \(\R^2\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp14945160-back}{}\quad{}Eigenvalue \(0\), eigenspace \(\R^n\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{6}{}{g:exercise:idp14945672}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp14947336-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp14949256-back}{}\quad{}F%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp14952456-back}{}\quad{}T%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp14963336-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}15\space\textperiodcentered\space{}Bases and Dimension\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp14924936}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp14923400-back}{}\quad{}\(\left\{ \left[ \begin{array}{c} 1\\ 0 \\ 2\\ 1 \\ 3\\ 3 \end{array} \right], \left[ \begin{array}{c} 1\\ 0 \\ 0\\ 2 \\ 1\\ 3 \end{array} \right], \left[ \begin{array}{r} 0\\ 1 \\ 1\\ 1 \\ -1\\ 1 \end{array} \right] \right\}\), \(\dim(\Col A) = 3\), \(\dim(\Nul A) = 2\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp14926472-back}{}\quad{}\(\left\{ \left[ \begin{array}{r} -3\\1\\0\\0\\0 \end{array} \right], \left[ \begin{array}{r} 0\\0\\-2\\1\\0 \end{array} \right] \right\}\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp15290248}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15288456-back}{}\quad{}\(\left\{ \left[ \begin{array}{r} 1 \\ -2 \\ 1 \end{array} \right] \right\}\), \(\rank(A) = 1\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15290888-back}{}\quad{}\(\left\{ \left[ \begin{array}{r} -2\\1\\0\\0 \end{array} \right], \left[ \begin{array}{c} 1\\0\\1\\0 \end{array} \right], \left[ \begin{array}{c} 1\\0\\0\\1 \end{array} \right] \right\}\), \(\nullity(A) = 3\)%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15296008-back}{}\quad{}\(\rank(A) + \nullity(A) = 1+3 = 4\)%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15294088-back}{}\quad{}\(\left\{ \left[ \begin{array}{r} 1\\2\\-1\\-1 \end{array} \right] \right\}\), dimension \(1\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp15320200}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp15322376-back}{}\quad{}What are the solutions to \(A \vx = \vzero\)?%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp15320840-back}{}\quad{}Use part (a) and the Rank-Nullity Theorem.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp15327752}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15328008-back}{}\quad{}\(\dim(W)\) can be \(0\), \(1\), \(2\), \(3\), or \(4\) corresponding to \(\{\vzero\}\), a line through the origin in \(\R^4\), a plane containing the origin in \(\R^4\), a copy of \(\R^3\) through the origin in \(\R^4\), \(\R^4\)%
\end{divisionsolution}%
\begin{divisionsolution}{8}{}{g:exercise:idp15340168}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp15334280-back}{}\quad{}Dimension two leads to two linearly independent vectors in each of \(W_i\).%
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp15338120}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15340296-back}{}\quad{}\(\dim(\Col A) = 3\), \(\dim(\Nul A) = 2\)%
\end{divisionsolution}%
\begin{divisionsolution}{11}{}{g:exercise:idp15348744}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15348616-back}{}\quad{}Not possible.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15341192-back}{}\quad{}\(\left[ \begin{array}{cc} 0\amp 1\\0\amp 0 \end{array} \right]\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{13}{}{g:exercise:idp15350024}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15355528-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15365384-back}{}\quad{}F%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15360136-back}{}\quad{}T%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15363976-back}{}\quad{}T%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15369096-back}{}\quad{}F%
\item[(k)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15369352-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}16\space\textperiodcentered\space{}Coordinate Vectors and Change of Basis\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp15712024}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15723928-back}{}\quad{}\([\vb]_{\CB} = [-2 \ 3]^{\tr}\).%
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp15725976}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15730712-back}{}\quad{}\(\left\{ [2 \ 1]^{\tr}, [1 \ 1]^{\tr}\right\}\), \(\left\{ [1 \ 0]^{\tr}, [3 \ 3]^{\tr}\right\}\).%
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp15728920}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15739288-back}{}\quad{}\([1 \ 0 \ 2]^{\tr}\), \([2 \ 1 \ 1]^{\tr}\)%
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp15736472}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp15741848-back}{}\quad{}Row reduce an appropriate matrix.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15744408-back}{}\quad{}\([\vv_1]_{\CB} = [ -1 \ 0 \ 2]^{\tr}\), \([\vv_2]_{\CB} = [-1 \ 1 \ 2]^{\tr}\), \([\vv_3]_{\CB} = [1 \ -1 \ 1]^{\tr}\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp15754136}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15754264-back}{}\quad{}Write \(\vv\) and \(\vw\) in terms of the basis vectors.%
\end{divisionsolution}%
\begin{divisionsolution}{11}{}{g:exercise:idp15765784}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15774872-back}{}\quad{}Use the trigonometric identities \(\cos(\theta+ \pi/2) = -\sin(\theta)\) and \(\sin(\theta+ \pi/2) = \cos(\theta)\).%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15782168-back}{}\quad{}Find \(\vx\) and then row reduce an appropriate matrix.%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15779736-back}{}\quad{}\([\vy]_{\CB} = \left[ \begin{array}{c} \sqrt{3}-\frac{3}{2} \\ 1+\frac{3\sqrt{3}}{2} \end{array} \right]\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{13}{}{g:exercise:idp15784984}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15790616-back}{}\quad{}\([ -1 \ 1 \ -2]^{\tr}\), \([1 \ 0 \ 1]^{\tr}\), \([1 \ 0 \ 2]^{\tr}\)%
\end{divisionsolution}%
\begin{divisionsolution}{14}{}{g:exercise:idp15786136}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15797528-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15795352-back}{}\quad{}F%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15805208-back}{}\quad{}T%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15802008-back}{}\quad{}T%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15815704-back}{}\quad{}T%
\item[(k)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp15817112-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}IV\space\textperiodcentered\space{}Eigenvalues and Eigenvectors\\
17\space\textperiodcentered\space{}The Determinant\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp16321304}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp16322968-back}{}\quad{}\(\det(2A) = \\ (8a_{11})\left[(a_{22})(a_{33}) - (a_{23})(a_{32})\right] - (8a_{12})\left[(a_{21})(a_{33}) - (a_{23})(a_{31})\right] + (8a_{13})\left[(a_{21})(a_{32}) - (a_{22})(a_{31})\right] = 8 \det(A)\).%
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp16318360}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp16331928-back}{}\quad{}\(\det(A^2) = \det(AA) = \det(A) \det(A) = [\det(A)]^2\).%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp16324888-back}{}\quad{}\(\det(A^k) = \det(AA^{k-1}) = \det(A) [\det(A)]^{k-1} = [\det(A)]^k\).%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp16324504-back}{}\quad{}Yes.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp16331032}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp16334616-back}{}\quad{}\((\det(A))^2\)%
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp16333208}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp16339224-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp16348568-back}{}\quad{}F%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp16346776-back}{}\quad{}T%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp16353304-back}{}\quad{}F%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp16355864-back}{}\quad{}T%
\item[(k)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp16361240-back}{}\quad{}F%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}18\space\textperiodcentered\space{}The Characteristic Equation\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{x:exercise:ex_determinant_eigenvalues}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp16645784-back}{}\quad{}\(\det(B) = -16\), eigenvalues \(-2\) and \(8\), \(\det(B)\) is the product of the eigenvalues%
\item[(b)]\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item[(i)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp16652312-back}{}\quad{}Each \(\lambda_i\) is a root of the characteristic polynomial.%
\item[(ii)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp16652824-back}{}\quad{}Evaluate \(p(\lambda)\) at \(\lambda = 0\).%
\item[(iii)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp16664216-back}{}\quad{}Use the fact that \(p(\lambda) = \det(A - \lambda I_n)\).%
\end{enumerate}
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp16662040}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp16672408-back}{}\quad{}Explain why \((A - \lambda I_n)^{\tr} = A^{\tr} - \lambda I_n\).%
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp16675608}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp16668952-back}{}\quad{}Write \(I_n\) as \(B\lambda I_nB^{-1}\) and then expand \(\det\left(BAB^{-1} - \lambda I_n \right)\).%
\end{divisionsolution}%
\begin{divisionsolution}{6}{}{g:exercise:idp16684312}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp16684440-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp16676632-back}{}\quad{}T%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp16689688-back}{}\quad{}F%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp16697368-back}{}\quad{}T%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp16694552-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}19\space\textperiodcentered\space{}Diagonalization\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp17175432}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17169288-back}{}\quad{}Not diagonalizable.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17170184-back}{}\quad{}Diagonalizable by \(P = \left[ \begin{array}{ccc} 1\amp 2\amp 1\\1\amp 3\amp 3\\1\amp 3\amp 4 \end{array} \right]\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp17178760}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17177864-back}{}\quad{}\(A = \left[ \begin{array}{cc} 1\amp 2\\0\amp 2 \end{array} \right]\), \(P_1 = \left[ \begin{array}{cc} 1\amp 2 \\ 0\amp 1 \end{array} \right]\), we have \(D_1 = \left[ \begin{array}{cc} 1\amp 0 \\ 0\amp 2 \end{array} \right]\), \(P_2 = \left[ \begin{array}{cc} 2\amp 1 \\ 1\amp 0 \end{array} \right]\), \(D_2 = \left[ \begin{array}{cc} 2\amp 0 \\ 0\amp 1 \end{array} \right]\)%
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp17190408}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17191048-back}{}\quad{}Yes%
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp17194504}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17196808-back}{}\quad{}Eigenvalues, 1; eigenvectors : \(\Span \{[1 \ 0]^{\tr}\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17200008-back}{}\quad{}Diagonalizable%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{x:exercise:ex_trace_eigenvalues}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17212936-back}{}\quad{}The \(ii\) entry of \(RS\) is \(r_{i1}s_{1i} + r_{i2}s_{2i} + \cdots + r_{in}s_{ni}\). The \(jj\) entry of \(SR\) is \(s_{j1}r_{1j} + s_{j2}r_{2j} + \cdots + s_{jn}r_{nj}\). Sum as \(i\) and \(j\) go from \(1\) to \(n\).%
\item[(b)]\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item[(i)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17219592-back}{}\quad{}\(\trace(D) = \trace\left(P^{-1}(AP)\right) = \trace\left((AP)P^{-1}\right) = \trace(A)\)%
\item[(ii)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17227400-back}{}\quad{}\(\trace\left(A\right) = \trace(D) = \sum_{i=1}^n \lambda_i\)%
\end{enumerate}
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{11}{}{x:exercise:ex_4_c_matrix_exponential_examples}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17239944-back}{}\quad{}\(e^A = \left[ \begin{array}{cc} e\amp e-1\\0\amp 1 \end{array} \right]\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp17239560-back}{}\quad{}Explain why \(B\) is not diagonalizable.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17234184-back}{}\quad{}\(e^B = I_2 + B = \left[ \begin{array}{cr} 1\amp -1 \\ 0\amp 1 \end{array} \right]\).%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17235208-back}{}\quad{}\(e^{A+B} = \left[ \begin{array}{cc} e\amp 0 \\ 0\amp 1 \end{array} \right]\)%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17243144-back}{}\quad{}No%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{13}{}{g:exercise:idp17255304}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17261576-back}{}\quad{}%
\begin{align*}
e^A \amp = e^{PDP^{-1}}\\
\amp = \sum_{k \geq 0} \frac{1}{k!} \left(PDP^{-1}\right)^k\\
\amp =  \sum_{k \geq 0} \frac{1}{k!} PD^kP^{-1}\\
\amp =  P\left(\sum_{k \geq 0} \frac{1}{k!} D^k\right) P^{-1}\\
\amp = Pe^DP^{-1}\text{.}
\end{align*}
%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17257992-back}{}\quad{}%
\begin{align*}
\det\left(e^A\right) \amp = \det\left(Pe^DP^{-1}\right)\\
\amp =  \det\left(e^D\right)\\
\amp = e^{\lambda_1}e^{\lambda^2} \cdots e^{\lambda_n}\\
\amp = e^{\lambda_1+\lambda^2+ \cdots + \lambda_n}\\
\amp = e^{\trace(A)}\text{.}
\end{align*}
%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{15}{}{g:exercise:idp17011464}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17278616-back}{}\quad{}T%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17277464-back}{}\quad{}F%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17283480-back}{}\quad{}T%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17286168-back}{}\quad{}F%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17291288-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}20\space\textperiodcentered\space{}Approximating Eigenvalues and Eigenvectors\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp17704744}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17713960-back}{}\quad{}Eigenvalues: \(3\) and \(-1\); T\([1 \ -1]^{\tr}\) is an eigenvector for \(A\) with eigenvalue \(-1\) and \([1 \ 1]^{\tr}\) is an eigenvector for \(A\) with eigenvalue \(3\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17715368-back}{}\quad{}As \(k\) increases, the vectors \(A^k \vx_0\) are approaching the vector \([1 \ 1]^{\tr}\), which is a dominant eigenvector of \(A\).%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17718440-back}{}\quad{}The Rayleigh quotients \(r_k = \frac{\vx_{k+1}\cdot \vx_k}{\vx_k \cdot \vx_k}\) approach the dominant eigenvalue \(3\).%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17723176-back}{}\quad{}Apply the power method to \(B = (A - 0 I_2)^{-1} = A^{-1}\). As \(k\) increases, the vectors \(B^k \vx_0\) are approaching the vector \(\frac{1}{2}[1 \ -1]^{\tr}\), which is an eigenvector of \(A\). The Rayleigh quotients approach the other eigenvalue \(-1\) of \(A\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp17729448}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17730088-back}{}\quad{}\([1 \ 1]^{\tr}\) is an eigenvector for \(A\) with eigenvalue \(1\), so the vectors \(A^k \vx_0\) are all equal to \(\vx_0\). We can adjust the seed to a non-eigenvector.%
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp17732776}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17749672-back}{}\quad{}\(8\) is the dominant eigenvalue of \(A\)%
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp17754536}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17755816-back}{}\quad{}Since \(\R^n\) has dimension \(n\), it follows that any set of \(n+1\) vectors is linearly dependent.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp17762216-back}{}\quad{}Proceed down the list \(c_{n-1}\), \(c_{n-2}\), etc., until you reach a weight that is non-zero.%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17773096-back}{}\quad{}\(\vzero = q(A) \vv = (A-\lambda I_n) Q(A)\vv\)%
\item[(d)]\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item[(i)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17774376-back}{}\quad{}\(q(t) = 24t - 10t^2+t^3\)%
\item[(ii)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17781288-back}{}\quad{}\(0\), \(4\), and \(6\)%
\item[(iii)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17778216-back}{}\quad{}For \(t=0\), we have \(Q(t) = (4-t)t-4)(t-6)\), and \([6 \ -6 \ 0]^{\tr}\) an eigenvector for \(A\) with eigenvalue \(0\). For \(t=4\) we have \(Q(t) = t(t-6)\), and \([24 \ 24 \ 0]^{\tr}\) is an eigenvector for \(A\) with eigenvalue \(4\). For \(t=6\) we have \(Q(t) = t(t-4)\), and \([0 \ -6 \ -12]^{\tr}\) is an eigenvector for \(A\) with eigenvalue \(6\).%
\end{enumerate}
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp17796520}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17805352-back}{}\quad{}If \(\beta\) is an eigenvalue of \(B\) with eigenvector \(\vx\), then \(\frac{1}{\beta} + \alpha\) is an eigenvalue of \(A\) with eigenvector \(\vx\).%
\end{divisionsolution}%
\begin{divisionsolution}{10}{}{g:exercise:idp17800232}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17801256-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp17811240-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}21\space\textperiodcentered\space{}Complex Eigenvalues\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp18074584}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp18075736-back}{}\quad{}Eigenvalues:\(\lambda_1 = 2+2\sqrt{2}i\) and \(\lambda_2 = 2-2\sqrt{2}i\); Eigenvectors: \([-\sqrt{2}i \ 1]^{\tr}\) and \([\sqrt{2}i \ 1]^{\tr}\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp18076248-back}{}\quad{}Eigenvalues: \(\lambda_1 = 2+i\) and \(\lambda_2 = 2-i\); Eigenvectors: \([-(1+i) \ 1]^{\tr}\), \([-(1-i) \ 1]^{\tr}\)%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp18082392-back}{}\quad{}Eigenvalues: \(\lambda_1 = -1+2i\) and \(\lambda_2 = -1-2i\); Eigenvectors: \([(1+i) \ 2]^{\tr}\) and \([(1-i) \ 2]^{\tr}\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp18084184}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp18081880-back}{}\quad{}Just the rotation matrices%
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp18084568}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp18091352-back}{}\quad{}\(\left[ \begin{array}{cr} 1\amp -2 \\ 2\amp 1 \end{array} \right]\)%
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp18093784}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp18104920-back}{}\quad{}Characteristic polynomial \(\lambda^2+a_1\lambda + a_0\); \(\left[ \begin{array}{cr} 0\amp -2\\1\amp 2 \end{array} \right]\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp18103384-back}{}\quad{}\(\det(C - \lambda I_3) = -\lambda^3 -a_2 \lambda^2 - a_1 \lambda - a_0\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{8}{}{g:exercise:idp18105944}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp18110424-back}{}\quad{}T%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp18109784-back}{}\quad{}F%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp18118872-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}22\space\textperiodcentered\space{}Properties of Determinants\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{x:exercise:ex_4_f_det_multiple}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp18728872-back}{}\quad{}\(\det(rA) = r^{n} \det(A)\)%
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp18736552}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp18742952-back}{}\quad{}\(A_1= \left[ \begin{array}{rrrr} 1\amp 1\amp 1\amp 1 \\ -1\amp 4\amp -1\amp -1\\ -1\amp -1\amp 4\amp -1\\ -1\amp -1\amp -1\amp 4 \end{array} \right]\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp18744360-back}{}\quad{}\(B= \left[ \begin{array}{cccc} 1\amp 1\amp 1\amp 1 \\ 0\amp 5\amp 0\amp 0\\ 0\amp 0\amp 5\amp 0\\ 0\amp 0\amp 0\amp 5 \end{array} \right]\)%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp18742440-back}{}\quad{}\(125 = \det(B) = \det(A_1) = \det(A)\)%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp18751912-back}{}\quad{}\(\det(A) = (n+1)^{n-1}\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp18745768}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp18746536-back}{}\quad{}%
\begin{align*}
A^{-1} \amp = \frac{1}{\det(A)} \adj(A)\\
\amp = \left[ \begin{array}{rcr} -1\amp 0\amp 1 \\ 0\amp 1\amp 0 \\ 2\amp 0\amp -1 \end{array} \right]
\end{align*}
%
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp18755752}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp18754600-back}{}\quad{}6%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp18754984-back}{}\quad{}\(-16\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp18758696}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp18762920-back}{}\quad{}\(|c-7|\), volume is \(0\) when the parallelelepiped is two-dimensional.%
\end{divisionsolution}%
\begin{divisionsolution}{12}{}{g:exercise:idp18771624}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp18781352-back}{}\quad{}Continue row reducing.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{13}{}{g:exercise:idp18778536}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp18780456-back}{}\quad{}T%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp18793768-back}{}\quad{}T%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp18790952-back}{}\quad{}F%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp18795944-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}V\space\textperiodcentered\space{}Orthogonality\\
23\space\textperiodcentered\space{}The Dot Product in \(\R^n\)\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp19075080}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19088392-back}{}\quad{}The angle between \(\vu\) and \(\vv\) is \(\frac{\pi}{2}\). The distance between \(\vu\) and \(\vv\) is \(\sqrt{10}\). The orthogonal projection of \(\vu\) onto \(\vv\) is \(\vzero\).%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19095560-back}{}\quad{}The angle between \(\vu\) and \(\vv\) is \(0\). The distance between \(\vu\) and \(\vv\) is \(\sqrt{2}\). \(\proj_{\vv} \vu = \vu\).%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19096328-back}{}\quad{}The angle between \(\vu\) and \(\vv\) is approximately \(104.96^{\circ}\). The distance between \(\vu\) and \(\vv\) is \(\sqrt{17}\). \(\proj_{\vv} \vu = \frac{\vu \cdot \vv}{||\vv||^2} \vv = -\frac{1}{10}[1 \ 3]^{\tr}\).%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19104264-back}{}\quad{}The angle between \(\vu\) and \(\vv\) is \(\frac{\pi}{2}\). The orthogonal projection of \(\vu\) onto \(\vv\) is \(\vzero\). The distance between \(\vu\) and \(\vv\) is \(\sqrt{11}\).%
\item[(e)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19102344-back}{}\quad{}The angle between \(\vu\) and \(\vv\) is approximately \(54.74^{\circ}\). The distance between \(\vu\) and \(\vv\) is \(\sqrt{2}\). \(\proj_{\vv} \vu = \frac{\vu \cdot \vv}{||\vv||^2} \vv = \frac{1}{3}[1 \ 1 \ 1]^{\tr}\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp19105288}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19108360-back}{}\quad{}\(h = 6 \pm 4\sqrt{3}\)%
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp19115912}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19124872-back}{}\quad{}Write the dot product as a matrix-vector product.%
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{x:exercise:ex_Pyth_Thm}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19132424-back}{}\quad{}Rewrite \(||\vu+\vv||^2\) using the dot product.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19138568-back}{}\quad{}From part (a), what can we say about \(2(\vu \cdot \vv)\)?%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp19147784}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19161224-back}{}\quad{}Consider cases of \(\vu \cdot \vv \leq 0\) separately. Then write the norm as a dot product.%
\end{divisionsolution}%
\begin{divisionsolution}{11}{}{g:exercise:idp19160968}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19158664-back}{}\quad{}Use the definition of \(W^\perp\).%
\end{divisionsolution}%
\begin{divisionsolution}{13}{}{g:exercise:idp19166344}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19168264-back}{}\quad{}If \(\vw \in W_2^\perp\) and \(\vv \in W_1\), in what other set is \(\vv\)?%
\end{divisionsolution}%
\begin{divisionsolution}{15}{}{g:exercise:idp19164936}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19172616-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19198888-back}{}\quad{}F%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19197864-back}{}\quad{}F%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19204904-back}{}\quad{}F%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19219624-back}{}\quad{}T%
\item[(k)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19216808-back}{}\quad{}T%
\item[(m)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19221544-back}{}\quad{}F%
\item[(o)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19227944-back}{}\quad{}T%
\item[(q)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19231272-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}24\space\textperiodcentered\space{}Orthogonal and Orthonormal Bases in \(\R^n\)\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp19491992}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19501336-back}{}\quad{}\(\left\{ [0 \ 1 \ 0]^{\tr}, \left[ \frac{3}{4} \ 0 \ 1\right]^{\tr} \right\}\)%
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{x:exercise:problem_orthog_decomp}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19511064-back}{}\quad{}Where are \(\vw_1\), \(\vw_2\), \(\ldots\), \(\vw_k\)?%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19507096-back}{}\quad{}Take the dot product of \(\vz\) with \(\vw_i\).%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19517336-back}{}\quad{}Use parts (a) and (b).%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19522584-back}{}\quad{}Collect terms in \(W\) and in \(W^{\perp}\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp19526680}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19524760-back}{}\quad{}Consider \((P \ve_i) \cdot (P \ve_j)\) where \(\ve_t\) is the \(t\)th standard basis vector for \(\R^n\).%
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp19555096}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19559320-back}{}\quad{}Think polar coordinates.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19561752-back}{}\quad{}What properties do the columns of an orthogonal matrix have?%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19563672-back}{}\quad{}What happens if \(\alpha = \theta+\frac{\pi}{2}\) and if \(\alpha = \theta-\frac{\pi}{2}\)?%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp19568408}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19571992-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19578520-back}{}\quad{}T%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19574680-back}{}\quad{}F%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19585688-back}{}\quad{}T%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19594904-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}25\space\textperiodcentered\space{}Projections onto Subspaces and the Gram-Schmidt Process in \(\R^n\)\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp19956104}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19962120-back}{}\quad{}\(\proj_W \vv = \frac{1}{2} [1 \ 0 \ 1]^{\tr}\), \(\proj_{\perp W} \vv = \frac{1}{2}[1 \ 0 \ -1]^{\tr}\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19960840-back}{}\quad{}\(\frac{1}{2} [1 \ 0 \ 1]^{\tr}\), \(\sqrt{\frac{1}{2}}\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp19961992}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19981320-back}{}\quad{}\(y = \frac{3}{2}x + \frac{2}{3}\).%
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp19984008}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19987592-back}{}\quad{}The Gram-Schmidt process will return an orthogonal set with \(\dim(\Span \ S)\) non-zero vectors and the rest all zero vectors.%
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp19983112}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19988616-back}{}\quad{}\(I_3 A\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19995144-back}{}\quad{}\(Q = \frac{1}{2}\left[ \begin{array}{cr} 1 \amp -1 \\ 1 \amp 1 \\ 1 \amp -1 \\ 1 \amp 1 \end{array} \right]\), \(R = \left[ \begin{array}{cc} 2 \amp 5 \\ 0 \amp 1  \end{array} \right]\)%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19990024-back}{}\quad{}\(Q = \left[ \begin{array}{ccc} 0 \amp 1 \amp 0 \\ 0 \amp 0 \amp 0 \\ 1 \amp 0 \amp 0 \\ 0 \amp 0 \amp 1 \end{array} \right]\), \(R =  \left[ \begin{array}{ccc} 3 \amp 0 \amp 4 \\ 0 \amp 1 \amp 0 \\ 0 \amp 0 \amp 5  \end{array} \right]\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp19996936}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp20006152-back}{}\quad{}\(\va_i \cdot \vu_j = r_{ji}\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp20004744-back}{}\quad{}See the discussion after \hyperref[x:activity:act_6_e_QR]{Activity~{\xreffont\ref{x:activity:act_6_e_QR}}}.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{11}{}{g:exercise:idp20012680}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp20016136-back}{}\quad{}What can we say about \(\vx\) if \(R \vx = \vzero\)?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp20015624-back}{}\quad{}Note that if \(R \vx = \vzero\), then \(A \vx = QR \vx = \vzero\). Use the fact that the columns of \(A\) are linearly independent.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19759496-back}{}\quad{}Let \(A = [\va_1 \ \va_2 \ \ldots \ \va_n] = [a_{ij}]\) be an \(n \times n\) orthogonal upper triangular matrices with positive diagonal entries. What does that tell us about \(\va_i \cdot \va_j\)?%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp19757960-back}{}\quad{}First, \(\va_i^{\tr}\va_j = \va_i \cdot \va_j = 0\) when \(i \neq j\) and \(\va_i \cdot \va_i = 1\). Use the fact that \(A\) is upper triangular to show that \(\va_i = \ve_i\).%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp19758600-back}{}\quad{}Write \(Q_1^{\tr}Q_1\) in terms of \(Q_2\) and \(S\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp20336264-back}{}\quad{}Use the fact that \(Q_1^{\tr}Q_1 = I_n = Q_2^{\tr}Q_2\) to show that \(I_n =  S^{\tr}S\)%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp20336136-back}{}\quad{}Use the previous parts of this problem along with the results of \hyperlink{x:exercise:ex_6_e_upper_triangular_props}{Exercise~{\xreffont 10}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp20340360-back}{}\quad{}Follow the hint.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{12}{}{g:exercise:idp20336776}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp20343944-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp20357000-back}{}\quad{}T%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp20353032-back}{}\quad{}T%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp20365064-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}26\space\textperiodcentered\space{}Least Squares Approximations\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp20596744}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp20615704-back}{}\quad{}\(f(x) \approx 35.5273 - 0.0527x\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp20627608-back}{}\quad{}Approximately \(32.10\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp20635416}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp20660632-back}{}\quad{}\(f(x) \approx 25.124 + 3.297x\)%
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{x:exercise:ex_6_f_not_li}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp20655384-back}{}\quad{}\(\left[ \begin{array}{cc} 1\amp 2 \\ 1\amp 2 \\ 1\amp 2 \end{array} \right] \left[ \begin{array}{c} a_0 \\ a_1 \end{array} \right] = \left[ \begin{array}{c} 1\\2\\3 \end{array} \right]\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp20669336-back}{}\quad{}\(A^{\tr}A\) is not invertible%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp20668312-back}{}\quad{}Infinitely many.%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp20662808-back}{}\quad{}\(x=2\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{6}{}{x:exercise:ex_6_f_ranks}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp20676504-back}{}\quad{}For part, see Exercise 12 in Section 15.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp20677656}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp20672536-back}{}\quad{}The augmented column of \([M^{\tr}M \ | \ M^{\tr}\vy]\) cannot be a pivot column.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp20679064-back}{}\quad{}See \hyperlink{x:exercise:ex_6_f_ranks}{Exercise~{\xreffont 6}}.%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp20685848-back}{}\quad{}Use the definition of the matrix product.%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp20683800-back}{}\quad{}See \hyperlink{x:exercise:ex_6_f_ranks}{Exercise~{\xreffont 6}}.%
\item[(e)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp20678552-back}{}\quad{}Combine parts (b) and (d).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp20710552}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp20710424-back}{}\quad{}T%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp20706072-back}{}\quad{}F%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp20712088-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}VI\space\textperiodcentered\space{}Applications of Orthogonality\\
27\space\textperiodcentered\space{}Orthogonal Diagonalization\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp21075352}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21076376-back}{}\quad{}\(\frac{1}{\sqrt{5}} \left[ \begin{array}{rc} -2\amp 2\\1\amp 1 \end{array} \right]\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21071768-back}{}\quad{}\(\left[ \begin{array}{crr} \frac{1}{\sqrt{3}}\amp -\frac{2}{\sqrt{6}}\amp 0 \\ \frac{1}{\sqrt{3}}\amp \frac{1}{\sqrt{6}}\amp -\frac{1}{\sqrt{2}}\\ \frac{1}{\sqrt{3}}\amp \frac{1}{\sqrt{6}}\amp \frac{1}{\sqrt{2}} \end{array} \right]\)%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21078040-back}{}\quad{}\(A\) is not orthogonally diagonalizable.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp21081880}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21087384-back}{}\quad{}\(\frac{1}{2} \left[ \begin{array}{cccc} 9\amp 3\amp 0\amp 0 \\ 3\amp 9\amp 0\amp 0 \\ 0\amp 0\amp 4\amp 0 \\ 0\amp 0\amp 0\amp 4 \end{array} \right]\)%
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{x:exercise:ex_7_a_spectral_decomposition}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21097624-back}{}\quad{}Determine \(P_i^{\tr}\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21098136-back}{}\quad{}Show that every column of \(P_i\) is a scalar multiple of \(\vu_i\).%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21098648-back}{}\quad{}Use the orthonormal basis to simplify \(P_i^2\).%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21101848-back}{}\quad{}Use the orthonormal basis to simplify \(P_iP_j\).%
\item[(e)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21105432-back}{}\quad{}Use the orthonormal basis to simplify \(P_i\vu_i\).%
\item[(f)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21106712-back}{}\quad{}Use the orthonormal basis to simplify \(P_i\vu_j\).%
\item[(g)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21106968-back}{}\quad{}Recall that \(\proj_{W_i} \vv = \frac{\vv \cdot \vu_i}{\vu_i \cdot \vu_i} \vu_i\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{6}{}{g:exercise:idp21110552}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21119128-back}{}\quad{}Try \(\vx = \textbf{e}_i\) and \(\vy = \textbf{e}_j\).%
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp21117464}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21130776-back}{}\quad{}A basis for the eigenspace of \(A\) corresponding to the eigenvalue \(-1\) is \(\{[0 \ 0 \ -2 \ 1]^{\tr}, [-2 \ 1 \ 0 \ 0]^{\tr}\}\) and a basis for the eigenspace of \(A\) corresponding to the eigenvalue 4 is \(\{[0 \ 0 \ 1 \ 2]^{\tr}, [1 \ 2 \ 0 \ 0]^{\tr}\}\). \(P_1 = \frac{1}{5} \left[ \begin{array}{ccrr} 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 4\amp -2 \\ 0\amp 0\amp -2\amp 1 \end{array} \right]\), \(P_2 = \frac{1}{5} \left[ \begin{array}{rrcc} 4\amp -2\amp 0\amp 0 \\ -2\amp 1\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0 \end{array} \right]\), \(P_3 = \frac{1}{5} \left[ \begin{array}{cccc} 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 1\amp 2 \\ 0\amp 0\amp 2\amp 41 \end{array} \right]\), \(P_4 = \frac{1}{5} \left[ \begin{array}{cccc} 1\amp 2\amp 0\amp 0 \\ 2\amp 4\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0 \\ 0\amp 0\amp 0\amp 0 \end{array} \right]\), \(\mu_1 = -1\), \(\mu_2 = 4\), \(Q_1 = \frac{1}{5} \left[ \begin{array}{rrrr} 4\amp -2\amp 0\amp 0 \\ -2\amp 1\amp 0\amp 0 \\ 0\amp 0\amp 4\amp -2 \\ 0\amp 0\amp -2\amp 1 \end{array} \right]\), and \(Q_2 = \frac{1}{5} \left[ \begin{array}{cccc} 1\amp 2\amp 0\amp 0 \\ 2\amp 4\amp 0\amp 0 \\ 0\amp 0\amp 1\amp 2 \\ 0\amp 0\amp 2\amp 4 \end{array} \right]\).%
\item[(b)]\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item[(i)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21139352-back}{}\quad{}Collect matrices with the same eigenvalues.%
\item[(ii)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21140504-back}{}\quad{}Use the fact that each \(P_i\) is a symmetric matrix.%
\item[(iii)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21138456-back}{}\quad{}Use Theorem 31.8.%
\item[(iv)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21142552-back}{}\quad{}Use Theorem 31.8.%
\item[(v)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21149592-back}{}\quad{}Explain why \(\{\vu_{1_j}\), \(\vu_{2_j}\), \(\ldots\), \(\vu_{m_j}\}\) is a orthonormal basis for \(E_{\mu_j}\).%
\end{enumerate}
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21145368-back}{}\quad{}The rank of \(Q_j\) is \(m_j\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{8}{}{g:exercise:idp21151000}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21146776-back}{}\quad{}T%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21158040-back}{}\quad{}F%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21157144-back}{}\quad{}T%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21166744-back}{}\quad{}T%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21164568-back}{}\quad{}T%
\item[(k)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21170072-back}{}\quad{}F%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}28\space\textperiodcentered\space{}Quadratic Forms and the Principal Axis Theorem\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp21502616}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21500184-back}{}\quad{}\(\left[ \begin{array}{rr} 1\amp -1\\-1\amp 4 \end{array} \right]\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21513112-back}{}\quad{}\(\left[ \begin{array}{ccc} 10\amp 0\amp 2\\0\amp 0\amp 1 \\ 2\amp 1\amp 1 \end{array} \right]\)%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21507608-back}{}\quad{}\(\left[ \begin{array}{rccr} 0\amp 1\amp 1\amp -\frac{1}{2}\\1\amp 5\amp 0\amp 0 \\ 1\amp 0\amp 0\amp 2 \\ -\frac{1}{2}\amp 0\amp 2\amp 8 \end{array} \right]\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{x:exercise:ex_7_b_constrained_optimization}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item[(i)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21525784-back}{}\quad{}Let \(P\) be a matrix that orthogonally diagonalizes \(A\), with \(P^{\tr}AP = D\). Use this to calculate \(Q(\vx)\).%
\item[(ii)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21524248-back}{}\quad{}Substitute in part i.%
\item[(iii)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21526424-back}{}\quad{}Make an argument similar to part ii.%
\end{enumerate}
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21528984-back}{}\quad{}The maximum value of \(Q(\vx)\) on the unit circle is 1 and it occurs at the input \(\frac{1}{\sqrt{2}}[1 \ -1]\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{x:exercise:ex_7_b_QF_characterization}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21540888-back}{}\quad{}\(1 = \lambda_1 y_1^2 + \lambda_2 y_2^2\).%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21551384-back}{}\quad{}An ellipse.%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21550616-back}{}\quad{}A hyperbola.%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21546520-back}{}\quad{}Two lines.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{x:exercise:ex_7_b_unique_A}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21570456-back}{}\quad{}Use the previous exercise to compare \(Q_A(\ve_i)\) and \(Q_B(\ve_i)\), then compare \(Q_A(\ve_i+\ve_j)\) to \(Q_B(\ve_i + \ve_j)\) for \(i \neq j\).%
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{x:exercise:ex_7_b_dot_product}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21585944-back}{}\quad{}\(I_n\)%
\end{divisionsolution}%
\begin{divisionsolution}{11}{}{x:exercise:ex_7_b_Pyth_Thm}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21601432-back}{}\quad{}Expand \(||\vu+\vv||^2\) using the dot product.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21601816-back}{}\quad{}Expand \(||\vu+\vv||^2\) using the dot product.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{13}{}{g:exercise:idp21619352}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp21624088-back}{}\quad{}Expand \(||\vu+\vv||^2\) using the dot product.%
\end{divisionsolution}%
\begin{divisionsolution}{14}{}{g:exercise:idp21625368}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21621912-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21620248-back}{}\quad{}T%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21636504-back}{}\quad{}T%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21629336-back}{}\quad{}F%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21640472-back}{}\quad{}T%
\item[(k)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21637144-back}{}\quad{}T%
\item[(m)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21652376-back}{}\quad{}T%
\item[(o)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp21648280-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}29\space\textperiodcentered\space{}The Singular Value Decomposition\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp22237592}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp22236056-back}{}\quad{}\(U = \left[ \begin{array}{cc} 1\amp 0\\0\amp 1 \end{array} \right]\), \(\Sigma = \left[ \begin{array}{cc} \sqrt{2}\amp 0\\0\amp 0 \end{array} \right]\), \(V = \frac{1}{\sqrt{2}}\left[ \begin{array}{cr} 1\amp -1\\1\amp 1 \end{array} \right]\).%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp22249496-back}{}\quad{}\(U = \frac{1}{\sqrt{2}}\left[ \begin{array}{c} 1\\0\\1 \end{array} \right]\), \(\Sigma = [\sqrt{2}]\), \(V = [1]\).%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp22247832-back}{}\quad{}\(U = \frac{1}{\sqrt{2}}\left[ \begin{array}{cr} 1\amp -1\\1\amp 1 \end{array} \right]\), \(\Sigma = \left[ \begin{array}{ccc} \sqrt{3}\amp 0\amp 0\\0\amp 1\amp 0 \end{array} \right]\), \(V = \left[ \begin{array}{crr} \frac{2}{\sqrt{6}}\amp 0\amp -\frac{1}{\sqrt{3}}\\ \frac{1}{\sqrt{6}}\amp -\frac{1}{\sqrt{2}}\amp \frac{1}{\sqrt{3}} \\ \frac{1}{\sqrt{6}}\amp \frac{1}{\sqrt{2}} \amp \frac{1}{\sqrt{3}} \end{array} \right]\).%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp22248344-back}{}\quad{}\(U = \left[ \begin{array}{crrr} \frac{1}{\sqrt{10}}\amp \frac{1}{\sqrt{10}}\amp \frac{2}{\sqrt{5}}\amp 0 \\ \frac{1}{\sqrt{10}}\amp -\frac{1}{\sqrt{10}}\amp 0\amp -\frac{2}{\sqrt{5}} \\ \frac{2}{\sqrt{10}}\amp -\frac{2}{\sqrt{10}}\amp 0\amp \frac{1}{\sqrt{5}} \\ \frac{2}{\sqrt{10}}\amp \frac{2}{\sqrt{10}}\amp -\frac{2}{\sqrt{5}}\amp 0 \end{array} \right]\), \(\Sigma = \left[ \begin{array}{cc} 5\amp 0 \\ 0\amp \sqrt{5} \\ 0\amp 0\\0\amp 0 \end{array} \right]\), \(V= \frac{1}{\sqrt{2}} \left[ \begin{array}{cr} 1\amp -1 \\ 1\amp 1 \end{array} \right]\).%
\item[(e)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp22252696-back}{}\quad{}\(U = \left[ \begin{array}{ccr} 0\amp 1\amp 0 \\ \frac{1}{\sqrt{2}}\amp 0\amp -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}\amp 0\amp \frac{1}{\sqrt{2}} \end{array} \right]\), \(\Sigma = \left[ \begin{array}{cccc} 3\amp 0\amp 0\amp 0 \\ 0\amp 2\amp 0\amp 0 \\ 0\amp 0\amp 1\amp 0 \end{array} \right]\), \(V = \left[ \begin{array}{ccrc} 0\amp 1\amp 0\amp 0 \\ \frac{1}{\sqrt{2}}\amp 0\amp -\frac{1}{\sqrt{2}}\amp 0 \\ \frac{1}{\sqrt{2}}\amp 0\amp \frac{1}{\sqrt{2}}\amp 0 \\ 0\amp 0\amp 0\amp 1 \end{array} \right]\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{2}{}{g:exercise:idp22256792}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp22266520-back}{}\quad{}Use the singular value decomposition \(U \Sigma V^{\tr}\) for \(A\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp22266648}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp22259864-back}{}\quad{}\(\sqrt{28}\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp22263320-back}{}\quad{}\(U = \left[ \begin{array}{crr} \frac{1}{\sqrt{14}}\amp \frac{2}{\sqrt{5}}\amp \frac{3}{\sqrt{70}} \\ \frac{2}{\sqrt{14}}\amp -\frac{1}{\sqrt{5}}\amp \frac{6}{\sqrt{70}} \\ \frac{3}{\sqrt{14}}\amp 0\amp -\frac{5}{\sqrt{70}} \end{array} \right]\), \(\Sigma = \left[ \begin{array}{cc} \sqrt{28}\amp 0 \\ 0\amp 0 \\ 0\amp 0 \end{array} \right]\), \(V = \frac{1}{\sqrt{2}}\left[ \begin{array}{cr} 1\amp -1 \\ 1\amp 1 \end{array} \right]\).%
\item[(c)]\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item[(i)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp22267672-back}{}\quad{}\(\left\{\frac{1}{\sqrt{2}}[-1 \ 1]^{\tr}\right\}\).%
\item[(ii)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp22271000-back}{}\quad{}\(\left\{\frac{1}{\sqrt{14}}[1 \ 2 \ 3]^{\tr}\right\}\).%
\item[(iii)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp22269592-back}{}\quad{}\(\left\{ \frac{1}{\sqrt{2}}[1 \ 1]^{\tr}\right\}\).%
\end{enumerate}
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{4}{}{g:exercise:idp22267800}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp22279960-back}{}\quad{}The set \(\{\vv_1, \vv_2, \ldots, \vv_n\}\) is an orthonormal basis of \(\R^n\). Use this to show that \(||A \vx||^2 \leq \sigma_1^2\) for any unit vector \(\vx\) in \(\R^n\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp22282520}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp22279704-back}{}\quad{}Find the transpose of an SVD for \(A\).%
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp22307224}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp22306328-back}{}\quad{}\(||A|| = \lambda_1\)%
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{x:exercise:ex_7_c_rank1}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp22324376-back}{}\quad{}Mimic \hyperlink{x:exercise:ex_7_a_spectral_decomposition}{Exercise~{\xreffont 5}} in \hyperref[x:chapter:chap_orthogonal_diagonalization]{Section~{\xreffont\ref{x:chapter:chap_orthogonal_diagonalization}}}.%
\end{divisionsolution}%
\begin{divisionsolution}{11}{}{g:exercise:idp22323736}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp22327064-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp22326168-back}{}\quad{}T%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp22329496-back}{}\quad{}F%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp22338712-back}{}\quad{}F%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}30\space\textperiodcentered\space{}Using the Singular Value Decomposition\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp23145368}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23156888-back}{}\quad{}\(60\), \(15\), and \(6\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23158040-back}{}\quad{}%
\begin{align*}
\amp A =  4\left([3 \ 0 \ 4]^{\tr}\right)\left([2 \ 1 \ 1]\right)\\
\amp - \left([4 \ 0 \ -3]^{\tr}\right)\left([1 \ 2 \ -2]\right)\\
\amp + 2\left(\frac{1}{5}[0 \ 5 \ 0]^{\tr}\right)\left([-2 \ 2 \ 1]\right)\text{.}
\end{align*}
%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23158936-back}{}\quad{}\(\left[ \begin{array}{ccc} 24\amp 12\amp 24 \\ 0\amp 0\amp 0 \\ 32\amp 16\amp 32 \end{array} \right]\), \(\frac{261}{3861} \approx 0.068\).%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23156120-back}{}\quad{}\(\left[ \begin{array}{ccc} 20\amp 4\amp 32 \\ 0\amp 0\amp 0 \\ 35\amp 22\amp 26 \end{array} \right]\), \(\frac{36}{3861} \approx 0.009\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp23167512}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23179416-back}{}\quad{}The ball goes up and then it comes down.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23182872-back}{}\quad{}\(\frac{266}{97} + \frac{12213}{970}x -\frac{1423}{9700}x^2\).%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23182360-back}{}\quad{}Approximately \(23.9\) and \(61.9\) degrees.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{4}{}{g:exercise:idp23176728}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(b)]\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item[(iii)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp23198104-back}{}\quad{}For any positive integer \(m\), the sum \(1+\sum_{k=0}^{m-1} 2^k\) is the partial sum of a geometric series with ratio \(2\) and so \(1+\sum_{k=0}^{m-1} 2^k = 1+\frac{1-2^m}{1-2} = 2^m\).%
\end{enumerate}
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp23197336}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23203096-back}{}\quad{}The eigenvalues for \(A\) are 2, 1, and 0, the eigenvalues for \(A^{\tr}A\) are 4, 1, and 0 with the same corresponding eigenvectors.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp22939928-back}{}\quad{}\(\left[ \begin{array}{rcc} 0\amp 1\amp 0 \\ -\frac{1}{\sqrt{2}}\amp 0\amp \frac{1}{\sqrt{2}}\\ \frac{1}{\sqrt{2}}\amp 0\amp \frac{1}{\sqrt{2}} \end{array} \right]\)%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp22939160-back}{}\quad{}\(B = \left[ \begin{array}{crr} 1\amp 0\amp 0 \\ 0\amp \frac{1}{\sqrt{2}}\amp -\frac{1}{\sqrt{2}} \\0\amp -\frac{1}{\sqrt{2}}\amp \frac{1}{\sqrt{2}} \end{array} \right]\)%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23282968-back}{}\quad{}\(A\) was symmetric to write a singular value decomposition for \(A\) in the form \(V \Sigma V^{\tr}\), \(A\) is positive definite%
\item[(e)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23280408-back}{}\quad{}\(\left[ \begin{array}{crr} 1\amp 0\amp 0 \\ 0\amp \frac{\sqrt[3]{2}}{2}\amp -\frac{\sqrt[3]{2}}{2} \\0\amp -\frac{\sqrt[3]{2}}{2}\amp \frac{\sqrt[3]{2}}{2} \end{array} \right]\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{6}{}{g:exercise:idp23279896}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp23284760-back}{}\quad{}\(\Sigma \Sigma^+\) is a symmetric matrix.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp23290776}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp23288088-back}{}\quad{}Use the fact that \(X= XAX\).%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp23287576-back}{}\quad{}Use the fact that \(Y= YAY\).%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp23298328-back}{}\quad{}Compare the results of (a) and (b).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp23297176}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23295256-back}{}\quad{}Use the Rank-Nullity Theorem.%
\end{divisionsolution}%
\begin{divisionsolution}{10}{}{g:exercise:idp23292952}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23296024-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23300120-back}{}\quad{}F%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23301144-back}{}\quad{}F%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}VII\space\textperiodcentered\space{}Vector Spaces\\
31\space\textperiodcentered\space{}Vector Spaces\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp23874312}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp23876360-back}{}\quad{}Use the fact that \(\vzero_1+\vv = \vv\) for any vector \(\vv\) in our vector space.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp23883400-back}{}\quad{}Same reasoning as in part (a).%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp23887496-back}{}\quad{}Use the transitive property of equality.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{4}{}{g:exercise:idp23885704}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp23896712-back}{}\quad{}Use the fact that \(-1 + 1 = 0\).%
\end{divisionsolution}%
\begin{divisionsolution}{6}{}{g:exercise:idp23892488}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23899784-back}{}\quad{}The intersection \(W_1 \cap W_2\) is a subspace of \(V\), but the union \(W_1 \cup W_2\) is not in general a subspace of \(V\).%
\end{divisionsolution}%
\begin{divisionsolution}{8}{}{g:exercise:idp23899400}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23908744-back}{}\quad{}The space \(W\) is the span of \(\left[ \begin{array}{cc} 1 \amp 1 \\ 0 \amp 1 \end{array} \right]\), \(\left[ \begin{array}{cc} 1 \amp 0 \\ 3\amp 1 \end{array} \right]\), and \(\left[ \begin{array}{cr} 0 \amp -2 \\ 1 \amp 1 \end{array} \right]\).%
\end{divisionsolution}%
\begin{divisionsolution}{10}{}{g:exercise:idp23915400}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp23914120-back}{}\quad{}Mimic the proof of \hyperref[x:theorem:thm_3_a_span_subspace]{Theorem~{\xreffont\ref{x:theorem:thm_3_a_span_subspace}}}.%
\end{divisionsolution}%
\begin{divisionsolution}{12}{}{g:exercise:idp23933832}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23935880-back}{}\quad{}Closure is by definition, the sequence \(\{0\}\) is the additive identity, the sequence \(\{-x_n\}\) is the additive inverse of the sequence \(\{x_n\}\). The other properties follow from the definitions of addition and multiplication by    scalars.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23934344-back}{}\quad{}The answer is no.%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23941128-back}{}\quad{}The answer is yes.%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23944840-back}{}\quad{}The answer is no.%
\item[(e)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23946504-back}{}\quad{}The answer is yes.%
\item[(f)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23952392-back}{}\quad{}To show that \(\ell^2\) is closed under addition, expand the square.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{14}{}{g:exercise:idp23954952}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23700616-back}{}\quad{}T%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23699592-back}{}\quad{}T%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23979688-back}{}\quad{}T%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23986216-back}{}\quad{}T%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp23987880-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}32\space\textperiodcentered\space{}Bases for Vector Spaces\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp24306696}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24303368-back}{}\quad{}This set is a basis for \(\R^3\).%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24306312-back}{}\quad{}The set is linearly independent in \(\pol_3\) but does not span \(\pol_3\).%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24305928-back}{}\quad{}The set is a basis for \(\pol_3\).%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24309640-back}{}\quad{}The set is linearly independent in \(M_{3 \times 2}\) but does not span \(M_{3 \times 2}\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp24309512}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24314504-back}{}\quad{}The set \(\{M_{ij}\}\) where \(M_{ij}\) is the matrix with a \(1\) in the \(ij\)th position and zeros everywhere else is a basis for \(\M_{2 \times 2}\), as is the set \(\{M'_{ij}\}\), where \(M'_{ij}\) is the matrix with a \(-1\) in the \(ij\)th position and zeros everywhere else.%
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp24320136}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24331272-back}{}\quad{}The set is a basis for \(V\).%
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp24330760}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24327944-back}{}\quad{}It is not possible.%
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp24337160}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp24335624-back}{}\quad{}Mimic the proof of \hyperref[x:theorem:thm_dependence]{Theorem~{\xreffont\ref{x:theorem:thm_dependence}}}.%
\end{divisionsolution}%
\begin{divisionsolution}{10}{}{g:exercise:idp24338056}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp24334984-back}{}\quad{}Mimic the proof of \hyperref[x:theorem:thm_minimal_spanning_set]{Theorem~{\xreffont\ref{x:theorem:thm_minimal_spanning_set}}}.%
\end{divisionsolution}%
\begin{divisionsolution}{11}{}{g:exercise:idp24341512}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp24341256-back}{}\quad{}Compare to \hyperref[x:theorem:thm_1_f_unique_representation]{Theorem~{\xreffont\ref{x:theorem:thm_1_f_unique_representation}}}.%
\end{divisionsolution}%
\begin{divisionsolution}{13}{}{g:exercise:idp24343816}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24356360-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24355720-back}{}\quad{}T%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24364040-back}{}\quad{}T%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24364552-back}{}\quad{}F%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24373256-back}{}\quad{}F%
\item[(k)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24367112-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}33\space\textperiodcentered\space{}The Dimension of a Vector Space\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp24673432}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24678168-back}{}\quad{}The set \(\{ 1+t^2, 2+t+2t^2+t^3, 1+t+t^3\}\) is a basis for \(W\) and \(\dim(W) = 3\).%
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp24684312}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24695064-back}{}\quad{}No.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24692504-back}{}\quad{}The set \(S\) is linearly dependent.%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24689304-back}{}\quad{}The set \(\{A,B,E\}\) forms a basis for \(\Span \ S\) and \(C = 2A - 3B\), \(D = 3A-2B\).%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24702104-back}{}\quad{}Let \(F=\left[ \begin{array}{ccc} 0\amp 0\amp 0\\1\amp 0\amp 0 \end{array} \right]\), \(G=\left[ \begin{array}{ccc} 0\amp 0\amp 0\\0\amp 1\amp 0 \end{array} \right]\), and \(H=\left[ \begin{array}{ccc} 0\amp 0\amp 0\\0\amp 0\amp 1 \end{array} \right]\). The set \(\{A,B,E,F,G,H\}\) is a basis for \(\M_{2 \times 3}\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp24709272}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24706584-back}{}\quad{}The set%
\begin{equation*}
\CB = \left\{\left[ \begin{array}{cr} 1\amp 0\\0\amp -1 \end{array}  \right], \left[ \begin{array}{cc} 0\amp 1\\0\amp 1 \end{array}  \right], \left[ \begin{array}{cc} 0\amp 0\\1\amp 0 \end{array}  \right]\right\}
\end{equation*}
is a basis for \(W\). It follows that \(\dim(W) = 3\).%
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp24712472}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24718104-back}{}\quad{}The set \(\CB = \{-1+t, -1+t^2\}\) is a basis for \(W\) and \(\dim(W) = 2\).%
\end{divisionsolution}%
\begin{divisionsolution}{8}{}{g:exercise:idp24717592}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp24721688-back}{}\quad{}See \hyperlink{x:exercise:problem_disjoint_subspaces}{problem~{\xreffont 12}} in the previous section.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp24728600}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp24721816-back}{}\quad{}Consider dimensions.%
\end{divisionsolution}%
\begin{divisionsolution}{11}{}{g:exercise:idp24736920}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp24739608-back}{}\quad{}Consider the row, column, and diagonal sums.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24741656-back}{}\quad{}Set up a system of equations. The dimension is 3.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{12}{}{g:exercise:idp24742552}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24742168-back}{}\quad{}T%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24751384-back}{}\quad{}T%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24750616-back}{}\quad{}F%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24754200-back}{}\quad{}T%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24759064-back}{}\quad{}F%
\item[(k)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp24762776-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}34\space\textperiodcentered\space{}Coordinate Vectors and Coordinate Transformations\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp25216152}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25227288-back}{}\quad{}\([\vb]_{\CB} = \left[ \begin{array}{r} -2\\3 \end{array} \right]\).%
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp25223704}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25230488-back}{}\quad{}Two such bases are \(\{[1 \ 0]^{\tr}, [3 \ 3]^{\tr}\}\) and \(\{[1 \ 1]^{\tr}, [3 \ 1]^{\tr}\}\).%
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp25235864}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25244952-back}{}\quad{}\(\CB = \{ [1 \ 0 \ 2]^{\tr}, [2 \ 1 \ 1]^{\tr}\}\).%
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp25238040}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25239960-back}{}\quad{}Show that \(\CB\) is linearly independent.%
\item[(b)]\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item[(i)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25246232-back}{}\quad{}\([p_1(t)]_{\CB} = [1 \ 0 \ 1]^{\tr}\), \([p_2(t)]_{\CB} = [1 \ 1 \ 1]^{\tr}\), and \([p_3(t)]_{\CB} = [2 \ -1 \ -1]^{\tr}\).%
\item[(ii)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25251480-back}{}\quad{}Row reduce the matrix \(\left[ \begin{array}{ccr} 1\amp 1\amp 2 \\ 0\amp 1\amp -1 \\ 2\amp 2\amp 1 \end{array} \right]\).%
\end{enumerate}
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp25265304}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25263128-back}{}\quad{}Show that \(\CB\) is linearly independent.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25269144-back}{}\quad{}\([A]_{\CB} = [0 \ 0 \ 0 \ 1]^{\tr}\), \([B]_{\CB} = [0 \ 1 \ 1 \ 0]^{\tr}\), \([C]_{\CB} = [1 \ 1 \ 1 \ 0]^{\tr}\), and \([D]_{\CB} = [1 \ -1 \ -1 \ 1]^{\tr}\).%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25264920-back}{}\quad{}Row reduce \([ [A]_{\CB} \ [B]_{\CB} \ [C]_{\CB} \ [D]_{\CB}]\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{11}{}{g:exercise:idp25270168}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25284760-back}{}\quad{}Use the fact that \(c\vu = c(u_1\vv_1 + u_2\vv_2 + \cdots + u_n\vv_n) = (cu_1)\vv_1 + (cu_2)\vv_2 + \cdots + (cu_n) \vv_n\).%
\end{divisionsolution}%
\begin{divisionsolution}{13}{}{g:exercise:idp25281304}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25293464-back}{}\quad{}Write \(\vx\) as a linear combination of basis vectors.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25301528-back}{}\quad{}Apply \(T\) to an appropriate vector and use the fact that \(T\) is one-to-one.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{15}{}{g:exercise:idp25303192}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25306904-back}{}\quad{}Find a vector \(\vy\) in \(V\) such that \(\vx = [\vy]_{\CB}\).%
\end{divisionsolution}%
\begin{divisionsolution}{17}{}{g:exercise:idp25316888}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25318040-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25326104-back}{}\quad{}F%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25322776-back}{}\quad{}T%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25329944-back}{}\quad{}T%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25338264-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}35\space\textperiodcentered\space{}Inner Product Spaces\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{x:exercise:ex_6_c_Cab}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25702920-back}{}\quad{}Use properties of continuous functions.%
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp25705992}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25728152-back}{}\quad{}\(2x^2+3y^2 = 1\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25722904-back}{}\quad{}An ellipse centered at the origin with major axis the segment from \(\left(0, -\frac{1}{\sqrt{3}}\right)\) to \(\left(0, \frac{1}{\sqrt{3}}\right)\) and minor axis the segment from \(\left( -\frac{1}{\sqrt{2}}, 0\right)\) to \(\left( \frac{1}{\sqrt{2}}, 0\right)\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{x:exercise:ex_6_c_ip_2}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25743128-back}{}\quad{}Verify the inner product properties. Why is the assumption that the \(a_i\) are positive necessary?%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25741464-back}{}\quad{}\(A = \left[ \begin{array}{ccccc} a_1\amp 0\amp 0\amp \cdots \amp 0 \\ 0\amp a_2\amp 0\amp \cdots \amp 0 \\ \amp \amp \amp \ddots\amp \\ 0\amp 0\amp 0\amp \cdots \amp a_n \end{array} \right]\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp25749016}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25747992-back}{}\quad{}No.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25750040-back}{}\quad{}See \hyperlink{x:exercise:ex_6_c_ip_1}{Exercise~{\xreffont 4}} and \hyperlink{x:exercise:ex_6_c_ip_2}{Exercise~{\xreffont 5}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25758744-back}{}\quad{}If \(A\) is a diagonal matrix with positive diagonal entries.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp25764248}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25761304-back}{}\quad{}Evaluate each side of the inequality.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25766680-back}{}\quad{}Write \(||\vw||^2\) as an inner product and expand.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{11}{}{g:exercise:idp25773592}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25770648-back}{}\quad{}Expand the inner product.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25780376-back}{}\quad{}Expand the inner product.%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25778072-back}{}\quad{}Convert \(A\) and \(B\) to vectors in \(\R^{n^2}\) whose entries are the entries in the first row followed by the entries in the second row and so on.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{13}{}{g:exercise:idp25792536}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25790104-back}{}\quad{}Compute the inner product of the vectors.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25790616-back}{}\quad{}Try to write \(\vv\) in terms of the basis vectors for \(W\).%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25795480-back}{}\quad{}\([1 \ 2 \ 1]^{\tr}\). Approximately 1.41.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{15}{}{g:exercise:idp25802776}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25805208-back}{}\quad{}Use the fact that \(\vzero = \vzero + \vzero\).%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25803288-back}{}\quad{}Use the fact that \(\langle \vu, \vv \rangle = \langle \vv, \vu \rangle\).%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25807512-back}{}\quad{}Same hint as part (b).%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25814808-back}{}\quad{}Use the fact that \(\vu - \vv = \vu + (-\vv)\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{16}{}{x:exercise:ex_6_c_orth_complement_basis}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25816216-back}{}\quad{}Mimic \hyperref[x:theorem:thm_6_a_orth_complement_basis]{Theorem~{\xreffont\ref{x:theorem:thm_6_a_orth_complement_basis}}}.%
\end{divisionsolution}%
\begin{divisionsolution}{17}{}{g:exercise:idp25816344}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp25819160-back}{}\quad{}Mimic \hyperref[x:theorem:thm_6_b_Orth_li]{Theorem~{\xreffont\ref{x:theorem:thm_6_b_Orth_li}}}.%
\end{divisionsolution}%
\begin{divisionsolution}{19}{}{g:exercise:idp25824280}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25828888-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25831192-back}{}\quad{}F%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25842328-back}{}\quad{}T%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25845400-back}{}\quad{}T%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25850392-back}{}\quad{}F%
\item[(k)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25851800-back}{}\quad{}T%
\item[(m)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25856792-back}{}\quad{}T%
\item[(o)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp25864088-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}36\space\textperiodcentered\space{}The Gram-Schmidt Process in Inner Product Spaces\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp26383624}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp26390536-back}{}\quad{}Recall the work done in \hyperref[x:activity:act_6_d_gs_examples]{Activity~{\xreffont\ref{x:activity:act_6_d_gs_examples}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp26391048-back}{}\quad{}\(\proj_W h(t) \approx -0.386t^2 -0.721t + 2.06\) \textbackslash{}item\(2-2t^2\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp26399880-back}{}\quad{}\(2 - 2t^2\)%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp26397832-back}{}\quad{}The least squares polynomial is an overall better fit.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp26408200}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp26401032-back}{}\quad{}Evaluate an appropriate linear combination at several well chosen values to set up a linear system.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp26406152-back}{}\quad{}\(\left\{1, \cos(t), \sin(t) - \frac{2}{\pi}\right\}\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp26409864}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp26413192-back}{}\quad{}\(\left\{1, t, -\frac{1}{3} + t^2, t^3-\frac{3}{5}t \right\}\)%
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp26420872}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp26431880-back}{}\quad{}\(\left\{1, \frac{1}{3}+t,  \frac{1}{2} + \frac{1}{2}t + t^2\right\}\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp26426120-back}{}\quad{}\(\proj_{\pol_1} (1+2t+3t^2) = \frac{1}{2}(-1+t)\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp26442504}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp26445576-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp26453256-back}{}\quad{}T%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp26450440-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}VIII\space\textperiodcentered\space{}Linear Transformations\\
37\space\textperiodcentered\space{}Linear Transformations\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp26976136}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp26979336-back}{}\quad{}Use properties of the definite integral from calculus.%
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{x:exercise:ex_8_a_inverse_isomorphism}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp26994696-back}{}\quad{}Show that \((S \circ T)(\vx) = \vx\) for all \(\vx\) in \(V\) and that \((T \circ S)(\vw) = \vw\) for all \(\vw\) in \(W\).%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp26992392-back}{}\quad{}Define \(S : \pol_3 \to \M_{2 \times 2}\) by%
\begin{equation*}
S(a+bt+ct^2+dt^3) = \left[ \begin{array}{cc} a\amp b\\ c\amp d \end{array}  \right]\text{.}
\end{equation*}
%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp26999944-back}{}\quad{}The property that \(T^{-1}(\vw) = \vv\) whenever \(T(\vv) = \vw\) is the key to this problem.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp27011464}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27015048-back}{}\quad{}linear transformation, one-to-one, onto%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27021064-back}{}\quad{}linear transformation, one-to-one, onto%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{x:exercise:ex_8_a_isomorphic_dimension}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27028104-back}{}\quad{}Use the fact that \(T\) is one-to-one to show that \(\CC\) is linearly independent, and that \(T\) is onto to show that \(\CC\) spans \(W\).%
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp27033992}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27037832-back}{}\quad{}Yes, but the vector space needs to be infinite dimensional.%
\end{divisionsolution}%
\begin{divisionsolution}{10}{}{g:exercise:idp27042312}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27042440-back}{}\quad{}See \hyperref[x:exploration:pa_3_b]{Preview Activity~{\xreffont\ref{x:exploration:pa_3_b}}}.%
\end{divisionsolution}%
\begin{divisionsolution}{11}{}{g:exercise:idp27040776}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27041544-back}{}\quad{}Show that \(T(\vv) = \vw\) has at most one solution for each \(\vw\) in \(W\).%
\end{divisionsolution}%
\begin{divisionsolution}{13}{}{g:exercise:idp26782344}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27128840-back}{}\quad{}Think of \(T(V')\) as the range of a suitable restriction of \(T\).%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27123080-back}{}\quad{}Use Exercise 12. It is possible.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{16}{}{g:exercise:idp27144968}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27146120-back}{}\quad{}Use \hyperlink{x:exercise:ex_8_a_rank_nullity}{Exercise~{\xreffont 15}}.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27152136-back}{}\quad{}Use \hyperlink{x:exercise:ex_8_a_rank_nullity}{Exercise~{\xreffont 15}}.%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{19}{}{g:exercise:idp27174408}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27181832-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27186184-back}{}\quad{}T%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27193224-back}{}\quad{}F%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27192968-back}{}\quad{}T%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27196680-back}{}\quad{}F%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}38\space\textperiodcentered\space{}The Matrix of a Linear Transformation\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{x:exercise:ex_8_b_matrix_transformation}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27506072-back}{}\quad{}\(r(t) = r_0(1) + r_1(t) + r_2(t_2)\) and \([r(t)]_{\CB} = \left[ \begin{array}{c} r_0\\r_1\\r_2 \end{array} \right]\).%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27506328-back}{}\quad{}\(T\) is a linear transformation%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27508632-back}{}\quad{}The coordinate mapping is a linear transformation.%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27507096-back}{}\quad{}\([T(p_0(t))]_{\CC} = \left[ \begin{array}{c} 1\\0\\1\\0 \end{array} \right]\), \([T(p_1(t))]_{\CC} = \left[ \begin{array}{c} 1\\1\\0\\0 \end{array} \right]\), \([T(p_2(t))]_{\CC} = \left[ \begin{array}{c} 0\\1\\0\\1 \end{array} \right]\)%
\item[(e)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27512088-back}{}\quad{}\(\left[ \begin{array}{ccc} 1\amp 1\amp 0 \\ 0\amp 1\amp 1 \\ 1\amp 0\amp 0 \\ 0\amp 0\amp 1 \end{array} \right]\).%
\item[(f)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27516568-back}{}\quad{}\([T(1+t-t^2)]_{\CC} = \left[ \begin{array}{r} 2\\0\\1\\-1 \end{array} \right]\), \(T(1+t-t^2) = 2(1) + 0(t) + 1(t^2) + (-1)t^3 = 2+t^2-t^3\).%
\item[(g)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27515800-back}{}\quad{}\(T(1+t-t^2) = 2 + t^2 - t^3\).%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp27531672}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27531928-back}{}\quad{}Use the linearity of both \(S\) and \(T\).%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27543320-back}{}\quad{}True%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{x:exercise:ex_8_b_Ker_Nul}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27553560-back}{}\quad{}\(\vx \in \Ker(T)\) if and only if \([\vx]_{\CB} \in \Nul [T]_{\CB}^{\CC}\) T%
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{x:exercise:ex_8_b_range_matrix}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27576600-back}{}\quad{}Since \(\vw\) is in the range of \(T\), there is a vector \(\vv\) so that \(T(\vv) = \vw\).%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27585048-back}{}\quad{}Since \(\vy\) is in the range of \(T'\), there exists a vector \(\vx\) in \(\R^n\) so that \(T'(\vx) = \vy\). What is \([\vw]_{\CC}\)?%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27586968-back}{}\quad{}How do we tell if \(T'\) is onto?%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{8}{}{g:exercise:idp27582488}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27589912-back}{}\quad{}You might consider using the Wronskian.%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27598488-back}{}\quad{}If \(T(f) = \cos(t)\), what does that say about the relationship between \([T]_{\CB}^{\CB}\), \([\cos(t)]_{\CB}\), and \([f]_{\CB}\)?%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp27597464}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27604760-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27618328-back}{}\quad{}T%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27624600-back}{}\quad{}T%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27637912-back}{}\quad{}T%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27646232-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}39\space\textperiodcentered\space{}Eigenvalues of Linear Transformations\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp27968392}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27970312-back}{}\quad{}Use properties of the derivative.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27972360-back}{}\quad{}\([T]_{\CS} = \left[ \begin{array}{rrr} -1\amp 1\amp 0 \\ 0\amp -2\amp 2 \\ 0\amp 0\amp -3 \end{array} \right]\).%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27974536-back}{}\quad{}\(-1\), \(-2\), and \(-3\) with bases \(\{1\}\), \(\{-1+t\}\), \(\{1-2t+t^2\}\)%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp27978888-back}{}\quad{}\(\CB = \{-1, -1+t, 1-2t+t^2\}\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{x:exercise:ex_8_c_derivative_operator}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27982088-back}{}\quad{}Use properties of differentiable functions.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27983496-back}{}\quad{}Use properties of the derivative.%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27995912-back}{}\quad{}What is \(D\left(e^{\lambda x}\right)\)?%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp27996040}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp27998088-back}{}\quad{}Use properties of the matrix transpose.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp28005128-back}{}\quad{}For which matrices is \(T(A) = A\)?%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp28003080-back}{}\quad{}When is it possible to have \(A^{\tr} = \lambda A\)?%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{6}{}{g:exercise:idp28004360}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28005768-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28011912-back}{}\quad{}T%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28006024-back}{}\quad{}T%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28015240-back}{}\quad{}T%
\end{enumerate}
\end{divisionsolution}%
\par\medskip
\noindent\textbf{\Large{}40\space\textperiodcentered\space{}The Jordan Canonical Form\\
\space\textperiodcentered\space{}Exercises}
\begin{divisionsolution}{1}{}{g:exercise:idp28807192}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28807576-back}{}\quad{}\(\left[ \begin{array}{ccc} 12\amp 1\amp 0 \\ 0\amp 12\amp 1 \\ 0\amp 0\amp 12 \end{array} \right]\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28804376-back}{}\quad{}\(\left[ \begin{array}{ccc} 9\amp 0\amp 0 \\ 0\amp 9\amp 1 \\ 0\amp 0\amp 9 \end{array} \right]\)%
\item[(c)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28807832-back}{}\quad{}\(\left[ \begin{array}{ccc} 12\amp 0\amp 0 \\ 0\amp 6\amp 0 \\ 0\amp 0\amp 6 \end{array} \right]\)%
\item[(d)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28815000-back}{}\quad{}\(\left[ \begin{array}{cccc} 2\amp 1\amp 0\amp 0 \\ 0\amp 2\amp 0\amp 0 \\ 0\amp 0\amp 1\amp 1\\ 0\amp 0\amp 0\amp 1 \end{array} \right]\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{3}{}{g:exercise:idp28816024}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28816536-back}{}\quad{}The Jordan normal form is \(\left[ \begin{array}{cccc} 2\amp 1\amp 0\amp 0 \\ 0\amp 2\amp 1\amp 0 \\ 0\amp 0\amp 2\amp 0 \\ 0\amp 0\amp 0\amp 4 \end{array} \right]\).%
\end{divisionsolution}%
\begin{divisionsolution}{5}{}{g:exercise:idp28822680}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28823448-back}{}\quad{}\(\left[ \begin{array}{cc} \lambda_1 \amp 0 \\ 0\amp \lambda_2 \end{array} \right]\), \(\left[ \begin{array}{cc} \lambda_1 \amp 1 \\ 0\amp \lambda_1 \end{array} \right]\), \(\left[ \begin{array}{ccc} \lambda_1 \amp 0 \amp 0 \\ 0\amp \lambda_2 \amp 0 \\ 0 \amp 0 \amp \lambda_3 \end{array} \right]\), \(\left[ \begin{array}{ccc} \lambda_1 \amp 1 \amp 0 \\ 0\amp \lambda_1 \amp 0 \\ 0 \amp 0 \amp \lambda_2 \end{array} \right]\), \(\left[ \begin{array}{ccc} \lambda_1 \amp 1 \amp 0 \\ 0\amp \lambda_1 \amp 1 \\ 0 \amp 0 \amp \lambda_1 \end{array} \right]\), \(\left[ \begin{array}{ccc} \lambda_1 \amp 1 \amp 0 \\ 0\amp \lambda_1 \amp 0 \\ 0 \amp 0 \amp \lambda_1 \end{array} \right]\).%
\end{divisionsolution}%
\begin{divisionsolution}{7}{}{g:exercise:idp28821016}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28826136-back}{}\quad{}\(\left[\begin{array}{rrrrrrrrrr} 4 \amp 1 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \\ 0 \amp 4 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \\ 0 \amp 0 \amp 2 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \\ 0 \amp 0 \amp 0 \amp 2 \amp 1 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \\ 0 \amp 0 \amp 0 \amp 0 \amp 2 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \\ 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 2 \amp 1 \amp 0 \amp 0 \amp 0 \\ 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 2 \amp 1 \amp 0 \amp 0 \\ 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 2 \amp 1 \amp 0 \\ 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 2 \amp 1 \\ 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 2 \end{array} \right]\)%
\end{divisionsolution}%
\begin{divisionsolution}{8}{}{g:exercise:idp28826008}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(e)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp28585496-back}{}\quad{}What matrix is \([H]_{\CC_G}^{\CC_F}\)?%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{9}{}{g:exercise:idp28579992}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp28582552-back}{}\quad{}Show that \(\vv_{1} = (A-\lambda I)^{k-1} \vv_k\) for each \(k\) from \(2\) to \(p\).%
\item[(b)]\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item[(i)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28876824-back}{}\quad{}\(\vv_p\) is not in \(\Nul (A-\lambda I)^{p-1}\), so \((A-\lambda I)^{p-1}\vv_p \neq \vzero\)%
\item[(ii)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28879512-back}{}\quad{}The same process as in i. shows that \(x_{p-1} = 0\).%
\item[(iii)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28882840-back}{}\quad{}Keep repeating the process.%
\end{enumerate}
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{11}{}{g:exercise:idp28886168}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28890136-back}{}\quad{}\(C = [\vu_2 \ \vu_1]\) with \(\vu_1 = [0 \ 1]^{\tr}\) and \(\vu_2 = (A-I_2)\vu_1 = [-1 \ -1]^{\tr}\)%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28890776-back}{}\quad{}\(J^k = \left[ \begin{array}{cc} 1\amp k \\ 0 \amp 1 \end{array} \right]\), \(A^k = \left[ \begin{array}{cc} 1+k\amp -k\\k\amp 1-k \end{array} \right]\)%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{13}{}{g:exercise:idp28898584}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp28899608-back}{}\quad{}Show that \((A - \lambda I)S(\vx) = \vzero\)%
\end{divisionsolution}%
\begin{divisionsolution}{15}{}{g:exercise:idp28908440}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28908568-back}{}\quad{}\(\left[ \begin{array}{ccc} 0\amp 0\amp 0 \\ 1\amp 0\amp 0 \\ 0\amp 0\amp 0 \end{array} \right]\) and \(\left[ \begin{array}{ccc} 0\amp 0\amp 0 \\ 0\amp 0\amp 0 \\ 1\amp 0\amp 0 \end{array} \right]\)%
\end{divisionsolution}%
\begin{divisionsolution}{17}{}{g:exercise:idp28909976}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp28916504-back}{}\quad{}Use a projection onto a subspace.%
\end{divisionsolution}%
\begin{divisionsolution}{19}{}{g:exercise:idp28919576}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28927000-back}{}\quad{}\(\{2,-2t,t^2\}\)%
\end{divisionsolution}%
\begin{divisionsolution}{21}{}{g:exercise:idp28939416}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\begin{enumerate}[font=\bfseries,label=(\roman*),ref=\theenumi.\roman*]
\item[(i)]\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp28937880-back}{}\quad{}Calculate powers of \(A\).%
\item[(ii)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28933400-back}{}\quad{}\((I_3-A)\left(I_3+A+A^2\right) = I_3\).%
\end{enumerate}
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28942104-back}{}\quad{}Calculate%
\begin{equation*}
(I-A)\left(I + A + \cdots + A^{m-1}\right)\text{.}
\end{equation*}
%
\end{enumerate}
\end{divisionsolution}%
\begin{divisionsolution}{23}{}{g:exercise:idp28950424}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint 1}.\hypertarget{g:hint:idp28962840-back}{}\quad{}For one direction, use the Cayley-Hamilton Theorem.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint 2}.\hypertarget{g:hint:idp28959640-back}{}\quad{}If \(A\) is nilpotent and \(\vx\) is an eigenvector of \(A\), what is \(A^m \vx\) for every positive integer \(m\)? If \(0\) is the only eigenvalue of \(A\), what is the characteristic polynomial of \(A\)?%
\end{divisionsolution}%
\begin{divisionsolution}{25}{}{g:exercise:idp28968856}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28964376-back}{}\quad{}F%
\item[(c)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28972952-back}{}\quad{}T%
\item[(e)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28973976-back}{}\quad{}F%
\item[(g)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28980248-back}{}\quad{}T%
\item[(i)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28986904-back}{}\quad{}T%
\item[(k)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp28995992-back}{}\quad{}F%
\item[(m)]\lititle{True\slash{}False.}\par%
\par\smallskip%
\noindent\textbf{\blocktitlefont Answer}.\hypertarget{g:answer:idp29003288-back}{}\quad{}F%
\end{enumerate}
\end{divisionsolution}%
\end{solutions-chapter}
%
\backmatter%
%
\clearpage\phantomsection%
\addcontentsline{toc}{part}{Back Matter}%
%
%% The index is here, setup is all in preamble
%% Index locators are cross-references, so same font here
{\xreffont\printindex}
%
\end{document}