<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*       on 2022-06-08T15:01:22-04:00       *-->
<!--*   A recent stable commit (2020-08-09):   *-->
<!--* 98f21740783f166a773df4dc83cab5293ab63a4a *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US">
<head xmlns:og="http://ogp.me/ns#" xmlns:book="https://ogp.me/ns/book#">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>The Dimension of a Vector Space</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:type" content="book">
<meta property="book:title" content="An Inquiry-Based Introduction to Linear Algebra and Applications">
<meta property="book:author" content="Feryal Alayont">
<meta property="book:author" content="Steven Schlicker">
<script>window.MathJax = {
  tex: {
    inlineMath: [['\\(','\\)']],
    tags: "none",
    tagSide: "right",
    tagIndent: ".8em",
    packages: {'[+]': ['base', 'extpfeil', 'ams', 'amscd', 'newcommand', 'knowl']}
  },
  options: {
    ignoreHtmlClass: "tex2jax_ignore|ignore-math",
    processHtmlClass: "process-math",
  },
  chtml: {
    scale: 0.88,
    mtextInheritFont: true
  },
  loader: {
    load: ['input/asciimath', '[tex]/extpfeil', '[tex]/amscd', '[tex]/newcommand', '[pretext]/mathjaxknowl3.js'],
    paths: {pretext: "https://pretextbook.org/js/lib"},
  },
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/themes/prism.css" rel="stylesheet">
<script async="" src="https://cse.google.com/cse.js?cx=a16e70a6cb1434676"></script><script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.13/pretext.js"></script><script>miniversion=0.674</script><script src="https://pretextbook.org/js/0.13/pretext_add_on.js?x=1"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script>sagecellEvalName='Evaluate (Sage)';
</script><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/banner_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/toc_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/knowls_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/style_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/colors_blue_grey.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/setcolors.css" rel="stylesheet" type="text/css">
<!-- 2019-10-12: Temporary - CSS file for experiments with styling --><link href="developer.css" rel="stylesheet" type="text/css">
</head>
<body class="pretext-book ignore-math has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div id="latex-macros" class="hidden-content process-math" style="display:none"><span class="process-math">\(\usepackage{amsmath}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\E}{\mathbb{E}}
\DeclareMathOperator{\ch}{char}
\newcommand{\N}{\mathbb{N}}
\newcommand{\W}{\mathbb{W}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\J}{\mathbb{J}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathbb{D}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\NE}{\mathcal{E}}
\newcommand{\Mn}[1]{\mathcal{M}_{#1 \times #1}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\Rn}{\R^n}
\newcommand{\Mat}{\mathbf}
\newcommand{\Seq}{\boldsymbol}
\newcommand{\seq}[1]{\boldsymbol{#1}}
\newcommand{\set}[1]{\mathcal{#1}}
\newcommand{\abs}[1]{\left\lvert{}#1\right\rvert}
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\cq}{\scalebox{.34}{\pscirclebox{ \textbf{?}}}}
\newcommand{\cqup}{\,$^{\text{\scalebox{.2}{\pscirclebox{\textbf{?}}}}}$}
\newcommand{\cqupmath}{\,^{\text{\scalebox{.2}{\pscirclebox{\textbf{?}}}}}}
\newcommand{\cqupmathnospace}{^{\text{\scalebox{.2}{\pscirclebox{\textbf{?}}}}}}
\newcommand{\uspace}[1]{\underline{}}
\newcommand{\muspace}[1]{\underline{\mspace{#1 mu}}}
\newcommand{\bspace}[1]{}
\newcommand{\ie}{\emph{i}.\emph{e}.}
\newcommand{\nq}[1]{\scalebox{.34}{\pscirclebox{\textbf{#1}}}}
\newcommand{\equalwhy}{\stackrel{\cqupmath}{=}}
\newcommand{\notequalwhy}{\stackrel{\cqupmath}{\neq}}
\newcommand{\Ker}{\text{Ker}}
\newcommand{\Image}{\text{Im}}
\newcommand{\polyp}{p(x) = a_nx^n + a_{n-1}x^{n-1} + \cdots a_2x^2 + a_1x + a_0}
\newcommand{\GL}{\text{GL}}
\newcommand{\SL}{\text{SL}}
\newcommand{\lcm}{\text{lcm}}
\newcommand{\Aut}{\text{Aut}}
\newcommand{\Inn}{\text{Inn}}
\newcommand{\Hol}{\text{Hol}}
\newcommand{\cl}{\text{cl}}
\newcommand{\bsq}{\hfill $\blacksquare$}
\newcommand{\NIMdot}{{ $\cdot$ }}
\newcommand{\eG}{e_{\scriptscriptstyle{G}}}
\newcommand{\eGroup}[1]{e_{\scriptscriptstyle{#1}}}
\newcommand{\Gdot}[1]{\cdot_{\scriptscriptstyle{#1}}}
\newcommand{\rbar}{\overline{r}}
\newcommand{\nuG}[1]{\nu_{\scriptscriptstyle{#1}}}
\newcommand{\rightarray}[2]{\left[ \begin{array}{#1} #2 \end{array} \right]}
\newcommand{\polymod}[1]{\mspace{5 mu}(\text{mod} #1)}
\newcommand{\ts}{\mspace{2 mu}}
\newcommand{\ds}{\displaystyle}
\newcommand{\adj}{\text{adj}}
\newcommand{\pol}{\mathbb{P}}
\newcommand{\CS}{\mathcal{S}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\CB}{\mathcal{B}}
\renewcommand{\CD}{\mathcal{D}}
\newcommand{\CP}{\mathcal{P}}
\newcommand{\CQ}{\mathcal{Q}}
\newcommand{\CW}{\mathcal{W}}
\newcommand{\rank}{\text{rank}}
\newcommand{\nullity}{\text{nullity}}
\newcommand{\trace}{\text{trace}}
\newcommand{\Area}{\text{Area}}
\newcommand{\Vol}{\text{Vol}}
\newcommand{\cov}{\text{cov}}
\newcommand{\var}{\text{var}}
\newcommand{\va}{\mathbf{a}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\vc}{\mathbf{c}}
\newcommand{\vd}{\mathbf{d}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\vf}{\mathbf{f}}
\newcommand{\vg}{\mathbf{g}}
\newcommand{\vh}{\mathbf{h}}
\newcommand{\vm}{\mathbf{m}}
\newcommand{\vn}{\mathbf{n}}
\newcommand{\vp}{\mathbf{p}}
\newcommand{\vq}{\mathbf{q}}
\newcommand{\vr}{\mathbf{r}}
\newcommand{\vs}{\mathbf{s}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\vzero}{\mathbf{0}}
\newcommand{\vi}{\mathbf{i}}
\newcommand{\vj}{\mathbf{j}}
\newcommand{\vk}{\mathbf{k}}
\newcommand{\vF}{\mathbf{F}}
\newcommand{\vP}{\mathbf{P}}
\newcommand{\nin}{}
\newcommand{\Dom}{\text{Dom}}
\newcommand{\tr}{\mathsf{T}}
\newcommand{\proj}{\text{proj}}
\newcommand{\comp}{\text{comp}}
\newcommand{\Row}{\text{Row }}
\newcommand{\Col}{\text{Col }}
\newcommand{\Nul}{\text{Nul }}
\newcommand{\Span}{\text{Span}}
\newcommand{\Range}{\text{Range}}
\newcommand{\Domain}{\text{Domain}}
\newcommand{\hthin}{\hlinewd{.1pt}}
\newcommand{\hthick}{\hlinewd{.7pt}}
\newcommand{\pbreaks}{1}
\newcommand{\pbreak}{
}
\newcommand{\lint}{\underline{}
\int}
\newcommand{\uint}{ \underline{}
\int}


\newcommand{\Si}{\text{Si}}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\)</span></div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="book-1.html"><span class="title">An Inquiry-Based Introduction to Linear Algebra and Applications</span></a></h1>
<p class="byline">Feryal Alayont, Steven Schlicker</p>
</div>
<div class="searchwrapper" role="search"><div class="gcse-search"></div></div>
</div></div>
<nav id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3">
<a class="index-button toolbar-item button" href="index-1.html" title="Index">Index</a><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="chap_bases.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="part-vec-spaces.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="chap_coordinate_vectors_vector_spaces.html" title="Next">Next</a></span>
</div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="chap_bases.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="part-vec-spaces.html" title="Up">Up</a><a class="next-button button toolbar-item" href="chap_coordinate_vectors_vector_spaces.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link frontmatter">
<a href="frontmatter.html" data-scroll="frontmatter" class="internal"><span class="title">Front Matter</span></a><ul><li><a href="fm_preface.html" data-scroll="fm_preface" class="internal">Preface</a></li></ul>
</li>
<li class="link part"><a href="part-systems.html" data-scroll="part-systems" class="internal"><span class="codenumber">I</span> <span class="title">Systems of Linear Equations</span></a></li>
<li class="link">
<a href="chap_intro_linear_systems.html" data-scroll="chap_intro_linear_systems" class="internal"><span class="codenumber">1</span> <span class="title">Introduction to Systems of Linear Equations</span></a><ul>
<li><a href="chap_intro_linear_systems.html#sec_appl_elec_circuits" data-scroll="sec_appl_elec_circuits" class="internal">Application: Electrical Circuits</a></li>
<li><a href="chap_intro_linear_systems.html#sec_intro_le_intro" data-scroll="sec_intro_le_intro" class="internal">Introduction</a></li>
<li><a href="chap_intro_linear_systems.html#sec_notation" data-scroll="sec_notation" class="internal">Notation and Terminology</a></li>
<li><a href="chap_intro_linear_systems.html#sec_solve_systems" data-scroll="sec_solve_systems" class="internal">Solving Systems of Linear Equations</a></li>
<li><a href="chap_intro_linear_systems.html#sec_geom_solu_sets" data-scroll="sec_geom_solu_sets" class="internal">The Geometry of Solution Sets of Linear Systems</a></li>
<li><a href="chap_intro_linear_systems.html#sec_intro_le_exam" data-scroll="sec_intro_le_exam" class="internal">Examples</a></li>
<li><a href="chap_intro_linear_systems.html#sec_intro_le_summ" data-scroll="sec_intro_le_summ" class="internal">Summary</a></li>
<li><a href="chap_intro_linear_systems.html#sec_intro_le_exer" data-scroll="sec_intro_le_exer" class="internal">Exercises</a></li>
<li><a href="chap_intro_linear_systems.html#sec_1_a_circuits" data-scroll="sec_1_a_circuits" class="internal">Project: Modeling an Electrical Circuit and the Wheatstone Bridge Circuit</a></li>
</ul>
</li>
<li class="link">
<a href="chap_matrix_representation.html" data-scroll="chap_matrix_representation" class="internal"><span class="codenumber">2</span> <span class="title">The Matrix Representation of a Linear System</span></a><ul>
<li><a href="chap_matrix_representation.html#sec_appl_area_curve" data-scroll="sec_appl_area_curve" class="internal">Application: Approximating Area Under a Curve</a></li>
<li><a href="chap_matrix_representation.html#sec_mtx_lin_intro" data-scroll="sec_mtx_lin_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_representation.html#sec_simp_mtx_sys" data-scroll="sec_simp_mtx_sys" class="internal">Simplifying Linear Systems Represented in Matrix Form</a></li>
<li><a href="chap_matrix_representation.html#sec_sys_inf_sols" data-scroll="sec_sys_inf_sols" class="internal">Linear Systems with Infinitely Many Solutions</a></li>
<li><a href="chap_matrix_representation.html#sec_sys_no_sols" data-scroll="sec_sys_no_sols" class="internal">Linear Systems with No Solutions</a></li>
<li><a href="chap_matrix_representation.html#sec_mtx_sys_exam" data-scroll="sec_mtx_sys_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_representation.html#sec_mtx_sys_summ" data-scroll="sec_mtx_sys_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_representation.html#sec_mtx_sys_exer" data-scroll="sec_mtx_sys_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_representation.html#sec_1_b_polynomial" data-scroll="sec_1_b_polynomial" class="internal">Project: Polynomial Interpolation to Approximate the Area Under a Curve</a></li>
</ul>
</li>
<li class="link">
<a href="chap_row_echelon_forms.html" data-scroll="chap_row_echelon_forms" class="internal"><span class="codenumber">3</span> <span class="title">Row Echelon Forms</span></a><ul>
<li><a href="chap_row_echelon_forms.html#sec_appl_chem_react" data-scroll="sec_appl_chem_react" class="internal">Application: Balancing Chemical Reactions</a></li>
<li><a href="chap_row_echelon_forms.html#sec_row_ech_intro" data-scroll="sec_row_ech_intro" class="internal">Introduction</a></li>
<li><a href="chap_row_echelon_forms.html#sec_mtx_ech_forms" data-scroll="sec_mtx_ech_forms" class="internal">The Echelon Forms of a Matrix</a></li>
<li><a href="chap_row_echelon_forms.html#sec_num_sols_ls" data-scroll="sec_num_sols_ls" class="internal">Determining the Number of Solutions of a Linear System</a></li>
<li><a href="chap_row_echelon_forms.html#sec_prod_ech_forms" data-scroll="sec_prod_ech_forms" class="internal">Producing the Echelon Forms</a></li>
<li><a href="chap_row_echelon_forms.html#sec_row_ech_exam" data-scroll="sec_row_ech_exam" class="internal">Examples</a></li>
<li><a href="chap_row_echelon_forms.html#sec_row_ech_summ" data-scroll="sec_row_ech_summ" class="internal">Summary</a></li>
<li><a href="chap_row_echelon_forms.html#sec_row_ech_exer" data-scroll="sec_row_ech_exer" class="internal">Exercises</a></li>
<li><a href="chap_row_echelon_forms.html#sec_prof_chem_react" data-scroll="sec_prof_chem_react" class="internal">Project: Modeling a Chemical Reaction</a></li>
</ul>
</li>
<li class="link">
<a href="chap_vector_representation.html" data-scroll="chap_vector_representation" class="internal"><span class="codenumber">4</span> <span class="title">Vector Representation</span></a><ul>
<li><a href="chap_vector_representation.html#sec_appl_knight" data-scroll="sec_appl_knight" class="internal">Application: The Knight's Tour</a></li>
<li><a href="chap_vector_representation.html#sec_vec_rep_intro" data-scroll="sec_vec_rep_intro" class="internal">Introduction</a></li>
<li><a href="chap_vector_representation.html#sec_vec_ops" data-scroll="sec_vec_ops" class="internal">Vectors and Vector Operations</a></li>
<li><a href="chap_vector_representation.html#sec_geom_vec_ops" data-scroll="sec_geom_vec_ops" class="internal">Geometric Representation of Vectors and Vector Operations</a></li>
<li><a href="chap_vector_representation.html#sec_lin_comb_vec" data-scroll="sec_lin_comb_vec" class="internal">Linear Combinations of Vectors</a></li>
<li><a href="chap_vector_representation.html#sec_vec_span" data-scroll="sec_vec_span" class="internal">The Span of a Set of Vectors</a></li>
<li><a href="chap_vector_representation.html#sec_vec_rep_exam" data-scroll="sec_vec_rep_exam" class="internal">Examples</a></li>
<li><a href="chap_vector_representation.html#sec_vec_rep_summ" data-scroll="sec_vec_rep_summ" class="internal">Summary</a></li>
<li><a href="chap_vector_representation.html#exercises-4" data-scroll="exercises-4" class="internal">Exercises</a></li>
<li><a href="chap_vector_representation.html#sec_proj_knight" data-scroll="sec_proj_knight" class="internal">Project: Analyzing Knight Moves</a></li>
</ul>
</li>
<li class="link">
<a href="chap_matrix_vector.html" data-scroll="chap_matrix_vector" class="internal"><span class="codenumber">5</span> <span class="title">The Matrix-Vector Form of a Linear System</span></a><ul>
<li><a href="chap_matrix_vector.html#sec_appl_model_econ" data-scroll="sec_appl_model_econ" class="internal">Application: Modeling an Economy</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form_intro" data-scroll="sec_mv_form_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_prod" data-scroll="sec_mv_prod" class="internal">The Matrix-Vector Product</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form" data-scroll="sec_mv_form" class="internal">The Matrix-Vector Form of a Linear System</a></li>
<li><a href="chap_matrix_vector.html#sec_homog_sys" data-scroll="sec_homog_sys" class="internal">Homogeneous and Nonhomogeneous Systems</a></li>
<li><a href="chap_matrix_vector.html#sec_geom_homog_sys" data-scroll="sec_geom_homog_sys" class="internal">The Geometry of Solutions to the Homogeneous System</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form_exam" data-scroll="sec_mv_form_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form_summ" data-scroll="sec_mv_form_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form_exer" data-scroll="sec_mv_form_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_vector.html#sec_proj_io_models" data-scroll="sec_proj_io_models" class="internal">Project: Input-Output Models</a></li>
</ul>
</li>
<li class="link">
<a href="chap_independence.html" data-scroll="chap_independence" class="internal"><span class="codenumber">6</span> <span class="title">Linear Dependence and Independence</span></a><ul>
<li><a href="chap_independence.html#sec_appl_bezier" data-scroll="sec_appl_bezier" class="internal">Application: Bézier Curves</a></li>
<li><a href="chap_independence.html#sec_indep_intro" data-scroll="sec_indep_intro" class="internal">Introduction</a></li>
<li><a href="chap_independence.html#lin_indep_intro_new" data-scroll="lin_indep_intro_new" class="internal">Linear Independence</a></li>
<li><a href="chap_independence.html#sec_determ_lin_ind" data-scroll="sec_determ_lin_ind" class="internal">Determining Linear Independence</a></li>
<li><a href="chap_independence.html#sec_min_span_set" data-scroll="sec_min_span_set" class="internal">Minimal Spanning Sets</a></li>
<li><a href="chap_independence.html#sec_indep_exam" data-scroll="sec_indep_exam" class="internal">Examples</a></li>
<li><a href="chap_independence.html#sec_indep_summ" data-scroll="sec_indep_summ" class="internal">Summary</a></li>
<li><a href="chap_independence.html#sec_indep_exer" data-scroll="sec_indep_exer" class="internal">Exercises</a></li>
<li><a href="chap_independence.html#sec_proj_bezier" data-scroll="sec_proj_bezier" class="internal">Project: Generating Bézier Curves</a></li>
</ul>
</li>
<li class="link">
<a href="chap_matrix_transformations.html" data-scroll="chap_matrix_transformations" class="internal"><span class="codenumber">7</span> <span class="title">Matrix Transformations</span></a><ul>
<li><a href="chap_matrix_transformations.html#sec_appl_graphics" data-scroll="sec_appl_graphics" class="internal">Application: Computer Graphics</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_intro" data-scroll="sec_mtx_trans_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_prop" data-scroll="sec_mtx_trans_prop" class="internal">Properties of Matrix Transformations</a></li>
<li><a href="chap_matrix_transformations.html#sec_trans_onto_oto" data-scroll="sec_trans_onto_oto" class="internal">Onto and One-to-One Transformations</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_exam" data-scroll="sec_mtx_trans_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_summ" data-scroll="sec_mtx_trans_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_exer" data-scroll="sec_mtx_trans_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_transformations.html#sec_proj_geom_mtx" data-scroll="sec_proj_geom_mtx" class="internal">Project: The Geometry of Matrix Transformations</a></li>
</ul>
</li>
<li class="link part"><a href="part-matrices.html" data-scroll="part-matrices" class="internal"><span class="codenumber">II</span> <span class="title">Matrices</span></a></li>
<li class="link">
<a href="chap_matrix_operations.html" data-scroll="chap_matrix_operations" class="internal"><span class="codenumber">8</span> <span class="title">Matrix Operations</span></a><ul>
<li><a href="chap_matrix_operations.html#sec_appl_mtx_mult" data-scroll="sec_appl_mtx_mult" class="internal">Application: Algorithms for Matrix Multiplication</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_ops_intro" data-scroll="sec_mtx_ops_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_add_smult" data-scroll="sec_mtx_add_smult" class="internal">Properties of Matrix Addition and Multiplication by Scalars</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_prod" data-scroll="sec_mtx_prod" class="internal">A Matrix Product</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_transpose" data-scroll="sec_mtx_transpose" class="internal">The Transpose of a Matrix</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_transpose_prop" data-scroll="sec_mtx_transpose_prop" class="internal">Properties of the Matrix Transpose</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_ops_exam" data-scroll="sec_mtx_ops_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_ops_summ" data-scroll="sec_mtx_ops_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_ops_exer" data-scroll="sec_mtx_ops_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_operations.html#sec_proj_starassen" data-scroll="sec_proj_starassen" class="internal">Project: Strassen's Algorithm and Partitioned Matrices</a></li>
</ul>
</li>
<li class="link">
<a href="chap_intro_eigenvals_eigenvects.html" data-scroll="chap_intro_eigenvals_eigenvects" class="internal"><span class="codenumber">9</span> <span class="title">Introduction to Eigenvalues and Eigenvectors</span></a><ul>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_appl_pagerank" data-scroll="sec_appl_pagerank" class="internal">Application: The Google PageRank Algorithm</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigen_intro" data-scroll="sec_eigen_intro" class="internal">Introduction</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigval_eigvec" data-scroll="sec_eigval_eigvec" class="internal">Eigenvalues and Eigenvectors</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_dynam_sys" data-scroll="sec_dynam_sys" class="internal">Dynamical Systems</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigen_exam" data-scroll="sec_eigen_exam" class="internal">Examples</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigen_summ" data-scroll="sec_eigen_summ" class="internal">Summary</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigen_exer" data-scroll="sec_eigen_exer" class="internal">Exercises</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_proj_pagerank" data-scroll="sec_proj_pagerank" class="internal">Project: Understanding the PageRank Algorithm</a></li>
</ul>
</li>
<li class="link">
<a href="chap_matrix_inverse.html" data-scroll="chap_matrix_inverse" class="internal"><span class="codenumber">10</span> <span class="title">The Inverse of a Matrix</span></a><ul>
<li><a href="chap_matrix_inverse.html#sec_appl_arms_race" data-scroll="sec_appl_arms_race" class="internal">Application: Modeling an Arms Race</a></li>
<li><a href="chap_matrix_inverse.html#sec_inverse_intro" data-scroll="sec_inverse_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_inverse.html#sec_mtx_invertible" data-scroll="sec_mtx_invertible" class="internal">Invertible Matrices</a></li>
<li><a href="chap_matrix_inverse.html#sec_mtx_inverse" data-scroll="sec_mtx_inverse" class="internal">Finding the Inverse of a Matrix</a></li>
<li><a href="chap_matrix_inverse.html#sec_mtx_inverse_props" data-scroll="sec_mtx_inverse_props" class="internal">Properties of the Matrix Inverse</a></li>
<li><a href="chap_matrix_inverse.html#sec_inverse_exam" data-scroll="sec_inverse_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_inverse.html#sec_inverse_summ" data-scroll="sec_inverse_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_inverse.html#sec_inverse_exer" data-scroll="sec_inverse_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_inverse.html#sec_proj_arms_race" data-scroll="sec_proj_arms_race" class="internal">Project: The Richardson Arms Race Model</a></li>
</ul>
</li>
<li class="link">
<a href="chap_IMT.html" data-scroll="chap_IMT" class="internal"><span class="codenumber">11</span> <span class="title">The Invertible Matrix Theorem</span></a><ul>
<li><a href="chap_IMT.html#sec_imt_intro" data-scroll="sec_imt_intro" class="internal">Introduction</a></li>
<li><a href="chap_IMT.html#sec_imt" data-scroll="sec_imt" class="internal">The Invertible Matrix Theorem</a></li>
<li><a href="chap_IMT.html#sec_imt_exam" data-scroll="sec_imt_exam" class="internal">Examples</a></li>
<li><a href="chap_IMT.html#sec_imt_summ" data-scroll="sec_imt_summ" class="internal">Summary</a></li>
<li><a href="chap_IMT.html#sec_imt_exer" data-scroll="sec_imt_exer" class="internal">Exercises</a></li>
</ul>
</li>
<li class="link part"><a href="part-vector-rn.html" data-scroll="part-vector-rn" class="internal"><span class="codenumber">III</span> <span class="title">The Vector Space <span class="process-math">\(\R^n\)</span></span></a></li>
<li class="link">
<a href="chap_R_n.html" data-scroll="chap_R_n" class="internal"><span class="codenumber">12</span> <span class="title">The Structure of <span class="process-math">\(\R^n\)</span></span></a><ul>
<li><a href="chap_R_n.html#sec_appl_romania" data-scroll="sec_appl_romania" class="internal">Application: Connecting GDP and Consumption in Romania</a></li>
<li><a href="chap_R_n.html#sec_rn_intro" data-scroll="sec_rn_intro" class="internal">Introduction</a></li>
<li><a href="chap_R_n.html#sec_vec_spaces" data-scroll="sec_vec_spaces" class="internal">Vector Spaces</a></li>
<li><a href="chap_R_n.html#sec_sub_space_span" data-scroll="sec_sub_space_span" class="internal">The Subspace Spanned by a Set of Vectors</a></li>
<li><a href="chap_R_n.html#sec_rn_exam" data-scroll="sec_rn_exam" class="internal">Examples</a></li>
<li><a href="chap_R_n.html#sec_rn_summ" data-scroll="sec_rn_summ" class="internal">Summary</a></li>
<li><a href="chap_R_n.html#sec_rn_exer" data-scroll="sec_rn_exer" class="internal">Exercises</a></li>
<li><a href="chap_R_n.html#sec_proj_ls_approx" data-scroll="sec_proj_ls_approx" class="internal">Project: Least Sqaures Linear Approximation</a></li>
</ul>
</li>
<li class="link">
<a href="chap_null_space.html" data-scroll="chap_null_space" class="internal"><span class="codenumber">13</span> <span class="title">The Null Space and Column Space of a Matrix</span></a><ul>
<li><a href="chap_null_space.html#sec_appl_lights_out" data-scroll="sec_appl_lights_out" class="internal">Application: The Lights Out Game</a></li>
<li><a href="chap_null_space.html#sec_null_intro" data-scroll="sec_null_intro" class="internal">Introduction</a></li>
<li><a href="chap_null_space.html#sec_null_kernel" data-scroll="sec_null_kernel" class="internal">The Null Space of a Matrix and the Kernel of a Matrix Transformation</a></li>
<li><a href="chap_null_space.html#sec_column_range" data-scroll="sec_column_range" class="internal">The Column Space of a Matrix and the Range of a Matrix Transformation</a></li>
<li><a href="chap_null_space.html#sec_row_space" data-scroll="sec_row_space" class="internal">The Row Space of a Matrix</a></li>
<li><a href="chap_null_space.html#sec_null_col_base" data-scroll="sec_null_col_base" class="internal">Bases for <span class="process-math">\(\Nul A\)</span> and <span class="process-math">\(\Col A\)</span></a></li>
<li><a href="chap_null_space.html#sec_null_exam" data-scroll="sec_null_exam" class="internal">Examples</a></li>
<li><a href="chap_null_space.html#sec_null_summ" data-scroll="sec_null_summ" class="internal">Summary</a></li>
<li><a href="chap_null_space.html#sec_null_exer" data-scroll="sec_null_exer" class="internal">Exercises</a></li>
<li><a href="chap_null_space.html#sec_proj_lights_out" data-scroll="sec_proj_lights_out" class="internal">Project: Solving the Lights Out Game</a></li>
</ul>
</li>
<li class="link">
<a href="chap_eigenspaces.html" data-scroll="chap_eigenspaces" class="internal"><span class="codenumber">14</span> <span class="title">Eigenspaces of a Matrix</span></a><ul>
<li><a href="chap_eigenspaces.html#sec_appl_pop_dynam" data-scroll="sec_appl_pop_dynam" class="internal">Application: Population Dynamics</a></li>
<li><a href="chap_eigenspaces.html#sec_egspace_intro" data-scroll="sec_egspace_intro" class="internal">Introduction</a></li>
<li><a href="chap_eigenspaces.html#sec_mtx_egspace" data-scroll="sec_mtx_egspace" class="internal">Eigenspaces of Matrix</a></li>
<li><a href="chap_eigenspaces.html#sec_lin_ind_egvec" data-scroll="sec_lin_ind_egvec" class="internal">Linearly Independent Eigenvectors</a></li>
<li><a href="chap_eigenspaces.html#sec_egspace_exam" data-scroll="sec_egspace_exam" class="internal">Examples</a></li>
<li><a href="chap_eigenspaces.html#sec_egspace_summ" data-scroll="sec_egspace_summ" class="internal">Summary</a></li>
<li><a href="chap_eigenspaces.html#sec_egspace_exer" data-scroll="sec_egspace_exer" class="internal">Exercises</a></li>
<li><a href="chap_eigenspaces.html#sec_proj_migration" data-scroll="sec_proj_migration" class="internal">Project: Modeling Population Migration</a></li>
</ul>
</li>
<li class="link">
<a href="chap_bases_dimension.html" data-scroll="chap_bases_dimension" class="internal"><span class="codenumber">15</span> <span class="title">Bases and Dimension</span></a><ul>
<li><a href="chap_bases_dimension.html#sec_appl_latt_crypt" data-scroll="sec_appl_latt_crypt" class="internal">Application: Lattice Based Cryptography</a></li>
<li><a href="chap_bases_dimension.html#sec_base_dim_intro" data-scroll="sec_base_dim_intro" class="internal">Introduction</a></li>
<li><a href="chap_bases_dimension.html#sec_dim_sub_rn" data-scroll="sec_dim_sub_rn" class="internal">The Dimension of a Subspace of <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_bases_dimension.html#sec_cond_basis_subspace" data-scroll="sec_cond_basis_subspace" class="internal">Conditions for a Basis of a Subspace of <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_bases_dimension.html#sec_find_basis_subspace" data-scroll="sec_find_basis_subspace" class="internal">Finding a Basis for a Subspace</a></li>
<li><a href="chap_bases_dimension.html#sec_mtx_rank" data-scroll="sec_mtx_rank" class="internal">Rank of a Matrix</a></li>
<li><a href="chap_bases_dimension.html#sec_base_dim_exam" data-scroll="sec_base_dim_exam" class="internal">Examples</a></li>
<li><a href="chap_bases_dimension.html#sec_base_dim_summ" data-scroll="sec_base_dim_summ" class="internal">Summary</a></li>
<li><a href="chap_bases_dimension.html#sec_base_dim_exer" data-scroll="sec_base_dim_exer" class="internal">Exercises</a></li>
<li><a href="chap_bases_dimension.html#sec_proj_ggh_crypto" data-scroll="sec_proj_ggh_crypto" class="internal">Project: The GGH Cryptosystem</a></li>
</ul>
</li>
<li class="link">
<a href="chap_coordinate_vectors.html" data-scroll="chap_coordinate_vectors" class="internal"><span class="codenumber">16</span> <span class="title">Coordinate Vectors and Change of Basis</span></a><ul>
<li><a href="chap_coordinate_vectors.html#sec_appl_orbits" data-scroll="sec_appl_orbits" class="internal">Application: Describing Orbits of Planets</a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_intro" data-scroll="sec_cob_intro" class="internal">Introduction</a></li>
<li><a href="chap_coordinate_vectors.html#sec_coor_base" data-scroll="sec_coor_base" class="internal">Bases as Coordinate Systems in <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_rn" data-scroll="sec_cob_rn" class="internal">Change of Basis in <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_coordinate_vectors.html#sec_mtx_cob" data-scroll="sec_mtx_cob" class="internal">The Change of Basis Matrix in <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_coordinate_vectors.html#sec_prop_mtx_cob" data-scroll="sec_prop_mtx_cob" class="internal">Properties of the Change of Basis Matrix</a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_exam" data-scroll="sec_cob_exam" class="internal">Examples</a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_summ" data-scroll="sec_cob_summ" class="internal">Summary</a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_exer" data-scroll="sec_cob_exer" class="internal">Exercises</a></li>
<li><a href="chap_coordinate_vectors.html#sec_proj_orbits_cob" data-scroll="sec_proj_orbits_cob" class="internal">Project: Planetary Orbits and Change of Basis</a></li>
</ul>
</li>
<li class="link part"><a href="part-eigen.html" data-scroll="part-eigen" class="internal"><span class="codenumber">IV</span> <span class="title">Eigenvalues and Eigenvectors</span></a></li>
<li class="link">
<a href="chap_determinants.html" data-scroll="chap_determinants" class="internal"><span class="codenumber">17</span> <span class="title">The Determinant</span></a><ul>
<li><a href="chap_determinants.html#sec_appl_area_vol" data-scroll="sec_appl_area_vol" class="internal">Application: Area and Volume</a></li>
<li><a href="chap_determinants.html#sec_det_intro" data-scroll="sec_det_intro" class="internal">Introduction</a></li>
<li><a href="chap_determinants.html#sec_det_square" data-scroll="sec_det_square" class="internal">The Determinant of a Square Matrix</a></li>
<li><a href="chap_determinants.html#sec_cofactors" data-scroll="sec_cofactors" class="internal">Cofactors</a></li>
<li><a href="chap_determinants.html#sec_det_3by3" data-scroll="sec_det_3by3" class="internal">The Determinant of a <span class="process-math">\(3 \times 3\)</span> Matrix</a></li>
<li><a href="chap_determinants.html#sec_det_remember" data-scroll="sec_det_remember" class="internal">Two Devices for Remembering Determinants</a></li>
<li><a href="chap_determinants.html#sec_det_exam" data-scroll="sec_det_exam" class="internal">Examples</a></li>
<li><a href="chap_determinants.html#sec_det_summ" data-scroll="sec_det_summ" class="internal">Summary</a></li>
<li><a href="chap_determinants.html#sec_det_exer" data-scroll="sec_det_exer" class="internal">Exercises</a></li>
<li><a href="chap_determinants.html#sec_proj_det_area_vol" data-scroll="sec_proj_det_area_vol" class="internal">Project: Area and Volume Using Determinants</a></li>
</ul>
</li>
<li class="link">
<a href="chap_characteristic_equation.html" data-scroll="chap_characteristic_equation" class="internal"><span class="codenumber">18</span> <span class="title">The Characteristic Equation</span></a><ul>
<li><a href="chap_characteristic_equation.html#sec_appl_thermo" data-scroll="sec_appl_thermo" class="internal">Application: Modeling the Second Law of Thermodynamics</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq_intro" data-scroll="sec_chareq_intro" class="internal">Introduction</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq" data-scroll="sec_chareq" class="internal">The Characteristic Equation</a></li>
<li><a href="chap_characteristic_equation.html#sec_egspace_geom" data-scroll="sec_egspace_geom" class="internal">Eigenspaces, A Geometric Example</a></li>
<li><a href="chap_characteristic_equation.html#sec_egspace_dims" data-scroll="sec_egspace_dims" class="internal">Dimensions of Eigenspaces</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq_exam" data-scroll="sec_chareq_exam" class="internal">Examples</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq_summ" data-scroll="sec_chareq_summ" class="internal">Summary</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq_exer" data-scroll="sec_chareq_exer" class="internal">Exercises</a></li>
<li><a href="chap_characteristic_equation.html#sec_proj_ehrenfest" data-scroll="sec_proj_ehrenfest" class="internal">Project: The Ehrenfest Model</a></li>
</ul>
</li>
<li class="link">
<a href="chap_diagonalization.html" data-scroll="chap_diagonalization" class="internal"><span class="codenumber">19</span> <span class="title">Diagonalization</span></a><ul>
<li><a href="chap_diagonalization.html#sec_appl_fib_num" data-scroll="sec_appl_fib_num" class="internal">Application: The Fibonacci Numbers</a></li>
<li><a href="chap_diagonalization.html#sec_diag_intro" data-scroll="sec_diag_intro" class="internal">Introduction</a></li>
<li><a href="chap_diagonalization.html#sec_diag" data-scroll="sec_diag" class="internal">Diagonalization</a></li>
<li><a href="chap_diagonalization.html#sec_mtx_similar" data-scroll="sec_mtx_similar" class="internal">Similar Matrices</a></li>
<li><a href="chap_diagonalization.html#sec_sim_mtx_trans" data-scroll="sec_sim_mtx_trans" class="internal">Similarity and Matrix Transformations</a></li>
<li><a href="chap_diagonalization.html#sec_diag_general" data-scroll="sec_diag_general" class="internal">Diagonalization in General</a></li>
<li><a href="chap_diagonalization.html#sec_diag_exam" data-scroll="sec_diag_exam" class="internal">Examples</a></li>
<li><a href="chap_diagonalization.html#sec_diag_summ" data-scroll="sec_diag_summ" class="internal">Summary</a></li>
<li><a href="chap_diagonalization.html#sec_diag_exer" data-scroll="sec_diag_exer" class="internal">Exercises</a></li>
<li><a href="chap_diagonalization.html#sec_proj_binet_fibo" data-scroll="sec_proj_binet_fibo" class="internal">Project: Binet's Formula for the Fibonacci Numbers</a></li>
</ul>
</li>
<li class="link">
<a href="chap_approx_eigenvalues.html" data-scroll="chap_approx_eigenvalues" class="internal"><span class="codenumber">20</span> <span class="title">Approximating Eigenvalues and Eigenvectors</span></a><ul>
<li><a href="chap_approx_eigenvalues.html#sec_appl_leslie_mtx" data-scroll="sec_appl_leslie_mtx" class="internal">Application: Leslie Matrices and Population Modeling</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_app_eigen_intro" data-scroll="sec_app_eigen_intro" class="internal">Introduction</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_power_method" data-scroll="sec_power_method" class="internal">The Power Method</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_power_method_inv" data-scroll="sec_power_method_inv" class="internal">The Inverse Power Method</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_app_eigen_exam" data-scroll="sec_app_eigen_exam" class="internal">Examples</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_app_eigen_summ" data-scroll="sec_app_eigen_summ" class="internal">Summary</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_app_eigen_exer" data-scroll="sec_app_eigen_exer" class="internal">Exercises</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_proj_sheep_herd" data-scroll="sec_proj_sheep_herd" class="internal">Project: Managing a Sheep Herd</a></li>
</ul>
</li>
<li class="link">
<a href="chap_complex_eigenvalues.html" data-scroll="chap_complex_eigenvalues" class="internal"><span class="codenumber">21</span> <span class="title">Complex Eigenvalues</span></a><ul>
<li><a href="chap_complex_eigenvalues.html#sec_appl_gershgorin" data-scroll="sec_appl_gershgorin" class="internal">Application: The Gershgorin Disk Theorem</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen_intro" data-scroll="sec_comp_eigen_intro" class="internal">Introduction</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen" data-scroll="sec_comp_eigen" class="internal">Complex Eigenvalues</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_mtx_rotate_scale" data-scroll="sec_mtx_rotate_scale" class="internal">Rotation and Scaling Matrices</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_mtx_comp_eigen" data-scroll="sec_mtx_comp_eigen" class="internal">Matrices with Complex Eigenvalues</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen_exam" data-scroll="sec_comp_eigen_exam" class="internal">Examples</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen_summ" data-scroll="sec_comp_eigen_summ" class="internal">Summary</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen_exer" data-scroll="sec_comp_eigen_exer" class="internal">Exercises</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_proj_gershgorin" data-scroll="sec_proj_gershgorin" class="internal">Project: Understanding the Gershgorin Disk Theorem</a></li>
</ul>
</li>
<li class="link">
<a href="chap_det_properties.html" data-scroll="chap_det_properties" class="internal"><span class="codenumber">22</span> <span class="title">Properties of Determinants</span></a><ul>
<li><a href="chap_det_properties.html#sec_det_prop_intro" data-scroll="sec_det_prop_intro" class="internal">Introduction</a></li>
<li><a href="chap_det_properties.html#sec_det_row_ops" data-scroll="sec_det_row_ops" class="internal">Elementary Row Operations and Their Effects on the Determinant</a></li>
<li><a href="chap_det_properties.html#sec_mtx_elem" data-scroll="sec_mtx_elem" class="internal">Elementary Matrices</a></li>
<li><a href="chap_det_properties.html#sec_det_geom" data-scroll="sec_det_geom" class="internal">Geometric Interpretation of the Determinant</a></li>
<li><a href="chap_det_properties.html#sec_inv_cramers" data-scroll="sec_inv_cramers" class="internal">An Explicit Formula for the Inverse and Cramer's Rule</a></li>
<li><a href="chap_det_properties.html#sec_det_transpose" data-scroll="sec_det_transpose" class="internal">The Determinant of the Transpose</a></li>
<li><a href="chap_det_properties.html#sec_det_row_swap" data-scroll="sec_det_row_swap" class="internal">Row Swaps and Determinants</a></li>
<li><a href="chap_det_properties.html#sec_cofactor_expand" data-scroll="sec_cofactor_expand" class="internal">Cofactor Expansions</a></li>
<li><a href="chap_det_properties.html#sec_mtx_lu_factor" data-scroll="sec_mtx_lu_factor" class="internal">The LU Factorization of a Matrix</a></li>
<li><a href="chap_det_properties.html#sec_det_prop_exam" data-scroll="sec_det_prop_exam" class="internal">Examples</a></li>
<li><a href="chap_det_properties.html#sec_det_prop_summ" data-scroll="sec_det_prop_summ" class="internal">Summary</a></li>
<li><a href="chap_det_properties.html#sec_det_prop_exer" data-scroll="sec_det_prop_exer" class="internal">Exercises</a></li>
</ul>
</li>
<li class="link part"><a href="part-orthog.html" data-scroll="part-orthog" class="internal"><span class="codenumber">V</span> <span class="title">Orthogonality</span></a></li>
<li class="link">
<a href="chap_dot_product.html" data-scroll="chap_dot_product" class="internal"><span class="codenumber">23</span> <span class="title">The Dot Product in <span class="process-math">\(\R^n\)</span></span></a><ul>
<li><a href="chap_dot_product.html#sec_appl_figs_computer" data-scroll="sec_appl_figs_computer" class="internal">Application: Hidden Figures in Computer Graphics</a></li>
<li><a href="chap_dot_product.html#sec_dot_prod_intro" data-scroll="sec_dot_prod_intro" class="internal">Introduction</a></li>
<li><a href="chap_dot_product.html#sec_dist_vec" data-scroll="sec_dist_vec" class="internal">The Distance Between Vectors</a></li>
<li><a href="chap_dot_product.html#sec_angle_vec" data-scroll="sec_angle_vec" class="internal">The Angle Between Two Vectors</a></li>
<li><a href="chap_dot_product.html#sec_orthog_proj" data-scroll="sec_orthog_proj" class="internal">Orthogonal Projections</a></li>
<li><a href="chap_dot_product.html#sec_orthog_comp" data-scroll="sec_orthog_comp" class="internal">Orthogonal Complements</a></li>
<li><a href="chap_dot_product.html#sec_dot_prod_exam" data-scroll="sec_dot_prod_exam" class="internal">Examples</a></li>
<li><a href="chap_dot_product.html#sec_dot_prod_summ" data-scroll="sec_dot_prod_summ" class="internal">Summary</a></li>
<li><a href="chap_dot_product.html#sec_dot_prod_exer" data-scroll="sec_dot_prod_exer" class="internal">Exercises</a></li>
<li><a href="chap_dot_product.html#sec_proj_back_face" data-scroll="sec_proj_back_face" class="internal">Project: Back-Face Culling</a></li>
</ul>
</li>
<li class="link">
<a href="chap_orthogonal_basis.html" data-scroll="chap_orthogonal_basis" class="internal"><span class="codenumber">24</span> <span class="title">Orthogonal and Orthonormal Bases in <span class="process-math">\(\R^n\)</span></span></a><ul>
<li><a href="chap_orthogonal_basis.html#sec_appl_3d_rotate" data-scroll="sec_appl_3d_rotate" class="internal">Application: Rotations in 3D</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_set_intro" data-scroll="sec_orthog_set_intro" class="internal">Introduction</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_sets" data-scroll="sec_orthog_sets" class="internal">Orthogonal Sets</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_bases_prop" data-scroll="sec_orthog_bases_prop" class="internal">Properties of Orthogonal Bases</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthon_bases" data-scroll="sec_orthon_bases" class="internal">Orthonormal Bases</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_mtx" data-scroll="sec_orthog_mtx" class="internal">Orthogonal Matrices</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_set_exam" data-scroll="sec_orthog_set_exam" class="internal">Examples</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_set_summ" data-scroll="sec_orthog_set_summ" class="internal">Summary</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_set_exer" data-scroll="sec_orthog_set_exer" class="internal">Exercises</a></li>
<li><a href="chap_orthogonal_basis.html#sec_proj_3d_rotate" data-scroll="sec_proj_3d_rotate" class="internal">Project: Understanding Rotations in 3-Space</a></li>
</ul>
</li>
<li class="link">
<a href="chap_gram_schmidt.html" data-scroll="chap_gram_schmidt" class="internal"><span class="codenumber">25</span> <span class="title">Projections onto Subspaces and the Gram-Schmidt Process in <span class="process-math">\(\R^n\)</span></span></a><ul>
<li><a href="chap_gram_schmidt.html#sec_mimo" data-scroll="sec_mimo" class="internal">Application: MIMO Systems</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_intro_noip" data-scroll="sec_gram_schmidt_intro_noip" class="internal">Introduction</a></li>
<li><a href="chap_gram_schmidt.html#sec_proj_subsp_orth" data-scroll="sec_proj_subsp_orth" class="internal">Projections onto Subspaces and Orthogonal Projections</a></li>
<li><a href="chap_gram_schmidt.html#sec_best_approx" data-scroll="sec_best_approx" class="internal">Best Approximations</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_process" data-scroll="sec_gram_schmidt_process" class="internal">The Gram-Schmidt Process</a></li>
<li><a href="chap_gram_schmidt.html#sec_qr_fact" data-scroll="sec_qr_fact" class="internal">The QR Factorization of a Matrix</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_examples" data-scroll="sec_gram_schmidt_examples" class="internal">Examples</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_summ_noips" data-scroll="sec_gram_schmidt_summ_noips" class="internal">Summary</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_exercises" data-scroll="sec_gram_schmidt_exercises" class="internal">Exercises</a></li>
<li><a href="chap_gram_schmidt.html#sec_project_mimo" data-scroll="sec_project_mimo" class="internal">Project: MIMO Systems and Householder Transformations</a></li>
</ul>
</li>
<li class="link">
<a href="chap_least_squares.html" data-scroll="chap_least_squares" class="internal"><span class="codenumber">26</span> <span class="title">Least Squares Approximations</span></a><ul>
<li><a href="chap_least_squares.html#sec_appl_fit_func" data-scroll="sec_appl_fit_func" class="internal">Application: Fitting Functions to Data</a></li>
<li><a href="chap_least_squares.html#sec_ls_intro" data-scroll="sec_ls_intro" class="internal">Introduction</a></li>
<li><a href="chap_least_squares.html#sec_ls_approx" data-scroll="sec_ls_approx" class="internal">Least Squares Approximations</a></li>
<li><a href="chap_least_squares.html#sec_ls_exam" data-scroll="sec_ls_exam" class="internal">Examples</a></li>
<li><a href="chap_least_squares.html#sec_ls_summ" data-scroll="sec_ls_summ" class="internal">Summary</a></li>
<li><a href="chap_least_squares.html#sec_ls_exer" data-scroll="sec_ls_exer" class="internal">Exercises</a></li>
<li><a href="chap_least_squares.html#sec_proj_ls_approx_other" data-scroll="sec_proj_ls_approx_other" class="internal">Project: Other Least Squares Approximations</a></li>
</ul>
</li>
<li class="link part"><a href="part-app-orthog.html" data-scroll="part-app-orthog" class="internal"><span class="codenumber">VI</span> <span class="title">Applications of Orthogonality</span></a></li>
<li class="link">
<a href="chap_orthogonal_diagonalization.html" data-scroll="chap_orthogonal_diagonalization" class="internal"><span class="codenumber">27</span> <span class="title">Orthogonal Diagonalization</span></a><ul>
<li><a href="chap_orthogonal_diagonalization.html#sec_appl_mulit_2nd_deriv" data-scroll="sec_appl_mulit_2nd_deriv" class="internal">Application: The Multivariable Second Derivative Test</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_orthog_diag_intro" data-scroll="sec_orthog_diag_intro" class="internal">Introduction</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_mtx_symm" data-scroll="sec_mtx_symm" class="internal">Symmetric Matrices</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_spec_decomp_symm_mtx" data-scroll="sec_spec_decomp_symm_mtx" class="internal">The Spectral Decomposition of a Symmetric Matrix <span class="process-math">\(A\)</span></a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_orthog_diag_exam" data-scroll="sec_orthog_diag_exam" class="internal">Examples</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_orthog_diag_summ" data-scroll="sec_orthog_diag_summ" class="internal">Summary</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_orthog_diag_exer" data-scroll="sec_orthog_diag_exer" class="internal">Exercises</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_proj_two_var_deriv" data-scroll="sec_proj_two_var_deriv" class="internal">Project: The Second Derivative Test for Functions of Two Variables</a></li>
</ul>
</li>
<li class="link">
<a href="chap_principal_axis_theorem.html" data-scroll="chap_principal_axis_theorem" class="internal"><span class="codenumber">28</span> <span class="title">Quadratic Forms and the Principal Axis Theorem</span></a><ul>
<li><a href="chap_principal_axis_theorem.html#sec_appl_tennis" data-scroll="sec_appl_tennis" class="internal">Application: The Tennis Racket Effect</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_intro" data-scroll="sec_pat_intro" class="internal">Introduction</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_eqs_quad_r2" data-scroll="sec_eqs_quad_r2" class="internal">Equations Involving Quadratic Forms in <span class="process-math">\(\R^2\)</span></a></li>
<li><a href="chap_principal_axis_theorem.html#sec_class_quad_forms" data-scroll="sec_class_quad_forms" class="internal">Classifying Quadratic Forms</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_inner_prod" data-scroll="sec_pat_inner_prod" class="internal">Inner Products</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_exam" data-scroll="sec_pat_exam" class="internal">Examples</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_summ" data-scroll="sec_pat_summ" class="internal">Summary</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_exer" data-scroll="sec_pat_exer" class="internal">Exercises</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_proj_tennis" data-scroll="sec_proj_tennis" class="internal">Project: The Tennis Racket Theorem</a></li>
</ul>
</li>
<li class="link">
<a href="chap_SVD.html" data-scroll="chap_SVD" class="internal"><span class="codenumber">29</span> <span class="title">The Singular Value Decomposition</span></a><ul>
<li><a href="chap_SVD.html#sec_appl_search_engn" data-scroll="sec_appl_search_engn" class="internal">Application: Search Engines and Semantics</a></li>
<li><a href="chap_SVD.html#sec_svd_intro" data-scroll="sec_svd_intro" class="internal">Introduction</a></li>
<li><a href="chap_SVD.html#sec_mtx_op_norm" data-scroll="sec_mtx_op_norm" class="internal">The Operator Norm of a Matrix</a></li>
<li><a href="chap_SVD.html#sec_svd" data-scroll="sec_svd" class="internal">The SVD</a></li>
<li><a href="chap_SVD.html#sec_svd_mtx_spaces" data-scroll="sec_svd_mtx_spaces" class="internal">SVD and the Null, Column, and Row Spaces of a Matrix</a></li>
<li><a href="chap_SVD.html#sec_svd_exam" data-scroll="sec_svd_exam" class="internal">Examples</a></li>
<li><a href="chap_SVD.html#sec_svd_summ" data-scroll="sec_svd_summ" class="internal">Summary</a></li>
<li><a href="chap_SVD.html#sec_svd_exer" data-scroll="sec_svd_exer" class="internal">Exercises</a></li>
<li><a href="chap_SVD.html#sec_proj_indexing" data-scroll="sec_proj_indexing" class="internal">Project: Latent Semantic Indexing</a></li>
</ul>
</li>
<li class="link">
<a href="chap_pseudoinverses.html" data-scroll="chap_pseudoinverses" class="internal"><span class="codenumber">30</span> <span class="title">Using the Singular Value Decomposition</span></a><ul>
<li><a href="chap_pseudoinverses.html#sec_appl_gps" data-scroll="sec_appl_gps" class="internal">Application: Global Positioning System</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudo_intro" data-scroll="sec_pseudo_intro" class="internal">Introduction</a></li>
<li><a href="chap_pseudoinverses.html#sec_img_conpress" data-scroll="sec_img_conpress" class="internal">Image Compression</a></li>
<li><a href="chap_pseudoinverses.html#sec_err_approx_img" data-scroll="sec_err_approx_img" class="internal">Calculating the Error in Approximating an Image</a></li>
<li><a href="chap_pseudoinverses.html#sec_mtx_cond_num" data-scroll="sec_mtx_cond_num" class="internal">The Condition Number of a Matrix</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudoinverses" data-scroll="sec_pseudoinverses" class="internal">Pseudoinverses</a></li>
<li><a href="chap_pseudoinverses.html#sec_ls_approx_SVD" data-scroll="sec_ls_approx_SVD" class="internal">Least Squares Approximations</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudo_exam" data-scroll="sec_pseudo_exam" class="internal">Examples</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudo_summ" data-scroll="sec_pseudo_summ" class="internal">Summary</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudo_exer" data-scroll="sec_pseudo_exer" class="internal">Exercises</a></li>
<li><a href="chap_pseudoinverses.html#sec_proj_gps" data-scroll="sec_proj_gps" class="internal">Project: GPS and Least Squares</a></li>
</ul>
</li>
<li class="link part"><a href="part-vec-spaces.html" data-scroll="part-vec-spaces" class="internal"><span class="codenumber">VII</span> <span class="title">Vector Spaces</span></a></li>
<li class="link">
<a href="chap_vector_spaces.html" data-scroll="chap_vector_spaces" class="internal"><span class="codenumber">31</span> <span class="title">Vector Spaces</span></a><ul>
<li><a href="chap_vector_spaces.html#sec_appl_hat_puzzle" data-scroll="sec_appl_hat_puzzle" class="internal">Application: The Hat Puzzle</a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space_intro" data-scroll="sec_vec_space_intro" class="internal">Introduction</a></li>
<li><a href="chap_vector_spaces.html#sec_space_like_rn" data-scroll="sec_space_like_rn" class="internal">Spaces with Similar Structure to <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space" data-scroll="sec_vec_space" class="internal">Vector Spaces</a></li>
<li><a href="chap_vector_spaces.html#sec_subspaces" data-scroll="sec_subspaces" class="internal">Subspaces</a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space_exam" data-scroll="sec_vec_space_exam" class="internal">Examples</a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space_summ" data-scroll="sec_vec_space_summ" class="internal">Summary</a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space_exer" data-scroll="sec_vec_space_exer" class="internal">Exercises</a></li>
<li><a href="chap_vector_spaces.html#sec_proj_hamming_hat_puzzle" data-scroll="sec_proj_hamming_hat_puzzle" class="internal">Project: Hamming Codes and the Hat Puzzle</a></li>
</ul>
</li>
<li class="link">
<a href="chap_bases.html" data-scroll="chap_bases" class="internal"><span class="codenumber">32</span> <span class="title">Bases for Vector Spaces</span></a><ul>
<li><a href="chap_bases.html#sec_img_compress" data-scroll="sec_img_compress" class="internal">Application: Image Compression</a></li>
<li><a href="chap_bases.html#sec_bases_intro" data-scroll="sec_bases_intro" class="internal">Introduction</a></li>
<li><a href="chap_bases.html#sec_lin_indep" data-scroll="sec_lin_indep" class="internal">Linear Independence</a></li>
<li><a href="chap_bases.html#sec_bases" data-scroll="sec_bases" class="internal">Bases</a></li>
<li><a href="chap_bases.html#sec_basis_vec_space" data-scroll="sec_basis_vec_space" class="internal">Finding a Basis for a Vector Space</a></li>
<li><a href="chap_bases.html#sec_bases_exam" data-scroll="sec_bases_exam" class="internal">Examples</a></li>
<li><a href="chap_bases.html#sec_bases_summ" data-scroll="sec_bases_summ" class="internal">Summary</a></li>
<li><a href="chap_bases.html#sec_bases_exer" data-scroll="sec_bases_exer" class="internal">Exercises</a></li>
<li><a href="chap_bases.html#sec_proj_img_compress" data-scroll="sec_proj_img_compress" class="internal">Project: Image Compression with Wavelets</a></li>
</ul>
</li>
<li class="link active">
<a href="chap_dimension.html" data-scroll="chap_dimension" class="internal"><span class="codenumber">33</span> <span class="title">The Dimension of a Vector Space</span></a><ul>
<li><a href="chap_dimension.html#sec_appl_pca" data-scroll="sec_appl_pca" class="internal">Application: Principal Component Analysis</a></li>
<li><a href="chap_dimension.html#sec_dims_intro" data-scroll="sec_dims_intro" class="internal">Introduction</a></li>
<li><a href="chap_dimension.html#sec_finite_dim_space" data-scroll="sec_finite_dim_space" class="internal">Finite Dimensional Vector Spaces</a></li>
<li><a href="chap_dimension.html#sec_dim_subspace" data-scroll="sec_dim_subspace" class="internal">The Dimension of a Subspace</a></li>
<li><a href="chap_dimension.html#sec_cond_basis_vec_space" data-scroll="sec_cond_basis_vec_space" class="internal">Conditions for a Basis of a Vector Space</a></li>
<li><a href="chap_dimension.html#sec_dims_exam" data-scroll="sec_dims_exam" class="internal">Examples</a></li>
<li><a href="chap_dimension.html#sec_dims_summ" data-scroll="sec_dims_summ" class="internal">Summary</a></li>
<li><a href="chap_dimension.html#sec_dims_exer" data-scroll="sec_dims_exer" class="internal">Exercises</a></li>
<li><a href="chap_dimension.html#sec_proj_pca" data-scroll="sec_proj_pca" class="internal">Project: Understanding Principal Component Analysis</a></li>
</ul>
</li>
<li class="link">
<a href="chap_coordinate_vectors_vector_spaces.html" data-scroll="chap_coordinate_vectors_vector_spaces" class="internal"><span class="codenumber">34</span> <span class="title">Coordinate Vectors and Coordinate Transformations</span></a><ul>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_appl_sums" data-scroll="sec_appl_sums" class="internal">Application: Calculating Sums</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coor_vec_intro" data-scroll="sec_coor_vec_intro" class="internal">Introduction</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coord_trans" data-scroll="sec_coord_trans" class="internal">The Coordinate Transformation</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coord_vec_exam" data-scroll="sec_coord_vec_exam" class="internal">Examples</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coord_vec_summ" data-scroll="sec_coord_vec_summ" class="internal">Summary</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coord_vec_exer" data-scroll="sec_coord_vec_exer" class="internal">Exercises</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_proj_sum_powers" data-scroll="sec_proj_sum_powers" class="internal">Project: Finding Formulas for Sums of Powers</a></li>
</ul>
</li>
<li class="link">
<a href="chap_inner_products.html" data-scroll="chap_inner_products" class="internal"><span class="codenumber">35</span> <span class="title">Inner Product Spaces</span></a><ul>
<li><a href="chap_inner_products.html#sec_appl_fourier" data-scroll="sec_appl_fourier" class="internal">Application: Fourier Series</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_intro" data-scroll="sec_inner_prod_intro" class="internal">Introduction</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_spaces" data-scroll="sec_inner_prod_spaces" class="internal">Inner Product Spaces</a></li>
<li><a href="chap_inner_products.html#sec_vec_length" data-scroll="sec_vec_length" class="internal">The Length of a Vector</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_orthog" data-scroll="sec_inner_prod_orthog" class="internal">Orthogonality in Inner Product Spaces</a></li>
<li><a href="chap_inner_products.html#sec_inner_prog_orthog_bases" data-scroll="sec_inner_prog_orthog_bases" class="internal">Orthogonal and Orthonormal Bases in Inner Product Spaces</a></li>
<li><a href="chap_inner_products.html#sec_orthog_proj_subspace" data-scroll="sec_orthog_proj_subspace" class="internal">Orthogonal Projections onto Subspaces</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_approx" data-scroll="sec_inner_prod_approx" class="internal">Best Approximations in Inner Product Spaces</a></li>
<li><a href="chap_inner_products.html#sec_orthog_comp_ip" data-scroll="sec_orthog_comp_ip" class="internal">Orthogonal Complements</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_exam" data-scroll="sec_inner_prod_exam" class="internal">Examples</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_summ" data-scroll="sec_inner_prod_summ" class="internal">Summary</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_exer" data-scroll="sec_inner_prod_exer" class="internal">Exercises</a></li>
<li><a href="chap_inner_products.html#sec_proj_fourier" data-scroll="sec_proj_fourier" class="internal">Project: Fourier Series and Musical Tones</a></li>
</ul>
</li>
<li class="link">
<a href="chap_gram_schmidt_ips.html" data-scroll="chap_gram_schmidt_ips" class="internal"><span class="codenumber">36</span> <span class="title">The Gram-Schmidt Process in Inner Product Spaces</span></a><ul>
<li><a href="chap_gram_schmidt_ips.html#sec_appl_gaussian_quad" data-scroll="sec_appl_gaussian_quad" class="internal">Application: Gaussian Quadrature</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_intro" data-scroll="sec_gram_schmidt_intro" class="internal">Introduction</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_inner_prod" data-scroll="sec_gram_schmidt_inner_prod" class="internal">The Gram-Schmidt Process using Inner Products</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_exam" data-scroll="sec_gram_schmidt_exam" class="internal">Examples</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_summ_ips" data-scroll="sec_gram_schmidt_summ_ips" class="internal">Summary</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_exer" data-scroll="sec_gram_schmidt_exer" class="internal">Exercises</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_proj_gaussian_quad" data-scroll="sec_proj_gaussian_quad" class="internal">Project: Gaussian Quadrature and Legendre Polynomials</a></li>
</ul>
</li>
<li class="link part"><a href="part-lin-trans.html" data-scroll="part-lin-trans" class="internal"><span class="codenumber">VIII</span> <span class="title">Linear Transformations</span></a></li>
<li class="link">
<a href="chap_linear_transformation.html" data-scroll="chap_linear_transformation" class="internal"><span class="codenumber">37</span> <span class="title">Linear Transformations</span></a><ul>
<li><a href="chap_linear_transformation.html#sec_appl_fractals" data-scroll="sec_appl_fractals" class="internal">Application: Fractals</a></li>
<li><a href="chap_linear_transformation.html#sec_lin_trans_intro" data-scroll="sec_lin_trans_intro" class="internal">Introduction</a></li>
<li><a href="chap_linear_transformation.html#sec_onto_oneone" data-scroll="sec_onto_oneone" class="internal">Onto and One-to-One Transformations</a></li>
<li><a href="chap_linear_transformation.html#sec_kernel_range" data-scroll="sec_kernel_range" class="internal">The Kernel and Range of Linear Transformation</a></li>
<li><a href="chap_linear_transformation.html#sec_isomorph" data-scroll="sec_isomorph" class="internal">Isomorphisms</a></li>
<li><a href="chap_linear_transformation.html#sec_lin_trans_exam" data-scroll="sec_lin_trans_exam" class="internal">Examples</a></li>
<li><a href="chap_linear_transformation.html#sec_lin_trans_summ" data-scroll="sec_lin_trans_summ" class="internal">Summary</a></li>
<li><a href="chap_linear_transformation.html#sec_lin_trans_exer" data-scroll="sec_lin_trans_exer" class="internal">Exercises</a></li>
<li><a href="chap_linear_transformation.html#sec_proj_fractals" data-scroll="sec_proj_fractals" class="internal">Project: Fractals via Iterated Function Systems</a></li>
</ul>
</li>
<li class="link">
<a href="chap_transformation_matrix.html" data-scroll="chap_transformation_matrix" class="internal"><span class="codenumber">38</span> <span class="title">The Matrix of a Linear Transformation</span></a><ul>
<li><a href="chap_transformation_matrix.html#sec_appl_secret" data-scroll="sec_appl_secret" class="internal">Application: Secret Sharing Algorithms</a></li>
<li><a href="chap_transformation_matrix.html#sec_mtxof_trans_intro" data-scroll="sec_mtxof_trans_intro" class="internal">Introduction</a></li>
<li><a href="chap_transformation_matrix.html#sec_trans_rn_rm" data-scroll="sec_trans_rn_rm" class="internal">Linear Transformations from <span class="process-math">\(\R^n\)</span> to <span class="process-math">\(\R^m\)</span></a></li>
<li><a href="chap_transformation_matrix.html#sec_mtx_lin_trans" data-scroll="sec_mtx_lin_trans" class="internal">The Matrix of a Linear Transformation</a></li>
<li><a href="chap_transformation_matrix.html#sec_ker_mtx" data-scroll="sec_ker_mtx" class="internal">A Connection between <span class="process-math">\(\Ker(T)\)</span> and a Matrix Representation of <span class="process-math">\(T\)</span></a></li>
<li><a href="chap_transformation_matrix.html#sec_mtxof_trans_exam" data-scroll="sec_mtxof_trans_exam" class="internal">Examples</a></li>
<li><a href="chap_transformation_matrix.html#sec_mtxof_trans_summ" data-scroll="sec_mtxof_trans_summ" class="internal">Summary</a></li>
<li><a href="chap_transformation_matrix.html#sec_mtxof_trans_exer" data-scroll="sec_mtxof_trans_exer" class="internal">Exercises</a></li>
<li><a href="chap_transformation_matrix.html#sec_proj_secret" data-scroll="sec_proj_secret" class="internal">Project: Shamir's Secret Sharing and Lagrange Polynomials</a></li>
</ul>
</li>
<li class="link">
<a href="chap_transformations_eigenvalues.html" data-scroll="chap_transformations_eigenvalues" class="internal"><span class="codenumber">39</span> <span class="title">Eigenvalues of Linear Transformations</span></a><ul>
<li><a href="chap_transformations_eigenvalues.html#sec_appl_diff_eq" data-scroll="sec_appl_diff_eq" class="internal">Application: Linear Differential Equations</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_eigen_trans_intro" data-scroll="sec_eigen_trans_intro" class="internal">Introduction</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_find_eigen_trans" data-scroll="sec_find_eigen_trans" class="internal">Finding Eigenvalues and Eigenvectors of Linear Transformations</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_diagonal" data-scroll="sec_diagonal" class="internal">Diagonalization</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_eigen_trans_exam" data-scroll="sec_eigen_trans_exam" class="internal">Examples</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_eigen_trans_summ" data-scroll="sec_eigen_trans_summ" class="internal">Summary</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_eigen_trans_exer" data-scroll="sec_eigen_trans_exer" class="internal">Exercises</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_proj_diff_eq" data-scroll="sec_proj_diff_eq" class="internal">Project: Linear Transformations and Differential Equations</a></li>
</ul>
</li>
<li class="link">
<a href="chap_JCF.html" data-scroll="chap_JCF" class="internal"><span class="codenumber">40</span> <span class="title">The Jordan Canonical Form</span></a><ul>
<li><a href="chap_JCF.html#sec_appl_epidemic" data-scroll="sec_appl_epidemic" class="internal">Application: The Bailey Model of an Epidemic</a></li>
<li><a href="chap_JCF.html#sec_jordan_intro" data-scroll="sec_jordan_intro" class="internal">Introduction</a></li>
<li><a href="chap_JCF.html#sec_eigen_dne" data-scroll="sec_eigen_dne" class="internal">When an Eigenvalue Decomposition Does Not Exist</a></li>
<li><a href="chap_JCF.html#sec_gen_eigen_jordan" data-scroll="sec_gen_eigen_jordan" class="internal">Generalized Eigenvectors and the Jordan Canonical Form</a></li>
<li><a href="chap_JCF.html#sec_mtx_jordan_geom" data-scroll="sec_mtx_jordan_geom" class="internal">Geometry of Matrix Transformations using the Jordan Canonical Form</a></li>
<li><a href="chap_JCF.html#sec_jordan_proof" data-scroll="sec_jordan_proof" class="internal">Proof of the Existence of the Jordan Canonical Form</a></li>
<li><a href="chap_JCF.html#sec_mtx_nilpotent" data-scroll="sec_mtx_nilpotent" class="internal">Nilpotent Matrices and Invariant Subspaces</a></li>
<li><a href="chap_JCF.html#sec_jordan" data-scroll="sec_jordan" class="internal">The Jordan Canonical Form</a></li>
<li><a href="chap_JCF.html#sec_jordan_exam" data-scroll="sec_jordan_exam" class="internal">Examples</a></li>
<li><a href="chap_JCF.html#sec_jordan_summ" data-scroll="sec_jordan_summ" class="internal">Summary</a></li>
<li><a href="chap_JCF.html#sec_jordan_exer" data-scroll="sec_jordan_exer" class="internal">Exercises</a></li>
<li><a href="chap_JCF.html#sec_proj_epidemic" data-scroll="sec_proj_epidemic" class="internal">Project: Modeling an Epidemic</a></li>
</ul>
</li>
<li class="link backmatter"><a href="backmatter-1.html" data-scroll="backmatter-1" class="internal"><span class="title">Back Matter</span></a></li>
<li class="link">
<a href="app_complex_numbers.html" data-scroll="app_complex_numbers" class="internal"><span class="codenumber">A</span> <span class="title">Complex Numbers</span></a><ul>
<li><a href="app_complex_numbers.html#sec_complex_numbers" data-scroll="sec_complex_numbers" class="internal">Complex Numbers</a></li>
<li><a href="app_complex_numbers.html#sec_conj_modulus" data-scroll="sec_conj_modulus" class="internal">Conjugates and Modulus</a></li>
<li><a href="app_complex_numbers.html#sec_complex_vect" data-scroll="sec_complex_vect" class="internal">Complex Vectors</a></li>
</ul>
</li>
<li class="link"><a href="app_answers.html" data-scroll="app_answers" class="internal"><span class="codenumber">B</span> <span class="title">Answers and Hints for Selected Exercises</span></a></li>
<li class="link"><a href="index-1.html" data-scroll="index-1" class="internal"><span class="title">Index</span></a></li>
</ul></nav><div class="extras"><nav><a class="pretext-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content">
<section class="chapter" id="chap_dimension"><h2 class="heading">
<span class="type">Chapter</span> <span class="codenumber">33</span> <span class="title">The Dimension of a Vector Space</span>
</h2>
<section class="introduction" id="introduction-560"><article class="objectives goal-like" id="objectives-33"><h3 class="heading"><span class="type">Focus Questions</span></h3>
<div class="introduction" id="introduction-561"><p id="p-5725">By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.</p></div>
<ul class="disc">
<li id="li-868"><p id="p-5726">What is a finite dimensional vector space?</p></li>
<li id="li-869"><p id="p-5727">What is the dimension of a finite dimensional vector space? What important result about bases of finite dimensional vector spaces makes dimension well-defined?</p></li>
<li id="li-870"><p id="p-5728">What must be true about any linearly independent subset of <span class="process-math">\(n\)</span> vectors in a vector space with dimension <span class="process-math">\(n\text{?}\)</span> Why?</p></li>
<li id="li-871"><p id="p-5729">What must be true about any subset of <span class="process-math">\(n\)</span> vectors in a vector space with dimension <span class="process-math">\(n\)</span> that spans the vector space? Why?</p></li>
</ul></article></section><section class="section" id="sec_appl_pca"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">Application: Principal Component Analysis</span>
</h3>
<p id="p-5730">The discipline of statistics is based on the idea of analyzing data. In large data sets it is usually the case that one wants to understand the relationships between the different data in the set. This can be difficult to do when the data set is large and it is impossible to visually examine the data for patterns. Principal Component Analysis (PCA) is a tool for identifying and representing underlying patterns in large data sets, and PCA has been called one of the most important and valuable linear algebra tools for statistical analysis. PCA is used to transform a collection of variables into a (usually smaller) number of uncorrelated variables called principal components. The principal components form the most meaningful basis from which to view data by removing extraneous information and revealing underlying relationships in the data. This presents a framework for how to reduce a complex data set to a lower dimension while retaining the important attributes of the data set. The output helps the experimenter determine which dynamics in the data are important and which can be ignored.</p></section><section class="section" id="sec_dims_intro"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">Introduction</span>
</h3>
<p id="p-5731">In <a href="chap_bases_dimension.html" class="internal" title="Chapter 15: Bases and Dimension">Chapter 15</a> we learned that any two bases for a subspace of <span class="process-math">\(\R^n\)</span> contain the same number of vectors. This allowed us to define the dimension of a subspace of <span class="process-math">\(\R^n\text{.}\)</span> In this section we extend the arguments we made in <a href="chap_bases_dimension.html" class="internal" title="Chapter 15: Bases and Dimension">Chapter 15</a> to arbitrary vector spaces and define the dimension of a vector space.</p>
<article class="exploration project-like" id="pa_5_c"><h4 class="heading">
<span class="type">Preview Activity</span><span class="space"> </span><span class="codenumber">33.1</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-562">
<p id="p-5732">The main tool we used to prove that any two bases for a subspace of <span class="process-math">\(\R^n\)</span> must contain the same number of elements was <a href="" class="xref" data-knowl="./knowl/thm_3_d_1.html" title="Theorem 15.1">Theorem 15.1</a>. In this preview activity we will show that the same argument can be used for vector spaces. More specifically, we will prove a special case of the following theorem generalizing <a href="" class="xref" data-knowl="./knowl/thm_3_d_1.html" title="Theorem 15.1">Theorem 15.1</a>.</p>
<article class="theorem theorem-like" id="thm_5_c_1"><h5 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">33.1</span><span class="period">.</span>
</h5>
<p id="p-5733">If <span class="process-math">\(V\)</span> is a vector space with a basis <span class="process-math">\(\CB = \{\vv_1, \vv_2, \ldots, \vv_k\}\)</span> of <span class="process-math">\(k\)</span> vectors, then any subset of <span class="process-math">\(V\)</span> containing more than <span class="process-math">\(k\)</span> vectors is linearly dependent.</p></article><p id="p-5734">Suppose <span class="process-math">\(V\)</span> is a vector space with basis <span class="process-math">\(\CB = \{\vv_1, \vv_2\}\text{.}\)</span> Consider the set <span class="process-math">\(U=\{\vu_1, \vu_2, \vu_3\}\)</span> of vectors in <span class="process-math">\(V\text{.}\)</span> We will show that <span class="process-math">\(U\)</span> is linearly dependent using a similar approach to the <a href="" class="xref" data-knowl="./knowl/pa_3_d.html" title="Preview Activity 15.1">Preview Activity 15.1</a>.</p>
</div>
<article class="task exercise-like" id="task-1929"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5735">What vector equation involving <span class="process-math">\(\vu_1, \vu_2, \vu_3\)</span> do we need to solve to determine linear independence/dependence of these vectors? Use <span class="process-math">\(x_1, x_2, x_3\)</span> for coefficients.</p></article><article class="task exercise-like" id="task-1930"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5736">Since <span class="process-math">\(\CB\)</span> is a basis of <span class="process-math">\(V\text{,}\)</span> it spans <span class="process-math">\(V\text{.}\)</span> Using this information, rewrite the vectors <span class="process-math">\(\vu_i\)</span> in terms of <span class="process-math">\(\vv_j\)</span> and substitute into the above equation to obtain another equation in terms of <span class="process-math">\(\vv_j\text{.}\)</span></p></article><article class="task exercise-like" id="task-1931"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-5737">Since <span class="process-math">\(\CB\)</span> is a basis of <span class="process-math">\(V\text{,}\)</span> the vectors <span class="process-math">\(\vv_1, \vv_2\)</span> are linearly independent. Using the equation in the previous part, determine what this means about the coefficients <span class="process-math">\(x_1, x_2, x_3\text{.}\)</span></p></article><article class="task exercise-like" id="task-1932"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<p id="p-5738">Express the conditions on <span class="process-math">\(x_1, x_2, x_3\)</span> in the form of a matrix-vector equation. Explain why there are infinitely many solutions for <span class="process-math">\(x_i\)</span>'s and why this means the vectors <span class="process-math">\(\vu_1, \vu_2, \vu_3\)</span> are linearly dependent.</p></article></article></section><section class="section" id="sec_finite_dim_space"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">Finite Dimensional Vector Spaces</span>
</h3>
<p id="p-5739"><a href="" class="xref" data-knowl="./knowl/thm_5_c_1.html" title="Theorem 33.1">Theorem 33.1</a> shows that if sets <span class="process-math">\(\CB_1\)</span> and <span class="process-math">\(\CB_2\)</span> are finite bases for a vector space <span class="process-math">\(V\text{,}\)</span> which are linearly independent by definition, then each cannot contain more elements than the other, so the number of elements in each basis must be equal.</p>
<article class="theorem theorem-like" id="thm_5_c_2"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">33.2</span><span class="period">.</span>
</h4>
<p id="p-5740">If a non-trivial vector space <span class="process-math">\(V\)</span> has a basis of <span class="process-math">\(n\)</span> vectors, then every basis of <span class="process-math">\(V\)</span> contains exactly <span class="process-math">\(n\)</span> vectors.</p></article><p id="p-5741"><a href="" class="xref" data-knowl="./knowl/thm_5_c_2.html" title="Theorem 33.2">Theorem 33.2</a> states that if a vector space <span class="process-math">\(V\)</span> has a basis with a finite number of vectors, then the number of vectors in a basis for that vector space is a well-defined number. In other words, the number of vectors in a basis is an <dfn class="terminology">invariant</dfn> of the vector space. This important number is given a name.</p>
<article class="definition definition-like" id="definition-74"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">33.3</span><span class="period">.</span>
</h4>
<p id="p-5742">A <dfn class="terminology">finite-dimensional</dfn> vector space is a vector space that can be spanned by a finite number of vectors. The <dfn class="terminology">dimension</dfn> of a non-trivial finite-dimensional vector space is the number of vectors in a basis for <span class="process-math">\(V\text{.}\)</span> The dimension of the trivial vector space is defined to be 0.</p></article><p id="p-5743">We denote the dimension of a finite dimensional vector space <span class="process-math">\(V\)</span> by <span class="process-math">\(\dim(V)\text{.}\)</span></p>
<p id="p-5744">Not every vector space is finite dimensional. We have seen, for example, that the vector space <span class="process-math">\(\pol\)</span> of all polynomials, regardless of degree, is not a finite-dimensional vector space. In fact, the polynomials</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
1, t, t^2, \ldots, t^n, \ldots
\end{equation*}
</div>
<p class="continuation">are linearly independent, so <span class="process-math">\(\pol\)</span> has an infinite linearly independent set and therefore has no finite basis. A vector space that has an infinite basis is called an <dfn class="terminology">infinite dimensional</dfn> vector space.</p>
<article class="activity project-like" id="act_5_c_1"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">33.2</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-563"><p id="p-5745">Since columns of the <span class="process-math">\(n \times n\)</span> identity matrix span <span class="process-math">\(\R^n\)</span> and are linearly independent, the columns of <span class="process-math">\(I_n\)</span> form a basis for <span class="process-math">\(\R^n\)</span> (the standard basis). Consequently, we have that <span class="process-math">\(\dim(\R^n) = n\text{.}\)</span> In this activity we determine the dimensions of other familiar vector spaces. Find the dimensions of each of the indicated vector spaces. Verify your answers.</p></div>
<article class="task exercise-like" id="task-1933"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5746"><span class="process-math">\(\pol_1\)</span></p></article><article class="task exercise-like" id="task-1934"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5747"><span class="process-math">\(\pol_2\)</span></p></article><article class="task exercise-like" id="task-1935"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-5748"><span class="process-math">\(\pol_n\)</span></p></article><article class="task exercise-like" id="task-1936"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<p id="p-5749"><span class="process-math">\(\M_{2 \times 3}\)</span></p></article><article class="task exercise-like" id="task-1937"><h5 class="heading"><span class="codenumber">(e)</span></h5>
<p id="p-5750"><span class="process-math">\(\M_{3 \times 4}\)</span></p></article><article class="task exercise-like" id="task-1938"><h5 class="heading"><span class="codenumber">(f)</span></h5>
<p id="p-5751"><span class="process-math">\(\M_{k \times n}\)</span></p></article></article><p id="p-5752">Finding the dimension of a finite-dimensional vector space amounts to finding a basis for the space.</p>
<article class="activity project-like" id="act_5_c_2"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">33.3</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-564"><p id="p-5753">Let <span class="process-math">\(W = \{(a+b) + (a-b)t + (2a+3b)t^2 \mid a, b \text{ are scalars } \}\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1939"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5754">Find a finite set of polynomials in <span class="process-math">\(W\)</span> that span <span class="process-math">\(W\text{.}\)</span></p></article><article class="task exercise-like" id="task-1940"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5755">Determine if the spanning set from part (a) is linearly independent or dependent. Clearly explain your process.</p></article><article class="task exercise-like" id="task-1941"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-5756">What is <span class="process-math">\(\dim(W)\text{?}\)</span> Explain.</p></article></article></section><section class="section" id="sec_dim_subspace"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">The Dimension of a Subspace</span>
</h3>
<p id="p-5757">Every subspace of a finite-dimensional vector space is a vector space, and since a subspace is contained in a vector space it is natural to think that the dimension of a subspace should be less than or equal to the dimension of the larger vector space. We verify that fact in this section.</p>
<article class="activity project-like" id="act_5_c_3"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">33.4</span><span class="period">.</span>
</h4>
<p id="p-5758">Let <span class="process-math">\(V\)</span> be a finite dimensional vector space of dimension <span class="process-math">\(n\)</span> and let <span class="process-math">\(W\)</span> be a subspace of <span class="process-math">\(V\text{.}\)</span> Explain why <span class="process-math">\(W\)</span> cannot have dimension larger than <span class="process-math">\(\dim(V)\text{,}\)</span> and if <span class="process-math">\(W \neq V\)</span> then <span class="process-math">\(\dim(W) \lt \dim(V)\text{.}\)</span></p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-88" id="hint-88"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-88"><div class="hint solution-like"><p id="p-5759">Use <a href="" class="xref" data-knowl="./knowl/thm_5_c_1.html" title="Theorem 33.1">Theorem 33.1</a>.</p></div></div>
</div></article></section><section class="section" id="sec_cond_basis_vec_space"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">Conditions for a Basis of a Vector Space</span>
</h3>
<p id="p-5760">There are two items we need to confirm before we can state that a subset <span class="process-math">\(\CB\)</span> of a subspace <span class="process-math">\(W\)</span> of a vector space is a basis for <span class="process-math">\(W\text{:}\)</span> the set <span class="process-math">\(\CB\)</span> must be linearly independent and span <span class="process-math">\(W\text{.}\)</span> We can reduce the amount of work it takes to show that a set is a basis if we know the dimension of the vector space in advance.</p>
<article class="activity project-like" id="act_5_c_5"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">33.5</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-565"><p id="p-5761">Let <span class="process-math">\(W\)</span> be a subspace of a vector space <span class="process-math">\(V\)</span> with <span class="process-math">\(\dim(W) = k\text{.}\)</span> We know that every basis of <span class="process-math">\(W\)</span> contains exactly <span class="process-math">\(k\)</span> vectors.</p></div>
<article class="task exercise-like" id="task-1942"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<div class="introduction" id="introduction-566"><p id="p-5762">Suppose that <span class="process-math">\(S\)</span> is a subset of <span class="process-math">\(W\)</span> that contains <span class="process-math">\(k\)</span> vectors and is linearly independent. In this part of the activity we will show that <span class="process-math">\(S\)</span> must span <span class="process-math">\(W\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1943"><h6 class="heading"><span class="codenumber">(i)</span></h6>
<p id="p-5763">Suppose that <span class="process-math">\(S\)</span> does not span <span class="process-math">\(W\text{.}\)</span> Explain why this implies that <span class="process-math">\(W\)</span> contains a set of <span class="process-math">\(k+1\)</span> linearly independent vectors.</p></article><article class="task exercise-like" id="task-1944"><h6 class="heading"><span class="codenumber">(ii)</span></h6>
<p id="p-5764">Explain why the result of i tells us that <span class="process-math">\(S\)</span> is a basis for <span class="process-math">\(W\text{.}\)</span></p></article></article><article class="task exercise-like" id="task-1945"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<div class="introduction" id="introduction-567"><p id="p-5765">Now suppose that <span class="process-math">\(S\)</span> is a subset of <span class="process-math">\(W\)</span> with <span class="process-math">\(k\)</span> vectors that spans <span class="process-math">\(W\text{.}\)</span> In this part of the activity we will show that <span class="process-math">\(S\)</span> must be linearly independent.</p></div>
<article class="task exercise-like" id="task-1946"><h6 class="heading"><span class="codenumber">(i)</span></h6>
<p id="p-5766">Suppose that <span class="process-math">\(S\)</span> is not linearly independent. Explain why we can then find a proper subset of <span class="process-math">\(S\)</span> that is linearly independent but has the same span as <span class="process-math">\(S\text{.}\)</span></p></article><article class="task exercise-like" id="task-1947"><h6 class="heading"><span class="codenumber">(ii)</span></h6>
<p id="p-5767">Explain why the result of i tells us that <span class="process-math">\(S\)</span> is a basis for <span class="process-math">\(W\text{.}\)</span></p></article></article></article><p id="p-5768">The result of <a href="" class="xref" data-knowl="./knowl/act_5_c_5.html" title="Activity 33.5">Activity 33.5</a> is summarized in the following theorem (compare to <a href="" class="xref" data-knowl="./knowl/thm_3_d_basis_properties.html" title="Theorem 15.4">Theorem 15.4</a>).</p>
<article class="theorem theorem-like" id="theorem-84"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">33.4</span><span class="period">.</span>
</h4>
<p id="p-5769">Let <span class="process-math">\(W\)</span> be a subspace of dimension <span class="process-math">\(k\)</span> of a vector space <span class="process-math">\(V\)</span> and let <span class="process-math">\(S\)</span> be a subset of <span class="process-math">\(W\)</span> containing exactly <span class="process-math">\(k\)</span> vectors.</p>
<ol class="decimal">
<li id="li-872"><p id="p-5770">If <span class="process-math">\(S\)</span> is linearly independent, then <span class="process-math">\(S\)</span> is a basis for <span class="process-math">\(W\text{.}\)</span></p></li>
<li id="li-873"><p id="p-5771">If <span class="process-math">\(S\)</span> spans <span class="process-math">\(W\text{,}\)</span> then <span class="process-math">\(S\)</span> is a basis for <span class="process-math">\(W\text{.}\)</span></p></li>
</ol></article></section><section class="section" id="sec_dims_exam"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">Examples</span>
</h3>
<p id="p-5772">What follows are worked examples that use the concepts from this section.</p>
<article class="example example-like" id="example-69"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">33.5</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-568"><p id="p-5773">Find a basis and dimension for each of the indicated subspaces of the given vector spaces.</p></div>
<article class="task exercise-like" id="task-1948"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5774"><span class="process-math">\(\{a+b(t+t^2) : a, b \in \R\}\)</span> in <span class="process-math">\(\pol_2\)</span></p>
<div class="solution solution-like" id="solution-197">
<h5 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h5>
<p id="p-5775">Let <span class="process-math">\(W = \{a+b(t+t^2) : a, b \in \R\}\text{.}\)</span> Every element in <span class="process-math">\(W\)</span> has the form</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
a+b(t+t^2) = a(1) + b(t+t^2)\text{.}
\end{equation*}
</div>
<p class="continuation">So <span class="process-math">\(W = \Span\{1, t+t^2\}\text{.}\)</span> Since neither <span class="process-math">\(1\)</span> nor <span class="process-math">\(t+t^2\)</span> is a scalar multiple of the other, the set <span class="process-math">\(\{1, t+t^2\}\)</span> is linearly independent. Thus, <span class="process-math">\(\{1, t+t^2\}\)</span> is a basis for <span class="process-math">\(W\)</span> and <span class="process-math">\(\dim(W) = 2\text{.}\)</span></p>
</div></article><article class="task exercise-like" id="task-1949"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5776"><span class="process-math">\(\Span\left\{1, \frac{1}{1+x^2}, \frac{2+x^2}{1+x^2}, \arctan(x)\right\}\)</span> in <span class="process-math">\(\F\)</span></p>
<div class="solution solution-like" id="solution-198">
<h5 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h5>
<p id="p-5777">Let <span class="process-math">\(W = \Span\left\{1, \frac{1}{1+x^2}, \frac{2+x^2}{1+x^2}, \arctan(x)\right\}\text{.}\)</span> To find a basis for <span class="process-math">\(W\text{,}\)</span> we find a linearly independent subset of <span class="process-math">\(\left\{1, \frac{1}{1+x^2}, \frac{2+x^2}{1+x^2}, \arctan(x)\right\}\text{.}\)</span> Consider the equation</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
c_1 (1) + c_2\left(\frac{1}{1+x^2}\right) + c_3\left(\frac{2+x^2}{1+x^2}\right) + c_4 \arctan(x) = 0\text{.}
\end{equation*}
</div>
<p class="continuation">To find the weights <span class="process-math">\(c_i\)</span> for which this equality of functions holds, we use the fact that the we must have equality for every <span class="process-math">\(x\text{.}\)</span> So we pick four different values for <span class="process-math">\(x\)</span> to obtain a linear system that we can solve for the weights. Evaluating both sides of the equation at <span class="process-math">\(x=0\text{,}\)</span> <span class="process-math">\(x=1\text{,}\)</span> <span class="process-math">\(x=-1\text{,}\)</span> and <span class="process-math">\(x=2\)</span> yields the equations</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-222">
\begin{align*}
{}c_1   \amp {}+{}   \amp {}c_2    \amp {}+{}  \amp {2}c_3  \amp {}  \amp {}    \amp = \ \amp 0\amp {}\\
{}c_1   \amp {}+{}   \amp {\frac{1}{2}}c_2  \amp {}+{}    \amp {\frac{3}{2}}c_3    \amp {}+{}  \amp {\frac{\pi}{4}}c_4    \amp = \ \amp 0\amp {}\\
{}c_1   \amp {}+{}   \amp {\frac{1}{2}}c_2  \amp {}+{}    \amp {\frac{3}{2}}c_3    \amp {}-{}  \amp {\frac{\pi}{4}}c_4    \amp = \ \amp 0\amp {}\\
{}c_1   \amp {}+{}   \amp {\frac{1}{5}}c_2  \amp {}+{}    \amp {\frac{6}{5}}c_3    \amp {}+{}  \amp {\arctan(2)}c_4    \amp = \ \amp 0\amp {.}
\end{align*}
</div>
<p class="continuation">The reduced row echelon form of the coefficient matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\left[  \begin{array}{cccr} 1\amp 1\amp 2\amp 0 \\ 1\amp \frac{1}{2}\amp \frac{3}{2}\amp \frac{\pi}{4} \\ 1\amp \frac{1}{2}\amp \frac{3}{2}\amp -\frac{\pi}{4} \\ 1\amp \frac{1}{5}\amp \frac{6}{5}\amp \arctan(2) \end{array}  \right]
\end{equation*}
</div>
<p class="continuation">is <span class="process-math">\(\left[ \begin{array}{cccc} 1\amp 0\amp 1\amp 0 \\ 0\amp 1\amp 1\amp 0 \\ 0\amp 0\amp 0\amp 1 \\ 0\amp 0\amp 0\amp 0 \end{array}  \right]\text{.}\)</span> The general solution to this linear system is <span class="process-math">\(c_1 = c_2 = -c_3\)</span> and <span class="process-math">\(c_4 = 0\text{.}\)</span> Notice that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
(-1)(1) + (-1)\left( \frac{1}{1+x^2}\right) + \frac{2+x^2}{1+x^2} = 0
\end{equation*}
</div>
<p class="continuation">or</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\frac{2+x^2}{1+x^2} = 1 + \left( \frac{1}{1+x^2}\right)\text{,}
\end{equation*}
</div>
<p class="continuation">so <span class="process-math">\(\frac{2+x^2}{1+x^2}\)</span> is a linear combination of the other vectors. The vectors corresponding to the pivot columns are linearly independent, so it follows that <span class="process-math">\(1\text{,}\)</span> <span class="process-math">\(\frac{1}{1+x^2}\text{,}\)</span> and <span class="process-math">\(\arctan(x)\)</span> are linearly independent. We conclude that <span class="process-math">\(\left\{1, \frac{1}{1+x^2},  \arctan(x)\right\}\)</span> is a basis for <span class="process-math">\(W\)</span> and <span class="process-math">\(\dim(W) = 3\text{.}\)</span></p>
</div></article><article class="task exercise-like" id="task-1950"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-5778"><span class="process-math">\(\{p(t) \in \pol_n : p(-t) = p(t)\}\)</span> in <span class="process-math">\(\pol_4\)</span> (The polynomials with the property that <span class="process-math">\(p(-t) = p(t)\)</span> are called <em class="emphasis">even</em> polynomials.)</p>
<div class="solution solution-like" id="solution-199">
<h5 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h5>
<p id="p-5779">Let <span class="process-math">\(W = \{p(t) \in \pol_n : p(-t) = p(t)\}\)</span> in <span class="process-math">\(\pol_4\text{.}\)</span> Let <span class="process-math">\(p(t) \in W\)</span> and suppose that <span class="process-math">\(p(t) = a_0 + a_1t+a_2t^2+a_3t^3+a_4t^4\text{.}\)</span> Since <span class="process-math">\(p(-t)=p(t)\text{,}\)</span> we must have <span class="process-math">\(p(1) = p(-1)\)</span> and <span class="process-math">\(p(3) = p(-3)\text{.}\)</span> Since <span class="process-math">\(p(1) = p(-1)\text{,}\)</span> it follows that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
a_0+a_1+a_2+a_3+a_4 = a_0-a_1+a_2-a_3+a_4
\end{equation*}
</div>
<p class="continuation">or</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
a_1+a_3 = 0\text{.}
\end{equation*}
</div>
<p class="continuation">Similarly, the fact that <span class="process-math">\(p(3) = p(-3)\)</span> yields the equation</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
a_0+3a_1+9a_2+27a_3+81a_4 = a_0-3a_1+9a_2-27a_3+81a_4
\end{equation*}
</div>
<p class="continuation">or</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
a_1+9a_3 = 0\text{.}
\end{equation*}
</div>
<p class="continuation">The reduced row echelon form of the coefficient matrix of system <span class="process-math">\(a_1+a_3 = 0\)</span> and <span class="process-math">\(a_1+9a_3 = 0\)</span> is <span class="process-math">\(I_2\text{,}\)</span> so it follows that <span class="process-math">\(a_1=a_3=0\text{.}\)</span> Thus, <span class="process-math">\(p(t) = a_0 + a_2t^2+a_4t^4\)</span> and so <span class="process-math">\(W = \Span\{1, t^2, t^4\}\text{.}\)</span> Equating like terms in the equation</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
c_1(1) + c_2(t^2) + c_3(t^4) = 0
\end{equation*}
</div>
<p class="continuation">yields <span class="process-math">\(c_1=c_2=c_3=0\text{.}\)</span> We conclude that <span class="process-math">\(\{1,t^2,t^4\}\)</span> is linearly independent and is therefore a basis for <span class="process-math">\(W\text{.}\)</span> Thus, <span class="process-math">\(\dim(W) = 3\text{.}\)</span> As an alternative solution, notice that <span class="process-math">\(p(t) = t\)</span> is not in <span class="process-math">\(W\text{.}\)</span> So <span class="process-math">\(W \neq V\)</span> and we know that <span class="process-math">\(\dim(W) \lt  \dim(V)\text{.}\)</span> Since <span class="process-math">\(1\text{,}\)</span> <span class="process-math">\(t^2\text{,}\)</span> and <span class="process-math">\(t^4\)</span> are in <span class="process-math">\(W\text{,}\)</span> we can show as above that <span class="process-math">\(1\text{,}\)</span> <span class="process-math">\(t^2\text{,}\)</span> and <span class="process-math">\(t^4\)</span> are linearly independent. We can conclude that <span class="process-math">\(\dim(W)=3\)</span> since it cannot be <span class="process-math">\(4\text{.}\)</span></p>
</div></article></article><article class="example example-like" id="example-70"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">33.6</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-569"><p id="p-5780">Let <span class="process-math">\(U_{n \times n}\)</span> be the set of all <span class="process-math">\(n \times n\)</span> upper triangular matrices. Recall that a matrix <span class="process-math">\(A = [a_{ij}]\)</span> is upper triangular if <span class="process-math">\(a_{ij} = 0\)</span> whenever <span class="process-math">\(i &gt; j\text{.}\)</span> That is, a matrix is upper triangular if all entries below the diagonal are 0.</p></div>
<article class="task exercise-like" id="task-1951"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5781">Show that <span class="process-math">\(U_{n \times n}\)</span> is a subspace of <span class="process-math">\(\M_{n \times n}\text{.}\)</span></p>
<div class="solution solution-like" id="solution-200">
<h5 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h5>
<p id="p-5782">Since the <span class="process-math">\(n \times n\)</span> zero matrix <span class="process-math">\(0_{n \times n}\)</span> has all entries equal to 0, it follows that <span class="process-math">\(0_{n \times n}\)</span> is in <span class="process-math">\(U_{n \times n}\text{.}\)</span> Let <span class="process-math">\(A = [a_{ij}]\)</span> and <span class="process-math">\(B = [b_{ij}]\)</span> be in <span class="process-math">\(U_{n \times n}\text{,}\)</span> and let <span class="process-math">\(C = [c_{ij}] = A+B\text{.}\)</span> Then <span class="process-math">\(c_{ij} = a_{ij} + b_{ij} = 0 + 0\)</span> when <span class="process-math">\(i &gt; j\text{.}\)</span> So <span class="process-math">\(C\)</span> is an upper triangular matrix and <span class="process-math">\(U_{n \times n}\)</span> is closed under addition. Let <span class="process-math">\(c\)</span> be a scalar. The <span class="process-math">\(ij\)</span>th entry of <span class="process-math">\(cA\)</span> is <span class="process-math">\(ca_{ij} = c(0) = 0\)</span> whenever <span class="process-math">\(i &gt; j\text{.}\)</span> So <span class="process-math">\(cA\)</span> is an upper triangular matrix and <span class="process-math">\(U_{n \times n}\)</span> is closed under multiplication by scalars. We conclude that <span class="process-math">\(U_{n \times n}\)</span> is a subspace of <span class="process-math">\(\M_{n \times n}\text{.}\)</span></p>
</div></article><article class="task exercise-like" id="task-1952"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5783">Find the dimensions of <span class="process-math">\(U_{2 \times 2}\)</span> and <span class="process-math">\(U_{3 \times 3}\text{.}\)</span> Explain. Make a conjecture as to what <span class="process-math">\(\dim(U_{n \times n})\)</span> is in terms of <span class="process-math">\(n\text{.}\)</span></p>
<div class="solution solution-like" id="solution-201">
<h5 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h5>
<p id="p-5784">Let <span class="process-math">\(M_{11} = \left[ \begin{array}{cc} 1\amp 0 \\ 0\amp 0 \end{array}  \right]\text{,}\)</span> <span class="process-math">\(M_{12} = \left[ \begin{array}{cc} 0\amp 1 \\ 0\amp 0 \end{array}  \right]\text{,}\)</span> and <span class="process-math">\(M_{22}=\left[ \begin{array}{cc} 0\amp 0 \\ 0\amp 1 \end{array}  \right]\text{.}\)</span> We will show that <span class="process-math">\(S = \{M_{11}, M_{12}, M_{22}\}\)</span> is a basis for <span class="process-math">\(U_{2 \times 2}\text{.}\)</span> Consider the equation</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
x_1M_{11} + x_2M_{12} + x_3 M_{22} = 0\text{.}
\end{equation*}
</div>
<p class="continuation">Equating like entries shows that <span class="process-math">\(x_1 = x_2 = x_3 = 0\text{,}\)</span> and so <span class="process-math">\(S\)</span> is linearly independent. If <span class="process-math">\(\left[ \begin{array}{cc} a\amp b \\ 0\amp c \end{array}  \right]\)</span> is in <span class="process-math">\(U_{2 \times 2}\text{,}\)</span> then</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\left[ \begin{array}{cc} a\amp b \\ 0\amp c \end{array}  \right] = aM_{11} + bM_{12} + cM_{22}
\end{equation*}
</div>
<p class="continuation">and so <span class="process-math">\(S\)</span> spans <span class="process-math">\(U_{2 \times 2}\text{.}\)</span> Thus, <span class="process-math">\(S\)</span> is a basis for <span class="process-math">\(U_{2 \times 2}\)</span> and so <span class="process-math">\(\dim(U_{2 \times 2}) = 3\text{.}\)</span> Similarly, for the <span class="process-math">\(3 \times 3\)</span> case let <span class="process-math">\(M_{ij}\)</span> for <span class="process-math">\(i \leq j\)</span> be the <span class="process-math">\(3 \times 3\)</span> matrix with a 1 in the <span class="process-math">\(ij\)</span> position and 0 in every other position. Let</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
S = \{M_{11}, M_{12}, M_{13}, M_{22}, M_{23}, M_{33}\}\text{.}
\end{equation*}
</div>
<p class="continuation">Equating corresponding entries shows that if</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
x_1M_{11} +  x_2M_{12} + x_3M_{13} + x_4 M_{22}+ x_5 M_{23} + x_6M_{33} = 0\text{,}
\end{equation*}
</div>
<p class="continuation">then <span class="process-math">\(x_1 = x_2 = x_3 = x_4 = x_5 = x_6 = 0\text{.}\)</span> So <span class="process-math">\(S\)</span> is a linearly independent set. If <span class="process-math">\(A = [a_{ij}]\)</span> is in <span class="process-math">\(U_{3 \times 3}\text{,}\)</span> then <span class="process-math">\(A = \sum_{i \geq j} a_{ij}M_{ij}\)</span> and <span class="process-math">\(S\)</span> spans <span class="process-math">\(U_{3 \times 3}\text{.}\)</span> We conclude that <span class="process-math">\(S\)</span> is a basis for <span class="process-math">\(U_{3 \times 3}\)</span> and <span class="process-math">\(\dim(U_{3 \times 3}) = 6\text{.}\)</span> In general, for an <span class="process-math">\(n \times n\)</span> matrix, the set of matrices <span class="process-math">\(M_{ij}\text{,}\)</span> one for each entry on and above the diagonal, is a basis for <span class="process-math">\(U_{n \times n}\text{.}\)</span> There are <span class="process-math">\(n\)</span> such matrices for the entries on the diagonal. The number of entries above the diagonal is equal to half the total number of entries (<span class="process-math">\(n^2\)</span>) minus half the number of entries on the diagonal (<span class="process-math">\(n\)</span>). So there is a total of <span class="process-math">\(\frac{n^2-n}{2}\)</span> such matrices for the entries above the diagonal. Therefore,</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\dim(U_{n \times n}) = n+\frac{n^2-n}{2} = \frac{n^2+n}{2}\text{.}
\end{equation*}
</div>
</div></article></article></section><section class="section" id="sec_dims_summ"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">Summary</span>
</h3>
<ul class="disc">
<li id="li-874"><p id="p-5785">A finite dimensional vector space is a vector space that can be spanned by a finite set of vectors.</p></li>
<li id="li-875"><p id="p-5786">We showed that any two bases for a finite dimensional vector space <em class="emphasis">must</em> contain the same number of vectors. Therefore, we can define the <dfn class="terminology">dimension</dfn> of a finite dimensional vector space <span class="process-math">\(V\)</span> to be the number of vectors in any basis for <span class="process-math">\(V\text{.}\)</span></p></li>
<li id="li-876"><p id="p-5787">If <span class="process-math">\(V\)</span> is a vector space with dimension <span class="process-math">\(n\)</span> and <span class="process-math">\(S\)</span> is any linearly independent subset of <span class="process-math">\(V\)</span> with <span class="process-math">\(n\)</span> vectors, then <span class="process-math">\(S\)</span> is a basis for <span class="process-math">\(V\text{.}\)</span> Otherwise, we could add vectors to <span class="process-math">\(S\)</span> to make a basis for <span class="process-math">\(V\)</span> and then <span class="process-math">\(V\)</span> would have a basis of more than <span class="process-math">\(n\)</span> vectors.</p></li>
<li id="li-877"><p id="p-5788">If <span class="process-math">\(V\)</span> is a vector space with dimension <span class="process-math">\(n\)</span> and <span class="process-math">\(S\)</span> is any subset of <span class="process-math">\(V\)</span> with <span class="process-math">\(n\)</span> vectors that spans <span class="process-math">\(V\text{,}\)</span> then <span class="process-math">\(S\)</span> is a basis for <span class="process-math">\(V\text{.}\)</span> Otherwise, we could remove vectors from <span class="process-math">\(S\)</span> to obtain a basis for <span class="process-math">\(V\)</span> and then <span class="process-math">\(V\)</span> would have a basis of fewer than <span class="process-math">\(n\)</span> vectors.</p></li>
<li id="li-878"><p id="p-5789">For any finite dimensional space <span class="process-math">\(V\)</span> and a subspace <span class="process-math">\(W\)</span> of <span class="process-math">\(V\text{,}\)</span> <span class="process-math">\(\dim(W)\leq \dim(V)\text{.}\)</span></p></li>
</ul></section><section class="exercises" id="sec_dims_exer"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber"></span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-336"><h4 class="heading"><span class="codenumber">1<span class="period">.</span></span></h4>
<p id="p-5790">Let <span class="process-math">\(W = \Span\{ 1+t^2, 2+t+2t^2+t^3, 1+t+t^3, t-t^2+t^3\}\)</span> in <span class="process-math">\(\pol_3\text{.}\)</span> Find a basis for <span class="process-math">\(W\text{.}\)</span> What is the dimension of <span class="process-math">\(W\text{?}\)</span></p></article><article class="exercise exercise-like" id="exercise-337"><h4 class="heading"><span class="codenumber">2<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-570"><p id="p-5792">Let <span class="process-math">\(A = \left[ \begin{array}{cc} 1\amp 2\\1\amp 0 \end{array} \right]\)</span> and <span class="process-math">\(B = \left[ \begin{array}{cr} 1\amp 0\\1\amp -1 \end{array} \right]\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1953"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5793">Are <span class="process-math">\(A\)</span> and <span class="process-math">\(B\)</span> linearly independent or dependent? Verify your result.</p></article><article class="task exercise-like" id="task-1954"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5794">Extend the set <span class="process-math">\(S = \{A,B\}\)</span> to a basis for <span class="process-math">\(\M_{2 \times 2}\text{.}\)</span> That is, find a basis for <span class="process-math">\(\M_{2 \times 2}\)</span> that contains both <span class="process-math">\(A\)</span> and <span class="process-math">\(B\text{.}\)</span></p></article></article><article class="exercise exercise-like" id="exercise-338"><h4 class="heading"><span class="codenumber">3<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-571"><p id="p-5795">Let <span class="process-math">\(A = \left[ \begin{array}{ccc} 1\amp 2\amp 0 \\ 3\amp 0\amp 2 \end{array} \right]\text{,}\)</span> <span class="process-math">\(B = \left[ \begin{array}{rcc} -1\amp 1\amp 1 \\ 2\amp 4\amp 0 \end{array} \right]\text{,}\)</span> <span class="process-math">\(C= \left[ \begin{array}{crr} 5\amp 1\amp -3 \\ 0\amp -12\amp 4 \end{array} \right]\text{,}\)</span> <span class="process-math">\(D = \left[ \begin{array}{crr} 5\amp 4\amp -2 \\ 5\amp -8\amp 6 \end{array} \right]\text{,}\)</span> and <span class="process-math">\(E = \left[ \begin{array}{rcc} 2\amp 0\amp 0 \\ -2\amp 0\amp 1 \end{array} \right]\)</span> in <span class="process-math">\(\M_{2 \times 3}\)</span> and let <span class="process-math">\(S = \left\{ A, B, C, D, E \right\}\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1955"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5796">Is <span class="process-math">\(S\)</span> a basis for <span class="process-math">\(\M_{2 \times 3}\text{?}\)</span> Explain.</p></article><article class="task exercise-like" id="task-1956"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5798">Determine if <span class="process-math">\(S\)</span> is a linearly independent or dependent set. Verify your result.</p></article><article class="task exercise-like" id="task-1957"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-5800">Find a basis <span class="process-math">\(\CB\)</span> for <span class="process-math">\(\Span \ S\)</span> that is a subset of <span class="process-math">\(S\)</span> and write all of the vectors in <span class="process-math">\(S\)</span> as linear combinations of the vectors in <span class="process-math">\(\CB\text{.}\)</span></p></article><article class="task exercise-like" id="task-1958"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<p id="p-5802">Extend your basis <span class="process-math">\(\CB\)</span> from part (c) to a basis <span class="process-math">\(\M_{2 \times 3}\text{.}\)</span> Explain your method.</p></article></article><article class="exercise exercise-like" id="exercise-339"><h4 class="heading"><span class="codenumber">4<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-572"><p id="p-5804">Determine the dimension of each of the following vector spaces.</p></div>
<article class="task exercise-like" id="task-1959"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5805"><span class="process-math">\(\Span\{2, 1+t, t^2\}\)</span> in <span class="process-math">\(\pol_2\)</span></p></article><article class="task exercise-like" id="task-1960"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5806">The space of all polynomials in <span class="process-math">\(\pol_3\)</span> whose constant terms is 0.</p></article><article class="task exercise-like" id="task-1961"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-5807"><span class="process-math">\(\Nul \left[ \begin{array}{cc} 1\amp 2 \\ 2\amp 4 \end{array} \right]\)</span></p></article><article class="task exercise-like" id="task-1962"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<p id="p-5808"><span class="process-math">\(\Span\left\{ [1 \ 2 \ 0 \ 1 \ -1]^{\tr}, [0 \ 1 \ 1 \ 1 \ 0]^{\tr}, [0 \ 3 \ 2 \ 3 \ 0]^{\tr}, [-1 \ 0 \ 1 \ 1 \ 1]^{\tr}\right\}\)</span> in <span class="process-math">\(\R^5\)</span></p></article></article><article class="exercise exercise-like" id="exercise-340"><h4 class="heading"><span class="codenumber">5<span class="period">.</span></span></h4>
<p id="p-5809">Let <span class="process-math">\(W\)</span> be the set of matrices in <span class="process-math">\(\M_{2 \times 2}\)</span> whose diagonal entries sum to 0. Show that <span class="process-math">\(W\)</span> is a subspace of <span class="process-math">\(\M_{2 \times 2}\text{,}\)</span> find a basis for <span class="process-math">\(W\text{,}\)</span> and then find <span class="process-math">\(\dim(W)\text{.}\)</span></p></article><article class="exercise exercise-like" id="exercise-341"><h4 class="heading"><span class="codenumber">6<span class="period">.</span></span></h4>
<p id="p-5811">Show that if <span class="process-math">\(W\)</span> is a subspace of a finite dimensional vector space <span class="process-math">\(V\text{,}\)</span> then any basis of <span class="process-math">\(W\)</span> can be extended to a basis of <span class="process-math">\(V\text{.}\)</span></p></article><article class="exercise exercise-like" id="exercise-342"><h4 class="heading"><span class="codenumber">7<span class="period">.</span></span></h4>
<p id="p-5812">Let <span class="process-math">\(W\)</span> be the set of all polynomials <span class="process-math">\(a+bt+ct^2\)</span> in <span class="process-math">\(\pol_2\)</span> such that <span class="process-math">\(a+b+c = 0\text{.}\)</span> Show that <span class="process-math">\(W\)</span> is a subspace of <span class="process-math">\(\pol_2\text{,}\)</span> find a basis for <span class="process-math">\(W\text{,}\)</span> and then find <span class="process-math">\(\dim(W)\text{.}\)</span></p></article><article class="exercise exercise-like" id="exercise-343"><h4 class="heading"><span class="codenumber">8<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-573"><p id="p-5814">Suppose <span class="process-math">\(W_1, W_2\)</span> are subspaces in a finite-dimensional space <span class="process-math">\(V\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1963"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5815">Show that it is not true in general that <span class="process-math">\(\dim(W_1)+\dim(W_2) \leq \dim(V)\text{.}\)</span></p></article><article class="task exercise-like" id="task-1964"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5816">Are there any conditions on <span class="process-math">\(W_1\)</span> and <span class="process-math">\(W_2\)</span> that will ensure that <span class="process-math">\(\dim(W_1)+\dim(W_2) \leq \dim(V)\text{?}\)</span></p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-89" id="hint-89"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-89"><div class="hint solution-like"><p id="p-5817">See <a href="" class="xref" data-knowl="./knowl/problem_disjoint_subspaces.html" title="Exercise 12">problem 12</a> in the previous section.</p></div></div>
</div></article></article><article class="exercise exercise-like" id="exercise-344"><h4 class="heading"><span class="codenumber">9<span class="period">.</span></span></h4>
<p id="p-5818">Suppose <span class="process-math">\(W_1 \subseteq W_2\)</span> are two subspaces of a finite-dimensional space. Show that if <span class="process-math">\(\dim(W_1)=\dim(W_2)\text{,}\)</span> then <span class="process-math">\(W_1=W_2\text{.}\)</span></p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-90" id="hint-90"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-90"><div class="hint solution-like"><p id="p-5819">Consider dimensions.</p></div></div>
</div></article><article class="exercise exercise-like" id="exercise-345"><h4 class="heading"><span class="codenumber">10<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-574"><p id="p-5820">Suppose <span class="process-math">\(W_1, W_2\)</span> are both three-dimensional subspaces inside <span class="process-math">\(\R^4\text{.}\)</span> In this exercise we will show that <span class="process-math">\(W_1\cap W_2\)</span> contains a plane. Let <span class="process-math">\(\{\vu_1, \vu_2, \vu_3\}\)</span> be a basis for <span class="process-math">\(W_1\)</span> and let <span class="process-math">\(\{\vv_1, \vv_2, \vv_3\}\)</span> be a basis for <span class="process-math">\(W_2\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1965"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5821">If <span class="process-math">\(\vv_1\text{,}\)</span> <span class="process-math">\(\vv_2\text{,}\)</span> and <span class="process-math">\(\vv_3\)</span> are all in <span class="process-math">\(W_1\text{,}\)</span> explain why <span class="process-math">\(W_1 \cap W_2\)</span> must contain a plane.</p></article><article class="task exercise-like" id="task-1966"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<div class="introduction" id="introduction-575"><p id="p-5822">Now we consider the case where not all of <span class="process-math">\(\vv_1\text{,}\)</span> <span class="process-math">\(\vv_2\text{,}\)</span> and <span class="process-math">\(\vv_3\)</span> are in <span class="process-math">\(W_1\text{.}\)</span> Since the arguments will be the same, let us assume that <span class="process-math">\(\vv_1\)</span> is not in <span class="process-math">\(W_1\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1967"><h6 class="heading"><span class="codenumber">(i)</span></h6>
<p id="p-5823">Explain why the set <span class="process-math">\(\CS = \{\vu_1, \vu_2, \vu_3, \vv_1\}\)</span> is a basis for <span class="process-math">\(\R^4\text{.}\)</span></p></article><article class="task exercise-like" id="task-1968"><h6 class="heading"><span class="codenumber">(ii)</span></h6>
<p id="p-5824">Explain why <span class="process-math">\(\vv_2\)</span> and <span class="process-math">\(\vv_3\)</span> can be written as linear combinations of the vectors in <span class="process-math">\(\CS\text{.}\)</span> Use these linear combinations to find two vectors that are in <span class="process-math">\(W_1 \cap W_2\text{.}\)</span> Then show that these vectors span a plane in <span class="process-math">\(W_1 \cap W_2\text{.}\)</span></p></article></article></article><article class="exercise exercise-like" id="exercise-346"><h4 class="heading"><span class="codenumber">11<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-576">
<p id="p-5825">A magic matrix is an <span class="process-math">\(n \times n\)</span> matrix in which the sum of the entries along any row, column, or diagonal is the same. For example, the <span class="process-math">\(3 \times 3\)</span> matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\left[ \begin{array}{rcc} 5\amp 4\amp 0 \\ -2\amp 3\amp 8 \\ 6\amp 2\amp 1 \end{array}  \right]
\end{equation*}
</div>
<p class="continuation">is a magic matrix. Note that the entries of a magic matrix can be any real numbers.</p>
</div>
<article class="task exercise-like" id="task-1969"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5826">Show that the set of <span class="process-math">\(n \times n\)</span> magic matrices is a subspace of <span class="process-math">\(\M_{n \times n}\text{.}\)</span></p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-91" id="hint-91"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-91"><div class="hint solution-like"><p id="p-5827">Consider the row, column, and diagonal sums.</p></div></div>
</div></article><article class="task exercise-like" id="task-1970"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5828">Let <span class="process-math">\(V\)</span> be the space of <span class="process-math">\(3 \times 3\)</span> magic matrices. Find a basis for <span class="process-math">\(V\)</span> and determine the dimension of <span class="process-math">\(V\text{.}\)</span></p></article></article><article class="exercise exercise-like" id="exercise-347"><h4 class="heading"><span class="codenumber">12<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-577"><p id="p-5830">Label each of the following statements as True or False. Provide justification for your response.</p></div>
<article class="task exercise-like" id="task-1971"><h5 class="heading">
<span class="codenumber">(a)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-5831">The dimension of a finite dimensional vector space is the minimum number of vectors needed to span that space.</p></article><article class="task exercise-like" id="task-1972"><h5 class="heading">
<span class="codenumber">(b)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-5833">The dimension of a finite dimensional vector space is the maximum number of linearly independent vectors that can exist in that space.</p></article><article class="task exercise-like" id="task-1973"><h5 class="heading">
<span class="codenumber">(c)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-5834">If <span class="process-math">\(n\)</span> vectors span an <span class="process-math">\(n\)</span>-dimensional vector space <span class="process-math">\(V\text{,}\)</span> then these vectors form a basis of <span class="process-math">\(V\text{.}\)</span></p></article><article class="task exercise-like" id="task-1974"><h5 class="heading">
<span class="codenumber">(d)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-5836">Any set of <span class="process-math">\(n\)</span> vectors form a basis in an <span class="process-math">\(n\)</span>-dimensional vector space.</p></article><article class="task exercise-like" id="task-1975"><h5 class="heading">
<span class="codenumber">(e)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-5837">Every vector in a vector space <span class="process-math">\(V\)</span> spans a one-dimensional subspace of <span class="process-math">\(V\text{.}\)</span></p></article><article class="task exercise-like" id="task-1976"><h5 class="heading">
<span class="codenumber">(f)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-5839">Any set of <span class="process-math">\(n\)</span> linearly independent vectors in a vector space <span class="process-math">\(V\)</span> of dimensional <span class="process-math">\(n\)</span> is a basis for <span class="process-math">\(V\text{.}\)</span></p></article><article class="task exercise-like" id="task-1977"><h5 class="heading">
<span class="codenumber">(g)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-5840">If <span class="process-math">\(\{\vv_1, \vv_2, \ldots, \vv_k\}\)</span> is linearly independent in <span class="process-math">\(V\text{,}\)</span> then <span class="process-math">\(\dim(V)\geq k\text{.}\)</span></p></article><article class="task exercise-like" id="task-1978"><h5 class="heading">
<span class="codenumber">(h)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-5842">If a set of <span class="process-math">\(k\)</span> vectors span <span class="process-math">\(V\text{,}\)</span> then any set of more than <span class="process-math">\(k\)</span> vectors in <span class="process-math">\(V\)</span> is linearly dependent.</p></article><article class="task exercise-like" id="task-1979"><h5 class="heading">
<span class="codenumber">(i)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-5843">If an infinite set of vectors span <span class="process-math">\(V\text{,}\)</span> then <span class="process-math">\(V\)</span> is infinite-dimensional.</p></article><article class="task exercise-like" id="task-1980"><h5 class="heading">
<span class="codenumber">(j)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-5845">If <span class="process-math">\(W_1, W_2\)</span> are both two-dimensional subspaces of a three dimensional vector space <span class="process-math">\(V\text{,}\)</span> then <span class="process-math">\(W_1\cap W_2\neq \{\vzero \}\text{.}\)</span></p></article><article class="task exercise-like" id="task-1981"><h5 class="heading">
<span class="codenumber">(k)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-5846">If <span class="process-math">\(\dim(V)=n\)</span> and <span class="process-math">\(W\)</span> is a subspace of <span class="process-math">\(V\)</span> with dimension <span class="process-math">\(n\text{,}\)</span> then <span class="process-math">\(W=V\text{.}\)</span></p></article></article></section><section class="section" id="sec_proj_pca"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">Project: Understanding Principal Component Analysis</span>
</h3>
<p id="p-5848">Suppose we were to film an experiment involving a ball that is bouncing up and down. Naively, we set up several cameras to follow the process of the experiment from different perspectives and collect the data. All of this data tells us something about the bouncing ball, but there may be no perspective that tells us an important piece of information — the axis along which the ball bounces. The question, then, is how we can extract from the data this important piece of information. Principal Component Analysis (PCA) is a tool for just this type of analysis.</p>
<figure class="table table-like" id="T_PCA_SAT_2"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">33.7<span class="period">.</span></span><span class="space"> </span>SAT data</figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="l m b1 r0 l0 t0 lines">State</td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(1\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(2\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(3\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(4\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(5\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(6\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(7\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(8\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(9\)</span></td>
<td class="c m b1 r0 l0 t0 lines"><span class="process-math">\(10\)</span></td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines">EBRW</td>
<td class="c m b0 r0 l0 t0 lines"><span class="process-math">\(595\)</span></td>
<td class="c m b0 r0 l0 t0 lines"><span class="process-math">\(540\)</span></td>
<td class="c m b0 r0 l0 t0 lines"><span class="process-math">\(522\)</span></td>
<td class="c m b0 r0 l0 t0 lines"><span class="process-math">\(508\)</span></td>
<td class="c m b0 r0 l0 t0 lines"><span class="process-math">\(565\)</span></td>
<td class="c m b0 r0 l0 t0 lines"><span class="process-math">\(512\)</span></td>
<td class="c m b0 r0 l0 t0 lines"><span class="process-math">\(643\)</span></td>
<td class="c m b0 r0 l0 t0 lines"><span class="process-math">\(574\)</span></td>
<td class="c m b0 r0 l0 t0 lines"><span class="process-math">\(534\)</span></td>
<td class="c m b0 r0 l0 t0 lines"><span class="process-math">\(539\)</span></td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines">Math</td>
<td class="c m b0 r0 l0 t0 lines"><span class="process-math">\(571\)</span></td>
<td class="c m b0 r0 l0 t0 lines"><span class="process-math">\(536\)</span></td>
<td class="c m b0 r0 l0 t0 lines"><span class="process-math">\(493\)</span></td>
<td class="c m b0 r0 l0 t0 lines"><span class="process-math">\(493\)</span></td>
<td class="c m b0 r0 l0 t0 lines"><span class="process-math">\(554\)</span></td>
<td class="c m b0 r0 l0 t0 lines"><span class="process-math">\(501\)</span></td>
<td class="c m b0 r0 l0 t0 lines"><span class="process-math">\(655\)</span></td>
<td class="c m b0 r0 l0 t0 lines"><span class="process-math">\(566\)</span></td>
<td class="c m b0 r0 l0 t0 lines"><span class="process-math">\(534\)</span></td>
<td class="c m b0 r0 l0 t0 lines"><span class="process-math">\(539\)</span></td>
</tr>
</table></div></figure><p id="p-5849">We will use an example to illustrate important concepts we will need. To realistically apply PCA we will have much more data than this, but for now we will restrict ourselves to only two variables so that we can visualize our results. <a href="" class="xref" data-knowl="./knowl/T_PCA_SAT_2.html" title="Table 33.7: SAT data">Table 33.7</a> presents information from ten states on two attributes related to the SAT — Evidence-Based Reading and Writing (EBRW) score and Math score. The SAT is made up of three sections: Reading, Writing and Language (also just called Writing), and Math. The The EBRW score is calculated by combining the Reading and Writing section scores — both the Math and EBRW are scored on a scale of 200-800.</p>
<p id="p-5850">Each attribute (Math, EBRW score) creates a vector whose entries are the student responses for that attribute. The data provides the average scores from participating students in each state. In this example we have two attribute vectors:</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-223">
\begin{align*}
\vx_1 \amp = [595 \ 540 \ 522 \ 508 \ 565 \ 512 \ 643 \ 574 \ 534 \ 539]^{\tr} \text{ and }\\
\vx_2 \amp = [571 \ 536 \ 493 \ 493 \ 554 \ 501 \ 655 \ 566 \ 534 \ 539]^{\tr}\text{.}
\end{align*}
</div>
<p id="p-5851">These vectors form the rows of a <span class="process-math">\(2 \times 10\)</span> matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/F_PCA_data_plot.html">
\begin{equation*}
X_0 = \left[ \begin{array}{c} \vx_1^{\tr} \\ \vx_2^{\tr} \end{array}  \right] = \left[ \begin{array}{cccccccccc}  595\amp 540\amp 522\amp 508\amp 565\amp 512\amp 643\amp 574\amp 534\amp 539 \\ 571\amp 536\amp 493\amp 493\amp 554\amp 501\amp 655\amp 566\amp 534\amp 539 \end{array}  \right]
\end{equation*}
</div>
<p class="continuation">that makes up our data set. A plot of the data is shown at left in <a href="" class="xref" data-knowl="./knowl/F_PCA_data_plot.html" title="Figure 33.8">Figure 33.8</a>, where the EBRW score is along the horizontal axis and the math score is along the vertical axis.</p>
<figure class="figure figure-like" id="F_PCA_data_plot"><div class="image-box" style="width: 70%; margin-left: 15%; margin-right: 15%;"><img src="external/PCA_data.svg" role="img" class="contained"></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">33.8<span class="period">.</span></span><span class="space"> </span>Two views of the data set (EBRW horizontal, math vertical).</figcaption></figure><p id="p-5852">The question we want to answer is, how do we represent our data set so that the most important features in the data set are revealed?</p>
<article class="project project-like" id="act_PCA_centering"><h4 class="heading">
<span class="type">Project Activity</span><span class="space"> </span><span class="codenumber">33.6</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-578"><p id="p-5853">Before analyzing a data set there is often some initial preparation that needs to be made. Issues that have to be dealt with include the problem that attributes might have different units and scales. For example, in a data set with attributes about people, height could be measured in inches while weight is measured in pounds. It is difficult to compare variables when they are on different scales. Another issue to consider is that some attributes are independent of the others (height, for example does not depend on hair color), while some are interrelated (body mass index depends on height and weight). To simplify our work, we will not address these type of problems. The only preparation we will do with our data is to center it.</p></div>
<article class="task exercise-like" id="task-1982"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5854">An important piece of information about a one-dimensional data set <span class="process-math">\(\vx = [x_1 \ x_2 \ x_3 \ \cdots \ x_n]^{\tr}\)</span>  is the sample average or mean</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\overline{\vx} = \sum_{i=1}^n x_i\text{.}
\end{equation*}
</div>
<p class="continuation">Calculate the means <span class="process-math">\(\overline{\vx_1}\)</span> and <span class="process-math">\(\overline{\vx_2}\)</span> for our SAT data from the matrix <span class="process-math">\(X_0\text{.}\)</span></p></article><article class="task exercise-like" id="task-1983"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5855">We can use these sample means to center our data at the origin by translating the data so that each column of our data matrix has mean <span class="process-math">\(0\text{.}\)</span> We do this by subtracting the mean for that row vector from each component of the vector. Determine the matrix <span class="process-math">\(X\)</span> that contains the centered data for our SAT data set from matrix <span class="process-math">\(X_0\text{.}\)</span></p></article></article><p id="p-5856">A plot of the centered data for our SAT data is shown at right in <a href="" class="xref" data-knowl="./knowl/F_PCA_data_plot.html" title="Figure 33.8">Figure 33.8</a>. Later we will see why centering the data is useful — it will more easily allow us to project onto subspaces. The goal of PCA is to find a matrix <span class="process-math">\(P\)</span> so that <span class="process-math">\(PX = Y\text{,}\)</span> and <span class="process-math">\(Y\)</span> is suitably transformed to identify the important aspects of the data. We will discuss what the important aspects are shortly. Before we do so, we need to discuss a way to compare the one dimensional data vectors <span class="process-math">\(\vx_1\)</span> and <span class="process-math">\(\vx_2\text{.}\)</span></p>
<article class="project project-like" id="act_PCA_covariance"><h4 class="heading">
<span class="type">Project Activity</span><span class="space"> </span><span class="codenumber">33.7</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-579"><p id="p-5857">To compare the two one dimensional data vectors, we need to consider variance and covariance.</p></div>
<article class="task exercise-like" id="task-1984"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5858">It is often useful to know how spread out a data set is, something the average doesn't tell us. For example, the data sets <span class="process-math">\([1 \ 2 \ 3]^{\tr}\)</span> and <span class="process-math">\([-2 \ 0 \ 8]^{\tr}\)</span> both have averages of <span class="process-math">\(2\text{,}\)</span> but the data in <span class="process-math">\([-2 \ 0 \ 8]^{\tr}\)</span> is more spread out. Variance provides one measure of how spread out a one-dimensional data set <span class="process-math">\(\vx = [x_1 \ x_2 \ x_3 \ \cdots \ x_n]^{\tr}\)</span> is. Variance is defined as</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\var(\vx) = \frac{1}{n-1} \sum_{i=1}^{n} \left(x_i - \overline{\vx} \right)^2\text{.}
\end{equation*}
</div>
<p class="continuation">The variance provides a measure of how far from the average the data is spread.<a href="" data-knowl="" class="id-ref fn-knowl original" data-refid="hk-fn-58" id="fn-58"><sup> 58 </sup></a>  Determine the variances of the two data vectors <span class="process-math">\(\vx_1\)</span> and <span class="process-math">\(\vx_2\text{.}\)</span> Which is more spread out?</p></article><article class="task exercise-like" id="task-1985"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5859">In general, we will have more than one-dimensional data, as in our SAT data set. It will be helpful to have a way to compare one-dimensional data sets to try to capture the idea of variance for different data sets — how much the data in two different data sets varies from the mean with respect to each other. One such measure is covariance — essentially the average of all corresponding products of deviations from the means. We define the covariance of two data vectors <span class="process-math">\(\vx = [x_1 \ x_2 \ \cdots \ x_n]^{\tr}\)</span> and <span class="process-math">\(\vy = [y_1 \ y_2 \ \cdots \ y_n]^{\tr}\)</span> as</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\cov(\vx,\vy) = \frac{1}{n-1} \sum_{i=1}^{n} \left(x_i-\overline{\vx}\right)\left(y_i-\overline{\vy}\right)\text{.}
\end{equation*}
</div>
<p class="continuation">Determine all covariances</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\cov(\vx_1,\vx_1), \  \cov(\vx_1,\vx_2), \  \cov(\vx_2,\vx_1), \ \text{ and }  \ \cov(\vx_2,\vx_2)\text{.}
\end{equation*}
</div>
<p class="continuation">How are <span class="process-math">\(\cov(\vx_1,\vx_2)\)</span> and <span class="process-math">\(\cov(\vx_2,\vx_1)\)</span> related? How are  <span class="process-math">\(\cov(\vx_1,\vx_1)\)</span> and <span class="process-math">\(\cov(\vx_2,\vx_2)\)</span> related to variances?</p></article><article class="task exercise-like" id="task-1986"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-5860">What is most important about covariance is its sign. Suppose <span class="process-math">\(\vy = [y_1 \ y_2 \ \ldots \ y_n]^{\tr}\text{,}\)</span> <span class="process-math">\(\vz = [z_1 \ z_2 \ \ldots \ z_n]^{\tr}\)</span> and <span class="process-math">\(\cov(\vy,\vz) &gt; 0\text{.}\)</span> Then if <span class="process-math">\(y_i\)</span> is larger than <span class="process-math">\(y_j\)</span> it is likely that <span class="process-math">\(z_i\)</span> is also larger than <span class="process-math">\(z_j\text{.}\)</span> For example, if <span class="process-math">\(\vy\)</span> is a vector that records a persons height from age <span class="process-math">\(2\)</span> to <span class="process-math">\(10\)</span> and <span class="process-math">\(\vz\)</span> records that same person's weight in the same years, we might expect that when <span class="process-math">\(y_i\)</span> increases so does <span class="process-math">\(z_i\text{.}\)</span> Similarly, if <span class="process-math">\(\cov(\vy,\vz) \lt 0\text{,}\)</span> then as one data set increases, the other decreases. As an example, if <span class="process-math">\(\vy\)</span> records the number of hours a student spends playing video games each semester ad <span class="process-math">\(\vz\)</span> gives the student's GPA for each semester, then we might expect that <span class="process-math">\(z_i\)</span> decreases as <span class="process-math">\(y_i\)</span> increases. When <span class="process-math">\(\cov(\vy,\vz) = 0\text{,}\)</span> then <span class="process-math">\(\vy\)</span> and <span class="process-math">\(\vz\)</span> are said to be uncorrelated or independent of each other. For our example <span class="process-math">\(\vx_1\)</span> and <span class="process-math">\(\vx_2\text{,}\)</span> what does <span class="process-math">\(\cov(\vx_1,\vx_2)\)</span> tell us about the relationship between <span class="process-math">\(\vx_1\)</span> and <span class="process-math">\(\vx_2\text{?}\)</span> Why should we expect this from the context?</p></article><article class="task exercise-like" id="task-1987"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<p id="p-5861">The covariance gives us information about the relationships between the attributes. So instead of working with the original data, we work with the covariance data. If we have <span class="process-math">\(m\)</span> data vectors <span class="process-math">\(\vx_1\text{,}\)</span> <span class="process-math">\(\vx_2\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\(\vx_m\)</span> in <span class="process-math">\(\R^n\text{,}\)</span> the <dfn class="terminology">covariance matrix</dfn> <span class="process-math">\(C = [c_{ij}]\)</span> satisfies <span class="process-math">\(c_{ij} = \cov(\vx_i, \vx_j)\text{.}\)</span> Calculate the covariance matrix for our SAT data. Then explain why <span class="process-math">\(C = \frac{1}{n-1}XX^{\tr}\text{.}\)</span></p></article></article><p id="p-5862">Recall that the goal of PCA is to find a matrix <span class="process-math">\(P\)</span> such that <span class="process-math">\(PX = Y\)</span> where <span class="process-math">\(P\)</span> transforms the data set to a coordinate system in which the important aspects of the data are revealed. We are now in a position to discuss what that means.</p>
<p id="p-5863">An ideal view of our data would be one in which we can see the direction of greatest variance and one that minimizes redundancy. With redundant data the variables are not independent — that is, covariance is nonzero. So we would like the covariances to all be zero (or as close to zero as possible) to remove redundancy in our data. That means that we would like a covariance matrix in which the non-diagonal entries are all zero. This will be possible if the covariance matrix is diagonalizable.</p>
<article class="project project-like" id="act_PCA_diagonalize"><h4 class="heading">
<span class="type">Project Activity</span><span class="space"> </span><span class="codenumber">33.8</span><span class="period">.</span>
</h4>
<p id="p-5864">Consider the covariance matrix <span class="process-math">\(C = \left[ \begin{array}{cc} 1760.18\amp 1967.62\\ 1967.62\amp 2319.29 \end{array} \right]\text{.}\)</span> Explain why we can find a matrix <span class="process-math">\(P\)</span> with determinant 1 whose columns are unit vectors that diagonalizes <span class="process-math">\(C\text{.}\)</span> Then find such a matrix. Use technology as appropriate.</p></article><p id="p-5865">For our purposes, we want to diagonalize <span class="process-math">\(XX^{\tr}\)</span> with <span class="process-math">\(PXX^{\tr}P^{-1}\text{,}\)</span> so the matrix <span class="process-math">\(P\)</span> that serve our purposes is the one whose <em class="emphasis">rows</em> are the eigenvectors of <span class="process-math">\(XX^{\tr}\text{.}\)</span> To understand why this matrix is the one we want, recall that we want to have <span class="process-math">\(PX = Y\text{,}\)</span> and we want to diagonalize <span class="process-math">\(XX^{\tr}\)</span> to a diagonal covariance matrix <span class="process-math">\(YY^{\tr}\text{.}\)</span> In this situation we will have (recalling that <span class="process-math">\(P^{-1}=P^{\tr}\)</span>)</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\frac{1}{n-1}YY^{\tr} = \frac{1}{n-1}(PX)(PX)^{\tr} = \frac{1}{n-1}P\left(XX^{\tr}\right)P^{\tr} = P\left(XX^{\tr}\right)P^{-1}\text{.}
\end{equation*}
</div>
<p id="p-5866">So the matrix <span class="process-math">\(P\)</span> that we want is exactly the one that diagonalizes <span class="process-math">\(XX^{\tr}\text{.}\)</span></p>
<article class="project project-like" id="act_PCA_max_min"><h4 class="heading">
<span class="type">Project Activity</span><span class="space"> </span><span class="codenumber">33.9</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-580"><p id="p-5867">There are two useful ways we can interpret the results of our work so far. An eigenvector of <span class="process-math">\(XX^{\tr}\)</span> that corresponds to the largest (also called the <em class="emphasis">dominant</em>) eigenvalue <span class="process-math">\(\lambda_1\)</span> is <span class="process-math">\([0.66 \ 0.76]^{\tr}\text{.}\)</span> A plot of the centered data along with the eigenspace <span class="process-math">\(E_{\lambda_1}\)</span> of <span class="process-math">\(XX^{\tr}\)</span> spanned by <span class="process-math">\(\vv_1 = [0.66 \ 0.76]^{\tr}\)</span> is shown at left in <a href="" class="xref" data-knowl="./knowl/F_PCA_pc.html" title="Figure 33.9">Figure 33.9</a>. The eigenvector <span class="process-math">\(\vv_1\)</span> is called the <dfn class="terminology">first principal component</dfn> of <span class="process-math">\(X\text{.}\)</span> Notice that this line <span class="process-math">\(E_{\lambda_1}\)</span> indicates the direction of greatest variation in the data. That is, the sum of the squares of the differences between the data points and the mean is as large as possible in this direction. In other words, when we project the data points onto <span class="process-math">\(E_{\lambda_1}\text{,}\)</span> as shown at right in <a href="" class="xref" data-knowl="./knowl/F_PCA_pc.html" title="Figure 33.9">Figure 33.9</a>, the variation of the resulting points is larger than it is for any other line. In other words, the data is most spread out in this direction.</p></div>
<article class="task exercise-like" id="task-1988"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5868">There is another way we can interpret this result. If we drop a perpendicular from one of our data points to the space <span class="process-math">\(E_{\lambda_1}\)</span> it creates a right triangle with sides of length <span class="process-math">\(a\text{,}\)</span> <span class="process-math">\(b\text{,}\)</span> and <span class="process-math">\(c\)</span> as illustrated in the middle of <a href="" class="xref" data-knowl="./knowl/F_PCA_pc.html" title="Figure 33.9">Figure 33.9</a>. Use this idea to explain why maximizing the variation also minimizes the sum of the squares of the distances from the data points to this line. As a result, we have projected our two-dimensional data onto the one-dimensional space that maximizes the variance of the data.</p>
<figure class="figure figure-like" id="F_PCA_pc"><div class="image-box" style="width: 70%; margin-left: 15%; margin-right: 15%;"><img src="external/PCA_spread.svg" role="img" class="contained"></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">33.9<span class="period">.</span></span><span class="space"> </span>The principal component.</figcaption></figure></article><article class="task exercise-like" id="task-1989"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5869">Recall that the matrix (to two decimal places) <span class="process-math">\(P = \left[ \begin{array}{rr} - 0.66\amp -0.76\\ 0.76\amp 0.66 \end{array} \right]\)</span> transforms the data set <span class="process-math">\(X\)</span> to a new data set <span class="process-math">\(Y=PX\)</span> whose covariance matrix is diagonal. Explain how the <span class="process-math">\(x\)</span>-axis is related to the transformed data set <span class="process-math">\(Y\text{.}\)</span></p>
<figure class="figure figure-like" id="F_PCA_P"><div class="image-box" style="width: 70%; margin-left: 15%; margin-right: 15%;"><img src="external/PCA_P.svg" role="img" class="contained"></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">33.10<span class="period">.</span></span><span class="space"> </span>Applying <span class="process-math">\(P\text{.}\)</span></figcaption></figure></article></article><p id="p-5870">The result of <a href="" class="xref" data-knowl="./knowl/act_PCA_max_min.html" title="Project Activity 33.9">Project Activity 33.9</a> is that we have reduced the problem from considering the data in a two-dimensional space to a one-dimensional space <span class="process-math">\(E_{\lambda_1}\)</span> where the most important aspect of the data is revealed. Of course, we eliminate some of the characteristics of the data, but the most important aspect is still included and highlighted. This is one of the important uses of PCA, data dimension reduction, which allows us to reveal key relationships between the variables that might not be evident when looking at a large dataset.</p>
<p id="p-5871">The second eigenvector of <span class="process-math">\(XX^{\tr}\)</span> also has meaning. A picture of the eigenspace <span class="process-math">\(E_{\lambda_2}\)</span> corresponding to the smaller eigenvector <span class="process-math">\(\lambda_2\)</span> of <span class="process-math">\(XX^{\tr}\)</span> is shown in <a href="" class="xref" data-knowl="./knowl/F_PCA_second_pc.html" title="Figure 33.11">Figure 33.11</a>. The second eigenvector of <span class="process-math">\(XX^{\tr}\)</span> is orthogonal to the first, and the direction of the second eigenvector tells us the direction of the second most amount of variance as can be seen in <a href="" class="xref" data-knowl="./knowl/F_PCA_second_pc.html" title="Figure 33.11">Figure 33.11</a>.</p>
<figure class="figure figure-like" id="F_PCA_second_pc"><div class="image-box" style="width: 50%; margin-left: 25%; margin-right: 25%;"><img src="external/PCA_second.svg" role="img" class="contained"></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">33.11<span class="period">.</span></span><span class="space"> </span>The second principal component.</figcaption></figure><p id="p-5872">To summarize, the unit eigenvector for the largest eigenvalue of <span class="process-math">\(XX^{\tr}\)</span> indicates the direction in which the data has the greatest variance. The direction of the unit eigenvector for the smaller eigenvalue shows the direction in which the data has the second largest variance. This direction is also perpendicular to the first (indicating <span class="process-math">\(0\)</span> covariance). The directions of the eigenvectors are called the <dfn class="terminology">principal components</dfn> of <span class="process-math">\(X\text{.}\)</span> The eigenvector with the highest eigenvalue is the first principal component of the data set and the other eigenvectors are ordered by the eigenvalues, highest to lowest. The principal components provide a new coordinate system from which to view our data — one in which we can see the maximum variance and in which there is zero covariance.</p>
<article class="project project-like" id="act_PCA_variances"><h4 class="heading">
<span class="type">Project Activity</span><span class="space"> </span><span class="codenumber">33.10</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-581"><p id="p-5873">We can use the eigenvalues of <span class="process-math">\(XX^{\tr}\)</span> to quantify the amount of variance that is accounted for by our projections. Notice that the points along the <span class="process-math">\(x\)</span>-axis at right in <a href="" class="xref" data-knowl="./knowl/F_PCA_P.html" title="Figure 33.10">Figure 33.10</a> are exactly the numbers in the first row of <span class="process-math">\(Y\text{.}\)</span> These numbers provide the projections of the data in <span class="process-math">\(Y\)</span> onto the <span class="process-math">\(x\)</span>-axis — the axis along which the data has its greatest variance.</p></div>
<article class="task exercise-like" id="task-1990"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5874">Calculate the variance of the data given by the first row of <span class="process-math">\(Y\text{.}\)</span> This is the variance of the data i the direction of the eigenspace <span class="process-math">\(E_{\lambda_1}\text{.}\)</span> How does the result compare to entries of the covariance matrix for <span class="process-math">\(Y\text{.}\)</span></p></article><article class="task exercise-like" id="task-1991"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5875">Repeat part (a) for the data along the second row of <span class="process-math">\(Y\text{.}\)</span></p></article><article class="task exercise-like" id="task-1992"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-5876">The total variance of the data set is the sum of the variances. Explain why the amount of variance in the data that is accounted for in the direction of <span class="process-math">\(E_{\lambda_1}\)</span> is</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\frac{\lambda_1}{\lambda_1 + \lambda_2}\text{.}
\end{equation*}
</div>
<p class="continuation">Then calculate this amount for the SAT data.</p></article></article><p id="p-5877">In general, PCA is most useful for larger data sets. The process is the same.</p>
<ul class="disc">
<li id="li-879"><p id="p-5878">Start with a set of data that forms the rows of an <span class="process-math">\(m \times n\)</span> matrix. We center the data by subtracting the mean of each row from the entries of that row to create a centered data set in a matrix <span class="process-math">\(X\text{.}\)</span></p></li>
<li id="li-880"><p id="p-5879">The principal components of <span class="process-math">\(X\)</span> are the eigenvectors of <span class="process-math">\(XX^{\tr}\text{,}\)</span> ordered so that they correspond to the eigenvalues of <span class="process-math">\(XX^{\tr}\)</span> in decreasing order.</p></li>
<li id="li-881"><p id="p-5880">Let <span class="process-math">\(P\)</span> be the matrix whose rows are the principal components of <span class="process-math">\(X\text{,}\)</span> ordered from highest to lowest. Then <span class="process-math">\(Y = PX\)</span> is suitably transformed to identify the important aspects of the data.</p></li>
<li id="li-882">
<p id="p-5881">If <span class="process-math">\(\lambda_1\text{,}\)</span> <span class="process-math">\(\lambda_2\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\(\lambda_n\)</span> are the eigenvalues of <span class="process-math">\(XX^{\tr}\)</span> in decreasing order, then the amount of variance in the data accounted for by the first <span class="process-math">\(r\)</span> principal components is given by</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\frac{\lambda_1+\lambda_2 + \cdots + \lambda_r}{\lambda_1+\lambda_2 + \cdots + \lambda_n}\text{.}
\end{equation*}
</div>
</li>
<li id="li-883"><p id="p-5882">The first <span class="process-math">\(r\)</span> rows of <span class="process-math">\(Y=PX\)</span> provide the projection of the data set <span class="process-math">\(X\)</span> onto an <span class="process-math">\(r\)</span>-dimensional space spanned by the first <span class="process-math">\(r\)</span> principal components of <span class="process-math">\(X\text{.}\)</span></p></li>
</ul>
<article class="project project-like" id="act_PCA_4D_SAT"><h4 class="heading">
<span class="type">Project Activity</span><span class="space"> </span><span class="codenumber">33.11</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-582">
<p id="p-5883">Let us now consider a problem with more than two variables. We continue to keep the data set small so that we can conveniently operate with it. <a href="" class="xref" data-knowl="./knowl/T_PCA_SAT.html" title="Table 33.12: SAT data">Table 33.12</a> presents additional information from ten states on four attributes related to the SAT — Participation rates, Evidence-Based Reading and Writing (EBRW) score, Math score, and average SAT score. Use technology as appropriate for this activity.</p>
<figure class="table table-like" id="T_PCA_SAT"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">33.12<span class="period">.</span></span><span class="space"> </span>SAT data</figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="l m b1 r0 l0 t0 lines">State</td>
<td class="l m b1 r0 l0 t0 lines"><span class="process-math">\(1\)</span></td>
<td class="l m b1 r0 l0 t0 lines"><span class="process-math">\(2\)</span></td>
<td class="l m b1 r0 l0 t0 lines"><span class="process-math">\(3\)</span></td>
<td class="l m b1 r0 l0 t0 lines"><span class="process-math">\(4\)</span></td>
<td class="l m b1 r0 l0 t0 lines"><span class="process-math">\(5\)</span></td>
<td class="l m b1 r0 l0 t0 lines"><span class="process-math">\(6\)</span></td>
<td class="l m b1 r0 l0 t0 lines"><span class="process-math">\(7\)</span></td>
<td class="l m b1 r0 l0 t0 lines"><span class="process-math">\(8\)</span></td>
<td class="l m b1 r0 l0 t0 lines"><span class="process-math">\(9\)</span></td>
<td class="l m b1 r0 l0 t0 lines"><span class="process-math">\(10\)</span></td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines">Rate</td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(6\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(60\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(97\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(100\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(64\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(99\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(4\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(23\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(79\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(70\)</span></td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines">EBRW</td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(595\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(540\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(522\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(508\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(565\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(512\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(643\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(574\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(534\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(539\)</span></td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines">Math</td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(571\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(536\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(493\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(493\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(554\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(501\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(655\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(566\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(534\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(539\)</span></td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines">SAT</td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(1166\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(1076\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(1014\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(1001\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(1120\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(1013\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(1296\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(1140\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(1068\)</span></td>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(1086\)</span></td>
</tr>
</table></div></figure>
</div>
<article class="task exercise-like" id="task-1993"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5884">Determine the centered data matrix <span class="process-math">\(X\)</span> for this data set.</p></article><article class="task exercise-like" id="task-1994"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5885">Find the covariance matrix for this data set. Round to four decimal places.</p></article><article class="task exercise-like" id="task-1995"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-5886">Find the principal components of <span class="process-math">\(X\text{.}\)</span> Include at least four decimal places accuracy.</p></article><article class="task exercise-like" id="task-1996"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<p id="p-5887">How much variation is accounted for in the data by the first principal component? In other words, if we reduce this data to one dimension, how much of the variation do we retain? Explain.</p></article><article class="task exercise-like" id="task-1997"><h5 class="heading"><span class="codenumber">(e)</span></h5>
<p id="p-5888">How much variation is accounted for in the data by the first two principal components? In other words, if we reduce this data to two dimensions, how much of the variation do we retain? Explain.</p></article></article><p id="p-5889">We conclude with a comment. A reasonable question to ask is how we interpret the principal components. Let <span class="process-math">\(P\)</span> be an orthogonal matrix such that <span class="process-math">\(PCP^{\tr}\)</span> is the diagonal matrix with the eigenvalues of <span class="process-math">\(C\)</span> along the diagonal, in decreasing order. We then have the new perspective <span class="process-math">\(Y = PX\)</span> from which to view the data. The first principal component <span class="process-math">\(\vp_1\)</span> (the first row of <span class="process-math">\(P\)</span>) determines the new variable <span class="process-math">\(\vy_1 = [y_i]\)</span> (the first row of <span class="process-math">\(Y\)</span>) in the following manner. Let <span class="process-math">\(\vp_1 = [p_i]^{\tr}\)</span> and let <span class="process-math">\(\vc_i\)</span> represent the columns of <span class="process-math">\(X\)</span> so that <span class="process-math">\(X = [\vc_1 \ \vc_2 \ \vc_3 \ \cdots \ \vc_{20}]\text{.}\)</span> Recognizing that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
PX =  \left[  \begin{array}{c} \vp_1^{\tr} \\ \vp_2^{\tr} \\ \vp_3^{\tr} \\ \vp_4^{\tr} \\\vp_5^{\tr} \\ \vp_6^{\tr} \end{array}  \right]  [\vc_1 \ \vc_2 \ \vc_3 \ \cdots \ \vc_{20}]\text{,}
\end{equation*}
</div>
<p class="continuation">we have that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
y_i = \vp_1^{\tr} \vc_i\text{.}
\end{equation*}
</div>
<p id="p-5890">That is,</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\vy_1 = [\vp_1^{\tr}\vc_1 \ \vp_1^{\tr} \vc_2 \ \vp_1^{\tr} \vc_3 \ \cdots \ \vp_{1}^{\tr} \vc_{20}]\text{.}
\end{equation*}
</div>
<p id="p-5891">So each <span class="process-math">\(y_i\)</span> is a linear combination of the original variables (contained in the <span class="process-math">\(\vc_i\)</span>) with weights from the first principal component. The other new variables are obtained in the same way from the remaining principal components. So even though the principal components may not have an easy interpretation in context, they are connected to the original data in this way. By reducing the data to a few important principal components — that is, visualizing the data in a subspace of small dimension — we can account for almost all of the variation in the data and relate that information back to the original data.</p></section></section><div class="hidden-content tex2jax_ignore" id="hk-fn-58"><div class="fn">It might seem that we should divide by <span class="process-math">\(n\)</span> instead of <span class="process-math">\(n-1\)</span> in the variance, but it is generally accepted to do this for reasons we won't get into. Suffice it to say that if we are using a sample of the entire population, then dividing by <span class="process-math">\(n-1\)</span> provides a variance whose square root is closer to the standard deviation than we would get if we divide by <span class="process-math">\(n\text{.}\)</span> If we are calculating the variance of an entire population, then we would divide by <span class="process-math">\(n\text{.}\)</span>
</div></div>
</div></main>
</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/components/prism-core.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/autoloader/prism-autoloader.min.js"></script>
</body>
</html>
