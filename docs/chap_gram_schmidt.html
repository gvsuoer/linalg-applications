<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*       on 2022-06-08T14:59:25-04:00       *-->
<!--*   A recent stable commit (2020-08-09):   *-->
<!--* 98f21740783f166a773df4dc83cab5293ab63a4a *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US">
<head xmlns:og="http://ogp.me/ns#" xmlns:book="https://ogp.me/ns/book#">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Projections onto Subspaces and the Gram-Schmidt Process in \R^n</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:type" content="book">
<meta property="book:title" content="An Inquiry-Based Introduction to Linear Algebra and Applications">
<meta property="book:author" content="Feryal Alayont">
<meta property="book:author" content="Steven Schlicker">
<script>window.MathJax = {
  tex: {
    inlineMath: [['\\(','\\)']],
    tags: "none",
    tagSide: "right",
    tagIndent: ".8em",
    packages: {'[+]': ['base', 'extpfeil', 'ams', 'amscd', 'newcommand', 'knowl']}
  },
  options: {
    ignoreHtmlClass: "tex2jax_ignore|ignore-math",
    processHtmlClass: "process-math",
  },
  chtml: {
    scale: 0.88,
    mtextInheritFont: true
  },
  loader: {
    load: ['input/asciimath', '[tex]/extpfeil', '[tex]/amscd', '[tex]/newcommand', '[pretext]/mathjaxknowl3.js'],
    paths: {pretext: "https://pretextbook.org/js/lib"},
  },
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/themes/prism.css" rel="stylesheet">
<script async="" src="https://cse.google.com/cse.js?cx=a16e70a6cb1434676"></script><script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.13/pretext.js"></script><script>miniversion=0.674</script><script src="https://pretextbook.org/js/0.13/pretext_add_on.js?x=1"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script>sagecellEvalName='Evaluate (Sage)';
</script><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/banner_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/toc_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/knowls_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/style_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/colors_blue_grey.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/setcolors.css" rel="stylesheet" type="text/css">
<!-- 2019-10-12: Temporary - CSS file for experiments with styling --><link href="developer.css" rel="stylesheet" type="text/css">
</head>
<body class="pretext-book ignore-math has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div id="latex-macros" class="hidden-content process-math" style="display:none"><span class="process-math">\(\usepackage{amsmath}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\E}{\mathbb{E}}
\DeclareMathOperator{\ch}{char}
\newcommand{\N}{\mathbb{N}}
\newcommand{\W}{\mathbb{W}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\J}{\mathbb{J}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathbb{D}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\NE}{\mathcal{E}}
\newcommand{\Mn}[1]{\mathcal{M}_{#1 \times #1}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\Rn}{\R^n}
\newcommand{\Mat}{\mathbf}
\newcommand{\Seq}{\boldsymbol}
\newcommand{\seq}[1]{\boldsymbol{#1}}
\newcommand{\set}[1]{\mathcal{#1}}
\newcommand{\abs}[1]{\left\lvert{}#1\right\rvert}
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\cq}{\scalebox{.34}{\pscirclebox{ \textbf{?}}}}
\newcommand{\cqup}{\,$^{\text{\scalebox{.2}{\pscirclebox{\textbf{?}}}}}$}
\newcommand{\cqupmath}{\,^{\text{\scalebox{.2}{\pscirclebox{\textbf{?}}}}}}
\newcommand{\cqupmathnospace}{^{\text{\scalebox{.2}{\pscirclebox{\textbf{?}}}}}}
\newcommand{\uspace}[1]{\underline{}}
\newcommand{\muspace}[1]{\underline{\mspace{#1 mu}}}
\newcommand{\bspace}[1]{}
\newcommand{\ie}{\emph{i}.\emph{e}.}
\newcommand{\nq}[1]{\scalebox{.34}{\pscirclebox{\textbf{#1}}}}
\newcommand{\equalwhy}{\stackrel{\cqupmath}{=}}
\newcommand{\notequalwhy}{\stackrel{\cqupmath}{\neq}}
\newcommand{\Ker}{\text{Ker}}
\newcommand{\Image}{\text{Im}}
\newcommand{\polyp}{p(x) = a_nx^n + a_{n-1}x^{n-1} + \cdots a_2x^2 + a_1x + a_0}
\newcommand{\GL}{\text{GL}}
\newcommand{\SL}{\text{SL}}
\newcommand{\lcm}{\text{lcm}}
\newcommand{\Aut}{\text{Aut}}
\newcommand{\Inn}{\text{Inn}}
\newcommand{\Hol}{\text{Hol}}
\newcommand{\cl}{\text{cl}}
\newcommand{\bsq}{\hfill $\blacksquare$}
\newcommand{\NIMdot}{{ $\cdot$ }}
\newcommand{\eG}{e_{\scriptscriptstyle{G}}}
\newcommand{\eGroup}[1]{e_{\scriptscriptstyle{#1}}}
\newcommand{\Gdot}[1]{\cdot_{\scriptscriptstyle{#1}}}
\newcommand{\rbar}{\overline{r}}
\newcommand{\nuG}[1]{\nu_{\scriptscriptstyle{#1}}}
\newcommand{\rightarray}[2]{\left[ \begin{array}{#1} #2 \end{array} \right]}
\newcommand{\polymod}[1]{\mspace{5 mu}(\text{mod} #1)}
\newcommand{\ts}{\mspace{2 mu}}
\newcommand{\ds}{\displaystyle}
\newcommand{\adj}{\text{adj}}
\newcommand{\pol}{\mathbb{P}}
\newcommand{\CS}{\mathcal{S}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\CB}{\mathcal{B}}
\renewcommand{\CD}{\mathcal{D}}
\newcommand{\CP}{\mathcal{P}}
\newcommand{\CQ}{\mathcal{Q}}
\newcommand{\CW}{\mathcal{W}}
\newcommand{\rank}{\text{rank}}
\newcommand{\nullity}{\text{nullity}}
\newcommand{\trace}{\text{trace}}
\newcommand{\Area}{\text{Area}}
\newcommand{\Vol}{\text{Vol}}
\newcommand{\cov}{\text{cov}}
\newcommand{\var}{\text{var}}
\newcommand{\va}{\mathbf{a}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\vc}{\mathbf{c}}
\newcommand{\vd}{\mathbf{d}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\vf}{\mathbf{f}}
\newcommand{\vg}{\mathbf{g}}
\newcommand{\vh}{\mathbf{h}}
\newcommand{\vm}{\mathbf{m}}
\newcommand{\vn}{\mathbf{n}}
\newcommand{\vp}{\mathbf{p}}
\newcommand{\vq}{\mathbf{q}}
\newcommand{\vr}{\mathbf{r}}
\newcommand{\vs}{\mathbf{s}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\vzero}{\mathbf{0}}
\newcommand{\vi}{\mathbf{i}}
\newcommand{\vj}{\mathbf{j}}
\newcommand{\vk}{\mathbf{k}}
\newcommand{\vF}{\mathbf{F}}
\newcommand{\vP}{\mathbf{P}}
\newcommand{\nin}{}
\newcommand{\Dom}{\text{Dom}}
\newcommand{\tr}{\mathsf{T}}
\newcommand{\proj}{\text{proj}}
\newcommand{\comp}{\text{comp}}
\newcommand{\Row}{\text{Row }}
\newcommand{\Col}{\text{Col }}
\newcommand{\Nul}{\text{Nul }}
\newcommand{\Span}{\text{Span}}
\newcommand{\Range}{\text{Range}}
\newcommand{\Domain}{\text{Domain}}
\newcommand{\hthin}{\hlinewd{.1pt}}
\newcommand{\hthick}{\hlinewd{.7pt}}
\newcommand{\pbreaks}{1}
\newcommand{\pbreak}{
}
\newcommand{\lint}{\underline{}
\int}
\newcommand{\uint}{ \underline{}
\int}


\newcommand{\Si}{\text{Si}}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\)</span></div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="book-1.html"><span class="title">An Inquiry-Based Introduction to Linear Algebra and Applications</span></a></h1>
<p class="byline">Feryal Alayont, Steven Schlicker</p>
</div>
<div class="searchwrapper" role="search"><div class="gcse-search"></div></div>
</div></div>
<nav id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3">
<a class="index-button toolbar-item button" href="index-1.html" title="Index">Index</a><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="chap_orthogonal_basis.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="part-orthog.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="chap_least_squares.html" title="Next">Next</a></span>
</div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="chap_orthogonal_basis.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="part-orthog.html" title="Up">Up</a><a class="next-button button toolbar-item" href="chap_least_squares.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link frontmatter">
<a href="frontmatter.html" data-scroll="frontmatter" class="internal"><span class="title">Front Matter</span></a><ul><li><a href="fm_preface.html" data-scroll="fm_preface" class="internal">Preface</a></li></ul>
</li>
<li class="link part"><a href="part-systems.html" data-scroll="part-systems" class="internal"><span class="codenumber">I</span> <span class="title">Systems of Linear Equations</span></a></li>
<li class="link">
<a href="chap_intro_linear_systems.html" data-scroll="chap_intro_linear_systems" class="internal"><span class="codenumber">1</span> <span class="title">Introduction to Systems of Linear Equations</span></a><ul>
<li><a href="chap_intro_linear_systems.html#sec_appl_elec_circuits" data-scroll="sec_appl_elec_circuits" class="internal">Application: Electrical Circuits</a></li>
<li><a href="chap_intro_linear_systems.html#sec_intro_le_intro" data-scroll="sec_intro_le_intro" class="internal">Introduction</a></li>
<li><a href="chap_intro_linear_systems.html#sec_notation" data-scroll="sec_notation" class="internal">Notation and Terminology</a></li>
<li><a href="chap_intro_linear_systems.html#sec_solve_systems" data-scroll="sec_solve_systems" class="internal">Solving Systems of Linear Equations</a></li>
<li><a href="chap_intro_linear_systems.html#sec_geom_solu_sets" data-scroll="sec_geom_solu_sets" class="internal">The Geometry of Solution Sets of Linear Systems</a></li>
<li><a href="chap_intro_linear_systems.html#sec_intro_le_exam" data-scroll="sec_intro_le_exam" class="internal">Examples</a></li>
<li><a href="chap_intro_linear_systems.html#sec_intro_le_summ" data-scroll="sec_intro_le_summ" class="internal">Summary</a></li>
<li><a href="chap_intro_linear_systems.html#sec_intro_le_exer" data-scroll="sec_intro_le_exer" class="internal">Exercises</a></li>
<li><a href="chap_intro_linear_systems.html#sec_1_a_circuits" data-scroll="sec_1_a_circuits" class="internal">Project: Modeling an Electrical Circuit and the Wheatstone Bridge Circuit</a></li>
</ul>
</li>
<li class="link">
<a href="chap_matrix_representation.html" data-scroll="chap_matrix_representation" class="internal"><span class="codenumber">2</span> <span class="title">The Matrix Representation of a Linear System</span></a><ul>
<li><a href="chap_matrix_representation.html#sec_appl_area_curve" data-scroll="sec_appl_area_curve" class="internal">Application: Approximating Area Under a Curve</a></li>
<li><a href="chap_matrix_representation.html#sec_mtx_lin_intro" data-scroll="sec_mtx_lin_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_representation.html#sec_simp_mtx_sys" data-scroll="sec_simp_mtx_sys" class="internal">Simplifying Linear Systems Represented in Matrix Form</a></li>
<li><a href="chap_matrix_representation.html#sec_sys_inf_sols" data-scroll="sec_sys_inf_sols" class="internal">Linear Systems with Infinitely Many Solutions</a></li>
<li><a href="chap_matrix_representation.html#sec_sys_no_sols" data-scroll="sec_sys_no_sols" class="internal">Linear Systems with No Solutions</a></li>
<li><a href="chap_matrix_representation.html#sec_mtx_sys_exam" data-scroll="sec_mtx_sys_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_representation.html#sec_mtx_sys_summ" data-scroll="sec_mtx_sys_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_representation.html#sec_mtx_sys_exer" data-scroll="sec_mtx_sys_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_representation.html#sec_1_b_polynomial" data-scroll="sec_1_b_polynomial" class="internal">Project: Polynomial Interpolation to Approximate the Area Under a Curve</a></li>
</ul>
</li>
<li class="link">
<a href="chap_row_echelon_forms.html" data-scroll="chap_row_echelon_forms" class="internal"><span class="codenumber">3</span> <span class="title">Row Echelon Forms</span></a><ul>
<li><a href="chap_row_echelon_forms.html#sec_appl_chem_react" data-scroll="sec_appl_chem_react" class="internal">Application: Balancing Chemical Reactions</a></li>
<li><a href="chap_row_echelon_forms.html#sec_row_ech_intro" data-scroll="sec_row_ech_intro" class="internal">Introduction</a></li>
<li><a href="chap_row_echelon_forms.html#sec_mtx_ech_forms" data-scroll="sec_mtx_ech_forms" class="internal">The Echelon Forms of a Matrix</a></li>
<li><a href="chap_row_echelon_forms.html#sec_num_sols_ls" data-scroll="sec_num_sols_ls" class="internal">Determining the Number of Solutions of a Linear System</a></li>
<li><a href="chap_row_echelon_forms.html#sec_prod_ech_forms" data-scroll="sec_prod_ech_forms" class="internal">Producing the Echelon Forms</a></li>
<li><a href="chap_row_echelon_forms.html#sec_row_ech_exam" data-scroll="sec_row_ech_exam" class="internal">Examples</a></li>
<li><a href="chap_row_echelon_forms.html#sec_row_ech_summ" data-scroll="sec_row_ech_summ" class="internal">Summary</a></li>
<li><a href="chap_row_echelon_forms.html#sec_row_ech_exer" data-scroll="sec_row_ech_exer" class="internal">Exercises</a></li>
<li><a href="chap_row_echelon_forms.html#sec_prof_chem_react" data-scroll="sec_prof_chem_react" class="internal">Project: Modeling a Chemical Reaction</a></li>
</ul>
</li>
<li class="link">
<a href="chap_vector_representation.html" data-scroll="chap_vector_representation" class="internal"><span class="codenumber">4</span> <span class="title">Vector Representation</span></a><ul>
<li><a href="chap_vector_representation.html#sec_appl_knight" data-scroll="sec_appl_knight" class="internal">Application: The Knight's Tour</a></li>
<li><a href="chap_vector_representation.html#sec_vec_rep_intro" data-scroll="sec_vec_rep_intro" class="internal">Introduction</a></li>
<li><a href="chap_vector_representation.html#sec_vec_ops" data-scroll="sec_vec_ops" class="internal">Vectors and Vector Operations</a></li>
<li><a href="chap_vector_representation.html#sec_geom_vec_ops" data-scroll="sec_geom_vec_ops" class="internal">Geometric Representation of Vectors and Vector Operations</a></li>
<li><a href="chap_vector_representation.html#sec_lin_comb_vec" data-scroll="sec_lin_comb_vec" class="internal">Linear Combinations of Vectors</a></li>
<li><a href="chap_vector_representation.html#sec_vec_span" data-scroll="sec_vec_span" class="internal">The Span of a Set of Vectors</a></li>
<li><a href="chap_vector_representation.html#sec_vec_rep_exam" data-scroll="sec_vec_rep_exam" class="internal">Examples</a></li>
<li><a href="chap_vector_representation.html#sec_vec_rep_summ" data-scroll="sec_vec_rep_summ" class="internal">Summary</a></li>
<li><a href="chap_vector_representation.html#exercises-4" data-scroll="exercises-4" class="internal">Exercises</a></li>
<li><a href="chap_vector_representation.html#sec_proj_knight" data-scroll="sec_proj_knight" class="internal">Project: Analyzing Knight Moves</a></li>
</ul>
</li>
<li class="link">
<a href="chap_matrix_vector.html" data-scroll="chap_matrix_vector" class="internal"><span class="codenumber">5</span> <span class="title">The Matrix-Vector Form of a Linear System</span></a><ul>
<li><a href="chap_matrix_vector.html#sec_appl_model_econ" data-scroll="sec_appl_model_econ" class="internal">Application: Modeling an Economy</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form_intro" data-scroll="sec_mv_form_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_prod" data-scroll="sec_mv_prod" class="internal">The Matrix-Vector Product</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form" data-scroll="sec_mv_form" class="internal">The Matrix-Vector Form of a Linear System</a></li>
<li><a href="chap_matrix_vector.html#sec_homog_sys" data-scroll="sec_homog_sys" class="internal">Homogeneous and Nonhomogeneous Systems</a></li>
<li><a href="chap_matrix_vector.html#sec_geom_homog_sys" data-scroll="sec_geom_homog_sys" class="internal">The Geometry of Solutions to the Homogeneous System</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form_exam" data-scroll="sec_mv_form_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form_summ" data-scroll="sec_mv_form_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form_exer" data-scroll="sec_mv_form_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_vector.html#sec_proj_io_models" data-scroll="sec_proj_io_models" class="internal">Project: Input-Output Models</a></li>
</ul>
</li>
<li class="link">
<a href="chap_independence.html" data-scroll="chap_independence" class="internal"><span class="codenumber">6</span> <span class="title">Linear Dependence and Independence</span></a><ul>
<li><a href="chap_independence.html#sec_appl_bezier" data-scroll="sec_appl_bezier" class="internal">Application: Bézier Curves</a></li>
<li><a href="chap_independence.html#sec_indep_intro" data-scroll="sec_indep_intro" class="internal">Introduction</a></li>
<li><a href="chap_independence.html#lin_indep_intro_new" data-scroll="lin_indep_intro_new" class="internal">Linear Independence</a></li>
<li><a href="chap_independence.html#sec_determ_lin_ind" data-scroll="sec_determ_lin_ind" class="internal">Determining Linear Independence</a></li>
<li><a href="chap_independence.html#sec_min_span_set" data-scroll="sec_min_span_set" class="internal">Minimal Spanning Sets</a></li>
<li><a href="chap_independence.html#sec_indep_exam" data-scroll="sec_indep_exam" class="internal">Examples</a></li>
<li><a href="chap_independence.html#sec_indep_summ" data-scroll="sec_indep_summ" class="internal">Summary</a></li>
<li><a href="chap_independence.html#sec_indep_exer" data-scroll="sec_indep_exer" class="internal">Exercises</a></li>
<li><a href="chap_independence.html#sec_proj_bezier" data-scroll="sec_proj_bezier" class="internal">Project: Generating Bézier Curves</a></li>
</ul>
</li>
<li class="link">
<a href="chap_matrix_transformations.html" data-scroll="chap_matrix_transformations" class="internal"><span class="codenumber">7</span> <span class="title">Matrix Transformations</span></a><ul>
<li><a href="chap_matrix_transformations.html#sec_appl_graphics" data-scroll="sec_appl_graphics" class="internal">Application: Computer Graphics</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_intro" data-scroll="sec_mtx_trans_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_prop" data-scroll="sec_mtx_trans_prop" class="internal">Properties of Matrix Transformations</a></li>
<li><a href="chap_matrix_transformations.html#sec_trans_onto_oto" data-scroll="sec_trans_onto_oto" class="internal">Onto and One-to-One Transformations</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_exam" data-scroll="sec_mtx_trans_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_summ" data-scroll="sec_mtx_trans_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_exer" data-scroll="sec_mtx_trans_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_transformations.html#sec_proj_geom_mtx" data-scroll="sec_proj_geom_mtx" class="internal">Project: The Geometry of Matrix Transformations</a></li>
</ul>
</li>
<li class="link part"><a href="part-matrices.html" data-scroll="part-matrices" class="internal"><span class="codenumber">II</span> <span class="title">Matrices</span></a></li>
<li class="link">
<a href="chap_matrix_operations.html" data-scroll="chap_matrix_operations" class="internal"><span class="codenumber">8</span> <span class="title">Matrix Operations</span></a><ul>
<li><a href="chap_matrix_operations.html#sec_appl_mtx_mult" data-scroll="sec_appl_mtx_mult" class="internal">Application: Algorithms for Matrix Multiplication</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_ops_intro" data-scroll="sec_mtx_ops_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_add_smult" data-scroll="sec_mtx_add_smult" class="internal">Properties of Matrix Addition and Multiplication by Scalars</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_prod" data-scroll="sec_mtx_prod" class="internal">A Matrix Product</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_transpose" data-scroll="sec_mtx_transpose" class="internal">The Transpose of a Matrix</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_transpose_prop" data-scroll="sec_mtx_transpose_prop" class="internal">Properties of the Matrix Transpose</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_ops_exam" data-scroll="sec_mtx_ops_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_ops_summ" data-scroll="sec_mtx_ops_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_ops_exer" data-scroll="sec_mtx_ops_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_operations.html#sec_proj_starassen" data-scroll="sec_proj_starassen" class="internal">Project: Strassen's Algorithm and Partitioned Matrices</a></li>
</ul>
</li>
<li class="link">
<a href="chap_intro_eigenvals_eigenvects.html" data-scroll="chap_intro_eigenvals_eigenvects" class="internal"><span class="codenumber">9</span> <span class="title">Introduction to Eigenvalues and Eigenvectors</span></a><ul>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_appl_pagerank" data-scroll="sec_appl_pagerank" class="internal">Application: The Google PageRank Algorithm</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigen_intro" data-scroll="sec_eigen_intro" class="internal">Introduction</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigval_eigvec" data-scroll="sec_eigval_eigvec" class="internal">Eigenvalues and Eigenvectors</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_dynam_sys" data-scroll="sec_dynam_sys" class="internal">Dynamical Systems</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigen_exam" data-scroll="sec_eigen_exam" class="internal">Examples</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigen_summ" data-scroll="sec_eigen_summ" class="internal">Summary</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigen_exer" data-scroll="sec_eigen_exer" class="internal">Exercises</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_proj_pagerank" data-scroll="sec_proj_pagerank" class="internal">Project: Understanding the PageRank Algorithm</a></li>
</ul>
</li>
<li class="link">
<a href="chap_matrix_inverse.html" data-scroll="chap_matrix_inverse" class="internal"><span class="codenumber">10</span> <span class="title">The Inverse of a Matrix</span></a><ul>
<li><a href="chap_matrix_inverse.html#sec_appl_arms_race" data-scroll="sec_appl_arms_race" class="internal">Application: Modeling an Arms Race</a></li>
<li><a href="chap_matrix_inverse.html#sec_inverse_intro" data-scroll="sec_inverse_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_inverse.html#sec_mtx_invertible" data-scroll="sec_mtx_invertible" class="internal">Invertible Matrices</a></li>
<li><a href="chap_matrix_inverse.html#sec_mtx_inverse" data-scroll="sec_mtx_inverse" class="internal">Finding the Inverse of a Matrix</a></li>
<li><a href="chap_matrix_inverse.html#sec_mtx_inverse_props" data-scroll="sec_mtx_inverse_props" class="internal">Properties of the Matrix Inverse</a></li>
<li><a href="chap_matrix_inverse.html#sec_inverse_exam" data-scroll="sec_inverse_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_inverse.html#sec_inverse_summ" data-scroll="sec_inverse_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_inverse.html#sec_inverse_exer" data-scroll="sec_inverse_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_inverse.html#sec_proj_arms_race" data-scroll="sec_proj_arms_race" class="internal">Project: The Richardson Arms Race Model</a></li>
</ul>
</li>
<li class="link">
<a href="chap_IMT.html" data-scroll="chap_IMT" class="internal"><span class="codenumber">11</span> <span class="title">The Invertible Matrix Theorem</span></a><ul>
<li><a href="chap_IMT.html#sec_imt_intro" data-scroll="sec_imt_intro" class="internal">Introduction</a></li>
<li><a href="chap_IMT.html#sec_imt" data-scroll="sec_imt" class="internal">The Invertible Matrix Theorem</a></li>
<li><a href="chap_IMT.html#sec_imt_exam" data-scroll="sec_imt_exam" class="internal">Examples</a></li>
<li><a href="chap_IMT.html#sec_imt_summ" data-scroll="sec_imt_summ" class="internal">Summary</a></li>
<li><a href="chap_IMT.html#sec_imt_exer" data-scroll="sec_imt_exer" class="internal">Exercises</a></li>
</ul>
</li>
<li class="link part"><a href="part-vector-rn.html" data-scroll="part-vector-rn" class="internal"><span class="codenumber">III</span> <span class="title">The Vector Space <span class="process-math">\(\R^n\)</span></span></a></li>
<li class="link">
<a href="chap_R_n.html" data-scroll="chap_R_n" class="internal"><span class="codenumber">12</span> <span class="title">The Structure of <span class="process-math">\(\R^n\)</span></span></a><ul>
<li><a href="chap_R_n.html#sec_appl_romania" data-scroll="sec_appl_romania" class="internal">Application: Connecting GDP and Consumption in Romania</a></li>
<li><a href="chap_R_n.html#sec_rn_intro" data-scroll="sec_rn_intro" class="internal">Introduction</a></li>
<li><a href="chap_R_n.html#sec_vec_spaces" data-scroll="sec_vec_spaces" class="internal">Vector Spaces</a></li>
<li><a href="chap_R_n.html#sec_sub_space_span" data-scroll="sec_sub_space_span" class="internal">The Subspace Spanned by a Set of Vectors</a></li>
<li><a href="chap_R_n.html#sec_rn_exam" data-scroll="sec_rn_exam" class="internal">Examples</a></li>
<li><a href="chap_R_n.html#sec_rn_summ" data-scroll="sec_rn_summ" class="internal">Summary</a></li>
<li><a href="chap_R_n.html#sec_rn_exer" data-scroll="sec_rn_exer" class="internal">Exercises</a></li>
<li><a href="chap_R_n.html#sec_proj_ls_approx" data-scroll="sec_proj_ls_approx" class="internal">Project: Least Sqaures Linear Approximation</a></li>
</ul>
</li>
<li class="link">
<a href="chap_null_space.html" data-scroll="chap_null_space" class="internal"><span class="codenumber">13</span> <span class="title">The Null Space and Column Space of a Matrix</span></a><ul>
<li><a href="chap_null_space.html#sec_appl_lights_out" data-scroll="sec_appl_lights_out" class="internal">Application: The Lights Out Game</a></li>
<li><a href="chap_null_space.html#sec_null_intro" data-scroll="sec_null_intro" class="internal">Introduction</a></li>
<li><a href="chap_null_space.html#sec_null_kernel" data-scroll="sec_null_kernel" class="internal">The Null Space of a Matrix and the Kernel of a Matrix Transformation</a></li>
<li><a href="chap_null_space.html#sec_column_range" data-scroll="sec_column_range" class="internal">The Column Space of a Matrix and the Range of a Matrix Transformation</a></li>
<li><a href="chap_null_space.html#sec_row_space" data-scroll="sec_row_space" class="internal">The Row Space of a Matrix</a></li>
<li><a href="chap_null_space.html#sec_null_col_base" data-scroll="sec_null_col_base" class="internal">Bases for <span class="process-math">\(\Nul A\)</span> and <span class="process-math">\(\Col A\)</span></a></li>
<li><a href="chap_null_space.html#sec_null_exam" data-scroll="sec_null_exam" class="internal">Examples</a></li>
<li><a href="chap_null_space.html#sec_null_summ" data-scroll="sec_null_summ" class="internal">Summary</a></li>
<li><a href="chap_null_space.html#sec_null_exer" data-scroll="sec_null_exer" class="internal">Exercises</a></li>
<li><a href="chap_null_space.html#sec_proj_lights_out" data-scroll="sec_proj_lights_out" class="internal">Project: Solving the Lights Out Game</a></li>
</ul>
</li>
<li class="link">
<a href="chap_eigenspaces.html" data-scroll="chap_eigenspaces" class="internal"><span class="codenumber">14</span> <span class="title">Eigenspaces of a Matrix</span></a><ul>
<li><a href="chap_eigenspaces.html#sec_appl_pop_dynam" data-scroll="sec_appl_pop_dynam" class="internal">Application: Population Dynamics</a></li>
<li><a href="chap_eigenspaces.html#sec_egspace_intro" data-scroll="sec_egspace_intro" class="internal">Introduction</a></li>
<li><a href="chap_eigenspaces.html#sec_mtx_egspace" data-scroll="sec_mtx_egspace" class="internal">Eigenspaces of Matrix</a></li>
<li><a href="chap_eigenspaces.html#sec_lin_ind_egvec" data-scroll="sec_lin_ind_egvec" class="internal">Linearly Independent Eigenvectors</a></li>
<li><a href="chap_eigenspaces.html#sec_egspace_exam" data-scroll="sec_egspace_exam" class="internal">Examples</a></li>
<li><a href="chap_eigenspaces.html#sec_egspace_summ" data-scroll="sec_egspace_summ" class="internal">Summary</a></li>
<li><a href="chap_eigenspaces.html#sec_egspace_exer" data-scroll="sec_egspace_exer" class="internal">Exercises</a></li>
<li><a href="chap_eigenspaces.html#sec_proj_migration" data-scroll="sec_proj_migration" class="internal">Project: Modeling Population Migration</a></li>
</ul>
</li>
<li class="link">
<a href="chap_bases_dimension.html" data-scroll="chap_bases_dimension" class="internal"><span class="codenumber">15</span> <span class="title">Bases and Dimension</span></a><ul>
<li><a href="chap_bases_dimension.html#sec_appl_latt_crypt" data-scroll="sec_appl_latt_crypt" class="internal">Application: Lattice Based Cryptography</a></li>
<li><a href="chap_bases_dimension.html#sec_base_dim_intro" data-scroll="sec_base_dim_intro" class="internal">Introduction</a></li>
<li><a href="chap_bases_dimension.html#sec_dim_sub_rn" data-scroll="sec_dim_sub_rn" class="internal">The Dimension of a Subspace of <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_bases_dimension.html#sec_cond_basis_subspace" data-scroll="sec_cond_basis_subspace" class="internal">Conditions for a Basis of a Subspace of <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_bases_dimension.html#sec_find_basis_subspace" data-scroll="sec_find_basis_subspace" class="internal">Finding a Basis for a Subspace</a></li>
<li><a href="chap_bases_dimension.html#sec_mtx_rank" data-scroll="sec_mtx_rank" class="internal">Rank of a Matrix</a></li>
<li><a href="chap_bases_dimension.html#sec_base_dim_exam" data-scroll="sec_base_dim_exam" class="internal">Examples</a></li>
<li><a href="chap_bases_dimension.html#sec_base_dim_summ" data-scroll="sec_base_dim_summ" class="internal">Summary</a></li>
<li><a href="chap_bases_dimension.html#sec_base_dim_exer" data-scroll="sec_base_dim_exer" class="internal">Exercises</a></li>
<li><a href="chap_bases_dimension.html#sec_proj_ggh_crypto" data-scroll="sec_proj_ggh_crypto" class="internal">Project: The GGH Cryptosystem</a></li>
</ul>
</li>
<li class="link">
<a href="chap_coordinate_vectors.html" data-scroll="chap_coordinate_vectors" class="internal"><span class="codenumber">16</span> <span class="title">Coordinate Vectors and Change of Basis</span></a><ul>
<li><a href="chap_coordinate_vectors.html#sec_appl_orbits" data-scroll="sec_appl_orbits" class="internal">Application: Describing Orbits of Planets</a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_intro" data-scroll="sec_cob_intro" class="internal">Introduction</a></li>
<li><a href="chap_coordinate_vectors.html#sec_coor_base" data-scroll="sec_coor_base" class="internal">Bases as Coordinate Systems in <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_rn" data-scroll="sec_cob_rn" class="internal">Change of Basis in <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_coordinate_vectors.html#sec_mtx_cob" data-scroll="sec_mtx_cob" class="internal">The Change of Basis Matrix in <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_coordinate_vectors.html#sec_prop_mtx_cob" data-scroll="sec_prop_mtx_cob" class="internal">Properties of the Change of Basis Matrix</a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_exam" data-scroll="sec_cob_exam" class="internal">Examples</a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_summ" data-scroll="sec_cob_summ" class="internal">Summary</a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_exer" data-scroll="sec_cob_exer" class="internal">Exercises</a></li>
<li><a href="chap_coordinate_vectors.html#sec_proj_orbits_cob" data-scroll="sec_proj_orbits_cob" class="internal">Project: Planetary Orbits and Change of Basis</a></li>
</ul>
</li>
<li class="link part"><a href="part-eigen.html" data-scroll="part-eigen" class="internal"><span class="codenumber">IV</span> <span class="title">Eigenvalues and Eigenvectors</span></a></li>
<li class="link">
<a href="chap_determinants.html" data-scroll="chap_determinants" class="internal"><span class="codenumber">17</span> <span class="title">The Determinant</span></a><ul>
<li><a href="chap_determinants.html#sec_appl_area_vol" data-scroll="sec_appl_area_vol" class="internal">Application: Area and Volume</a></li>
<li><a href="chap_determinants.html#sec_det_intro" data-scroll="sec_det_intro" class="internal">Introduction</a></li>
<li><a href="chap_determinants.html#sec_det_square" data-scroll="sec_det_square" class="internal">The Determinant of a Square Matrix</a></li>
<li><a href="chap_determinants.html#sec_cofactors" data-scroll="sec_cofactors" class="internal">Cofactors</a></li>
<li><a href="chap_determinants.html#sec_det_3by3" data-scroll="sec_det_3by3" class="internal">The Determinant of a <span class="process-math">\(3 \times 3\)</span> Matrix</a></li>
<li><a href="chap_determinants.html#sec_det_remember" data-scroll="sec_det_remember" class="internal">Two Devices for Remembering Determinants</a></li>
<li><a href="chap_determinants.html#sec_det_exam" data-scroll="sec_det_exam" class="internal">Examples</a></li>
<li><a href="chap_determinants.html#sec_det_summ" data-scroll="sec_det_summ" class="internal">Summary</a></li>
<li><a href="chap_determinants.html#sec_det_exer" data-scroll="sec_det_exer" class="internal">Exercises</a></li>
<li><a href="chap_determinants.html#sec_proj_det_area_vol" data-scroll="sec_proj_det_area_vol" class="internal">Project: Area and Volume Using Determinants</a></li>
</ul>
</li>
<li class="link">
<a href="chap_characteristic_equation.html" data-scroll="chap_characteristic_equation" class="internal"><span class="codenumber">18</span> <span class="title">The Characteristic Equation</span></a><ul>
<li><a href="chap_characteristic_equation.html#sec_appl_thermo" data-scroll="sec_appl_thermo" class="internal">Application: Modeling the Second Law of Thermodynamics</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq_intro" data-scroll="sec_chareq_intro" class="internal">Introduction</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq" data-scroll="sec_chareq" class="internal">The Characteristic Equation</a></li>
<li><a href="chap_characteristic_equation.html#sec_egspace_geom" data-scroll="sec_egspace_geom" class="internal">Eigenspaces, A Geometric Example</a></li>
<li><a href="chap_characteristic_equation.html#sec_egspace_dims" data-scroll="sec_egspace_dims" class="internal">Dimensions of Eigenspaces</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq_exam" data-scroll="sec_chareq_exam" class="internal">Examples</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq_summ" data-scroll="sec_chareq_summ" class="internal">Summary</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq_exer" data-scroll="sec_chareq_exer" class="internal">Exercises</a></li>
<li><a href="chap_characteristic_equation.html#sec_proj_ehrenfest" data-scroll="sec_proj_ehrenfest" class="internal">Project: The Ehrenfest Model</a></li>
</ul>
</li>
<li class="link">
<a href="chap_diagonalization.html" data-scroll="chap_diagonalization" class="internal"><span class="codenumber">19</span> <span class="title">Diagonalization</span></a><ul>
<li><a href="chap_diagonalization.html#sec_appl_fib_num" data-scroll="sec_appl_fib_num" class="internal">Application: The Fibonacci Numbers</a></li>
<li><a href="chap_diagonalization.html#sec_diag_intro" data-scroll="sec_diag_intro" class="internal">Introduction</a></li>
<li><a href="chap_diagonalization.html#sec_diag" data-scroll="sec_diag" class="internal">Diagonalization</a></li>
<li><a href="chap_diagonalization.html#sec_mtx_similar" data-scroll="sec_mtx_similar" class="internal">Similar Matrices</a></li>
<li><a href="chap_diagonalization.html#sec_sim_mtx_trans" data-scroll="sec_sim_mtx_trans" class="internal">Similarity and Matrix Transformations</a></li>
<li><a href="chap_diagonalization.html#sec_diag_general" data-scroll="sec_diag_general" class="internal">Diagonalization in General</a></li>
<li><a href="chap_diagonalization.html#sec_diag_exam" data-scroll="sec_diag_exam" class="internal">Examples</a></li>
<li><a href="chap_diagonalization.html#sec_diag_summ" data-scroll="sec_diag_summ" class="internal">Summary</a></li>
<li><a href="chap_diagonalization.html#sec_diag_exer" data-scroll="sec_diag_exer" class="internal">Exercises</a></li>
<li><a href="chap_diagonalization.html#sec_proj_binet_fibo" data-scroll="sec_proj_binet_fibo" class="internal">Project: Binet's Formula for the Fibonacci Numbers</a></li>
</ul>
</li>
<li class="link">
<a href="chap_approx_eigenvalues.html" data-scroll="chap_approx_eigenvalues" class="internal"><span class="codenumber">20</span> <span class="title">Approximating Eigenvalues and Eigenvectors</span></a><ul>
<li><a href="chap_approx_eigenvalues.html#sec_appl_leslie_mtx" data-scroll="sec_appl_leslie_mtx" class="internal">Application: Leslie Matrices and Population Modeling</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_app_eigen_intro" data-scroll="sec_app_eigen_intro" class="internal">Introduction</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_power_method" data-scroll="sec_power_method" class="internal">The Power Method</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_power_method_inv" data-scroll="sec_power_method_inv" class="internal">The Inverse Power Method</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_app_eigen_exam" data-scroll="sec_app_eigen_exam" class="internal">Examples</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_app_eigen_summ" data-scroll="sec_app_eigen_summ" class="internal">Summary</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_app_eigen_exer" data-scroll="sec_app_eigen_exer" class="internal">Exercises</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_proj_sheep_herd" data-scroll="sec_proj_sheep_herd" class="internal">Project: Managing a Sheep Herd</a></li>
</ul>
</li>
<li class="link">
<a href="chap_complex_eigenvalues.html" data-scroll="chap_complex_eigenvalues" class="internal"><span class="codenumber">21</span> <span class="title">Complex Eigenvalues</span></a><ul>
<li><a href="chap_complex_eigenvalues.html#sec_appl_gershgorin" data-scroll="sec_appl_gershgorin" class="internal">Application: The Gershgorin Disk Theorem</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen_intro" data-scroll="sec_comp_eigen_intro" class="internal">Introduction</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen" data-scroll="sec_comp_eigen" class="internal">Complex Eigenvalues</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_mtx_rotate_scale" data-scroll="sec_mtx_rotate_scale" class="internal">Rotation and Scaling Matrices</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_mtx_comp_eigen" data-scroll="sec_mtx_comp_eigen" class="internal">Matrices with Complex Eigenvalues</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen_exam" data-scroll="sec_comp_eigen_exam" class="internal">Examples</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen_summ" data-scroll="sec_comp_eigen_summ" class="internal">Summary</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen_exer" data-scroll="sec_comp_eigen_exer" class="internal">Exercises</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_proj_gershgorin" data-scroll="sec_proj_gershgorin" class="internal">Project: Understanding the Gershgorin Disk Theorem</a></li>
</ul>
</li>
<li class="link">
<a href="chap_det_properties.html" data-scroll="chap_det_properties" class="internal"><span class="codenumber">22</span> <span class="title">Properties of Determinants</span></a><ul>
<li><a href="chap_det_properties.html#sec_det_prop_intro" data-scroll="sec_det_prop_intro" class="internal">Introduction</a></li>
<li><a href="chap_det_properties.html#sec_det_row_ops" data-scroll="sec_det_row_ops" class="internal">Elementary Row Operations and Their Effects on the Determinant</a></li>
<li><a href="chap_det_properties.html#sec_mtx_elem" data-scroll="sec_mtx_elem" class="internal">Elementary Matrices</a></li>
<li><a href="chap_det_properties.html#sec_det_geom" data-scroll="sec_det_geom" class="internal">Geometric Interpretation of the Determinant</a></li>
<li><a href="chap_det_properties.html#sec_inv_cramers" data-scroll="sec_inv_cramers" class="internal">An Explicit Formula for the Inverse and Cramer's Rule</a></li>
<li><a href="chap_det_properties.html#sec_det_transpose" data-scroll="sec_det_transpose" class="internal">The Determinant of the Transpose</a></li>
<li><a href="chap_det_properties.html#sec_det_row_swap" data-scroll="sec_det_row_swap" class="internal">Row Swaps and Determinants</a></li>
<li><a href="chap_det_properties.html#sec_cofactor_expand" data-scroll="sec_cofactor_expand" class="internal">Cofactor Expansions</a></li>
<li><a href="chap_det_properties.html#sec_mtx_lu_factor" data-scroll="sec_mtx_lu_factor" class="internal">The LU Factorization of a Matrix</a></li>
<li><a href="chap_det_properties.html#sec_det_prop_exam" data-scroll="sec_det_prop_exam" class="internal">Examples</a></li>
<li><a href="chap_det_properties.html#sec_det_prop_summ" data-scroll="sec_det_prop_summ" class="internal">Summary</a></li>
<li><a href="chap_det_properties.html#sec_det_prop_exer" data-scroll="sec_det_prop_exer" class="internal">Exercises</a></li>
</ul>
</li>
<li class="link part"><a href="part-orthog.html" data-scroll="part-orthog" class="internal"><span class="codenumber">V</span> <span class="title">Orthogonality</span></a></li>
<li class="link">
<a href="chap_dot_product.html" data-scroll="chap_dot_product" class="internal"><span class="codenumber">23</span> <span class="title">The Dot Product in <span class="process-math">\(\R^n\)</span></span></a><ul>
<li><a href="chap_dot_product.html#sec_appl_figs_computer" data-scroll="sec_appl_figs_computer" class="internal">Application: Hidden Figures in Computer Graphics</a></li>
<li><a href="chap_dot_product.html#sec_dot_prod_intro" data-scroll="sec_dot_prod_intro" class="internal">Introduction</a></li>
<li><a href="chap_dot_product.html#sec_dist_vec" data-scroll="sec_dist_vec" class="internal">The Distance Between Vectors</a></li>
<li><a href="chap_dot_product.html#sec_angle_vec" data-scroll="sec_angle_vec" class="internal">The Angle Between Two Vectors</a></li>
<li><a href="chap_dot_product.html#sec_orthog_proj" data-scroll="sec_orthog_proj" class="internal">Orthogonal Projections</a></li>
<li><a href="chap_dot_product.html#sec_orthog_comp" data-scroll="sec_orthog_comp" class="internal">Orthogonal Complements</a></li>
<li><a href="chap_dot_product.html#sec_dot_prod_exam" data-scroll="sec_dot_prod_exam" class="internal">Examples</a></li>
<li><a href="chap_dot_product.html#sec_dot_prod_summ" data-scroll="sec_dot_prod_summ" class="internal">Summary</a></li>
<li><a href="chap_dot_product.html#sec_dot_prod_exer" data-scroll="sec_dot_prod_exer" class="internal">Exercises</a></li>
<li><a href="chap_dot_product.html#sec_proj_back_face" data-scroll="sec_proj_back_face" class="internal">Project: Back-Face Culling</a></li>
</ul>
</li>
<li class="link">
<a href="chap_orthogonal_basis.html" data-scroll="chap_orthogonal_basis" class="internal"><span class="codenumber">24</span> <span class="title">Orthogonal and Orthonormal Bases in <span class="process-math">\(\R^n\)</span></span></a><ul>
<li><a href="chap_orthogonal_basis.html#sec_appl_3d_rotate" data-scroll="sec_appl_3d_rotate" class="internal">Application: Rotations in 3D</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_set_intro" data-scroll="sec_orthog_set_intro" class="internal">Introduction</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_sets" data-scroll="sec_orthog_sets" class="internal">Orthogonal Sets</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_bases_prop" data-scroll="sec_orthog_bases_prop" class="internal">Properties of Orthogonal Bases</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthon_bases" data-scroll="sec_orthon_bases" class="internal">Orthonormal Bases</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_mtx" data-scroll="sec_orthog_mtx" class="internal">Orthogonal Matrices</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_set_exam" data-scroll="sec_orthog_set_exam" class="internal">Examples</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_set_summ" data-scroll="sec_orthog_set_summ" class="internal">Summary</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_set_exer" data-scroll="sec_orthog_set_exer" class="internal">Exercises</a></li>
<li><a href="chap_orthogonal_basis.html#sec_proj_3d_rotate" data-scroll="sec_proj_3d_rotate" class="internal">Project: Understanding Rotations in 3-Space</a></li>
</ul>
</li>
<li class="link active">
<a href="chap_gram_schmidt.html" data-scroll="chap_gram_schmidt" class="internal"><span class="codenumber">25</span> <span class="title">Projections onto Subspaces and the Gram-Schmidt Process in <span class="process-math">\(\R^n\)</span></span></a><ul>
<li><a href="chap_gram_schmidt.html#sec_mimo" data-scroll="sec_mimo" class="internal">Application: MIMO Systems</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_intro_noip" data-scroll="sec_gram_schmidt_intro_noip" class="internal">Introduction</a></li>
<li><a href="chap_gram_schmidt.html#sec_proj_subsp_orth" data-scroll="sec_proj_subsp_orth" class="internal">Projections onto Subspaces and Orthogonal Projections</a></li>
<li><a href="chap_gram_schmidt.html#sec_best_approx" data-scroll="sec_best_approx" class="internal">Best Approximations</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_process" data-scroll="sec_gram_schmidt_process" class="internal">The Gram-Schmidt Process</a></li>
<li><a href="chap_gram_schmidt.html#sec_qr_fact" data-scroll="sec_qr_fact" class="internal">The QR Factorization of a Matrix</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_examples" data-scroll="sec_gram_schmidt_examples" class="internal">Examples</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_summ_noips" data-scroll="sec_gram_schmidt_summ_noips" class="internal">Summary</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_exercises" data-scroll="sec_gram_schmidt_exercises" class="internal">Exercises</a></li>
<li><a href="chap_gram_schmidt.html#sec_project_mimo" data-scroll="sec_project_mimo" class="internal">Project: MIMO Systems and Householder Transformations</a></li>
</ul>
</li>
<li class="link">
<a href="chap_least_squares.html" data-scroll="chap_least_squares" class="internal"><span class="codenumber">26</span> <span class="title">Least Squares Approximations</span></a><ul>
<li><a href="chap_least_squares.html#sec_appl_fit_func" data-scroll="sec_appl_fit_func" class="internal">Application: Fitting Functions to Data</a></li>
<li><a href="chap_least_squares.html#sec_ls_intro" data-scroll="sec_ls_intro" class="internal">Introduction</a></li>
<li><a href="chap_least_squares.html#sec_ls_approx" data-scroll="sec_ls_approx" class="internal">Least Squares Approximations</a></li>
<li><a href="chap_least_squares.html#sec_ls_exam" data-scroll="sec_ls_exam" class="internal">Examples</a></li>
<li><a href="chap_least_squares.html#sec_ls_summ" data-scroll="sec_ls_summ" class="internal">Summary</a></li>
<li><a href="chap_least_squares.html#sec_ls_exer" data-scroll="sec_ls_exer" class="internal">Exercises</a></li>
<li><a href="chap_least_squares.html#sec_proj_ls_approx_other" data-scroll="sec_proj_ls_approx_other" class="internal">Project: Other Least Squares Approximations</a></li>
</ul>
</li>
<li class="link part"><a href="part-app-orthog.html" data-scroll="part-app-orthog" class="internal"><span class="codenumber">VI</span> <span class="title">Applications of Orthogonality</span></a></li>
<li class="link">
<a href="chap_orthogonal_diagonalization.html" data-scroll="chap_orthogonal_diagonalization" class="internal"><span class="codenumber">27</span> <span class="title">Orthogonal Diagonalization</span></a><ul>
<li><a href="chap_orthogonal_diagonalization.html#sec_appl_mulit_2nd_deriv" data-scroll="sec_appl_mulit_2nd_deriv" class="internal">Application: The Multivariable Second Derivative Test</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_orthog_diag_intro" data-scroll="sec_orthog_diag_intro" class="internal">Introduction</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_mtx_symm" data-scroll="sec_mtx_symm" class="internal">Symmetric Matrices</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_spec_decomp_symm_mtx" data-scroll="sec_spec_decomp_symm_mtx" class="internal">The Spectral Decomposition of a Symmetric Matrix <span class="process-math">\(A\)</span></a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_orthog_diag_exam" data-scroll="sec_orthog_diag_exam" class="internal">Examples</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_orthog_diag_summ" data-scroll="sec_orthog_diag_summ" class="internal">Summary</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_orthog_diag_exer" data-scroll="sec_orthog_diag_exer" class="internal">Exercises</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_proj_two_var_deriv" data-scroll="sec_proj_two_var_deriv" class="internal">Project: The Second Derivative Test for Functions of Two Variables</a></li>
</ul>
</li>
<li class="link">
<a href="chap_principal_axis_theorem.html" data-scroll="chap_principal_axis_theorem" class="internal"><span class="codenumber">28</span> <span class="title">Quadratic Forms and the Principal Axis Theorem</span></a><ul>
<li><a href="chap_principal_axis_theorem.html#sec_appl_tennis" data-scroll="sec_appl_tennis" class="internal">Application: The Tennis Racket Effect</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_intro" data-scroll="sec_pat_intro" class="internal">Introduction</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_eqs_quad_r2" data-scroll="sec_eqs_quad_r2" class="internal">Equations Involving Quadratic Forms in <span class="process-math">\(\R^2\)</span></a></li>
<li><a href="chap_principal_axis_theorem.html#sec_class_quad_forms" data-scroll="sec_class_quad_forms" class="internal">Classifying Quadratic Forms</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_inner_prod" data-scroll="sec_pat_inner_prod" class="internal">Inner Products</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_exam" data-scroll="sec_pat_exam" class="internal">Examples</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_summ" data-scroll="sec_pat_summ" class="internal">Summary</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_exer" data-scroll="sec_pat_exer" class="internal">Exercises</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_proj_tennis" data-scroll="sec_proj_tennis" class="internal">Project: The Tennis Racket Theorem</a></li>
</ul>
</li>
<li class="link">
<a href="chap_SVD.html" data-scroll="chap_SVD" class="internal"><span class="codenumber">29</span> <span class="title">The Singular Value Decomposition</span></a><ul>
<li><a href="chap_SVD.html#sec_appl_search_engn" data-scroll="sec_appl_search_engn" class="internal">Application: Search Engines and Semantics</a></li>
<li><a href="chap_SVD.html#sec_svd_intro" data-scroll="sec_svd_intro" class="internal">Introduction</a></li>
<li><a href="chap_SVD.html#sec_mtx_op_norm" data-scroll="sec_mtx_op_norm" class="internal">The Operator Norm of a Matrix</a></li>
<li><a href="chap_SVD.html#sec_svd" data-scroll="sec_svd" class="internal">The SVD</a></li>
<li><a href="chap_SVD.html#sec_svd_mtx_spaces" data-scroll="sec_svd_mtx_spaces" class="internal">SVD and the Null, Column, and Row Spaces of a Matrix</a></li>
<li><a href="chap_SVD.html#sec_svd_exam" data-scroll="sec_svd_exam" class="internal">Examples</a></li>
<li><a href="chap_SVD.html#sec_svd_summ" data-scroll="sec_svd_summ" class="internal">Summary</a></li>
<li><a href="chap_SVD.html#sec_svd_exer" data-scroll="sec_svd_exer" class="internal">Exercises</a></li>
<li><a href="chap_SVD.html#sec_proj_indexing" data-scroll="sec_proj_indexing" class="internal">Project: Latent Semantic Indexing</a></li>
</ul>
</li>
<li class="link">
<a href="chap_pseudoinverses.html" data-scroll="chap_pseudoinverses" class="internal"><span class="codenumber">30</span> <span class="title">Using the Singular Value Decomposition</span></a><ul>
<li><a href="chap_pseudoinverses.html#sec_appl_gps" data-scroll="sec_appl_gps" class="internal">Application: Global Positioning System</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudo_intro" data-scroll="sec_pseudo_intro" class="internal">Introduction</a></li>
<li><a href="chap_pseudoinverses.html#sec_img_conpress" data-scroll="sec_img_conpress" class="internal">Image Compression</a></li>
<li><a href="chap_pseudoinverses.html#sec_err_approx_img" data-scroll="sec_err_approx_img" class="internal">Calculating the Error in Approximating an Image</a></li>
<li><a href="chap_pseudoinverses.html#sec_mtx_cond_num" data-scroll="sec_mtx_cond_num" class="internal">The Condition Number of a Matrix</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudoinverses" data-scroll="sec_pseudoinverses" class="internal">Pseudoinverses</a></li>
<li><a href="chap_pseudoinverses.html#sec_ls_approx_SVD" data-scroll="sec_ls_approx_SVD" class="internal">Least Squares Approximations</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudo_exam" data-scroll="sec_pseudo_exam" class="internal">Examples</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudo_summ" data-scroll="sec_pseudo_summ" class="internal">Summary</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudo_exer" data-scroll="sec_pseudo_exer" class="internal">Exercises</a></li>
<li><a href="chap_pseudoinverses.html#sec_proj_gps" data-scroll="sec_proj_gps" class="internal">Project: GPS and Least Squares</a></li>
</ul>
</li>
<li class="link part"><a href="part-vec-spaces.html" data-scroll="part-vec-spaces" class="internal"><span class="codenumber">VII</span> <span class="title">Vector Spaces</span></a></li>
<li class="link">
<a href="chap_vector_spaces.html" data-scroll="chap_vector_spaces" class="internal"><span class="codenumber">31</span> <span class="title">Vector Spaces</span></a><ul>
<li><a href="chap_vector_spaces.html#sec_appl_hat_puzzle" data-scroll="sec_appl_hat_puzzle" class="internal">Application: The Hat Puzzle</a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space_intro" data-scroll="sec_vec_space_intro" class="internal">Introduction</a></li>
<li><a href="chap_vector_spaces.html#sec_space_like_rn" data-scroll="sec_space_like_rn" class="internal">Spaces with Similar Structure to <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space" data-scroll="sec_vec_space" class="internal">Vector Spaces</a></li>
<li><a href="chap_vector_spaces.html#sec_subspaces" data-scroll="sec_subspaces" class="internal">Subspaces</a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space_exam" data-scroll="sec_vec_space_exam" class="internal">Examples</a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space_summ" data-scroll="sec_vec_space_summ" class="internal">Summary</a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space_exer" data-scroll="sec_vec_space_exer" class="internal">Exercises</a></li>
<li><a href="chap_vector_spaces.html#sec_proj_hamming_hat_puzzle" data-scroll="sec_proj_hamming_hat_puzzle" class="internal">Project: Hamming Codes and the Hat Puzzle</a></li>
</ul>
</li>
<li class="link">
<a href="chap_bases.html" data-scroll="chap_bases" class="internal"><span class="codenumber">32</span> <span class="title">Bases for Vector Spaces</span></a><ul>
<li><a href="chap_bases.html#sec_img_compress" data-scroll="sec_img_compress" class="internal">Application: Image Compression</a></li>
<li><a href="chap_bases.html#sec_bases_intro" data-scroll="sec_bases_intro" class="internal">Introduction</a></li>
<li><a href="chap_bases.html#sec_lin_indep" data-scroll="sec_lin_indep" class="internal">Linear Independence</a></li>
<li><a href="chap_bases.html#sec_bases" data-scroll="sec_bases" class="internal">Bases</a></li>
<li><a href="chap_bases.html#sec_basis_vec_space" data-scroll="sec_basis_vec_space" class="internal">Finding a Basis for a Vector Space</a></li>
<li><a href="chap_bases.html#sec_bases_exam" data-scroll="sec_bases_exam" class="internal">Examples</a></li>
<li><a href="chap_bases.html#sec_bases_summ" data-scroll="sec_bases_summ" class="internal">Summary</a></li>
<li><a href="chap_bases.html#sec_bases_exer" data-scroll="sec_bases_exer" class="internal">Exercises</a></li>
<li><a href="chap_bases.html#sec_proj_img_compress" data-scroll="sec_proj_img_compress" class="internal">Project: Image Compression with Wavelets</a></li>
</ul>
</li>
<li class="link">
<a href="chap_dimension.html" data-scroll="chap_dimension" class="internal"><span class="codenumber">33</span> <span class="title">The Dimension of a Vector Space</span></a><ul>
<li><a href="chap_dimension.html#sec_appl_pca" data-scroll="sec_appl_pca" class="internal">Application: Principal Component Analysis</a></li>
<li><a href="chap_dimension.html#sec_dims_intro" data-scroll="sec_dims_intro" class="internal">Introduction</a></li>
<li><a href="chap_dimension.html#sec_finite_dim_space" data-scroll="sec_finite_dim_space" class="internal">Finite Dimensional Vector Spaces</a></li>
<li><a href="chap_dimension.html#sec_dim_subspace" data-scroll="sec_dim_subspace" class="internal">The Dimension of a Subspace</a></li>
<li><a href="chap_dimension.html#sec_cond_basis_vec_space" data-scroll="sec_cond_basis_vec_space" class="internal">Conditions for a Basis of a Vector Space</a></li>
<li><a href="chap_dimension.html#sec_dims_exam" data-scroll="sec_dims_exam" class="internal">Examples</a></li>
<li><a href="chap_dimension.html#sec_dims_summ" data-scroll="sec_dims_summ" class="internal">Summary</a></li>
<li><a href="chap_dimension.html#sec_dims_exer" data-scroll="sec_dims_exer" class="internal">Exercises</a></li>
<li><a href="chap_dimension.html#sec_proj_pca" data-scroll="sec_proj_pca" class="internal">Project: Understanding Principal Component Analysis</a></li>
</ul>
</li>
<li class="link">
<a href="chap_coordinate_vectors_vector_spaces.html" data-scroll="chap_coordinate_vectors_vector_spaces" class="internal"><span class="codenumber">34</span> <span class="title">Coordinate Vectors and Coordinate Transformations</span></a><ul>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_appl_sums" data-scroll="sec_appl_sums" class="internal">Application: Calculating Sums</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coor_vec_intro" data-scroll="sec_coor_vec_intro" class="internal">Introduction</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coord_trans" data-scroll="sec_coord_trans" class="internal">The Coordinate Transformation</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coord_vec_exam" data-scroll="sec_coord_vec_exam" class="internal">Examples</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coord_vec_summ" data-scroll="sec_coord_vec_summ" class="internal">Summary</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coord_vec_exer" data-scroll="sec_coord_vec_exer" class="internal">Exercises</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_proj_sum_powers" data-scroll="sec_proj_sum_powers" class="internal">Project: Finding Formulas for Sums of Powers</a></li>
</ul>
</li>
<li class="link">
<a href="chap_inner_products.html" data-scroll="chap_inner_products" class="internal"><span class="codenumber">35</span> <span class="title">Inner Product Spaces</span></a><ul>
<li><a href="chap_inner_products.html#sec_appl_fourier" data-scroll="sec_appl_fourier" class="internal">Application: Fourier Series</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_intro" data-scroll="sec_inner_prod_intro" class="internal">Introduction</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_spaces" data-scroll="sec_inner_prod_spaces" class="internal">Inner Product Spaces</a></li>
<li><a href="chap_inner_products.html#sec_vec_length" data-scroll="sec_vec_length" class="internal">The Length of a Vector</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_orthog" data-scroll="sec_inner_prod_orthog" class="internal">Orthogonality in Inner Product Spaces</a></li>
<li><a href="chap_inner_products.html#sec_inner_prog_orthog_bases" data-scroll="sec_inner_prog_orthog_bases" class="internal">Orthogonal and Orthonormal Bases in Inner Product Spaces</a></li>
<li><a href="chap_inner_products.html#sec_orthog_proj_subspace" data-scroll="sec_orthog_proj_subspace" class="internal">Orthogonal Projections onto Subspaces</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_approx" data-scroll="sec_inner_prod_approx" class="internal">Best Approximations in Inner Product Spaces</a></li>
<li><a href="chap_inner_products.html#sec_orthog_comp_ip" data-scroll="sec_orthog_comp_ip" class="internal">Orthogonal Complements</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_exam" data-scroll="sec_inner_prod_exam" class="internal">Examples</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_summ" data-scroll="sec_inner_prod_summ" class="internal">Summary</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_exer" data-scroll="sec_inner_prod_exer" class="internal">Exercises</a></li>
<li><a href="chap_inner_products.html#sec_proj_fourier" data-scroll="sec_proj_fourier" class="internal">Project: Fourier Series and Musical Tones</a></li>
</ul>
</li>
<li class="link">
<a href="chap_gram_schmidt_ips.html" data-scroll="chap_gram_schmidt_ips" class="internal"><span class="codenumber">36</span> <span class="title">The Gram-Schmidt Process in Inner Product Spaces</span></a><ul>
<li><a href="chap_gram_schmidt_ips.html#sec_appl_gaussian_quad" data-scroll="sec_appl_gaussian_quad" class="internal">Application: Gaussian Quadrature</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_intro" data-scroll="sec_gram_schmidt_intro" class="internal">Introduction</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_inner_prod" data-scroll="sec_gram_schmidt_inner_prod" class="internal">The Gram-Schmidt Process using Inner Products</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_exam" data-scroll="sec_gram_schmidt_exam" class="internal">Examples</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_summ_ips" data-scroll="sec_gram_schmidt_summ_ips" class="internal">Summary</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_exer" data-scroll="sec_gram_schmidt_exer" class="internal">Exercises</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_proj_gaussian_quad" data-scroll="sec_proj_gaussian_quad" class="internal">Project: Gaussian Quadrature and Legendre Polynomials</a></li>
</ul>
</li>
<li class="link part"><a href="part-lin-trans.html" data-scroll="part-lin-trans" class="internal"><span class="codenumber">VIII</span> <span class="title">Linear Transformations</span></a></li>
<li class="link">
<a href="chap_linear_transformation.html" data-scroll="chap_linear_transformation" class="internal"><span class="codenumber">37</span> <span class="title">Linear Transformations</span></a><ul>
<li><a href="chap_linear_transformation.html#sec_appl_fractals" data-scroll="sec_appl_fractals" class="internal">Application: Fractals</a></li>
<li><a href="chap_linear_transformation.html#sec_lin_trans_intro" data-scroll="sec_lin_trans_intro" class="internal">Introduction</a></li>
<li><a href="chap_linear_transformation.html#sec_onto_oneone" data-scroll="sec_onto_oneone" class="internal">Onto and One-to-One Transformations</a></li>
<li><a href="chap_linear_transformation.html#sec_kernel_range" data-scroll="sec_kernel_range" class="internal">The Kernel and Range of Linear Transformation</a></li>
<li><a href="chap_linear_transformation.html#sec_isomorph" data-scroll="sec_isomorph" class="internal">Isomorphisms</a></li>
<li><a href="chap_linear_transformation.html#sec_lin_trans_exam" data-scroll="sec_lin_trans_exam" class="internal">Examples</a></li>
<li><a href="chap_linear_transformation.html#sec_lin_trans_summ" data-scroll="sec_lin_trans_summ" class="internal">Summary</a></li>
<li><a href="chap_linear_transformation.html#sec_lin_trans_exer" data-scroll="sec_lin_trans_exer" class="internal">Exercises</a></li>
<li><a href="chap_linear_transformation.html#sec_proj_fractals" data-scroll="sec_proj_fractals" class="internal">Project: Fractals via Iterated Function Systems</a></li>
</ul>
</li>
<li class="link">
<a href="chap_transformation_matrix.html" data-scroll="chap_transformation_matrix" class="internal"><span class="codenumber">38</span> <span class="title">The Matrix of a Linear Transformation</span></a><ul>
<li><a href="chap_transformation_matrix.html#sec_appl_secret" data-scroll="sec_appl_secret" class="internal">Application: Secret Sharing Algorithms</a></li>
<li><a href="chap_transformation_matrix.html#sec_mtxof_trans_intro" data-scroll="sec_mtxof_trans_intro" class="internal">Introduction</a></li>
<li><a href="chap_transformation_matrix.html#sec_trans_rn_rm" data-scroll="sec_trans_rn_rm" class="internal">Linear Transformations from <span class="process-math">\(\R^n\)</span> to <span class="process-math">\(\R^m\)</span></a></li>
<li><a href="chap_transformation_matrix.html#sec_mtx_lin_trans" data-scroll="sec_mtx_lin_trans" class="internal">The Matrix of a Linear Transformation</a></li>
<li><a href="chap_transformation_matrix.html#sec_ker_mtx" data-scroll="sec_ker_mtx" class="internal">A Connection between <span class="process-math">\(\Ker(T)\)</span> and a Matrix Representation of <span class="process-math">\(T\)</span></a></li>
<li><a href="chap_transformation_matrix.html#sec_mtxof_trans_exam" data-scroll="sec_mtxof_trans_exam" class="internal">Examples</a></li>
<li><a href="chap_transformation_matrix.html#sec_mtxof_trans_summ" data-scroll="sec_mtxof_trans_summ" class="internal">Summary</a></li>
<li><a href="chap_transformation_matrix.html#sec_mtxof_trans_exer" data-scroll="sec_mtxof_trans_exer" class="internal">Exercises</a></li>
<li><a href="chap_transformation_matrix.html#sec_proj_secret" data-scroll="sec_proj_secret" class="internal">Project: Shamir's Secret Sharing and Lagrange Polynomials</a></li>
</ul>
</li>
<li class="link">
<a href="chap_transformations_eigenvalues.html" data-scroll="chap_transformations_eigenvalues" class="internal"><span class="codenumber">39</span> <span class="title">Eigenvalues of Linear Transformations</span></a><ul>
<li><a href="chap_transformations_eigenvalues.html#sec_appl_diff_eq" data-scroll="sec_appl_diff_eq" class="internal">Application: Linear Differential Equations</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_eigen_trans_intro" data-scroll="sec_eigen_trans_intro" class="internal">Introduction</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_find_eigen_trans" data-scroll="sec_find_eigen_trans" class="internal">Finding Eigenvalues and Eigenvectors of Linear Transformations</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_diagonal" data-scroll="sec_diagonal" class="internal">Diagonalization</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_eigen_trans_exam" data-scroll="sec_eigen_trans_exam" class="internal">Examples</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_eigen_trans_summ" data-scroll="sec_eigen_trans_summ" class="internal">Summary</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_eigen_trans_exer" data-scroll="sec_eigen_trans_exer" class="internal">Exercises</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_proj_diff_eq" data-scroll="sec_proj_diff_eq" class="internal">Project: Linear Transformations and Differential Equations</a></li>
</ul>
</li>
<li class="link">
<a href="chap_JCF.html" data-scroll="chap_JCF" class="internal"><span class="codenumber">40</span> <span class="title">The Jordan Canonical Form</span></a><ul>
<li><a href="chap_JCF.html#sec_appl_epidemic" data-scroll="sec_appl_epidemic" class="internal">Application: The Bailey Model of an Epidemic</a></li>
<li><a href="chap_JCF.html#sec_jordan_intro" data-scroll="sec_jordan_intro" class="internal">Introduction</a></li>
<li><a href="chap_JCF.html#sec_eigen_dne" data-scroll="sec_eigen_dne" class="internal">When an Eigenvalue Decomposition Does Not Exist</a></li>
<li><a href="chap_JCF.html#sec_gen_eigen_jordan" data-scroll="sec_gen_eigen_jordan" class="internal">Generalized Eigenvectors and the Jordan Canonical Form</a></li>
<li><a href="chap_JCF.html#sec_mtx_jordan_geom" data-scroll="sec_mtx_jordan_geom" class="internal">Geometry of Matrix Transformations using the Jordan Canonical Form</a></li>
<li><a href="chap_JCF.html#sec_jordan_proof" data-scroll="sec_jordan_proof" class="internal">Proof of the Existence of the Jordan Canonical Form</a></li>
<li><a href="chap_JCF.html#sec_mtx_nilpotent" data-scroll="sec_mtx_nilpotent" class="internal">Nilpotent Matrices and Invariant Subspaces</a></li>
<li><a href="chap_JCF.html#sec_jordan" data-scroll="sec_jordan" class="internal">The Jordan Canonical Form</a></li>
<li><a href="chap_JCF.html#sec_jordan_exam" data-scroll="sec_jordan_exam" class="internal">Examples</a></li>
<li><a href="chap_JCF.html#sec_jordan_summ" data-scroll="sec_jordan_summ" class="internal">Summary</a></li>
<li><a href="chap_JCF.html#sec_jordan_exer" data-scroll="sec_jordan_exer" class="internal">Exercises</a></li>
<li><a href="chap_JCF.html#sec_proj_epidemic" data-scroll="sec_proj_epidemic" class="internal">Project: Modeling an Epidemic</a></li>
</ul>
</li>
<li class="link backmatter"><a href="backmatter-1.html" data-scroll="backmatter-1" class="internal"><span class="title">Back Matter</span></a></li>
<li class="link">
<a href="app_complex_numbers.html" data-scroll="app_complex_numbers" class="internal"><span class="codenumber">A</span> <span class="title">Complex Numbers</span></a><ul>
<li><a href="app_complex_numbers.html#sec_complex_numbers" data-scroll="sec_complex_numbers" class="internal">Complex Numbers</a></li>
<li><a href="app_complex_numbers.html#sec_conj_modulus" data-scroll="sec_conj_modulus" class="internal">Conjugates and Modulus</a></li>
<li><a href="app_complex_numbers.html#sec_complex_vect" data-scroll="sec_complex_vect" class="internal">Complex Vectors</a></li>
</ul>
</li>
<li class="link"><a href="app_answers.html" data-scroll="app_answers" class="internal"><span class="codenumber">B</span> <span class="title">Answers and Hints for Selected Exercises</span></a></li>
<li class="link"><a href="index-1.html" data-scroll="index-1" class="internal"><span class="title">Index</span></a></li>
</ul></nav><div class="extras"><nav><a class="pretext-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content">
<section class="chapter" id="chap_gram_schmidt"><h2 class="heading">
<span class="type">Chapter</span> <span class="codenumber">25</span> <span class="title">Projections onto Subspaces and the Gram-Schmidt Process in <span class="process-math">\(\R^n\)</span></span>
</h2>
<section class="introduction" id="introduction-404"><article class="objectives goal-like" id="objectives-25"><h3 class="heading"><span class="type">Focus Questions</span></h3>
<div class="introduction" id="introduction-405"><p id="p-4210">By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.</p></div>
<ul class="disc">
<li id="li-673"><p id="p-4211">What is a projection of a vector onto a subspace and why are such projections important?</p></li>
<li id="li-674"><p id="p-4212">What is a projection of a vector orthogonal to a subspace and why are such orthogonal projections important?</p></li>
<li id="li-675"><p id="p-4213">What is the Gram-Schmidt process and why is it useful?</p></li>
<li id="li-676"><p id="p-4214">What is the QR-factorization of a matrix and why is it useful?</p></li>
</ul></article></section><section class="section" id="sec_mimo"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">Application: MIMO Systems</span>
</h3>
<p id="p-4215">MIMO (Multiple Input Multiple Output) systems are widely used to increase data transmission rates and improve the quality of wireless communication systems. MIMO is one of several forms of smart antenna technology, and it makes use of multiple antennas at the transmitter to send signals and multiple antennas at the receiver to accept the signals. MIMO systems transmit the same data on multiple streams, which introduces redundancy into the system. MIMO systems can utilize bounced and reflected signals to actually improve signal strength. This is very useful in urban environments in the presence of large buildings and the absence of direct line-of-sight transmission. MIMO systems can transmit several information streams in parallel (known as spatial multiplexing), allowing for increased data transformation rates. The presence of multiple receiver antennas allows for greater reliability in the system.</p>
<figure class="figure figure-like" id="F_MIMO"><div class="image-box" style="width: 50%; margin-left: 25%; margin-right: 25%;"><img src="external/MIMO.svg" role="img" class="contained"></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">25.1<span class="period">.</span></span><span class="space"> </span>A MIMO system.</figcaption></figure><p id="p-4216">In a MIMO system, every transmitter antenna sends a signal to every receiver antenna as illustrated in <a href="" class="xref" data-knowl="./knowl/F_MIMO.html" title="Figure 25.1">Figure 25.1</a>. In each of these transmissions the signal can be disturbed in some way. Two types of disturbances are <dfn class="terminology">fading</dfn> and <dfn class="terminology">noise</dfn>. Fading is due to time variations in the received signal, which can occur because of atmospheric conditions or obstacles over the path which are varying with respect to time. In a MIMO system we have <dfn class="terminology">multipath fading</dfn> for the different paths the signal takes from different antennas to receivers, which causes fluctuations in amplitude, phase and angle of arrival of the received signal. Noise is an unwanted signal which interferes with actual signal. Both fading and noise result in a received signal that is different than the transmitted signal. The problem is how to recover the original signal from the transmitted signal. The QR decomposition is used in this process. To improve the efficiency of MIMO systems, different methods are introduced to determine a QR decomposition. We will discuss MIMO systems in more detail later in this section, and how Householder transformations can be used to find QR decompositions.</p></section><section class="section" id="sec_gram_schmidt_intro_noip"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">Introduction</span>
</h3>
<p id="p-4217">In many situations (least squares approximations, for example) want to find a vector <span class="process-math">\(\vw\)</span> in a subspace <span class="process-math">\(W\)</span> of <span class="process-math">\(\R^n\)</span> that is the “best” approximation to a vector <span class="process-math">\(\vv\)</span> not in the subspace. A natural measure of “best” is to find a vector <span class="process-math">\(\vw\)</span>in <span class="process-math">\(W\)</span> if one exists, such that the distance from <span class="process-math">\(\vw\)</span> to <span class="process-math">\(\vu\)</span> is a minimum. This means we want to find <span class="process-math">\(\vw\)</span> so that the quantity</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
||\vw - \vu ||
\end{equation*}
</div>
<p class="continuation">is as small as possible over all vectors <span class="process-math">\(\vw\)</span> in <span class="process-math">\(W\)</span> To do this, we will need to find a suitable projection of the vector <span class="process-math">\(\vv\)</span> onto the subspace <span class="process-math">\(W\)</span> We have already done this in the case that <span class="process-math">\(W = \Span\{\vw_1\}\)</span> is the span of a single vector -- we can project the vector <span class="process-math">\(\vv\)</span> in the direction of <span class="process-math">\(\vw_1\text{.}\)</span> Our goal is to generalize this to project a vector <span class="process-math">\(\vv\)</span> onto an entire subspace in a useful way.</p>
<article class="exploration project-like" id="pa_6_e_1"><h4 class="heading">
<span class="type">Preview Activity</span><span class="space"> </span><span class="codenumber">25.1</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-406">
<p id="p-4218">Let <span class="process-math">\(\CB = \{\vw_1, \vw_2\}\)</span> be a basis for a subspace <span class="process-math">\(W\)</span> of <span class="process-math">\(\R^3\)</span> where <span class="process-math">\(\vw_1 = [1 \ 0 \ 0]^{\tr}\)</span> and <span class="process-math">\(\vw_2 = [0 \ 1 \ 0]^{\tr}\)</span> Note that <span class="process-math">\(\CB\)</span> is an orthonormal basis for <span class="process-math">\(W\)</span> Let <span class="process-math">\(\vv = [1 \ 2 \ 1]^{\tr}\)</span> Note that <span class="process-math">\(W\)</span> is the <span class="process-math">\(xy\)</span>plane and that <span class="process-math">\(\vv\)</span> is not in <span class="process-math">\(W\)</span> as illustrated in <a href="" class="xref" data-knowl="./knowl/F_6_d_o_proj.html" title="Figure 25.2">Figure 25.2</a>.</p>
<figure class="figure figure-like" id="F_6_d_o_proj"><div class="image-box" style="width: 50%; margin-left: 25%; margin-right: 25%;"><img src="external/6_d_O_proj.svg" role="img" class="contained"></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">25.2<span class="period">.</span></span><span class="space"> </span>The space <span class="process-math">\(W\)</span> and vectors <span class="process-math">\(\vw_1\text{,}\)</span> <span class="process-math">\(\vw_2\text{,}\)</span> and <span class="process-math">\(\vv\text{.}\)</span></figcaption></figure>
</div>
<article class="task exercise-like" id="task-1396"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4219">Find the projection <span class="process-math">\(\vu_1\)</span>of <span class="process-math">\(\vv\)</span>onto <span class="process-math">\(W_1 = \Span\{\vw_1\}\)</span></p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-42" id="hint-42"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-42"><div class="hint solution-like"><p id="p-4220">See Equation <a href="" class="xref" data-knowl="./knowl/eq_6_a_projection.html" title="Equation 23.2">(23.2)</a>.</p></div></div>
</div></article><article class="task exercise-like" id="task-1397"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-4221">Find the projection <span class="process-math">\(\vu_2\)</span>of <span class="process-math">\(\vv\)</span>onto <span class="process-math">\(W_2 = \Span\{\vw_2\}\)</span></p></article><article class="task exercise-like" id="task-1398"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-4222">Calculate the distance between <span class="process-math">\(\vv\)</span>and <span class="process-math">\(\vu_1\)</span>and <span class="process-math">\(\vv\)</span>and <span class="process-math">\(\vu_2\)</span> Which of <span class="process-math">\(\vu_1\)</span>and <span class="process-math">\(\vu_2\)</span>is closer to <span class="process-math">\(\vv\text{?}\)</span></p></article><article class="task exercise-like" id="task-1399"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<p id="p-4223">Show that the vector <span class="process-math">\(\frac{3}{4} [1 \ 2 \ 0]^{\tr}\)</span>is in <span class="process-math">\(W\)</span>and find the distance between <span class="process-math">\(\vv\)</span>and <span class="process-math">\(\frac{3}{4} [1 \ 2 \ 0]^{\tr}\)</span></p></article><article class="task exercise-like" id="task-1400"><h5 class="heading"><span class="codenumber">(e)</span></h5>
<p id="p-4224">Part (4) shows that neither vector <span class="process-math">\(\vu_1 = \proj_{\vw_1} \vv\)</span>nor <span class="process-math">\(\vu_2 = \proj_{\vw_2} \vv\)</span>is the vector in <span class="process-math">\(W\)</span>that is closest to <span class="process-math">\(\vv\)</span> We should probably expect this neither projection uses the fact that the other vector might contribute to the closest vector. Let us instead consider the sum <span class="process-math">\(\vu_3 = \vu_1+\vu_2\)</span> Calculate the components of this vector <span class="process-math">\(\vu_3\)</span>and determine the distance between <span class="process-math">\(\vu_3\)</span>and <span class="process-math">\(\vv\)</span> Which of the three vectors <span class="process-math">\(\vu_1\)</span> <span class="process-math">\(\vu_2\)</span> and <span class="process-math">\(\vu_3\)</span>in <span class="process-math">\(W\)</span>is closest to <span class="process-math">\(\vv\text{?}\)</span></p></article><article class="task exercise-like" id="task-1401"><h5 class="heading"><span class="codenumber">(f)</span></h5>
<p id="p-4225">A picture of <span class="process-math">\(\vw_1\)</span> <span class="process-math">\(\vw_2\)</span> <span class="process-math">\(W\)</span> and <span class="process-math">\(\vv\)</span>is shown in <a href="" class="xref" data-knowl="./knowl/F_6_d_o_proj.html" title="Figure 25.2">Figure 25.2</a>. Draw in <span class="process-math">\(\vu_1\)</span> <span class="process-math">\(\vu_2\)</span> and <span class="process-math">\(\vu_3\)</span> Draw a picture of the vector <span class="process-math">\(\vw\)</span>in <span class="process-math">\(W\)</span>that appears to be closest to <span class="process-math">\(\vv\)</span> Does this vector seem to have any relationship to <span class="process-math">\(\vu_1\)</span> <span class="process-math">\(\vu_2\)</span> or <span class="process-math">\(\vu_3\text{?}\)</span> If yes, what relationship do you see?</p></article></article></section><section class="section" id="sec_proj_subsp_orth"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">Projections onto Subspaces and Orthogonal Projections</span>
</h3>
<p id="p-4226"><a href="" class="xref" data-knowl="./knowl/pa_6_e_1.html" title="Preview Activity 25.1">Preview Activity 25.1</a> gives an indication of how we can project a vector <span class="process-math">\(\vv\)</span> in <span class="process-math">\(\R^n\)</span> onto a subspace <span class="process-math">\(W\)</span> of <span class="process-math">\(\R^n\text{.}\)</span> If we have an orthogonal basis for <span class="process-math">\(W\text{,}\)</span> we can just add the projections of <span class="process-math">\(\vv\)</span> onto each basis vector. The resulting vector is called the projection of <span class="process-math">\(\vv\)</span> onto the subspace <span class="process-math">\(W\text{.}\)</span> As we did with projections onto vectors, we can also define the projection of <span class="process-math">\(\vv\)</span> orthogonal to <span class="process-math">\(W\text{.}\)</span> Note that to make this all work out properly, we will see that we need an orthogonal basis for <span class="process-math">\(W\text{.}\)</span></p>
<article class="definition definition-like" id="definition-58"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">25.3</span><span class="period">.</span>
</h4>
<p id="p-4227">Let <span class="process-math">\(W\)</span> be a subspace of <span class="process-math">\(\R^n\)</span> and let <span class="process-math">\(\{\vw_1, \vw_2, \ldots, \vw_m\}\)</span> be an orthogonal basis for <span class="process-math">\(W\text{.}\)</span> The <dfn class="terminology">projection of the vector <span class="process-math">\(\vv\)</span> in <span class="process-math">\(V\)</span> onto <span class="process-math">\(W\)</span></dfn> is the vector</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\proj_W \vv = \frac{\vv \cdot \vw_1}{\vw_1 \cdot \vw_1} \vw_1 + \frac{\vv \cdot \vw_2}{\vw_2 \cdot \vw_2}  \vw_2 + \cdots + \frac{\vv \cdot \vw_m}{\vw_m \cdot \vw_m}  \vw_m\text{.}
\end{equation*}
</div>
<p id="p-4228">The <dfn class="terminology">projection of <span class="process-math">\(\vv\)</span> orthogonal to <span class="process-math">\(W\)</span></dfn> is the vector</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\proj_{\perp W} \vv = \vv - \proj_W \vv\text{.}
\end{equation*}
</div></article><p id="p-4229">The notation <span class="process-math">\(\proj_{\perp W} \vv\)</span> indicates that we expect this vector to be orthogonal to every vector in <span class="process-math">\(W\text{.}\)</span> We address this in the following activity.</p>
<article class="activity project-like" id="act_6_e_orth_projection"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">25.2</span><span class="period">.</span>
</h4>
<p id="p-4230">Let <span class="process-math">\(W = \Span \{\vw_1, \vw_2\}\)</span> in <span class="process-math">\(\R^3\text{,}\)</span> with <span class="process-math">\(\vw_1=[1 \ 0 \ 0]^{\tr}\)</span> and <span class="process-math">\(\vw_2= [0 \ 1 \ 0]^{\tr}\text{,}\)</span> and <span class="process-math">\(\vv = [1 \ 2 \ 1]^{\tr}\)</span> as in <a href="" class="xref" data-knowl="./knowl/pa_6_e_1.html" title="Preview Activity 25.1">Preview Activity 25.1</a>. Recall that <span class="process-math">\(\proj_W \vv = [1 \ 2 \ 0]^{\tr}\text{.}\)</span> Find the projection of <span class="process-math">\(\vv\)</span> orthogonal to <span class="process-math">\(W\)</span> and show directly that <span class="process-math">\(\proj_{\perp W} \vv\)</span> is orthogonal to the basis vectors for <span class="process-math">\(W\)</span> (and hence to every vector in <span class="process-math">\(W\)</span>).</p></article><p id="p-4231"><a href="" class="xref" data-knowl="./knowl/act_6_e_orth_projection.html" title="Activity 25.2">Activity 25.2</a> indicates that the vector <span class="process-math">\(\proj_{\perp W} \vv\)</span> is in fact orthogonal to every vector in <span class="process-math">\(W\text{.}\)</span> To see that this is true in general, let <span class="process-math">\(\CB = \{\vw_1, \vw_2, \ldots, \vw_m\}\)</span> be an orthogonal basis for a subspace <span class="process-math">\(W\)</span> of <span class="process-math">\(\R^n\)</span> and let <span class="process-math">\(\vv\)</span> be a vector in <span class="process-math">\(\R^n\text{.}\)</span> Let</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/act_6_e_orth_projection.html">
\begin{equation*}
\vw =  \proj_W \vv = \frac{\vv \cdot \vw_1}{\vw_1 \cdot \vw_1} \vw_1 + \frac{\vv \cdot \vw_2}{\vw_2 \cdot \vw_2}  \vw_2 + \cdots + \frac{\vv \cdot \vw_m}{\vw_m \cdot \vw_m}  \vw_m\text{.}
\end{equation*}
</div>
<p id="p-4232">Then <span class="process-math">\(\vv - \vw\)</span> is the projection of <span class="process-math">\(\vv\)</span> orthogonal to <span class="process-math">\(W\text{.}\)</span> We will show that <span class="process-math">\(\vv - \vw\)</span> is orthogonal to every basis vector for <span class="process-math">\(W\text{.}\)</span> Since <span class="process-math">\(\CB\)</span> is an orthogonal basis for <span class="process-math">\(W\text{,}\)</span> we know that <span class="process-math">\(\vw_i \cdot \vw_j = 0\)</span> for <span class="process-math">\(i \neq j\text{.}\)</span> So if <span class="process-math">\(k\)</span> is between 1 and <span class="process-math">\(m\)</span> then</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-168">
\begin{align*}
\vw_k \cdot (\vv - \vw) \amp = (\vw_k \cdot \vv) - (\vw_k \cdot \vw)\\
\amp = (\vw_k \cdot \vv) - \left[\vw_k \cdot \left(\ds \frac{\vv \cdot \vw_1}{\vw_1 \cdot \vw_1} \vw_1 + \frac{\vv \cdot \vw_2}{\vw_2 \cdot \vw_2}  \vw_2 + \cdots + \frac{\vv \cdot \vw_m}{\vw_m \cdot \vw_m}  \vw_m \right)\right]\\
\amp = (\vw_k \cdot \vv) - \left(\frac{\vv \cdot \vw_k}{\vw_k \cdot \vw_k}\right) (\vw_k \cdot \vw_k)\\
\amp = (\vw_k \cdot \vv) - (\vv \cdot \vw_k)\\
\amp = 0\text{.}
\end{align*}
</div>
<p id="p-4233">So the vector <span class="process-math">\(\vv - \vw\)</span> is orthogonal to every basis vector for <span class="process-math">\(W\text{,}\)</span> and therefore to every vector in <span class="process-math">\(W\)</span> (<a href="" class="xref" data-knowl="./knowl/thm_6_a_dot_pd_orth_complement_basis.html" title="Theorem 23.16">Theorem 23.16</a>). Because <span class="process-math">\(\CB\)</span> is a basis for <span class="process-math">\(W\text{,}\)</span> if <span class="process-math">\(\vx\)</span> is in <span class="process-math">\(W\text{,}\)</span> then</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/thm_6_a_dot_pd_orth_complement_basis.html">
\begin{equation*}
\vx = c_1 \vw_1 + c_2 \vw_2 + \cdots + c_m \vw_m
\end{equation*}
</div>
<p class="continuation">for some scalars <span class="process-math">\(c_1\text{,}\)</span> <span class="process-math">\(c_2\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\(c_m\text{.}\)</span> So</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/thm_6_a_dot_pd_orth_complement_basis.html">
\begin{equation*}
(\vv - \vw) \cdot \vx = \sum_{k=1}^m c_k (\vv - \vw) \cdot \vw_k = 0\text{,}
\end{equation*}
</div>
<p class="continuation">and so <span class="process-math">\(\vv - \vw\)</span> is orthogonal to every vector in <span class="process-math">\(W\text{.}\)</span></p></section><section class="section" id="sec_best_approx"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">Best Approximations</span>
</h3>
<p id="p-4234">We have seen that <span class="process-math">\(\proj_{\perp W} \vv\)</span> is orthogonal to every vector in <span class="process-math">\(W\text{,}\)</span> which suggests that <span class="process-math">\(\proj_W \vv\)</span> is in fact the vector in <span class="process-math">\(W\)</span> that is closest to <span class="process-math">\(\vv\text{.}\)</span> We now verify this fact and conclude that <span class="process-math">\(\proj_W \vv\)</span> is the vector in <span class="process-math">\(W\)</span> closest to <span class="process-math">\(\vv\)</span> and therefore the best approximation of <span class="process-math">\(\vv\)</span> by a vector in <span class="process-math">\(W\text{.}\)</span></p>
<article class="theorem theorem-like" id="theorem-60"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">25.4</span><span class="period">.</span>
</h4>
<p id="p-4235">Let <span class="process-math">\(\B = \{\vw_1, \vw_2, \ldots, \vw_m\}\)</span> be an orthogonal basis for a subspace <span class="process-math">\(W\)</span> of <span class="process-math">\(\R^n\)</span> and let <span class="process-math">\(\vv\)</span> be a vector in <span class="process-math">\(\R^n\text{.}\)</span> Then</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
||\vv - \proj_W \vv || \lt  || \vv - \vx ||
\end{equation*}
</div>
<p class="continuation">for every vector <span class="process-math">\(\vx\)</span> different from <span class="process-math">\(\proj_W \vv\)</span> in <span class="process-math">\(W\text{.}\)</span></p></article><article class="proof" id="proof-13"><h4 class="heading"><span class="type">Proof<span class="period">.</span></span></h4>
<p id="p-4236">Let <span class="process-math">\(\B = \{\vw_1, \vw_2, \ldots, \vw_m\}\)</span> be an orthogonal basis for a subspace <span class="process-math">\(W\)</span> of <span class="process-math">\(\R^n\)</span> and let <span class="process-math">\(\vv\)</span> be a vector in <span class="process-math">\(\R^n\text{.}\)</span> Let <span class="process-math">\(\vx\)</span> be a vector in <span class="process-math">\(W\text{.}\)</span> The vector <span class="process-math">\(\proj_W \vv - \vx\)</span> is in <span class="process-math">\(W\text{,}\)</span> so is orthogonal to <span class="process-math">\(\proj_{\perp W} \vv = \vv - \proj_W \vv\text{.}\)</span> Thus, the dotted triangle whose vertices are the tips of the vectors <span class="process-math">\(\vv\text{,}\)</span> <span class="process-math">\(\vx\text{,}\)</span> and <span class="process-math">\(\proj_W \vv\)</span> in <a href="" class="xref" data-knowl="./knowl/F_best_approx.html" title="Figure 25.5">Figure 25.5</a> is a right triangle.</p>
<figure class="figure figure-like" id="F_best_approx"><div class="image-box" style="width: 60%; margin-left: 20%; margin-right: 20%;"><img src="external/6_d_best_approx.svg" role="img" class="contained"></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">25.5<span class="period">.</span></span><span class="space"> </span>The best approximation to <span class="process-math">\(\vv\)</span> in <span class="process-math">\(W\)</span></figcaption></figure><p id="p-4237">The Pythagorean theorem then shows that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
|| \vv - \vx ||^2 = || \proj_{\perp W} \vv ||^2 + || \proj_W \vv - \vx ||^2\text{.}
\end{equation*}
</div>
<p id="p-4238">Now <span class="process-math">\(\vx \neq \proj_W \vv\text{,}\)</span> so <span class="process-math">\(|| \proj_W \vv - \vx ||^2 &gt; 0\text{.}\)</span> This implies</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
|| \vv - \vx ||^2 &gt; || \proj_{\perp W} \vv ||^2
\end{equation*}
</div>
<p class="continuation">and it follows that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
|| \vv - \vx || &gt; || \proj_{\perp W} \vv ||\text{.}
\end{equation*}
</div></article><p id="p-4239">This theorem shows that the distance from <span class="process-math">\(\proj_W \vv\)</span> to <span class="process-math">\(\vv\)</span> is less than the distance from any other vector in <span class="process-math">\(W\)</span> to <span class="process-math">\(\vv\text{.}\)</span> So <span class="process-math">\(\proj_W \vv\)</span> is the best approximation to <span class="process-math">\(\vv\)</span> of all the vectors in <span class="process-math">\(W\text{.}\)</span></p>
<p id="p-4240">If <span class="process-math">\(\vv = [v_1 \ v_2 \ v_3 \ \ldots \ v_m]^{\tr}\)</span> and <span class="process-math">\(\proj_W \vv =  [w_1 \ w_2 \ w_3 \ \ldots \ w_m]^{\tr}\text{,}\)</span> then the square of the error in approximating <span class="process-math">\(\vv\)</span> by <span class="process-math">\(\proj_W \vv\)</span> is given by</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
|| \vv - \proj_W \vv ||^2 = \sum_{i=1}^m (v_i - w_i)^2\text{.}
\end{equation*}
</div>
<p id="p-4241">So <span class="process-math">\(\proj_W \vv\)</span> minimizes this sum of squares over all vectors in <span class="process-math">\(W\text{.}\)</span> As a result, we call <span class="process-math">\(\proj_W \vv\)</span> the <dfn class="terminology">least squares approximation</dfn> to <span class="process-math">\(\vv\text{.}\)</span></p>
<article class="activity project-like" id="activity-97"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">25.3</span><span class="period">.</span>
</h4>
<p id="p-4242">Let <span class="process-math">\(\CB = \left\{\left[ \begin{array}{r} 1 \\ 0 \\ -1 \\ 0 \end{array} \right], \left[ \begin{array}{r} 0 \\ 1 \\ 0 \\ -1 \end{array} \right], \left[ \begin{array}{r} -1 \\ 1 \\ -1 \\ 1 \end{array} \right]\right\}\)</span> and let <span class="process-math">\(W = \Span(\CB)\)</span> in <span class="process-math">\(\R^4\text{.}\)</span> Find the best approximation in <span class="process-math">\(W\)</span> to the vector <span class="process-math">\(\vv = \left[ \begin{array}{r} 2 \\ 3 \\ 1 \\ -1 \end{array} \right]\)</span> in <span class="process-math">\(W\text{.}\)</span></p></article></section><section class="section" id="sec_gram_schmidt_process"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">The Gram-Schmidt Process</span>
</h3>
<p id="p-4243">We have seen that orthogonal bases make computations very convenient. However, until now we had no convenient way to construct orthogonal bases. The problem we address in this section is how to create an orthogonal basis from any basis.</p>
<article class="exploration project-like" id="pa_6_d_2_no_inner_product"><h4 class="heading">
<span class="type">Preview Activity</span><span class="space"> </span><span class="codenumber">25.4</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-407"><p id="p-4244">Let <span class="process-math">\(W = \Span \{\vv_1, \vv_2, \vv_3\}\)</span> in <span class="process-math">\(\R^4\text{,}\)</span> where <span class="process-math">\(\vv_1 = [1 \ 1 \ 1 \ 1]^{\tr}\text{,}\)</span> <span class="process-math">\(\vv_2 = [-1 \ 4 \ 4 \ -1]^{\tr}\text{,}\)</span> and <span class="process-math">\(\vv_3 = [2 \ -1 \ 1 \ 0]^{\tr}\text{.}\)</span> Our goal in this preview activity is to begin to understand how we can find an orthogonal set <span class="process-math">\(\CB = \{\vw_1, \vw_2, \vw_3\}\)</span> in <span class="process-math">\(\R^4\)</span> so that <span class="process-math">\(\Span \ \CB = W\text{.}\)</span> To begin, we could start by letting <span class="process-math">\(\vw_1 = \vv_1\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1402"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4245">Now we want to find a vector in <span class="process-math">\(W\)</span> that is orthogonal to <span class="process-math">\(\vw_1\text{.}\)</span> Let <span class="process-math">\(W_1 = \Span\{\vw_1\}\text{.}\)</span> Explain why <span class="process-math">\(\vw_2 = \proj_{\perp W_1} \vv_2\)</span> is in <span class="process-math">\(W\)</span> and is orthogonal to <span class="process-math">\(\vw_1\text{.}\)</span> Then calculate the vector <span class="process-math">\(\vw_2\text{.}\)</span></p></article><article class="task exercise-like" id="task-1403"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-4246">Next we need to find a third vector <span class="process-math">\(\vw_3\)</span> that is in <span class="process-math">\(W\)</span> and is orthogonal to both <span class="process-math">\(\vw_1\)</span> and <span class="process-math">\(\vw_2\text{.}\)</span> Let <span class="process-math">\(W_2 = \Span \{\vw_1, \vw_2\}\text{.}\)</span> Explain why <span class="process-math">\(\vw_3 = \proj_{\perp W_2} \vv_3\)</span> is in <span class="process-math">\(W\)</span> and is orthogonal to both <span class="process-math">\(\vw_1\)</span> and <span class="process-math">\(\vw_2\text{.}\)</span> Then calculate the vector <span class="process-math">\(\vw_3\text{.}\)</span></p></article><article class="task exercise-like" id="task-1404"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-4247">Explain why the set <span class="process-math">\(\{\vw_1, \vw_2, \vw_3\}\)</span> is an orthogonal basis for <span class="process-math">\(W\text{.}\)</span></p></article></article><p id="p-4248"><a href="" class="xref" data-knowl="./knowl/pa_6_d_2_no_inner_product.html" title="Preview Activity 25.4">Preview Activity 25.4</a> shows the first steps of the Gram-Schmidt process to construct an orthogonal basis from any basis of a subspace in <span class="process-math">\(\R^n\text{.}\)</span> To understand why the process works in general, let <span class="process-math">\(\{\vv_1, \vv_2, \ldots, \vv_m\}\)</span> be a basis for a subspace <span class="process-math">\(W\)</span> of <span class="process-math">\(\R^n\text{.}\)</span> Let <span class="process-math">\(\vw_1 = \vv_1\)</span> and let <span class="process-math">\(W_1 = \Span\{\vw_1\}\text{.}\)</span> Since <span class="process-math">\(\vw_1 = \vv_1\)</span> we have that <span class="process-math">\(W_1 = \Span\{\vw_1\} = \Span\{\vv_1\}\text{.}\)</span></p>
<p id="p-4249">The vectors <span class="process-math">\(\vv_1 = \vw_1\)</span> and <span class="process-math">\(\vv_2\)</span> are possibly not orthogonal, but we know the orthogonal projection of <span class="process-math">\(\vv_2\)</span> onto <span class="process-math">\(W_1^{\perp}\)</span> is orthogonal to <span class="process-math">\(\vw_1\text{.}\)</span> Let</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\vw_2 = \proj_{\perp W_1} \vv_2 = \vv_2 -  \frac{ \vv_2 \cdot \vw_1 }{ \vw_1 \cdot  \vw_1 } \vw_1\text{.}
\end{equation*}
</div>
<p id="p-4250">Then <span class="process-math">\(\{\vw_1, \vw_2\}\)</span> is an orthogonal set. Note that <span class="process-math">\(\vw_1 = \vv_1 \neq \vzero\text{,}\)</span> and the fact that <span class="process-math">\(\vv_2 \notin W_1\)</span> implies that <span class="process-math">\(\vw_2 \neq \vzero\text{.}\)</span> So the set <span class="process-math">\(\{\vw_1, \vw_2\}\)</span> is linearly independent, being a set of non-zero orthogonal vectors. Now the question is whether <span class="process-math">\(\Span\{\vw_1,\vw_2\} = \Span\{\vv_1,\vv_2\}\text{.}\)</span> Note that <span class="process-math">\(\vw_2\)</span> is a linear combination of <span class="process-math">\(\vv_1\)</span> and <span class="process-math">\(\vv_2\text{,}\)</span> so <span class="process-math">\(\vw_2\)</span> is in <span class="process-math">\(\Span\{\vv_1,\vv_2\}\text{.}\)</span> Since <span class="process-math">\(\Span\{\vw_1,\vw_2\}\)</span> is a 2-dimensional subspace of the 2-dimensional space <span class="process-math">\(\Span\{\vv_1,\vv_2\}\text{,}\)</span> it must be true that <span class="process-math">\(\Span\{\vw_1,\vw_2\} = W_2 = \Span\{\vv_1,\vv_2\}\text{.}\)</span></p>
<p id="p-4251">Now we take the next step, adding <span class="process-math">\(\vv_3\)</span> into the mix. The vector</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\vw_3 = \proj_{\perp W_2} \vv_3 = \vv_3 - \frac{ \vv_3 \cdot  \vw_1 }{ \vw_1 \cdot  \vw_1 } \vw_1 - \frac{ \vv_3 \cdot  \vw_2 }{ \vw_2 \cdot  \vw_2 } \vw_2
\end{equation*}
</div>
<p class="continuation">is orthogonal to both <span class="process-math">\(\vw_1\)</span> and <span class="process-math">\(\vw_2\)</span> and, by construction, <span class="process-math">\(\vw_3\)</span> is a linear combination of <span class="process-math">\(\vv_1\text{,}\)</span> <span class="process-math">\(\vv_2\text{,}\)</span> and <span class="process-math">\(\vv_3\text{.}\)</span> So <span class="process-math">\(\vw_3\)</span> is in <span class="process-math">\(W_3 = \Span\{\vv_1, \vv_2, \vv_3\}\text{.}\)</span> The fact that <span class="process-math">\(\vv_3 \notin W_2\)</span> implies that <span class="process-math">\(\vw_3 \neq \vzero\)</span> and <span class="process-math">\(\{\vw_1, \vw_2, \vw_3\}\)</span> is a linearly independent set. Since <span class="process-math">\(\Span\{\vw_1, \vw_2, \vw_3\}\)</span> is a 3-dimensional subspace of the 3-dimensional space <span class="process-math">\(\Span\{\vv_1,\vv_2,\vv_3\}\text{,}\)</span> we conclude that <span class="process-math">\(\Span\{\vw_1, \vw_2, \vw_3\} = W_3 = \Span\{\vv_1,\vv_2,\vv_3\}\text{.}\)</span></p>
<p id="p-4252">We continue inductively in this same manner. If we have constructed a set <span class="process-math">\(\{\vw_1\text{,}\)</span> <span class="process-math">\(\vw_2\text{,}\)</span> <span class="process-math">\(\vw_3\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\(\vw_{k-1}\}\)</span> of <span class="process-math">\(k-1\)</span> orthogonal vectors such that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\Span\{\vw_1, \vw_2, \vw_3, \ldots, \vw_{k-1}\} = \Span\{\vv_1,\vv_2,\vv_3, \ldots, \vv_{k-1}\}\text{,}
\end{equation*}
</div>
<p class="continuation">then we let</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-169">
\begin{align*}
\vw_{k} \amp = \proj_{\perp W_{k-1}} \vv_k\\
\amp = \vv_k - \ds \frac{ \vv_k \cdot  \vw_1 }{ \vw_1 \cdot  \vw_1 } \vw_1 - \frac{ \vv_k \cdot  \vw_2 }{ \vw_2 \cdot  \vw_2 } \vw_2 - \cdots - \frac{ \vv_k \cdot  \vw_{k-1} }{ \vw_{k-1} \cdot  \vw_{k-1} } \vw_{k-1}\text{,}
\end{align*}
</div>
<p class="continuation">where</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
W_{k-1} = \Span\{\vw_1, \vw_2, \vw_3, \ldots, \vw_{k-1}\}\text{.}
\end{equation*}
</div>
<p id="p-4253">We know that <span class="process-math">\(\vw_k\)</span> is orthogonal to <span class="process-math">\(\vw_1\text{,}\)</span> <span class="process-math">\(\vw_2\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\(\vw_{k-1}\text{.}\)</span> Since <span class="process-math">\(\vw_1\text{,}\)</span> <span class="process-math">\(\vw_2\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\(\vw_{k-1}\text{,}\)</span> and <span class="process-math">\(\vv_k\)</span> are all in <span class="process-math">\(W_k = \Span\{\vv_1, \vv_2, \ldots, \vv_k\}\)</span> we see that <span class="process-math">\(\vw_k\)</span> is also in <span class="process-math">\(W_k\text{.}\)</span> Since <span class="process-math">\(\vv_k \notin W_{k-1}\)</span> implies that <span class="process-math">\(\vw_k \neq \vzero\)</span> and <span class="process-math">\(\{\vw_1, \vw_2, \ldots, \vw_k\}\)</span> is a linearly independent set. Then <span class="process-math">\(\Span\{\vw_1, \vw_2, \vw_3, \ldots, \vw_{k}\}\)</span> is a <span class="process-math">\(k\)</span>-dimensional subspace of the <span class="process-math">\(k\)</span>-dimensional space <span class="process-math">\(W_k\text{,}\)</span> so it follows that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\Span\{\vw_1, \vw_2, \vw_3, \ldots, \vw_{k}\} = W_k = \Span\{\vv_1,\vv_2,\vv_3, \ldots, \vv_{k}\}\text{.}
\end{equation*}
</div>
<p id="p-4254">This process will end when we have an orthogonal set <span class="process-math">\(\{\vw_1\text{,}\)</span> <span class="process-math">\(\vw_2\text{,}\)</span> <span class="process-math">\(\vw_3\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\(\vw_{m}\}\)</span> with <span class="process-math">\(\Span\{\vw_1\text{,}\)</span> <span class="process-math">\(\vw_2\text{,}\)</span> <span class="process-math">\(\vw_3\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\(\vw_{m}\}\)</span> = <span class="process-math">\(W\text{.}\)</span></p>
<p id="p-4255">We summarize the process in the following theorem.</p>
<article class="theorem theorem-like" id="thm_6_d_Gram_Schmidt_noips"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">25.6</span><span class="period">.</span><span class="space"> </span><span class="title">The Gram-Schmidt Process.</span>
</h4>
<p id="p-4256">Let <span class="process-math">\(\{\vv_1, \vv_2, \ldots, \vv_m\}\)</span> be a basis for a subspace <span class="process-math">\(W\)</span> of <span class="process-math">\(\R^n\text{.}\)</span> The set <span class="process-math">\(\{\vw_1, \vw_2, \vw_3, \ldots, \vw_{m}\}\)</span> defined by</p>
<ul class="disc">
<li id="li-677"><p id="p-4257"><span class="process-math">\(\vw_1 = \vv_1\text{,}\)</span></p></li>
<li id="li-678"><p id="p-4258"><span class="process-math">\(\vw_2 = \vv_2 - \ds \frac{ \vv_2 \cdot \vw_1 }{ \vw_1 \cdot \vw_1 } \vw_1\text{,}\)</span></p></li>
<li id="li-679"><p id="p-4259"><span class="process-math">\(\vw_3 = \vv_3 - \ds \frac{ \vv_3 \cdot \vw_1 }{ \vw_1 \cdot \vw_1 } \vw_1 - \frac{ \vv_3 \cdot \vw_2 }{ \vw_2 \cdot \vw_2 } \vw_2\text{,}\)</span> <span class="process-math">\(\vdots\)</span></p></li>
<li id="li-680"><p id="p-4260"><span class="process-math">\(\vw_m = \vv_m - \ds \frac{ \vv_m \cdot \vw_1 }{ \vw_1 \cdot \vw_1 } \vw_1 - \frac{ \vv_m \cdot \vw_2 }{ \vw_2 \cdot \vw_2 } \vw_2 - \cdots - \frac{ \vv_m \cdot \vw_{m-1} }{ \vw_{m-1} \cdot \vw_{m-1} } \vw_{m-1}\text{.}\)</span></p></li>
</ul>
<p id="p-4261">is an orthogonal basis for <span class="process-math">\(W\text{.}\)</span> Moreover,</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\Span\{\vw_1, \vw_2, \vw_3, \ldots, \vw_{k}\} = \Span\{\vv_1,\vv_2,\vv_3, \ldots, \vv_{k}\}
\end{equation*}
</div>
<p class="continuation">for <span class="process-math">\(1\leq k\leq m\text{.}\)</span></p></article><p id="p-4262">The Gram-Schmidt process builds an orthogonal basis <span class="process-math">\(\{\vw_1, \vw_2, \vw_3, \ldots, \vw_{m}\}\)</span> for us from a given basis. To make an orthonormal basis <span class="process-math">\(\{\vu_1, \vu_2, \vu_3, \ldots, \vu_{m}\}\text{,}\)</span> all we need do is normalize each basis vector: that is, for each <span class="process-math">\(i\text{,}\)</span> we let</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\vu_i = \ds \frac{\vw_i}{|| \vw_i ||} \,\text{.}
\end{equation*}
</div>
<article class="activity project-like" id="act_6_d_gs_examples"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">25.5</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-408"><p id="p-4263">Let <span class="process-math">\(W = \Span \left\{\left[ \begin{array}{c} 1\\1\\0\\0 \end{array} \right], \left[ \begin{array}{c} 0\\1\\1\\0 \end{array} \right], \left[ \begin{array}{c} 0\\0\\1\\1 \end{array} \right] \right\}\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1405"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4264">Use the Gram-Schmidt process to find an orthogonal basis for <span class="process-math">\(W\text{.}\)</span></p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-43" id="hint-43"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-43"><div class="hint solution-like"><p id="p-4265">Order your vectors carefully to minimize computation.</p></div></div>
</div></article><article class="task exercise-like" id="task-1406"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-4266">Find the projection of the vector <span class="process-math">\(\vv = [0 \ 1 \ 1 \ 1]^{\tr}\)</span> onto <span class="process-math">\(W\text{.}\)</span></p></article></article></section><section class="section" id="sec_qr_fact"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">The QR Factorization of a Matrix</span>
</h3>
<p id="p-4267">There are several different factorizations, or decompositions, of matrices where each matrix is written as a product of certain types of matrices: LU decomposition using lower and upper triangular matrices (see <a href="chap_det_properties.html" class="internal" title="Chapter 22: Properties of Determinants">Chapter 22</a>), EVD (EigenVector Decomposition) decomposition using eigenvectors and diagonal matrices (see <a href="chap_diagonalization.html" class="internal" title="Chapter 19: Diagonalization">Chapter 19</a>), and in this section we will introduce the QR decomposition of a matrix. The QR decomposition is one of the most important tools for matrix computations. Jack Dongarra and Francis Sullivan<a href="" data-knowl="" class="id-ref fn-knowl original" data-refid="hk-fn-40" id="fn-40"><sup> 40 </sup></a> nominated the QR decomposition as one of the “10 algorithms with the greatest influence on the development and practice of science and engineering in the 20th century.” The QR algorithm's importance stems from the fact that is a “genuinely new contribution to the field of numerical analysis and not just a refinement of ideas given by Newton, Gauss, Hadamard, or Schur.”<a href="" data-knowl="" class="id-ref fn-knowl original" data-refid="hk-fn-41" id="fn-41"><sup> 41 </sup></a> Most computer algebra systems (e.g., MATLAB) use a variant of the QR decomposition to compute eigenvalues. While there are other methods for approximating eigenvalues, the QR algorithm stands out. For example, the power method is useful when a matrix is large and contains a lot of zero entries (such matrices are called <dfn class="terminology">sparse</dfn>). When most of the entries of a matrix are nonzero, the power method is not recommended. The better, more sophisticated, alternative in these situations is to use the QR decomposition. The QR factorization has applications to solving least squares problems and approximating eigenvalues of matrices.</p>
<article class="activity project-like" id="act_6_e_QR"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">25.6</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-409"><p id="p-4268">Let <span class="process-math">\(A = \left[ \begin{array}{cc} 1\amp 0 \\ 0\amp 0 \\ 0\amp 2 \end{array} \right]\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1407"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4269">Find an orthonormal basis <span class="process-math">\(\CB\)</span> for <span class="process-math">\(\Col A\text{.}\)</span> Let <span class="process-math">\(Q\)</span> be the matrix whose columns are these orthonormal basis vectors.</p></article><article class="task exercise-like" id="task-1408"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-4270">Write the columns of <span class="process-math">\(A\)</span> as linear combinations of the columns of <span class="process-math">\(Q\text{.}\)</span> That is, if <span class="process-math">\(A = [\va_1 \ \va_2]\text{,}\)</span> find <span class="process-math">\([\va_1]_{\CB}\)</span> and <span class="process-math">\([\va_2]_{\CB}\text{.}\)</span> Let <span class="process-math">\(R = [[\va_1]_{\CB} \ [\va_2]_{\CB}]\text{.}\)</span></p></article><article class="task exercise-like" id="task-1409"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-4271">Find the product <span class="process-math">\(QR\)</span> and compare to <span class="process-math">\(A\text{.}\)</span></p></article></article><p id="p-4272"><a href="" class="xref" data-knowl="./knowl/act_6_e_QR.html" title="Activity 25.6">Activity 25.6</a> contains the main ideas to find the QR factorization of a matrix. Let</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/act_6_e_QR.html">
\begin{equation*}
A = [\va_1  \ \va_2  \ \va_3  \ \cdots  \ \va_n ]
\end{equation*}
</div>
<p class="continuation">be an <span class="process-math">\(m \times n\)</span> matrix with rank<a href="" data-knowl="" class="id-ref fn-knowl original" data-refid="hk-fn-42" id="fn-42"><sup> 42 </sup></a> <span class="process-math">\(n\text{.}\)</span> We can use the Gram-Schmidt process to find an orthonormal basis <span class="process-math">\(\{\vu_1, \vu_2, \ldots, \vu_n\}\)</span> for <span class="process-math">\(\Col A\text{.}\)</span> Recall also that <span class="process-math">\(\Span\{\vu_1, \vu_2, \ldots, \vu_k\}  = \Span\{\va_1, \va_2, \ldots, \va_k\}\)</span> for any <span class="process-math">\(k\)</span> between 1 and <span class="process-math">\(n\text{.}\)</span> Let</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/act_6_e_QR.html">
\begin{equation*}
Q = [\vu_1 \   \vu_2 \  \vu_3 \  \cdots \  \vu_n ]\text{.}
\end{equation*}
</div>
<p id="p-4273">If <span class="process-math">\(k\)</span> is between 1 and <span class="process-math">\(n\text{,}\)</span> then <span class="process-math">\(\va_k\)</span> is in <span class="process-math">\(\Span\{\vu_1, \vu_2, \ldots, \vu_k\}\)</span> and</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\va_k = r_{1k}\vu_1 + r_{2k}\vu_2 + \cdots + r_{kk}\vu_k
\end{equation*}
</div>
<p class="continuation">for some scalars <span class="process-math">\(r_{1k}\text{,}\)</span> <span class="process-math">\(r_{2k}\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\(r_{kk}\text{.}\)</span> Then</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
Q \left[ \begin{array}{c} r_{1k} \\ r_{2k} \\ \vdots \\ r_{kk} \\ 0 \\ \vdots \\ 0 \end{array}  \right] = \va_k\text{.}
\end{equation*}
</div>
<p id="p-4274">If we let <span class="process-math">\(\vr_k = \left[ \begin{array}{c} r_{1k} \\ r_{2k} \\ \vdots \\ r_{kk} \\ 0 \\ \vdots \\ 0 \end{array}  \right]\)</span> for <span class="process-math">\(k\)</span> from 1 to <span class="process-math">\(n\text{,}\)</span> then</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A = [Q\vr_1  \ Q\vr_2  \ Q\vr_3  \ \cdots  \ Q\vr_k]\text{.}
\end{equation*}
</div>
<p id="p-4275">This is the QR factorization of <span class="process-math">\(A\)</span> into the product</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A = QR
\end{equation*}
</div>
<p class="continuation">where the columns of <span class="process-math">\(Q\)</span> form an orthonormal basis for <span class="process-math">\(\Col A\)</span> and</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
R = [\vr_1  \ \vr_2  \ \vr_3  \ \cdots  \ \vr_n ] = \left[ \begin{array}{cccccc} r_{11}\amp r_{12}\amp r_{13}\amp  \cdots \amp r_{1n-1}\amp r_{1n} \\ 0\amp r_{22}\amp r_{23}\amp  \cdots \amp r_{2n-1}\amp r_{2n} \\ 0\amp 0\amp r_{33}\amp  \cdots \amp r_{3n-1}\amp r_{3n} \\ \vdots\amp \vdots \amp \vdots \amp \vdots \amp \vdots \amp \vdots \\ 0\amp 0\amp 0\amp  \cdots \amp 0\amp r_{nn} \end{array}  \right]
\end{equation*}
</div>
<p class="continuation">is an upper triangular matrix. Note that <span class="process-math">\(Q\)</span> is an <span class="process-math">\(m \times n\)</span> matrix and <span class="process-math">\(R = [r_{ij}]\)</span> is an <span class="process-math">\(n \times n\)</span> matrix with <span class="process-math">\(r_{ii} \neq 0\)</span> for each <span class="process-math">\(i\text{.}\)</span></p>
<article class="activity project-like" id="activity-100"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">25.7</span><span class="period">.</span>
</h4>
<p id="p-4276">The set <span class="process-math">\(\{\vu_1, \vu_2\}\text{,}\)</span> where <span class="process-math">\(\vu_1 = [1 \ 1 \ 1 \ 1]^{\tr}\)</span> and <span class="process-math">\(\vu_2 = [1 \ -1 \ 1 \ -1]^{\tr}\)</span> is an orthogonal basis for the column space of a <span class="process-math">\(4 \times 2\)</span> matrix <span class="process-math">\(A = [\va_1 \ \va_2]\text{.}\)</span> Moreover, <span class="process-math">\(\va_1 = 2\vu_1\)</span> and <span class="process-math">\(\va_2 = 3\vu_2+4\vu_2\text{.}\)</span> What is <span class="process-math">\(A\text{?}\)</span></p></article><p id="p-4277">The QR factorization provides a widely used algorithm (the QR algorithm) for approximating all of the eigenvalues of a matrix. The computer system MATLAB utilizes four versions of the QR algorithm to approximate the eigenvalues of real symmetric matrices, eigenvalues of real nonsymmetric matrices, eigenvalues of pairs of complex matrices, and singular values of general matrices.</p>
<p id="p-4278">The algorithm works as follows.</p>
<ul class="disc">
<li id="li-681"><p id="p-4279">Start with an <span class="process-math">\(n \times n\)</span> matrix <span class="process-math">\(A\text{.}\)</span> Let <span class="process-math">\(A_1 = A\text{.}\)</span></p></li>
<li id="li-682"><p id="p-4280">Find the QR factorization for <span class="process-math">\(A_1\)</span> and write it as <span class="process-math">\(A_1 = Q_1R_1\text{,}\)</span> where <span class="process-math">\(Q_1\)</span> is orthogonal and <span class="process-math">\(R_1\)</span> is upper triangular.</p></li>
<li id="li-683"><p id="p-4281">Let <span class="process-math">\(A_2 = Q_1^{-1}A_1Q_1 = Q_1^{\tr}AQ_1 = R_1Q_1\text{.}\)</span> Find the QR factorization of <span class="process-math">\(A_2\)</span> and write it as <span class="process-math">\(A_2 = Q_2R_2\text{.}\)</span></p></li>
<li id="li-684"><p id="p-4282">Let <span class="process-math">\(A_3 = Q_2^{-1}A_2Q_2 = Q_2^{\tr}AQ_2 = R_2Q_2\text{.}\)</span> Find the QR factorization of <span class="process-math">\(A_3\)</span> and write it as <span class="process-math">\(A_3 = Q_3R_3\text{.}\)</span></p></li>
<li id="li-685"><p id="p-4283">Continue in this manner to obtain a sequence <span class="process-math">\(\{A_k\}\)</span> where <span class="process-math">\(A_k = Q_kR_k\)</span> and <span class="process-math">\(A_{k+1} = R_kQ_k\text{.}\)</span></p></li>
</ul>
<p id="p-4284">Note that <span class="process-math">\(A_{k+1} = Q_k^{-1}A_kQ_k\)</span> and so all of the matrices <span class="process-math">\(A_k\)</span> are similar to each other and therefore all have the same eigenvalues. We won't go into the details, but it can be shown that if the eigenvalues of <span class="process-math">\(A\)</span> are real and have distinct absolute values, then the sequence <span class="process-math">\(\{A_i\}\)</span> converges to an upper triangular matrix with the eigenvalues of <span class="process-math">\(A\)</span> as the diagonal entries. If some of the eigenvalues of <span class="process-math">\(A\)</span> are complex, then the sequence <span class="process-math">\(\{A_i\}\)</span> converges to a block upper triangular matrix, where the diagonal blocks are either <span class="process-math">\(1 \times 1\)</span> (approximating the real eigenvalues of <span class="process-math">\(A\)</span>) or <span class="process-math">\(2 \times 2\)</span> (which provide a pair of conjugate complex eigenvalues of <span class="process-math">\(A\)</span>).</p></section><section class="section" id="sec_gram_schmidt_examples"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">Examples</span>
</h3>
<p id="p-4285">What follows are worked examples that use the concepts from this section.</p>
<article class="example example-like" id="example-50"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">25.7</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-410"><p id="p-4286">Let <span class="process-math">\(W\)</span> be the subspace of <span class="process-math">\(\R^4\)</span> spanned by <span class="process-math">\(\vw_1 = [1 \ 0 \ 0 \ 0]^{\tr}\text{,}\)</span> <span class="process-math">\(\vw_2 = [1 \ 1 \ 1 \ 0]^{\tr}\text{,}\)</span> and <span class="process-math">\(\vw_3 = [1 \ 2 \ 0 \ 1]^{\tr}\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1410"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4287">Use the Gram-Schmidt process to find an orthonormal basis for the subspace <span class="process-math">\(W\)</span> of <span class="process-math">\(\R^4\)</span> spanned by <span class="process-math">\(\vw_1 = [1 \ 0 \ 0 \ 0]^{\tr}\text{,}\)</span> <span class="process-math">\(\vw_2 = [1 \ 1 \ 1 \ 0]^{\tr}\text{,}\)</span> and <span class="process-math">\(\vw_3 = [1 \ 2 \ 0 \ 1]^{\tr}\text{.}\)</span></p>
<div class="solution solution-like" id="solution-152">
<h5 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h5>
<p id="p-4288">First note that <span class="process-math">\(\vw_1\text{,}\)</span> <span class="process-math">\(\vw_2\text{,}\)</span> and <span class="process-math">\(\vw_3\)</span> are linearly independent. We let <span class="process-math">\(\vv_1 = \vw_1\)</span> and the Gram-Schmidt process gives us</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-170">
\begin{align*}
\vv_2 \amp = \vw_2 - \frac{\vw_2 \cdot \vv_1}{\vv_1 \cdot \vv_1} \vv_1\\
\amp = [1 \ 1 \ 1 \ 0]^{\tr} - \frac{1}{1}[1 \ 0 \ 0 \ 0]^{\tr}\\
\amp = [0 \ 1 \ 1 \ 0]^{\tr}
\end{align*}
</div>
<p class="continuation">and</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-171">
\begin{align*}
\vv_3 \amp = \vw_3 - \frac{\vw_3 \cdot \vv_1}{\vv_1 \cdot \vv_1} \vv_1 - \frac{\vw_3 \cdot \vv_2}{\vv_2 \cdot \vv_2} \vv_2\\
\amp = [1 \ 2 \ 0 \ 1]^{\tr} - \frac{1}{1}[1 \ 0 \ 0 \ 0]^{\tr} - \frac{2}{2}[0 \ 1 \ 1 \ 0]^{\tr}\\
\amp = [0 \ 1 \ -1 \ 1]^{\tr}\text{.}
\end{align*}
</div>
<p class="continuation">So <span class="process-math">\(\{\vv_1, \vv_2, \vv_3\}\)</span> is an orthogonal basis for <span class="process-math">\(W\text{.}\)</span> An orthonormal basis is found by dividing each vector by its magnitude, so</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\{[1 \ 0 \ 0 \ 0]^{\tr}, \frac{1}{\sqrt{2}} [0 \ 1 \ 1 \ 0]^{\tr}, \frac{1}{\sqrt{3}}[0 \ 1 \ -1 \ 1]^{\tr}\}
\end{equation*}
</div>
<p class="continuation">is an orthonormal basis for <span class="process-math">\(W\text{.}\)</span></p>
</div></article><article class="task exercise-like" id="task-1411"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-4289">Find a QR factorization of the matrix <span class="process-math">\(A = \left[ \begin{array}{cccc} 1\amp 1\amp 1\amp 0 \\ 0\amp 1\amp 2\amp 0 \\ 0\amp 1\amp 0\amp 0 \\ 0\amp 0\amp 1\amp 1 \end{array} \right]\text{.}\)</span></p>
<div class="solution solution-like" id="solution-153">
<h5 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h5>
<p id="p-4290">Technology shows that the reduced row echelon form of <span class="process-math">\(A\)</span> is <span class="process-math">\(I_4\text{,}\)</span> so the columns of <span class="process-math">\(A\)</span> are linearly independent and <span class="process-math">\(A\)</span> has rank <span class="process-math">\(4\text{.}\)</span> From part (a) we have an orthogonal basis for the span of the first three columns of <span class="process-math">\(A\text{.}\)</span> To find a fourth vector to add so that the span is <span class="process-math">\(\Col \ A\text{,}\)</span> we apply the Gram-Schmidt process one more time with <span class="process-math">\(\vw_4 = [0 \ 0 \ 0 \ 1]^{\tr}\text{:}\)</span></p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-172">
\begin{align*}
\vv_4 \amp = \vw_4 - \frac{\vw_4 \cdot \vv_1}{\vv_1 \cdot \vv_1} \vv_1 - \frac{\vw_4 \cdot \vv_2}{\vv_2 \cdot \vv_2} \vv_2 - \frac{\vw_4 \cdot \vv_3}{\vv_3 \cdot \vv_3} \vv_3\\
\amp = [0 \ 0 \ 0 \ 1]^{\tr} - \frac{0}{1}[1 \ 0 \ 0 \ 0]^{\tr} - \frac{0}{2}[0 \ 1 \ 1 \ 0]^{\tr} - \frac{1}{3}[0 \ 1 \ -1 \ 1]^{\tr}\\
\amp = \frac{1}{3} [0 \ -1 \ 1 \ 2]^{\tr}\text{.}
\end{align*}
</div>
<p class="continuation">So <span class="process-math">\(\{\vu_1, \vu_2, \vu_3, \vu_4\}\)</span> is an orthonormal basis for <span class="process-math">\(\Col \ A\text{,}\)</span> where</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\begin{array}{ccc} \vu_1 = [1 \ 0 \ 0 \ 0]^{\tr} \amp  \amp \vu_2 = \frac{\sqrt{2}}{2}[0 \ 1 \ 1 \ 0]^{\tr} \\ \vu_3 = \frac{\sqrt{3}}{3}[0 \ 1 \ -1 \ 1]^{\tr} \amp    \amp \vu_4 = \frac{\sqrt{6}}{6} [0 \ -1 \ 1 \ 2]^{\tr}. \end{array}
\end{equation*}
</div>
<p class="continuation">This makes</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
Q = \left[  \begin{array}{ccrr} 1\amp 0\amp 0\amp 0 \\ 0\amp \frac{\sqrt{2}}{2}\amp \frac{\sqrt{3}}{3}\amp -\frac{\sqrt{6}}{6} \\ 0\amp \frac{\sqrt{2}}{2}\amp -\frac{\sqrt{3}}{3}\amp \frac{\sqrt{6}}{6} \\ 0\amp 0\amp \frac{\sqrt{3}}{3}\amp \frac{\sqrt{6}}{3} \end{array}  \right]\text{.}
\end{equation*}
</div>
<p class="continuation">To find the matrix <span class="process-math">\(R\text{,}\)</span> we write the columns of <span class="process-math">\(A\)</span> in terms of the basis vectors <span class="process-math">\(\vu_1\text{,}\)</span> <span class="process-math">\(\vu_2\text{,}\)</span> <span class="process-math">\(\vu_3\text{,}\)</span> and <span class="process-math">\(\vu_4\text{.}\)</span> Technology shows that the reduced row echelon form of <span class="process-math">\([Q  \ | \ A]\)</span> is</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\left[   \begin{array}{cccc|cccc} 1\amp 0\amp 0\amp 0\amp 1\amp 1\amp 1\amp 0 \\ 0\amp 1\amp 0\amp 0\amp 0\amp \sqrt{2}\amp \sqrt{2}\amp 0 \\ 0\amp 0\amp 1\amp 0\amp 0\amp 0\amp \sqrt{3}\amp \frac{\sqrt{3}}{3} \\ 0\amp 0\amp 0\amp 1\amp 0\amp 0\amp 0\amp \frac{\sqrt{6}}{3} \end{array}  \right]\text{.}
\end{equation*}
</div>
<p class="continuation">So</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
R = \left[  \begin{array}{cccc} 1\amp 1\amp 1\amp 0 \\ 0\amp \sqrt{2}\amp \sqrt{2}\amp 0 \\ 0\amp 0\amp \sqrt{3}\amp \frac{\sqrt{3}}{3} \\ 0\amp 0\amp 0\amp \frac{\sqrt{6}}{3} \end{array}  \right]\text{.}
\end{equation*}
</div>
</div></article></article><article class="example example-like" id="example-51"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">25.8</span><span class="period">.</span>
</h4>
<p id="p-4291">Let <span class="process-math">\(W = \Span\{[1 \ 0 \ 1 \ 0]^{\tr}, [0 \ 0 \ 1 \ -1]^{\tr}, [1 \ 0 \ -1 \ 0]^{\tr}\}\text{.}\)</span> Find the vector in <span class="process-math">\(W\)</span> that is closest to the vector <span class="process-math">\([1 \ 1 \ 1 \ 1]^{\tr}\text{.}\)</span> Provide a numeric measure of the error in approximating <span class="process-math">\([1 \ 1 \ 1 \ 1]^{\tr}\)</span> by this vector.</p>
<div class="solution solution-like" id="solution-154">
<h4 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h4>
<p id="p-4292">Our job is to find <span class="process-math">\(\proj_{W} [1 \ 1 \ 1 \ 1]^{\tr}\text{.}\)</span> To do this, we need an orthogonal basis of <span class="process-math">\(W\text{.}\)</span> Let <span class="process-math">\(\vv_1 = [1  \ 0 \ 1 \ 0]^{\tr}\text{,}\)</span> <span class="process-math">\(\vv_2 = [0 \ 0 \ 1 \ -1]^{\tr}\text{,}\)</span> and <span class="process-math">\(\vv_3 = [1 \ 0 \ -1 \ 0]^{\tr}\text{.}\)</span> Technology shows that each column of the matrix <span class="process-math">\([\vv_1 \ \vv_2 \ \vv_3]\)</span> is a pivot column, so the set <span class="process-math">\(\{\vv_1, \vv_2, \vv_3\}\)</span> is a basis for <span class="process-math">\(W\text{.}\)</span> We apply the Gram-Schmidt process to this basis to obtain an orthogonal basis <span class="process-math">\(\{\vw_1, \vw_2, \vw_3\}\)</span> of <span class="process-math">\(W\text{.}\)</span> We start with <span class="process-math">\(\vw_1= \vv_1\text{,}\)</span> then</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-173">
\begin{align*}
\vw_2 \amp = \vv_2 - \frac{\vv_2 \cdot \vw_1}{ \vw_1 \cdot \vw_1} \vw_1\\
\amp = [0 \ 0 \ 1 \ -1]^{\tr} - \frac{1}{2} [1  \ 0 \ 1 \ 0]^{\tr}\\
\amp = \frac{1}{2}[-1 \ 0 \ 1 \ -2]^{\tr}
\end{align*}
</div>
<p class="continuation">and</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-174">
\begin{align*}
\vw_3 \amp = \vv_3 - \frac{\vv_3 \cdot \vw_1}{ \vw_1 \cdot \vw_1} \vw_1 - \frac{\vv_3 \cdot \vw_2}{ \vw_2 \cdot \vw_2} \vw_2\\
\amp = [1 \ 0 \ -1 \ 0]^{\tr} - \frac{0}{2} [1  \ 0 \ 1 \ 0]^{\tr} + \frac{2}{3} \left(\frac{1}{2}[-1 \ 0 \ 1 \ -2]^{\tr}\right)\\
\amp = \frac{1}{3} [2 \ 0 \ -2 \ -2]^{\tr}\text{.}
\end{align*}
</div>
<p class="continuation">Then, letting <span class="process-math">\(\vz = [1 \ 1 \ 1 \ 1]^{\tr}\)</span> we have</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-175">
\begin{align*}
\proj_{W} \vz \amp = \frac{\vz \cdot \vw_1}{\vw_1 \cdot \vw_1} \vw_1 +  \frac{\vz \cdot \vw_2}{\vw_2 \cdot \vw_2} \vw_2 +  \frac{\vz \cdot \vw_3}{\vw_3 \cdot \vw_3} \vw_3\\
\amp = \frac{2}{2}[1  \ 0 \ 1 \ 0]^{\tr} + \frac{-1}{3/2}\frac{1}{2}[-1 \ 0 \ 1 \ -2]^{\tr} + \frac{-2/3}{4/3}  \frac{1}{3} [2 \ 0 \ -2 \ -2]^{\tr}\\
\amp = [1  \ 0 \ 1 \ 0]^{\tr} - \frac{1}{3}[-1 \ 0 \ 1 \ -2]^{\tr} - \frac{1}{6}[2 \ 0 \ -2 \ -2]^{\tr}\\
\amp = [1 \ 0 \ 1 \ 1]^{\tr}\text{.}
\end{align*}
</div>
<p class="continuation">The norm of <span class="process-math">\(\proj_{W^{\perp}} \vz = \vz - \proj_W \vz\)</span> tells us how well our projection <span class="process-math">\([1 \ 0 \ 1 \ 1]^{\tr}\)</span> approximates <span class="process-math">\(\vz = [1 \ 1 \ 1 \ 1]^{\tr}\text{.}\)</span> Now</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
||\proj_{W^{\perp}} \vz || = ||[1 \ 1 \ 1 \ 1]^{\tr} - [1 \ 0 \ 1 \ 1]^{\tr}|| = ||[0 \ 1 \ 0 \ 0]^{\tr}|| = 1\text{,}
\end{equation*}
</div>
<p class="continuation">so <span class="process-math">\([1 \ 0 \ 1 \ 1]^{\tr}\)</span> is one unit away from <span class="process-math">\(\vz = [1 \ 1 \ 1 \ 1]^{\tr}\text{.}\)</span></p>
</div></article></section><section class="section" id="sec_gram_schmidt_summ_noips"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">Summary</span>
</h3>
<ul class="disc">
<li id="li-686">
<p id="p-4293">The projection of the vector <span class="process-math">\(\vv\)</span> in <span class="process-math">\(V\)</span> onto <span class="process-math">\(W\)</span> is the vector</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\proj_W \vv = \frac{\vv \cdot \vw_1}{\vw_1 \cdot \vw_1} \vw_1 + \frac{\vv \cdot \vw_2}{\vw_2 \cdot \vw_2}  \vw_2 + \cdots + \frac{\vv \cdot \vw_m}{\vw_m \cdot \vw_m}  \vw_m\text{,}
\end{equation*}
</div>
<p class="continuation">where <span class="process-math">\(W\)</span> is the a subspace of <span class="process-math">\(\R^n\)</span> with orthogonal basis <span class="process-math">\(\{\vw_1, \vw_2, \ldots, \vw_m\}\text{.}\)</span> These projections are important in that <span class="process-math">\(\proj_W \vv\)</span> is the best approximation of the vector <span class="process-math">\(\vv\)</span> by a vector in <span class="process-math">\(W\)</span> in the least squares sense.</p>
</li>
<li id="li-687">
<p id="p-4294">With <span class="process-math">\(W\)</span> as in (a), the projection of <span class="process-math">\(\vv\)</span> orthogonal to <span class="process-math">\(W\)</span> is the vector</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\proj_{\perp W} \vv = \vv - \proj_W \vv\text{.}
\end{equation*}
</div>
<p class="continuation">The norm of <span class="process-math">\(\proj_{\perp W} \vv\)</span> provides a measure of how well <span class="process-math">\(\proj_W \vv\)</span> approximates the vector <span class="process-math">\(\vv\text{.}\)</span></p>
</li>
<li id="li-688">
<p id="p-4295">The Gram-Schmidt process produces an orthogonal basis from any basis. It works as follows. Let <span class="process-math">\(\{\vv_1, \vv_2, \ldots, \vv_m\}\)</span> be a basis for a subspace <span class="process-math">\(W\)</span> of <span class="process-math">\(\R^n\text{.}\)</span> The set <span class="process-math">\(\{\vw_1\text{,}\)</span> <span class="process-math">\(\vw_2\text{,}\)</span> <span class="process-math">\(\vw_3\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\(\vw_{m}\}\)</span> defined by</p>
<ul class="circle">
<li id="li-689"><p id="p-4296"><span class="process-math">\(\vw_1 = \vv_1\text{,}\)</span></p></li>
<li id="li-690"><p id="p-4297"><span class="process-math">\(\vw_2 = \vv_2 - \ds \frac{\vv_2 \cdot \vw_1}{\vw_1 \cdot \vw_1} \vw_1\text{,}\)</span></p></li>
<li id="li-691"><p id="p-4298"><span class="process-math">\(\vw_3 = \vv_3 - \ds \frac{ \vv_3 \cdot \vw_1 }{ \vw_1 \cdot \vw_1 } \vw_1 - \frac{ \vv_3 \cdot \vw_2 }{ \vw_2 \cdot \vw_2 } \vw_2\text{,}\)</span> <span class="process-math">\(\vdots\)</span></p></li>
<li id="li-692"><p id="p-4299"><span class="process-math">\(\vw_m = \vv_m - \ds \frac{ \vv_m \cdot \vw_1 }{ \vw_1 \cdot \vw_1 } \vw_1 - \frac{ \vv_m \cdot \vw_2 }{ \vw_2 \cdot \vw_2 } \vw_2 - \cdots - \frac{ \vv_m \cdot \vw_{m-1} }{ \vw_{m-1} \cdot \vw_{m-1} } \vw_{m-1}\text{.}\)</span></p></li>
</ul>
<p class="continuation">is an orthogonal basis for <span class="process-math">\(W\text{.}\)</span> Moreover,</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\Span\{\vw_1, \vw_2, \vw_3, \ldots, \vw_{k}\} = \Span\{\vv_1,\vv_2,\vv_3, \ldots, \vv_{k}\}
\end{equation*}
</div>
<p class="continuation">for each <span class="process-math">\(k\)</span> between 1 and <span class="process-math">\(m\text{.}\)</span></p>
</li>
<li id="li-693">
<p id="p-4300">The QR factorization has applications to solving least squares problems and approximating eigenvalues of matrices. The QR factorization writes an <span class="process-math">\(m \times n\)</span> matrix with rank <span class="process-math">\(n\)</span> as a product <span class="process-math">\(A = QR\text{,}\)</span> where the columns of <span class="process-math">\(Q\)</span> form an orthonormal basis for <span class="process-math">\(\Col A\)</span> and</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
R = [\vr_1 \ | \ \vr_2 \ | \ \vr_3 \ | \ \cdots \ | \ \vr_n ] = \left[ \begin{array}{cccccc} r_{11}\amp r_{12}\amp r_{13}\amp  \cdots \amp r_{1n-1}\amp r_{1n} \\ 0\amp r_{22}\amp r_{23}\amp  \cdots \amp r_{2n-1}\amp r_{2n} \\ 0\amp 0\amp r_{33}\amp  \cdots \amp r_{3n-1}\amp r_{3n} \\ \vdots\amp \vdots \amp \vdots \amp \vdots \amp \vdots \amp \vdots \\ 0\amp 0\amp 0\amp  \cdots \amp 0\amp r_{nn} \end{array}  \right]
\end{equation*}
</div>
<p class="continuation">is an upper triangular matrix.</p>
</li>
</ul></section><section class="exercises" id="sec_gram_schmidt_exercises"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber"></span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-245"><h4 class="heading"><span class="codenumber">1<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-411"><p id="p-4301">Let <span class="process-math">\(W = \Span \{\vw_1, \vw_2\}\)</span> in <span class="process-math">\(\R^3\text{,}\)</span> with <span class="process-math">\(\vw_1=[1 \ 2 \ 1]^{\tr}\)</span> and <span class="process-math">\(\vw_2= [1 \ -1 \ 1]^{\tr}\text{,}\)</span> and <span class="process-math">\(\vv = [1 \ 0 \ 0]^{\tr}\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1412"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4302">Find <span class="process-math">\(\proj_W \vv\)</span> and <span class="process-math">\(\proj_{\perp W} \vv\)</span></p></article><article class="task exercise-like" id="task-1413"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-4304">Find the vector in <span class="process-math">\(W\)</span> that is closest to <span class="process-math">\(\vv\text{.}\)</span> How close is this vector to <span class="process-math">\(\vv\text{?}\)</span></p></article></article><article class="exercise exercise-like" id="exercise-246"><h4 class="heading"><span class="codenumber">2<span class="period">.</span></span></h4>
<p id="p-4306">Let <span class="process-math">\(\CB = \left\{\left[ \begin{array}{r} 1 \\ 0 \\ -1 \\ 0 \end{array} \right], \left[ \begin{array}{r} 0 \\ 1 \\ 0 \\ -1 \end{array} \right], \left[ \begin{array}{r} -1 \\ 1 \\ -1 \\ 1 \end{array} \right]\right\}\)</span> and let <span class="process-math">\(W = \Span(\CB)\)</span> in <span class="process-math">\(\R^4\text{.}\)</span> Find the best approximation in <span class="process-math">\(W\)</span> to the vector <span class="process-math">\(\vv = \left[ \begin{array}{r} 2 \\ 3 \\ 1 \\ -1 \end{array} \right]\)</span> in <span class="process-math">\(W\)</span> and give a numeric estimate of how good this approximation is.</p></article><article class="exercise exercise-like" id="exercise-247"><h4 class="heading"><span class="codenumber">3<span class="period">.</span></span></h4>
<p id="p-4307">In this exercise we determine the least-squares line (the line of best fit in the least squares sense) to a set of <span class="process-math">\(k\)</span> data points <span class="process-math">\((x_1,y_1)\text{,}\)</span> <span class="process-math">\((x_2,y_2)\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\((x_k,y_k)\)</span> in the plane. In this case, we want to fit a line of the form <span class="process-math">\(f(x) = mx+b\)</span> to the data. If the data were to lie on a line, then we would have a solution to the system</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-176">
\begin{align*}
mx_1 + b \amp = y_1\\
mx_2 + b \amp = y_2\\
\vdots  \amp  \vdots\\
mx_k + b \amp = y_k
\end{align*}
</div>
<p class="continuation">This system can be written as</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
m \vw_1 + b\vw_2 = \vy\text{,}
\end{equation*}
</div>
<p class="continuation">where <span class="process-math">\(\vw_1 = [x_1 \ x_2 \ \cdots \ x_k]^{\tr}\text{,}\)</span> <span class="process-math">\(\vw_2 = [1 \ 1 \ \cdots \ 1]^{\tr}\text{,}\)</span> and <span class="process-math">\(\vy = [y_1 \ y_2 \ \cdots \ y_k]^{\tr}\text{.}\)</span> If the data does not lie on a line, then the system won't have a solution. Instead, we want to minimize the square of the distance between <span class="process-math">\(\vy\)</span> and a vector of the form <span class="process-math">\(m\vw_1 + b \vw_2\text{.}\)</span> That is, minimize</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="eq_6_d_LS_line_1">
\begin{equation}
||\vy - (m \vw_1 + b\vw_2)||^2\text{.}\tag{25.1}
\end{equation}
</div>
<p class="continuation">Rephrasing this in terms of projections, we are looking for the vector in <span class="process-math">\(W = \Span\{\vw_1, \vw_2\}\)</span> that is closest to <span class="process-math">\(\vy\text{.}\)</span> In other words, the values of <span class="process-math">\(m\)</span> and <span class="process-math">\(b\)</span> will occur as the weights when we write <span class="process-math">\(\proj_{W} \vy\)</span> as a linear combination of <span class="process-math">\(\vw_1\)</span> and <span class="process-math">\(\vw_2\text{.}\)</span> The one wrinkle in this problem is that we need an orthogonal basis for <span class="process-math">\(W\)</span> to find this projection. Find the least squares line for the data points <span class="process-math">\((1,2)\text{,}\)</span> <span class="process-math">\((2,4)\)</span> and <span class="process-math">\((3,5)\)</span> in <span class="process-math">\(\R^2\text{.}\)</span></p></article><article class="exercise exercise-like" id="exercise-248"><h4 class="heading"><span class="codenumber">4<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-412"><p id="p-4309">Each set <span class="process-math">\(S\)</span> is linearly independent. Use the Gram-Schmidt process to create an orthogonal set of vectors with the same span as <span class="process-math">\(S\text{.}\)</span> Then find an orthonormal basis for the same span.</p></div>
<article class="task exercise-like" id="task-1414"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4310"><span class="process-math">\(S = \left\{[1 \ 1 \ 1]^{\tr}, [5 \ -1 \ 2]^{\tr}\right\}\)</span> in <span class="process-math">\(\R^3\)</span></p></article><article class="task exercise-like" id="task-1415"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-4311"><span class="process-math">\(S = \left\{[1 \ 0 \ 2]^{\tr}, [-1 \ 1 \ 1]^{\tr}, [1 \ 1 \ 19]^{\tr} \right\}\)</span> in <span class="process-math">\(\R^3\)</span></p></article><article class="task exercise-like" id="task-1416"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-4312"><span class="process-math">\(S = \left\{[1 \ 0 \ 1 \ 0 \ 1 \ 0 \ 1]^{\tr}, [-1 \ 2 \ 3 \ 0 \ 1 \ 0 \ 1]^{\tr}, [1 \ 0 \ 4 \ -1 \ 2 \ 0 \ 1]^{\tr}, [1 \ 1 \ 1 \ 1 \ 1 \ 1 \ 1]^{\tr} \right\}\)</span> in <span class="process-math">\(\R^7\)</span></p></article></article><article class="exercise exercise-like" id="exercise-249"><h4 class="heading"><span class="codenumber">5<span class="period">.</span></span></h4>
<p id="p-4313">Let <span class="process-math">\(S = \{\vv_1, \vv_2, \vv_3\}\)</span> be a set of linearly dependent vectors in <span class="process-math">\(\R^n\)</span> for some integer <span class="process-math">\(n \geq 3\text{.}\)</span> What is the result if the Gram-Schmidt process is applied to the set <span class="process-math">\(S\text{?}\)</span> Explain.</p></article><article class="exercise exercise-like" id="exercise-250"><h4 class="heading"><span class="codenumber">6<span class="period">.</span></span></h4>
<p id="p-4315">A fellow student wants to find a QR factorization for a <span class="process-math">\(3 \times 4\)</span> matrix. What would you tell this student and why?</p></article><article class="exercise exercise-like" id="exercise-251"><h4 class="heading"><span class="codenumber">7<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-413"><p id="p-4316">Find the QR factorizations of the given matrices.</p></div>
<article class="task exercise-like" id="task-1417"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4317"><span class="process-math">\(\left[ \begin{array}{ccc} 1\amp 2\amp 3 \\ 0\amp 4\amp 5 \\ 0\amp 0\amp 6 \end{array} \right]\)</span></p></article><article class="task exercise-like" id="task-1418"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-4319"><span class="process-math">\(\left[ \begin{array}{cc} 1\amp 2 \\ 1\amp 3 \\ 1\amp 2 \\ 1\amp 3 \end{array} \right]\)</span></p></article><article class="task exercise-like" id="task-1419"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-4321"><span class="process-math">\(\left[ \begin{array}{ccc} 0\amp 1\amp 0\\0\amp 0\amp 0\\3\amp 0\amp 4 \\ 0\amp 0\amp 5 \end{array} \right]\text{.}\)</span></p></article></article><article class="exercise exercise-like" id="exercise-252"><h4 class="heading"><span class="codenumber">8<span class="period">.</span></span></h4>
<p id="p-4323">Find an orthonormal basis <span class="process-math">\(\{\vu_1, \vu_2, \vu_3\}\)</span> of <span class="process-math">\(\R^3\)</span> such that <span class="process-math">\(\Span\{\vu_1,\vu_2\} = \Span\{[1 \ 2 \ 3]^{\tr}, [1 \ 1 \ 0]^{\tr}\}\text{.}\)</span></p></article><article class="exercise exercise-like" id="exercise-253"><h4 class="heading"><span class="codenumber">9<span class="period">.</span></span></h4>
<article class="task exercise-like" id="task-1420"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4324">Let <span class="process-math">\(A = \left[ \begin{array}{cc} 2\amp 1\\2\amp 6\\2\amp 6\\2\amp 1 \end{array} \right] = [\va_1 \ \va_2]\text{.}\)</span> A QR factorization of <span class="process-math">\(A\)</span> has <span class="process-math">\(Q = [\vu_1 \ \vu_2]\)</span> with <span class="process-math">\(\vu_1 = \frac{1}{2}[1 \ 1 \ 1 \ 1]^{\tr}\)</span> and <span class="process-math">\(\vu_2 = \frac{1}{2}[-1 \ 1 \ 1 \ -1]^{\tr}\text{,}\)</span> and <span class="process-math">\(\vu_3 = \frac{1}{\sqrt{12}}[1 \ -1 \ -1 \ 3]^{\tr}\)</span> and <span class="process-math">\(R = [r_{ij}] = \left[ \begin{array}{cc} 4\amp 7 \\ 0\amp 5 \end{array} \right]\text{.}\)</span> Calculate the dot products <span class="process-math">\(\va_i \cdot \vu_j\)</span> for <span class="process-math">\(i\)</span> and <span class="process-math">\(j\)</span> between <span class="process-math">\(1\)</span> and <span class="process-math">\(2\text{.}\)</span> How are these dot products connected to the entries of <span class="process-math">\(R\text{?}\)</span></p></article><article class="task exercise-like" id="task-1421"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-4326">Explain why the result of part (a) is true in general. That is, if <span class="process-math">\(A = [\va_1 \ \ \va_2 \ \ldots \ \va_n]\)</span> has QR factorization with <span class="process-math">\(Q = [\vu_1 \ \vu_2 \ \ldots \ \vu_n]\)</span> and <span class="process-math">\(R = [r_{ij}]\text{,}\)</span> then <span class="process-math">\(r_{ij} = \va_j \cdot \vu_i\text{.}\)</span></p></article></article><article class="exercise exercise-like" id="ex_6_e_upper_triangular_props"><h4 class="heading"><span class="codenumber">10<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-414"><p id="p-4328">Upper triangular matrices play an important role in the QR decomposition. In this exercise we examine two properties of upper triangular matrices.</p></div>
<article class="task exercise-like" id="task-1422"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4329">Show that if <span class="process-math">\(R\)</span> and <span class="process-math">\(S\)</span> are upper triangular <span class="process-math">\(n \times n\)</span> matrices with positive diagonal entries, then <span class="process-math">\(RS\)</span> is also an upper triangular <span class="process-math">\(n \times n\)</span> matrices with positive diagonal entries.</p></article><article class="task exercise-like" id="task-1423"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-4330">Show that if <span class="process-math">\(R\)</span> is an invertible upper triangular matrix with positive diagonal entries, then <span class="process-math">\(R^{-1}\)</span> is also an upper triangular matrix with positive diagonal entries.</p></article></article><article class="exercise exercise-like" id="exercise-255"><h4 class="heading"><span class="codenumber">11<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-415"><p id="p-4331">In this exercise we demonstrate that the QR decomposition of an <span class="process-math">\(m \times n\)</span> matrix with linearly independent columns is unique.</p></div>
<article class="task exercise-like" id="task-1424"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4332">Suppose that <span class="process-math">\(A = QR\text{,}\)</span> where <span class="process-math">\(Q\)</span> is an <span class="process-math">\(m \times n\)</span> matrix with orthogonal columns and <span class="process-math">\(R\)</span> is an <span class="process-math">\(n \times n\)</span> upper triangular matrix. Show that <span class="process-math">\(R\)</span> is invertible.</p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-44" id="hint-44"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-44"><div class="hint solution-like"><p id="p-4333">What can we say about <span class="process-math">\(\vx\)</span> if <span class="process-math">\(R \vx = \vzero\text{?}\)</span></p></div></div>
</div></article><article class="task exercise-like" id="task-1425"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-4335">Show that the only <span class="process-math">\(n \times n\)</span> orthogonal upper triangular matrix with positive diagonal entries is the identity matrix <span class="process-math">\(I_n\text{.}\)</span></p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-45" id="hint-45"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-45"><div class="hint solution-like"><p id="p-4336">Let <span class="process-math">\(A = [\va_1 \ \va_2 \ \ldots \ \va_n] = [a_{ij}]\)</span> be an <span class="process-math">\(n \times n\)</span> orthogonal upper triangular matrices with positive diagonal entries. What does that tell us about <span class="process-math">\(\va_i \cdot \va_j\text{?}\)</span></p></div></div>
</div></article><article class="task exercise-like" id="task-1426"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-4338">Show that if <span class="process-math">\(Q_1\)</span> and <span class="process-math">\(Q_2\)</span> are <span class="process-math">\(m \times n\)</span> matrices with orthogonal columns, and <span class="process-math">\(S\)</span> is a matrix such that <span class="process-math">\(Q_1 = Q_2S\text{,}\)</span> then <span class="process-math">\(S\)</span> is an orthogonal matrix.</p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-46" id="hint-46"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-46"><div class="hint solution-like"><p id="p-4339">Write <span class="process-math">\(Q_1^{\tr}Q_1\)</span> in terms of <span class="process-math">\(Q_2\)</span> and <span class="process-math">\(S\text{.}\)</span></p></div></div>
</div></article><article class="task exercise-like" id="task-1427"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<p id="p-4341">Suppose that <span class="process-math">\(Q_1\)</span> and <span class="process-math">\(Q_2\)</span> are <span class="process-math">\(m \times n\)</span> matrices with orthogonal columns and <span class="process-math">\(R_1\)</span> and <span class="process-math">\(R_2\)</span> are <span class="process-math">\(n \times n\)</span> upper triangular matrices such that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A = Q_1R_1 = Q_2R_2\text{.}
\end{equation*}
</div>
<p class="continuation">Show that <span class="process-math">\(Q_1 = Q_2\)</span> and <span class="process-math">\(R_1 = R_2\text{.}\)</span> Conclude that the QR factorization of a matrix is unique.</p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-47" id="hint-47"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-47"><div class="hint solution-like"><p id="p-4342">Use the previous parts of this problem along with the results of <a href="" class="xref" data-knowl="./knowl/ex_6_e_upper_triangular_props.html" title="Exercise 10">Exercise 10</a>.</p></div></div>
</div></article></article><article class="exercise exercise-like" id="exercise-256"><h4 class="heading"><span class="codenumber">12<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-416"><p id="p-4344">Label each of the following statements as True or False. Provide justification for your response. Throughout, let <span class="process-math">\(V\)</span> be a vector space.</p></div>
<article class="task exercise-like" id="task-1428"><h5 class="heading">
<span class="codenumber">(a)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-4345">If <span class="process-math">\(\{\vw_1, \vw_2\}\)</span> is a basis for a subspace <span class="process-math">\(W\)</span> of <span class="process-math">\(\R^3\text{,}\)</span> then the vector <span class="process-math">\(\frac{\vv \cdot \vw_1}{\vw_1 \cdot \vw_1} \vw_1 + \frac{\vv \cdot \vw_2}{\vw_2 \cdot \vw_2} \vw_2\)</span> is the vector in <span class="process-math">\(W\)</span> closest to <span class="process-math">\(\vv\text{.}\)</span></p></article><article class="task exercise-like" id="task-1429"><h5 class="heading">
<span class="codenumber">(b)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-4347">If <span class="process-math">\(W\)</span> is a subspace of <span class="process-math">\(\R^n\text{,}\)</span> then the vector <span class="process-math">\(\vv - \proj_{W} \vv\)</span> is orthogonal to every vector in <span class="process-math">\(W\text{.}\)</span></p></article><article class="task exercise-like" id="task-1430"><h5 class="heading">
<span class="codenumber">(c)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-4348">If <span class="process-math">\(\vu_1\text{,}\)</span> <span class="process-math">\(\vu_2\text{,}\)</span> <span class="process-math">\(\vu_3\)</span> are vectors in <span class="process-math">\(\R^n\text{,}\)</span> then the Gram-Schmidt process constructs an orthogonal set of vectors <span class="process-math">\(\{\vv_1, \vv_2, \vv_3\}\)</span> with the same span as <span class="process-math">\(\{\vu_1, \vu_2, \vu_3\}\text{.}\)</span></p></article><article class="task exercise-like" id="task-1431"><h5 class="heading">
<span class="codenumber">(d)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-4350">Any set <span class="process-math">\(\{\vu_1, \vu_2, \ldots, \vu_k\}\)</span> of orthogonal vectors in <span class="process-math">\(\R^n\)</span> can be extended to an orthogonal basis of <span class="process-math">\(V\text{.}\)</span></p></article><article class="task exercise-like" id="task-1432"><h5 class="heading">
<span class="codenumber">(e)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-4351">If <span class="process-math">\(A\)</span> is an <span class="process-math">\(n \times n\)</span> matrix with <span class="process-math">\(AA^{\tr} = I_n\text{,}\)</span> then the rows of <span class="process-math">\(A\)</span> form an orthogonal set.</p></article><article class="task exercise-like" id="task-1433"><h5 class="heading">
<span class="codenumber">(f)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-4353">Every nontrivial subspace of <span class="process-math">\(\R^n\)</span> has an orthogonal basis.</p></article><article class="task exercise-like" id="task-1434"><h5 class="heading">
<span class="codenumber">(g)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-4354">If <span class="process-math">\(W\)</span> is a subspace of <span class="process-math">\(\R^n\)</span> satisfying <span class="process-math">\(W^\perp=\{\vzero\}\text{,}\)</span> then <span class="process-math">\(W=\R^n\text{.}\)</span></p></article></article></section><section class="section" id="sec_project_mimo"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">Project: MIMO Systems and Householder Transformations</span>
</h3>
<section class="introduction" id="introduction-417"><p id="p-4356">In a simplified model, a MIMO system will have <span class="process-math">\(k\)</span> transmitting antennas and <span class="process-math">\(m\)</span> receiving antennas. We record a transmitted symbol in a vector <span class="process-math">\(\vx = [x_1 \ x_2 \ \ldots \ x_k]^{\tr}\)</span> (one <span class="process-math">\(x_i\)</span> for each transmitting antenna) and the received symbol as a vector <span class="process-math">\(\vy = [y_1 \ y_2 \ \ldots \ y_m]^{\tr}\)</span> (one <span class="process-math">\(y_j\)</span> for each received symbol).</p>
<p id="p-4357">Between each transmit antenna and each receiver antenna is a fading channel. The MIMO system is the collection of fading channels. If we let <span class="process-math">\(h_{ij}\)</span> be the fading between the <span class="process-math">\(j\)</span>th transmitter and <span class="process-math">\(i\)</span>th receiver, then we can represent this multipath fading as an <span class="process-math">\(m \times k\)</span> matrix <span class="process-math">\(H = [h_{ij}]\text{.}\)</span> We assume that there is also some noise in the received signal that we represent by <span class="process-math">\(n_j\)</span> (for the <span class="process-math">\(j\)</span>th receiving antenna). Our MIMO system is then represented as the matrix-vector equation</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\vy = H\vx + \vn\text{,}
\end{equation*}
</div>
<p class="continuation">where <span class="process-math">\(\vn = [n_1 \ n_2 \ \ldots \ n_m]^{\tr}\text{.}\)</span> The goal is to reproduce the original signal <span class="process-math">\(\vx\)</span> from the received signal <span class="process-math">\(\vy\text{.}\)</span> This is where the QR decomposition comes into play.</p>
<article class="project project-like" id="act_QR_example_1"><h4 class="heading">
<span class="type">Project Activity</span><span class="space"> </span><span class="codenumber">25.8</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-418"><p id="p-4358">To see why and how the QR decomposition is used in MIMO systems, we begin with an example. Assume that we have two transmitters and three receivers, and suppose <span class="process-math">\(H = \left[ \begin{array}{cc} 2\amp 1\\2\amp 1\\1\amp 5 \end{array}  \right]\text{.}\)</span> From this point, to simplify the situation we assume that noise is negligible and consider the system <span class="process-math">\(\vy = H\vx\text{.}\)</span> Assume that the received signal is <span class="process-math">\(\vy = \left[ \begin{array}{c} 2\\1\\4 \end{array}  \right]\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1435"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-4359">Show that the system <span class="process-math">\(H \vx = \vy\)</span> is inconsistent.</p></article><article class="task exercise-like" id="task-1436"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-4360">When we receive a signal, we need to interpret it, and that is difficult if we cannot find a solution. One reason that the system might not have a solution is that the elements in <span class="process-math">\(\vx\)</span> have to come from a specified alphabet, and so we are looking for solutions that are in that alphabet, and there may be no direct solution in that alphabet. As a result, we generally have to find a “best” solution in the alphabet space. That can be done with the QR decomposition. Assume that <span class="process-math">\(H\)</span> has a QR decomposition <span class="process-math">\(H = QR\)</span> with <span class="process-math">\(Q\)</span> having orthonormal columns and <span class="process-math">\(R\)</span> a diagonal matrix. Explain how the equation <span class="process-math">\(\vy = H\vx\)</span> can be transformed into</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="eq_QR_system_example">
\begin{equation}
Q^{\tr} \vy = R\vx\text{.}\tag{25.2}
\end{equation}
</div></article><article class="task exercise-like" id="task-1437"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-4361">Return to our example of <span class="process-math">\(H = \left[ \begin{array}{cc} 2\amp 1\\2\amp 1\\1\amp 5 \end{array}  \right]\text{.}\)</span> Assume that</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_QR_system_example.html">
\begin{equation*}
H = QR =  \left[  \begin{array}{cr} \frac{2}{3}\amp  -\frac{\sqrt{2}}{6} \\ \frac{2}{3}\amp  -\frac{\sqrt{2}}{6} \\ \frac{1}{3}\amp  \frac{2\sqrt{2}}{3} \end{array}  \right] \left[ \begin{array}{cc} 3\amp 3\\0\amp 3\sqrt{2} \end{array}  \right]\text{.}
\end{equation*}
</div>
<p class="continuation">Use <a href="" class="xref" data-knowl="./knowl/eq_QR_system_example.html" title="Equation 25.2">(25.2)</a> to find <span class="process-math">\(\vx\)</span> if <span class="process-math">\(\vy = \left[ \begin{array}{c} 2\\1\\4 \end{array}  \right]\text{.}\)</span></p></article></article><p id="p-4362">In <a href="" class="xref" data-knowl="./knowl/act_QR_example_1.html" title="Project Activity 25.8">Project Activity 25.8</a> we saw that a QR decomposition allows us to replace the system <span class="process-math">\(\vy = H\vx\)</span> with an upper triangular system <span class="process-math">\(Q^{\tr}\vy = R\vx\)</span> that has a solution. This solution is what is called a least squares solution (which we will discuss in more detail in a later section). The key to all of this is finding an efficient way to calculate a QR decomposition. This is the focus of the remainder of this project.</p></section><section class="subsection" id="subsection-3"><h4 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber"></span> <span class="title">Householder Transformations and the QR Decomposition</span>
</h4>
<p id="p-4363">There are three main ways to compute a QR decomposition.</p>
<ul class="disc">
<li id="li-694"><p id="p-4364">The method discussed in this section uses Gram-Schmidt orthogonalization. This method is not recommended for numerical use as it is inherently numerically unstable. Under certain circumstances, cancellation can destroy the accuracy of the computed vectors and the resulting vectors may not be orthogonal.</p></li>
<li id="li-695"><p id="p-4365">The method that we will discuss in this project using Householder transformations (developed by Alston S. Householder). In Gaussian elimination, we can zero out entries below the diagonal by multiplying by elementary matrices to perform row operations. Householder transformations do something similar, allowing us to replace columns with columns of mostly zeros, which then allow for more efficient computations.</p></li>
<li id="li-696"><p id="p-4366">Givens (or Jacobi) rotations (used by W. Givens and originally invented by Jacobi for use with in solving the symmetric eigenvalue problem) is another method that allows us to selectively produce zeros into a matrix. We will focus on Householder transformations in this project.</p></li>
</ul>
<p id="p-4367">The first step in a QR decomposition using Householder matrices is to create a matrix that is upper triangular. To get to this form, we use a <dfn class="terminology">Householder transformation</dfn>. A Householder transformation (or matrix) is a matrix of the form</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
H = I - 2 \frac{\vv \vv^{\tr}}{||\vv||^2}\text{,}
\end{equation*}
</div>
<p class="continuation">where <span class="process-math">\(\vv\)</span> is a vector in <span class="process-math">\(\R^n\)</span> and <span class="process-math">\(I\)</span> is the <span class="process-math">\(n \times n\)</span> identity matrix.</p>
<article class="project project-like" id="project-80"><h5 class="heading">
<span class="type">Project Activity</span><span class="space"> </span><span class="codenumber">25.9</span><span class="period">.</span>
</h5>
<div class="introduction" id="introduction-419"><p id="p-4368">We will discover some properties of Householder transformations in this activity.</p></div>
<article class="task exercise-like" id="task-1438"><h6 class="heading"><span class="codenumber">(a)</span></h6>
<p id="p-4369">Let <span class="process-math">\(\vx\)</span> be any vector and let <span class="process-math">\(\vy\)</span> be a unit vector. In this part of the exercise we show that, with a suitable choice of <span class="process-math">\(\vv\text{,}\)</span> the Householder transformation <span class="process-math">\(H\)</span> transforms <span class="process-math">\(\vx\)</span> into a vector of the same length parallel to <span class="process-math">\(\vy\text{.}\)</span> That is,</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
H \vx = -\sigma \vy
\end{equation*}
</div>
<p class="continuation">for some scalar <span class="process-math">\(\sigma\text{.}\)</span> A similar argument shows that <span class="process-math">\(H (\vx - \sigma \vy) = \sigma \vy\text{.}\)</span> Letting <span class="process-math">\(\vy = \ve_1\)</span> gives us the following result. <article class="lemma theorem-like" id="lem_Householder"><h6 class="heading">
<span class="type">Lemma</span><span class="space"> </span><span class="codenumber">25.9</span><span class="period">.</span>
</h6>
<p id="p-4370">Let <span class="process-math">\(\vx\)</span> be any vector, let <span class="process-math">\(\sigma = ||\vx||\text{,}\)</span> and let <span class="process-math">\(\vv = \vx \pm \sigma \ve_1\text{.}\)</span> Then <span class="process-math">\(H \vx = \mp \sigma \ve_1\text{.}\)</span></p></article></p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-48" id="hint-48"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-48"><div class="hint solution-like"><p id="p-4371">Let <span class="process-math">\(\sigma = ||\vx||\text{,}\)</span> and let <span class="process-math">\(\vv = \vx + \sigma \vy\text{.}\)</span> Apply <span class="process-math">\(H\)</span> to <span class="process-math">\(\vx\text{,}\)</span> factor out <span class="process-math">\(\vx + \sigma \vy\text{,}\)</span> and ultimately show that <span class="process-math">\(H \vx = -\sigma \vy\text{.}\)</span></p></div></div>
</div></article><article class="task exercise-like" id="task-1439"><h6 class="heading"><span class="codenumber">(b)</span></h6>
<p id="p-4372">The Householder matrix <span class="process-math">\(H\)</span> has two other important properties. Show that <span class="process-math">\(H\)</span> is symmetric and orthogonal.</p></article></article><p id="p-4373">There are many advantages to using Householder matrices. One is that instead of having to store all entries of the matrix <span class="process-math">\(H = I - 2 \frac{\vv \vv^{\tr}}{||\vv||^2}\text{,}\)</span> all we need to store is the entries of <span class="process-math">\(\vv\text{.}\)</span> Another advantage of Householder transformations is that they can be used to efficiently calculate QR decompositions. The next project activity shows how to use <a href="" class="xref" data-knowl="./knowl/lem_Householder.html" title="Lemma 25.9">Lemma 25.9</a> and Householder transformations to compute a QR decomposition through an example.</p>
<article class="project project-like" id="act_MIMO_Householder"><h5 class="heading">
<span class="type">Project Activity</span><span class="space"> </span><span class="codenumber">25.10</span><span class="period">.</span>
</h5>
<div class="introduction" id="introduction-420">
<p id="p-4374">Assume that we have five receiving antennas and three transmitting antennas, and that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A = \left[ \begin{array}{crc} 1.0\amp 0.0\amp 1.0 \\ 7.0\amp 7.0\amp 8.0 \\1.0\amp 2.0\amp 1.0 \\ 7.0\amp 7.0\amp 6.0 \\ 0.0\amp -1.0\amp 0.0 \end{array}  \right]\text{.}
\end{equation*}
</div>
<p id="p-4375">(We use <span class="process-math">\(A\)</span> now instead of <span class="process-math">\(H\)</span> so as to avoid confusion with Householder matrices.) The calculations in this activity can get rather messy, so use appropriate technology and round entries to four decimal places to the right of the decimal.</p>
</div>
<article class="task exercise-like" id="task-1440"><h6 class="heading"><span class="codenumber">(a)</span></h6>
<p id="p-4376">Let <span class="process-math">\(\vx_1\)</span> be the first column of <span class="process-math">\(A\text{.}\)</span> Identify an appropriate Householder transformation <span class="process-math">\(H_1\)</span> so that <span class="process-math">\(H_1 \vx\)</span> is a constant multiple of <span class="process-math">\([1 \ 0 \ 0 \ 0 \ 0]^{\tr}\text{.}\)</span> Determine the matrix <span class="process-math">\(A_1\text{,}\)</span> where <span class="process-math">\(A_1 = H_1 A\text{.}\)</span> (Special note: when deciding which of <span class="process-math">\(\vx \pm \sigma \ve_1\)</span> to use to create <span class="process-math">\(H\text{,}\)</span> it is best to use the one whose sign is the same as <span class="process-math">\(x_1\)</span> (the first entry of <span class="process-math">\(\vx\)</span>). We won't go into the details, but this helps prevent problems due to cancellation,)</p></article><article class="task exercise-like" id="task-1441"><h6 class="heading"><span class="codenumber">(b)</span></h6>
<div class="introduction" id="introduction-421"><p id="p-4377">Next we consider just the bottom right <span class="process-math">\(4 \times 2\)</span> portion <span class="process-math">\(\hat{A}_2\)</span> of the matrix <span class="process-math">\(A_1\)</span> found in part (a).</p></div>
<article class="task exercise-like" id="task-1442"><h6 class="heading"><span class="codenumber">(i)</span></h6>
<p id="p-4378">Repeat the process on <span class="process-math">\(\hat{A}_2\)</span> to find a Householder matrix <span class="process-math">\(\hat{H}_2\)</span> that will make the first column of <span class="process-math">\(\hat{A}_2\)</span> have all zeros below its first entry.</p></article><article class="task exercise-like" id="task-1443"><h6 class="heading"><span class="codenumber">(ii)</span></h6>
<p id="p-4379">Let</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
H_2 =  \left[ \begin{array}{cc} I_1\amp 0 \\ 0\amp \hat{H}_2 \end{array}  \right]\text{.}
\end{equation*}
</div>
<p class="continuation">Explain what the matrix <span class="process-math">\(A_2 = H_2H_1A\)</span> is.</p></article></article><article class="task exercise-like" id="task-1444"><h6 class="heading"><span class="codenumber">(c)</span></h6>
<div class="introduction" id="introduction-422"><p id="p-4380">As a final step, we consider just the bottom right <span class="process-math">\(3 \times 1\)</span> portion <span class="process-math">\(\hat{A}_3\)</span> of the matrix <span class="process-math">\(A_2\)</span> and repeat the process.</p></div>
<article class="task exercise-like" id="task-1445"><h6 class="heading"><span class="codenumber">(i)</span></h6>
<p id="p-4381">Find a matrix <span class="process-math">\(\hat{H}_3\)</span> that produces zeros below the first entry.</p></article><article class="task exercise-like" id="task-1446"><h6 class="heading"><span class="codenumber">(ii)</span></h6>
<p id="p-4382">Let</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
H_3 = \left[ \begin{array}{cc}I_2\amp 0 \\ 0\amp \hat{H}_3 \end{array}  \right]\text{.}
\end{equation*}
</div>
<p class="continuation">What matrix is <span class="process-math">\(H_3H_2H_1A\text{?}\)</span> Why?</p></article></article><article class="task exercise-like" id="task-1447"><h6 class="heading"><span class="codenumber">(d)</span></h6>
<p id="p-4383">Explain why <span class="process-math">\(Q = H_1H_2H_3\)</span> is an orthogonal matrix. Then find an upper triangular matrix <span class="process-math">\(R\)</span> such that <span class="process-math">\(A = QR\text{.}\)</span></p></article></article><p id="p-4384"><a href="" class="xref" data-knowl="./knowl/act_MIMO_Householder.html" title="Project Activity 25.10">Project Activity 25.10</a> illustrates the general method for finding a QR decomposition of a matrix <span class="process-math">\(A\)</span> by using Householder transformations to reduce <span class="process-math">\(A\)</span> to an upper triangular matrix <span class="process-math">\(R\text{.}\)</span> The product of the Householder matrices is then the orthogonal matrix <span class="process-math">\(Q\text{.}\)</span> One final note: You may have noticed that the matrix <span class="process-math">\(Q\)</span> you found in this project was a square <span class="process-math">\(5 \times 5\)</span> matrix and <span class="process-math">\(R\)</span> was a <span class="process-math">\(5 \times 3\)</span> matrix, and so they have different sizes than the QR decomposition in this section. We could use the Gram-Schmidt process to obtain these larger matrices by extending the columns of a <span class="process-math">\(5 \times 3\)</span> matrix <span class="process-math">\(Q\)</span> to form an orthonormal basis for <span class="process-math">\(\R^5\text{.}\)</span> However, if we want a QR decomposition for <span class="process-math">\(A\)</span> in which <span class="process-math">\(Q\)</span> is <span class="process-math">\(5 \times 3\)</span> and <span class="process-math">\(R\)</span> is <span class="process-math">\(3 \times 3\)</span> from the work we did in <a href="" class="xref" data-knowl="./knowl/act_MIMO_Householder.html" title="Project Activity 25.10">Project Activity 25.10</a>, we can simply delete the last two columns of the matrix <span class="process-math">\(Q\)</span> and the last two rows of the matrix <span class="process-math">\(R\)</span> we calculated. This smaller QR decomposition is often referred to as a <dfn class="terminology">thin</dfn> QR decomposition.</p></section></section></section><div class="hidden-content tex2jax_ignore" id="hk-fn-40"><div class="fn">Jack Dongarra and Francis Sullivan. Introduction to the top 10 algorithms. Computing in Science and Engineering, 2: 22-23, 2000.</div></div>
<div class="hidden-content tex2jax_ignore" id="hk-fn-41"><div class="fn">[3] Beresford N. Parlett. The QR algorithm. Computing in Science and Engineering, 2:38-42, 2000.</div></div>
<div class="hidden-content tex2jax_ignore" id="hk-fn-42"><div class="fn">Recall that the rank of a matrix <span class="process-math">\(A\)</span> is the dimension of the column space of <span class="process-math">\(A\text{.}\)</span>
</div></div>
</div></main>
</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/components/prism-core.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/autoloader/prism-autoloader.min.js"></script>
</body>
</html>
