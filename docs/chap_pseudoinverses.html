<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*       on 2022-05-25T11:13:40-04:00       *-->
<!--*   A recent stable commit (2020-08-09):   *-->
<!--* 98f21740783f166a773df4dc83cab5293ab63a4a *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US">
<head xmlns:og="http://ogp.me/ns#" xmlns:book="https://ogp.me/ns/book#">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Using the Singular Value Decomposition</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:type" content="book">
<meta property="book:title" content="An Inquiry-Based Introduction to Linear Algebra and Applications">
<meta property="book:author" content="Feryal Alayont">
<meta property="book:author" content="Steven Schlicker">
<script>window.MathJax = {
  tex: {
    inlineMath: [['\\(','\\)']],
    tags: "none",
    tagSide: "right",
    tagIndent: ".8em",
    packages: {'[+]': ['base', 'extpfeil', 'ams', 'amscd', 'newcommand', 'knowl']}
  },
  options: {
    ignoreHtmlClass: "tex2jax_ignore|ignore-math",
    processHtmlClass: "process-math",
  },
  chtml: {
    scale: 0.88,
    mtextInheritFont: true
  },
  loader: {
    load: ['input/asciimath', '[tex]/extpfeil', '[tex]/amscd', '[tex]/newcommand', '[pretext]/mathjaxknowl3.js'],
    paths: {pretext: "https://pretextbook.org/js/lib"},
  },
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/themes/prism.css" rel="stylesheet">
<script async="" src="https://cse.google.com/cse.js?cx=a16e70a6cb1434676"></script><script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.13/pretext.js"></script><script>miniversion=0.674</script><script src="https://pretextbook.org/js/0.13/pretext_add_on.js?x=1"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script>sagecellEvalName='Evaluate (Sage)';
</script><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/banner_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/toc_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/knowls_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/style_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/colors_blue_grey.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/setcolors.css" rel="stylesheet" type="text/css">
<!-- 2019-10-12: Temporary - CSS file for experiments with styling --><link href="developer.css" rel="stylesheet" type="text/css">
</head>
<body class="pretext-book ignore-math has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div id="latex-macros" class="hidden-content process-math" style="display:none"><span class="process-math">\(\usepackage{amsmath}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\E}{\mathbb{E}}
\DeclareMathOperator{\ch}{char}
\newcommand{\N}{\mathbb{N}}
\newcommand{\W}{\mathbb{W}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\J}{\mathbb{J}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathbb{D}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\NE}{\mathcal{E}}
\newcommand{\Mn}[1]{\mathcal{M}_{#1 \times #1}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\Rn}{\R^n}
\newcommand{\Mat}{\mathbf}
\newcommand{\Seq}{\boldsymbol}
\newcommand{\seq}[1]{\boldsymbol{#1}}
\newcommand{\set}[1]{\mathcal{#1}}
\newcommand{\abs}[1]{\left\lvert{}#1\right\rvert}
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\cq}{\scalebox{.34}{\pscirclebox{ \textbf{?}}}}
\newcommand{\cqup}{\,$^{\text{\scalebox{.2}{\pscirclebox{\textbf{?}}}}}$}
\newcommand{\cqupmath}{\,^{\text{\scalebox{.2}{\pscirclebox{\textbf{?}}}}}}
\newcommand{\cqupmathnospace}{^{\text{\scalebox{.2}{\pscirclebox{\textbf{?}}}}}}
\newcommand{\uspace}[1]{\underline{}}
\newcommand{\muspace}[1]{\underline{\mspace{#1 mu}}}
\newcommand{\bspace}[1]{}
\newcommand{\ie}{\emph{i}.\emph{e}.}
\newcommand{\nq}[1]{\scalebox{.34}{\pscirclebox{\textbf{#1}}}}
\newcommand{\equalwhy}{\stackrel{\cqupmath}{=}}
\newcommand{\notequalwhy}{\stackrel{\cqupmath}{\neq}}
\newcommand{\Ker}{\text{Ker}}
\newcommand{\Image}{\text{Im}}
\newcommand{\polyp}{p(x) = a_nx^n + a_{n-1}x^{n-1} + \cdots a_2x^2 + a_1x + a_0}
\newcommand{\GL}{\text{GL}}
\newcommand{\SL}{\text{SL}}
\newcommand{\lcm}{\text{lcm}}
\newcommand{\Aut}{\text{Aut}}
\newcommand{\Inn}{\text{Inn}}
\newcommand{\Hol}{\text{Hol}}
\newcommand{\cl}{\text{cl}}
\newcommand{\bsq}{\hfill $\blacksquare$}
\newcommand{\NIMdot}{{ $\cdot$ }}
\newcommand{\eG}{e_{\scriptscriptstyle{G}}}
\newcommand{\eGroup}[1]{e_{\scriptscriptstyle{#1}}}
\newcommand{\Gdot}[1]{\cdot_{\scriptscriptstyle{#1}}}
\newcommand{\rbar}{\overline{r}}
\newcommand{\nuG}[1]{\nu_{\scriptscriptstyle{#1}}}
\newcommand{\rightarray}[2]{\left[ \begin{array}{#1} #2 \end{array} \right]}
\newcommand{\polymod}[1]{\mspace{5 mu}(\text{mod} #1)}
\newcommand{\ts}{\mspace{2 mu}}
\newcommand{\ds}{\displaystyle}
\newcommand{\adj}{\text{adj}}
\newcommand{\pol}{\mathbb{P}}
\newcommand{\CS}{\mathcal{S}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\CB}{\mathcal{B}}
\renewcommand{\CD}{\mathcal{D}}
\newcommand{\CP}{\mathcal{P}}
\newcommand{\CQ}{\mathcal{Q}}
\newcommand{\CW}{\mathcal{W}}
\newcommand{\rank}{\text{rank}}
\newcommand{\nullity}{\text{nullity}}
\newcommand{\trace}{\text{trace}}
\newcommand{\Area}{\text{Area}}
\newcommand{\Vol}{\text{Vol}}
\newcommand{\cov}{\text{cov}}
\newcommand{\var}{\text{var}}
\newcommand{\va}{\mathbf{a}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\vc}{\mathbf{c}}
\newcommand{\vd}{\mathbf{d}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\vf}{\mathbf{f}}
\newcommand{\vg}{\mathbf{g}}
\newcommand{\vh}{\mathbf{h}}
\newcommand{\vm}{\mathbf{m}}
\newcommand{\vn}{\mathbf{n}}
\newcommand{\vp}{\mathbf{p}}
\newcommand{\vq}{\mathbf{q}}
\newcommand{\vr}{\mathbf{r}}
\newcommand{\vs}{\mathbf{s}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\vzero}{\mathbf{0}}
\newcommand{\vi}{\mathbf{i}}
\newcommand{\vj}{\mathbf{j}}
\newcommand{\vk}{\mathbf{k}}
\newcommand{\vF}{\mathbf{F}}
\newcommand{\vP}{\mathbf{P}}
\newcommand{\nin}{}
\newcommand{\Dom}{\text{Dom}}
\newcommand{\tr}{\mathsf{T}}
\newcommand{\proj}{\text{proj}}
\newcommand{\comp}{\text{comp}}
\newcommand{\Row}{\text{Row }}
\newcommand{\Col}{\text{Col }}
\newcommand{\Nul}{\text{Nul }}
\newcommand{\Span}{\text{Span}}
\newcommand{\Range}{\text{Range}}
\newcommand{\Domain}{\text{Domain}}
\newcommand{\hthin}{\hlinewd{.1pt}}
\newcommand{\hthick}{\hlinewd{.7pt}}
\newcommand{\pbreaks}{1}
\newcommand{\pbreak}{
}
\newcommand{\lint}{\underline{}
\int}
\newcommand{\uint}{ \underline{}
\int}


\newcommand{\Si}{\text{Si}}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\)</span></div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="book-1.html"><span class="title">An Inquiry-Based Introduction to Linear Algebra and Applications</span></a></h1>
<p class="byline">Feryal Alayont, Steven Schlicker</p>
</div>
<div class="searchwrapper" role="search"><div class="gcse-search"></div></div>
</div></div>
<nav id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3">
<a class="index-button toolbar-item button" href="index-1.html" title="Index">Index</a><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="chap_SVD.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="part-app-orthog.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="part-vec-spaces.html" title="Next">Next</a></span>
</div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="chap_SVD.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="part-app-orthog.html" title="Up">Up</a><a class="next-button button toolbar-item" href="part-vec-spaces.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link frontmatter">
<a href="frontmatter.html" data-scroll="frontmatter" class="internal"><span class="title">Front Matter</span></a><ul><li><a href="fm_preface.html" data-scroll="fm_preface" class="internal">Preface</a></li></ul>
</li>
<li class="link part"><a href="part-systems.html" data-scroll="part-systems" class="internal"><span class="codenumber">I</span> <span class="title">Systems of Linear Equations</span></a></li>
<li class="link">
<a href="chap_intro_linear_systems.html" data-scroll="chap_intro_linear_systems" class="internal"><span class="codenumber">1</span> <span class="title">Introduction to Systems of Linear Equations</span></a><ul>
<li><a href="chap_intro_linear_systems.html#sec_appl_elec_circuits" data-scroll="sec_appl_elec_circuits" class="internal">Application: Electrical Circuits</a></li>
<li><a href="chap_intro_linear_systems.html#sec_intro_le_intro" data-scroll="sec_intro_le_intro" class="internal">Introduction</a></li>
<li><a href="chap_intro_linear_systems.html#sec_notation" data-scroll="sec_notation" class="internal">Notation and Terminology</a></li>
<li><a href="chap_intro_linear_systems.html#sec_solve_systems" data-scroll="sec_solve_systems" class="internal">Solving Systems of Linear Equations</a></li>
<li><a href="chap_intro_linear_systems.html#sec_geom_solu_sets" data-scroll="sec_geom_solu_sets" class="internal">The Geometry of Solution Sets of Linear Systems</a></li>
<li><a href="chap_intro_linear_systems.html#sec_intro_le_exam" data-scroll="sec_intro_le_exam" class="internal">Examples</a></li>
<li><a href="chap_intro_linear_systems.html#sec_intro_le_summ" data-scroll="sec_intro_le_summ" class="internal">Summary</a></li>
<li><a href="chap_intro_linear_systems.html#sec_intro_le_exer" data-scroll="sec_intro_le_exer" class="internal">Exercises</a></li>
<li><a href="chap_intro_linear_systems.html#sec_1_a_circuits" data-scroll="sec_1_a_circuits" class="internal">Project: Modeling an Electrical Circuit and the Wheatstone Bridge Circuit</a></li>
</ul>
</li>
<li class="link">
<a href="chap_matrix_representation.html" data-scroll="chap_matrix_representation" class="internal"><span class="codenumber">2</span> <span class="title">The Matrix Representation of a Linear System</span></a><ul>
<li><a href="chap_matrix_representation.html#sec_appl_area_curve" data-scroll="sec_appl_area_curve" class="internal">Application: Approximating Area Under a Curve</a></li>
<li><a href="chap_matrix_representation.html#sec_mtx_lin_intro" data-scroll="sec_mtx_lin_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_representation.html#sec_simp_mtx_sys" data-scroll="sec_simp_mtx_sys" class="internal">Simplifying Linear Systems Represented in Matrix Form</a></li>
<li><a href="chap_matrix_representation.html#sec_sys_inf_sols" data-scroll="sec_sys_inf_sols" class="internal">Linear Systems with Infinitely Many Solutions</a></li>
<li><a href="chap_matrix_representation.html#sec_sys_no_sols" data-scroll="sec_sys_no_sols" class="internal">Linear Systems with No Solutions</a></li>
<li><a href="chap_matrix_representation.html#sec_mtx_sys_exam" data-scroll="sec_mtx_sys_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_representation.html#sec_mtx_sys_summ" data-scroll="sec_mtx_sys_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_representation.html#sec_mtx_sys_exer" data-scroll="sec_mtx_sys_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_representation.html#sec_1_b_polynomial" data-scroll="sec_1_b_polynomial" class="internal">Project: Polynomial Interpolation to Approximate the Area Under a Curve</a></li>
</ul>
</li>
<li class="link">
<a href="chap_row_echelon_forms.html" data-scroll="chap_row_echelon_forms" class="internal"><span class="codenumber">3</span> <span class="title">Row Echelon Forms</span></a><ul>
<li><a href="chap_row_echelon_forms.html#sec_appl_chem_react" data-scroll="sec_appl_chem_react" class="internal">Application: Balancing Chemical Reactions</a></li>
<li><a href="chap_row_echelon_forms.html#sec_row_ech_intro" data-scroll="sec_row_ech_intro" class="internal">Introduction</a></li>
<li><a href="chap_row_echelon_forms.html#sec_mtx_ech_forms" data-scroll="sec_mtx_ech_forms" class="internal">The Echelon Forms of a Matrix</a></li>
<li><a href="chap_row_echelon_forms.html#sec_num_sols_ls" data-scroll="sec_num_sols_ls" class="internal">Determining the Number of Solutions of a Linear System</a></li>
<li><a href="chap_row_echelon_forms.html#sec_prod_ech_forms" data-scroll="sec_prod_ech_forms" class="internal">Producing the Echelon Forms</a></li>
<li><a href="chap_row_echelon_forms.html#sec_row_ech_exam" data-scroll="sec_row_ech_exam" class="internal">Examples</a></li>
<li><a href="chap_row_echelon_forms.html#sec_row_ech_summ" data-scroll="sec_row_ech_summ" class="internal">Summary</a></li>
<li><a href="chap_row_echelon_forms.html#sec_row_ech_exer" data-scroll="sec_row_ech_exer" class="internal">Exercises</a></li>
<li><a href="chap_row_echelon_forms.html#sec_prof_chem_react" data-scroll="sec_prof_chem_react" class="internal">Project: Modeling a Chemical Reaction</a></li>
</ul>
</li>
<li class="link">
<a href="chap_vector_representation.html" data-scroll="chap_vector_representation" class="internal"><span class="codenumber">4</span> <span class="title">Vector Representation</span></a><ul>
<li><a href="chap_vector_representation.html#sec_appl_knight" data-scroll="sec_appl_knight" class="internal">Application: The Knight's Tour</a></li>
<li><a href="chap_vector_representation.html#sec_vec_rep_intro" data-scroll="sec_vec_rep_intro" class="internal">Introduction</a></li>
<li><a href="chap_vector_representation.html#sec_vec_ops" data-scroll="sec_vec_ops" class="internal">Vectors and Vector Operations</a></li>
<li><a href="chap_vector_representation.html#sec_geom_vec_ops" data-scroll="sec_geom_vec_ops" class="internal">Geometric Representation of Vectors and Vector Operations</a></li>
<li><a href="chap_vector_representation.html#sec_lin_comb_vec" data-scroll="sec_lin_comb_vec" class="internal">Linear Combinations of Vectors</a></li>
<li><a href="chap_vector_representation.html#sec_vec_span" data-scroll="sec_vec_span" class="internal">The Span of a Set of Vectors</a></li>
<li><a href="chap_vector_representation.html#sec_vec_rep_exam" data-scroll="sec_vec_rep_exam" class="internal">Examples</a></li>
<li><a href="chap_vector_representation.html#sec_vec_rep_summ" data-scroll="sec_vec_rep_summ" class="internal">Summary</a></li>
<li><a href="chap_vector_representation.html#exercises-4" data-scroll="exercises-4" class="internal">Exercises</a></li>
<li><a href="chap_vector_representation.html#sec_proj_knight" data-scroll="sec_proj_knight" class="internal">Project: Analyzing Knight Moves</a></li>
</ul>
</li>
<li class="link">
<a href="chap_matrix_vector.html" data-scroll="chap_matrix_vector" class="internal"><span class="codenumber">5</span> <span class="title">The Matrix-Vector Form of a Linear System</span></a><ul>
<li><a href="chap_matrix_vector.html#sec_appl_model_econ" data-scroll="sec_appl_model_econ" class="internal">Application: Modeling an Economy</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form_intro" data-scroll="sec_mv_form_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_prod" data-scroll="sec_mv_prod" class="internal">The Matrix-Vector Product</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form" data-scroll="sec_mv_form" class="internal">The Matrix-Vector Form of a Linear System</a></li>
<li><a href="chap_matrix_vector.html#sec_homog_sys" data-scroll="sec_homog_sys" class="internal">Homogeneous and Nonhomogeneous Systems</a></li>
<li><a href="chap_matrix_vector.html#sec_geom_homog_sys" data-scroll="sec_geom_homog_sys" class="internal">The Geometry of Solutions to the Homogeneous System</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form_exam" data-scroll="sec_mv_form_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form_summ" data-scroll="sec_mv_form_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_vector.html#sec_mv_form_exer" data-scroll="sec_mv_form_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_vector.html#sec_proj_io_models" data-scroll="sec_proj_io_models" class="internal">Project: Input-Output Models</a></li>
</ul>
</li>
<li class="link">
<a href="chap_independence.html" data-scroll="chap_independence" class="internal"><span class="codenumber">6</span> <span class="title">Linear Dependence and Independence</span></a><ul>
<li><a href="chap_independence.html#sec_appl_bezier" data-scroll="sec_appl_bezier" class="internal">Application: Bézier Curves</a></li>
<li><a href="chap_independence.html#sec_indep_intro" data-scroll="sec_indep_intro" class="internal">Introduction</a></li>
<li><a href="chap_independence.html#lin_indep_intro_new" data-scroll="lin_indep_intro_new" class="internal">Linear Independence</a></li>
<li><a href="chap_independence.html#sec_determ_lin_ind" data-scroll="sec_determ_lin_ind" class="internal">Determining Linear Independence</a></li>
<li><a href="chap_independence.html#sec_min_span_set" data-scroll="sec_min_span_set" class="internal">Minimal Spanning Sets</a></li>
<li><a href="chap_independence.html#sec_indep_exam" data-scroll="sec_indep_exam" class="internal">Examples</a></li>
<li><a href="chap_independence.html#sec_indep_summ" data-scroll="sec_indep_summ" class="internal">Summary</a></li>
<li><a href="chap_independence.html#sec_indep_exer" data-scroll="sec_indep_exer" class="internal">Exercises</a></li>
<li><a href="chap_independence.html#sec_proj_bezier" data-scroll="sec_proj_bezier" class="internal">Project: Generating Bézier Curves</a></li>
</ul>
</li>
<li class="link">
<a href="chap_matrix_transformations.html" data-scroll="chap_matrix_transformations" class="internal"><span class="codenumber">7</span> <span class="title">Matrix Transformations</span></a><ul>
<li><a href="chap_matrix_transformations.html#sec_appl_graphics" data-scroll="sec_appl_graphics" class="internal">Application: Computer Graphics</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_intro" data-scroll="sec_mtx_trans_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_prop" data-scroll="sec_mtx_trans_prop" class="internal">Properties of Matrix Transformations</a></li>
<li><a href="chap_matrix_transformations.html#sec_trans_onto_oto" data-scroll="sec_trans_onto_oto" class="internal">Onto and One-to-One Transformations</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_exam" data-scroll="sec_mtx_trans_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_summ" data-scroll="sec_mtx_trans_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_transformations.html#sec_mtx_trans_exer" data-scroll="sec_mtx_trans_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_transformations.html#sec_proj_geom_mtx" data-scroll="sec_proj_geom_mtx" class="internal">Project: The Geometry of Matrix Transformations</a></li>
</ul>
</li>
<li class="link part"><a href="part-matrices.html" data-scroll="part-matrices" class="internal"><span class="codenumber">II</span> <span class="title">Matrices</span></a></li>
<li class="link">
<a href="chap_matrix_operations.html" data-scroll="chap_matrix_operations" class="internal"><span class="codenumber">8</span> <span class="title">Matrix Operations</span></a><ul>
<li><a href="chap_matrix_operations.html#sec_appl_mtx_mult" data-scroll="sec_appl_mtx_mult" class="internal">Application: Algorithms for Matrix Multiplication</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_ops_intro" data-scroll="sec_mtx_ops_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_add_smult" data-scroll="sec_mtx_add_smult" class="internal">Properties of Matrix Addition and Multiplication by Scalars</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_prod" data-scroll="sec_mtx_prod" class="internal">A Matrix Product</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_transpose" data-scroll="sec_mtx_transpose" class="internal">The Transpose of a Matrix</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_transpose_prop" data-scroll="sec_mtx_transpose_prop" class="internal">Properties of the Matrix Transpose</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_ops_exam" data-scroll="sec_mtx_ops_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_ops_summ" data-scroll="sec_mtx_ops_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_operations.html#sec_mtx_ops_exer" data-scroll="sec_mtx_ops_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_operations.html#sec_proj_starassen" data-scroll="sec_proj_starassen" class="internal">Project: Strassen's Algorithm and Partitioned Matrices</a></li>
</ul>
</li>
<li class="link">
<a href="chap_intro_eigenvals_eigenvects.html" data-scroll="chap_intro_eigenvals_eigenvects" class="internal"><span class="codenumber">9</span> <span class="title">Introduction to Eigenvalues and Eigenvectors</span></a><ul>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_appl_pagerank" data-scroll="sec_appl_pagerank" class="internal">Application: The Google PageRank Algorithm</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigen_intro" data-scroll="sec_eigen_intro" class="internal">Introduction</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigval_eigvec" data-scroll="sec_eigval_eigvec" class="internal">Eigenvalues and Eigenvectors</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_dynam_sys" data-scroll="sec_dynam_sys" class="internal">Dynamical Systems</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigen_exam" data-scroll="sec_eigen_exam" class="internal">Examples</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigen_summ" data-scroll="sec_eigen_summ" class="internal">Summary</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_eigen_exer" data-scroll="sec_eigen_exer" class="internal">Exercises</a></li>
<li><a href="chap_intro_eigenvals_eigenvects.html#sec_proj_pagerank" data-scroll="sec_proj_pagerank" class="internal">Project: Understanding the PageRank Algorithm</a></li>
</ul>
</li>
<li class="link">
<a href="chap_matrix_inverse.html" data-scroll="chap_matrix_inverse" class="internal"><span class="codenumber">10</span> <span class="title">The Inverse of a Matrix</span></a><ul>
<li><a href="chap_matrix_inverse.html#sec_appl_arms_race" data-scroll="sec_appl_arms_race" class="internal">Application: Modeling an Arms Race</a></li>
<li><a href="chap_matrix_inverse.html#sec_inverse_intro" data-scroll="sec_inverse_intro" class="internal">Introduction</a></li>
<li><a href="chap_matrix_inverse.html#sec_mtx_invertible" data-scroll="sec_mtx_invertible" class="internal">Invertible Matrices</a></li>
<li><a href="chap_matrix_inverse.html#sec_mtx_inverse" data-scroll="sec_mtx_inverse" class="internal">Finding the Inverse of a Matrix</a></li>
<li><a href="chap_matrix_inverse.html#sec_mtx_inverse_props" data-scroll="sec_mtx_inverse_props" class="internal">Properties of the Matrix Inverse</a></li>
<li><a href="chap_matrix_inverse.html#sec_inverse_exam" data-scroll="sec_inverse_exam" class="internal">Examples</a></li>
<li><a href="chap_matrix_inverse.html#sec_inverse_summ" data-scroll="sec_inverse_summ" class="internal">Summary</a></li>
<li><a href="chap_matrix_inverse.html#sec_inverse_exer" data-scroll="sec_inverse_exer" class="internal">Exercises</a></li>
<li><a href="chap_matrix_inverse.html#sec_proj_arms_race" data-scroll="sec_proj_arms_race" class="internal">Project: The Richardson Arms Race Model</a></li>
</ul>
</li>
<li class="link">
<a href="chap_IMT.html" data-scroll="chap_IMT" class="internal"><span class="codenumber">11</span> <span class="title">The Invertible Matrix Theorem</span></a><ul>
<li><a href="chap_IMT.html#sec_imt_intro" data-scroll="sec_imt_intro" class="internal">Introduction</a></li>
<li><a href="chap_IMT.html#sec_imt" data-scroll="sec_imt" class="internal">The Invertible Matrix Theorem</a></li>
<li><a href="chap_IMT.html#sec_imt_exam" data-scroll="sec_imt_exam" class="internal">Examples</a></li>
<li><a href="chap_IMT.html#sec_imt_summ" data-scroll="sec_imt_summ" class="internal">Summary</a></li>
<li><a href="chap_IMT.html#sec_imt_exer" data-scroll="sec_imt_exer" class="internal">Exercises</a></li>
</ul>
</li>
<li class="link part"><a href="part-vector-rn.html" data-scroll="part-vector-rn" class="internal"><span class="codenumber">III</span> <span class="title">The Vector Space <span class="process-math">\(\R^n\)</span></span></a></li>
<li class="link">
<a href="chap_R_n.html" data-scroll="chap_R_n" class="internal"><span class="codenumber">12</span> <span class="title">The Structure of <span class="process-math">\(\R^n\)</span></span></a><ul>
<li><a href="chap_R_n.html#sec_appl_romania" data-scroll="sec_appl_romania" class="internal">Application: Connecting GDP and Consumption in Romania</a></li>
<li><a href="chap_R_n.html#sec_rn_intro" data-scroll="sec_rn_intro" class="internal">Introduction</a></li>
<li><a href="chap_R_n.html#sec_vec_spaces" data-scroll="sec_vec_spaces" class="internal">Vector Spaces</a></li>
<li><a href="chap_R_n.html#sec_sub_space_span" data-scroll="sec_sub_space_span" class="internal">The Subspace Spanned by a Set of Vectors</a></li>
<li><a href="chap_R_n.html#sec_rn_exam" data-scroll="sec_rn_exam" class="internal">Examples</a></li>
<li><a href="chap_R_n.html#sec_rn_summ" data-scroll="sec_rn_summ" class="internal">Summary</a></li>
<li><a href="chap_R_n.html#sec_rn_exer" data-scroll="sec_rn_exer" class="internal">Exercises</a></li>
<li><a href="chap_R_n.html#sec_proj_ls_approx" data-scroll="sec_proj_ls_approx" class="internal">Project: Least Sqaures Linear Approximation</a></li>
</ul>
</li>
<li class="link">
<a href="chap_null_space.html" data-scroll="chap_null_space" class="internal"><span class="codenumber">13</span> <span class="title">The Null Space and Column Space of a Matrix</span></a><ul>
<li><a href="chap_null_space.html#sec_appl_lights_out" data-scroll="sec_appl_lights_out" class="internal">Application: The Lights Out Game</a></li>
<li><a href="chap_null_space.html#sec_null_intro" data-scroll="sec_null_intro" class="internal">Introduction</a></li>
<li><a href="chap_null_space.html#sec_null_kernel" data-scroll="sec_null_kernel" class="internal">The Null Space of a Matrix and the Kernel of a Matrix Transformation</a></li>
<li><a href="chap_null_space.html#sec_column_range" data-scroll="sec_column_range" class="internal">The Column Space of a Matrix and the Range of a Matrix Transformation</a></li>
<li><a href="chap_null_space.html#sec_row_space" data-scroll="sec_row_space" class="internal">The Row Space of a Matrix</a></li>
<li><a href="chap_null_space.html#sec_null_col_base" data-scroll="sec_null_col_base" class="internal">Bases for <span class="process-math">\(\Nul A\)</span> and <span class="process-math">\(\Col A\)</span></a></li>
<li><a href="chap_null_space.html#sec_null_exam" data-scroll="sec_null_exam" class="internal">Examples</a></li>
<li><a href="chap_null_space.html#sec_null_summ" data-scroll="sec_null_summ" class="internal">Summary</a></li>
<li><a href="chap_null_space.html#sec_null_exer" data-scroll="sec_null_exer" class="internal">Exercises</a></li>
<li><a href="chap_null_space.html#sec_proj_lights_out" data-scroll="sec_proj_lights_out" class="internal">Project: Solving the Lights Out Game</a></li>
</ul>
</li>
<li class="link">
<a href="chap_eigenspaces.html" data-scroll="chap_eigenspaces" class="internal"><span class="codenumber">14</span> <span class="title">Eigenspaces of a Matrix</span></a><ul>
<li><a href="chap_eigenspaces.html#sec_appl_pop_dynam" data-scroll="sec_appl_pop_dynam" class="internal">Application: Population Dynamics</a></li>
<li><a href="chap_eigenspaces.html#sec_egspace_intro" data-scroll="sec_egspace_intro" class="internal">Introduction</a></li>
<li><a href="chap_eigenspaces.html#sec_mtx_egspace" data-scroll="sec_mtx_egspace" class="internal">Eigenspaces of Matrix</a></li>
<li><a href="chap_eigenspaces.html#sec_lin_ind_egvec" data-scroll="sec_lin_ind_egvec" class="internal">Linearly Independent Eigenvectors</a></li>
<li><a href="chap_eigenspaces.html#sec_egspace_exam" data-scroll="sec_egspace_exam" class="internal">Examples</a></li>
<li><a href="chap_eigenspaces.html#sec_egspace_summ" data-scroll="sec_egspace_summ" class="internal">Summary</a></li>
<li><a href="chap_eigenspaces.html#sec_egspace_exer" data-scroll="sec_egspace_exer" class="internal">Exercises</a></li>
<li><a href="chap_eigenspaces.html#sec_proj_migration" data-scroll="sec_proj_migration" class="internal">Project: Modeling Population Migration</a></li>
</ul>
</li>
<li class="link">
<a href="chap_bases_dimension.html" data-scroll="chap_bases_dimension" class="internal"><span class="codenumber">15</span> <span class="title">Bases and Dimension</span></a><ul>
<li><a href="chap_bases_dimension.html#sec_appl_latt_crypt" data-scroll="sec_appl_latt_crypt" class="internal">Application: Lattice Based Cryptography</a></li>
<li><a href="chap_bases_dimension.html#sec_base_dim_intro" data-scroll="sec_base_dim_intro" class="internal">Introduction</a></li>
<li><a href="chap_bases_dimension.html#sec_dim_sub_rn" data-scroll="sec_dim_sub_rn" class="internal">The Dimension of a Subspace of <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_bases_dimension.html#sec_cond_basis_subspace" data-scroll="sec_cond_basis_subspace" class="internal">Conditions for a Basis of a Subspace of <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_bases_dimension.html#sec_find_basis_subspace" data-scroll="sec_find_basis_subspace" class="internal">Finding a Basis for a Subspace</a></li>
<li><a href="chap_bases_dimension.html#sec_mtx_rank" data-scroll="sec_mtx_rank" class="internal">Rank of a Matrix</a></li>
<li><a href="chap_bases_dimension.html#sec_base_dim_exam" data-scroll="sec_base_dim_exam" class="internal">Examples</a></li>
<li><a href="chap_bases_dimension.html#sec_base_dim_summ" data-scroll="sec_base_dim_summ" class="internal">Summary</a></li>
<li><a href="chap_bases_dimension.html#sec_base_dim_exer" data-scroll="sec_base_dim_exer" class="internal">Exercises</a></li>
<li><a href="chap_bases_dimension.html#sec_proj_ggh_crypto" data-scroll="sec_proj_ggh_crypto" class="internal">Project: The GGH Cryptosystem</a></li>
</ul>
</li>
<li class="link">
<a href="chap_coordinate_vectors.html" data-scroll="chap_coordinate_vectors" class="internal"><span class="codenumber">16</span> <span class="title">Coordinate Vectors and Change of Basis</span></a><ul>
<li><a href="chap_coordinate_vectors.html#sec_appl_orbits" data-scroll="sec_appl_orbits" class="internal">Application: Describing Orbits of Planets</a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_intro" data-scroll="sec_cob_intro" class="internal">Introduction</a></li>
<li><a href="chap_coordinate_vectors.html#sec_coor_base" data-scroll="sec_coor_base" class="internal">Bases as Coordinate Systems in <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_rn" data-scroll="sec_cob_rn" class="internal">Change of Basis in <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_coordinate_vectors.html#sec_mtx_cob" data-scroll="sec_mtx_cob" class="internal">The Change of Basis Matrix in <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_coordinate_vectors.html#sec_prop_mtx_cob" data-scroll="sec_prop_mtx_cob" class="internal">Properties of the Change of Basis Matrix</a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_exam" data-scroll="sec_cob_exam" class="internal">Examples</a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_summ" data-scroll="sec_cob_summ" class="internal">Summary</a></li>
<li><a href="chap_coordinate_vectors.html#sec_cob_exer" data-scroll="sec_cob_exer" class="internal">Exercises</a></li>
<li><a href="chap_coordinate_vectors.html#sec_proj_orbits_cob" data-scroll="sec_proj_orbits_cob" class="internal">Project: Planetary Orbits and Change of Basis</a></li>
</ul>
</li>
<li class="link part"><a href="part-eigen.html" data-scroll="part-eigen" class="internal"><span class="codenumber">IV</span> <span class="title">Eigenvalues and Eigenvectors</span></a></li>
<li class="link">
<a href="chap_determinants.html" data-scroll="chap_determinants" class="internal"><span class="codenumber">17</span> <span class="title">The Determinant</span></a><ul>
<li><a href="chap_determinants.html#sec_appl_area_vol" data-scroll="sec_appl_area_vol" class="internal">Application: Area and Volume</a></li>
<li><a href="chap_determinants.html#sec_det_intro" data-scroll="sec_det_intro" class="internal">Introduction</a></li>
<li><a href="chap_determinants.html#sec_det_square" data-scroll="sec_det_square" class="internal">The Determinant of a Square Matrix</a></li>
<li><a href="chap_determinants.html#sec_cofactors" data-scroll="sec_cofactors" class="internal">Cofactors</a></li>
<li><a href="chap_determinants.html#sec_det_3by3" data-scroll="sec_det_3by3" class="internal">The Determinant of a <span class="process-math">\(3 \times 3\)</span> Matrix</a></li>
<li><a href="chap_determinants.html#sec_det_remember" data-scroll="sec_det_remember" class="internal">Two Devices for Remembering Determinants</a></li>
<li><a href="chap_determinants.html#sec_det_exam" data-scroll="sec_det_exam" class="internal">Examples</a></li>
<li><a href="chap_determinants.html#sec_det_summ" data-scroll="sec_det_summ" class="internal">Summary</a></li>
<li><a href="chap_determinants.html#sec_det_exer" data-scroll="sec_det_exer" class="internal">Exercises</a></li>
<li><a href="chap_determinants.html#sec_proj_det_area_vol" data-scroll="sec_proj_det_area_vol" class="internal">Project: Area and Volume Using Determinants</a></li>
</ul>
</li>
<li class="link">
<a href="chap_characteristic_equation.html" data-scroll="chap_characteristic_equation" class="internal"><span class="codenumber">18</span> <span class="title">The Characteristic Equation</span></a><ul>
<li><a href="chap_characteristic_equation.html#sec_appl_thermo" data-scroll="sec_appl_thermo" class="internal">Application: Modeling the Second Law of Thermodynamics</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq_intro" data-scroll="sec_chareq_intro" class="internal">Introduction</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq" data-scroll="sec_chareq" class="internal">The Characteristic Equation</a></li>
<li><a href="chap_characteristic_equation.html#sec_egspace_geom" data-scroll="sec_egspace_geom" class="internal">Eigenspaces, A Geometric Example</a></li>
<li><a href="chap_characteristic_equation.html#sec_egspace_dims" data-scroll="sec_egspace_dims" class="internal">Dimensions of Eigenspaces</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq_exam" data-scroll="sec_chareq_exam" class="internal">Examples</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq_summ" data-scroll="sec_chareq_summ" class="internal">Summary</a></li>
<li><a href="chap_characteristic_equation.html#sec_chareq_exer" data-scroll="sec_chareq_exer" class="internal">Exercises</a></li>
<li><a href="chap_characteristic_equation.html#sec_proj_ehrenfest" data-scroll="sec_proj_ehrenfest" class="internal">Project: The Ehrenfest Model</a></li>
</ul>
</li>
<li class="link">
<a href="chap_diagonalization.html" data-scroll="chap_diagonalization" class="internal"><span class="codenumber">19</span> <span class="title">Diagonalization</span></a><ul>
<li><a href="chap_diagonalization.html#sec_appl_fib_num" data-scroll="sec_appl_fib_num" class="internal">Application: The Fibonacci Numbers</a></li>
<li><a href="chap_diagonalization.html#sec_diag_intro" data-scroll="sec_diag_intro" class="internal">Introduction</a></li>
<li><a href="chap_diagonalization.html#sec_diag" data-scroll="sec_diag" class="internal">Diagonalization</a></li>
<li><a href="chap_diagonalization.html#sec_mtx_similar" data-scroll="sec_mtx_similar" class="internal">Similar Matrices</a></li>
<li><a href="chap_diagonalization.html#sec_sim_mtx_trans" data-scroll="sec_sim_mtx_trans" class="internal">Similarity and Matrix Transformations</a></li>
<li><a href="chap_diagonalization.html#sec_diag_general" data-scroll="sec_diag_general" class="internal">Diagonalization in General</a></li>
<li><a href="chap_diagonalization.html#sec_diag_exam" data-scroll="sec_diag_exam" class="internal">Examples</a></li>
<li><a href="chap_diagonalization.html#sec_diag_summ" data-scroll="sec_diag_summ" class="internal">Summary</a></li>
<li><a href="chap_diagonalization.html#sec_diag_exer" data-scroll="sec_diag_exer" class="internal">Exercises</a></li>
<li><a href="chap_diagonalization.html#sec_proj_binet_fibo" data-scroll="sec_proj_binet_fibo" class="internal">Project: Binet's Formula for the Fibonacci Numbers</a></li>
</ul>
</li>
<li class="link">
<a href="chap_approx_eigenvalues.html" data-scroll="chap_approx_eigenvalues" class="internal"><span class="codenumber">20</span> <span class="title">Approximating Eigenvalues and Eigenvectors</span></a><ul>
<li><a href="chap_approx_eigenvalues.html#sec_appl_leslie_mtx" data-scroll="sec_appl_leslie_mtx" class="internal">Application: Leslie Matrices and Population Modeling</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_app_eigen_intro" data-scroll="sec_app_eigen_intro" class="internal">Introduction</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_power_method" data-scroll="sec_power_method" class="internal">The Power Method</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_power_method_inv" data-scroll="sec_power_method_inv" class="internal">The Inverse Power Method</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_app_eigen_exam" data-scroll="sec_app_eigen_exam" class="internal">Examples</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_app_eigen_summ" data-scroll="sec_app_eigen_summ" class="internal">Summary</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_app_eigen_exer" data-scroll="sec_app_eigen_exer" class="internal">Exercises</a></li>
<li><a href="chap_approx_eigenvalues.html#sec_proj_sheep_herd" data-scroll="sec_proj_sheep_herd" class="internal">Project: Managing a Sheep Herd</a></li>
</ul>
</li>
<li class="link">
<a href="chap_complex_eigenvalues.html" data-scroll="chap_complex_eigenvalues" class="internal"><span class="codenumber">21</span> <span class="title">Complex Eigenvalues</span></a><ul>
<li><a href="chap_complex_eigenvalues.html#sec_appl_gershgorin" data-scroll="sec_appl_gershgorin" class="internal">Application: The Gershgorin Disk Theorem</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen_intro" data-scroll="sec_comp_eigen_intro" class="internal">Introduction</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen" data-scroll="sec_comp_eigen" class="internal">Complex Eigenvalues</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_mtx_rotate_scale" data-scroll="sec_mtx_rotate_scale" class="internal">Rotation and Scaling Matrices</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_mtx_comp_eigen" data-scroll="sec_mtx_comp_eigen" class="internal">Matrices with Complex Eigenvalues</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen_exam" data-scroll="sec_comp_eigen_exam" class="internal">Examples</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen_summ" data-scroll="sec_comp_eigen_summ" class="internal">Summary</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_comp_eigen_exer" data-scroll="sec_comp_eigen_exer" class="internal">Exercises</a></li>
<li><a href="chap_complex_eigenvalues.html#sec_proj_gershgorin" data-scroll="sec_proj_gershgorin" class="internal">Project: Understanding the Gershgorin Disk Theorem</a></li>
</ul>
</li>
<li class="link">
<a href="chap_det_properties.html" data-scroll="chap_det_properties" class="internal"><span class="codenumber">22</span> <span class="title">Properties of Determinants</span></a><ul>
<li><a href="chap_det_properties.html#sec_det_prop_intro" data-scroll="sec_det_prop_intro" class="internal">Introduction</a></li>
<li><a href="chap_det_properties.html#sec_det_row_ops" data-scroll="sec_det_row_ops" class="internal">Elementary Row Operations and Their Effects on the Determinant</a></li>
<li><a href="chap_det_properties.html#sec_mtx_elem" data-scroll="sec_mtx_elem" class="internal">Elementary Matrices</a></li>
<li><a href="chap_det_properties.html#sec_det_geom" data-scroll="sec_det_geom" class="internal">Geometric Interpretation of the Determinant</a></li>
<li><a href="chap_det_properties.html#sec_inv_cramers" data-scroll="sec_inv_cramers" class="internal">An Explicit Formula for the Inverse and Cramer's Rule</a></li>
<li><a href="chap_det_properties.html#sec_det_transpose" data-scroll="sec_det_transpose" class="internal">The Determinant of the Transpose</a></li>
<li><a href="chap_det_properties.html#sec_det_row_swap" data-scroll="sec_det_row_swap" class="internal">Row Swaps and Determinants</a></li>
<li><a href="chap_det_properties.html#sec_cofactor_expand" data-scroll="sec_cofactor_expand" class="internal">Cofactor Expansions</a></li>
<li><a href="chap_det_properties.html#sec_mtx_lu_factor" data-scroll="sec_mtx_lu_factor" class="internal">The LU Factorization of a Matrix</a></li>
<li><a href="chap_det_properties.html#sec_det_prop_exam" data-scroll="sec_det_prop_exam" class="internal">Examples</a></li>
<li><a href="chap_det_properties.html#sec_det_prop_summ" data-scroll="sec_det_prop_summ" class="internal">Summary</a></li>
<li><a href="chap_det_properties.html#sec_det_prop_exer" data-scroll="sec_det_prop_exer" class="internal">Exercises</a></li>
</ul>
</li>
<li class="link part"><a href="part-orthog.html" data-scroll="part-orthog" class="internal"><span class="codenumber">V</span> <span class="title">Orthogonality</span></a></li>
<li class="link">
<a href="chap_dot_product.html" data-scroll="chap_dot_product" class="internal"><span class="codenumber">23</span> <span class="title">The Dot Product in <span class="process-math">\(\R^n\)</span></span></a><ul>
<li><a href="chap_dot_product.html#sec_appl_figs_computer" data-scroll="sec_appl_figs_computer" class="internal">Application: Hidden Figures in Computer Graphics</a></li>
<li><a href="chap_dot_product.html#sec_dot_prod_intro" data-scroll="sec_dot_prod_intro" class="internal">Introduction</a></li>
<li><a href="chap_dot_product.html#sec_dist_vec" data-scroll="sec_dist_vec" class="internal">The Distance Between Vectors</a></li>
<li><a href="chap_dot_product.html#sec_angle_vec" data-scroll="sec_angle_vec" class="internal">The Angle Between Two Vectors</a></li>
<li><a href="chap_dot_product.html#sec_orthog_proj" data-scroll="sec_orthog_proj" class="internal">Orthogonal Projections</a></li>
<li><a href="chap_dot_product.html#sec_orthog_comp" data-scroll="sec_orthog_comp" class="internal">Orthogonal Complements</a></li>
<li><a href="chap_dot_product.html#sec_dot_prod_exam" data-scroll="sec_dot_prod_exam" class="internal">Examples</a></li>
<li><a href="chap_dot_product.html#sec_dot_prod_summ" data-scroll="sec_dot_prod_summ" class="internal">Summary</a></li>
<li><a href="chap_dot_product.html#sec_dot_prod_exer" data-scroll="sec_dot_prod_exer" class="internal">Exercises</a></li>
<li><a href="chap_dot_product.html#sec_proj_back_face" data-scroll="sec_proj_back_face" class="internal">Project: Back-Face Culling</a></li>
</ul>
</li>
<li class="link">
<a href="chap_orthogonal_basis.html" data-scroll="chap_orthogonal_basis" class="internal"><span class="codenumber">24</span> <span class="title">Orthogonal and Orthonormal Bases in <span class="process-math">\(\R^n\)</span></span></a><ul>
<li><a href="chap_orthogonal_basis.html#sec_appl_3d_rotate" data-scroll="sec_appl_3d_rotate" class="internal">Application: Rotations in 3D</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_set_intro" data-scroll="sec_orthog_set_intro" class="internal">Introduction</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_sets" data-scroll="sec_orthog_sets" class="internal">Orthogonal Sets</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_bases_prop" data-scroll="sec_orthog_bases_prop" class="internal">Properties of Orthogonal Bases</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthon_bases" data-scroll="sec_orthon_bases" class="internal">Orthonormal Bases</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_mtx" data-scroll="sec_orthog_mtx" class="internal">Orthogonal Matrices</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_set_exam" data-scroll="sec_orthog_set_exam" class="internal">Examples</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_set_summ" data-scroll="sec_orthog_set_summ" class="internal">Summary</a></li>
<li><a href="chap_orthogonal_basis.html#sec_orthog_set_exer" data-scroll="sec_orthog_set_exer" class="internal">Exercises</a></li>
<li><a href="chap_orthogonal_basis.html#sec_proj_3d_rotate" data-scroll="sec_proj_3d_rotate" class="internal">Project: Understanding Rotations in 3-Space</a></li>
</ul>
</li>
<li class="link">
<a href="chap_gram_schmidt.html" data-scroll="chap_gram_schmidt" class="internal"><span class="codenumber">25</span> <span class="title">Projections onto Subspaces and the Gram-Schmidt Process in <span class="process-math">\(\R^n\)</span></span></a><ul>
<li><a href="chap_gram_schmidt.html#sec_mimo" data-scroll="sec_mimo" class="internal">Application: MIMO Systems</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_intro_noip" data-scroll="sec_gram_schmidt_intro_noip" class="internal">Introduction</a></li>
<li><a href="chap_gram_schmidt.html#sec_proj_subsp_orth" data-scroll="sec_proj_subsp_orth" class="internal">Projections onto Subspaces and Orthogonal Projections</a></li>
<li><a href="chap_gram_schmidt.html#sec_best_approx" data-scroll="sec_best_approx" class="internal">Best Approximations</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_process" data-scroll="sec_gram_schmidt_process" class="internal">The Gram-Schmidt Process</a></li>
<li><a href="chap_gram_schmidt.html#sec_qr_fact" data-scroll="sec_qr_fact" class="internal">The QR Factorization of a Matrix</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_examples" data-scroll="sec_gram_schmidt_examples" class="internal">Examples</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_summ_noips" data-scroll="sec_gram_schmidt_summ_noips" class="internal">Summary</a></li>
<li><a href="chap_gram_schmidt.html#sec_gram_schmidt_exercises" data-scroll="sec_gram_schmidt_exercises" class="internal">Exercises</a></li>
<li><a href="chap_gram_schmidt.html#sec_project_mimo" data-scroll="sec_project_mimo" class="internal">Project: MIMO Systems and Householder Transformations</a></li>
</ul>
</li>
<li class="link">
<a href="chap_least_squares.html" data-scroll="chap_least_squares" class="internal"><span class="codenumber">26</span> <span class="title">Least Squares Approximations</span></a><ul>
<li><a href="chap_least_squares.html#sec_appl_fit_func" data-scroll="sec_appl_fit_func" class="internal">Application: Fitting Functions to Data</a></li>
<li><a href="chap_least_squares.html#sec_ls_intro" data-scroll="sec_ls_intro" class="internal">Introduction</a></li>
<li><a href="chap_least_squares.html#sec_ls_approx" data-scroll="sec_ls_approx" class="internal">Least Squares Approximations</a></li>
<li><a href="chap_least_squares.html#sec_ls_exam" data-scroll="sec_ls_exam" class="internal">Examples</a></li>
<li><a href="chap_least_squares.html#sec_ls_summ" data-scroll="sec_ls_summ" class="internal">Summary</a></li>
<li><a href="chap_least_squares.html#sec_ls_exer" data-scroll="sec_ls_exer" class="internal">Exercises</a></li>
<li><a href="chap_least_squares.html#sec_proj_ls_approx_other" data-scroll="sec_proj_ls_approx_other" class="internal">Project: Other Least Squares Approximations</a></li>
</ul>
</li>
<li class="link part"><a href="part-app-orthog.html" data-scroll="part-app-orthog" class="internal"><span class="codenumber">VI</span> <span class="title">Applications of Orthogonality</span></a></li>
<li class="link">
<a href="chap_orthogonal_diagonalization.html" data-scroll="chap_orthogonal_diagonalization" class="internal"><span class="codenumber">27</span> <span class="title">Orthogonal Diagonalization</span></a><ul>
<li><a href="chap_orthogonal_diagonalization.html#sec_appl_mulit_2nd_deriv" data-scroll="sec_appl_mulit_2nd_deriv" class="internal">Application: The Multivariable Second Derivative Test</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_orthog_diag_intro" data-scroll="sec_orthog_diag_intro" class="internal">Introduction</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_mtx_symm" data-scroll="sec_mtx_symm" class="internal">Symmetric Matrices</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_spec_decomp_symm_mtx" data-scroll="sec_spec_decomp_symm_mtx" class="internal">The Spectral Decomposition of a Symmetric Matrix <span class="process-math">\(A\)</span></a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_orthog_diag_exam" data-scroll="sec_orthog_diag_exam" class="internal">Examples</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_orthog_diag_summ" data-scroll="sec_orthog_diag_summ" class="internal">Summary</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_orthog_diag_exer" data-scroll="sec_orthog_diag_exer" class="internal">Exercises</a></li>
<li><a href="chap_orthogonal_diagonalization.html#sec_proj_two_var_deriv" data-scroll="sec_proj_two_var_deriv" class="internal">Project: The Second Derivative Test for Functions of Two Variables</a></li>
</ul>
</li>
<li class="link">
<a href="chap_principal_axis_theorem.html" data-scroll="chap_principal_axis_theorem" class="internal"><span class="codenumber">28</span> <span class="title">Quadratic Forms and the Principal Axis Theorem</span></a><ul>
<li><a href="chap_principal_axis_theorem.html#sec_appl_tennis" data-scroll="sec_appl_tennis" class="internal">Application: The Tennis Racket Effect</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_intro" data-scroll="sec_pat_intro" class="internal">Introduction</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_eqs_quad_r2" data-scroll="sec_eqs_quad_r2" class="internal">Equations Involving Quadratic Forms in <span class="process-math">\(\R^2\)</span></a></li>
<li><a href="chap_principal_axis_theorem.html#sec_class_quad_forms" data-scroll="sec_class_quad_forms" class="internal">Classifying Quadratic Forms</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_inner_prod" data-scroll="sec_pat_inner_prod" class="internal">Inner Products</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_exam" data-scroll="sec_pat_exam" class="internal">Examples</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_summ" data-scroll="sec_pat_summ" class="internal">Summary</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_pat_exer" data-scroll="sec_pat_exer" class="internal">Exercises</a></li>
<li><a href="chap_principal_axis_theorem.html#sec_proj_tennis" data-scroll="sec_proj_tennis" class="internal">Project: The Tennis Racket Theorem</a></li>
</ul>
</li>
<li class="link">
<a href="chap_SVD.html" data-scroll="chap_SVD" class="internal"><span class="codenumber">29</span> <span class="title">The Singular Value Decomposition</span></a><ul>
<li><a href="chap_SVD.html#sec_appl_search_engn" data-scroll="sec_appl_search_engn" class="internal">Application: Search Engines and Semantics</a></li>
<li><a href="chap_SVD.html#sec_svd_intro" data-scroll="sec_svd_intro" class="internal">Introduction</a></li>
<li><a href="chap_SVD.html#sec_mtx_op_norm" data-scroll="sec_mtx_op_norm" class="internal">The Operator Norm of a Matrix</a></li>
<li><a href="chap_SVD.html#sec_svd" data-scroll="sec_svd" class="internal">The SVD</a></li>
<li><a href="chap_SVD.html#sec_svd_mtx_spaces" data-scroll="sec_svd_mtx_spaces" class="internal">SVD and the Null, Column, and Row Spaces of a Matrix</a></li>
<li><a href="chap_SVD.html#sec_svd_exam" data-scroll="sec_svd_exam" class="internal">Examples</a></li>
<li><a href="chap_SVD.html#sec_svd_summ" data-scroll="sec_svd_summ" class="internal">Summary</a></li>
<li><a href="chap_SVD.html#sec_svd_exer" data-scroll="sec_svd_exer" class="internal">Exercises</a></li>
</ul>
</li>
<li class="link active">
<a href="chap_pseudoinverses.html" data-scroll="chap_pseudoinverses" class="internal"><span class="codenumber">30</span> <span class="title">Using the Singular Value Decomposition</span></a><ul>
<li><a href="chap_pseudoinverses.html#sec_appl_gps" data-scroll="sec_appl_gps" class="internal">Application: Global Positioning System</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudo_intro" data-scroll="sec_pseudo_intro" class="internal">Introduction</a></li>
<li><a href="chap_pseudoinverses.html#sec_img_conpress" data-scroll="sec_img_conpress" class="internal">Image Compression</a></li>
<li><a href="chap_pseudoinverses.html#sec_err_approx_img" data-scroll="sec_err_approx_img" class="internal">Calculating the Error in Approximating an Image</a></li>
<li><a href="chap_pseudoinverses.html#sec_mtx_cond_num" data-scroll="sec_mtx_cond_num" class="internal">The Condition Number of a Matrix</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudoinverses" data-scroll="sec_pseudoinverses" class="internal">Pseudoinverses</a></li>
<li><a href="chap_pseudoinverses.html#sec_ls_approx_SVD" data-scroll="sec_ls_approx_SVD" class="internal">Least Squares Approximations</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudo_exam" data-scroll="sec_pseudo_exam" class="internal">Examples</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudo_summ" data-scroll="sec_pseudo_summ" class="internal">Summary</a></li>
<li><a href="chap_pseudoinverses.html#sec_pseudo_exer" data-scroll="sec_pseudo_exer" class="internal">Exercises</a></li>
<li><a href="chap_pseudoinverses.html#sec_proj_gps" data-scroll="sec_proj_gps" class="internal">Project: GPS and Least Squares</a></li>
</ul>
</li>
<li class="link part"><a href="part-vec-spaces.html" data-scroll="part-vec-spaces" class="internal"><span class="codenumber">VII</span> <span class="title">Vector Spaces</span></a></li>
<li class="link">
<a href="chap_vector_spaces.html" data-scroll="chap_vector_spaces" class="internal"><span class="codenumber">31</span> <span class="title">Vector Spaces</span></a><ul>
<li><a href="chap_vector_spaces.html#sec_appl_hat_puzzle" data-scroll="sec_appl_hat_puzzle" class="internal">Application: The Hat Puzzle</a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space_intro" data-scroll="sec_vec_space_intro" class="internal">Introduction</a></li>
<li><a href="chap_vector_spaces.html#sec_space_like_rn" data-scroll="sec_space_like_rn" class="internal">Spaces with Similar Structure to <span class="process-math">\(\R^n\)</span></a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space" data-scroll="sec_vec_space" class="internal">Vector Spaces</a></li>
<li><a href="chap_vector_spaces.html#sec_subspaces" data-scroll="sec_subspaces" class="internal">Subspaces</a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space_exam" data-scroll="sec_vec_space_exam" class="internal">Examples</a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space_summ" data-scroll="sec_vec_space_summ" class="internal">Summary</a></li>
<li><a href="chap_vector_spaces.html#sec_vec_space_exer" data-scroll="sec_vec_space_exer" class="internal">Exercises</a></li>
<li><a href="chap_vector_spaces.html#sec_proj_hamming_hat_puzzle" data-scroll="sec_proj_hamming_hat_puzzle" class="internal">Project: Hamming Codes and the Hat Puzzle</a></li>
</ul>
</li>
<li class="link">
<a href="chap_bases.html" data-scroll="chap_bases" class="internal"><span class="codenumber">32</span> <span class="title">Bases for Vector Spaces</span></a><ul>
<li><a href="chap_bases.html#sec_img_compress" data-scroll="sec_img_compress" class="internal">Application: Image Compression</a></li>
<li><a href="chap_bases.html#sec_bases_intro" data-scroll="sec_bases_intro" class="internal">Introduction</a></li>
<li><a href="chap_bases.html#sec_lin_indep" data-scroll="sec_lin_indep" class="internal">Linear Independence</a></li>
<li><a href="chap_bases.html#sec_bases" data-scroll="sec_bases" class="internal">Bases</a></li>
<li><a href="chap_bases.html#sec_basis_vec_space" data-scroll="sec_basis_vec_space" class="internal">Finding a Basis for a Vector Space</a></li>
<li><a href="chap_bases.html#sec_bases_exam" data-scroll="sec_bases_exam" class="internal">Examples</a></li>
<li><a href="chap_bases.html#sec_bases_summ" data-scroll="sec_bases_summ" class="internal">Summary</a></li>
<li><a href="chap_bases.html#sec_bases_exer" data-scroll="sec_bases_exer" class="internal">Exercises</a></li>
<li><a href="chap_bases.html#sec_proj_img_compress" data-scroll="sec_proj_img_compress" class="internal">Project: Image Compression with Wavelets</a></li>
</ul>
</li>
<li class="link">
<a href="chap_dimension.html" data-scroll="chap_dimension" class="internal"><span class="codenumber">33</span> <span class="title">The Dimension of a Vector Space</span></a><ul>
<li><a href="chap_dimension.html#sec_appl_pca" data-scroll="sec_appl_pca" class="internal">Application: Principal Component Analysis</a></li>
<li><a href="chap_dimension.html#sec_dims_intro" data-scroll="sec_dims_intro" class="internal">Introduction</a></li>
<li><a href="chap_dimension.html#sec_finite_dim_space" data-scroll="sec_finite_dim_space" class="internal">Finite Dimensional Vector Spaces</a></li>
<li><a href="chap_dimension.html#sec_dim_subspace" data-scroll="sec_dim_subspace" class="internal">The Dimension of a Subspace</a></li>
<li><a href="chap_dimension.html#sec_cond_basis_vec_space" data-scroll="sec_cond_basis_vec_space" class="internal">Conditions for a Basis of a Vector Space</a></li>
<li><a href="chap_dimension.html#sec_dims_exam" data-scroll="sec_dims_exam" class="internal">Examples</a></li>
<li><a href="chap_dimension.html#sec_dims_summ" data-scroll="sec_dims_summ" class="internal">Summary</a></li>
<li><a href="chap_dimension.html#sec_dims_exer" data-scroll="sec_dims_exer" class="internal">Exercises</a></li>
<li><a href="chap_dimension.html#sec_proj_pca" data-scroll="sec_proj_pca" class="internal">Project: Understanding Principal Component Analysis</a></li>
</ul>
</li>
<li class="link">
<a href="chap_coordinate_vectors_vector_spaces.html" data-scroll="chap_coordinate_vectors_vector_spaces" class="internal"><span class="codenumber">34</span> <span class="title">Coordinate Vectors and Coordinate Transformations</span></a><ul>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_appl_sums" data-scroll="sec_appl_sums" class="internal">Application: Calculating Sums</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coor_vec_intro" data-scroll="sec_coor_vec_intro" class="internal">Introduction</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coord_trans" data-scroll="sec_coord_trans" class="internal">The Coordinate Transformation</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coord_vec_exam" data-scroll="sec_coord_vec_exam" class="internal">Examples</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coord_vec_summ" data-scroll="sec_coord_vec_summ" class="internal">Summary</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_coord_vec_exer" data-scroll="sec_coord_vec_exer" class="internal">Exercises</a></li>
<li><a href="chap_coordinate_vectors_vector_spaces.html#sec_proj_sum_powers" data-scroll="sec_proj_sum_powers" class="internal">Project: Finding Formulas for Sums of Powers</a></li>
</ul>
</li>
<li class="link">
<a href="chap_inner_products.html" data-scroll="chap_inner_products" class="internal"><span class="codenumber">35</span> <span class="title">Inner Product Spaces</span></a><ul>
<li><a href="chap_inner_products.html#sec_appl_fourier" data-scroll="sec_appl_fourier" class="internal">Application: Fourier Series</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_intro" data-scroll="sec_inner_prod_intro" class="internal">Introduction</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_spaces" data-scroll="sec_inner_prod_spaces" class="internal">Inner Product Spaces</a></li>
<li><a href="chap_inner_products.html#sec_vec_length" data-scroll="sec_vec_length" class="internal">The Length of a Vector</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_orthog" data-scroll="sec_inner_prod_orthog" class="internal">Orthogonality in Inner Product Spaces</a></li>
<li><a href="chap_inner_products.html#sec_inner_prog_orthog_bases" data-scroll="sec_inner_prog_orthog_bases" class="internal">Orthogonal and Orthonormal Bases in Inner Product Spaces</a></li>
<li><a href="chap_inner_products.html#sec_orthog_proj_subspace" data-scroll="sec_orthog_proj_subspace" class="internal">Orthogonal Projections onto Subspaces</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_approx" data-scroll="sec_inner_prod_approx" class="internal">Best Approximations in Inner Product Spaces</a></li>
<li><a href="chap_inner_products.html#sec_orthog_comp_ip" data-scroll="sec_orthog_comp_ip" class="internal">Orthogonal Complements</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_exam" data-scroll="sec_inner_prod_exam" class="internal">Examples</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_summ" data-scroll="sec_inner_prod_summ" class="internal">Summary</a></li>
<li><a href="chap_inner_products.html#sec_inner_prod_exer" data-scroll="sec_inner_prod_exer" class="internal">Exercises</a></li>
<li><a href="chap_inner_products.html#sec_proj_fourier" data-scroll="sec_proj_fourier" class="internal">Project: Fourier Series and Musical Tones</a></li>
</ul>
</li>
<li class="link">
<a href="chap_gram_schmidt_ips.html" data-scroll="chap_gram_schmidt_ips" class="internal"><span class="codenumber">36</span> <span class="title">The Gram-Schmidt Process in Inner Product Spaces</span></a><ul>
<li><a href="chap_gram_schmidt_ips.html#sec_appl_gaussian_quad" data-scroll="sec_appl_gaussian_quad" class="internal">Application: Gaussian Quadrature</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_intro" data-scroll="sec_gram_schmidt_intro" class="internal">Introduction</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_inner_prod" data-scroll="sec_gram_schmidt_inner_prod" class="internal">The Gram-Schmidt Process using Inner Products</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_exam" data-scroll="sec_gram_schmidt_exam" class="internal">Examples</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_summ_ips" data-scroll="sec_gram_schmidt_summ_ips" class="internal">Summary</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_gram_schmidt_exer" data-scroll="sec_gram_schmidt_exer" class="internal">Exercises</a></li>
<li><a href="chap_gram_schmidt_ips.html#sec_proj_gaussian_quad" data-scroll="sec_proj_gaussian_quad" class="internal">Project: Gaussian Quadrature and Legendre Polynomials</a></li>
</ul>
</li>
<li class="link part"><a href="part-lin-trans.html" data-scroll="part-lin-trans" class="internal"><span class="codenumber">VIII</span> <span class="title">Linear Transformations</span></a></li>
<li class="link">
<a href="chap_linear_transformation.html" data-scroll="chap_linear_transformation" class="internal"><span class="codenumber">37</span> <span class="title">Linear Transformations</span></a><ul>
<li><a href="chap_linear_transformation.html#sec_appl_fractals" data-scroll="sec_appl_fractals" class="internal">Application: Fractals</a></li>
<li><a href="chap_linear_transformation.html#sec_lin_trans_intro" data-scroll="sec_lin_trans_intro" class="internal">Introduction</a></li>
<li><a href="chap_linear_transformation.html#sec_onto_oneone" data-scroll="sec_onto_oneone" class="internal">Onto and One-to-One Transformations</a></li>
<li><a href="chap_linear_transformation.html#sec_kernel_range" data-scroll="sec_kernel_range" class="internal">The Kernel and Range of Linear Transformation</a></li>
<li><a href="chap_linear_transformation.html#sec_isomorph" data-scroll="sec_isomorph" class="internal">Isomorphisms</a></li>
<li><a href="chap_linear_transformation.html#sec_lin_trans_exam" data-scroll="sec_lin_trans_exam" class="internal">Examples</a></li>
<li><a href="chap_linear_transformation.html#sec_lin_trans_summ" data-scroll="sec_lin_trans_summ" class="internal">Summary</a></li>
<li><a href="chap_linear_transformation.html#sec_lin_trans_exer" data-scroll="sec_lin_trans_exer" class="internal">Exercises</a></li>
<li><a href="chap_linear_transformation.html#sec_proj_fractals" data-scroll="sec_proj_fractals" class="internal">Project: Fractals via Iterated Function Systems</a></li>
</ul>
</li>
<li class="link">
<a href="chap_transformation_matrix.html" data-scroll="chap_transformation_matrix" class="internal"><span class="codenumber">38</span> <span class="title">The Matrix of a Linear Transformation</span></a><ul>
<li><a href="chap_transformation_matrix.html#sec_appl_secret" data-scroll="sec_appl_secret" class="internal">Application: Secret Sharing Algorithms</a></li>
<li><a href="chap_transformation_matrix.html#sec_mtxof_trans_intro" data-scroll="sec_mtxof_trans_intro" class="internal">Introduction</a></li>
<li><a href="chap_transformation_matrix.html#sec_trans_rn_rm" data-scroll="sec_trans_rn_rm" class="internal">Linear Transformations from <span class="process-math">\(\R^n\)</span> to <span class="process-math">\(\R^m\)</span></a></li>
<li><a href="chap_transformation_matrix.html#sec_mtx_lin_trans" data-scroll="sec_mtx_lin_trans" class="internal">The Matrix of a Linear Transformation</a></li>
<li><a href="chap_transformation_matrix.html#sec_ker_mtx" data-scroll="sec_ker_mtx" class="internal">A Connection between <span class="process-math">\(\Ker(T)\)</span> and a Matrix Representation of <span class="process-math">\(T\)</span></a></li>
<li><a href="chap_transformation_matrix.html#sec_mtxof_trans_exam" data-scroll="sec_mtxof_trans_exam" class="internal">Examples</a></li>
<li><a href="chap_transformation_matrix.html#sec_mtxof_trans_summ" data-scroll="sec_mtxof_trans_summ" class="internal">Summary</a></li>
<li><a href="chap_transformation_matrix.html#sec_mtxof_trans_exer" data-scroll="sec_mtxof_trans_exer" class="internal">Exercises</a></li>
<li><a href="chap_transformation_matrix.html#sec_proj_secret" data-scroll="sec_proj_secret" class="internal">Project: Shamir's Secret Sharing and Lagrange Polynomials</a></li>
</ul>
</li>
<li class="link">
<a href="chap_transformations_eigenvalues.html" data-scroll="chap_transformations_eigenvalues" class="internal"><span class="codenumber">39</span> <span class="title">Eigenvalues of Linear Transformations</span></a><ul>
<li><a href="chap_transformations_eigenvalues.html#sec_appl_diff_eq" data-scroll="sec_appl_diff_eq" class="internal">Application: Linear Differential Equations</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_eigen_trans_intro" data-scroll="sec_eigen_trans_intro" class="internal">Introduction</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_find_eigen_trans" data-scroll="sec_find_eigen_trans" class="internal">Finding Eigenvalues and Eigenvectors of Linear Transformations</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_diagonal" data-scroll="sec_diagonal" class="internal">Diagonalization</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_eigen_trans_exam" data-scroll="sec_eigen_trans_exam" class="internal">Examples</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_eigen_trans_summ" data-scroll="sec_eigen_trans_summ" class="internal">Summary</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_eigen_trans_exer" data-scroll="sec_eigen_trans_exer" class="internal">Exercises</a></li>
<li><a href="chap_transformations_eigenvalues.html#sec_proj_diff_eq" data-scroll="sec_proj_diff_eq" class="internal">Project: Linear Transformations and Differential Equations</a></li>
</ul>
</li>
<li class="link">
<a href="chap_JCF.html" data-scroll="chap_JCF" class="internal"><span class="codenumber">40</span> <span class="title">The Jordan Canonical Form</span></a><ul>
<li><a href="chap_JCF.html#sec_appl_epidemic" data-scroll="sec_appl_epidemic" class="internal">Application: The Bailey Model of an Epidemic</a></li>
<li><a href="chap_JCF.html#sec_jordan_intro" data-scroll="sec_jordan_intro" class="internal">Introduction</a></li>
<li><a href="chap_JCF.html#sec_eigen_dne" data-scroll="sec_eigen_dne" class="internal">When an Eigenvalue Decomposition Does Not Exist</a></li>
<li><a href="chap_JCF.html#sec_gen_eigen_jordan" data-scroll="sec_gen_eigen_jordan" class="internal">Generalized Eigenvectors and the Jordan Canonical Form</a></li>
<li><a href="chap_JCF.html#sec_mtx_jordan_geom" data-scroll="sec_mtx_jordan_geom" class="internal">Geometry of Matrix Transformations using the Jordan Canonical Form</a></li>
<li><a href="chap_JCF.html#sec_jordan_proof" data-scroll="sec_jordan_proof" class="internal">Proof of the Existence of the Jordan Canonical Form</a></li>
<li><a href="chap_JCF.html#sec_mtx_nilpotent" data-scroll="sec_mtx_nilpotent" class="internal">Nilpotent Matrices and Invariant Subspaces</a></li>
<li><a href="chap_JCF.html#sec_jordan" data-scroll="sec_jordan" class="internal">The Jordan Canonical Form</a></li>
<li><a href="chap_JCF.html#sec_jordan_exam" data-scroll="sec_jordan_exam" class="internal">Examples</a></li>
<li><a href="chap_JCF.html#sec_jordan_summ" data-scroll="sec_jordan_summ" class="internal">Summary</a></li>
<li><a href="chap_JCF.html#sec_jordan_exer" data-scroll="sec_jordan_exer" class="internal">Exercises</a></li>
<li><a href="chap_JCF.html#sec_proj_epidemic" data-scroll="sec_proj_epidemic" class="internal">Project: Modeling an Epidemic</a></li>
</ul>
</li>
<li class="link backmatter"><a href="backmatter-1.html" data-scroll="backmatter-1" class="internal"><span class="title">Back Matter</span></a></li>
<li class="link">
<a href="app_complex_numbers.html" data-scroll="app_complex_numbers" class="internal"><span class="codenumber">A</span> <span class="title">Complex Numbers</span></a><ul>
<li><a href="app_complex_numbers.html#sec_complex_numbers" data-scroll="sec_complex_numbers" class="internal">Complex Numbers</a></li>
<li><a href="app_complex_numbers.html#sec_conj_modulus" data-scroll="sec_conj_modulus" class="internal">Conjugates and Modulus</a></li>
<li><a href="app_complex_numbers.html#sec_complex_vect" data-scroll="sec_complex_vect" class="internal">Complex Vectors</a></li>
</ul>
</li>
<li class="link"><a href="app_answers.html" data-scroll="app_answers" class="internal"><span class="codenumber">B</span> <span class="title">Answers and Hints for Selected Exercises</span></a></li>
<li class="link"><a href="index-1.html" data-scroll="index-1" class="internal"><span class="title">Index</span></a></li>
</ul></nav><div class="extras"><nav><a class="pretext-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content">
<section class="chapter" id="chap_pseudoinverses"><h2 class="heading">
<span class="type">Chapter</span> <span class="codenumber">30</span> <span class="title">Using the Singular Value Decomposition</span>
</h2>
<section class="introduction" id="introduction-496"><article class="objectives goal-like" id="objectives-30"><h3 class="heading"><span class="type">Focus Questions</span></h3>
<div class="introduction" id="introduction-497"><p id="p-5046">By the end of this section, you should be able to give precise and thorough answers to the questions listed below. You may want to keep these questions in mind to focus your thoughts as you complete the section.</p></div>
<ul class="disc">
<li id="li-767"><p id="p-5047">What is the condition number of a matrix and what does it tell us about the matrix?</p></li>
<li id="li-768"><p id="p-5048">What is the pseudoinverse of a matrix?</p></li>
<li id="li-769"><p id="p-5049">Why are pseudoinverses useful?</p></li>
<li id="li-770"><p id="p-5050">How does the pseudoinverse of a matrix allow us to find least squares solutions to linear systems?</p></li>
</ul></article></section><section class="section" id="sec_appl_gps"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">Application: Global Positioning System</span>
</h3>
<p id="p-5051">You are probably familiar with the Global Positioning System (GPS). The system allows anyone with the appropriate software to accurately determine their location at any time. The applications are almost endless, including getting real-time driving directions while in your car, guiding missiles, and providing distances on golf courses.</p>
<p id="p-5052">The GPS is a worldwide radio-navigation system owned by the US government and operated by the US Air Force. GPS is one of four global navigation satellite systems. At least twenty four GPS satellites orbit the Earth at an altitude of approximately 11,000 nautical miles. The satellites are placed so that at any time at least four of them can be accessed by a GPS receiver. Each satellite carries an atomic clock to relay a time stamp along with its position in space. There are five ground stations to coordinate and ensure that the system is working properly.</p>
<p id="p-5053">The system works by triangulation, but there is also error involved in the measurements that go into determining position. Later in this section we will see how the method of least squares can be used to determine the receiver's position.</p></section><section class="section" id="sec_pseudo_intro"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">Introduction</span>
</h3>
<p id="p-5054">A singular value decomposition has many applications, and in this section we discuss how a singular value decomposition can be used in image compression, to determine how sensitive a matrix can be to rounding errors in the process of row reduction, and to solve least squares problems.</p></section><section class="section" id="sec_img_conpress"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">Image Compression</span>
</h3>
<p id="p-5055">The digital age has brought many new opportunities for the collection, analysis, and dissemination of information. Along with these opportunities come new difficulties as well. All of this digital information must be stored in some way and be retrievable in an efficient manner. A singular value decomposition of digitally stored information can be used to compress the information or clean up corrupted information. In this section we will see how a singular value decomposition can be used in image compression. While a singular value decomposition is normally used with very large matrices, we will restrict ourselves to small examples so that we can more clearly see how a singular value decomposition is applied.</p>
<article class="exploration project-like" id="pa_7_d_1"><h4 class="heading">
<span class="type">Preview Activity</span><span class="space"> </span><span class="codenumber">30.1</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-498">
<p id="p-5056">Let <span class="process-math">\(A = \frac{1}{4}\left[ \begin{array}{ccrr} 67\amp 29\amp -31\amp -73 \\ 29\amp 67\amp -73\amp -31 \\ 31\amp 73\amp -67\amp -29  \\ 73\amp 31\amp -29\amp -67 \end{array}  \right]\text{.}\)</span> A singular value decomposition for <span class="process-math">\(A\)</span> is <span class="process-math">\(U \Sigma V^{\tr}\text{,}\)</span> where</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-200">
\begin{align*}
U \amp = [\vu_1 \ \vu_2 \ \vu_3 \ \vu_4] = \frac{1}{2} \left[ \begin{array}{crrr} 1\amp -1\amp 1\amp -1\\
1\amp 1\amp 1\amp 1\\
1\amp 1\amp -1\amp -1\\
1\amp -1\amp -1\amp 1 \end{array} \right],\\
\Sigma \amp =  \left[ \begin{array}{cccc} 50\amp 0\amp 0\amp 0\\
0\amp 20\amp 0\amp 0\\
0\amp 0\amp 2\amp 0\\
0\amp 0\amp 0\amp 1 \end{array} \right],\\
V \amp = [\vv_1 \ \vv_2 \ \vv_3 \ \vv_4] = \frac{1}{2} \left[ \begin{array}{rrrr} 1\amp 1\amp -1\amp 1\\
1\amp -1\amp -1\amp -1\\
-1\amp 1\amp -1\amp -1\\
-1\amp -1\amp -1\amp 1 \end{array} \right]\text{.}
\end{align*}
</div>
</div>
<article class="task exercise-like" id="task-1691"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5057">Write the summands in the corresponding outer product decomposition of <span class="process-math">\(A\text{.}\)</span></p></article><article class="task exercise-like" id="task-1692"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<div class="introduction" id="introduction-499">
<p id="p-5058">The outer product decomposition of <span class="process-math">\(A\)</span> writes <span class="process-math">\(A\)</span> as a sum of rank 1 matrices (the summands <span class="process-math">\(\sigma_i \vu_i \vv_i^{\tr})\text{.}\)</span> Each summand contains some information about the matrix <span class="process-math">\(A\text{.}\)</span> Since <span class="process-math">\(\sigma_1\)</span> is the largest of the singular values, it is reasonable to expect that the summand <span class="process-math">\(A_1 = \sigma_1 \vu_1  \vv_1^{\tr}\)</span> contains the most information about <span class="process-math">\(A\)</span> among all of the summands. To get a measure of how much information <span class="process-math">\(A_1\)</span> contains of <span class="process-math">\(A\text{,}\)</span> we can think of <span class="process-math">\(A\)</span> as simply a long vector in <span class="process-math">\(\R^{mn}\)</span> where we have folded the data into a rectangular array (we will see later why taking the norm as the norm of the vector in <span class="process-math">\(\R^{nm}\)</span> makes sense, but for now, just use this definition). If we are interested in determining the error in approximating an image by a compressed image, it makes sense to use the standard norm in <span class="process-math">\(\R^{mn}\)</span> to determine length and distance, which is really just the Frobenius norm that comes from the Frobenius inner product defined by</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_7_d_Frobenius_ip.html" id="eq_7_d_Frobenius_ip">
\begin{equation}
\langle U,V \rangle = \sum u_{ij}v_{ij}\text{,}\tag{30.1}
\end{equation}
</div>
<p class="continuation">where <span class="process-math">\(U = [u_{ij}]\)</span> and <span class="process-math">\(V = [v_{ij}]\)</span> are <span class="process-math">\(m \times n\)</span> matrices. (That <a href="" class="xref" data-knowl="./knowl/eq_7_d_Frobenius_ip.html" title="Equation 30.1">(30.1)</a> defines an inner product on the set of all <span class="process-math">\(n \times n\)</span> matrices is left to discuss in a later section.) So in this section all the norms for matrices will refer to the Frobenius norm. Rather than computing the distance between <span class="process-math">\(A_1\)</span> and <span class="process-math">\(A\)</span> to measure the error, we are more interested in the relative error</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_7_d_Frobenius_ip.html">
\begin{equation*}
\frac{||A-A_1||}{||A||}\text{.}
\end{equation*}
</div>
</div>
<article class="task exercise-like" id="task-1693"><h6 class="heading"><span class="codenumber">(i)</span></h6>
<p id="p-5059">Calculate the relative error in approximating <span class="process-math">\(A\)</span> by <span class="process-math">\(A_1\text{.}\)</span> What does this tell us about how much information <span class="process-math">\(A_1\)</span> contains about <span class="process-math">\(A\text{?}\)</span></p></article><article class="task exercise-like" id="task-1694"><h6 class="heading"><span class="codenumber">(ii)</span></h6>
<p id="p-5060">Let <span class="process-math">\(A_2 = \sum_{k=1}^2 \sigma_k \vu_k \vv_k^{\tr}\text{.}\)</span> Calculate the relative error in approximating <span class="process-math">\(A\)</span> by <span class="process-math">\(A_2\text{.}\)</span> What does this tell us about how much information <span class="process-math">\(A_2\)</span> contains about <span class="process-math">\(A\text{?}\)</span></p></article><article class="task exercise-like" id="task-1695"><h6 class="heading"><span class="codenumber">(iii)</span></h6>
<p id="p-5061">Let <span class="process-math">\(A_3 = \sum_{k=1}^3 \sigma_k \vu_k \vv_k^{\tr}\text{.}\)</span> Calculate the relative error in approximating <span class="process-math">\(A\)</span> by <span class="process-math">\(A_3\text{.}\)</span> What does this tell us about how much information <span class="process-math">\(A_3\)</span> contains about <span class="process-math">\(A\text{?}\)</span></p></article><article class="task exercise-like" id="task-1696"><h6 class="heading"><span class="codenumber">(iv)</span></h6>
<p id="p-5062">Let <span class="process-math">\(A_4 = \sum_{k=1}^4 \sigma_k \vu_k \vv_k^{\tr}\text{.}\)</span> Calculate the relative error in approximating <span class="process-math">\(A\)</span> by <span class="process-math">\(A_4\text{.}\)</span> What does this tell us about how much information <span class="process-math">\(A_4\)</span> contains about <span class="process-math">\(A\text{?}\)</span> Why?</p></article></article></article><p id="p-5063">The first step in compressing an image is to digitize the image. There are many ways to do this and we will consider one of the simplest ways and only work with gray-scale images, with the scale from 0 (black) to 255 (white). A digital image can be created by taking a small grid of squares (called pixels) and coloring each pixel with some shade of gray. The resolution of this grid is a measure of how many pixels are used per square inch. As an example, consider the 16 by 16 pixel picture of a flower shown in <a href="" class="xref" data-knowl="./knowl/Fig_7_c_Flower.html" title="Figure 30.1">Figure 30.1</a>.</p>
<figure class="figure figure-like" id="Fig_7_c_Flower"><div class="image-box" style="width: 50%; margin-left: 25%; margin-right: 25%;"><img src="external/7_c_Flower.svg" role="img" class="contained"></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">30.1<span class="period">.</span></span><span class="space"> </span>A 16 by 16 pixel image</figcaption></figure><p id="p-5064">To store this image pixel by pixel would require <span class="process-math">\(16 \times 16 = 256\)</span> units of storage space (1 for each pixel). If we let <span class="process-math">\(M\)</span> be the matrix whose <span class="process-math">\(i,j\)</span>th entry is the scale of the <span class="process-math">\(i,j\)</span>th pixel, then <span class="process-math">\(M\)</span> is the matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\tiny{ \left[ \begin{array}{cccccccccccccccc} 240\amp 240\amp 240\amp 240\amp 130\amp 130\amp 240\amp 130\amp 130\amp 240\amp 240\amp 240\amp 240\amp 240\amp 240\amp 240\\ 240\amp 240\amp 240\amp 130\amp 175\amp 175\amp 130\amp 175\amp 175\amp 130\amp 240\amp 240\amp 240\amp 240\amp 240\amp 240 \\ 240\amp 240\amp 130\amp 130\amp 175\amp 175\amp 130\amp 175\amp 175\amp 130\amp 130\amp 240\amp 240\amp 240\amp 240\amp 240\\ 240\amp 130\amp 175\amp 175\amp 130\amp 175\amp 175\amp 175\amp 130\amp 175\amp 175\amp 130\amp 240\amp 240\amp 240\amp 240\\ 240\amp 240\amp 130\amp 175\amp 175\amp 130\amp 175\amp 130\amp 175\amp 175\amp 130\amp 240\amp 240\amp 240\amp 240\amp 240\\ 255\amp 240\amp 240\amp 130\amp 130\amp 175\amp 175\amp 175\amp 130\amp 130\amp 240\amp 240\amp 225\amp 240\amp 240\amp 240 \\ 240\amp 240\amp 130\amp 175\amp 175\amp 130\amp 130\amp 130\amp 175\amp 175\amp 130\amp 240\amp 225\amp 255\amp 240\amp 240\\ 240\amp 240\amp 130\amp 175\amp 130\amp 240\amp 130\amp 240\amp 130\amp 175\amp 130\amp 240\amp 255\amp 255\amp 255\amp 240\\ 240\amp 240\amp 240\amp 130\amp 240\amp 240\amp 75\amp 240\amp 240\amp 130\amp 240\amp 255\amp 255\amp 255\amp 255\amp 255\\ 240 \amp 240\amp 240\amp 240\amp 240\amp 240\amp 75\amp 240\amp 240\amp 240\amp 240\amp 240\amp 240\amp 240\amp 240\amp 240 \\ 240\amp 240\amp 240\amp 75\amp 75\amp 240\amp 75\amp 240\amp 75\amp 75\amp 240\amp 240\amp 240\amp 240\amp 240\amp 240\\ 50\amp 240\amp 240\amp 240\amp 75\amp 240\amp 75\amp 240\amp 75\amp 240\amp 240\amp 240\amp 240\amp 50\amp 240\amp 240\\ 240\amp 75\amp 240\amp 240\amp 240\amp 75\amp 75\amp 75 \amp 240\amp 240\amp 50\amp 240\amp 50\amp 240\amp 240\amp 50\\ 240\amp 240\amp 75\amp 240\amp 240\amp 240\amp 75\amp 240\amp 240\amp 50\amp 240\amp 50\amp 240\amp 240\amp 50\amp 240\\ 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\\ 75\amp 75\amp 75\amp 75 \amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75\amp 75 \end{array}  \right]}\text{.}
\end{equation*}
</div>
<p id="p-5065">Recall that if <span class="process-math">\(U \Sigma V^{\tr}\)</span> is a singular value decomposition for <span class="process-math">\(M\text{,}\)</span> then we can also write <span class="process-math">\(M\)</span> in the form</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_7_c_SVD.html">
\begin{equation*}
M=\sigma_1 \vu_1\vv_1^{\tr} + \sigma_2 \vu_2\vv_2^{\tr} + \sigma_3 \vu_3\vv_3^{\tr} + \cdots + \sigma_{16} \vu_{16}\vv_{16}^{\tr}\text{.}
\end{equation*}
</div>
<p class="continuation">given in <a href="" class="xref" data-knowl="./knowl/eq_7_c_SVD.html" title="Equation 29.2">(29.2)</a>. For this <span class="process-math">\(M\text{,}\)</span> the singular values are approximately</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_7_c_SVD.html" id="eq_7_c_Flower_sing_values">
\begin{equation}
\left[ \begin{array}{c}  3006.770088367795 \\ 439.13109000200205 \\ 382.1756550649652 \\ 312.1181752764884 \\ 254.45105800344953 \\ 203.36470770057494 \\ 152.8696215072527 \\ 101.29084240890717 \\ 63.80803769229468 \\ 39.6189181773536 \\ 17.091891798245463 \\ 12.304589419140656 \\ 4.729898943556077 \\ 2.828719409809012 \\ 6.94442317024232 \times 10^{-15} \\2.19689952047833 \times 10^{-15} \end{array}  \right]\text{.}\tag{30.2}
\end{equation}
</div>
<p id="p-5066">Notice that some of these singular values are very small compared to others. As in <a href="" class="xref" data-knowl="./knowl/pa_7_d_1.html" title="Preview Activity 30.1">Preview Activity 30.1</a>, the terms with the largest singular values contain most of the information about the matrix. Thus, we shouldn't lose much information if we eliminate the small singular values. In this particular example, the last 4 singular values are significantly smaller than the rest. If we let</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/pa_7_d_1.html ./knowl/F_7_c_Compress.html">
\begin{equation*}
M_{12} = \sigma_1 \vu_1\vv_1^{\tr} + \sigma_2 \vu_2\vv_2^{\tr} + \sigma_3 \vu_3\vv_3^{\tr} + \cdots + \sigma_{12} \vu_{12}\vv_{12}^{\tr}\text{,}
\end{equation*}
</div>
<p class="continuation">then we should expect the image determined by <span class="process-math">\(M_{12}\)</span> to be close to the image made by <span class="process-math">\(M\text{.}\)</span> The two images are presented side by side in <a href="" class="xref" data-knowl="./knowl/F_7_c_Compress.html" title="Figure 30.2">Figure 30.2</a>.</p>
<figure class="figure figure-like" id="F_7_c_Compress"><div class="sidebyside"><div class="sbsrow" style="margin-left:0%;margin-right:0%;">
<div class="sbspanel top" style="width:50%;"><img src="external/7_c_Flower.svg" role="img" class="contained"></div>
<div class="sbspanel top" style="width:50%;"><img src="external/7_c_CFlower.svg" role="img" class="contained"></div>
</div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">30.2<span class="period">.</span></span><span class="space"> </span>A 16 by 16 pixel image and a compressed image using a singular value decomposition.</figcaption></figure><p id="p-5067">This small example illustrates the general idea. Suppose we had a satellite image that was <span class="process-math">\(1000 \times 1000\)</span> pixels and we let <span class="process-math">\(M\)</span> represent this image. If we have a singular value decomposition of this image <span class="process-math">\(M\text{,}\)</span> say</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
M = \sigma_1 \vu_1\vv_1^{\tr} + \sigma_2 \vu_2\vv_2^{\tr} + \sigma_3 \vu_3\vv_3^{\tr} + \cdots + \sigma_{r} \vu_{r}\vv_{r}^{\tr}\text{,}
\end{equation*}
</div>
<p class="continuation">if the rank of <span class="process-math">\(M\)</span> is large, it is likely that many of the singular values will be very small. If we only keep <span class="process-math">\(s\)</span> of the singular values, we can approximate <span class="process-math">\(M\)</span> by</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
M_s = \sigma_1 \vu_1\vv_1^{\tr} + \sigma_2 \vu_2\vv_2^{\tr} + \sigma_3 \vu_3\vv_3^{\tr} + \cdots + \sigma_{s} \vu_{s}\vv_{s}^{\tr}
\end{equation*}
</div>
<p class="continuation">and store the image with only the vectors <span class="process-math">\(\sigma_1 \vu_1\text{,}\)</span> <span class="process-math">\(\sigma_2 \vu_2\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\(\sigma_{s}\vu_s\text{,}\)</span> <span class="process-math">\(\vv_1\text{,}\)</span> <span class="process-math">\(\vv_1\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\(\vv_s\text{.}\)</span> For example, if we only need 10 of the singular values of a satellite image (<span class="process-math">\(s = 10\)</span>), then we can store the satellite image with only 20 vectors in <span class="process-math">\(\R^{1000}\)</span> or with <span class="process-math">\(20 \times 1000 = 20,000\)</span> numbers instead of <span class="process-math">\(1000 \times 1000 = 1,000,000\)</span> numbers.</p>
<p id="p-5068">A similar process can be used to denoise data.<a href="" data-knowl="" class="id-ref fn-knowl original" data-refid="hk-fn-49" id="fn-49"><sup> 49 </sup></a></p></section><section class="section" id="sec_err_approx_img"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">Calculating the Error in Approximating an Image</span>
</h3>
<p id="p-5069">In the context where a matrix represents an image, the operator aspect of the matrix is irrelevant — we are only interested in the matrix as a holder of information. In this situation, we think of an <span class="process-math">\(m \times n\)</span> matrix as simply a long vector in <span class="process-math">\(\R^{mn}\)</span> where we have folded the data into a rectangular array. If we are interested in determining the error in approximating an image by a compressed image, it makes sense to use the standard norm in <span class="process-math">\(\R^{mn}\)</span> to determine length and distance. This leads to what is called the <dfn class="terminology">Frobenius</dfn> norm of a matrix. The Frobenius norm <span class="process-math">\(||M||_F\)</span> of an <span class="process-math">\(m \times n\)</span> matrix <span class="process-math">\(M = [m_{ij}]\)</span> is</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
||M||_F = \sqrt{ \sum m_{ij}^2 }\text{.}
\end{equation*}
</div>
<p id="p-5070">There is a natural corresponding inner product on the set of <span class="process-math">\(m \times n\)</span> matrices (called the <dfn class="terminology">Frobenius product</dfn>) defined by</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\langle A,B \rangle = \sum a_{ij}b_{ij}\text{,}
\end{equation*}
</div>
<p class="continuation">where <span class="process-math">\(A = [a_{ij}]\)</span> and <span class="process-math">\(B = [b_{ij}]\)</span> are <span class="process-math">\(m \times n\)</span> matrices. Note that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
||A||_F = \sqrt{\langle A, A\rangle}\text{.}
\end{equation*}
</div>
<p id="p-5071">If an <span class="process-math">\(m \times n\)</span> matrix <span class="process-math">\(M\)</span> of rank <span class="process-math">\(r\)</span> has a singular value decomposition <span class="process-math">\(M = U \Sigma V^{\tr}\text{,}\)</span> we have seen that we can write <span class="process-math">\(M\)</span> as an outer product</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_7_c_outer_product.html" id="eq_7_c_outer_product">
\begin{equation}
M = \sigma_1 \vu_1\vv_1^{\tr} + \sigma_2 \vu_2\vv_2^{\tr} + \sigma_3 \vu_3\vv_3^{\tr} + \cdots + \sigma_{r} \vu_{r}\vv_{r}^{\tr}\text{,}\tag{30.3}
\end{equation}
</div>
<p class="continuation">where the <span class="process-math">\(\vu_i\)</span> are the columns of <span class="process-math">\(U\)</span> and the <span class="process-math">\(\vv_j\)</span> the columns of <span class="process-math">\(V\text{.}\)</span> Each of the products <span class="process-math">\(\vu_i \vv_i^{\tr}\)</span> is an <span class="process-math">\(m \times n\)</span> matrix. Since the columns of <span class="process-math">\(\vu_i \vv_i^{\tr}\)</span> are all scalar multiples of <span class="process-math">\(\vu_i\text{,}\)</span> the matrix <span class="process-math">\(\vu_i \vv_i^{\tr}\)</span> is a rank 1 matrix. So <a href="" class="xref" data-knowl="./knowl/eq_7_c_outer_product.html" title="Equation 30.3">(30.3)</a> expresses <span class="process-math">\(M\)</span> as a sum of rank 1 matrices. Moreover, if we let <span class="process-math">\(\vx\)</span> and <span class="process-math">\(\vw\)</span> be <span class="process-math">\(m \times 1\)</span> vectors and let <span class="process-math">\(\vy\)</span> and <span class="process-math">\(\vz\)</span> be <span class="process-math">\(n \times 1\)</span> vectors with <span class="process-math">\(\vy = [y_1 \ y_2 \ \ldots \ y_n]^{\tr}\)</span> and <span class="process-math">\(\vz = [z_1 \ z_2 \ \ldots \ z_n]^{\tr}\text{,}\)</span> then</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_7_c_outer_product.html" id="md-201">
\begin{align*}
\langle \vx\vy^{\tr}, \vw\vz^{\tr} \rangle \amp = \langle [y_1\vx \ y_2\vx \ \cdots \ y_n\vx], [z_1\vw \ z_2\vw \ \cdots \ z_n\vw] \rangle\\
\amp = \sum (y_i\vx) \cdot (z_i\vw)\\
\amp = \sum (y_iz_i)(\vx \cdot \vw)\\
\amp = (\vx \cdot \vw) \sum (y_iz_i)\\
\amp = (\vx \cdot \vw) (\vy \cdot \vz)\text{.}
\end{align*}
</div>
<p id="p-5072">Using the vectors from the singular value decomposition of <span class="process-math">\(M\)</span> as in <a href="" class="xref" data-knowl="./knowl/eq_7_c_outer_product.html" title="Equation 30.3">(30.3)</a> we see that</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_7_c_outer_product.html">
\begin{equation*}
\langle \vu_i\vv_i^{\tr}, \vu_j\vv_j^{\tr} \rangle = (\vu_i \cdot \vu_j)(\vv_i \cdot \vv_j) = \begin{cases}0, \amp \text{ if }  i\neq j, \\ 1, \amp \text{ if }  i = j. \end{cases}
\end{equation*}
</div>
<p id="p-5073">It follows that</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="eq_7_c_SVD_norm">
\begin{equation}
||M||_F^2 = \sum \sigma_i^2 (\vu_i \cdot \vu_i)(\vv_i \cdot \vv_i) = \sum \sigma_i^2\text{.}\tag{30.4}
\end{equation}
</div>
<article class="activity project-like" id="activity-112"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">30.2</span><span class="period">.</span>
</h4>
<p id="p-5074">Verify <a href="" class="xref" data-knowl="./knowl/eq_7_c_SVD_norm.html" title="Equation 30.4">(30.4)</a> that <span class="process-math">\(||M||_F^2 = \sum \sigma_i^2\text{.}\)</span></p></article><p id="p-5075">When we used the singular value decomposition to approximate the image defined by <span class="process-math">\(M\text{,}\)</span> we replaced <span class="process-math">\(M\)</span> with a matrix of the form</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="eq_7_c_outer_product_k">
\begin{equation}
M_k = \sigma_1 \vu_1\vv_1^{\tr} + \sigma_2 \vu_2\vv_2^{\tr} + \sigma_3 \vu_3\vv_3^{\tr} + \cdots + \sigma_{k} \vu_{k}\vv_{k}^{\tr}\text{.}\tag{30.5}
\end{equation}
</div>
<p id="p-5076">We call <span class="process-math">\(M_k\)</span> the rank <span class="process-math">\(k\)</span> approximation to <span class="process-math">\(M\text{.}\)</span> Notice that the outer product expansion in <a href="" class="xref" data-knowl="./knowl/eq_7_c_outer_product_k.html" title="Equation 30.5">(30.5)</a> is in fact a singular value decomposition for <span class="process-math">\(M_k\text{.}\)</span> The error <span class="process-math">\(E_k\)</span> in approximating <span class="process-math">\(M\)</span> with <span class="process-math">\(M_k\)</span> is</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_7_c_outer_product_k.html" id="eq_7_c_outer_product_error">
\begin{equation}
E_k = M - M_k = \sigma_{k+1} \vu_{k+1}\vv_{k+1}^{\tr} + \sigma_{k+2} \vu_{k+2}\vv_{k+2}^{\tr} + \cdots + \sigma_{r} \vu_{r}\vv_{r}^{\tr}\text{.}\tag{30.6}
\end{equation}
</div>
<p id="p-5077">Once again, notice that <a href="" class="xref" data-knowl="./knowl/eq_7_c_outer_product_error.html" title="Equation 30.6">(30.6)</a> is a singular value decomposition for <span class="process-math">\(E_k\text{.}\)</span> We define the relative error in approximating <span class="process-math">\(M\)</span> with <span class="process-math">\(M_k\)</span> as</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_7_c_outer_product_error.html">
\begin{equation*}
\ds \frac{||E_k||}{||M||}\text{.}
\end{equation*}
</div>
<p id="p-5078">Now <a href="" class="xref" data-knowl="./knowl/eq_7_c_SVD_norm.html" title="Equation 30.4">(30.4)</a> shows that</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_7_c_SVD_norm.html">
\begin{equation*}
\ds \frac{||E_k||}{||M||} = \sqrt{ \frac{\sum_{i=k+1}^r \sigma_i^2}{\sum_{i=1}^r \sigma_i^2} }\text{.}
\end{equation*}
</div>
<p id="p-5079">In applications, we often want to retain a certain degree of accuracy in our approximations and this error term can help us accomplish that.</p>
<p id="p-5080">In our flower example, the singular values of <span class="process-math">\(M\)</span> are given in <a href="" class="xref" data-knowl="./knowl/eq_7_c_Flower_sing_values.html" title="Equation 30.2">(30.2)</a>. The relative error in approximating <span class="process-math">\(M\)</span> with <span class="process-math">\(M_{12}\)</span> is</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_7_c_Flower_sing_values.html">
\begin{equation*}
\sqrt{ \frac{\sum_{i=13}^{16} \sigma_i^2}{\sum_{i=1}^{16} \sigma_i^2} } \approx 0.0018\text{.}
\end{equation*}
</div>
<p id="p-5081">Errors (rounded to 4 decimal places) for approximating <span class="process-math">\(M\)</span> with some of the <span class="process-math">\(M_k\)</span> are shown in <a href="" class="xref" data-knowl="./knowl/T_7_c_Errors.html" title="Table 30.3: Errors in approximating M by M_k">Table 30.3</a></p>
<figure class="table table-like" id="T_7_c_Errors"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">30.3<span class="period">.</span></span><span class="space"> </span>Errors in approximating <span class="process-math">\(M\)</span> by <span class="process-math">\(M_k\)</span></figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="c m b1 r1 l1 t1 lines"><span class="process-math">\(k\)</span></td>
<td class="c m b1 r1 l0 t1 lines">10</td>
<td class="c m b1 r1 l0 t1 lines">9</td>
<td class="c m b1 r1 l0 t1 lines">8</td>
<td class="c m b1 r1 l0 t1 lines">7</td>
<td class="c m b1 r1 l0 t1 lines">6</td>
</tr>
<tr>
<td class="c m b2 r1 l1 t0 lines"><span class="process-math">\(\frac{||E_k||}{||M||}\)</span></td>
<td class="c m b2 r1 l0 t0 lines">0.0070</td>
<td class="c m b2 r1 l0 t0 lines">0.0146</td>
<td class="c m b2 r1 l0 t0 lines">0.0252</td>
<td class="c m b2 r1 l0 t0 lines">0.0413</td>
<td class="c m b2 r1 l0 t0 lines">0.06426</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines"><span class="process-math">\(k\)</span></td>
<td class="c m b1 r1 l0 t0 lines">5</td>
<td class="c m b1 r1 l0 t0 lines">4</td>
<td class="c m b1 r1 l0 t0 lines">3</td>
<td class="c m b1 r1 l0 t0 lines">2</td>
<td class="c m b1 r1 l0 t0 lines">1</td>
</tr>
<tr>
<td class="c m b1 r1 l1 t0 lines"><span class="process-math">\(\frac{||E_k||}{||M||}\)</span></td>
<td class="c m b1 r1 l0 t0 lines">0.0918</td>
<td class="c m b1 r1 l0 t0 lines">0.1231</td>
<td class="c m b1 r1 l0 t0 lines">0.1590</td>
<td class="c m b1 r1 l0 t0 lines">0.201</td>
<td class="c m b1 r1 l0 t0 lines">0.2460</td>
</tr>
</table></div></figure><article class="activity project-like" id="activity-113"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">30.3</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-500"><p id="p-5082">Let <span class="process-math">\(M\)</span> represent the flower image.</p></div>
<article class="task exercise-like" id="task-1697"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5083">Find the relative errors in approximating <span class="process-math">\(M\)</span> by <span class="process-math">\(M_{13}\)</span> and <span class="process-math">\(M_{14}\text{.}\)</span> You may use the fact that <span class="process-math">\(\sqrt{\sum_{i=1}^{16} \sigma_i^2} \approx 3102.0679\text{.}\)</span></p></article><article class="task exercise-like" id="task-1698"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5084">About how much of the information in the image is contained in the rank 1 approximation? Explain.</p></article></article></section><section class="section" id="sec_mtx_cond_num"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">The Condition Number of a Matrix</span>
</h3>
<p id="p-5085">A singular value decomposition for a matrix <span class="process-math">\(A\)</span> can tell us a lot about how difficult it is to accurately solve a system <span class="process-math">\(A \vx = \vb\text{.}\)</span> Solutions to systems of linear equations can be very sensitive to rounding as the next exercise demonstrates.</p>
<article class="activity project-like" id="act_7_c_cond_num"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">30.4</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-501"><p id="p-5086">Find the solution to each of the systems.</p></div>
<article class="task exercise-like" id="task-1699"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5087"><span class="process-math">\(\left[ \begin{array}{cc} 1.0000\amp 1.0000 \\ 1.0000\amp 1.0005 \end{array} \right] \left[ \begin{array}{c} x \\ y \end{array} \right] = \left[ \begin{array}{c} 2.0000 \\ 2.0050 \end{array} \right]\)</span></p></article><article class="task exercise-like" id="task-1700"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5088"><span class="process-math">\(\left[ \begin{array}{cc} 1.000\amp 1.000 \\ 1.000\amp 1.001 \end{array} \right] \left[ \begin{array}{c} x \\ y \end{array} \right] = \left[ \begin{array}{c} 2.000 \\ 2.005 \end{array} \right]\)</span></p></article></article><p id="p-5089">Notice that a simple rounding in the <span class="process-math">\((2,2)\)</span> entry of the coefficient matrix led to a significantly different solution. If there are rounding errors at any stage of the Gaussian elimination process, they can be compounded by further row operations. This is an important problem since computers can only approximate irrational numbers with rational numbers and so rounding can be critical. Finding ways of dealing with these kinds of errors is an area of on-going research in numerical linear algebra. This problem is given a name.</p>
<article class="definition definition-like" id="definition-67"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">30.4</span><span class="period">.</span>
</h4>
<p id="p-5090">A matrix <span class="process-math">\(A\)</span> is <dfn class="terminology">ill-conditioned</dfn> if relatively small changes in any entries of <span class="process-math">\(A\)</span> can produce significant changes in solutions to the system <span class="process-math">\(A\vx = \vb\text{.}\)</span></p></article><p id="p-5091">A matrix that is not ill-conditioned is said to be <dfn class="terminology">well-conditioned</dfn>. Since small changes in entries of ill-conditioned matrices can lead to large errors in computations, it is an important problem in linear algebra to have a way to measure how ill-conditioned a matrix is. This idea will ultimately lead us to the condition number of a matrix.</p>
<p id="p-5092">Suppose we want to solve the system <span class="process-math">\(A \vx = \vb\text{,}\)</span> where <span class="process-math">\(A\)</span> is an invertible matrix. <a href="" class="xref" data-knowl="./knowl/act_7_c_cond_num.html" title="Activity 30.4">Activity 30.4</a> illustrates that if <span class="process-math">\(A\)</span> is really close to being singular, then small changes in the entries of <span class="process-math">\(A\)</span> can have significant effects on the solution to the system. So the system can be very hard to solve accurately if <span class="process-math">\(A\)</span> is close to singular. It is important to have a sense of how “good” we can expect any calculated solution to be. Suppose we think we solve the system <span class="process-math">\(A \vx = \vb\)</span> but, through rounding error in our calculation of <span class="process-math">\(A\text{,}\)</span> get a solution <span class="process-math">\(\vx'\)</span> so that <span class="process-math">\(A \vx' = \vb'\text{,}\)</span> where <span class="process-math">\(\vb'\)</span> is not exactly <span class="process-math">\(\vb\text{.}\)</span> Let <span class="process-math">\(\Delta \vx\)</span> be the error in our calculated solution and <span class="process-math">\(\Delta \vb\)</span> the difference between <span class="process-math">\(\vb'\)</span> and <span class="process-math">\(\vb\text{.}\)</span> We would like to know how large the error <span class="process-math">\(||\Delta \vx||\)</span> can be. But this isn't exactly the right question. We could scale everything to make <span class="process-math">\(||\Delta \vx||\)</span> as large as we want. What we really need is a measure of the <em class="emphasis">relative error</em> <span class="process-math">\(\frac{||\Delta \vx||}{||\vx||}\text{,}\)</span> or how big the error is compared to <span class="process-math">\(||\vx||\)</span> itself. More specifically, we want to know how large the relative error in <span class="process-math">\(\Delta \vx\)</span> is compared to the relative error in <span class="process-math">\(\Delta \vb\text{.}\)</span> In other words, we want to know how good the relative error in <span class="process-math">\(\Delta \vb\)</span> is as a predictor of the relative error in <span class="process-math">\(\Delta \vx\)</span> (we may have some control over the relative error in <span class="process-math">\(\Delta \vb\text{,}\)</span> perhaps by keeping more significant digits). So we want know if there is a best constant <span class="process-math">\(C\)</span> such that</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/act_7_c_cond_num.html">
\begin{equation*}
\frac{||\Delta \vx||}{||\vx||} \leq C \frac{||\Delta \vb||}{||\vb||}\text{.}
\end{equation*}
</div>
<p id="p-5093">This best constant <span class="process-math">\(C\)</span> is the condition number — a measure of how well the relative error in <span class="process-math">\(\Delta \vb\)</span> predicts the relative error in <span class="process-math">\(\Delta \vx\text{.}\)</span> How can we find <span class="process-math">\(C\text{?}\)</span></p>
<p id="p-5094">Since <span class="process-math">\(A \vx' = \vb'\)</span> we have</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A(\vx + \Delta \vx) = \vb + \Delta \vb\text{.}
\end{equation*}
</div>
<p id="p-5095">Distributing on the left and using the fact that <span class="process-math">\(A\vx = \vb\)</span> gives us</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="eq_7_c_cn">
\begin{equation}
A\Delta \vx = \Delta \vb\text{.}\tag{30.7}
\end{equation}
</div>
<p id="p-5096">We return for a moment to the operator norm of a matrix. This is an appropriate norm to use here since we are considering <span class="process-math">\(A\)</span> to be a transformation. Recall that if <span class="process-math">\(A\)</span> is an <span class="process-math">\(m \times n\)</span> matrix, we defined the operator norm of <span class="process-math">\(A\)</span> to be</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
||A|| = \max_{||\vx|| \neq \vzero} \left\{\frac{||A\vx||}{||\vx||} \right\} = \max_{||\vx||=1} \{||A\vx||\}\text{.}
\end{equation*}
</div>
<p class="continuation">One important property that the norm has is that if the product <span class="process-math">\(AB\)</span> is defined, then</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
||AB|| \leq ||A|| \ ||B||\text{.}
\end{equation*}
</div>
<p class="continuation">To see why, notice that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\frac{||AB\vx||}{||\vx||} = \frac{||A(B\vx)||}{||B\vx||} \ \frac{||B\vx||}{||\vx||}\text{.}
\end{equation*}
</div>
<p id="p-5097">Now <span class="process-math">\(\frac{||A(B\vx)||}{||B\vx||} \leq ||A||\)</span> and <span class="process-math">\(\frac{||B\vx||}{||\vx||} \leq ||B||\)</span> by the definition of the norm, so we conclude that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\frac{||AB\vx||}{||\vx||} \leq ||A|| \ ||B||
\end{equation*}
</div>
<p class="continuation">for every <span class="process-math">\(\vx\text{.}\)</span> Thus,</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
||AB|| \leq ||A|| \ ||B||\text{.}
\end{equation*}
</div>
<p id="p-5098">Now we can find the condition number. From <span class="process-math">\(A \Delta \vx = \Delta \vb\)</span> we have</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\Delta \vx = A^{-1} \Delta \vb\text{,}
\end{equation*}
</div>
<p class="continuation">so</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="eq_7_c_cn2">
\begin{equation}
||\Delta \vx|| \leq ||A^{-1}|| \ ||\Delta \vb||\text{.}\tag{30.8}
\end{equation}
</div>
<p id="p-5099">Similarly, <span class="process-math">\(\vb = A\vx\)</span> implies that <span class="process-math">\(||\vb|| \leq ||A|| \ ||\vx||\)</span> or</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="eq_7_c_cn3">
\begin{equation}
\frac{1}{||\vx||} \leq \frac{||A||}{||\vb||}\text{.}\tag{30.9}
\end{equation}
</div>
<p id="p-5100">Combining <a href="" class="xref" data-knowl="./knowl/eq_7_c_cn2.html" title="Equation 30.8">(30.8)</a> and <a href="" class="xref" data-knowl="./knowl/eq_7_c_cn3.html" title="Equation 30.9">(30.9)</a> gives</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_7_c_cn2.html ./knowl/eq_7_c_cn3.html" id="md-202">
\begin{align*}
\frac{||\Delta \vx||}{||\vx||} \amp \leq \frac{||A^{-1}|| \ ||\Delta \vb||}{||\vx||}\\
\amp = ||A^{-1}|| \ ||\Delta \vb|| \left(\frac{1}{||\vx||}\right)\\
\amp \leq ||A^{-1}|| \ ||\Delta \vb||  \frac{||A||}{||\vb||}\\
\amp = ||A^{-1}|| \ ||A|| \ \frac{||\Delta \vb||}{||\vb||}\text{.}
\end{align*}
</div>
<p id="p-5101">This constant <span class="process-math">\(||A^{-1}|| \ ||A||\)</span> is the best bound and so is called the condition number of <span class="process-math">\(A\text{.}\)</span></p>
<article class="definition definition-like" id="definition-68"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">30.5</span><span class="period">.</span>
</h4>
<p id="p-5102">The <dfn class="terminology">condition number</dfn> of an invertible matrix <span class="process-math">\(A\)</span> is the number <span class="process-math">\(||A^{-1}|| \ ||A||\text{.}\)</span></p></article><p id="p-5103">How does a singular value decomposition tell us about the condition number of a matrix? Recall that the maximum value of <span class="process-math">\(||A\vx||\)</span> for <span class="process-math">\(\vx\)</span> on the unit <span class="process-math">\(n\)</span>-sphere is <span class="process-math">\(\sigma_1\text{.}\)</span> So <span class="process-math">\(||A|| = \sigma_1\text{.}\)</span> If <span class="process-math">\(A\)</span> is an invertible matrix and <span class="process-math">\(A = U \Sigma V^{\tr}\)</span> is a singular value decomposition for <span class="process-math">\(A\text{,}\)</span> then</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A^{-1} = (U \Sigma V^{\tr})^{-1} = (V^{\tr})^{-1} \Sigma^{-1} U^{-1} = V \Sigma^{-1} U^{\tr}\text{,}
\end{equation*}
</div>
<p class="continuation">where</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\Sigma^{-1} = \left[ \begin{array}{ccccc} \frac{1}{\sigma_1}\amp \amp \amp \amp 0 \\ \amp  \frac{1}{\sigma_2}\amp \amp \amp  \\ \amp \amp  \frac{1}{\sigma_3}\amp \amp  \\ \amp   \amp  \amp  \ddots \amp   \\ 0\amp \amp \amp \amp  \frac{1}{\sigma_n} \end{array}  \right]\text{.}
\end{equation*}
</div>
<p id="p-5104">Now <span class="process-math">\(V \Sigma^{-1} U^{\tr}\)</span> is a singular value decomposition for <span class="process-math">\(A^{-1}\)</span> with the diagonal entries in reverse order, so</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
||A^{-1}|| = \frac{1}{\sigma_n}\text{.}
\end{equation*}
</div>
<p id="p-5105">Therefore, the condition number of <span class="process-math">\(A\)</span> is</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
||A^{-1}|| \ ||A|| = \frac{\sigma_1}{\sigma_n}\text{.}
\end{equation*}
</div>
<article class="activity project-like" id="activity-115"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">30.5</span><span class="period">.</span>
</h4>
<p id="p-5106">Let <span class="process-math">\(A = \left[ \begin{array}{cc} 1.0000\amp 1.0000 \\ 1.0000\amp 1.0005 \end{array} \right]\text{.}\)</span> A computer algebra system gives the singular values of <span class="process-math">\(A\)</span> as 2.00025003124999934 and 0.000249968750000509660. What is the condition number of <span class="process-math">\(A\text{.}\)</span> What does that tell us about <span class="process-math">\(A\text{?}\)</span> Does this seem reasonable given the result of <a href="" class="xref" data-knowl="./knowl/act_7_c_cond_num.html" title="Activity 30.4">Activity 30.4</a>?</p></article><article class="activity project-like" id="activity-116"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">30.6</span><span class="period">.</span>
</h4>
<article class="task exercise-like" id="task-1701"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5107">What is the smallest the condition number of a matrix can be? Find an entire class of matrices with this smallest condition number.</p></article><article class="task exercise-like" id="task-1702"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5108">What is the condition number of an orthogonal matrix? Why does this make sense?</p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-73" id="hint-73"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-73"><div class="hint solution-like"><p id="p-5109">If <span class="process-math">\(P\)</span> is an orthogonal matrix, what is <span class="process-math">\(||P \vx||\)</span> for any vector <span class="process-math">\(\vx\text{?}\)</span> What does this make <span class="process-math">\(||P||\text{?}\)</span></p></div></div>
</div></article><article class="task exercise-like" id="task-1703"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-5110">What is the condition number of an invertible symmetric matrix in terms of its eigenvalues?</p></article><article class="task exercise-like" id="task-1704"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<p id="p-5111">Why do we not define the condition number of a non-invertible matrix? If we did, what would the condition number have to be? Why?</p></article></article></section><section class="section" id="sec_pseudoinverses"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">Pseudoinverses</span>
</h3>
<p id="p-5112">Not every matrix is invertible, so we cannot always solve a matrix equation <span class="process-math">\(A \vx = \vb\text{.}\)</span> However, every matrix has a pseudoinverse <span class="process-math">\(A^+\)</span> that acts something like an inverse. Even when we can't solve a matrix equation <span class="process-math">\(A \vx = \vb\)</span> because <span class="process-math">\(\vb\)</span> isn't in <span class="process-math">\(\Col A\text{,}\)</span> we can use the pseudoinverse of <span class="process-math">\(A\)</span> to “solve” the equation <span class="process-math">\(A \vx = \vb\)</span> with the “solution” <span class="process-math">\(A^+ \vb\text{.}\)</span> While not an exact solution, <span class="process-math">\(A^+ \vb\)</span> turns out to be the best approximation to a solution in the least squares sense. We will use the singular value decomposition to find the pseudoinverse of a matrix.</p>
<article class="exploration project-like" id="pa_7_d_2"><h4 class="heading">
<span class="type">Preview Activity</span><span class="space"> </span><span class="codenumber">30.7</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-502">
<p id="p-5113">Let <span class="process-math">\(A = \left[\begin{array}{ccc} 1\amp 1\amp 0\\ 0\amp 1\amp 1 \end{array}  \right]\text{.}\)</span> The singular value decomposition of <span class="process-math">\(A\)</span> is <span class="process-math">\(U \Sigma V^{\tr}\)</span> where</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-203">
\begin{align*}
U \amp = \frac{\sqrt{2}}{2} \left[ \begin{array}{cr} 1\amp -1\\
1\amp 1 \end{array} \right],\\
\Sigma \amp = \left[ \begin{array}{ccc} \sqrt{3}\amp 0\amp 0\\
0\amp 1\amp 0 \end{array} \right],\\
V \amp = \frac{1}{6} \left[ \begin{array}{rrr} \sqrt{6}\amp -3\sqrt{2}\amp 2\sqrt{3}\\
2\sqrt{6}\amp 0\amp -2\sqrt{3}\\
\sqrt{6}\amp 3\sqrt{2}\amp 2\sqrt{3} \end{array} \right]\text{.}
\end{align*}
</div>
</div>
<article class="task exercise-like" id="task-1705"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5114">Explain why <span class="process-math">\(A\)</span> is not an invertible matrix.</p></article><article class="task exercise-like" id="task-1706"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5115">Explain why the matrices <span class="process-math">\(U\)</span> and <span class="process-math">\(V\)</span> are invertible. How are <span class="process-math">\(U^{-1}\)</span> and <span class="process-math">\(V^{-1}\)</span> related to <span class="process-math">\(U^{\tr}\)</span> and <span class="process-math">\(V^{\tr}\text{?}\)</span></p></article><article class="task exercise-like" id="task-1707"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-5116">Recall that one property of invertible matrices is that the inverse of a product of invertible matrices is the product of the inverses in the reverse order. If <span class="process-math">\(A\)</span> were invertible, then <span class="process-math">\(A^{-1}\)</span> would be <span class="process-math">\(\left(U \Sigma V^{\tr}\right)^{-1} = V \Sigma^{-1} U^{\tr}\text{.}\)</span> Even though <span class="process-math">\(U\)</span> and <span class="process-math">\(V\)</span> are invertible, the matrix <span class="process-math">\(\Sigma\)</span> is not. But <span class="process-math">\(\Sigma\)</span> does contain non-zero eigenvalues that have reciprocals, so consider the matrix <span class="process-math">\(\Sigma^+ = \left[ \begin{array}{cc} \frac{1}{\sqrt{3}}\amp 0 \\ 0\amp 1 \\ 0\amp 0 \end{array} \right]\text{.}\)</span> Calculate the products <span class="process-math">\(\Sigma \Sigma^+\)</span> and <span class="process-math">\(\Sigma^+ \Sigma\text{.}\)</span> How are the results similar to that obtained with a matrix inverse?</p></article><article class="task exercise-like" id="task-1708"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<div class="introduction" id="introduction-503"><p id="p-5117">The only matrix in the singular value decomposition of <span class="process-math">\(A\)</span> that is not invertible is <span class="process-math">\(\Sigma\text{.}\)</span> But the matrix <span class="process-math">\(\Sigma^{+}\)</span> acts somewhat like an inverse of <span class="process-math">\(\Sigma\text{,}\)</span> so let us define <span class="process-math">\(A^+\)</span> as <span class="process-math">\(V \Sigma^+ U^{\tr}\text{.}\)</span> Now we explore a few properties of the matrix <span class="process-math">\(A^{+}\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1709"><h6 class="heading"><span class="codenumber">(i)</span></h6>
<p id="p-5118">Calculate <span class="process-math">\(AA^+\)</span> and <span class="process-math">\(A^+A\)</span> for <span class="process-math">\(A = \left[\begin{array}{ccc} 1\amp 1\amp 0\\ 0\amp 1\amp 1 \end{array} \right]\text{.}\)</span> What do you notice?</p></article><article class="task exercise-like" id="task-1710"><h6 class="heading"><span class="codenumber">(ii)</span></h6>
<p id="p-5119">Calculate <span class="process-math">\(A^+AA^+\)</span> and <span class="process-math">\(AA^+A\)</span> for <span class="process-math">\(A = \left[\begin{array}{ccc} 1\amp 1\amp 0\\ 0\amp 1\amp 1 \end{array} \right]\text{.}\)</span> What do you notice?</p></article></article></article><p id="p-5120">Only some square matrices have inverses. However, every matrix has a pseudoinverse. A pseudoinverse <span class="process-math">\(A^{+}\)</span> of a matrix <span class="process-math">\(A\)</span> provides something like an inverse when a matrix doesn't have an inverse. Pseudoinverses are useful to approximate solutions to linear systems. If <span class="process-math">\(A\)</span> is invertible, then the equation <span class="process-math">\(A \vx = \vb\)</span> has the solution <span class="process-math">\(\vx = A^{-1}\vb\text{,}\)</span> but when <span class="process-math">\(A\)</span> is not invertible and <span class="process-math">\(\vb\)</span> is not in <span class="process-math">\(\Col A\text{,}\)</span> then the equation <span class="process-math">\(A \vx = \vb\)</span> has no solution. In the invertible case of an <span class="process-math">\(n \times n\)</span> matrix <span class="process-math">\(A\text{,}\)</span> there is a matrix <span class="process-math">\(B\)</span> so that <span class="process-math">\(AB = BA = I_n\text{.}\)</span> This also implies that <span class="process-math">\(BAB = B\)</span> and <span class="process-math">\(ABA = A\text{.}\)</span> To mimic this situation when <span class="process-math">\(A\)</span> is not invertible, we search for a matrix <span class="process-math">\(A^+\)</span> (a pseudoinverse of <span class="process-math">\(A\)</span>) so that <span class="process-math">\(AA^+A = A\)</span> and <span class="process-math">\(A^+AA^+ = A^+\text{,}\)</span> as we saw in <a href="" class="xref" data-knowl="./knowl/pa_7_d_2.html" title="Preview Activity 30.7">Preview Activity 30.7</a>. Then it turns out that <span class="process-math">\(A^+\)</span> acts something like an inverse for <span class="process-math">\(A\text{.}\)</span> In this case, we approximate the solution to <span class="process-math">\(A \vx = \vb\)</span> by <span class="process-math">\(\vx^* = A^+\vb\text{,}\)</span> and we will see that the vector <span class="process-math">\(A\vx^* = AA^+\vb\)</span> turns out to be the vector in <span class="process-math">\(\Col A\)</span> that is closest to <span class="process-math">\(\vb\)</span> in the least squares sense.</p>
<p id="p-5121">A reasonable question to ask is how we can find a pseudoinverse of a matrix <span class="process-math">\(A\text{.}\)</span> A singular value decomposition provides an answer to this question. If <span class="process-math">\(A\)</span> is an invertible <span class="process-math">\(n \times n\)</span> matrix, then 0 is not an eigenvalue of <span class="process-math">\(A\text{.}\)</span> As a result, in the singular value decomposition <span class="process-math">\(U \Sigma V^{\tr}\)</span> of <span class="process-math">\(A\text{,}\)</span> the matrix <span class="process-math">\(\Sigma\)</span> is an invertible matrix (note that <span class="process-math">\(U\text{,}\)</span> <span class="process-math">\(\Sigma\text{,}\)</span> and <span class="process-math">\(V\)</span> are all <span class="process-math">\(n \times n\)</span> matrices in this case). So</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A^{-1} = \left(U \Sigma V^{\tr}\right)^{-1} = V \Sigma^{-1} U^{\tr}\text{,}
\end{equation*}
</div>
<p class="continuation">where</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\Sigma^{-1} = \left[ \begin{array}{ccccc} \frac{1}{\sigma_1}\amp \amp \amp \amp  \\  \amp  \frac{1}{\sigma_2}\amp \amp 0\amp  \\  \amp \amp  \frac{1}{\sigma_3}\amp \amp  \\ \amp 0  \amp  \amp  \ddots \amp   \\ \amp \amp \amp \amp  \frac{1}{\sigma_n} \end{array}  \right]\text{.}
\end{equation*}
</div>
<p id="p-5122">In this case, <span class="process-math">\(V \Sigma^{-1} U^{\tr}\)</span> is a singular value decomposition for <span class="process-math">\(A^{-1}\text{.}\)</span></p>
<p id="p-5123">To understand in general how a pseudoinverse is found, let <span class="process-math">\(A\)</span> be an <span class="process-math">\(m \times n\)</span> matrix with <span class="process-math">\(m \neq n\text{,}\)</span> or an <span class="process-math">\(n \times n\)</span> with rank less than <span class="process-math">\(n\text{.}\)</span> In these cases <span class="process-math">\(A\)</span> does not have an inverse. But as in <a href="" class="xref" data-knowl="./knowl/pa_7_d_2.html" title="Preview Activity 30.7">Preview Activity 30.7</a>, a singular value decomposition provides a pseudoinverse <span class="process-math">\(A^+\)</span> for <span class="process-math">\(A\text{.}\)</span> Let <span class="process-math">\(U \Sigma V^{\tr}\)</span> be a singular value decomposition of an <span class="process-math">\(m \times n\)</span> matrix <span class="process-math">\(A\)</span> of rank <span class="process-math">\(r\text{,}\)</span> with</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/pa_7_d_2.html">
\begin{equation*}
\Sigma = \left[ \begin{array}{ccccc|c} \sigma_1\amp \amp \amp \amp \amp  \\ \amp  \sigma_2\amp \amp 0\amp \amp 0 \\ \amp \amp  \sigma_3\amp \amp \amp  \\ \amp  0 \amp  \amp  \ddots \amp  \amp  \\ \amp \amp \amp \amp  \sigma_r \\ \hline \amp \amp 0\amp \amp \amp 0 \end{array}  \right]
\end{equation*}
</div>
<p id="p-5124"> The matrices <span class="process-math">\(U\)</span> and <span class="process-math">\(V\)</span> are invertible, but the matrix <span class="process-math">\(\Sigma\)</span> is not if <span class="process-math">\(A\)</span> is not invertible. If we let <span class="process-math">\(\Sigma^+\)</span> be the <span class="process-math">\(n \times m\)</span> matrix defined by</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\Sigma^{+} = \left[ \begin{array}{ccccc|c} \frac{1}{\sigma_1}\amp \amp \amp \amp \amp  \\ \amp  \frac{1}{\sigma_2}\amp \amp 0\amp \amp 0 \\ \amp \amp  \frac{1}{\sigma_3}\amp \amp \amp  \\ \amp  0 \amp  \amp  \ddots \amp  \amp  \\ \amp \amp \amp \amp  \frac{1}{\sigma_r} \\ \hline \amp \amp 0\amp \amp \amp 0 \end{array}  \right]\text{,}
\end{equation*}
</div>
<p class="continuation">then <span class="process-math">\(\Sigma^{+}\)</span> will act much like an inverse of <span class="process-math">\(\Sigma\)</span> might. In fact, it is not difficult to see that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\Sigma\Sigma^{+} = \left[ \begin{array}{c|c} I_r\amp 0 \\ \hline 0\amp 0 \end{array}  \right] \text{ and }  \Sigma^{+}\Sigma = \left[ \begin{array}{c|c} I_r\amp 0 \\ \hline 0\amp 0 \end{array}  \right]\text{,}
\end{equation*}
</div>
<p class="continuation">where <span class="process-math">\(\Sigma\Sigma^{+}\)</span> is an <span class="process-math">\(m \times m\)</span> matrix and <span class="process-math">\(\Sigma^{+}\Sigma\)</span> is an <span class="process-math">\(n \times n\)</span> matrix.</p>
<p id="p-5125">The matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="eq_pseudoinverse">
\begin{equation}
A^{+} = V\Sigma^{+}U^{\tr}\tag{30.10}
\end{equation}
</div>
<p class="continuation">is a <dfn class="terminology">pseudoinverse</dfn> of <span class="process-math">\(A\text{.}\)</span></p>
<article class="activity project-like" id="activity-117"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">30.8</span><span class="period">.</span>
</h4>
<article class="task exercise-like" id="task-1711"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5126">Find the pseudoinverse <span class="process-math">\(A^+\)</span> of <span class="process-math">\(A = \left[ \begin{array}{rc} 0\amp 5\\ 4\amp 3 \\ -2\amp 1 \end{array}  \right]\text{.}\)</span> Use the singular value decomposition <span class="process-math">\(U \Sigma V^{\tr}\)</span> of <span class="process-math">\(A\text{,}\)</span> where</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
U =  \left[ \begin{array}{crc} \frac{\sqrt{2}}{2}\amp \frac{\sqrt{3}}{3}\amp 1 \\ \frac{\sqrt{2}}{2}\amp -\frac{\sqrt{3}}{3}\amp 0 \\ 0\amp \frac{\sqrt{3}}{3}\amp 0 \end{array}  \right], \ \Sigma =  \left[ \begin{array}{cc} \sqrt{40}\amp 0 \\ 0\amp \sqrt{15} \\ 0\amp 0 \end{array}  \right], \ V = \frac{1}{\sqrt{5}} \left[ \begin{array}{cr} 1\amp -2 \\ 2\amp 1 \end{array}  \right]\text{.}
\end{equation*}
</div></article><article class="task exercise-like" id="task-1712"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5127">The vector <span class="process-math">\(\vb = \left[ \begin{array}{c} 0\\0\\1 \end{array} \right]\)</span> is not in <span class="process-math">\(\Col A\text{.}\)</span> The vector <span class="process-math">\(\vx^* = A^+ \vb\)</span> is an approximation to a solution of <span class="process-math">\(A \vx = \vb\text{,}\)</span> and <span class="process-math">\(AA^+\vb\)</span> is in <span class="process-math">\(\Col A\text{.}\)</span> Find <span class="process-math">\(A\vx^*\)</span> and determine how far <span class="process-math">\(A\vx^*\)</span> is from <span class="process-math">\(\vb\text{.}\)</span></p></article></article><p id="p-5128">Pseudoinverses satisfy several properties that are similar to those of inverses. For example, we had an example in <a href="" class="xref" data-knowl="./knowl/pa_7_d_2.html" title="Preview Activity 30.7">Preview Activity 30.7</a> where <span class="process-math">\(AA^{+}A = A\)</span> and <span class="process-math">\(A^+AA^+ = A^+\text{.}\)</span> That <span class="process-math">\(A^+\)</span> always satisfies these properties is the subject of the next activity.</p>
<article class="activity project-like" id="act_7_d_Moore-Penrose"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">30.9</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-504"><p id="p-5129">Let <span class="process-math">\(A\)</span> be an <span class="process-math">\(m \times n\)</span> matrix with singular value decomposition <span class="process-math">\(U \Sigma V^{\tr}\text{.}\)</span> Let <span class="process-math">\(A^{+}\)</span> be defined as in <a href="" class="xref" data-knowl="./knowl/eq_pseudoinverse.html" title="Equation 30.10">(30.10)</a>.</p></div>
<article class="task exercise-like" id="task-1713"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5130">Show that <span class="process-math">\(AA^{+}A = A\text{.}\)</span></p></article><article class="task exercise-like" id="task-1714"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5131">Show that <span class="process-math">\(A^{+}AA^{+} = A^{+}\text{.}\)</span></p></article></article><p id="p-5132"><a href="" class="xref" data-knowl="./knowl/act_7_d_Moore-Penrose.html" title="Activity 30.9">Activity 30.9</a> shows that <span class="process-math">\(A^{+}\)</span> satisfies properties that are similar to those of an inverse of <span class="process-math">\(A\text{.}\)</span> In fact, <span class="process-math">\(A^{+}\)</span> satisfies several other properties (that together can be used as defining properties) as stated in the next theorem. The conditions of <a href="" class="xref" data-knowl="./knowl/thm_7_d_pseudoinverse.html" title="Theorem 30.6: The Moore-Penrose Conditions">Theorem 30.6</a> are called the <dfn class="terminology">Penrose</dfn> or <dfn class="terminology">Moore-Penrose</dfn> conditions.<a href="" data-knowl="" class="id-ref fn-knowl original" data-refid="hk-fn-50" id="fn-50"><sup> 50 </sup></a> Verification of the remaining parts of this theorem are left for the exercises.</p>
<article class="theorem theorem-like" id="thm_7_d_pseudoinverse"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">30.6</span><span class="period">.</span><span class="space"> </span><span class="title">The Moore-Penrose Conditions.</span>
</h4>
<p id="p-5133">A pseudoinverse of a matrix <span class="process-math">\(A\)</span> is a matrix <span class="process-math">\(A^+\)</span> that satisfies the following properties.</p>
<ol class="decimal">
<li id="li-771"><p id="p-5134"><span class="process-math">\(\displaystyle AA^{+}A = A\)</span></p></li>
<li id="li-772"><p id="p-5135"><span class="process-math">\(\displaystyle A^{+}AA^{+} = A^{+}\)</span></p></li>
<li id="li-773"><p id="p-5136"><span class="process-math">\(\displaystyle (AA^{+})^{\tr} = AA^{+}\)</span></p></li>
<li id="li-774"><p id="p-5137"><span class="process-math">\(\displaystyle (A^{+}A)^{\tr} = A^{+}A\)</span></p></li>
</ol></article><p id="p-5138">Also, there is a unique matrix <span class="process-math">\(A^+\)</span> that satisfies these properties. The verification of this property is left to the exercises.</p></section><section class="section" id="sec_ls_approx_SVD"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">Least Squares Approximations</span>
</h3>
<p id="p-5139">The pseudoinverse of a matrix is also connected to least squares solutions of linear systems as we encountered in <a href="chap_orthogonal_basis.html" class="internal" title="Chapter 24: Orthogonal and Orthonormal Bases in \R^n">Chapter 24</a>. Recall from <a href="chap_orthogonal_basis.html" class="internal" title="Chapter 24: Orthogonal and Orthonormal Bases in \R^n">Chapter 24</a>, that if the columns of <span class="process-math">\(A\)</span> are linearly independent, then the least squares solution to <span class="process-math">\(A\vx = \vb\)</span> is <span class="process-math">\(\vx = \left(A^{\tr}A\right)^{-1}A^{\tr} \vb\text{.}\)</span> In this section we will see how to use a pseudoinverse to solve a least squares problem, and verify that if the columns of <span class="process-math">\(A\)</span> are linearly dependent, then <span class="process-math">\(\left(A^{\tr}A\right)^{-1}A^{\tr}\)</span> is in fact the pseudoinverse of <span class="process-math">\(A\text{.}\)</span></p>
<p id="p-5140">Let <span class="process-math">\(U \Sigma V^{\tr}\)</span> be a singular value decomposition for an <span class="process-math">\(m \times n\)</span> matrix <span class="process-math">\(A\)</span> of rank <span class="process-math">\(r\text{.}\)</span> Then the columns of</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/chap_gram_schmidt.html">
\begin{equation*}
U = [\vu_1 \ \vu_2  \ \cdots  \ \vu_m]
\end{equation*}
</div>
<p class="continuation">form an orthonormal basis for <span class="process-math">\(\R^m\)</span> and <span class="process-math">\(\{\vu_1, \vu_2, \ldots, \vu_r\}\)</span> is a basis for <span class="process-math">\(\Col A\text{.}\)</span> Remember from <a href="chap_gram_schmidt.html" class="internal" title="Chapter 25: Projections onto Subspaces and the Gram-Schmidt Process in \R^n">Chapter 25</a> that if <span class="process-math">\(\vb\)</span> is any vector in <span class="process-math">\(\R^m\text{,}\)</span> then</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/chap_gram_schmidt.html">
\begin{equation*}
\proj_{\Col A} \vb = (\vb \cdot \vu_1) \vu_1 + (\vb \cdot \vu_2) \vu_2 + \cdots + (\vb \cdot \vu_r) \vu_r
\end{equation*}
</div>
<p class="continuation">is the least squares approximation of the vector <span class="process-math">\(\vb\)</span> by a vector in <span class="process-math">\(\Col A\text{.}\)</span> We can extend this sum to all of columns of <span class="process-math">\(U\)</span> as</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/chap_gram_schmidt.html">
\begin{equation*}
\proj_{\Col A} \vb = (\vb \cdot \vu_1) \vu_1 + (\vb \cdot \vu_2) \vu_2 + \cdots + (\vb \cdot \vu_r) \vu_r + 0 \vu_{r+1} + 0 \vu_{r+2} + \cdots + 0 \vu_m\text{.}
\end{equation*}
</div>
<p id="p-5141">It follows that</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-204">
\begin{align*}
\proj_{\Col A} \vb \amp = \sum_{i=1}^r \vu_i (\vu_i \cdot \vb)\\
\amp = \sum_{i=1}^r \vu_i (\vu_i^{\tr}\vb)\\
\amp = \sum_{i=1}^r (\vu_i\vu_i^{\tr}) \vb\\
\amp = \left(\sum_{i=1}^r (1)(\vu_i\vu_i^{\tr})\right)\vb + \left(\sum_{i=r+1}^m 0(\vu_i\vu_i^{\tr})\right) \vb\\
\amp = (UDU^{\tr}) \vb\text{,}
\end{align*}
</div>
<p class="continuation">where</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
D = \left[ \begin{array}{c|c} I_r \amp  \vzero \\ \hline \vzero \amp  \vzero \end{array}  \right]\text{.}
\end{equation*}
</div>
<p id="p-5142">Now, if <span class="process-math">\(\vz = A^{+} \vb\text{,}\)</span> then</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A\vz = (U\Sigma V^{\tr})(V \Sigma^+ U^{\tr} \vb) = (U \Sigma \Sigma^+ U^{\tr})\vb = (UDU^{\tr})\vb = \proj_{\Col A} \vb\text{,}
\end{equation*}
</div>
<p class="continuation">and hence the vector <span class="process-math">\(A \vz = AA^+ \vb\)</span> is the vector <span class="process-math">\(A\vx\)</span> in <span class="process-math">\(\Col A\)</span> that minimizes <span class="process-math">\(||A \vx - \vb||\text{.}\)</span> Thus, <span class="process-math">\(A\vz\)</span> is in actuality the least squares approximation to <span class="process-math">\(\vb\text{.}\)</span> So a singular value decomposition allows us to construct the pseudoinverse of a matrix <span class="process-math">\(A\)</span> and then directly solve the least squares problem.</p>
<p id="p-5143">If the columns of <span class="process-math">\(A\)</span> are linearly independent, then we do not need to use an SVD to find the pseudoinverse, as the next activity illustrates.</p>
<article class="activity project-like" id="act_7_b_lin_indep_cols"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">30.10</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-505">
<p id="p-5144">Having to calculate eigenvalues and eigenvectors for a matrix to produce a singular value decomposition to find pseudoinverse can be computationally intense. As we demonstrate in this activity, the process is easier if the columns of <span class="process-math">\(A\)</span> are linearly independent. More specifically, we will prove the following theorem.</p>
<article class="theorem theorem-like" id="thm_7_d_SVD_cols_lin_indep"><h5 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">30.7</span><span class="period">.</span>
</h5>
<p id="p-5145">If the columns of a matrix <span class="process-math">\(A\)</span> are linearly independent, then <span class="process-math">\(A^{+} = \left(A^{\tr}A\right)^{-1}A^{\tr}\text{.}\)</span></p></article><p id="p-5146">To see how, suppose that <span class="process-math">\(A\)</span> is an <span class="process-math">\(m \times n\)</span> matrix with linearly independent columns.</p>
</div>
<article class="task exercise-like" id="task-1715"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5147">Given that the columns of <span class="process-math">\(A\)</span> are linearly independent, what must be the relationship between <span class="process-math">\(n\)</span> and <span class="process-math">\(m\text{?}\)</span></p></article><article class="task exercise-like" id="task-1716"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5148">Since the columns of <span class="process-math">\(A\)</span> are linearly independent, it follows that <span class="process-math">\(A^{\tr}A\)</span> is invertible (see <a href="" class="xref" data-knowl="./knowl/act_LS_invertible.html" title="Activity 26.4">Activity 26.4</a>). So the eigenvalues of <span class="process-math">\(A^{\tr}A\)</span> are all non-zero. Let <span class="process-math">\(\sigma_1\text{,}\)</span> <span class="process-math">\(\sigma_2\text{,}\)</span> <span class="process-math">\(\ldots\text{,}\)</span> <span class="process-math">\(\sigma_r\)</span> be the singular values of <span class="process-math">\(A\text{.}\)</span> How is <span class="process-math">\(r\)</span> related to <span class="process-math">\(n\text{,}\)</span> and what do <span class="process-math">\(\Sigma\)</span> and <span class="process-math">\(\Sigma^{+}\)</span> look like?</p></article><article class="task exercise-like" id="task-1717"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-5149">Let us now investigate the form of the invertible matrix <span class="process-math">\(A^{\tr}A\)</span> (note that neither <span class="process-math">\(A\)</span> nor <span class="process-math">\(A^{\tr}\)</span> is necessarily invertible). If a singular value decomposition of <span class="process-math">\(A\)</span> is <span class="process-math">\(U \Sigma V^{\tr}\text{,}\)</span> show that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A^{\tr}A = V \Sigma^{\tr} \Sigma V^{\tr}\text{.}
\end{equation*}
</div></article><article class="task exercise-like" id="task-1718"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<p id="p-5150">Let <span class="process-math">\(\lambda_i = \sigma_i^2\)</span> for <span class="process-math">\(i\)</span> from 1 to <span class="process-math">\(n\text{.}\)</span> It is straightforward to see that <span class="process-math">\(\Sigma^{\tr} \Sigma\)</span> is an <span class="process-math">\(n \times n\)</span> diagonal matrix <span class="process-math">\(D\text{,}\)</span> where</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
D = \Sigma^{\tr} \Sigma = \left[ \begin{array}{ccccc} \lambda_1\amp \amp \amp \amp  \\ \amp  \lambda_2\amp \amp 0\amp  \\ \amp \amp  \lambda_3\amp \amp  \\ \amp   \amp  \amp  \ddots \amp    \\ \amp 0\amp \amp \amp  \lambda_n \end{array}  \right]\text{.}
\end{equation*}
</div>
<p class="continuation">Then <span class="process-math">\((A^{\tr}A)^{-1} = VD^{-1}V^{\tr}\text{.}\)</span> Recall that <span class="process-math">\(A^{+} = V \Sigma^{+} U^{\tr}\text{,}\)</span> so to relate <span class="process-math">\(A^{\tr}A\)</span> to <span class="process-math">\(A^{+}\)</span> we need a product that is equal to <span class="process-math">\(\Sigma^{+}\text{.}\)</span> Explain why</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
D^{-1} \Sigma^{\tr} = \Sigma^{+}\text{.}
\end{equation*}
</div></article><article class="task exercise-like" id="task-1719"><h5 class="heading"><span class="codenumber">(e)</span></h5>
<p id="p-5151">Complete the activity by showing that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\left(A^{\tr}A\right)^{-1} A^{\tr} = A^{+}\text{.}
\end{equation*}
</div></article></article><p id="p-5152">Therefore, to calculate <span class="process-math">\(A^{+}\)</span> and solve a least squares problem, <a href="" class="xref" data-knowl="./knowl/thm_7_d_SVD_cols_lin_indep.html" title="Theorem 30.7">Theorem 30.7</a> shows that as long as the columns of <span class="process-math">\(A\)</span> are linearly independent, we can avoid using a singular value decomposition of <span class="process-math">\(A\)</span> in finding <span class="process-math">\(A^{+}\text{.}\)</span></p></section><section class="section" id="sec_pseudo_exam"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">Examples</span>
</h3>
<p id="p-5153">What follows are worked examples that use the concepts from this section.</p>
<article class="example example-like" id="example-62"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">30.8</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-506">
<p id="p-5154">Let</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A = \left[ \begin{array}{ccc} 2\amp 5\amp 4\\6\amp 3\amp 0\\6\amp 3\amp 0\\2\amp 5\amp 4 \end{array}  \right]\text{.}
\end{equation*}
</div>
<p id="p-5155">The eigenvalues of <span class="process-math">\(A^{\tr}A\)</span> are <span class="process-math">\(\lambda_1 = 144\text{,}\)</span> <span class="process-math">\(\lambda_2 = 36\text{,}\)</span> and <span class="process-math">\(\lambda_3=0\)</span> with corresponding eigenvectors</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\vw_1 = \left[ \begin{array}{c} 2\\2\\1 \end{array}  \right], \ \vw_1 = \left[ \begin{array}{r} -2\\1\\2 \end{array}  \right], \ \text{ and }  \ \vw_1 = \left[ \begin{array}{r} 1\\-2\\2 \end{array}  \right]\text{.}
\end{equation*}
</div>
<p id="p-5156">In addition,</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A \vw_1 = \left[ \begin{array}{c} 18\\18\\18\\18 \end{array}  \right] \ \text{ and }  A \vw_2 = \left[ \begin{array}{r} 9\\-9\\-9\\9 \end{array}  \right]\text{.}
\end{equation*}
</div>
</div>
<article class="task exercise-like" id="task-1720"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5157">Find orthogonal matrices <span class="process-math">\(U\)</span> and <span class="process-math">\(V\text{,}\)</span> and the matrix <span class="process-math">\(\Sigma\text{,}\)</span> so that <span class="process-math">\(U \Sigma V^{\tr}\)</span> is a singular value decomposition of <span class="process-math">\(A\text{.}\)</span></p>
<div class="solution solution-like" id="solution-176">
<h5 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h5>
<p id="p-5158">Normalizing the eigenvectors <span class="process-math">\(\vw_1\text{,}\)</span> <span class="process-math">\(\vw_2\text{,}\)</span> and <span class="process-math">\(\vw_3\)</span> to normal eigenvectors <span class="process-math">\(\vv_1\text{,}\)</span> <span class="process-math">\(\vv_2\text{,}\)</span> and <span class="process-math">\(\vv_3\text{,}\)</span> respectively, gives us an orthogonal matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
V = \left[  \begin{array}{crr} \frac{2}{3}\amp -\frac{2}{3}\amp \frac{1}{3}\\ \frac{2}{3}\amp \frac{1}{3}\amp - \frac{2}{3}\\  \frac{1}{3}\amp \frac{2}{3}\amp \frac{2}{3} \end{array}  \right]\text{.}
\end{equation*}
</div>
<p class="continuation">Now <span class="process-math">\(A \vv_i = A \frac{\vw_i}{||\vw_i||} = \frac{1}{||\vw_i||} A \vw_i\text{,}\)</span> so normalizing the vectors <span class="process-math">\(A \vw_1\)</span> and <span class="process-math">\(A \vw_2\)</span> gives us vectors</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\vu_1 = \frac{1}{2} \left[ \begin{array}{c} 1\\1\\1\\1 \end{array}  \right] \ \text{ and }  \ \vu_2 = \frac{1}{2} \left[ \begin{array}{r} 1\\-1\\-1\\1 \end{array}  \right]
\end{equation*}
</div>
<p class="continuation">that are the first two columns of our matrix <span class="process-math">\(U\text{.}\)</span> Given that <span class="process-math">\(U\)</span> is a <span class="process-math">\(4 \times 4\)</span> matrix, we need to find two other vectors orthogonal to <span class="process-math">\(\vu_1\)</span> and <span class="process-math">\(\vu_2\)</span> that will combine with <span class="process-math">\(\vu_1\)</span> and <span class="process-math">\(\vu_2\)</span> to form an orthogonal basis for <span class="process-math">\(\R^4\text{.}\)</span> Letting <span class="process-math">\(\vz_1 = [1 \ 1 \ 1 \ 1]^{\tr}\text{,}\)</span> <span class="process-math">\(\vz_2 = [1 \ -1 \ -1 \ 1]^{\tr}\text{,}\)</span> <span class="process-math">\(\vz_3 = [1 \ 0 \ 0 \ 0]^{\tr}\text{,}\)</span> and <span class="process-math">\(\vz_4 = [0 \ 1 \ 0 \ 1]^{\tr}\text{,}\)</span> a computer algebra system shows that the reduced row echelon form of the matrix <span class="process-math">\([\vz_1 \ \vz_2 \ \vz_3 \ \vz_4]\)</span> is <span class="process-math">\(I_4\text{,}\)</span> so that vectors <span class="process-math">\(\vz_1\text{,}\)</span> <span class="process-math">\(\vz_2\text{,}\)</span> <span class="process-math">\(\vz_3\text{,}\)</span> <span class="process-math">\(\vz_4\)</span> are linearly independent. Letting <span class="process-math">\(\vw_1 = \vz_1\)</span> and <span class="process-math">\(\vw_2 = \vz_2\text{,}\)</span> the Gram-Schmidt process shows that the set <span class="process-math">\(\{\vw_1, \vw_2, \vw_3, \vw_4\}\)</span> is an orthogonal basis for <span class="process-math">\(\R^4\text{,}\)</span> where <span class="process-math">\(\vw_3 = \frac{1}{4} [2 \ 0 \ 0 \ -2]^{\tr}\)</span> and (using <span class="process-math">\([1 \ 0 \ 0 \ -1]^{\tr}\)</span> for <span class="process-math">\(\vw_3\)</span>) <span class="process-math">\(\vw_4 = \frac{1}{4} [0 \ 2 \ -2 \ 0]^{\tr}\text{.}\)</span> The set <span class="process-math">\(\{\vu_1, \vu_2, \vu_3, \vu_4\}\)</span> where <span class="process-math">\(\vu_1 = \frac{1}{2}[1 \ 1 \ 1 \ 1]^{\tr}\text{,}\)</span> <span class="process-math">\(\vu_2 = \frac{1}{2}[1 \ -1 \ -1 \ 1]^{\tr}\text{,}\)</span> <span class="process-math">\(\vu_3 = \frac{1}{\sqrt{2}}[1 \ 0 \ 0 \ -1]^{\tr}\)</span> and <span class="process-math">\(\vu_4 = \frac{1}{\sqrt{2}}[0 \ 1 \ -1 \ 0]^{\tr}\)</span> is an orthonormal basis for <span class="process-math">\(\R^4\)</span> and we can let</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
U = \left[  \begin{array}{crrr} \frac{1}{2} \amp  \frac{1}{2} \amp  \frac{1}{\sqrt{2}} \amp  0 \\ \frac{1}{2} \amp  -\frac{1}{2} \amp  0 \amp  \frac{1}{\sqrt{2}} \\  \frac{1}{2} \amp  -\frac{1}{2} \amp  0 \amp  -\frac{1}{\sqrt{2}} \\ \frac{1}{2} \amp  \frac{1}{2} \amp  -\frac{1}{\sqrt{2}} \amp  0 \end{array}  \right]\text{.}
\end{equation*}
</div>
<p class="continuation">The singular values of <span class="process-math">\(A\)</span> are <span class="process-math">\(\sigma_1 = \sqrt{\lambda_1} = 12\)</span> and <span class="process-math">\(\sigma_2 = \sqrt{\lambda_2} = 6\text{,}\)</span> and so</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\Sigma = \begin{bmatrix}12\amp 0\amp 0 \\ 0\amp 6\amp 0 \\0\amp 0\amp 0 \\ 0\amp 0\amp 0 \end{bmatrix}\text{.}
\end{equation*}
</div>
<p class="continuation">Therefore, a singular value decomposition of <span class="process-math">\(A\)</span> is <span class="process-math">\(U \Sigma V^{\tr}\)</span> of</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\left[ \begin{array}{crrr} \frac{1}{2} \amp  \frac{1}{2} \amp  \frac{1}{\sqrt{2}} \amp  0 \\ \frac{1}{2} \amp  -\frac{1}{2} \amp  0 \amp  \frac{1}{\sqrt{2}} \\  \frac{1}{2} \amp  -\frac{1}{2} \amp  0 \amp  -\frac{1}{\sqrt{2}} \\ \frac{1}{2} \amp  \frac{1}{2} \amp  -\frac{1}{\sqrt{2}} \amp  0 \end{array}  \right] \begin{bmatrix}12\amp 0\amp 0 \\ 0\amp 6\amp 0 \\0\amp 0\amp 0 \\ 0\amp 0\amp 0 \end{bmatrix}  \left[ \begin{array}{rrc} \frac{2}{3}\amp \frac{2}{3}\amp \frac{1}{3}\\ -\frac{2}{3}\amp \frac{1}{3}\amp \frac{2}{3}\\  \frac{1}{3}\amp -\frac{2}{3}\amp \frac{2}{3} \end{array}  \right]\text{.}
\end{equation*}
</div>
</div></article><article class="task exercise-like" id="task-1721"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5159">Determine the best rank 1 approximation to <span class="process-math">\(A\text{.}\)</span> Give an appropriate numerical estimate as to how good this approximation is to <span class="process-math">\(A\text{.}\)</span></p>
<div class="solution solution-like" id="solution-177">
<h5 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h5>
<p id="p-5160">The outer product decomposition of <span class="process-math">\(A\)</span> is</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A = \sigma_1 \vu_1 \vv_1^{\tr} + \sigma_2 \vu_2 \vv_2^{\tr}\text{.}
\end{equation*}
</div>
<p class="continuation">So the rank one approximation to <span class="process-math">\(A\)</span> is</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\sigma_1 \vu_1 \vv_1^{\tr} = 12 \left(\frac{1}{2}\right) \left[ \begin{array}{c} 1\\1\\1\\1 \end{array}  \right] \left[ \begin{array}{ccc} \frac{2}{3} \amp  \frac{2}{3} \amp  \frac{1}{3} \end{array}  \right] = \left[ \begin{array}{ccc} 4\amp 4\amp 2\\ 4\amp 4\amp 2 \\ 4\amp 4\amp 2\\ 4\amp 4\amp 2 \end{array}   \right]\text{.}
\end{equation*}
</div>
<p class="continuation">The error in approximating <span class="process-math">\(A\)</span> with this rank one approximation is</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\sqrt{\frac{\sigma_2^2}{\sigma_1^2+\sigma_2^2}} = \sqrt{\frac{36}{180}} = \sqrt{\frac{1}{5}} \approx 0.447\text{.}
\end{equation*}
</div>
</div></article><article class="task exercise-like" id="task-1722"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-5161">Find the pseudoinverse <span class="process-math">\(A^+\)</span> of <span class="process-math">\(A\text{.}\)</span></p>
<div class="solution solution-like" id="solution-178">
<h5 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h5>
<p id="p-5162">Given that <span class="process-math">\(A = U \Sigma V^{\tr}\text{,}\)</span> we use the pseudoinverse <span class="process-math">\(\Sigma^+\)</span> of <span class="process-math">\(\Sigma\)</span> to find the pseudoinverse <span class="process-math">\(A^+\)</span> of <span class="process-math">\(A\)</span> by</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A^+ = V \Sigma^+ U^{\tr}\text{.}
\end{equation*}
</div>
<p class="continuation">Now</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\Sigma^+ = \left[  \begin{array}{ccc} \frac{1}{12}\amp 0\amp 0 \\ 0\amp \frac{1}{6}\amp 0 \\0\amp 0\amp 0 \\ 0\amp 0\amp 0 \end{array}  \right]\text{,}
\end{equation*}
</div>
<p class="continuation">so</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-205">
\begin{align*}
A^+ \amp = \left[  \begin{array}{crr} \frac{2}{3}\amp -\frac{2}{3}\amp \frac{1}{3}\\
\frac{2}{3}\amp \frac{1}{3}\amp - \frac{2}{3}\\
\frac{1}{3}\amp \frac{2}{3}\amp \frac{2}{3}\end{array} \right] \left[  \begin{array}{ccc} \frac{1}{12}\amp 0\amp 0\\
0\amp \frac{1}{6}\amp 0\\
0\amp 0\amp 0\\
0\amp 0\amp 0 \end{array} \right] \left[  \begin{array}{crrr} \frac{1}{2} \amp  \frac{1}{2} \amp  \frac{1}{\sqrt{2}} \amp  0\\
\frac{1}{2} \amp  -\frac{1}{2} \amp  0 \amp  \frac{1}{\sqrt{2}}\\
\frac{1}{2} \amp  -\frac{1}{2} \amp  0 \amp  -\frac{1}{\sqrt{2}}\\
\frac{1}{2} \amp  \frac{1}{2} \amp  -\frac{1}{\sqrt{2}} \amp  0 \end{array} \right]^{\tr}\\
\amp =  \frac{1}{72} \left[ \begin{array}{rrrr} -2\amp 6\amp 6\amp -2\\
4\amp 0\amp 0\amp 4\\
5\amp -3\amp -3\amp 5 \end{array} \right]\text{.}
\end{align*}
</div>
</div></article><article class="task exercise-like" id="task-1723"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<p id="p-5163">Let <span class="process-math">\(\vb = \left[ \begin{array}{c} 1\\0\\1\\0 \end{array}  \right]^{\tr}\text{.}\)</span> Does the matrix equation</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A \vx = \vb
\end{equation*}
</div>
<p class="continuation">have a solution? If so, find the solution. If not, find the best approximation you can to a solution to this matrix equation.</p>
<div class="solution solution-like" id="solution-179">
<h5 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h5>
<p id="p-5164">Augmenting <span class="process-math">\(A\)</span> with <span class="process-math">\(\vb\)</span> and row reducing shows that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
[A \ \vb ] \sim  \left[ \begin{array}{crrr} 2\amp 5\amp 4\amp 1\\ 0\amp -12\amp -12\amp -3 \\ 0\amp 0\amp 0\amp 1\\ 0\amp 0\amp 0\amp 0 \end{array}   \right]\text{,}
\end{equation*}
</div>
<p class="continuation">so <span class="process-math">\(\vb\)</span> is not in <span class="process-math">\(\Col A\)</span> and the equation <span class="process-math">\(A\vx = \vb\)</span> has no solution. However, the best approximation to a solution to <span class="process-math">\(A \vx = \vb\)</span> is found using the pseudoinverse <span class="process-math">\(A^+\)</span> of <span class="process-math">\(A\text{.}\)</span> That best solution is</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-206">
\begin{align*}
\vx^*\amp = AA^+ \vb\\
\amp = \left[ \begin{array}{ccc} 2\amp 5\amp 4\\
6\amp 3\amp 0\\
6\amp 3\amp 0\\
2\amp 5\amp 4 \end{array} \right] \frac{1}{72} \left[ \begin{array}{rrrr} -2\amp 6\amp 6\amp -2\\
4\amp 0\amp 0\amp 4\\
5\amp -3\amp -3\amp 5 \end{array} \right] \left[ \begin{array}{c} 1\\
0\\
1\\
0 \end{array} \right]\\
\amp = \frac{1}{2} \left[ \begin{array}{c} 2\\
1\\
1\\
2 \end{array} \right]\text{.}
\end{align*}
</div>
</div></article><article class="task exercise-like" id="task-1724"><h5 class="heading"><span class="codenumber">(e)</span></h5>
<p id="p-5165">Use the orthogonal basis <span class="process-math">\(\{\frac{1}{2}[1 \ 1 \ 1 \ 1]^{\tr}, \frac{1}{2}[1 \ -1 \ -1 \ 1]^{\tr}\}\)</span> of <span class="process-math">\(\Col A\)</span> to find the projection of <span class="process-math">\(\vb\)</span> onto <span class="process-math">\(\Col A\text{.}\)</span> Compare to your solution in part (c).</p>
<div class="solution solution-like" id="solution-180">
<h5 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h5>
<p id="p-5166">The rank of <span class="process-math">\(A\)</span> is 2 and an orthonormal basis for <span class="process-math">\(\Col A\)</span> is <span class="process-math">\(\{\vu_1, \vu_2\}\text{,}\)</span> where <span class="process-math">\(\vu_1 = \frac{1}{2}[1 \ 1 \ 1 \ 1]^{\tr}\)</span> and <span class="process-math">\(\vu_2 = \frac{1}{2}[1 \ -1 \ -1 \ 1]^{\tr}\text{.}\)</span> So</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-207">
\begin{align*}
\proj_{\Col A} \vb \amp = (\vb \cdot \vu_1) \vu_1 + (\vb \cdot \vu_2) \vu_2\\
\amp = \left(\frac{3}{2}\right)\left(\frac{1}{2}\right)[1 \ 1 \ 1 \ 1]^{\tr} + \left(\frac{1}{2}\right)\left(\frac{1}{2}\right)[1 \ -1 \ -1 \ 1]^{\tr}\\
\amp = \frac{1}{2}[2 \ 1 \ 1 \ 2]^{\tr}
\end{align*}
</div>
<p class="continuation">as expected from part (c).</p>
</div></article></article><article class="example example-like" id="example-63"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">30.9</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-507">
<p id="p-5167"><a href="" class="xref" data-knowl="./knowl/T_Debt_per_capita.html" title="Table 30.10: U.S. per capita debt">Table 30.10</a> shows the per capita debt in the U.S. in from 2014 to 2019 (source <a class="external" href="https://www.statista.com/statistics/203064/national-debt-of-the-united-states-per-capita/" target="_blank">statistica.com</a><a href="" data-knowl="" class="id-ref fn-knowl original" data-refid="hk-fn-51" id="fn-51"><sup> 51 </sup></a>).</p>
<figure class="table table-like" id="T_Debt_per_capita"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">30.10<span class="period">.</span></span><span class="space"> </span>U.S. per capita debt</figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="c m b1 r1 l0 t0 lines">Year</td>
<td class="c m b1 r0 l0 t0 lines">2014</td>
<td class="c m b1 r0 l0 t0 lines">2015</td>
<td class="c m b1 r0 l0 t0 lines">2016</td>
<td class="c m b1 r0 l0 t0 lines">2017</td>
<td class="c m b1 r0 l0 t0 lines">2018</td>
<td class="c m b1 r0 l0 t0 lines">2019</td>
</tr>
<tr>
<td class="c m b0 r1 l0 t0 lines">Debt</td>
<td class="c m b0 r0 l0 t0 lines">55905</td>
<td class="c m b0 r0 l0 t0 lines">56513</td>
<td class="c m b0 r0 l0 t0 lines">60505</td>
<td class="c m b0 r0 l0 t0 lines">62174</td>
<td class="c m b0 r0 l0 t0 lines">65697</td>
<td class="c m b0 r0 l0 t0 lines">69064</td>
</tr>
</table></div></figure>
</div>
<article class="task exercise-like" id="task-1725"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5168">Set up a linear system of the form <span class="process-math">\(A \vx = \vb\)</span> whose least squares solution provides a linear fit to the data.</p>
<div class="solution solution-like" id="solution-181">
<h5 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h5>
<p id="p-5169">A linear approximation <span class="process-math">\(f(x) = a_0 + a_1x\)</span> to the system would satisfy the equation <span class="process-math">\(A \vx = \vb\text{,}\)</span> where <span class="process-math">\(A = \left[ \begin{array}{cc} 1\amp 2014 \\ 1\amp 2015 \\ 1\amp 2016 \\ 1\amp 2017 \\ 1\amp 2018 \\ 1\amp 2019 \end{array} \right]\text{,}\)</span> <span class="process-math">\(\vx = \left[ \begin{array}{c} a_0 \\ a_1 \end{array} \right]\text{,}\)</span> and <span class="process-math">\(\vb = \left[ \begin{array}{c} 55905 \\ 56513 \\ 60505 \\ 62174 \\ 65697 \\ 69064 \end{array} \right]\text{.}\)</span></p>
</div></article><article class="task exercise-like" id="task-1726"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5170">Use technology to approximate a singular value decomposition (round to four decimal places). Use this svd to approximate the pseudoinverse of <span class="process-math">\(A\text{.}\)</span> Then use this pseudoinverse to approximate the least squares linear approximation to the system.</p>
<div class="solution solution-like" id="solution-182">
<h5 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h5>
<p id="p-5171">Technology shows that a singular value decomposition of <span class="process-math">\(A\)</span> is approximately <span class="process-math">\(U \Sigma V^{\tr}\text{,}\)</span> where</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-208">
\begin{align*}
U \amp = \left[\begin{array}{rrrrrr} 0.4077 \amp  0.5980 \amp  -0.3997 \amp  -0.3615 \amp  -0.3233 \amp  -0.2851\\
0.4079 \amp  0.3589 \amp  -0.0621 \amp  0.1880 \amp  0.4381 \amp  0.6882\\
0.4081 \amp  0.1199 \amp  0.8817 \amp  -0.1181 \amp  -0.1178 \amp  -0.1176\\
0.4083 \amp  -0.1192 \amp  -0.1291 \amp  0.8229 \amp  -0.2251 \amp  -0.2730\\
0.4085 \amp  -0.3582 \amp  -0.1400 \amp  -0.2361 \amp  0.6677 \amp  -0.4285\\
0.4087 \amp  -0.5973 \amp  -0.1508 \amp  -0.2952 \amp  -0.4396 \amp  0.4160 \end{array}\right]\\
\Sigma \amp =  \left[\begin{array}{cc} 4939.3984 \amp  0.0\\
0.0 \amp  0.0021\\
0.0 \amp  0.0\\
0.0 \amp  0.0\\
0.0 \amp  0.0\\
0.0 \amp  0.0 \end{array}\right]\\
V \amp = \left[\begin{array}{cr} 0.0005 \amp  1.0000\\
1.0000 \amp  -0.0005 \end{array}\right]\text{.}
\end{align*}
</div>
<p class="continuation">Thus, with <span class="process-math">\(\Sigma^+ = \left[  \begin{array}{cccccc} \frac{1}{4939.3984}\amp 0\amp 0\amp 0\amp 0\amp 0 \\ 0\amp \frac{1}{0.0021}\amp 0\amp 0\amp 0\amp 0 \end{array}  \right]\text{,}\)</span> we have that the pseudoinverse of <span class="process-math">\(A\)</span> is approximately</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A^+ = V \Sigma^+ U^{\tr} = \left[\begin{array}{rrrrrr} 288.2381 \amp  173.0095 \amp  57.7809 \amp  -57.4476 \amp  -172.6762 \amp  -287.9048 \\ -0.14286 \amp  -0.0857 \amp  -0.0286 \amp  0.02857 \amp  0.0857 \amp  0.14286 \end{array} \right]\text{.}
\end{equation*}
</div>
<p class="continuation">So our least squares linear approximation is found by</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A^{+} \vb = \left[ \begin{array}{r} -5412635.9714 \\ 2714.7429 \end{array}  \right]\text{.}
\end{equation*}
</div>
<p class="continuation">This makes our least squares linear approximation to be (to four decimal places)</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
f(x) = -5412635.9714 + 2714.7429x\text{.}
\end{equation*}
</div>
</div></article><article class="task exercise-like" id="task-1727"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-5172">Calculate <span class="process-math">\(\left(A^{\tr}A\right)^{-1}A^{\tr}\)</span> directly and compare to the pseudoinverse you found in part (b).</p>
<div class="solution solution-like" id="solution-183">
<h5 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h5>
<p id="p-5173">Calculating <span class="process-math">\(\left(A^{\tr}A\right)^{-1}A^{\tr}\)</span> gives the same matrix as <span class="process-math">\(A^+\text{,}\)</span> so we obtain the same linear approximation.</p>
</div></article><article class="task exercise-like" id="task-1728"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<p id="p-5174">Use your approximation to estimate the U.S. per capita debt in 2020.</p>
<div class="solution solution-like" id="solution-184">
<h5 class="heading">
<span class="type">Solution</span><span class="period">.</span>
</h5>
<p id="p-5175">The approximate U.S. per capita debt in 2020 is</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
f(2020) = -5412635.9714 + 2714.7429(2020) = 71144.60\text{.}
\end{equation*}
</div>
</div></article></article></section><section class="section" id="sec_pseudo_summ"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">Summary</span>
</h3>
<ul class="disc">
<li id="li-775"><p id="p-5176">The condition number of an <span class="process-math">\(m \times n\)</span> matrix <span class="process-math">\(A\)</span> is the number <span class="process-math">\(||A^{-1}|| \ ||A||\text{.}\)</span> The condition number provides a measure of how well the relative error in a calculated value <span class="process-math">\(\Delta \vb\)</span> predicts the relative error in <span class="process-math">\(\Delta \vx\)</span> when we are trying to solve a system <span class="process-math">\(A \vx = \vb\text{.}\)</span></p></li>
<li id="li-776">
<p id="p-5177">A pseudoinverse <span class="process-math">\(A^{+}\)</span> of a matrix <span class="process-math">\(A\)</span> can be found through a singular value decomposition. Let <span class="process-math">\(U \Sigma V^{\tr}\)</span> be a singular value decomposition of an <span class="process-math">\(m \times n\)</span> matrix <span class="process-math">\(A\)</span> of rank <span class="process-math">\(r\text{,}\)</span> with</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\Sigma = \left[ \begin{array}{ccccc|c} \sigma_1\amp \amp \amp \amp \amp  \\ \amp  \sigma_2\amp \amp 0\amp \amp  \\ \amp \amp  \sigma_3\amp \amp \amp 0 \\ \amp  0 \amp  \amp  \ddots \amp  \amp  \\ \amp \amp \amp \amp  \sigma_r \\ \hline \amp \amp 0\amp \amp \amp 0 \end{array}  \right]
\end{equation*}
</div>
<p class="continuation">If <span class="process-math">\(\Sigma^+\)</span> is the <span class="process-math">\(n \times m\)</span> matrix defined by</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\Sigma^{+} = \left[ \begin{array}{ccccc|c} \frac{1}{\sigma_1}\amp \amp \amp \amp \amp  \\ \amp  \frac{1}{\sigma_2}\amp \amp 0\amp \amp  \\ \amp \amp  \frac{1}{\sigma_3}\amp \amp \amp 0 \\ \amp  0 \amp  \amp  \ddots \amp  \amp  \\ \amp \amp \amp \amp  \frac{1}{\sigma_r} \\ \hline \amp \amp 0\amp \amp \amp 0 \end{array}  \right]\text{,}
\end{equation*}
</div>
<p class="continuation">then <span class="process-math">\(A^{+} = V\Sigma^{+}U^{\tr}\text{.}\)</span></p>
</li>
<li id="li-777"><p id="p-5178">A pseudoinverse <span class="process-math">\(A^{+}\)</span> of a matrix <span class="process-math">\(A\)</span> acts like an inverse for <span class="process-math">\(A\text{.}\)</span> So if we can't solve a matrix equation <span class="process-math">\(A \vx = \vb\)</span> because <span class="process-math">\(\vb\)</span> isn't in <span class="process-math">\(\Col A\text{,}\)</span> we can use the pseudoinverse of <span class="process-math">\(A\)</span> to “solve” the equation <span class="process-math">\(A \vx = \vb\)</span> with the “solution” <span class="process-math">\(A^+ \vb\text{.}\)</span> While not an exact solution, <span class="process-math">\(A^+ \vb\)</span> turns out to be the best approximation to a solution in the least squares sense.</p></li>
</ul></section><section class="exercises" id="sec_pseudo_exer"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber"></span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-299"><h4 class="heading"><span class="codenumber">1<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-508">
<p id="p-5179">Let <span class="process-math">\(A = \left[ \begin{array}{rcc} 20\amp 4\amp 32 \\ -4\amp 4\amp 2 \\ 35\amp 22\amp 26 \end{array}  \right]\text{.}\)</span> Then <span class="process-math">\(A\)</span> has singular value decomposition <span class="process-math">\(U \Sigma V^{\tr}\)</span> , where</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-209">
\begin{align*}
U \amp = \frac{1}{5}\left[ \begin{array}{crc} 3\amp 4\amp 0\\
0\amp 0\amp 5\\
4\amp -3\amp 0 \end{array} \right]\\
\Sigma \amp = \left[ \begin{array}{crc} 60\amp 0\amp 0\\
0\amp 15\amp 0\\
0\amp 0\amp 6 \end{array} \right]\\
V \amp = \frac{1}{3}\left[ \begin{array}{crr} 2\amp -1\amp -2\\
1\amp -2\amp 2\\
2\amp 2\amp 1 \end{array} \right]\text{.}
\end{align*}
</div>
</div>
<article class="task exercise-like" id="task-1729"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5180">What are the singular values of <span class="process-math">\(A\text{?}\)</span></p></article><article class="task exercise-like" id="task-1730"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5182">Write the outer product decomposition of <span class="process-math">\(A\text{.}\)</span></p></article><article class="task exercise-like" id="task-1731"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-5184">Find the best rank 1 approximation to <span class="process-math">\(A\text{.}\)</span> What is the relative error in approximating <span class="process-math">\(A\)</span> by this rank 1 matrix?</p></article><article class="task exercise-like" id="task-1732"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<p id="p-5186">Find the best rank 2 approximation to <span class="process-math">\(A\text{.}\)</span> What is the relative error in approximating <span class="process-math">\(A\)</span> by this rank 2 matrix?</p></article></article><article class="exercise exercise-like" id="exercise-300"><h4 class="heading"><span class="codenumber">2<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-509"><p id="p-5188">Let <span class="process-math">\(A = \left[ \begin{array}{ccrr} 861\amp 3969\amp 70\amp 140 \\ 3969\amp 861\amp 70\amp 140 \\ 3969\amp 861\amp -70\amp -140 \\ 861\amp 3969\amp -70\amp -140 \end{array} \right]\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1733"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5189">Find a singular value decomposition for <span class="process-math">\(A\text{.}\)</span></p></article><article class="task exercise-like" id="task-1734"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5190">What are the singular values of <span class="process-math">\(A\text{?}\)</span></p></article><article class="task exercise-like" id="task-1735"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-5191">Write the outer product decomposition of <span class="process-math">\(A\text{.}\)</span></p></article><article class="task exercise-like" id="task-1736"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<p id="p-5192">Find the best rank 1, 2, and 3 approximations to <span class="process-math">\(A\text{.}\)</span> How much information about <span class="process-math">\(A\)</span> does each of these approximations contain?</p></article></article><article class="exercise exercise-like" id="exercise-301"><h4 class="heading"><span class="codenumber">3<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-510">
<p id="p-5193">Assume that the number of feet traveled by a batted baseball at various angles in degrees (all hit at the same bat speed) is given in <a href="" class="xref" data-knowl="./knowl/T_7_d_batting.html" title="Table 30.11: Distance traveled by batted ball">Table 30.11</a>.</p>
<figure class="table table-like" id="T_7_d_batting"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">30.11<span class="period">.</span></span><span class="space"> </span>Distance traveled by batted ball</figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="l m b1 r2 l1 t1 lines">Angle</td>
<td class="c m b1 r1 l0 t1 lines"><span class="process-math">\(10^{\circ}\)</span></td>
<td class="c m b1 r1 l0 t1 lines"><span class="process-math">\(20^{\circ}\)</span></td>
<td class="c m b1 r1 l0 t1 lines"><span class="process-math">\(30^{\circ}\)</span></td>
<td class="c m b1 r1 l0 t1 lines"><span class="process-math">\(40^{\circ}\)</span></td>
<td class="c m b1 r1 l0 t1 lines"><span class="process-math">\(50^{\circ}\)</span></td>
<td class="c m b1 r1 l0 t1 lines"><span class="process-math">\(60^{\circ}\)</span></td>
</tr>
<tr>
<td class="l m b1 r2 l1 t0 lines">Distance</td>
<td class="c m b1 r1 l0 t0 lines">116</td>
<td class="c m b1 r1 l0 t0 lines">190</td>
<td class="c m b1 r1 l0 t0 lines">254</td>
<td class="c m b1 r1 l0 t0 lines">285</td>
<td class="c m b1 r1 l0 t0 lines">270</td>
<td class="c m b1 r1 l0 t0 lines">230</td>
</tr>
</table></div></figure>
</div>
<article class="task exercise-like" id="task-1737"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5194">Plot the data and explain why a quadratic function is likely a better fit to the data than a linear function.</p></article><article class="task exercise-like" id="task-1738"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5196">Find the least squares quadratic approximation to this data. Plot the quadratic function on same axes as your data.</p></article><article class="task exercise-like" id="task-1739"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-5198">At what angle (or angles), to the nearest degree, must a player bat the ball in order for the ball to travel a distance of 220 feet?</p></article></article><article class="exercise exercise-like" id="exercise-302"><h4 class="heading"><span class="codenumber">4<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-511"><p id="p-5200">How close can a matrix be to being non-invertible? We explore that idea in this exercise. Let <span class="process-math">\(A = [a_{ij}]\)</span> be the <span class="process-math">\(n \times n\)</span> upper triangular matrix with 1s along the diagonal and with every other entry being <span class="process-math">\(-1\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1740"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5201">What is <span class="process-math">\(\det(A)\text{?}\)</span> What are the eigenvalues of <span class="process-math">\(A\text{?}\)</span> Is <span class="process-math">\(A\)</span> invertible?</p></article><article class="task exercise-like" id="task-1741"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<div class="introduction" id="introduction-512"><p id="p-5202">Let <span class="process-math">\(B = [b_{ij}]\)</span> be the <span class="process-math">\(n \times n\)</span> matrix so that <span class="process-math">\(b_{n1} = -\frac{1}{2^{n-2}}\)</span> and <span class="process-math">\(b_{ij} = a_{ij}\)</span> for all other <span class="process-math">\(i\)</span> and <span class="process-math">\(j\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1742"><h6 class="heading"><span class="codenumber">(i)</span></h6>
<p id="p-5203">For the matrix <span class="process-math">\(B\)</span> with <span class="process-math">\(n=3\text{,}\)</span> show that the equation <span class="process-math">\(B \vx = \vzero\)</span> has a non-trivial solution. Find one non-trivial solution.</p></article><article class="task exercise-like" id="task-1743"><h6 class="heading"><span class="codenumber">(ii)</span></h6>
<p id="p-5204">For the matrix <span class="process-math">\(B\)</span> with <span class="process-math">\(n=4\text{,}\)</span> show that the equation <span class="process-math">\(B \vx = \vzero\)</span> has a non-trivial solution. Find one non-trivial solution.</p></article><article class="task exercise-like" id="task-1744"><h6 class="heading"><span class="codenumber">(iii)</span></h6>
<p id="p-5205">Use the pattern established in parts (i.) and (ii.) to find a non-trivial solution to the equation <span class="process-math">\(B \vx = \vzero\)</span> for an arbitrary value of <span class="process-math">\(n\text{.}\)</span> Be sure to verify that you have a solution. Is <span class="process-math">\(B\)</span> invertible?</p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-74" id="hint-74"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-74"><div class="hint solution-like"><p id="p-5206">For any positive integer <span class="process-math">\(m\text{,}\)</span> the sum <span class="process-math">\(1+\sum_{k=0}^{m-1} 2^k\)</span> is the partial sum of a geometric series with ratio <span class="process-math">\(2\)</span> and so <span class="process-math">\(1+\sum_{k=0}^{m-1} 2^k = 1+\frac{1-2^m}{1-2} = 2^m\text{.}\)</span></p></div></div>
</div></article><article class="task exercise-like" id="task-1745"><h6 class="heading"><span class="codenumber">(iv)</span></h6>
<p id="p-5207">Explain why <span class="process-math">\(B\)</span> is not an invertible matrix. Notice that <span class="process-math">\(A\)</span> and <span class="process-math">\(B\)</span> differ by a single entry, and that <span class="process-math">\(A\)</span> is invertible and <span class="process-math">\(B\)</span> is not. Let us examine how close <span class="process-math">\(A\)</span> is to <span class="process-math">\(B\text{.}\)</span> Calculate <span class="process-math">\(|| A - B ||_F\text{?}\)</span> What happens to <span class="process-math">\(||A - B||_F\)</span> as <span class="process-math">\(n\)</span> goes to infinity? How close can an invertible matrix be to becoming non-invertible?</p></article></article></article><article class="exercise exercise-like" id="exercise-303"><h4 class="heading"><span class="codenumber">5<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-513"><p id="p-5208">Let <span class="process-math">\(A = \left[ \begin{array}{crr} 1\amp 0\amp 0 \\ 0\amp 1\amp -1 \\ 0\amp -1\amp 1 \end{array}  \right]\text{.}\)</span> In this exercise we find a matrix <span class="process-math">\(B\)</span> so that <span class="process-math">\(B^2 = A\text{,}\)</span> that is, find a square root of the matrix <span class="process-math">\(A\text{.}\)</span></p></div>
<article class="task exercise-like" id="task-1746"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5209">Find the eigenvalues and corresponding eigenvectors for <span class="process-math">\(A\)</span> and <span class="process-math">\(A^{\tr}A\text{.}\)</span> Explain what you see.</p></article><article class="task exercise-like" id="task-1747"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5211">Find a matrix <span class="process-math">\(V\)</span> that orthogonally diagonalizes <span class="process-math">\(A^{\tr}A\text{.}\)</span></p></article><article class="task exercise-like" id="task-1748"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-5213"><a href="" class="xref" data-knowl="./knowl/ex_7_c_Symmetric_SVD.html" title="Exercise 8">Exercise 8</a> in <a href="chap_SVD.html" class="internal" title="Chapter 29: The Singular Value Decomposition">Chapter 29</a> shows if <span class="process-math">\(U \Sigma V^{\tr}\)</span> is a singular value decomposition for a symmetric matrix <span class="process-math">\(A\text{,}\)</span> then so is <span class="process-math">\(V \Sigma V^{\tr}\text{.}\)</span> Recall that <span class="process-math">\(A^n = \left(V \Sigma V^{\tr}\right)^n = V \Sigma^n V^{\tr}\)</span> for any positive integer <span class="process-math">\(n\text{.}\)</span> We can exploit this idea to define <span class="process-math">\(\sqrt{A}\)</span> to be the matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/ex_7_c_Symmetric_SVD.html ./knowl/chap_SVD.html">
\begin{equation*}
V \Sigma^{1/2}V^{\tr}\text{,}
\end{equation*}
</div>
<p class="continuation">where <span class="process-math">\(\Sigma^{1/2}\)</span> is the matrix whose diagonal entries are the square roots of the corresponding entries of <span class="process-math">\(\Sigma\text{.}\)</span> Let <span class="process-math">\(B = \sqrt{A}\text{.}\)</span> Calculate <span class="process-math">\(B\)</span> and show that <span class="process-math">\(B^2 = A\text{.}\)</span></p></article><article class="task exercise-like" id="task-1749"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<p id="p-5215">Why was it important that <span class="process-math">\(A\)</span> be a symmetric matrix for this process to work, and what had to be true about the eigenvalues of <span class="process-math">\(A\)</span> for this to work?</p></article><article class="task exercise-like" id="task-1750"><h5 class="heading"><span class="codenumber">(e)</span></h5>
<p id="p-5217">Can you extend the process in this exercise to find a cube root of <span class="process-math">\(A\text{?}\)</span></p></article></article><article class="exercise exercise-like" id="exercise-304"><h4 class="heading"><span class="codenumber">6<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-514"><p id="p-5219">Let <span class="process-math">\(A\)</span> be an <span class="process-math">\(m \times n\)</span> matrix with singular value decomposition <span class="process-math">\(U \Sigma V^{\tr}\text{.}\)</span> Let <span class="process-math">\(A^{+}\)</span> be defined as in <a href="" class="xref" data-knowl="./knowl/eq_pseudoinverse.html" title="Equation 30.10">(30.10)</a>. In this exercise we prove the remaining parts of <a href="" class="xref" data-knowl="./knowl/thm_7_d_pseudoinverse.html" title="Theorem 30.6: The Moore-Penrose Conditions">Theorem 30.6</a>.</p></div>
<article class="task exercise-like" id="task-1751"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5220">Show that <span class="process-math">\((AA^{+})^{\tr} = AA^{+}\text{.}\)</span></p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-75" id="hint-75"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-75"><div class="hint solution-like"><p id="p-5221"><span class="process-math">\(\Sigma \Sigma^+\)</span> is a symmetric matrix.</p></div></div>
</div></article><article class="task exercise-like" id="task-1752"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5222">Show that <span class="process-math">\((A^{+}A)^{\tr} = A^{+}A\text{.}\)</span></p></article></article><article class="exercise exercise-like" id="exercise-305"><h4 class="heading"><span class="codenumber">7<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-515"><p id="p-5223">In this exercise we show that the pseudoinverse of a matrix is the unique matrix that satisfies the Moore-Penrose conditions. Let <span class="process-math">\(A\)</span> be an <span class="process-math">\(m \times n\)</span> matrix with singular value decomposition <span class="process-math">\(U \Sigma V^{\tr}\)</span> and pseudoinverse <span class="process-math">\(X = V\Sigma^{+}U^{\tr}\text{.}\)</span> To show that <span class="process-math">\(A^{+}\)</span> is the unique matrix that satisfies the Moore-Penrose conditions, suppose that there is another matrix <span class="process-math">\(Y\)</span> that also satisfies the Moore-Penrose conditions.</p></div>
<article class="task exercise-like" id="task-1753"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5224">Show that <span class="process-math">\(X = YAX\text{.}\)</span></p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-76" id="hint-76"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-76"><div class="hint solution-like"><p id="p-5225">Use the fact that <span class="process-math">\(X= XAX\text{.}\)</span></p></div></div>
</div></article><article class="task exercise-like" id="task-1754"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5226">Show that <span class="process-math">\(Y = YAX\text{.}\)</span></p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-77" id="hint-77"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-77"><div class="hint solution-like"><p id="p-5227">Use the fact that <span class="process-math">\(Y= YAY\text{.}\)</span></p></div></div>
</div></article><article class="task exercise-like" id="task-1755"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-5228">How do the results of parts (a) and (b) show that <span class="process-math">\(A^{+}\)</span> is the unique matrix satisfying the Moore-Penrose conditions?</p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-78" id="hint-78"><span class="type">Hint</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-78"><div class="hint solution-like"><p id="p-5229">Compare the results of (a) and (b).</p></div></div>
</div></article></article><article class="exercise exercise-like" id="exercise-306"><h4 class="heading"><span class="codenumber">8<span class="period">.</span></span></h4>
<p id="p-5230">Find the pseudoinverse of the <span class="process-math">\(m \times n\)</span> zero matrix <span class="process-math">\(A=0\text{.}\)</span> Explain the conclusion.</p></article><article class="exercise exercise-like" id="exercise-307"><h4 class="heading"><span class="codenumber">9<span class="period">.</span></span></h4>
<p id="p-5231">In all of the examples that we have done finding a singular value decomposition of a matrix, it has been the case (though we haven't mentioned it), that if <span class="process-math">\(A\)</span> is an <span class="process-math">\(m \times n\)</span> matrix, then <span class="process-math">\(\rank(A) = \rank\left(A^{\tr}A\right)\text{.}\)</span> Prove this result.</p></article><article class="exercise exercise-like" id="exercise-308"><h4 class="heading"><span class="codenumber">10<span class="period">.</span></span></h4>
<div class="introduction" id="introduction-516"><p id="p-5233">Label each of the following statements as True or False. Provide justification for your response.</p></div>
<article class="task exercise-like" id="task-1756"><h5 class="heading">
<span class="codenumber">(a)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-5234">A matrix has a pseudo-inverse if and only if the matrix is singular.</p></article><article class="task exercise-like" id="task-1757"><h5 class="heading">
<span class="codenumber">(b)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-5236">The pseudoinverse of an invertible matrix <span class="process-math">\(A\)</span> is the matrix <span class="process-math">\(A^{-1}\text{.}\)</span></p></article><article class="task exercise-like" id="task-1758"><h5 class="heading">
<span class="codenumber">(c)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-5237">If the columns of <span class="process-math">\(A\)</span> are linearly dependent, then there is no least squares solution to <span class="process-math">\(A\vx = \vb\text{.}\)</span></p></article><article class="task exercise-like" id="task-1759"><h5 class="heading">
<span class="codenumber">(d)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-5239">If the columns of <span class="process-math">\(A\)</span> are linearly independent, then there is a unique least squares solution to <span class="process-math">\(A\vx = \vb\text{.}\)</span></p></article><article class="task exercise-like" id="task-1760"><h5 class="heading">
<span class="codenumber">(e)</span><span class="space"> </span><span class="title">True/False.</span>
</h5>
<p id="p-5240">If <span class="process-math">\(T\)</span> is the matrix transformation defined by a matrix <span class="process-math">\(A\)</span> and <span class="process-math">\(S\)</span> is the matrix transformation defined by <span class="process-math">\(A^{+}\text{,}\)</span> then <span class="process-math">\(T\)</span> and <span class="process-math">\(S\)</span> are inverse transformations.</p></article></article></section><section class="section" id="sec_proj_gps"><h3 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber"></span> <span class="title">Project: GPS and Least Squares</span>
</h3>
<p id="p-5242">In this project we discuss some of the details about how the GPS works. The idea is based on intersections of spheres. To build a basic understanding of the system, we begin with a 2-dimensional example.</p>
<article class="project project-like" id="act_GPS_plane_ex"><h4 class="heading">
<span class="type">Project Activity</span><span class="space"> </span><span class="codenumber">30.11</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-517">
<p id="p-5243">Suppose that there are three base stations <span class="process-math">\(A\text{,}\)</span> <span class="process-math">\(B\text{,}\)</span> and <span class="process-math">\(C\)</span> in <span class="process-math">\(\R^2\)</span> that can send and receive signals from your mobile phone. Assume that <span class="process-math">\(A\)</span> is located at point <span class="process-math">\((-1,-2)\text{,}\)</span> <span class="process-math">\(B\)</span> at point <span class="process-math">\((36,5)\text{,}\)</span> and <span class="process-math">\(C\)</span> at point <span class="process-math">\((16,35)\text{.}\)</span> Also assume that your mobile phone location is point <span class="process-math">\((x,y)\text{.}\)</span> Based on the time that it takes to receive the signals from the three base stations, it can be determined that your distance to base station <span class="process-math">\(A\)</span> is <span class="process-math">\(28\)</span> km, your distance to base station <span class="process-math">\(B\)</span> is <span class="process-math">\(26\)</span> km, and your distance to base station <span class="process-math">\(C\)</span> is <span class="process-math">\(14\)</span> km using a coordinate system with measurements in kilometers based on a reference point chosen to be <span class="process-math">\((0,0)\text{.}\)</span> Due to limitations on the measurement equipment, these measurements all contain some unknown error which we will denote as <span class="process-math">\(z\text{.}\)</span> The goal is to determine your location in <span class="process-math">\(\R^2\)</span> based on this information.</p>
<p id="p-5244">If the distance readings were accurate, then the point <span class="process-math">\((x,y)\)</span> would lie on the circle centered at <span class="process-math">\(A\)</span> of radius <span class="process-math">\(29\text{.}\)</span> The distance from <span class="process-math">\((x,y)\)</span> to base station <span class="process-math">\(A\)</span> can be represented in two different ways: <span class="process-math">\(28\)</span> km and <span class="process-math">\(\sqrt{(x+1)^2 + (y+2)^2}\text{.}\)</span> However, there is some error in the measurements (due to the receiver clock and satellite clocks not being snychronized), so we really have</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\sqrt{(x+1)^2 + (y+2)^2} + z = 28\text{,}
\end{equation*}
</div>
<p class="continuation">where <span class="process-math">\(z\)</span> is the error. Similarly, <span class="process-math">\((x,y)\)</span> must also satisfy</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\sqrt{(x-36)^2 + (y-5)^2} + z = 26
\end{equation*}
</div>
<p class="continuation">and</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\sqrt{(x-16)^2 + (y-35)^2} + z = 14\text{.}
\end{equation*}
</div>
</div>
<article class="task exercise-like" id="task-1761"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5245">Explain how these three equations can be written in the equivalent form</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="mdn-24">
\begin{align}
(x+1)^2+(y+2)^2\amp =(28-z)^2\tag{30.11}\\
(x-36)^2 + (y-5)^2 \amp = (26-z)^2\tag{30.12}\\
(x-16)^2 + (y-35)^2 \amp = (14-z)^2\text{.}\tag{30.13}
\end{align}
</div>
<p class="continuation"><figure class="figure figure-like" id="F_GPS_circles"><div class="image-box" style="width: 50%; margin-left: 25%; margin-right: 25%;"><img src="external/7_d_Ex2_D_circles.svg" role="img" class="contained"></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">30.12<span class="period">.</span></span><span class="space"> </span>Intersections of circles.</figcaption></figure></p></article><article class="task exercise-like" id="task-1762"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5246">If all measurements were accurate, your position would be at the intersection of the circles centered at <span class="process-math">\(A\)</span> with radius <span class="process-math">\(28\)</span> km, centered at <span class="process-math">\(B\)</span> with radius <span class="process-math">\(26\)</span> km, and centered at <span class="process-math">\(C\)</span> with radius <span class="process-math">\(14\)</span> km as shown in <a href="" class="xref" data-knowl="./knowl/F_GPS_circles.html" title="Figure 30.12">Figure 30.12</a>. Even though the figure might seem to imply it, because of the error in the measurements the three circles do not intersect in one point. So instead, we want to find the best estimate of a point of intersection that we can. The system of <a href="" class="xref" data-knowl="./knowl/eq_GPS_2D_1.html" title="Equation 30.11">equations (30.11)</a>, <a href="" class="xref" data-knowl="./knowl/eq_GPS_2D_2.html" title="Equation 30.12">(30.12)</a>, and <a href="" class="xref" data-knowl="./knowl/eq_GPS_2D_3.html" title="Equation 30.13">(30.13)</a> is non-linear and can be difficult to solve, if it even has a solution. To approximate a solution, we can linearize the system. To do this, show that if we subtract corresponding sides of equation  <a href="" class="xref" data-knowl="./knowl/eq_GPS_2D_1.html" title="Equation 30.11">(30.11)</a> from <a href="" class="xref" data-knowl="./knowl/eq_GPS_2D_2.html" title="Equation 30.12">(30.12)</a> and expand both sides, we can obtain the linear equation</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/F_GPS_circles.html ./knowl/eq_GPS_2D_1.html ./knowl/eq_GPS_2D_2.html ./knowl/eq_GPS_2D_3.html ./knowl/eq_GPS_2D_1.html ./knowl/eq_GPS_2D_2.html">
\begin{equation*}
37x + 7y + 2z = 712
\end{equation*}
</div>
<p class="continuation">in the unknowns <span class="process-math">\(x\text{,}\)</span> <span class="process-math">\(y\text{,}\)</span> and <span class="process-math">\(z\text{.}\)</span></p></article><article class="task exercise-like" id="task-1763"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-5247">Repeat the process in part (b), subtracting <a href="" class="xref" data-knowl="./knowl/eq_GPS_2D_1.html" title="Equation 30.11">(30.11)</a> from <a href="" class="xref" data-knowl="./knowl/eq_GPS_2D_3.html" title="Equation 30.13">(30.13)</a> and show that we can obtain the linear equation</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_GPS_2D_1.html ./knowl/eq_GPS_2D_3.html">
\begin{equation*}
17x + 37y + 14z = 1032
\end{equation*}
</div>
<p class="continuation">in <span class="process-math">\(x\text{,}\)</span> <span class="process-math">\(y\text{,}\)</span> and <span class="process-math">\(z\text{.}\)</span></p></article><article class="task exercise-like" id="task-1764"><h5 class="heading"><span class="codenumber">(d)</span></h5>
<p id="p-5248">We have reduced our system of three non-linear equations to the system</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-211">
\begin{alignat*}{4}
{37}x   \amp {}+{}   \amp {7}y   \amp {}+{}  \amp {2}z   \amp = \amp {}   \amp 712\\
{17}x    \amp {}+{}   \amp {37}y   \amp {}+{}   \amp {14}z  \amp = \amp {}  \amp 1032
\end{alignat*}
</div>
<p class="continuation">of two linear equations in the unknowns <span class="process-math">\(x\text{,}\)</span> <span class="process-math">\(y\text{,}\)</span> and <span class="process-math">\(z\text{.}\)</span> Use technology to find a pseudoinverse of the coefficient matrix of this system. Use the pseudoinverse to find the least squares solution to this system. Does your solution correspond to an approximate point of intersection of the three circles?</p></article></article><p id="p-5249"><a href="" class="xref" data-knowl="./knowl/act_GPS_plane_ex.html" title="Project Activity 30.11">Project Activity 30.11</a> provides the basic idea behind GPS. Suppose you receive a signal from a GPS satellite. The transmission from satellite <span class="process-math">\(i\)</span> provides four pieces of information — a location <span class="process-math">\((x_i,y_i,z_i)\)</span> and a time stamp <span class="process-math">\(t_i\)</span> according to the satellite's atomic clock. The time stamp allows the calculation of the distance between you and the <span class="process-math">\(i\)</span>th satellite. The transmission travel time is calculated by subtracting the current time on the GPS receiver from the satellite's time stamp. Distance is then found by multiplying the transmission travel time by the rate, which is the speed of light <span class="process-math">\(c=299792.458\)</span> km/s.<a href="" data-knowl="" class="id-ref fn-knowl original" data-refid="hk-fn-52" id="fn-52"><sup> 52 </sup></a> So distance is found as <span class="process-math">\(c(t_i-d)\text{,}\)</span> where <span class="process-math">\(d\)</span> is the time at the receiver. This signal places your location within in a sphere of that radius from the center of the satellite. If you receive a signal at the same time from two satellites, then your position is at the intersection of two spheres. As can be seen at left in <a href="" class="xref" data-knowl="./knowl/F_Spheres.html" title="Figure 30.13">Figure 30.13</a>, that intersection is a circle. So your position has been narrowed quite a bit. Now if you receive simultaneous signals from three spheres, your position is narrowed to the intersection of three spheres, or two points as shown at right in <a href="" class="xref" data-knowl="./knowl/F_Spheres.html" title="Figure 30.13">Figure 30.13</a>. So if we could receive perfect information from three satellites, then your location would be exactly determined.</p>
<figure class="figure figure-like" id="F_Spheres"><div class="sidebyside"><div class="sbsrow" style="margin-left:0%;margin-right:0%;">
<div class="sbspanel top" style="width:50%;"><img src="external/7_d_two_spheres.jpg" class="contained"></div>
<div class="sbspanel top" style="width:50%;"><img src="external/7_d_three_spheres.jpg" class="contained"></div>
</div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">30.13<span class="period">.</span></span><span class="space"> </span>Intersections of spheres.</figcaption></figure><p id="p-5250">There is a problem with the above analysis — calculating the distances. These distances are determined by the time it takes for the signal to travel from the satellite to the GPS receiver. The times are measured by the clocks in the satellites and the clocks in the receivers. Since the GPS receiver clock is unlikely to be perfectly synchronized with the satellite clock, the distance calculations are not perfect. In addition, the rate at which the signal travels can change as the signal moves through the ionosphere and the troposphere. As a result, the calculated distance measurements are not exact, and are referred to as <dfn class="terminology">pseudoranges</dfn>. In our calculations we need to factor in the error related to the time discrepancy and other factors. We will incorporate these errors into our measure of <span class="process-math">\(d\)</span> and treat <span class="process-math">\(d\)</span> as an unknown. (Of course, this is all more complicated that is presented here, but this provides the general idea.)</p>
<p id="p-5251">To ensure accuracy, the GPS uses signals from four satellites. Assume a satellite is positioned at point <span class="process-math">\((x_1,y_1,z_1)\)</span> at a distance <span class="process-math">\(d_1\)</span> from the GPS receiver located at point <span class="process-math">\((x,y,z)\text{.}\)</span> The distance can also be measured in two ways: as</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\sqrt{(x-x_1)^2+(y-y_1)^2+(z-z_1)^2}\text{.}
\end{equation*}
</div>
<p class="continuation">and as <span class="process-math">\(c(t_1-d)\text{.}\)</span> So</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
c(t_1-d) = \sqrt{(x-x_1)^2 + (y-y_1)^2 + (z-z_1)^2}\text{.}
\end{equation*}
</div>
<p id="p-5252">Again, we are treating <span class="process-math">\(d\)</span> as an unknown, so this equation has the four unknowns <span class="process-math">\(x\text{,}\)</span> <span class="process-math">\(y\text{,}\)</span> <span class="process-math">\(z\text{,}\)</span> and <span class="process-math">\(d\text{.}\)</span> Using signals from four satellites produces the system of equations</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="mdn-25">
\begin{align}
\sqrt{(x-x_1)^2 + (y-y_1)^2 + (z-z_1)^2} \amp =  c(t_1-d)\tag{30.14}\\
\sqrt{(x-x_2)^2 + (y-y_2)^2 + (z-z_2)^2}  \amp =  c(t_2-d)\tag{30.15}\\
\sqrt{(x-x_3)^2 + (y-y_3)^2 + (z-z_3)^2} \amp =  c(t_3-d)\tag{30.16}\\
\sqrt{(x-x_4)^2 + (y-y_4)^2 + (z-z_4)^2} \amp =  c(t_4-d)\text{.}\tag{30.17}
\end{align}
</div>
<article class="project project-like" id="act_GPS_3D"><h4 class="heading">
<span class="type">Project Activity</span><span class="space"> </span><span class="codenumber">30.12</span><span class="period">.</span>
</h4>
<div class="introduction" id="introduction-518"><p id="p-5253">The system of equations <a href="" class="xref" data-knowl="./knowl/eq_GPS_1_sqrt.html" title="Equation 30.14">(30.14)</a>, <a href="" class="xref" data-knowl="./knowl/eq_GPS_2_sqrt.html" title="Equation 30.15">(30.15)</a>, <a href="" class="xref" data-knowl="./knowl/eq_GPS_3_sqrt.html" title="Equation 30.16">(30.16)</a>, and <a href="" class="xref" data-knowl="./knowl/eq_GPS_4_sqrt.html" title="Equation 30.17">(30.17)</a> is a non-linear system and is difficult to solve, if it even has a solution. We want a method that will provide at least an approximate solution as well as apply if we use more than four satellites. We choose a reference node (say <span class="process-math">\((x_1, y_1, z_1)\)</span>) and make calculations relative to that node as we did in <a href="" class="xref" data-knowl="./knowl/act_GPS_plane_ex.html" title="Project Activity 30.11">Project Activity 30.11</a>.</p></div>
<article class="task exercise-like" id="task-1765"><h5 class="heading"><span class="codenumber">(a)</span></h5>
<p id="p-5254">First square both sides of the equations <a href="" class="xref" data-knowl="./knowl/eq_GPS_1_sqrt.html" title="Equation 30.14">(30.14)</a>, <a href="" class="xref" data-knowl="./knowl/eq_GPS_2_sqrt.html" title="Equation 30.15">(30.15)</a>, <a href="" class="xref" data-knowl="./knowl/eq_GPS_3_sqrt.html" title="Equation 30.16">(30.16)</a>, and <a href="" class="xref" data-knowl="./knowl/eq_GPS_4_sqrt.html" title="Equation 30.17">(30.17)</a> to remove the roots. Then subtract corresponding sides of the new first equation (involving <span class="process-math">\((x_1,y_1,z_1)\)</span>) from the new second equation  (involving <span class="process-math">\((x_2,y_2,z_2)\)</span>) to show that we can obtain the linear equation</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/eq_GPS_1_sqrt.html ./knowl/eq_GPS_2_sqrt.html ./knowl/eq_GPS_3_sqrt.html ./knowl/eq_GPS_4_sqrt.html">
\begin{equation*}
2(x_2-x_1)x + 2(y_2-y_1)y + 2(z_2-z_1)z + 2c^2(t_1-t_2)d = c^2(t_1^2-t_2^2)  - h_1 +h_2\text{,}
\end{equation*}
</div>
<p class="continuation">where <span class="process-math">\(h_i = x_i^2 + y_i^2 + z_i^2\text{.}\)</span> (Note that the unknowns are <span class="process-math">\(x\text{,}\)</span> <span class="process-math">\(y\text{,}\)</span> <span class="process-math">\(z\text{,}\)</span> and <span class="process-math">\(d\)</span> — all other quantities are known.)</p></article><article class="task exercise-like" id="task-1766"><h5 class="heading"><span class="codenumber">(b)</span></h5>
<p id="p-5255">Use the result of part (a) to write a linear system that can be obtained by subtracting the first equation from the third and fourth equations as well.</p></article><article class="task exercise-like" id="task-1767"><h5 class="heading"><span class="codenumber">(c)</span></h5>
<p id="p-5256">The linearizations from part (b) determine a system <span class="process-math">\(A \vx = \vb\)</span> of linear equations. Identify <span class="process-math">\(A\text{,}\)</span> <span class="process-math">\(\vx\text{,}\)</span> and <span class="process-math">\(\vb\text{.}\)</span> Then explain how we can approximate a best solution to this system in the least squares sense.</p></article></article><p id="p-5257">We conclude this project with a final note. At times a GPS receiver may only be able to receive signals from three satellites. In these situations, the receiver can substitute the surface of the Earth as a fourth sphere and continue the computation.</p></section></section><div class="hidden-content tex2jax_ignore" id="hk-fn-49"><div class="fn">For example, as stated in <a class="external" href="http://www2.imm.dtu.dk/~pch/Projekter/tsvd.html" target="_blank"><code class="code-inline tex2jax_ignore">http://www2.imm.dtu.dk/~pch/Projekter/tsvd.html</code></a>, “The SVD [singular value decomposition] has also applications in digital signal processing, e.g., as a method for noise reduction. The central idea is to let a matrix <span class="process-math">\(A\)</span> represent the noisy signal, compute the SVD, and then discard small singular values of <span class="process-math">\(A\text{.}\)</span> It can be shown that the small singular values mainly represent the noise, and thus the rank-<span class="process-math">\(k\)</span> matrix <span class="process-math">\(A_k\)</span> represents a filtered signal with less noise.”</div></div>
<div class="hidden-content tex2jax_ignore" id="hk-fn-50"><div class="fn">
<a href="" class="xref" data-knowl="./knowl/thm_7_d_pseudoinverse.html" title="Theorem 30.6: The Moore-Penrose Conditions">Theorem 30.6</a> is often given as the definition of a pseudoinverse.</div></div>
<div class="hidden-content tex2jax_ignore" id="hk-fn-51"><div class="fn"><code class="code-inline tex2jax_ignore">statista.com/statistics/203064/national-debt-of-the-united-states-per-capita/</code></div></div>
<div class="hidden-content tex2jax_ignore" id="hk-fn-52"><div class="fn">The signals travel in radio waves, which are electromagnetic waves, and travel at the speed of light. Also, <span class="process-math">\(c\)</span> is the speed of light in a vacuum, but atmosphere is not too dense so we assume this value of <span class="process-math">\(c\)</span>
</div></div>
</div></main>
</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/components/prism-core.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/autoloader/prism-autoloader.min.js"></script>
</body>
</html>
