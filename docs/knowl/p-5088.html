<!DOCTYPE html>
<html lang="en-US">
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*       on 2022-03-23T14:44:36-04:00       *-->
<!--*   A recent stable commit (2020-08-09):   *-->
<!--* 98f21740783f166a773df4dc83cab5293ab63a4a *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="robots" content="noindex, nofollow">
</head>
<body class="ignore-math">
<h5 class="heading"><span class="type">Paragraph</span></h5>
<p>The outer product decomposition of <span class="process-math">\(A\)</span> writes <span class="process-math">\(A\)</span> as a sum of rank 1 matrices (the summands <span class="process-math">\(\sigma_i \vu_i \vv_i^{\tr})\text{.}\)</span> Each summand contains some information about the matrix <span class="process-math">\(A\text{.}\)</span> Since <span class="process-math">\(\sigma_1\)</span> is the largest of the singular values, it is reasonable to expect that the summand <span class="process-math">\(A_1 = \sigma_1 \vu_1  \vv_1^{\tr}\)</span> contains the most information about <span class="process-math">\(A\)</span> among all of the summands. To get a measure of how much information <span class="process-math">\(A_1\)</span> contains of <span class="process-math">\(A\text{,}\)</span> we can think of <span class="process-math">\(A\)</span> as simply a long vector in <span class="process-math">\(\R^{mn}\)</span> where we have folded the data into a rectangular array (we will see later why taking the norm as the norm of the vector in <span class="process-math">\(\R^{nm}\)</span> makes sense, but for now, just use this definition). If we are interested in determining the error in approximating an image by a compressed image, it makes sense to use the standard norm in <span class="process-math">\(\R^{mn}\)</span> to determine length and distance, which is really just the Frobenius norm that comes from the Frobenius inner product defined by</p>
<div class="displaymath process-math">
\begin{equation}
\langle U,V \rangle = \sum u_{ij}v_{ij}\text{,}\tag{30.1}
\end{equation}
</div>
<p class="continuation">where <span class="process-math">\(U = [u_{ij}]\)</span> and <span class="process-math">\(V = [v_{ij}]\)</span> are <span class="process-math">\(m \times n\)</span> matrices. (That <a href="" class="xref" data-knowl="./knowl/eq_7_d_Frobenius_ip.html" title="Equation 30.1">(30.1)</a> defines an inner product on the set of all <span class="process-math">\(n \times n\)</span> matrices is left to discuss in a later section.) So in this section all the norms for matrices will refer to the Frobenius norm. Rather than computing the distance between <span class="process-math">\(A_1\)</span> and <span class="process-math">\(A\)</span> to measure the error, we are more interested in the relative error</p>
<div class="displaymath process-math">
\begin{equation*}
\frac{||A-A_1||}{||A||}\text{.}
\end{equation*}
</div>
<span class="incontext"><a href="sec_img_conpress.html#p-5088">in-context</a></span>
</body>
</html>
