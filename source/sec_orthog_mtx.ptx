<?xml version="1.0" encoding="UTF-8" ?>
<section xml:id="sec_orthog_mtx">
  <title>Orthogonal Matrices</title>
  <p>
    We have seen in the diagonalization process that we diagonalize a matrix <m>A</m> with a matrix <m>P</m> whose columns are linearly independent eigenvectors of <m>A</m>.
    In general, calculating the inverse of the matrix whose columns are eigenvectors of <m>A</m> in the diagonalization process can be time consuming,
    but if the columns form an orthonormal set,
    then the calculation is very straightforward.
  </p>

  <activity xml:id="act_6_b_orthogonal_inverse">
    <introduction>
    <p>
      Let <m>\vu_1 = \frac{1}{3}[2 \ 1 \ 2]^{\tr}</m>,
      <m>\vu_2 = \frac{1}{3}[-2 \ 2 \ 1]^{\tr}</m>,
      and <m>\vu_3 = \frac{1}{3}[1 \ 2 \ -2]^{\tr}</m>.
      It is not difficult to see that the set
      <m>\{\vu_1, \vu_2, \vu_3\}</m> is an orthonormal basis for <m>\R^3</m>.
      Let
      <me>
        A = [\vu_1 \ \vu_2 \ \vu_3] = \frac{1}{3} \left[ \begin{array}{crr} 2\amp -2\amp 1\\1\amp 2\amp 2\\2\amp 1\amp -2 \end{array}  \right]
      </me>. 
    </p>
    </introduction>
      <task>
        <statement>
          <p>
            Use the definition of the matrix-matrix product to find the entries of the second row of the matrix product <m>A^{\tr}A</m>.
            Why should you have expected the result?
            
          </p>
        </statement>
        <hint>
          <p>
            How are the rows of <m>A^{\tr}</m> related to the columns of <m>A</m>?
          </p>
        </hint>
      </task>
      <task>
        <statement>
          <p>
            With the result of part (a) in mind,
            what is the matrix product <m>A^{\tr}A</m>?
            What does this tell us about the relationship between <m>A^{\tr}</m> and <m>A^{-1}</m>?
            Use technology to calculate <m>A^{-1}</m> and confirm your answer.
          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            Suppose <m>P</m> is an <m>n \times n</m> matrix whose columns form an orthonormal basis for <m>\R^n</m>.
            Explain why <m>P^{\tr}P = I_n</m>.
          </p>
        </statement>
      </task>
    
  </activity>

  <p>
    The result of <xref ref="act_6_b_orthogonal_inverse"></xref>
    is that if the columns of a square matrix <m>P</m> form an orthonormal set,
    then <m>P^{-1} = P^{\tr}</m>.
    This makes calculating <m>P^{-1}</m> very easy.
    Note, however,
    that this only works if the columns of <m>P</m> form an orthonormal basis for <m>\Col P</m>.
    You should also note that if <m>P</m> is an
    <m>n \times n</m> matrix satisfying <m>P^{\tr}P = I_n</m>,
    then the columns of <m>P</m> must form an orthonormal set.
    Matrices like this appear quite often and are given a special name.
  </p>

  <definition>
  <idx><h>matrix</h><h>orthogonal</h></idx>
    <statement>
      <p>
        An <term>orthogonal</term>
        matrix is an <m>n \times n</m> matrix <m>P</m> such that <m>P^{\tr}P = I_n</m>.<fn>
        It isn't clear why such matrices are called orthogonal since the columns are actually orthonormal,
        but that is the standard terminology in mathematics.
        </fn>
      </p>
    </statement>
  </definition>

  <activity>
    <introduction>
    <p>
      As a special case,
      we apply the result of <xref ref="act_6_b_orthogonal_inverse"></xref>
      to a <m>2 \times 2</m> rotation matrix <m>P = \left[ \begin{array}{cr} \cos(\theta) \amp -\sin(\theta) \\ \sin(\theta) \amp \cos(\theta) \end{array} \right]</m>.
    </p>
    </introduction>
      <task>
        <statement>
          <p>
            Show that the columns of <m>P</m> form an orthonormal set.
          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            Use the fact that <m>P^{-1} = P^{\tr}</m> to find <m>P^{-1}</m>.
            Explain how this shows that the inverse of a rotation matrix by an angle <m>\theta</m> is just another rotation matrix but by the angle <m>-\theta</m>.
          </p>
        </statement>
      </task>
    
  </activity>

  <p>
    Orthogonal matrices are useful because they satisfy some special properties.
    For example, if <m>P</m> is an orthogonal
    <m>n \times n</m> matrix and <m>\vx, \vy \in \R^n</m>, then
    <me>
      (P\vx) \cdot (P\vy) = (P\vx)^{\tr}(P\vy) = \vx^{\tr}P^{\tr}P\vy = \vx^{\tr}\vy = \vx \cdot \vy
    </me>.
  </p>
  <p>
  <idx><h>isometry</h></idx>
    This property tells us that the matrix transformation <m>T</m> defined by
    <m>T(\vx) = P\vx</m> preserves dot products and, hence,
    orthogonality.
    In addition,
    <me>
      ||P\vx||^2 = P\vx \cdot P\vx = \vx \cdot \vx = ||\vx||^2
    </me>,
    so <m>||P\vx|| = ||\vx||</m>.
    This means that <m>T</m> preserves length.
    Such a transformation is called an <term>isometry</term>
    and it is convenient to work with functions that don't expand or contract things.
    Moreover, if <m>\vx</m> and <m>\vy</m> are nonzero vectors, then
    <me>
      \frac{P\vx \cdot P\vy}{||P\vx|| \ ||P\vy||} = \frac{\vx \cdot \vy}{||\vx|| \ ||\vy||}
    </me>.
  </p>
  <p>
    Thus <m>T</m> also preserves angles.
    Transformations defined by orthogonal matrices are very well behaved transformations.
    To summarize,
  </p>

  <theorem>
    <statement>
      <p>
        Let <m>P</m> be an <m>n \times n</m> orthogonal matrix and let <m>\vx, \vy \in \R^n</m>.
        Then
        <ol>
          <li>
            <p>
              <m>(P\vx) \cdot (P\vy) = \vx \cdot \vy</m>,
            </p>
          </li>
          <li>
            <p>
              <m>||P\vx|| = ||\vx||</m>, and
            </p>
          </li>
          <li>
            <p>
              <m>\frac{P\vx \cdot P\vy}{||P\vx|| \ ||P\vy||} = \frac{\vx \cdot \vy}{||\vx|| \ ||\vy||}</m> if <m>\vx</m> and <m>\vy</m> are nonzero.
            </p>
          </li>
        </ol>
      </p>
    </statement>
  </theorem>

  <p>
    We have discussed orthogonal and orthonormal bases for subspaces of <m>\R^n</m> in this section.
    There are reasonable questions that follow, such as
    <ul>
      <li>
        <p>
          Can we always find an orthogonal
          (or orthonormal)
          basis for any subspace of <m>\R^n</m>?
        </p>
      </li>
      <li>
        <p>
          Given a vector <m>\vv</m> in <m>W</m>,
          can we find an orthogonal basis of <m>W</m> that contain <m>\vv</m>?
        </p>
      </li>
    </ul>

    We will answer these questions in subsequent sections.
  </p>
</section>