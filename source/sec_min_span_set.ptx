<section xml:id="sec_min_span_set">
  <title>Minimal Spanning Sets</title>
  <p>
    It is important to note the differences and connections between linear independence,
    span, and minimal spanning set.
    <ul>
      <li>
        <p>
          The set <m>S = \left\{ \left[ \begin{array}{c} 1 \\ 0 \\ 0 \end{array} \right], \left[ \begin{array}{c} 0 \\ 1 \\ 0 \end{array} \right] \right\}</m> is not a minimal spanning set for <m>\R^3</m> even though <m>S</m> is a linearly independent set.
          Note that <m>S</m> does not span <m>\R^3</m> since the vector
          <m>\left[ \begin{array}{c} 0 \\ 0 \\ 1 \end{array} \right]</m> is not in <m>\Span \ S</m>.
        </p>
      </li>
      <li>
        <p>
          The set <m>T = \left\{ \left[ \begin{array}{c} 1 \\ 0 \\ 0 \end{array}  \right], \left[ \begin{array}{c} 0 \\ 1 \\ 0 \end{array}  \right] , \left[ \begin{array}{c} 0 \\ 0 \\ 1 \end{array}  \right] , \left[ \begin{array}{c} 1 \\ 1 \\ 1 \end{array}  \right] \right\}</m> is not a minimal spanning set for <m>\R^3</m> even though <m>\Span \ T = \R^3</m>.
          Note that
          <me>
            \left[ \begin{array}{c} 1 \\ 1 \\ 1 \end{array}  \right] = \left[ \begin{array}{c} 1 \\ 0 \\ 0 \end{array}  \right] + \left[ \begin{array}{c} 0 \\ 1 \\ 0 \end{array}  \right] + \left[ \begin{array}{c} 0 \\ 0 \\ 1 \end{array}  \right]
          </me>,
          so <m>T</m> is not a linearly independent set.
        </p>
      </li>
      <li>
        <p>
          The set <m>U = \left\{ \left[ \begin{array}{c} 1 \\ 0 \\ 0 \end{array} \right], \left[ \begin{array}{c} 0 \\ 1 \\ 0 \end{array} \right] , \left[ \begin{array}{c} 0 \\ 0 \\ 1 \end{array} \right] \right\}</m> is a minimal spanning set for <m>\R^3</m> since it satisfies both characteristics of a minimal spanning set:
          <m>\Span \ U = \R^3</m> AND <m>U</m> is linearly independent.
        </p>
      </li>
    </ul>
  </p>
  <p>
    The three concepts <mdash/> linear independence,
    span, and minimal spanning set <mdash/> are different.
    The important point to note is that minimal spanning set must be both linearly independent and span the space.
  </p>
  <p>
    To find a minimal spanning set we will often need to find a smallest subset of a given set of vectors that has the same span as the original set of vectors.
    In this section we determine a method for doing so.
  </p>
  <activity xml:id="act_1_f_4">
    <p>
      Let <m>\vv_1 = \left[ \begin{array}{r} -1 \\ 0 \\ 2 \end{array} \right]</m>,
      <m>\vv_2 = \left[ \begin{array}{r} 2 \\ 0 \\ -4 \end{array} \right]</m>,
      <m>\vv_3 = \left[ \begin{array}{c} 0 \\ 1 \\ 3 \end{array} \right]</m>,
      and <m>\vv_4 = \left[ \begin{array}{r} -3 \\ 4 \\ 18 \end{array} \right]</m> in <m>\R^3</m>.
      Assume that the reduced row echelon form of the matrix
      <m>A = \left[ \begin{array}{rrcr} -1\amp 2\amp 0\amp -3 \\ 0\amp 0\amp 1\amp 4 \\ 2\amp -4\amp 3\amp 18 \end{array} \right]</m> is <m>\left[ \begin{array}{crcc} 1\amp -2\amp 0\amp 3 \\ 0\amp 0\amp 1\amp 4 \\ 0\amp 0\amp 0\amp 0 \end{array} \right]</m>.
      <ul>
        <li>
          <p>
            Write the general solution to the homogeneous system <m>A \vx = \vzero</m>,
            where <m>\vx = \left[ \begin{array}{c} x_1 \\ x_2 \\ x_3 \\ x_4 \end{array} \right]</m>.
            Write all linear combinations of <m>\vv_1</m>,
            <m>\vv_2</m>, <m>\vv_3</m>,
            and <m>\vv_4</m> that are equal to <m>\vzero</m>,
            using weights that only involve <m>x_2</m> and <m>x_4</m>.
          </p>
        </li>
        <li>
          <p>
            Explain how we can conveniently choose the weights in the general solution to
            <m>A \vx = \vzero</m> to show that the vector <m>\vv_4</m> is a linear combination of <m>\vv_1</m>,
            <m>\vv_2</m>, and <m>\vv_3</m>.
            What does this tell us about
            <m>\Span \{\vv_1, \vv_2, \vv_3\}</m> and <m>\Span \{\vv_1, \vv_2, \vv_3, \vv_4\}</m>?
          </p>
        </li>
        <li>
          <p>
            Explain how we can conveniently choose the weights in the general solution to
            <m>A \vx = \vzero</m> to show why the vector <m>\vv_2</m> is a linear combination of <m>\vv_1</m> and <m>\vv_3</m>.
            What does this tell us about
            <m>\Span \{\vv_1, \vv_3\}</m> and <m>\Span \{\vv_1, \vv_2, \vv_3 \}</m>?
          </p>
        </li>
        <li>
          <p>
            Is <m>\{\vv_1, \vv_3\}</m> a minimal spanning set for <m>\Span \{\vv_1, \vv_2, \vv_3, \vv_4\}</m>?
            Explain your response.
          </p>
        </li>
      </ul>
    </p>
  </activity>
  <p>
    <xref ref="act_1_f_4">Activity</xref>
    illustrates how we can use a matrix to determine a minimal spanning set for a given set of vectors <m>\{\vv_1, \vv_2, \ldots, \vv_k\}</m> in <m>\R^n</m>.
    <ul>
      <li>
        <p>
          Form the matrix <m>A = [\vv_1 \ \vv_2 \ \cdots \ \vv_k]</m>.
        </p>
      </li>
      <li>
        <p>
          Find the reduced row echelon form
          <m>[B \ | \ \vzero]</m> of <m>[A \ | \ \vzero]</m>.
          If <m>B</m> contains non-pivot columns,
          say for example that the <m>i</m>th column is a non-pivot column,
          then we can choose the weight <m>x_i</m> corresponding to the <m>i</m>th column to be 1 and all weights corresponding to the other non-pivot columns to be 0 to make a linear combination of the columns of <m>A</m> that is equal to <m>\vzero</m>.
          This allows us to write <m>\vv_i</m> as a linear combination of the vectors corresponding to the pivot columns of <m>A</m> as we did in the proof of <xref ref="thm_Independence">Theorem</xref>.
          So every vector corresponding to a non-pivot column is in the span of the set of vectors corresponding to the pivot columns.
          The vectors corresponding to the pivot columns are linearly independent,
          since the matrix with those columns has every column as a pivot column.
          Thus, the set of vectors corresponding to the pivot columns of <m>A</m> forms a minimal spanning set for <m>\{\vv_1, \vv_2, \ldots, \vv_k\}</m>.
        </p>
      </li>
    </ul>
  </p>
  <p>
    <em>IMPORTANT NOTE!</em> The set of pivot columns of the reduced row echelon form of <m>A</m> will normally not have the same span as the set of columns of <m>A</m>,
    so it is critical that we use columns of <m>A</m>, NOT <m>B</m> in our minimal spanning set.
  </p>
  <activity xml:id="act_1_f_5">
    <p>
      Find a minimal spanning set for the span of the set
      <me>
        \left\{ \left[ \begin{array}{c} 1 \\ 1 \\ 0 \\ 0 \end{array}  \right], \left[ \begin{array}{c} 2 \\ 3 \\ 0 \\ 0 \end{array}  \right], \left[ \begin{array}{c} 0 \\ 1 \\ 2 \\ 0 \end{array}  \right], \left[ \begin{array}{c} 4 \\ 1 \\ 0 \\ 0 \end{array}  \right] \right\}
      </me>.
    </p>
  </activity>
  <p>
    <xref ref="act_1_f_4">Activity</xref>
    also illustrates a general process by which we can find a minimal spanning set <mdash/> that is the smallest subset of vectors that has the same span.
    This process will be useful later when we consider vectors in arbitrary vector spaces.
    The idea is that if we can write one of the vectors in a set <m>S</m> as a linear combination of the remaining vectors,
    then we can remove that vector from the set and maintain the same span.
    In other words,
    begin with the span of a set <m>S</m> and follow these steps:
    <ul>
      <li>
        <title>Step 1</title>
        <p>
          If <m>S</m> is a linearly independent set,
          we already have a minimal spanning set.
        </p>
      </li>
      <li>
        <title>Step 2</title>
        <p>
          If <m>S</m> is not a linearly independent set,
          then one of the vectors in <m>S</m> is a linear combination of the others.
          Remove that vector from <m>S</m> to obtain a new set <m>T</m>.
          It will be the case that <m>\Span \ T = \Span \ S</m>.
        </p>
      </li>
      <li>
        <title>Step 3</title>
        <p>
          If <m>T</m> is a linearly independent set,
          then <m>T</m> is a minimal spanning set.
          If not, repeat steps 2 and 3 for the set <m>T</m> until you arrive at a linearly independent set.
        </p>
      </li>
    </ul>
  </p>
  <p>
    This process is guaranteed to stop as long as the set contains at least one nonzero vector.
    A verification of the statement in Step 2 that
    <m>\Span \ T = \Span \ S</m> is given in the next theorem.
  </p>
  <theorem xml:id="thm_minimal_spanning_set">
    <statement>
      <p>
        Let <m>\{\vv_1, \vv_2, \ldots, \vv_k\}</m> be a set of vectors in <m>\R^n</m> so that for some <m>i</m> between 1 and <m>k</m>,
        <m>\vv_i</m> is in <m>\Span \{\vv_1, \vv_2, \ldots, \vv_{i-1}, \vv_{i+1}, \ldots, \vv_k\}</m>.
        Then
        <me>
          \Span \{\vv_1, \vv_2, \ldots, \vv_k\} = \Span \{\vv_1, \vv_2, \ldots, \vv_{i-1}, \vv_{i+1}, \ldots, \vv_k\}
        </me>.
      </p>
    </statement>
  </theorem>
  <proof>
    <p>
      Let <m>\{\vv_1, \vv_2, \ldots, \vv_k\}</m> be a set of vectors in <m>\R^n</m> so that <m>\vv_i</m> is in the span of <m>\vv_1</m>,
      <m>\vv_2</m>,
      <m>\ldots</m>, <m>\vv_{i-1}</m>,
      <m>\vv_{i+1}</m>, <m>\ldots</m>,
      and <m>\vv_k</m> for some <m>i</m> between 1 and <m>k</m>.
      To show that
      <me>
        \Span \{\vv_1, \vv_2, \ldots, \vv_k\} = \Span \{\vv_1,  \ldots, \vv_{i-1}, \vv_{i+1}, \ldots, \vv_k\}
      </me>,
      we need to show that
      <ol>
        <li>
          <p>
            every vector in <m>\Span \{\vv_1, \vv_2, \ldots, \vv_k\}</m> is in <m>\Span\{\vv_1, \ldots, \vv_{i-1}, \vv_{i+1}, \ldots, \vv_k\}</m>, and
          </p>
        </li>
        <li>
          <p>
            every vector in <m>\Span \{\vv_1, \ldots, \vv_{i-1}, \vv_{i+1}, \ldots, \vv_k\}</m> is in <m>\Span \{\vv_1, \ldots, \vv_k\}</m>.
          </p>
        </li>
      </ol>
    </p>
    <p>
      Let us consider the second containment.
      Let <m>\vx</m> be a vector in the span of <m>\vv_1</m>,
      <m>\vv_2</m>, <m>\ldots</m>,
      <m>\vv_{i-1}</m>, <m>\vv_{i+1}</m>,
      <m>\ldots</m>, and <m>\vv_k</m>.
      Then
      <me>
        \vx = x_1\vv_1 + x_2\vv_2 + \cdots + x_{i-1}\vv_{i-1} + x_{i+1}\vv_{i+1} + \cdots + x_k\vv_k
      </me>
      for some scalars <m>x_1</m>, <m>x_2</m>, <m>\ldots</m>,
      <m>x_{i-1}</m>, <m>x_{i+1}</m>,
      <m>\ldots</m>, <m>x_k</m>.
      Note that
      <me>
        \vx = x_1\vv_1 + x_2\vv_2 + \cdots + x_{i-1}\vv_{i-1} + (0)\vv_i + x_{i+1}\vv_{i+1} + \cdots + x_k\vv_k
      </me>
      as well, so <m>\vx</m> is in <m>\Span \{\vv_1, \vv_2, \ldots, \vv_k\}</m>.
      Thus,
      <me>
        \Span \{\vv_1, \vv_2, \ldots, \vv_{i-1}, \vv_{i+1}, \ldots, \vv_k\} \subseteq  \Span \{\vv_1, \vv_2, \ldots, \vv_k\}
      </me>.
    </p>
    <p>
      (This same argument shows a more general statement that if <m>S</m> is a subset of <m>T</m>,
      then <m>\Span \ S \subseteq \Span \ T</m>.)
    </p>
    <p>
      Now we demonstrate the first containment.
      Here we need the assumption that <m>\vv_i</m> is in <m>\Span \{\vv_1</m>,
      <m>\vv_2</m>,
      <m>\ldots</m>, <m>\vv_{i-1}</m>,
      <m>\vv_{i+1}</m>, <m>\ldots</m>,
      <m>\vv_k\}</m> for some <m>i</m> between 1 and <m>k</m>.
      That assumption gives us
      <men xml:id="eq_lin_depend_containment">
        \vv_i = c_1 \vv_1 + c_2\vv_2 + \cdots + c_{i-1}\vv_{i-1} + c_{i+1}\vv_{i+1} + \cdots + c_k\vv_k
      </men>
      for some scalars <m>c_1</m>, <m>c_2</m>, <m>\ldots</m>,
      <m>c_{i-1}</m>, <m>c_{i+1}</m>,
      <m>\ldots</m>, <m>c_k</m>.
      Now let <m>\vx</m> be a vector in the span of <m>\vv_1</m>,
      <m>\vv_2</m>, <m>\ldots</m>, <m>\vv_k</m>.
      Then
      <me>
        \vx = x_1\vv_1 + x_2\vv_2 + \cdots + x_k\vv_k
      </me>
      for some scalars <m>x_1</m>, <m>x_2</m>,
      <m>\ldots</m>, <m>x_k</m>.
      Substituting from <xref ref="eq_lin_depend_containment"/> shows that
      <md>
        <mrow>\vx \amp = x_1\vv_1 + x_2\vv_2 + \cdots + x_k\vv_k</mrow>
        <mrow>\amp = x_1\vv_1 + x_2\vv_2 + \cdots + x_{i-1} \vv_{i-1} + x_i \vv_i + x_{i+1}\vv_{i+1} + \cdots + x_k\vv_k</mrow>
        <mrow>\amp = x_1\vv_1 + x_2\vv_2 + \cdots + x_{i-1} \vv_{i-1}</mrow>
        <mrow>\amp \qquad + x_i[c_1 \vv_1 + c_2\vv_2 + \cdots + c_{i-1}\vv_{i-1} + c_{i+1}\vv_{i+1} + \cdots + c_k\vv_k]</mrow>
        <mrow>\amp \qquad + x_{i+1}\vv_{i+1} + \cdots + x_k\vv_k</mrow>
        <mrow>\amp = (x_1+x_ic_1)\vv_1 + (x_2+x_ic_2)\vv_2 + \cdots + (x_{i-1}+x_ic_{i-1}) \vv_{i-1}</mrow>
        <mrow>\amp \qquad + (x_{i+1}+x_ic_{i+1}) \vv_{i+1} \cdots + (x_k+x_ic_k)\vv_k</mrow>
      </md>.
    </p>
    <p>
      So <m>\vx</m> is in <m>\Span \{\vv_1, \vv_2, \ldots, \vv_{i-1}, \vv_{i+1}, \ldots, \vv_k\}</m> and
      <me>
        \Span \{\vv_1, \vv_2, \ldots, \vv_k\} \subseteq \Span \{\vv_1, \vv_2, \ldots, \vv_{i-1}, \vv_{i+1}, \ldots, \vv_k\}
      </me>.
    </p>
    <p>
      Since the two sets are subsets of each other, they must be equal sets.
      We conclude that
      <me>
        \Span \{\vv_1, \vv_2, \ldots, \vv_k\} = \Span \{\vv_1, \vv_2, \ldots, \vv_{i-1}, \vv_{i+1}, \ldots, \vv_k\}
      </me>.
    </p>
  </proof>
  <p>
    The result of <xref ref="thm_minimal_spanning_set">Theorem</xref>
    is that if we have a finite set <m>S</m> of vectors in <m>\R^n</m>,
    we can eliminate those vectors that are linear combinations of others until we obtain a smallest set of vectors that still has the same span.
    As mentioned earlier, we call such a minimal spanning set a basis.
  </p>
  <definition xml:id="def_1_f_basis">
    <statement>
      <p>
        Let <m>S</m> be a set of vectors in <m>\R^n</m>.
        A subset <m>B</m> of <m>S</m> is a <term>basis</term>
            <idx><h>basis</h></idx>
        for <m>\Span \ S</m> if <m>B</m> is linearly independent and <m>\Span \ B = \Span \ S</m>.
      </p>
    </statement>
  </definition>
  <p>
    <em>IMPORTANT NOTE: </em> A basis is defined by two characteristics.
    A basis must span the space in question and a basis must be a linearly independent set.
    It is the linear independence that makes a basis a
    <em>minimal</em> spanning set.
  </p>
  <p>
    We have worked with a familiar basis in <m>\R^2</m> throughout our mathematical careers.
    A vector <m>\left[ \begin{array}{c} a \\ b \end{array}  \right]</m> in <m>\R^2</m> can be written as
    <me>
      \left[ \begin{array}{c} a \\ b \end{array}  \right] = a\left[ \begin{array}{c} 1 \\ 0 \end{array}  \right] + b\left[ \begin{array}{c} 0 \\ 1 \end{array}  \right]
    </me>.
  </p>
  <p>
    So the set <m>\{\ve_1, \ve_2\}</m>,
    where <m>\ve_1 = \left[ \begin{array}{c} 1 \\ 0 \end{array} \right]</m> and <m>\ve_2 = \left[ \begin{array}{c} 0 \\ 1 \end{array} \right]</m> spans <m>\R^2</m>.
    Since the columns of <m>[\ve_1 \ \ve_2]</m> are linearly independent,
    so is the set <m>\{\ve_1, \ve_2\}</m>.
    Therefore, the set <m>\{\ve_1, \ve_2\}</m> is a basis for <m>\R^2</m>.
    The vector <m>\ve_1</m> is in the direction of the positive <m>x</m>-axis and the vector <m>\ve_2</m> is in the direction of the positive <m>y</m>-axis,
    so decomposing a vector <m>\left[ \begin{array}{c} a \\ b \end{array} \right]</m> as a linear combination of <m>\ve_1</m> and <m>\ve_2</m> is akin to identifying the vector with the point <m>(a,b)</m> as we discussed earlier.
    The set <m>\{\ve_1, \ve_2\}</m> is called the
    <em>standard basis</em> for <m>\R^2</m>.
  </p>
  <p>
    This idea is not restricted to <m>\R^2</m>.
    Consider the vectors
    <me>
      \ve_1 = \left[ \begin{array}{c} 1\\0\\0 \\ \vdots \\ 0 \\ 0 \end{array}  \right], \ \ve_2 = \left[ \begin{array}{c} 0\\1\\0 \\ \vdots \\ 0 \\ 0 \end{array}  \right], \ \cdots, \ \ve_n = \left[ \begin{array}{c} 0\\0\\0 \\ \vdots \\ 0 \\ 1 \end{array}  \right]
    </me>
    in <m>\R^n</m>.
    That is, the vector <m>\ve_i</m> is the vector with a 1 in the <m>i</m>th position and 0s everywhere else.
    Since the matrix <m>[\ve_1 \ \ve_2 \ \cdots \ \ve_n]</m> has a pivot in each row and column,
    the set <m>\{\ve_1, \ve_2, \ldots, \ve_n\}</m> is a basis for <m>\R^n</m>.
    The set <m>\{\ve_1, \ve_2, \ldots, \ve_n\}</m> is called the
    <em>standard basis</em><idx><h>standard basis</h><h>for <m>\R^n</m></h></idx> for <m>\R^n</m>.
  </p>
  <p>
    As we will see later, bases<fn>
    The plural of basis is bases.
    </fn> are of fundamental importance in linear algebra in that bases will allow us to define the dimension of a vector space and will provide us with coordinate systems.
  </p>
  <p>
    We conclude this section with an important theorem that is similar <xref ref="thm_IMT_1_e">Theorem</xref>.
  </p>
  <theorem xml:id="thm_IMT_1_f">
    <statement>
      <p>
        Let <m>A</m> be an <m>m \times n</m> matrix.
        The following statements are equivalent.
        <ol>
          <li>
            <p>
              The matrix equation <m>A \vx = \vb</m> has a unique solution for every vector <m>\vb</m> in the span of the columns of <m>A</m>.
            </p>
          </li>
          <li>
            <p>
              The matrix equation <m>A \vx = \vzero</m> has the unique solution <m>\vx = \vzero</m>.
            </p>
          </li>
          <li>
            <p>
              The columns of <m>A</m> are linearly independent.
            </p>
          </li>
          <li>
            <p>
              The matrix <m>A</m> has a pivot position in each column.
            </p>
          </li>
        </ol>
      </p>
    </statement>
  </theorem>
</section>