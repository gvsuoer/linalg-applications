<?xml version="1.0" encoding="UTF-8" ?>
<section xml:id="sec_dot_prod_exer">
  <title>Exercises</title>
  <ol>
    <li>
      <p>
        For each of the following pairs of vectors,
        find <m>\vu \cdot \vv</m>,
        calculate the angle between <m>\vu</m> and <m>\vv</m>,
        determine if <m>\vu</m> and <m>\vv</m> are orthogonal,
        find <m>||\vu||</m> and <m>||\vv||</m>,
        calculate the distance between <m>\vu</m> and <m>\vv</m>,
        and determine the orthogonal projection of <m>\vu</m> onto <m>\vv</m>.
        <ul>
          <li>
            <p>
              <m>\vu = [1 \ 2]^{\tr}</m>, <m>\vv = [-2 \ 1]^{\tr}</m>
            </p>
          </li>
          <li>
            <p>
              <m>\vu = [2 \ -2]^{\tr}</m>, <m>\vv = [1 \ -1]^{\tr}</m>
            </p>
          </li>
          <li>
            <p>
              <m>\vu = [2 \ -1]^{\tr}</m>, <m>\vv = [1 \ 3]^{\tr}</m>
            </p>
          </li>
          <li>
            <p>
              <m>\vu = [1 \ 2 \ 0]^{\tr}</m>,
              <m>\vv = [-2 \ 1 \ 1]^{\tr}</m>
            </p>
          </li>
          <li>
            <p>
              <m>\vu = [0 \ 0 \ 1]^{\tr}</m>,
              <m>\vv = [1 \ 1 \ 1]^{\tr}</m>
            </p>
          </li>
        </ul>
      </p>
    </li>
    <li>
      <p>
        Given <m>\vu=[2 \ 1\ 2]^\tr</m>,
        find a vector <m>\vv</m> so that the angle between <m>\vu</m> and <m>\vv</m> is
        <m>60^\circ</m> and the orthogonal projection of <m>\vv</m> onto <m>\vu</m> has length 2.
      </p>
    </li>
    <li>
      <p>
        For which value(s) of <m>h</m> is the angle between <m>[1\ 1\ h]^\tr</m> and
        <m>[1\ 2\ 1]^\tr</m> equal to <m>60^\circ</m>?
      </p>
    </li>
    <li>
      <p>
        Let <m>A = [a_{ij}]</m> be a
        <m>k \times m</m> matrix with rows <m>\vr_1</m>,
        <m>\vr_2</m>, <m>\ldots</m>, <m>\vr_k</m>,
        and let <m>B = [\vb_1 \ \vb_2 \ \cdots \ \vb_n]</m> be an
        <m>m \times n</m> matrix with columns <m>\vb_1</m>, <m>\vb_2</m>,
        <m>\ldots</m>, <m>\vb_n</m>.
        Show that we can write the matrix product <m>AB</m> in a shorthand way as <m>AB = [\vr_i \cdot \vb_j]</m>.
      </p>
    </li>
    <li>
      <p>
        Let <m>A</m> be an <m>m \times n</m>,
        <m>\vu</m> a vector in <m>\R^n</m> and <m>\vv</m> a vector in <m>\R^m</m>.
        Show that
        <me>
          A\vu \cdot \vv = \vu \cdot A^{\tr} \vv
        </me>.
      </p>
    </li>
    <li>
      <p>
        Let <m>\vu</m>,
        <m>\vv</m>, and <m>\vw</m> be vectors in <m>\R^n</m>.
        Show that
        <ul>
          <li>
            <p>
              <m>(\vu + \vv) \cdot \vw = (\vu \cdot \vw) + (\vv \cdot \vw)</m> (the dot product
              <em>distributes over vector addition</em>)
            </p>
          </li>
          <li>
            <p>
              If <m>c</m> is an arbitrary constant,
              then <m>(c\vu) \cdot \vv = \vu \cdot (c\vv) = c(\vu \cdot \vv)</m>
            </p>
          </li>
        </ul>
      </p>
    </li>
    <li xml:id="ex_Pyth_Thm">
      <p>
        The Pythagorean Theorem states that if <m>a</m> and <m>b</m> are the lengths of the legs of a right triangle whose hypotenuse has length <m>c</m>,
        then <m>a^2+b^2=c^2</m>.
        If we think of the legs as defining vectors <m>\vu</m> and <m>\vv</m>,
        then the hypotenuse is the vector <m>\vu+\vv</m> and we can restate the Pythagorean Theorem<idx><h>Pythagorean Theorem in <m>\R^n</m></h></idx> as
        <me>
          ||\vu+\vv||^2 = ||\vu||^2+||\vv||^2
        </me>.
        In this exercise we show that this result holds in any dimension.
        <ul>
          <li>
            <p>
              Let <m>\vu</m> and <m>\vv</m> be orthogonal vectors in <m>\R^n</m>.
              Show that <m>||\vu+\vv||^2 = ||\vu||^2+||\vv||^2</m>.
              <hint>
                <p>
                  Rewrite <m>||\vu+\vv||^2</m> using the dot product.
                </p>
              </hint>
            </p>
          </li>
          <li>
            <p>
              Must it be true that if <m>\vu</m> and <m>\vv</m> are vectors in <m>\R^n</m> with <m>||\vu+\vv||^2 = ||\vu||^2+||\vv||^2</m>,
              then <m>\vu</m> and <m>\vv</m> are orthogonal?
              If not, provide a counterexample.
              If true, verify the statement.
            </p>
          </li>
        </ul>
      </p>
    </li>
    <li xml:id="ex_Cauchy_Schwarz">
      <p>
        The Cauchy-Schwarz inequality,
    <idx><h>Cauchy-Schwarz inequality</h><h>in <m>\R^n</m></h></idx>
        <men xml:id="eq_6_a_Cauchy_Schwartz">
          |\vu \cdot \vv| \leq ||\vu|| \ \||\vv||
        </men>
        for any vectors <m>\vu</m> and <m>\vv</m> in <m>\R^n</m>,
        is considered one of the most important inequalities in mathematics.
        We verify the Cauchy-Schwarz inequality in this exercise.
        Let <m>\vu</m> and <m>\vv</m> be vectors in <m>\R^n</m>.
        <ul>
          <li>
            <p>
              Explain why the inequality <xref ref="eq_6_a_Cauchy_Schwartz"/> is true if either <m>\vu</m> or <m>\vv</m> is the zero vector.
              As a consequence,
              we assume that <m>\vu</m> and <m>\vv</m> are nonzero vectors for the remainder of this exercise.
            </p>
          </li>
          <li>
            <p>
              Let <m>\vw = \proj_{\vv} \vu = \frac{\vu \cdot \vv}{||\vv||^2} \vv</m> and let <m>\vz = \vu - \vw</m>.
              We know that <m>\vw \cdot \vz = 0</m>.
              Use <xref ref="ex_Pyth_Thm">Exercise</xref> of this section to show that
              <me>
                ||\vu||^2 \geq ||\vw||^2
              </me>.
            </p>
          </li>
          <li>
            <p>
              Now show that <m>||\vw||^2 = \frac{|\vu \cdot \vv|^2}{||\vv||^2}</m>.
            </p>
          </li>
          <li>
            <p>
              Combine parts (b) and (c) to explain why equation <xref ref="eq_6_a_Cauchy_Schwartz"/> is true.
            </p>
          </li>
        </ul>
      </p>
    </li>
    <li>
      <p>
        Let <m>\vu</m> and <m>\vv</m> be vectors in <m>\R^n</m>.
        Then <m>\vu</m>, <m>\vv</m> and <m>\vu+\vv</m> form a triangle.
        We should then expect that the length of any one side of the triangle is smaller than the sum of the lengths of the other sides
        (since the straight line distance is the shortest distance between two points).
        In other words, we expect that
        <men xml:id="eq_6_a_triangle_inequality">
          ||\vu + \vv|| \leq ||\vu|| + ||\vv||
        </men>.
        Equation <xref ref="eq_6_a_triangle_inequality"/> is called the
        <em>Triangle Inequality</em>.
          <idx><h>triangle inequality in <m>\R^n</m></h></idx>
      Use the Cauchy-Schwarz inequality
        (<xref ref="ex_Cauchy_Schwarz">Exercise</xref>)
        to prove the triangle inequality.
      </p>
    </li>
    <li>
      <p>
        Let <m>W</m> be a subspace of <m>\R^n</m> for some <m>n</m>.
        Show that <m>W^{\perp}</m> is also a subspace of <m>\R^n</m>.
      </p>
    </li>
    <li>
      <p>
        Let <m>W</m> be a subspace of <m>\R^n</m>.
        Show that <m>W</m> is a subspace of <m>(W^\perp)^\perp</m>.
      </p>
    </li>
    <li>
      <p>
        If <m>W</m> is a subspace of <m>\R^n</m> for some <m>n</m>,
        what is <m>W \cap W^{\perp}</m>?
        Verify your answer.
      </p>
    </li>
    <li>
      <p>
        Suppose <m>W_1\subseteq W_2</m> are two subspaces of <m>\R^n</m>.
        Show that <m>W_2^\perp \subseteq W_1^\perp</m>.
      </p>
    </li>
    <li>
      <p>
        What are <m>\left(\R^n\right)^{\perp}</m> and <m>\{\vzero\}^{\perp}</m> in <m>\R^n</m>?
        Justify your answers.
      </p>
    </li>
    <li>
      <p>
        Label each of the following statements as True or False.
        Provide justification for your response.
        <ul>
          <li>
            <p>
              <em>True/False</em> The dot product is defined between any two vectors.
            </p>
          </li>
          <li>
            <p>
              <em>True/False</em> If <m>\vu</m> and <m>\vv</m> are vectors in <m>\R^n</m>,
              then <m>\vu \cdot \vv</m> is another vector in <m>\R^n</m>.
            </p>
          </li>
          <li>
            <p>
              <em>True/False</em> If <m>\vu</m> and <m>\vv</m> are vectors in <m>\R^n</m>,
              then <m>\vu \cdot \vv</m> is always non-negative.
            </p>
          </li>
          <li>
            <p>
              <em>True/False</em> If <m>\vv</m> is a vector in <m>\R^n</m>,
              then <m>\vv \cdot \vv</m> is never negative.
            </p>
          </li>
          <li>
            <p>
              <em>True/False</em> If <m>\vu</m> and <m>\vv</m> are vectors in <m>\R^n</m> and <m>\vu \cdot \vv = 0</m>,
              then <m>\vu = \vv = \vzero</m>.
            </p>
          </li>
          <li>
            <p>
              <em>True/False</em> If <m>\vv</m> is a vector in <m>\R^n</m> and
              <m>\vv \cdot \vv = 0</m>, then <m>\vv = \vzero</m>.
            </p>
          </li>
          <li>
            <p>
              <em>True/False</em> The norm of the sum of vectors is the sum of the norms of the vectors.
            </p>
          </li>
          <li>
            <p>
              <em>True/False</em> If <m>\vu</m> and <m>\vv</m> are vectors in <m>\R^n</m>,
              then <m>\proj_{\vv} \vu</m> is a vector in the same direction as <m>\vu</m>.
            </p>
          </li>
          <li>
            <p>
              <em>True/False</em> The only subspace <m>W</m> of <m>\R^n</m> for which <m>W^\perp=\{\vzero\}</m> is <m>W=\R^n</m>.
            </p>
          </li>
          <li>
            <p>
              <em>True/False</em> If a vector <m>\vu</m> is orthogonal to <m>\vv_1</m> and <m>\vv_2</m>,
              then <m>\vu</m> is also orthogonal to <m>\vv_1+\vv_2</m>.
            </p>
          </li>
          <li>
            <p>
              <em>True/False</em> If a vector <m>\vu</m> is orthogonal to <m>\vv_1</m> and <m>\vv_2</m>,
              then <m>\vu</m> is also orthogonal to all linear combinations of <m>\vv_1</m> and <m>\vv_2</m>.
            </p>
          </li>
          <li>
            <p>
              <em>True/False</em> If <m>\vu\neq \vzero</m> and <m>\vv</m> are parallel,
              then the orthogonal projection of <m>\vv</m> onto <m>\vu</m> equals <m>\vv</m>.
            </p>
          </li>
          <li>
            <p>
              <em>True/False</em> If <m>\vu\neq \vzero</m> and <m>\vv</m> are orthogonal,
              then the orthogonal projection of <m>\vv</m> onto <m>\vu</m> equals <m>\vv</m>.
            </p>
          </li>
          <li>
            <p>
              <em>True/False</em> For any vector <m>\vv</m> and <m>\vu\neq \vzero</m>,
              <m>||\proj_\vu \vv|| \leq ||\vv||</m>.
            </p>
          </li>
          <li>
            <p>
              <em>True/False</em> Given an <m>m\times n</m> matrix,
              <m>\dim(\Row A)+\dim(\Row A)^\perp = n</m>.
            </p>
          </li>
          <li>
            <p>
              <em>True/False</em> If <m>A</m> is a square matrix,
              then the columns of <m>A</m> are orthogonal to the vectors in <m>\Nul A</m>.
            </p>
          </li>
          <li>
            <p>
              <em>True/False</em> The vectors in the null space of an
              <m>m \times n</m> matrix are orthogonal to vectors in the row space of <m>A</m>.
            </p>
          </li>
        </ul>
      </p>
    </li>
  </ol>
</section>