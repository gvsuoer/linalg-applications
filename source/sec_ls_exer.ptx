<?xml version="1.0" encoding="UTF-8" ?>
<section xml:id="sec_ls_exer">
  <title>Exercises</title>
  <ol>
    <li>
      <p>
        The University of Denver Infant Study Center investigated whether babies take longer to learn to crawl in cold months,
        when they are often bundled in clothes that restrict their movement,
        than in warmer months.
        The study sought a relationship between babies' first crawling age and the average temperature during the month they first try to crawl
        (about 6 months after birth).
        Some of the data from the study is in <xref ref="T_7_d_infants">Table</xref>.
        Let <m>x</m> represent the temperature in degrees Fahrenheit and <m>C(x)</m> the average crawling age in months.
        <table xml:id="T_7_d_infants">
          <title>Crawling age</title>
          <tabular>
            <row bottom="minor">
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell></cell>
            </row>
            <row>
              <cell><m>x</m></cell>
              <cell>33</cell>
              <cell>37</cell>
              <cell>48</cell>
              <cell>57</cell>
            </row>
            <row bottom="minor">
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell></cell>
            </row>
            <row>
              <cell><m>C(x)</m></cell>
              <cell>33.83</cell>
              <cell>33.35</cell>
              <cell>33.38</cell>
              <cell>32.32</cell>
            </row>
            <row bottom="minor">
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell></cell>
            </row>
          </tabular>
        </table>
        <ul>
          <li>
            <p>
              Find the least squares line to fit this data.
              Plot the data and your line on the same set of axes.
              (We aren't concerned about whether a linear fit is really a good choice outside of this data set,
              we just fit a line to it to see what happens.)
            </p>
          </li>
          <li>
            <p>
              Use your least squares line to predict the average crawling age when the temperature is 65.
            </p>
          </li>
        </ul>
      </p>
    </li>
    <li>
      <p>
        The cost, in cents,
        of a first class postage stamp in years from 1981 to 1995 is shown in <xref ref="T_7_d_stamps">Table</xref>.
        <ul>
          <li>
            <p>
              Find the least squares line to fit this data.
              Plot the data and your line on the same set of axes.
            </p>
          </li>
          <li>
            <p>
              Now find the least squares quadratic approximation to this data.
              Plot the quadratic function on same axes as your linear function.
            </p>
          </li>
          <li>
            <p>
              Use your least squares line and quadratic to predict the cost of a postage stamp in this year.
              Look up the cost of a stamp today and determine how accurate your prediction is.
              Which function gives a better approximation?
              Provide reasons for any discrepancies.
            </p>
          </li>
        </ul>
      </p>
      <table xml:id="T_7_d_stamps">
        <title>Cost of postage</title>
        <tabular>
          <row bottom="minor">
            <cell></cell>
            <cell></cell>
            <cell></cell>
            <cell></cell>
            <cell></cell>
            <cell></cell>
          </row>
          <row>
            <cell>Year</cell>
            <cell>1981</cell>
            <cell>1985</cell>
            <cell>1988</cell>
            <cell>1991</cell>
            <cell>1995</cell>
          </row>
          <row bottom="minor">
            <cell></cell>
            <cell></cell>
            <cell></cell>
            <cell></cell>
            <cell></cell>
            <cell></cell>
          </row>
          <row>
            <cell>Cost</cell>
            <cell>20</cell>
            <cell>22</cell>
            <cell>25</cell>
            <cell>29</cell>
            <cell>32</cell>
          </row>
          <row bottom="minor">
            <cell></cell>
            <cell></cell>
            <cell></cell>
            <cell></cell>
            <cell></cell>
            <cell></cell>
          </row>
        </tabular>
      </table>
    </li>
    <li>
      <p>
        According to <em>The Song of Insects</em>
        by G.W. Pierce (Harvard College Press, 1948) the sound of striped ground crickets chirping,
        in number of chirps per second,
        is related to the temperature.
        So the number of chirps per second could be a predictor of temperature.
        The data Pierce collected is shown in the table and scatterplot below,
        where <m>x</m> is the (average) number of chirps per second and <m>y</m> is the temperature in degrees Fahrenheit.
        <sidebyside>
          <tabular>
            <row bottom="minor">
              <cell></cell>
              <cell></cell>
            </row>
            <row>
              <cell><m>x</m></cell>
              <cell><m>y</m></cell>
            </row>
            <row bottom="minor">
              <cell></cell>
              <cell></cell>
            </row>
            <row>
              <cell>20.0</cell>
              <cell>88.6</cell>
            </row>
            <row bottom="minor">
              <cell></cell>
              <cell></cell>
            </row>
            <row>
              <cell>16.0</cell>
              <cell>71.6</cell>
            </row>
            <row bottom="minor">
              <cell></cell>
              <cell></cell>
            </row>
            <row>
              <cell>19.8</cell>
              <cell>93.3</cell>
            </row>
            <row bottom="minor">
              <cell></cell>
              <cell></cell>
            </row>
            <row>
              <cell>18.4</cell>
              <cell>84.3</cell>
            </row>
            <row bottom="minor">
              <cell></cell>
              <cell></cell>
            </row>
            <row>
              <cell>17.1</cell>
              <cell>80.6</cell>
            </row>
            <row bottom="minor">
              <cell></cell>
              <cell></cell>
            </row>
            <row>
              <cell>15.5</cell>
              <cell>75.2</cell>
            </row>
            <row bottom="minor">
              <cell></cell>
              <cell></cell>
            </row>
            <row>
              <cell>14.7</cell>
              <cell>69.7</cell>
            </row>
            <row bottom="minor">
              <cell></cell>
              <cell></cell>
            </row>
            <row>
              <cell>17.1</cell>
              <cell>82.0</cell>
            </row>
            <row bottom="minor">
              <cell></cell>
              <cell></cell>
            </row>
            <row>
              <cell>15.4</cell>
              <cell>69.4</cell>
            </row>
            <row bottom="minor">
              <cell></cell>
              <cell></cell>
            </row>
            <row>
              <cell>16.2</cell>
              <cell>83.3</cell>
            </row>
            <row bottom="minor">
              <cell></cell>
              <cell></cell>
            </row>
            <row>
              <cell>15.0</cell>
              <cell>79.6</cell>
            </row>
            <row bottom="minor">
              <cell></cell>
              <cell></cell>
            </row>
            <row>
              <cell>17.2</cell>
              <cell>82.6</cell>
            </row>
            <row bottom="minor">
              <cell></cell>
              <cell></cell>
            </row>
            <row>
              <cell>16.0</cell>
              <cell>80.6</cell>
            </row>
            <row bottom="minor">
              <cell></cell>
              <cell></cell>
            </row>
            <row>
              <cell>17.0</cell>
              <cell>83.5</cell>
            </row>
            <row bottom="minor">
              <cell></cell>
              <cell></cell>
            </row>
            <row>
              <cell>14.4</cell>
              <cell>76.3</cell>
            </row>
            <row bottom="minor">
              <cell></cell>
              <cell></cell>
            </row>
          </tabular>
          <image width="73%" source="10_7_crickets.eps"/>
        </sidebyside>
        The relationship between <m>x</m> and <m>y</m> is not exactly linear,
        but looks to have a linear pattern.
        It could be that the relationship is really linear but experimental error causes the data to be slightly inaccurate.
        Or perhaps the data is not linear,
        but only approximately linear.
        Find the least squares linear approximation to the data.
      </p>
    </li>
    <li>
      <p>
        We showed that if the columns of <m>A</m> are linearly independent,
        then <m>A^{\tr}A</m> is invertible.
        Show that the reverse implication is also true.
        That is, show that if <m>A^{\tr}A</m> is invertible,
        then the columns of <m>A</m> are linearly independent.
      </p>
    </li>
    <li xml:id="ex_6_f_not_li">
      <p>
        Consider the small data set of points <m>S = \{(2,1), (2,2), (2,3)\}</m>.
        <ul>
          <li>
            <p>
              Find a linear system <m>A \vx = \vb</m> whose solution would define a least squares linear approximation to the data in set <m>S</m>.
            </p>
          </li>
          <li>
            <p>
              Explain what happens when we attempt to find the least squares solution <m>\vx^*</m> using the matrix <m>\left(A^{\tr}A\right)^{-1}A^{\tr}</m>.
              Why does this happen?
            </p>
          </li>
          <li>
            <p>
              Does the system <m>A \vx = \vb</m> have a least squares solution?
              If so, how many and what are they?
              If not, why not?
            </p>
          </li>
          <li>
            <p>
              Fit a linear function of the form <m>x = a_0 + a_1y</m> to the data.
              Why should you have expected the answer?
            </p>
          </li>
        </ul>
      </p>
    </li>
    <li xml:id="ex_6_f_ranks">
      <p>
        Let <m>M</m> and <m>N</m> be any matrices such that <m>MN</m> is defined.
        In this exercise we investigate relationships between ranks of various matrices.
        <ul>
          <li>
            <p>
              Show that <m>\Col MN</m> is a subspace of <m>\Col M</m>.
              Use this result to explain why <m>\rank(MN) \leq \rank(M)</m>.
            </p>
          </li>
          <li>
            <p>
              Show that <m>\rank\left(M^{\tr}M\right) = \rank(M) = \rank\left(M^{\tr}\right)</m>.
              <hint>
                <p>
                  For part, see Exercise 12 in Section 15.
                </p>
              </hint>
            </p>
          </li>
          <li>
            <p>
              Show that <m>\rank(MN) \leq \min\{\rank(M), \rank(N)\}</m>.
            </p>
          </li>
        </ul>
      </p>
    </li>
    <li>
      <p>
        We have seen that if the columns of a matrix <m>M</m> are linearly independent, then
        <me>
          \va^* = \left(M^{\tr}M\right)^{-1}M^{\tr} \vb
        </me>
        is a least squares solution to <m>M \va = \vy</m>.
        What if the columns of <m>M</m> are linearly dependent?
        From Activity 26.1, a least squares solution to
        <m>M \va = \vy</m> is a solution to the equation <m>\left(M^{\tr}M\right)\va = M^{\tr}\vy</m>.
        In this exercise we demonstrate that
        <m>\left(M^{\tr}M\right)\va = M^{\tr}\vy</m> always has a solution.
        <ul>
          <li>
            <p>
              Explain why it is enough to show that the rank of the augmented matrix
              <m>[M^{\tr}M \ | \ M^{\tr}\vy]</m> is the same as the rank of <m>M^{\tr}M</m>.
            </p>
          </li>
          <li>
            <p>
              Explain why <m>\rank\left([M^{\tr}M \ | \ M^{\tr}\vy]\right) \geq \rank(M)</m>.
              <hint>
                <p>
                  See <xref ref="ex_6_f_ranks">Exercise</xref>.
                </p>
              </hint>
            </p>
          </li>
          <li>
            <p>
              Explain why <m>[M^{\tr}M \ | \ M^{\tr}\vy] = M^{\tr}[M \ | \ \vy]</m>.
            </p>
          </li>
          <li>
            <p>
              Explain why <m>\rank\left([M^{\tr}M \ | \ M^{\tr}\vy]\right) \leq \rank\left(M^{\tr} \right)</m>.
              <hint>
                <p>
                  See <xref ref="ex_6_f_ranks">Exercise</xref>.
                </p>
              </hint>
            </p>
          </li>
          <li>
            <p>
              Finally, explain why <m>\rank\left([M^{\tr}M \ | \ M^{\tr}\vy]\right) = \rank\left(M^{\tr}M\right)</m>.
            </p>
          </li>
        </ul>
      </p>
    </li>
    <li>
      <p>
        If <m>A</m> is an <m>m \times n</m> matrix with linearly independent columns,
        the least squares solution <m>\vx^* = \left(A^{\tr}A\right)^{-1}A^{\tr} \vb</m> to
        <m>A \vx = \vb</m> has the property that
        <m>A \vx^* = A\left(A^{\tr}A\right)^{-1}A^{\tr} \vb</m> is the vector in <m>\Col A</m> that is closest to <m>\vb</m>.
        That is, <m>A\left(A^{\tr}A\right)^{-1}A^{\tr} \vb</m> is the projection of <m>\vb</m> onto <m>\Col A</m>.
        The matrix <m>P = A\left(A^{\tr}A\right)^{-1}A^{\tr}</m> is called a
        <em>projection matrix</em>.
        Projection matrices have special properties.
        <ul>
          <li>
            <p>
              Show that <m>P^2 = P = P^{\tr}</m>.
            </p>
          </li>
          <li>
            <p>
              In general, we define projection matrices as follows.
              <definition>
                <statement>
                  <p>
                    A square matrix <m>E</m> is a <term>projection matrix</term>
          <idx><h>projection matrix</h></idx>
                    if <m>E^2 = E</m>.
                  </p>
                </statement>
              </definition>
              Show that <m>E = \left[ \begin{array}{cc} 0\amp 1\\ 0\amp 1 \end{array} \right]</m> is a projection matrix.
              Onto what does <m>E</m> project?
            </p>
          </li>
          <li>
            <p>
              Notice that the projection matrix from part (b) is not an orthogonal matrix.
              <definition>
                <statement>
                  <p>
                    A square matrix <m>E</m> is a <term>orthogonal projection matrix</term>
                    if <m>E^2 = E = E^{\tr}</m>.
                  </p>
                </statement>
              </definition>
              Show that <m>E = \left[ \begin{array}{cc} \frac{1}{2}\amp \frac{1}{2} \\ \frac{1}{2}\amp \frac{1}{2} \end{array} \right]</m> is an orthogonal projection matrix.
              Onto what does <m>E</m> project?
            </p>
          </li>
          <li>
            <p>
              If <m>E</m> is an <m>n \times n</m> orthogonal projection matrix,
              show that if <m>\vb</m> is in <m>\R^n</m>,
              then <m>\vb - E\vb</m> is orthogonal to every vector in <m>\Col E</m>.
              (Hence, <m>E</m> projects orthogonally onto <m>\Col E</m>.)
            </p>
          </li>
          <li>
            <p>
              Recall the projection <m>\proj_{\vv} \vu</m> of a vector <m>\vu</m> in the direction of a vector <m>\vv</m> is give by <m>\proj_{\vv} \vu = \frac{\vu \cdot \vv}{\vv \cdot \vv} \vv</m>.
              Show that <m>\proj_{\vv} \vu = E\vu</m>,
              where <m>E</m> is the orthogonal projection matrix <m>\frac{1}{\vv \cdot \vv} \left(\vv \vv^{\tr} \right)</m>.
              Illustrate with <m>\vv = [1 \ 1]^{\tr}</m>.
            </p>
          </li>
        </ul>
      </p>
    </li>
    <li>
      <p>
        Label each of the following statements as True or False.
        Provide justification for your response.
        <ul>
          <li>
            <p>
              <em>True/False</em> If the columns of <m>A</m> are linearly independent,
              then there is a unique least squares solution to <m>A\vx = \vb</m>.
            </p>
          </li>
          <li>
            <p>
              <em>True/False</em> Let <m>\{\vv_1, \vv_2, \ldots, \vv_n\}</m> be an orthonormal basis for <m>\R^n</m>.
              The least squares solution to
              <m>[\vv_1 \ \vv_2 \ \cdots \ \vv_{n-1}] \vx = \vv_n</m> is <m>\vx = \vzero</m>.
            </p>
          </li>
          <li>
            <p>
              <em>True/False</em> The least squares line to the data points <m>(0,0)</m>,
              <m>(1,0)</m>,
              and <m>(0,1)</m> is <m>y=x</m>.
            </p>
          </li>
          <li>
            <p>
              <em>True/False</em> If the columns of a matrix <m>A</m> are not invertible and the vector <m>\vb</m> is not in <m>\Col A</m>,
              then there is no least squares solution to <m>A \vx = \vb</m>.
            </p>
          </li>
          <li>
            <p>
              <em>True/False</em> Every matrix equation of the form
              <m>A \vx = \vb</m> has a least squares solution.
            </p>
          </li>
          <li>
            <p>
              <em>True/False</em> If the columns of <m>A</m> are linearly independent,
              then the least squares solution to
              <m>A \vx = \vb</m> is the orthogonal projection of <m>\vb</m> onto <m>\Col A</m>.
            </p>
          </li>
        </ul>
      </p>
    </li>
  </ol>
</section>