<?xml version="1.0" encoding="UTF-8" ?>
<exercises xml:id="sec_ls_exer">
  
  <exercise>
    <introduction>
      <p>
        The University of Denver Infant Study Center investigated whether babies take longer to learn to crawl in cold months,
        when they are often bundled in clothes that restrict their movement,
        than in warmer months.
        The study sought a relationship between babies' first crawling age and the average temperature during the month they first try to crawl
        (about 6 months after birth).
        Some of the data from the study is in <xref ref="T_7_d_infants"></xref>.
        Let <m>x</m> represent the temperature in degrees Fahrenheit and <m>C(x)</m> the average crawling age in months.
      </p>
        <table xml:id="T_7_d_infants">
          <title>Crawling age</title>
          <tabular left="minor">
            <col top="minor"/>
            <col top="minor" halgin="center"/>
            <col top="minor" halgin="center"/>
            <col top="minor" halgin="center"/>
            <col top="minor" halgin="center"/>
            
            <row bottom="minor">
              <cell right="medium"><m>x</m></cell>
              <cell right="minor">33</cell>
              <cell right="minor">37</cell>
              <cell right="minor">48</cell>
              <cell right="minor">57</cell>
            </row>
            
            <row bottom="minor">
              <cell right="medium"><m>C(x)</m></cell>
              <cell right="minor">33.83</cell>
              <cell right="minor">33.35</cell>
              <cell right="minor">33.38</cell>
              <cell right="minor">32.32</cell>
            </row>
            
          </tabular>
        </table>
      </introduction>
        <task>
          <statement>
            <p>
              Find the least squares line to fit this data.
              Plot the data and your line on the same set of axes.
              (We aren't concerned about whether a linear fit is really a good choice outside of this data set,
              we just fit a line to it to see what happens.)
            </p>
          </statement>
          <answer>
            <p>
              <m>f(x) \approx 35.5273 - 0.0527x</m>
            </p>
          </answer>
        </task>
        <task>
          <statement>
            <p>
              Use your least squares line to predict the average crawling age when the temperature is 65.
            </p>
          </statement>
          <answer>
            <p>
              Approximately <m>32.10</m>.
            </p>
          </answer>
        </task>
  </exercise>

  <exercise>
    <introduction>
      <p>
        The cost, in cents,
        of a first class postage stamp in years from 1981 to 1995 is shown in <xref ref="T_7_d_stamps"></xref>.
      </p>
      <table xml:id="T_7_d_stamps">
        <title>Cost of postage</title>
        <tabular halign="center" left="minor">
          <col top="minor"/>
          <col top="minor"/>
          <col top="minor"/>
          <col top="minor"/>
          <col top="minor"/>
          <col top="minor"/>

          <row bottom="minor">
            <cell right="medium">Year</cell>
            <cell right="minor">1981</cell>
            <cell right="minor">1985</cell>
            <cell right="minor">1988</cell>
            <cell right="minor">1991</cell>
            <cell right="minor">1995</cell>
          </row>
          
          <row bottom="minor">
            <cell right="medium">Cost</cell>
            <cell right="minor">20</cell>
            <cell right="minor">22</cell>
            <cell right="minor">25</cell>
            <cell right="minor">29</cell>
            <cell right="minor">32</cell>
          </row>
          
        </tabular>
      </table>
    </introduction>
        <task>
          <statement>
            <p>
              Find the least squares line to fit this data.
              Plot the data and your line on the same set of axes.
            </p>
          </statement>
        </task>
        <task>
          <statement>
            <p>
              Now find the least squares quadratic approximation to this data.
              Plot the quadratic function on same axes as your linear function.
            </p>
          </statement>
        </task>
        <task>
          <statement>
            <p>
              Use your least squares line and quadratic to predict the cost of a postage stamp in this year.
              Look up the cost of a stamp today and determine how accurate your prediction is.
              Which function gives a better approximation?
              Provide reasons for any discrepancies.
            </p>
          </statement>
        </task>
  </exercise>  
  
  <exercise>
    <statement>
      <p>
        According to <pubtitle>The Song of Insects</pubtitle>
        by G.W. Pierce (Harvard College Press, 1948) the sound of striped ground crickets chirping,
        in number of chirps per second,
        is related to the temperature.
        So the number of chirps per second could be a predictor of temperature.
        The data Pierce collected is shown in the table and scatterplot below,
        where <m>x</m> is the (average) number of chirps per second and <m>y</m> is the temperature in degrees Fahrenheit.
      </p>
        <sidebyside>
          <tabular left="minor" halign="center">
            <col top="minor"/>
            <col top="minor"/>

            <row bottom="minor">
              <cell right="minor"><m>x</m></cell>
              <cell right="minor"><m>y</m></cell>
            </row>
           
            <row bottom="minor">
              <cell right="minor">20.0</cell>
              <cell right="minor">88.6</cell>
            </row>
            
            <row bottom="minor">
              <cell right="minor">16.0</cell>
              <cell right="minor">71.6</cell>
            </row>
            
            
            <row bottom="minor">
              <cell right="minor">19.8</cell>
              <cell right="minor">93.3</cell>
            </row>
            
            <row bottom="minor">
              <cell right="minor">18.4</cell>
              <cell right="minor">84.3</cell>
            </row>
            
            <row bottom="minor">
              <cell right="minor">17.1</cell>
              <cell right="minor">80.6</cell>
            </row>
            
            <row bottom="minor">
              <cell right="minor">15.5</cell>
              <cell right="minor">75.2</cell>
            </row>
           
            <row bottom="minor">
              <cell right="minor">14.7</cell>
              <cell right="minor">69.7</cell>
            </row>
            
            <row bottom="minor">
              <cell right="minor">17.1</cell>
              <cell right="minor">82.0</cell>
            </row>
            
            <row bottom="minor">
              <cell right="minor">15.4</cell>
              <cell right="minor">69.4</cell>
            </row>
            
            <row bottom="minor">
              <cell right="minor">16.2</cell>
              <cell right="minor">83.3</cell>
            </row>
            
            <row bottom="minor">
              <cell right="minor">15.0</cell>
              <cell right="minor">79.6</cell>
            </row>
           
            <row bottom="minor">
              <cell right="minor">17.2</cell>
              <cell right="minor">82.6</cell>
            </row>
            
            <row bottom="minor">
              <cell right="minor">16.0</cell>
              <cell right="minor">80.6</cell>
            </row>
            
            <row bottom="minor">
              <cell right="minor">17.0</cell>
              <cell right="minor">83.5</cell>
            </row>
            
            <row bottom="minor">
              <cell right="minor">14.4</cell>
              <cell right="minor">76.3</cell>
            </row>
            
          </tabular>

          <image width="30%" source="10_7_crickets"/>
        </sidebyside>
      <p>
        The relationship between <m>x</m> and <m>y</m> is not exactly linear,
        but looks to have a linear pattern.
        It could be that the relationship is really linear but experimental error causes the data to be slightly inaccurate.
        Or perhaps the data is not linear,
        but only approximately linear.
        Find the least squares linear approximation to the data.
      </p>
    </statement>
    <answer>
      <p>
        <m>f(x) \approx 25.124 + 3.297x</m>
      </p>
    </answer>
  </exercise>

  <exercise>
    <statement>
      <p>
        We showed that if the columns of <m>A</m> are linearly independent,
        then <m>A^{\tr}A</m> is invertible.
        Show that the reverse implication is also true.
        That is, show that if <m>A^{\tr}A</m> is invertible,
        then the columns of <m>A</m> are linearly independent.
      </p>
    </statement>
  </exercise>

  <exercise xml:id="ex_6_f_not_li">
    <introduction>
      <p>
        Consider the small data set of points <m>S = \{(2,1), (2,2), (2,3)\}</m>.
      </p>
    </introduction>
        <task>
          <statement>
            <p>
              Find a linear system <m>A \vx = \vb</m> whose solution would define a least squares linear approximation to the data in set <m>S</m>.
            </p>
          </statement>
          <answer>
            <p>
              <m>\left[ \begin{array}{cc} 1\amp 2 \\ 1\amp 2 \\ 1\amp 2 \end{array} \right] \left[ \begin{array}{c} a_0 \\ a_1 \end{array} \right] = \left[ \begin{array}{c} 1\\2\\3 \end{array} \right]</m>
            </p>
          </answer>
        </task>
        <task>
          <statement>
            <p>
              Explain what happens when we attempt to find the least squares solution <m>\vx^*</m> using the matrix <m>\left(A^{\tr}A\right)^{-1}A^{\tr}</m>.
              Why does this happen?
            </p>
          </statement>
          <answer>
            <p>
              <m>A^{\tr}A</m> is not invertible
            </p>
          </answer>
        </task>
        <task>
          <statement>
            <p>
              Does the system <m>A \vx = \vb</m> have a least squares solution?
              If so, how many and what are they?
              If not, why not?
            </p>
          </statement>
          <answer>
            <p>
              Infinitely many.
            </p>
          </answer>
        </task>
        <task>
          <statement>
            <p>
              Fit a linear function of the form <m>x = a_0 + a_1y</m> to the data.
              Why should you have expected the answer?
            </p>
          </statement>
          <answer>
            <p>
              <m>x=2</m>
            </p>
          </answer>
        </task>
  </exercise>

  <exercise xml:id="ex_6_f_ranks">
    <introduction>
      <p>
        Let <m>M</m> and <m>N</m> be any matrices such that <m>MN</m> is defined.
        In this exercise we investigate relationships between ranks of various matrices.
      </p>
    </introduction>
        <task>
          <statement>
            <p>
              Show that <m>\Col MN</m> is a subspace of <m>\Col M</m>.
              Use this result to explain why <m>\rank(MN) \leq \rank(M)</m>.
            </p>
          </statement>
        </task>
        <task>
          <statement>
            <p>
              Show that <m>\rank\left(M^{\tr}M\right) = \rank(M) = \rank\left(M^{\tr}\right)</m>.
              
            </p>
          </statement>
          <hint>
            <p>
              For part, see Exercise 12 in Section 15.
            </p>
          </hint>
        </task>
        <task>
          <statement>
            <p>
              Show that <m>\rank(MN) \leq \min\{\rank(M), \rank(N)\}</m>.
            </p>
          </statement>
        </task>
  </exercise>

  <exercise>
    <introduction>
      <p>
        We have seen that if the columns of a matrix <m>M</m> are linearly independent, then
        <me>
          \va^* = \left(M^{\tr}M\right)^{-1}M^{\tr} \vb
        </me>
        is a least squares solution to <m>M \va = \vy</m>.
        What if the columns of <m>M</m> are linearly dependent?
        From Activity 26.1, a least squares solution to
        <m>M \va = \vy</m> is a solution to the equation <m>\left(M^{\tr}M\right)\va = M^{\tr}\vy</m>.
        In this exercise we demonstrate that
        <m>\left(M^{\tr}M\right)\va = M^{\tr}\vy</m> always has a solution.
      </p>
    </introduction>
        <task>
          <statement>
            <p>
              Explain why it is enough to show that the rank of the augmented matrix
              <m>[M^{\tr}M \ | \ M^{\tr}\vy]</m> is the same as the rank of <m>M^{\tr}M</m>.
            </p>
          </statement>
          <answer>
            <p>
              The augmented column of <m>[M^{\tr}M \ | \ M^{\tr}\vy]</m> cannot be a pivot column.
            </p>
          </answer>
        </task>
        <task>
          <statement>
            <p>
              Explain why <m>\rank\left([M^{\tr}M \ | \ M^{\tr}\vy]\right) \geq \rank(M)</m>.
              
            </p>
          </statement>
          <hint>
            <p>
              See <xref ref="ex_6_f_ranks"></xref>.
            </p>
          </hint>
        </task>
        <task>
          <statement>
            <p>
              Explain why <m>[M^{\tr}M \ | \ M^{\tr}\vy] = M^{\tr}[M \ | \ \vy]</m>.
            </p>
          </statement>
          <hint>
            <p>
              Use the definition of the matrix product.
            </p>
          </hint>
        </task>
        <task>
          <statement>
            <p>
              Explain why <m>\rank\left([M^{\tr}M \ | \ M^{\tr}\vy]\right) \leq \rank\left(M^{\tr} \right)</m>.
              
            </p>
          </statement>
          <hint>
            <p>
              See <xref ref="ex_6_f_ranks"></xref>.
            </p>
          </hint>
        </task>
        <task>
          <statement>
            <p>
              Finally, explain why <m>\rank\left([M^{\tr}M \ | \ M^{\tr}\vy]\right) = \rank\left(M^{\tr}M\right)</m>.
            </p>
          </statement>
          <hint>
            <p>
              Combine parts (b) and (d).
            </p>
          </hint>
        </task>
  </exercise>

  <exercise>
    <introduction>
      <p>
        If <m>A</m> is an <m>m \times n</m> matrix with linearly independent columns,
        the least squares solution <m>\vx^* = \left(A^{\tr}A\right)^{-1}A^{\tr} \vb</m> to
        <m>A \vx = \vb</m> has the property that
        <m>A \vx^* = A\left(A^{\tr}A\right)^{-1}A^{\tr} \vb</m> is the vector in <m>\Col A</m> that is closest to <m>\vb</m>.
        That is, <m>A\left(A^{\tr}A\right)^{-1}A^{\tr} \vb</m> is the projection of <m>\vb</m> onto <m>\Col A</m>.
        The matrix <m>P = A\left(A^{\tr}A\right)^{-1}A^{\tr}</m> is called a
        <term>projection matrix</term>.
        Projection matrices have special properties.
      </p>
    </introduction>
        <task>
          <statement>
            <p>
              Show that <m>P^2 = P = P^{\tr}</m>.
            </p>
          </statement>
        </task>
        <task>
          <statement>
            <p>
              In general, we define projection matrices as follows.
              <definition>
              <idx><h>projection matrix</h></idx>
                <statement>
                  <p>
                    A square matrix <m>E</m> is a <term>projection matrix</term>
          
                    if <m>E^2 = E</m>.
                  </p>
                </statement>
              </definition>
              Show that <m>E = \left[ \begin{array}{cc} 0\amp 1\\ 0\amp 1 \end{array} \right]</m> is a projection matrix.
              Onto what does <m>E</m> project?
            </p>
          </statement>
        </task>
        <task>
          <statement>
            <p>
              Notice that the projection matrix from part (b) is not an orthogonal matrix.
              <definition>
                <statement>
                  <p>
                    A square matrix <m>E</m> is a <term>orthogonal projection matrix</term>
                    if <m>E^2 = E = E^{\tr}</m>.
                  </p>
                </statement>
              </definition>
              Show that <m>E = \left[ \begin{array}{cc} \frac{1}{2}\amp \frac{1}{2} \\ \frac{1}{2}\amp \frac{1}{2} \end{array} \right]</m> is an orthogonal projection matrix.
              Onto what does <m>E</m> project?
            </p>
          </statement>
        </task>
        <task>
          <statement>
            <p>
              If <m>E</m> is an <m>n \times n</m> orthogonal projection matrix,
              show that if <m>\vb</m> is in <m>\R^n</m>,
              then <m>\vb - E\vb</m> is orthogonal to every vector in <m>\Col E</m>.
              (Hence, <m>E</m> projects orthogonally onto <m>\Col E</m>.)
            </p>
          </statement>
        </task>
        <task>
          <statement>
            <p>
              Recall the projection <m>\proj_{\vv} \vu</m> of a vector <m>\vu</m> in the direction of a vector <m>\vv</m> is give by <m>\proj_{\vv} \vu = \frac{\vu \cdot \vv}{\vv \cdot \vv} \vv</m>.
              Show that <m>\proj_{\vv} \vu = E\vu</m>,
              where <m>E</m> is the orthogonal projection matrix <m>\frac{1}{\vv \cdot \vv} \left(\vv \vv^{\tr} \right)</m>.
              Illustrate with <m>\vv = [1 \ 1]^{\tr}</m>.
            </p>
          </statement>
        </task>
  </exercise>

  <exercise>
    <introduction>
      <p>
        Label each of the following statements as True or False.
        Provide justification for your response.
      </p>
    </introduction>
        <task>
          <title>True/False</title>
          <statement>
            <p>
              If the columns of <m>A</m> are linearly independent,
              then there is a unique least squares solution to <m>A\vx = \vb</m>.
            </p>
          </statement>
          <answer>
            <p>
              T
            </p>
          </answer>
        </task>
        <task>
          <title>True/False</title>
          <statement>
            <p>
              Let <m>\{\vv_1, \vv_2, \ldots, \vv_n\}</m> be an orthonormal basis for <m>\R^n</m>.
              The least squares solution to
              <m>[\vv_1 \ \vv_2 \ \cdots \ \vv_{n-1}] \vx = \vv_n</m> is <m>\vx = \vzero</m>.
            </p>
          </statement>
        </task>
        <task>
          <title>True/False</title>
          <statement>
            <p>
              The least squares line to the data points <m>(0,0)</m>,
              <m>(1,0)</m>,
              and <m>(0,1)</m> is <m>y=x</m>.
            </p>
          </statement>
          <answer>
            <p>
              F
            </p>
          </answer>
        </task>
        <task>
          <title>True/False</title>
          <statement>
            <p>
              If the columns of a matrix <m>A</m> are not invertible and the vector <m>\vb</m> is not in <m>\Col A</m>,
              then there is no least squares solution to <m>A \vx = \vb</m>.
            </p>
          </statement>
        </task>
        <task>
          <title>True/False</title>
          <statement>
            <p>
              Every matrix equation of the form
              <m>A \vx = \vb</m> has a least squares solution.
            </p>
          </statement>
          <answer>
            <p>
              T
            </p>
          </answer>
        </task>
        <task>
          <title>True/False</title>
          <statement>
            <p>
              If the columns of <m>A</m> are linearly independent,
              then the least squares solution to
              <m>A \vx = \vb</m> is the orthogonal projection of <m>\vb</m> onto <m>\Col A</m>.
            </p>
          </statement>
        </task>
  
  </exercise>
</exercises>