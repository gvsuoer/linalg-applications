<?xml version="1.0" encoding="UTF-8" ?>
<section xml:id="sec_pseudoinverses">
  <title>Pseudoinverses</title>
  <p>
    Not every matrix is invertible,
    so we cannot always solve a matrix equation <m>A \vx = \vb</m>.
    However, every matrix has a pseudoinverse <m>A^+</m> that acts something like an inverse.
    Even when we can't solve a matrix equation
    <m>A \vx = \vb</m> because <m>\vb</m> isn't in <m>\Col A</m>,
    we can use the pseudoinverse of <m>A</m> to
    <q>solve</q>
    the equation <m>A \vx = \vb</m> with the
    <q>solution</q>
    <m>A^+ \vb</m>.
    While not an exact solution,
    <m>A^+ \vb</m> turns out to be the best approximation to a solution in the least squares sense.
    We will use the singular value decomposition to find the pseudoinverse of a matrix.
  </p>

  <exploration xml:id="pa_7_d_2">
    <introduction>
    <p>
      Let <m>A = \left[\begin{array}{ccc} 1\amp 1\amp 0\\ 0\amp 1\amp 1 \end{array}  \right]</m>.
      The singular value decomposition of <m>A</m> is <m>U \Sigma V^{\tr}</m> where
      <md>
        <mrow>U \amp = \frac{\sqrt{2}}{2} \left[ \begin{array}{cr} 1\amp -1</mrow>
        <mrow>1\amp 1 \end{array} \right],</mrow>
        <mrow>\Sigma \amp = \left[ \begin{array}{ccc} \sqrt{3}\amp 0\amp 0</mrow>
        <mrow>0\amp 1\amp 0 \end{array} \right],</mrow>
        <mrow>V \amp = \frac{1}{6} \left[ \begin{array}{rrr} \sqrt{6}\amp -3\sqrt{2}\amp 2\sqrt{3}</mrow>
        <mrow>2\sqrt{6}\amp 0\amp -2\sqrt{3}</mrow>
        <mrow>\sqrt{6}\amp 3\sqrt{2}\amp 2\sqrt{3} \end{array} \right]</mrow>
      </md>.
    </p>
    </introduction>
      <task>
        <statement>
          <p>
            Explain why <m>A</m> is not an invertible matrix.
          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            Explain why the matrices <m>U</m> and <m>V</m> are invertible.
            How are <m>U^{-1}</m> and <m>V^{-1}</m> related to <m>U^{\tr}</m> and <m>V^{\tr}</m>?
          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            Recall that one property of invertible matrices is that the inverse of a product of invertible matrices is the product of the inverses in the reverse order.
            If <m>A</m> were invertible,
            then <m>A^{-1}</m> would be <m>\left(U \Sigma V^{\tr}\right)^{-1} = V \Sigma^{-1} U^{\tr}</m>.
            Even though <m>U</m> and <m>V</m> are invertible,
            the matrix <m>\Sigma</m> is not.
            But <m>\Sigma</m> does contain non-zero eigenvalues that have reciprocals,
            so consider the matrix <m>\Sigma^+ = \left[ \begin{array}{cc} \frac{1}{\sqrt{3}}\amp 0 \\ 0\amp 1 \\ 0\amp 0 \end{array} \right]</m>.
            Calculate the products <m>\Sigma \Sigma^+</m> and <m>\Sigma^+ \Sigma</m>.
            How are the results similar to that obtained with a matrix inverse?
          </p>
        </statement>
      </task>
      <task>
        <introduction>
          <p>
            The only matrix in the singular value decomposition of <m>A</m> that is not invertible is <m>\Sigma</m>.
            But the matrix <m>\Sigma^{+}</m> acts somewhat like an inverse of <m>\Sigma</m>,
            so let us define <m>A^+</m> as <m>V \Sigma^+ U^{\tr}</m>.
            Now we explore a few properties of the matrix <m>A^{+}</m>.
          </p>
        </introduction>
            <task>
              <statement>
                <p>
                  Calculate <m>AA^+</m> and <m>A^+A</m> for <m>A = \left[\begin{array}{ccc} 1\amp 1\amp 0\\ 0\amp 1\amp 1 \end{array} \right]</m>.
                  What do you notice?
                </p>
              </statement>
            </task>
            <task>
              <statement>
                <p>
                  Calculate <m>A^+AA^+</m> and <m>AA^+A</m> for <m>A = \left[\begin{array}{ccc} 1\amp 1\amp 0\\ 0\amp 1\amp 1 \end{array} \right]</m>.
                  What do you notice?
                </p>
              </statement>
            </task>
          
        </task>
  
  </exploration>

  <p>
    Only some square matrices have inverses.
    However, every matrix has a pseudoinverse.
    A pseudoinverse <m>A^{+}</m> of a matrix <m>A</m> provides something like an inverse when a matrix doesn't have an inverse.
    Pseudoinverses are useful to approximate solutions to linear systems.
    If <m>A</m> is invertible,
    then the equation <m>A \vx = \vb</m> has the solution <m>\vx = A^{-1}\vb</m>,
    but when <m>A</m> is not invertible and <m>\vb</m> is not in <m>\Col A</m>,
    then the equation <m>A \vx = \vb</m> has no solution.
    In the invertible case of an <m>n \times n</m> matrix <m>A</m>,
    there is a matrix <m>B</m> so that <m>AB = BA = I_n</m>.
    This also implies that <m>BAB = B</m> and <m>ABA = A</m>.
    To mimic this situation when <m>A</m> is not invertible,
    we search for a matrix <m>A^+</m> (a pseudoinverse of <m>A</m>) so that
    <m>AA^+A = A</m> and <m>A^+AA^+ = A^+</m>,
    as we saw in <xref ref="pa_7_d_2"></xref>.
    Then it turns out that <m>A^+</m> acts something like an inverse for <m>A</m>.
    In this case, we approximate the solution to
    <m>A \vx = \vb</m> by <m>\vx^* = A^+\vb</m>,
    and we will see that the vector
    <m>A\vx^* = AA^+\vb</m> turns out to be the vector in <m>\Col A</m> that is closest to <m>\vb</m> in the least squares sense.
  </p>
  <p>
    A reasonable question to ask is how we can find a pseudoinverse of a matrix <m>A</m>.
    A singular value decomposition provides an answer to this question.
    If <m>A</m> is an invertible <m>n \times n</m> matrix,
    then 0 is not an eigenvalue of <m>A</m>.
    As a result,
    in the singular value decomposition <m>U \Sigma V^{\tr}</m> of <m>A</m>,
    the matrix <m>\Sigma</m> is an invertible matrix
    (note that <m>U</m>, <m>\Sigma</m>,
    and <m>V</m> are all <m>n \times n</m> matrices in this case).
    So
    <me>
      A^{-1} = \left(U \Sigma V^{\tr}\right)^{-1} = V \Sigma^{-1} U^{\tr}
    </me>,
    where
    <me>
      \Sigma^{-1} = \left[ \begin{array}{ccccc} \frac{1}{\sigma_1}\amp \amp \amp \amp  \\  \amp  \frac{1}{\sigma_2}\amp \amp 0\amp  \\  \amp \amp  \frac{1}{\sigma_3}\amp \amp  \\ \amp 0  \amp  \amp  \ddots \amp   \\ \amp \amp \amp \amp  \frac{1}{\sigma_n} \end{array}  \right]
    </me>.
  </p>
  <p>
    In this case,
    <m>V \Sigma^{-1} U^{\tr}</m> is a singular value decomposition for <m>A^{-1}</m>.
  </p>
  <p>
    To understand in general how a pseudoinverse is found,
    let <m>A</m> be an <m>m \times n</m> matrix with <m>m \neq n</m>,
    or an <m>n \times n</m> with rank less than <m>n</m>.
    In these cases <m>A</m> does not have an inverse.
    But as in <xref ref="pa_7_d_2"></xref>,
    a singular value decomposition provides a pseudoinverse <m>A^+</m> for <m>A</m>.
    Let <m>U \Sigma V^{\tr}</m> be a singular value decomposition of an
    <m>m \times n</m> matrix <m>A</m> of rank <m>r</m>, with
    <me>
      \Sigma = \left[ \begin{array}{ccccc|c} \sigma_1\amp \amp \amp \amp \amp  \\ \amp  \sigma_2\amp \amp 0\amp \amp 0 \\ \amp \amp  \sigma_3\amp \amp \amp  \\ \amp  0 \amp  \amp  \ddots \amp  \amp  \\ \amp \amp \amp \amp  \sigma_r \\ \hline \amp \amp 0\amp \amp \amp 0 \end{array}  \right]
    </me>
  </p>
  <p>
  <idx><h>pseudoinverse</h></idx>
    The matrices <m>U</m> and <m>V</m> are invertible,
    but the matrix <m>\Sigma</m> is not if <m>A</m> is not invertible.
    If we let <m>\Sigma^+</m> be the <m>n \times m</m> matrix defined by
    <me>
      \Sigma^{+} = \left[ \begin{array}{ccccc|c} \frac{1}{\sigma_1}\amp \amp \amp \amp \amp  \\ \amp  \frac{1}{\sigma_2}\amp \amp 0\amp \amp 0 \\ \amp \amp  \frac{1}{\sigma_3}\amp \amp \amp  \\ \amp  0 \amp  \amp  \ddots \amp  \amp  \\ \amp \amp \amp \amp  \frac{1}{\sigma_r} \\ \hline \amp \amp 0\amp \amp \amp 0 \end{array}  \right]
    </me>,
    then <m>\Sigma^{+}</m> will act much like an inverse of <m>\Sigma</m> might.
    In fact, it is not difficult to see that
    <me>
      \Sigma\Sigma^{+} = \left[ \begin{array}{c|c} I_r\amp 0 \\ \hline 0\amp 0 \end{array}  \right] \text{ and }  \Sigma^{+}\Sigma = \left[ \begin{array}{c|c} I_r\amp 0 \\ \hline 0\amp 0 \end{array}  \right]
    </me>,
    where <m>\Sigma\Sigma^{+}</m> is an <m>m \times m</m> matrix and
    <m>\Sigma^{+}\Sigma</m> is an <m>n \times n</m> matrix.
  </p>
  <p>
    The matrix
    <men xml:id="eq_pseudoinverse">
      A^{+} = V\Sigma^{+}U^{\tr}
    </men>
    is a <term>pseudoinverse</term>
    of <m>A</m>.
  </p>

  <activity>
    <task>
      <statement>
        <p>
          Find the pseudoinverse <m>A^+</m> of <m>A = \left[ \begin{array}{rc} 0\amp 5\\ 4\amp 3 \\ -2\amp 1 \end{array}  \right]</m>.
          Use the singular value decomposition <m>U \Sigma V^{\tr}</m> of <m>A</m>, where
          <me>
            U =  \left[ \begin{array}{crc} \frac{\sqrt{2}}{2}\amp \frac{\sqrt{3}}{3}\amp 1 \\ \frac{\sqrt{2}}{2}\amp -\frac{\sqrt{3}}{3}\amp 0 \\ 0\amp \frac{\sqrt{3}}{3}\amp 0 \end{array}  \right], \ \Sigma =  \left[ \begin{array}{cc} \sqrt{40}\amp 0 \\ 0\amp \sqrt{15} \\ 0\amp 0 \end{array}  \right], \ V = \frac{1}{\sqrt{5}} \left[ \begin{array}{cr} 1\amp -2 \\ 2\amp 1 \end{array}  \right]
          </me>.
        </p>
      </statement>
    </task>
    <task>
      <statement>
        <p>
          The vector <m>\vb = \left[ \begin{array}{c} 0\\0\\1 \end{array} \right]</m> is not in <m>\Col A</m>.
          The vector <m>\vx^* = A^+ \vb</m> is an approximation to a solution of <m>A \vx = \vb</m>,
          and <m>AA^+\vb</m> is in <m>\Col A</m>.
          Find <m>A\vx^*</m> and determine how far <m>A\vx^*</m> is from <m>\vb</m>.
        </p>
      </statement>
    </task>
  </activity>

  <p>
    Pseudoinverses satisfy several properties that are similar to those of inverses.
    For example,
    we had an example in <xref ref="pa_7_d_2"></xref>
    where <m>AA^{+}A = A</m> and <m>A^+AA^+ = A^+</m>.
    That <m>A^+</m> always satisfies these properties is the subject of the next activity.
  </p>

  <activity xml:id="act_7_d_Moore-Penrose">
    <introduction>
    <p>
      Let <m>A</m> be an <m>m \times n</m> matrix with singular value decomposition <m>U \Sigma V^{\tr}</m>.
      Let <m>A^{+}</m> be defined as in <xref ref="eq_pseudoinverse"/>.
    </p>
    </introduction>
      <task>
        <statement>
          <p>
            Show that <m>AA^{+}A = A</m>.
          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            Show that <m>A^{+}AA^{+} = A^{+}</m>.
          </p>
        </statement>
      </task>
    
  </activity>

  <p>
    <xref ref="act_7_d_Moore-Penrose"></xref>
    shows that <m>A^{+}</m> satisfies properties that are similar to those of an inverse of <m>A</m>.
    In fact, <m>A^{+}</m> satisfies several other properties
    (that together can be used as defining properties)
    as stated in the next theorem.
    The conditions of <xref ref="thm_7_d_pseudoinverse"></xref>
    are called the <term>Penrose</term>
    or <term>Moore-Penrose</term> conditions.<fn>
    <xref ref="thm_7_d_pseudoinverse"></xref>
    is often given as the definition of a pseudoinverse.
    </fn> Verification of the remaining parts of this theorem are left for the exercises.
  </p>
  
  <theorem xml:id="thm_7_d_pseudoinverse">
    <title>The Moore-Penrose Conditions</title>
    <statement>
      <p>
        A pseudoinverse of a matrix <m>A</m> is a matrix <m>A^+</m> that satisfies the following properties.
        <ol>
          <li>
            <p>
              <m>AA^{+}A = A</m>
            </p>
          </li>
          <li>
            <p>
              <m>A^{+}AA^{+} = A^{+}</m>
            </p>
          </li>
          <li>
            <p>
              <m>(AA^{+})^{\tr} = AA^{+}</m>
            </p>
          </li>
          <li>
            <p>
              <m>(A^{+}A)^{\tr} = A^{+}A</m>
            </p>
          </li>
        </ol>
      </p>
    </statement>
  </theorem>
  <p>
    Also, there is a unique matrix <m>A^+</m> that satisfies these properties.
    The verification of this property is left to the exercises.
  </p>
</section>